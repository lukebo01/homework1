<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.14886] The VoxCeleb Speaker Recognition Challenge: A Retrospective</title><meta property="og:description" content="The VoxCeleb Speaker Recognition Challenges (VoxSRC) were a series of challenges and workshops that ran annually from 2019 to 2023.
The challenges primarily evaluated the tasks of speaker recognition and diarisation un…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The VoxCeleb Speaker Recognition Challenge: A Retrospective">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The VoxCeleb Speaker Recognition Challenge: A Retrospective">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.14886">

<!--Generated on Thu Sep  5 17:27:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Speaker verification,  Speaker diarisation
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The VoxCeleb Speaker Recognition Challenge: A Retrospective</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jaesung Huh, Joon Son Chung, Arsha Nagrani, Andrew Brown, Jee-weon Jung, Daniel Garcia-Romero, Andrew Zisserman
</span><span class="ltx_author_notes">
This work is funded by the EPSRC programme grant EP/T028572/1 VisualAI and by the MSIT, Korea, under the ITRC (Information Technology Research Center) support program (IITP-2024-RS-2023-00259991) supervised by the IITP.
For the purpose of Open Access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.
Jaesung Huh and Andrew Zisserman are with Visual Geometry Group, University of Oxford, 25 Banbury Rd, Oxford, Oxfordshire, UK, OX2 6NN (e-mail: <a href="mailto:jaesung@robots.ox.ac.uk" title="" class="ltx_ref ltx_href">jaesung@robots.ox.ac.uk</a>;<a href="mailto:az@robots.ox.ac.uk" title="" class="ltx_ref ltx_href">az@robots.ox.ac.uk</a>).Arsha Nagrani was at Visual Geometry Group, University of Oxford when this work was done. She is currently with Google Research, Cambridge, MA 02142, USA (e-mail:<a href="mailto:anagrani@google.com" title="" class="ltx_ref ltx_href">anagrani@google.com</a>).Andrew Brown was at Visual Geometry Group, University of Oxford when this work was done. He is currently with GenAI, Meta, New York, NY 10003 USA (e-mail: <a href="mailto:aebrown@meta.com" title="" class="ltx_ref ltx_href">aebrown@meta.com</a>).Joon Son Chung is with the Korea Advanced Institute of Science, and
Technology, Daejeon 34141, South Korea (e-mail: <a href="mailto:joonson@kaist.ac.kr" title="" class="ltx_ref ltx_href">joonson@kaist.ac.kr</a>).Jee-weon Jung is with Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, Pennsylvania, USA (e-mail: <a href="mailto:jeeweonj@ieee.org" title="" class="ltx_ref ltx_href">jeeweonj@ieee.org</a>).Daniel Garcia-Romero was at Johns Hopkins University, USA when this work was done. He is now with AWS AI, Arlington, VA 22202 USA (e-mail: <a href="mailto:dgromero@amazon.com" title="" class="ltx_ref ltx_href">dgromero@amazon.com</a>). </span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The VoxCeleb Speaker Recognition Challenges (VoxSRC) were a series of challenges and workshops that ran annually from 2019 to 2023.
The challenges primarily evaluated the tasks of speaker recognition and diarisation under various settings including: closed and open
training data; as well as supervised, self-supervised, and semi-supervised training for domain adaptation.
The challenges also provided publicly available training and evaluation datasets for each task and setting, with new test sets released each year.</p>
<p id="id2.id2" class="ltx_p">In this paper, we provide a review of these challenges that covers: what they explored; the methods developed by the
challenge participants and how these evolved; and also the current state of the field for speaker verification and diarisation.
We chart the progress in performance over the five installments of the challenge on a common evaluation dataset and provide a detailed analysis of how each year’s special focus affected participants’ performance.</p>
<p id="id3.id3" class="ltx_p">This paper is aimed both at researchers who want an overview of the speaker recognition and diarisation field, and also at challenge organisers who want to benefit from the successes and avoid the mistakes of the VoxSRC challenges.
We end with a discussion of the current strengths of the field and open challenges.
Project page : <a target="_blank" href="https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/workshop.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/workshop.html</a></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Speaker verification, Speaker diarisation

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The VoxCeleb Speaker Recognition Challenges (VoxSRC) were a series of challenges held annually from 2019 to 2023 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
The goals of the challenges were threefold: (i) to explore and promote novel research in the field of speaker recognition and diarisation, encouraging important directions such as self-supervised learning and domain adaptation; (ii) to measure and calibrate the state of the art through public evaluation tools; and (iii) to provide free and open-source data to the community, accessible to all.
The primary tasks were <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">speaker verification</span> (“do these two speech segments come from the same speaker?”) and <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">speaker diarisation</span> (“label a multispeaker segment with who speaks when”).
VoxSRC consisted of an annual competition and workshop (co-located with the Interspeech conference), where each year’s results and methods were discussed.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">When the challenge was first introduced in 2019, there were already a number of noteworthy speaker recognition challenges, such as those organised by the National Institute
in Standards of Technology (NIST) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, and Speakers In the Wild (SITW) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and speaker diarisation challenges, such as DIHARD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
While these challenges provide immeasurable value to the community, the goal of VoxSRC was to provide complementary support in the form of open-source contributions – all the training and validation data was (and will continue to be) free and available to
researchers irrespective of whether they enter the challenges
or not, and evaluation is performed via a public leaderboard that is visible to all.
VoxSRC workshops were also <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">free</span> for participants to attend.
The focus of VoxSRC has been on unconstrained speech from the web, with data consisting of noisy, varied and sometimes very short and fleeting speech segments.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This paper serves as a retrospective on all five VoxSRC challenges, including their mechanics, methods, results, and discussion.
We hope that this paper will be useful for two audiences: (i) <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">speaker recognition researchers</span> aiming to determine what the state of the art in the field has been over the last five years, as measured by performance on the VoxSRC datasets, as well as wishing to understand how best practises and techniques have evolved during this period; (ii) <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">challenge organisers</span>, for whom we hope our learnings serve as a useful guide for the organisation of future challenges.
The structure of this report is as follows: we begin by describing the two primary tasks, <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">speaker verification</span> and <span id="S1.p3.1.4" class="ltx_text ltx_font_italic">speaker diarisation</span> and tracks we have hosted over the last five years (Section <a href="#S2" title="II Tasks and Tracks ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>).
We then describe the datasets (Section <a href="#S3" title="III Datasets ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>), and challenge mechanics (Section <a href="#S4" title="IV Challenge mechanics ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>), followed by results for each track (Section <a href="#S5" title="V Results ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>) and a detailed analysis of the winners’ method (Section <a href="#S6" title="VI Detailed analysis ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>).
We describe how we hosted the workshops (Section <a href="#S7" title="VII Workshop ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a>), and finally we reflect on the trends over the years of the challenge, discuss current challenges in speaker recognition, and conclude with lessons for future challenge organisers (Section <a href="#S8" title="VIII Discussion and the Future ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VIII</span></a>).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.14886/assets/Figures/timeline.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Timeline showing the progression of the VoxSRC workshops (dots), as well as when key datasets were released (triangles).</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Tasks and Tracks</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section provides an overview of the tracks hosted over the past five years, covering two primary challenge tasks: <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">speaker verification</span> and <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">speaker diarisation</span>.
Within the speaker verification task, there were four different tracks over the last four years depending on the type of data participants are allowed to use.
In the following, we first specify the task description, and then describe the dataset constraints and the motivation behind each of the tracks.
Please also refer to Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> which shows the progression of the VoxSRC workshops.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Speaker verification (2019–2023)</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The speaker verification task is to determine whether two utterances are spoken by the same speaker or not.
The evaluation is conducted using a list of utterance pairs (i.e. trials), and each trial is processed independently.
VoxSRC participants submitted a real-valued prediction score for each trial pair so that evaluation could be performed using our metrics (See Section <a href="#S4" title="IV Challenge mechanics ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>).
Four tracks were defined based on the choice of training data: (i) Speaker verification – closed, (ii) Speaker verification – open, (iii) Self-supervised speaker verification, and (iv) Semi-supervised domain adaptation.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS1.5.1.1" class="ltx_text">II-A</span>1 </span>Speaker verification – closed (2019–2023)</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">This has been the main challenge track since the first VoxSRC challenge in 2019.
It is closed in that for training their systems, participants were only allowed to use the publicly available VoxCeleb2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> dev set, which spans 1,092,009 utterances from 5,994 different speakers.
The motivation here was to enable comparisons between training algorithms and model architecture approaches while keeping the data fixed.</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p">There were two reasons for choosing only VoxCeleb2 dev set and not the rest of VoxCeleb.
First, we used the utterances from VoxCeleb1 as the validation set, and we did not want to have any overlap between train and validation set in our setting.
Second, the size of VoxCeleb2 dev set is adequate to train large models.
The details of VoxCeleb2 are described in Section <a href="#S3" title="III Datasets ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS2.5.1.1" class="ltx_text">II-A</span>2 </span>Speaker verification – open (2019–2023)</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">In this track, participants were permitted to use any other data in addition to the VoxCeleb2 dev set for training <span id="S2.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">except</span> the test data.
Regardless of whether the training data is public or not, participants here were encouraged to achieve state-of-the-art performance, pushing the limit every year.
The test data was the same as that used for the closed track to quantify the effect of external training data on speaker models.</p>
</div>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS3.5.1.1" class="ltx_text">II-A</span>3 </span>Self-supervised speaker verification (2020–2021)</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">In response to the progress of self-supervised learning approaches in diverse domains such as vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, we introduced a self-supervised speaker verification track to investigate methods for training speaker verification models without labels.
Participants in this track were only allowed to use the VoxCeleb2 dev set <span id="S2.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_italic">without</span> labels to train the model.
The test data was identical to the previous two tracks, which allowed the
performance gap to be studied between methods training with and without labels.</p>
</div>
</section>
<section id="S2.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS4.5.1.1" class="ltx_text">II-A</span>4 </span>Semi-supervised domain adaptation (2022–2023)</h4>

<div id="S2.SS1.SSS4.p1" class="ltx_para">
<p id="S2.SS1.SSS4.p1.1" class="ltx_p">This domain adaptation track aimed to assess how models pretrained on large labelled data in a source domain can adapt to a new target domain, given (i) a large set of unlabelled data from the target domain and (ii) a small set of labelled data from the target domain.
This setting was especially relevant to low-resource real-world scenarios, where large-scale labelled data in the source domain, and a small set of labelled data in the target domain are available in addition to large-scale unlabelled data in the target domain.
Specifically, we focused on the task of speaker verification from one language which is the source domain (mainly English), to a different language in a target domain (Chinese).
Here we used VoxCeleb2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> as the source domain and CNCeleb <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> as the target domain.
Participants were allowed to use the VoxCeleb2 dev set <span id="S2.SS1.SSS4.p1.1.1" class="ltx_text ltx_font_italic">with</span> speaker labels, a large subset of CNCeleb <span id="S2.SS1.SSS4.p1.1.2" class="ltx_text ltx_font_italic">without</span> speaker labels which contains 454,946 utterances from 1,807 speakers, and a small subset of CNCeleb <span id="S2.SS1.SSS4.p1.1.3" class="ltx_text ltx_font_italic">with</span> speaker labels which consists of 1,000 utterances from 50 speakers.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Speaker diarisation (2020–2023)</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The speaker diarisation task is to identify “who speaks when” given audio containing single or multi-speaker speech segments.
Participants were required to (i) identify speaker regions, and (ii) assign a speaker label to each region in the audio file.
Participants did not need to match speech segments to specific known speakers; instead, they could simply cluster the segments according to different speakers.
Participants were allowed to use any data to train their models <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">except</span> the test data.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Datasets</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section describes the training, validation and test sets used in the challenges.
Each year, we created (i) validation sets for which labels were available to the public, and (ii) test sets constructed from hidden data to make the task challenging and interesting, for which labels were not available publicly.
For each track, the validation set featured a data distribution similar to that of the corresponding test set.
This section details the datasets used each year, encompassing both publicly available and organiser-created hidden datasets.
Additionally, we include a discussion of the annotation methods.
We show the training set for each track in Table <a href="#S3.T1" title="TABLE I ‣ III Datasets ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.
The complete statistics of the validation and test sets each year are given in the Appendix <a href="#A1" title="Appendix A Statistics of validation and test sets ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">During the VoxSRC challenges, the labels of the test sets were not released. However, following the end of the challenges, they have now been released for the community.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/workshop.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/workshop.html</a></span></span></span></p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Training sets per each track. Statistics of validation and test sets are in the Appendix <a href="#A1" title="Appendix A Statistics of validation and test sets ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</figcaption>
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:146.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-4.4pt,1.5pt) scale(0.980053551359978,0.980053551359978) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Track</span></td>
<td id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></td>
<td id="S3.T1.1.1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt"><span id="S3.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Training set</span></td>
</tr>
<tr id="S3.T1.1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Speaker verification – closed</td>
<td id="S3.T1.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">2019 – 2023</td>
<td id="S3.T1.1.1.2.2.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">VoxCeleb2 dev set</td>
</tr>
<tr id="S3.T1.1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_t">Speaker verification – open</td>
<td id="S3.T1.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_t">2019 – 2023</td>
<td id="S3.T1.1.1.3.3.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">Any data except the test audio files</td>
</tr>
<tr id="S3.T1.1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_t">Self-supervised speaker verification</td>
<td id="S3.T1.1.1.4.4.2" class="ltx_td ltx_align_left ltx_border_t">2020 – 2021</td>
<td id="S3.T1.1.1.4.4.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">VoxCeleb2 dev set without labels</td>
</tr>
<tr id="S3.T1.1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_t">Semi-supervised domain adaptation</td>
<td id="S3.T1.1.1.5.5.2" class="ltx_td ltx_align_left ltx_border_t">2022 – 2023</td>
<td id="S3.T1.1.1.5.5.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<table id="S3.T1.1.1.5.5.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.5.5.3.1.1" class="ltx_tr">
<td id="S3.T1.1.1.5.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- VoxCeleb2 dev set with labels</td>
</tr>
<tr id="S3.T1.1.1.5.5.3.1.2" class="ltx_tr">
<td id="S3.T1.1.1.5.5.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- A large subset of CNCeleb2 without labels</td>
</tr>
<tr id="S3.T1.1.1.5.5.3.1.3" class="ltx_tr">
<td id="S3.T1.1.1.5.5.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">- A small subset of CNCeleb1 with labels</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T1.1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Speaker diarisation</td>
<td id="S3.T1.1.1.6.6.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">2020 – 2023</td>
<td id="S3.T1.1.1.6.6.3" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t">Any data except the test audio files</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Speaker verification (Tracks 1 and 2)</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The challenge datasets were based on VoxCeleb <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, a large-scale speaker recognition dataset comprising utterances from celebrities, sampled from interviews and TV shows on YouTube.
The dataset was created by extracting multiple single-speaker utterances from each YouTube video using face tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, face verification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and active speaker detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
This dataset consists of two versions.
VoxCeleb1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> includes over 150,000 utterances from 1,251 speakers in 22,496 unique recordings.
The audio dataset covers diverse background environments and recording conditions.
VoxCeleb2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> expands the initial version to a larger scale, containing over a million utterances from 6,112 speakers in 150,480 recordings, making it five times larger than VoxCeleb1.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.5.1.1" class="ltx_text">III-A</span>1 </span>Training set</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">For the closed track (Track 1), participants were only allowed to use the VoxCeleb2 dev set for training their systems.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Note, the “VoxCeleb2 dev set” used in VoxSRC refers to the training set of the original VoxCeleb2 dataset.</span></span></span>
The dev set contains 1,092,009 utterances from 5,994 speakers.
For the open track (Track 2), participants could use any data except the challenge test set.
The training set on these two tracks remained the same for all five years.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.5.1.1" class="ltx_text">III-A</span>2 </span>Validation set</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">The validation sets for all five years were based on VoxCeleb1.
In addition, the validation set for 2020 and 2023 challenges made use of audio clips from the VoxMovies dataset, and the 2022 and 2023 challenges utilised audio segments from the VoxConverse dataset.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS3.5.1.1" class="ltx_text">III-A</span>3 </span>Test set</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">The test data was created from YouTube videos in the same way as the training and validation sets, but they came from identities that do not appear in VoxCeleb1 or VoxCeleb2.
The test data was checked manually for any errors using the same procedure described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
This was done using a simple web-based tool that shows all video segments for each identity.
To monitor the performance improvements over time, the VoxSRC 2019 test trials were always included in the test sets of all subsequent years.</p>
</div>
</section>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS4.5.1.1" class="ltx_text">III-A</span>4 </span>Themes</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.1" class="ltx_p">Each year’s challenge was designed to emphasise important research directions.
The following paragraphs provide additional information on how the data was collected for each challenge.</p>
</div>
<div id="S3.SS1.SSS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS4.p2.1" class="ltx_p"><span id="S3.SS1.SSS4.p2.1.1" class="ltx_text ltx_font_bold">Speakers at the Movies (2020).</span>
In VoxSRC 2020, we introduced a new set of speaker segments from movie material, namely the VoxMovies dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, into the validation and test sets, which served as out-of-domain data for some of the identities in VoxCeleb.
VoxMovies is a speaker recognition dataset comprising utterances from movies, collected using a similar pipeline to VoxCeleb.
It poses greater challenges than VoxCeleb because actors tend to disguise their voices and put more emotions in their speech in movies compared to in interviews.
The utterances vary in emotion, accent, and background noise and therefore come from an entirely different domain to the VoxCeleb utterances, which mostly contain celebrity interviews.</p>
</div>
<div id="S3.SS1.SSS4.p3" class="ltx_para">
<p id="S3.SS1.SSS4.p3.1" class="ltx_p">Utterances trials were constructed from the hidden sets of VoxMovies and VoxCeleb to create more challenging test pairs.
Pairs were constructed in two different ways: (i) both utterances originated from either VoxCeleb or VoxMovies; or (ii) one utterance from VoxCeleb and the other from VoxMovies.</p>
</div>
<div id="S3.SS1.SSS4.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS4.p4.1" class="ltx_p"><span id="S3.SS1.SSS4.p4.1.1" class="ltx_text ltx_font_bold">Cross-lingual pairs (2021).</span>
In VoxSRC 2021, multi-lingual verification pairs were introduced into the speaker verification validation and test data.
The goals of this were twofold: first to promote the fairness and accessibility of speaker verification models, so as to allow people from diverse language groups to use these deep learning models; and second, to provide a more challenging test set for the speaker verification tracks.</p>
</div>
<div id="S3.SS1.SSS4.p5" class="ltx_para">
<p id="S3.SS1.SSS4.p5.1" class="ltx_p">Due to the design of the dataset collection pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, the VoxCeleb datasets consist of mainly English-speaking speech segments.
In the validation and test sets in 2021, a multi-lingual focus was added by sampling more positive and negative pairs that contain non-English speech segments.
This required the use of language labels, which do not exist for the VoxCeleb datasets.
We obtained the language labels using a three-step pipeline consisting of a combination of automatic and manual annotation.
First, we obtained automatic language predictions for each VoxCeleb video using a model trained on VoxLingua107 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> to make language predictions across 107 languages.
We assumed each speaker in a video uses only one language and randomly selected one utterance per video together with its predicted language label.
Second, we manually annotated the correctness of the language predictions for the 12 most frequently occurring languages using
a customised LISA annotation tool <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
The 12 languages include English, French, Dutch, Italian, German, Spanish, Hindi, Portuguese, Russian, Chinese, Japanese, and Korean.
Third, we used these manual annotations to obtain language-specific classification thresholds for the automatic predictions.
These thresholds were then applied to classify each speech segment in VoxCeleb1 as either one or none of these 12 languages.</p>
</div>
<div id="S3.SS1.SSS4.p6" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS4.p6.1" class="ltx_p"><span id="S3.SS1.SSS4.p6.1.1" class="ltx_text ltx_font_bold">Hard positive pairs with large age gaps (2022). </span>
We curated hard positive validation and test pairs where the age of the speaker differs considerably between the two utterances.
These hard positives were found by selecting utterance pairs from the same speaker that have a large age gap (i.e. two audio files for the same identity where the age is very different) via a two-step process in the hidden video data.
In this data, for each video segment, we had the face location, the identity and the audio recording.
First, the age of the speaker was estimated by predicting the age for a random set of frames, using an open-source face age prediction network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and averaging the result.
Second, we sampled positive pairs from utterances for the same speaker with large age gaps.
We performed this process within VoxCeleb1 to create the validation set, and within the hidden data to create the test set.</p>
</div>
<div id="S3.SS1.SSS4.p7" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS4.p7.1" class="ltx_p"><span id="S3.SS1.SSS4.p7.1.1" class="ltx_text ltx_font_bold">Hard negative pairs that share the same background noise (2022).</span>
We constructed hard negative validation and test pairs by sampling utterances from different speakers within the same video that therefore share very similar acoustic conditions.
Most of the negative pairs in the VoxCeleb training datasets are from different videos, therefore speaker verification systems might be able to use the environmental cues as a shortcut for speaker prediction.
Our goal here was to construct harder negative pairs by sampling utterances from different speakers that are sourced from the same video.
In this case, the environmental conditions are shared across the two utterances and only the speaker’s identity changes.
We sampled the hard negative pairs using the VoxConverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> speaker diarisation dataset, where each audio file consists of multiple short speech segments from different speakers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
To generate these trials, we first cropped short speech segments.
We then removed segments that were either too short (<math id="S3.SS1.SSS4.p7.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS1.SSS4.p7.1.m1.1a"><mo id="S3.SS1.SSS4.p7.1.m1.1.1" xref="S3.SS1.SSS4.p7.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS4.p7.1.m1.1b"><lt id="S3.SS1.SSS4.p7.1.m1.1.1.cmml" xref="S3.SS1.SSS4.p7.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS4.p7.1.m1.1c">&lt;</annotation></semantics></math>1.5s) or had overlapping speech.
Finally, we selected trials using two segments from different speakers within the same audio file.</p>
</div>
<div id="S3.SS1.SSS4.p8" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS4.p8.1" class="ltx_p"><span id="S3.SS1.SSS4.p8.1.1" class="ltx_text ltx_font_bold">Combination of all themes (2023).</span>
In the final year, we composed a test set including all scenarios aforementioned, making it the most comprehensive test set.
This includes all test pairs from the years 2019, 2021, 2022, and 2023, as well as approximately 200,000 randomly selected pairs from 2020.
To avoid redundancy, we ensured that each pair in the VoxSRC 2019 test set was included only once.
Note, only 200,000 of the approximately 1.7 million pairs in the VoxSRC2020 test set are included in order to balance the number of test pairs for each year, and also to reduce the computational expense of evaluation.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Self-supervised speaker verification (Track 3 in 2020 and 2021)</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">For the self-supervised track introduced in 2020, participants were allowed to use VoxCeleb2 dev set <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">without</span> labels.
Training, validation and test sets are otherwise identical to Track 1 and 2 in the respective years.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Semi-supervised domain adaptation (Track 3 in 2022 and 2023)</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">As described in Section <a href="#S2.SS1.SSS4" title="II-A4 Semi-supervised domain adaptation (2022–2023) ‣ II-A Speaker verification (2019–2023) ‣ II Tasks and Tracks ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span>4</span></a>, the domain adaptation that we focused on was from one language in the source domain (mainly English) to a different language in the target domain (Chinese).
For this challenge, the VoxCeleb dataset represents the source domain, and
CNCeleb <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> represents the target domain.
CNCeleb is a large-scale speaker verification dataset, mostly from Chinese speakers, which contains more than 600,000 utterances from 3,000 identities.
Among the 11 genres CNCeleb spans, we removed “singing”, “play”, “movie”, “advertisement” and “drama” genres to focus on language domain adaptation tasks.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS1.5.1.1" class="ltx_text">III-C</span>1 </span>Training set</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Participants were allowed to use three types of datasets in this track:</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">VoxCeleb2 dev set <span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">with</span> speaker labels (source domain). This can be used for pretraining.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">A large subset of CNCeleb2 <span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">without</span> speaker labels (target domain).
It consists of 454,946 utterances from 1,807 identities. This can be used for domain adaptation.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">A small subset of CNCeleb1 <span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">with</span> speaker labels (target domain) comprising 50 speakers with 20 utterances per speaker.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS2.5.1.1" class="ltx_text">III-C</span>2 </span>Validation set</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">We provided a list of trial speech pairs from identities in the target domain.
These utterances were sampled from CNCeleb1.</p>
</div>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS3.5.1.1" class="ltx_text">III-C</span>3 </span>Test set</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p">The test set consists of 56 disjoint identities not present in either CNCeleb1 or CNCeleb2.
We used the hidden set of CNCeleb, provided by the authors, and constructed 30,000 and 80,000 pairs in years 2022 and 2023, respectively.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Speaker diarisation (Track 4 in 2020 to 2023)</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The VoxConverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> dataset is an audio-visual speaker diarisation dataset which includes 448 videos from YouTube.
These videos are mostly from debates, talk shows and news segments.
It has multi-speaker, variable-length audio segments, with overlaps and challenging acoustic conditions.
Inspired by other audio-visual dataset creation pipelines such as VoxCeleb <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and VGGSound <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, the pipeline leverages an automatic audio-visual speaker diarisation method using active speaker detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, audio-visual source separation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and speaker verification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, followed by manual verification.
Only audio files were provided for this challenge.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS4.SSS1.5.1.1" class="ltx_text">III-D</span>1 </span>Training set</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">Participants were allowed to use any public or internal datasets <span id="S3.SS4.SSS1.p1.1.1" class="ltx_text ltx_font_italic">except</span> for the test data to train their systems.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS4.SSS2.5.1.1" class="ltx_text">III-D</span>2 </span>Validation set</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">We provided the dev set of VoxConverse as the validation set for the 2020 challenge.
The participants were allowed to use the entire VoxConverse as the validation set for the remaining years.</p>
</div>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS4.SSS3.5.1.1" class="ltx_text">III-D</span>3 </span>Test set</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p id="S3.SS4.SSS3.p1.1" class="ltx_p">In the year 2020, we used the VoxConverse test set as our challenge test set, which was included in the validation set for subsequent years.
The test sets for the following years were curated using the same pipeline as VoxConverse, but we utilised a hidden set of videos that were not public.
These videos, sourced from YouTube, span diverse categories including news, documentaries, lectures, and commercials.
We curated test sets with 264, 360, and 413 audio files in the years 2021, 2022, and 2023, respectively.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Challenge mechanics</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section outlines the challenge evaluation metrics for all tracks, followed by an overview of how we hosted the challenge including submission rules and formats.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Evaluation metrics</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Each year, we released a validation toolkit to allow participants to assess their systems.
The code in this toolkit was identical to the one which organisers use for evaluation, preventing possible performance differences stemming from implementation mismatches.
All verification tracks shared the same metrics, minimum Detection Cost Function (minDCF) and Equal Error Rate (EER).
The diarisation track used Diarisation Error Rate (DER) and Jaccard Error Rate (JER) as evaluation metrics. The evaluation protocols and metrics are adopted, with minor modifications, directly from NIST SRE challenges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> for the verification tracks, and from DIHARD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> for the diarisation track.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.5.1.1" class="ltx_text">IV-A</span>1 </span>Speaker verification</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p"><span id="S4.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_bold">minDCF</span> is the calibration insensitive metric to measure speaker verification performance.
The DCF is computed as:</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.1" class="ltx_Math" alttext="C_{DET}=C_{miss}\times P_{miss}\times P_{tar}+C_{fa}\times P_{fa}\times(1-P_{tar})" display="block"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><msub id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml">C</mi><mrow id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.2" xref="S4.E1.m1.1.1.3.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.3.1" xref="S4.E1.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.3.3.3" xref="S4.E1.m1.1.1.3.3.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.3.1a" xref="S4.E1.m1.1.1.3.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.3.3.4" xref="S4.E1.m1.1.1.3.3.4.cmml">T</mi></mrow></msub><mo id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.3" xref="S4.E1.m1.1.1.1.3.cmml"><msub id="S4.E1.m1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.3.2.cmml"><mi id="S4.E1.m1.1.1.1.3.2.2" xref="S4.E1.m1.1.1.1.3.2.2.cmml">C</mi><mrow id="S4.E1.m1.1.1.1.3.2.3" xref="S4.E1.m1.1.1.1.3.2.3.cmml"><mi id="S4.E1.m1.1.1.1.3.2.3.2" xref="S4.E1.m1.1.1.1.3.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.3.2.3.1" xref="S4.E1.m1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.3.2.3.3" xref="S4.E1.m1.1.1.1.3.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.3.2.3.1a" xref="S4.E1.m1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.3.2.3.4" xref="S4.E1.m1.1.1.1.3.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.3.2.3.1b" xref="S4.E1.m1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.3.2.3.5" xref="S4.E1.m1.1.1.1.3.2.3.5.cmml">s</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m1.1.1.1.3.1" xref="S4.E1.m1.1.1.1.3.1.cmml">×</mo><msub id="S4.E1.m1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.3.3.cmml"><mi id="S4.E1.m1.1.1.1.3.3.2" xref="S4.E1.m1.1.1.1.3.3.2.cmml">P</mi><mrow id="S4.E1.m1.1.1.1.3.3.3" xref="S4.E1.m1.1.1.1.3.3.3.cmml"><mi id="S4.E1.m1.1.1.1.3.3.3.2" xref="S4.E1.m1.1.1.1.3.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.3.3.3.1" xref="S4.E1.m1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.3.3.3.3" xref="S4.E1.m1.1.1.1.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.3.3.3.1a" xref="S4.E1.m1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.3.3.3.4" xref="S4.E1.m1.1.1.1.3.3.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.3.3.3.1b" xref="S4.E1.m1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.3.3.3.5" xref="S4.E1.m1.1.1.1.3.3.3.5.cmml">s</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m1.1.1.1.3.1a" xref="S4.E1.m1.1.1.1.3.1.cmml">×</mo><msub id="S4.E1.m1.1.1.1.3.4" xref="S4.E1.m1.1.1.1.3.4.cmml"><mi id="S4.E1.m1.1.1.1.3.4.2" xref="S4.E1.m1.1.1.1.3.4.2.cmml">P</mi><mrow id="S4.E1.m1.1.1.1.3.4.3" xref="S4.E1.m1.1.1.1.3.4.3.cmml"><mi id="S4.E1.m1.1.1.1.3.4.3.2" xref="S4.E1.m1.1.1.1.3.4.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.3.4.3.1" xref="S4.E1.m1.1.1.1.3.4.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.3.4.3.3" xref="S4.E1.m1.1.1.1.3.4.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.3.4.3.1a" xref="S4.E1.m1.1.1.1.3.4.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.3.4.3.4" xref="S4.E1.m1.1.1.1.3.4.3.4.cmml">r</mi></mrow></msub></mrow><mo id="S4.E1.m1.1.1.1.2" xref="S4.E1.m1.1.1.1.2.cmml">+</mo><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><msub id="S4.E1.m1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.3.2.cmml">C</mi><mrow id="S4.E1.m1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.3.3.cmml"><mi id="S4.E1.m1.1.1.1.1.3.3.2" xref="S4.E1.m1.1.1.1.1.3.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.3.3.1" xref="S4.E1.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.1.3.3.3" xref="S4.E1.m1.1.1.1.1.3.3.3.cmml">a</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.2.cmml">×</mo><msub id="S4.E1.m1.1.1.1.1.4" xref="S4.E1.m1.1.1.1.1.4.cmml"><mi id="S4.E1.m1.1.1.1.1.4.2" xref="S4.E1.m1.1.1.1.1.4.2.cmml">P</mi><mrow id="S4.E1.m1.1.1.1.1.4.3" xref="S4.E1.m1.1.1.1.1.4.3.cmml"><mi id="S4.E1.m1.1.1.1.1.4.3.2" xref="S4.E1.m1.1.1.1.1.4.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.4.3.1" xref="S4.E1.m1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.1.4.3.3" xref="S4.E1.m1.1.1.1.1.4.3.3.cmml">a</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E1.m1.1.1.1.1.2a" xref="S4.E1.m1.1.1.1.1.2.cmml">×</mo><mrow id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml"><mn id="S4.E1.m1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S4.E1.m1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S4.E1.m1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.1.1.1.3.2.cmml">P</mi><mrow id="S4.E1.m1.1.1.1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.3.3.2" xref="S4.E1.m1.1.1.1.1.1.1.1.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.1.1.1.3.3.1" xref="S4.E1.m1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.1.1.1.1.3.3.3" xref="S4.E1.m1.1.1.1.1.1.1.1.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.1.1.1.3.3.1a" xref="S4.E1.m1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.1.1.1.1.3.3.4" xref="S4.E1.m1.1.1.1.1.1.1.1.3.3.4.cmml">r</mi></mrow></msub></mrow><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><eq id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2"></eq><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2">𝐶</ci><apply id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3"><times id="S4.E1.m1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.1"></times><ci id="S4.E1.m1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.2">𝐷</ci><ci id="S4.E1.m1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3">𝐸</ci><ci id="S4.E1.m1.1.1.3.3.4.cmml" xref="S4.E1.m1.1.1.3.3.4">𝑇</ci></apply></apply><apply id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"><plus id="S4.E1.m1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.2"></plus><apply id="S4.E1.m1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.3"><times id="S4.E1.m1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.3.1"></times><apply id="S4.E1.m1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.3.2.1.cmml" xref="S4.E1.m1.1.1.1.3.2">subscript</csymbol><ci id="S4.E1.m1.1.1.1.3.2.2.cmml" xref="S4.E1.m1.1.1.1.3.2.2">𝐶</ci><apply id="S4.E1.m1.1.1.1.3.2.3.cmml" xref="S4.E1.m1.1.1.1.3.2.3"><times id="S4.E1.m1.1.1.1.3.2.3.1.cmml" xref="S4.E1.m1.1.1.1.3.2.3.1"></times><ci id="S4.E1.m1.1.1.1.3.2.3.2.cmml" xref="S4.E1.m1.1.1.1.3.2.3.2">𝑚</ci><ci id="S4.E1.m1.1.1.1.3.2.3.3.cmml" xref="S4.E1.m1.1.1.1.3.2.3.3">𝑖</ci><ci id="S4.E1.m1.1.1.1.3.2.3.4.cmml" xref="S4.E1.m1.1.1.1.3.2.3.4">𝑠</ci><ci id="S4.E1.m1.1.1.1.3.2.3.5.cmml" xref="S4.E1.m1.1.1.1.3.2.3.5">𝑠</ci></apply></apply><apply id="S4.E1.m1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.1.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.1.3.3.2">𝑃</ci><apply id="S4.E1.m1.1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.1.3.3.3"><times id="S4.E1.m1.1.1.1.3.3.3.1.cmml" xref="S4.E1.m1.1.1.1.3.3.3.1"></times><ci id="S4.E1.m1.1.1.1.3.3.3.2.cmml" xref="S4.E1.m1.1.1.1.3.3.3.2">𝑚</ci><ci id="S4.E1.m1.1.1.1.3.3.3.3.cmml" xref="S4.E1.m1.1.1.1.3.3.3.3">𝑖</ci><ci id="S4.E1.m1.1.1.1.3.3.3.4.cmml" xref="S4.E1.m1.1.1.1.3.3.3.4">𝑠</ci><ci id="S4.E1.m1.1.1.1.3.3.3.5.cmml" xref="S4.E1.m1.1.1.1.3.3.3.5">𝑠</ci></apply></apply><apply id="S4.E1.m1.1.1.1.3.4.cmml" xref="S4.E1.m1.1.1.1.3.4"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.3.4.1.cmml" xref="S4.E1.m1.1.1.1.3.4">subscript</csymbol><ci id="S4.E1.m1.1.1.1.3.4.2.cmml" xref="S4.E1.m1.1.1.1.3.4.2">𝑃</ci><apply id="S4.E1.m1.1.1.1.3.4.3.cmml" xref="S4.E1.m1.1.1.1.3.4.3"><times id="S4.E1.m1.1.1.1.3.4.3.1.cmml" xref="S4.E1.m1.1.1.1.3.4.3.1"></times><ci id="S4.E1.m1.1.1.1.3.4.3.2.cmml" xref="S4.E1.m1.1.1.1.3.4.3.2">𝑡</ci><ci id="S4.E1.m1.1.1.1.3.4.3.3.cmml" xref="S4.E1.m1.1.1.1.3.4.3.3">𝑎</ci><ci id="S4.E1.m1.1.1.1.3.4.3.4.cmml" xref="S4.E1.m1.1.1.1.3.4.3.4">𝑟</ci></apply></apply></apply><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1"><times id="S4.E1.m1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2"></times><apply id="S4.E1.m1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.2">𝐶</ci><apply id="S4.E1.m1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3"><times id="S4.E1.m1.1.1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3.3.1"></times><ci id="S4.E1.m1.1.1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.3.2">𝑓</ci><ci id="S4.E1.m1.1.1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3.3">𝑎</ci></apply></apply><apply id="S4.E1.m1.1.1.1.1.4.cmml" xref="S4.E1.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.4.1.cmml" xref="S4.E1.m1.1.1.1.1.4">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.4.2.cmml" xref="S4.E1.m1.1.1.1.1.4.2">𝑃</ci><apply id="S4.E1.m1.1.1.1.1.4.3.cmml" xref="S4.E1.m1.1.1.1.1.4.3"><times id="S4.E1.m1.1.1.1.1.4.3.1.cmml" xref="S4.E1.m1.1.1.1.1.4.3.1"></times><ci id="S4.E1.m1.1.1.1.1.4.3.2.cmml" xref="S4.E1.m1.1.1.1.1.4.3.2">𝑓</ci><ci id="S4.E1.m1.1.1.1.1.4.3.3.cmml" xref="S4.E1.m1.1.1.1.1.4.3.3">𝑎</ci></apply></apply><apply id="S4.E1.m1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1"><minus id="S4.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S4.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.2">1</cn><apply id="S4.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3.2">𝑃</ci><apply id="S4.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3.3"><times id="S4.E1.m1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3.3.1"></times><ci id="S4.E1.m1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3.3.2">𝑡</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3.3.3">𝑎</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.3.3.4.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3.3.4">𝑟</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">C_{DET}=C_{miss}\times P_{miss}\times P_{tar}+C_{fa}\times P_{fa}\times(1-P_{tar})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.10" class="ltx_p"><math id="S4.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="P_{miss}" display="inline"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><msub id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.p2.1.m1.1.1.2" xref="S4.SS1.SSS1.p2.1.m1.1.1.2.cmml">P</mi><mrow id="S4.SS1.SSS1.p2.1.m1.1.1.3" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.cmml"><mi id="S4.SS1.SSS1.p2.1.m1.1.1.3.2" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.1.m1.1.1.3.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.1.m1.1.1.3.3" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.1.m1.1.1.3.1a" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.1.m1.1.1.3.4" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.1.m1.1.1.3.1b" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.1.m1.1.1.3.5" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.5.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.1b"><apply id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.2">𝑃</ci><apply id="S4.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.3"><times id="S4.SS1.SSS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.1"></times><ci id="S4.SS1.SSS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.2">𝑚</ci><ci id="S4.SS1.SSS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.3">𝑖</ci><ci id="S4.SS1.SSS1.p2.1.m1.1.1.3.4.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.4">𝑠</ci><ci id="S4.SS1.SSS1.p2.1.m1.1.1.3.5.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.3.5">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.1c">P_{miss}</annotation></semantics></math> and <math id="S4.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="P_{fa}" display="inline"><semantics id="S4.SS1.SSS1.p2.2.m2.1a"><msub id="S4.SS1.SSS1.p2.2.m2.1.1" xref="S4.SS1.SSS1.p2.2.m2.1.1.cmml"><mi id="S4.SS1.SSS1.p2.2.m2.1.1.2" xref="S4.SS1.SSS1.p2.2.m2.1.1.2.cmml">P</mi><mrow id="S4.SS1.SSS1.p2.2.m2.1.1.3" xref="S4.SS1.SSS1.p2.2.m2.1.1.3.cmml"><mi id="S4.SS1.SSS1.p2.2.m2.1.1.3.2" xref="S4.SS1.SSS1.p2.2.m2.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.2.m2.1.1.3.1" xref="S4.SS1.SSS1.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.2.m2.1.1.3.3" xref="S4.SS1.SSS1.p2.2.m2.1.1.3.3.cmml">a</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.2.m2.1b"><apply id="S4.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1.2">𝑃</ci><apply id="S4.SS1.SSS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1.3"><times id="S4.SS1.SSS1.p2.2.m2.1.1.3.1.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1.3.1"></times><ci id="S4.SS1.SSS1.p2.2.m2.1.1.3.2.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1.3.2">𝑓</ci><ci id="S4.SS1.SSS1.p2.2.m2.1.1.3.3.cmml" xref="S4.SS1.SSS1.p2.2.m2.1.1.3.3">𝑎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.2.m2.1c">P_{fa}</annotation></semantics></math> are normalised error rates by counting the errors from positive and negative pair trials respectively, and <math id="S4.SS1.SSS1.p2.3.m3.1" class="ltx_Math" alttext="P_{tar}" display="inline"><semantics id="S4.SS1.SSS1.p2.3.m3.1a"><msub id="S4.SS1.SSS1.p2.3.m3.1.1" xref="S4.SS1.SSS1.p2.3.m3.1.1.cmml"><mi id="S4.SS1.SSS1.p2.3.m3.1.1.2" xref="S4.SS1.SSS1.p2.3.m3.1.1.2.cmml">P</mi><mrow id="S4.SS1.SSS1.p2.3.m3.1.1.3" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.cmml"><mi id="S4.SS1.SSS1.p2.3.m3.1.1.3.2" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.3.m3.1.1.3.1" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.3.m3.1.1.3.3" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.3.m3.1.1.3.1a" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.3.m3.1.1.3.4" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.4.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.3.m3.1b"><apply id="S4.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.2">𝑃</ci><apply id="S4.SS1.SSS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.3"><times id="S4.SS1.SSS1.p2.3.m3.1.1.3.1.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.1"></times><ci id="S4.SS1.SSS1.p2.3.m3.1.1.3.2.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.2">𝑡</ci><ci id="S4.SS1.SSS1.p2.3.m3.1.1.3.3.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.3">𝑎</ci><ci id="S4.SS1.SSS1.p2.3.m3.1.1.3.4.cmml" xref="S4.SS1.SSS1.p2.3.m3.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.3.m3.1c">P_{tar}</annotation></semantics></math> is a prior probability that a target speaker event occurs in the real world, which is provided by the evaluator.
minDCF is the minimum value of <math id="S4.SS1.SSS1.p2.4.m4.1" class="ltx_Math" alttext="C_{DET}" display="inline"><semantics id="S4.SS1.SSS1.p2.4.m4.1a"><msub id="S4.SS1.SSS1.p2.4.m4.1.1" xref="S4.SS1.SSS1.p2.4.m4.1.1.cmml"><mi id="S4.SS1.SSS1.p2.4.m4.1.1.2" xref="S4.SS1.SSS1.p2.4.m4.1.1.2.cmml">C</mi><mrow id="S4.SS1.SSS1.p2.4.m4.1.1.3" xref="S4.SS1.SSS1.p2.4.m4.1.1.3.cmml"><mi id="S4.SS1.SSS1.p2.4.m4.1.1.3.2" xref="S4.SS1.SSS1.p2.4.m4.1.1.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.4.m4.1.1.3.1" xref="S4.SS1.SSS1.p2.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.4.m4.1.1.3.3" xref="S4.SS1.SSS1.p2.4.m4.1.1.3.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.4.m4.1.1.3.1a" xref="S4.SS1.SSS1.p2.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.4.m4.1.1.3.4" xref="S4.SS1.SSS1.p2.4.m4.1.1.3.4.cmml">T</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.4.m4.1b"><apply id="S4.SS1.SSS1.p2.4.m4.1.1.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1.2">𝐶</ci><apply id="S4.SS1.SSS1.p2.4.m4.1.1.3.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1.3"><times id="S4.SS1.SSS1.p2.4.m4.1.1.3.1.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1.3.1"></times><ci id="S4.SS1.SSS1.p2.4.m4.1.1.3.2.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1.3.2">𝐷</ci><ci id="S4.SS1.SSS1.p2.4.m4.1.1.3.3.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1.3.3">𝐸</ci><ci id="S4.SS1.SSS1.p2.4.m4.1.1.3.4.cmml" xref="S4.SS1.SSS1.p2.4.m4.1.1.3.4">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.4.m4.1c">C_{DET}</annotation></semantics></math> by varying the threshold.
We set <math id="S4.SS1.SSS1.p2.5.m5.1" class="ltx_Math" alttext="C_{miss}=C_{fa}=1" display="inline"><semantics id="S4.SS1.SSS1.p2.5.m5.1a"><mrow id="S4.SS1.SSS1.p2.5.m5.1.1" xref="S4.SS1.SSS1.p2.5.m5.1.1.cmml"><msub id="S4.SS1.SSS1.p2.5.m5.1.1.2" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.cmml"><mi id="S4.SS1.SSS1.p2.5.m5.1.1.2.2" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.2.cmml">C</mi><mrow id="S4.SS1.SSS1.p2.5.m5.1.1.2.3" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3.cmml"><mi id="S4.SS1.SSS1.p2.5.m5.1.1.2.3.2" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.5.m5.1.1.2.3.1" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.5.m5.1.1.2.3.3" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.5.m5.1.1.2.3.1a" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.5.m5.1.1.2.3.4" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.5.m5.1.1.2.3.1b" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.5.m5.1.1.2.3.5" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3.5.cmml">s</mi></mrow></msub><mo id="S4.SS1.SSS1.p2.5.m5.1.1.3" xref="S4.SS1.SSS1.p2.5.m5.1.1.3.cmml">=</mo><msub id="S4.SS1.SSS1.p2.5.m5.1.1.4" xref="S4.SS1.SSS1.p2.5.m5.1.1.4.cmml"><mi id="S4.SS1.SSS1.p2.5.m5.1.1.4.2" xref="S4.SS1.SSS1.p2.5.m5.1.1.4.2.cmml">C</mi><mrow id="S4.SS1.SSS1.p2.5.m5.1.1.4.3" xref="S4.SS1.SSS1.p2.5.m5.1.1.4.3.cmml"><mi id="S4.SS1.SSS1.p2.5.m5.1.1.4.3.2" xref="S4.SS1.SSS1.p2.5.m5.1.1.4.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.5.m5.1.1.4.3.1" xref="S4.SS1.SSS1.p2.5.m5.1.1.4.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.5.m5.1.1.4.3.3" xref="S4.SS1.SSS1.p2.5.m5.1.1.4.3.3.cmml">a</mi></mrow></msub><mo id="S4.SS1.SSS1.p2.5.m5.1.1.5" xref="S4.SS1.SSS1.p2.5.m5.1.1.5.cmml">=</mo><mn id="S4.SS1.SSS1.p2.5.m5.1.1.6" xref="S4.SS1.SSS1.p2.5.m5.1.1.6.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.5.m5.1b"><apply id="S4.SS1.SSS1.p2.5.m5.1.1.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1"><and id="S4.SS1.SSS1.p2.5.m5.1.1a.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1"></and><apply id="S4.SS1.SSS1.p2.5.m5.1.1b.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1"><eq id="S4.SS1.SSS1.p2.5.m5.1.1.3.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.3"></eq><apply id="S4.SS1.SSS1.p2.5.m5.1.1.2.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.5.m5.1.1.2.1.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS1.p2.5.m5.1.1.2.2.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.2">𝐶</ci><apply id="S4.SS1.SSS1.p2.5.m5.1.1.2.3.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3"><times id="S4.SS1.SSS1.p2.5.m5.1.1.2.3.1.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3.1"></times><ci id="S4.SS1.SSS1.p2.5.m5.1.1.2.3.2.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3.2">𝑚</ci><ci id="S4.SS1.SSS1.p2.5.m5.1.1.2.3.3.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3.3">𝑖</ci><ci id="S4.SS1.SSS1.p2.5.m5.1.1.2.3.4.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3.4">𝑠</ci><ci id="S4.SS1.SSS1.p2.5.m5.1.1.2.3.5.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.2.3.5">𝑠</ci></apply></apply><apply id="S4.SS1.SSS1.p2.5.m5.1.1.4.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.4"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.5.m5.1.1.4.1.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.4">subscript</csymbol><ci id="S4.SS1.SSS1.p2.5.m5.1.1.4.2.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.4.2">𝐶</ci><apply id="S4.SS1.SSS1.p2.5.m5.1.1.4.3.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.4.3"><times id="S4.SS1.SSS1.p2.5.m5.1.1.4.3.1.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.4.3.1"></times><ci id="S4.SS1.SSS1.p2.5.m5.1.1.4.3.2.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.4.3.2">𝑓</ci><ci id="S4.SS1.SSS1.p2.5.m5.1.1.4.3.3.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.4.3.3">𝑎</ci></apply></apply></apply><apply id="S4.SS1.SSS1.p2.5.m5.1.1c.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1"><eq id="S4.SS1.SSS1.p2.5.m5.1.1.5.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.5"></eq><share href="#S4.SS1.SSS1.p2.5.m5.1.1.4.cmml" id="S4.SS1.SSS1.p2.5.m5.1.1d.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1"></share><cn type="integer" id="S4.SS1.SSS1.p2.5.m5.1.1.6.cmml" xref="S4.SS1.SSS1.p2.5.m5.1.1.6">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.5.m5.1c">C_{miss}=C_{fa}=1</annotation></semantics></math> and <math id="S4.SS1.SSS1.p2.6.m6.1" class="ltx_Math" alttext="P_{tar}=0.05" display="inline"><semantics id="S4.SS1.SSS1.p2.6.m6.1a"><mrow id="S4.SS1.SSS1.p2.6.m6.1.1" xref="S4.SS1.SSS1.p2.6.m6.1.1.cmml"><msub id="S4.SS1.SSS1.p2.6.m6.1.1.2" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.cmml"><mi id="S4.SS1.SSS1.p2.6.m6.1.1.2.2" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.2.cmml">P</mi><mrow id="S4.SS1.SSS1.p2.6.m6.1.1.2.3" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.3.cmml"><mi id="S4.SS1.SSS1.p2.6.m6.1.1.2.3.2" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.6.m6.1.1.2.3.1" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.6.m6.1.1.2.3.3" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.6.m6.1.1.2.3.1a" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.6.m6.1.1.2.3.4" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.3.4.cmml">r</mi></mrow></msub><mo id="S4.SS1.SSS1.p2.6.m6.1.1.1" xref="S4.SS1.SSS1.p2.6.m6.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS1.p2.6.m6.1.1.3" xref="S4.SS1.SSS1.p2.6.m6.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.6.m6.1b"><apply id="S4.SS1.SSS1.p2.6.m6.1.1.cmml" xref="S4.SS1.SSS1.p2.6.m6.1.1"><eq id="S4.SS1.SSS1.p2.6.m6.1.1.1.cmml" xref="S4.SS1.SSS1.p2.6.m6.1.1.1"></eq><apply id="S4.SS1.SSS1.p2.6.m6.1.1.2.cmml" xref="S4.SS1.SSS1.p2.6.m6.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.6.m6.1.1.2.1.cmml" xref="S4.SS1.SSS1.p2.6.m6.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS1.p2.6.m6.1.1.2.2.cmml" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.2">𝑃</ci><apply id="S4.SS1.SSS1.p2.6.m6.1.1.2.3.cmml" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.3"><times id="S4.SS1.SSS1.p2.6.m6.1.1.2.3.1.cmml" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.3.1"></times><ci id="S4.SS1.SSS1.p2.6.m6.1.1.2.3.2.cmml" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.3.2">𝑡</ci><ci id="S4.SS1.SSS1.p2.6.m6.1.1.2.3.3.cmml" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.3.3">𝑎</ci><ci id="S4.SS1.SSS1.p2.6.m6.1.1.2.3.4.cmml" xref="S4.SS1.SSS1.p2.6.m6.1.1.2.3.4">𝑟</ci></apply></apply><cn type="float" id="S4.SS1.SSS1.p2.6.m6.1.1.3.cmml" xref="S4.SS1.SSS1.p2.6.m6.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.6.m6.1c">P_{tar}=0.05</annotation></semantics></math> in our cost function.
The value of <math id="S4.SS1.SSS1.p2.7.m7.1" class="ltx_Math" alttext="P_{tar}=0.05" display="inline"><semantics id="S4.SS1.SSS1.p2.7.m7.1a"><mrow id="S4.SS1.SSS1.p2.7.m7.1.1" xref="S4.SS1.SSS1.p2.7.m7.1.1.cmml"><msub id="S4.SS1.SSS1.p2.7.m7.1.1.2" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.cmml"><mi id="S4.SS1.SSS1.p2.7.m7.1.1.2.2" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.2.cmml">P</mi><mrow id="S4.SS1.SSS1.p2.7.m7.1.1.2.3" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.3.cmml"><mi id="S4.SS1.SSS1.p2.7.m7.1.1.2.3.2" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.7.m7.1.1.2.3.1" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.7.m7.1.1.2.3.3" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.7.m7.1.1.2.3.1a" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.7.m7.1.1.2.3.4" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.3.4.cmml">r</mi></mrow></msub><mo id="S4.SS1.SSS1.p2.7.m7.1.1.1" xref="S4.SS1.SSS1.p2.7.m7.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS1.p2.7.m7.1.1.3" xref="S4.SS1.SSS1.p2.7.m7.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.7.m7.1b"><apply id="S4.SS1.SSS1.p2.7.m7.1.1.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1"><eq id="S4.SS1.SSS1.p2.7.m7.1.1.1.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1.1"></eq><apply id="S4.SS1.SSS1.p2.7.m7.1.1.2.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.7.m7.1.1.2.1.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1.2">subscript</csymbol><ci id="S4.SS1.SSS1.p2.7.m7.1.1.2.2.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.2">𝑃</ci><apply id="S4.SS1.SSS1.p2.7.m7.1.1.2.3.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.3"><times id="S4.SS1.SSS1.p2.7.m7.1.1.2.3.1.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.3.1"></times><ci id="S4.SS1.SSS1.p2.7.m7.1.1.2.3.2.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.3.2">𝑡</ci><ci id="S4.SS1.SSS1.p2.7.m7.1.1.2.3.3.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.3.3">𝑎</ci><ci id="S4.SS1.SSS1.p2.7.m7.1.1.2.3.4.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1.2.3.4">𝑟</ci></apply></apply><cn type="float" id="S4.SS1.SSS1.p2.7.m7.1.1.3.cmml" xref="S4.SS1.SSS1.p2.7.m7.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.7.m7.1c">P_{tar}=0.05</annotation></semantics></math> is similar to the proportion of positive pairs (4%) in the VoxSRC 2019 test set, and is also used in NIST SRE evaluation.
<span id="S4.SS1.SSS1.p2.10.1" class="ltx_text ltx_font_bold">EER</span> corresponds to the value where False Acceptance (FA) and False Rejection (FR) rates are equal.
EER is independent of parameters, unlike minDCF which is dependent on a set of predefined parameters such as <math id="S4.SS1.SSS1.p2.8.m8.1" class="ltx_Math" alttext="C_{miss}" display="inline"><semantics id="S4.SS1.SSS1.p2.8.m8.1a"><msub id="S4.SS1.SSS1.p2.8.m8.1.1" xref="S4.SS1.SSS1.p2.8.m8.1.1.cmml"><mi id="S4.SS1.SSS1.p2.8.m8.1.1.2" xref="S4.SS1.SSS1.p2.8.m8.1.1.2.cmml">C</mi><mrow id="S4.SS1.SSS1.p2.8.m8.1.1.3" xref="S4.SS1.SSS1.p2.8.m8.1.1.3.cmml"><mi id="S4.SS1.SSS1.p2.8.m8.1.1.3.2" xref="S4.SS1.SSS1.p2.8.m8.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.8.m8.1.1.3.1" xref="S4.SS1.SSS1.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.8.m8.1.1.3.3" xref="S4.SS1.SSS1.p2.8.m8.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.8.m8.1.1.3.1a" xref="S4.SS1.SSS1.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.8.m8.1.1.3.4" xref="S4.SS1.SSS1.p2.8.m8.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.8.m8.1.1.3.1b" xref="S4.SS1.SSS1.p2.8.m8.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.8.m8.1.1.3.5" xref="S4.SS1.SSS1.p2.8.m8.1.1.3.5.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.8.m8.1b"><apply id="S4.SS1.SSS1.p2.8.m8.1.1.cmml" xref="S4.SS1.SSS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.8.m8.1.1.1.cmml" xref="S4.SS1.SSS1.p2.8.m8.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p2.8.m8.1.1.2.cmml" xref="S4.SS1.SSS1.p2.8.m8.1.1.2">𝐶</ci><apply id="S4.SS1.SSS1.p2.8.m8.1.1.3.cmml" xref="S4.SS1.SSS1.p2.8.m8.1.1.3"><times id="S4.SS1.SSS1.p2.8.m8.1.1.3.1.cmml" xref="S4.SS1.SSS1.p2.8.m8.1.1.3.1"></times><ci id="S4.SS1.SSS1.p2.8.m8.1.1.3.2.cmml" xref="S4.SS1.SSS1.p2.8.m8.1.1.3.2">𝑚</ci><ci id="S4.SS1.SSS1.p2.8.m8.1.1.3.3.cmml" xref="S4.SS1.SSS1.p2.8.m8.1.1.3.3">𝑖</ci><ci id="S4.SS1.SSS1.p2.8.m8.1.1.3.4.cmml" xref="S4.SS1.SSS1.p2.8.m8.1.1.3.4">𝑠</ci><ci id="S4.SS1.SSS1.p2.8.m8.1.1.3.5.cmml" xref="S4.SS1.SSS1.p2.8.m8.1.1.3.5">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.8.m8.1c">C_{miss}</annotation></semantics></math>,<math id="S4.SS1.SSS1.p2.9.m9.1" class="ltx_Math" alttext="C_{fa}" display="inline"><semantics id="S4.SS1.SSS1.p2.9.m9.1a"><msub id="S4.SS1.SSS1.p2.9.m9.1.1" xref="S4.SS1.SSS1.p2.9.m9.1.1.cmml"><mi id="S4.SS1.SSS1.p2.9.m9.1.1.2" xref="S4.SS1.SSS1.p2.9.m9.1.1.2.cmml">C</mi><mrow id="S4.SS1.SSS1.p2.9.m9.1.1.3" xref="S4.SS1.SSS1.p2.9.m9.1.1.3.cmml"><mi id="S4.SS1.SSS1.p2.9.m9.1.1.3.2" xref="S4.SS1.SSS1.p2.9.m9.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.9.m9.1.1.3.1" xref="S4.SS1.SSS1.p2.9.m9.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.9.m9.1.1.3.3" xref="S4.SS1.SSS1.p2.9.m9.1.1.3.3.cmml">a</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.9.m9.1b"><apply id="S4.SS1.SSS1.p2.9.m9.1.1.cmml" xref="S4.SS1.SSS1.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.9.m9.1.1.1.cmml" xref="S4.SS1.SSS1.p2.9.m9.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p2.9.m9.1.1.2.cmml" xref="S4.SS1.SSS1.p2.9.m9.1.1.2">𝐶</ci><apply id="S4.SS1.SSS1.p2.9.m9.1.1.3.cmml" xref="S4.SS1.SSS1.p2.9.m9.1.1.3"><times id="S4.SS1.SSS1.p2.9.m9.1.1.3.1.cmml" xref="S4.SS1.SSS1.p2.9.m9.1.1.3.1"></times><ci id="S4.SS1.SSS1.p2.9.m9.1.1.3.2.cmml" xref="S4.SS1.SSS1.p2.9.m9.1.1.3.2">𝑓</ci><ci id="S4.SS1.SSS1.p2.9.m9.1.1.3.3.cmml" xref="S4.SS1.SSS1.p2.9.m9.1.1.3.3">𝑎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.9.m9.1c">C_{fa}</annotation></semantics></math> and <math id="S4.SS1.SSS1.p2.10.m10.1" class="ltx_Math" alttext="P_{tar}" display="inline"><semantics id="S4.SS1.SSS1.p2.10.m10.1a"><msub id="S4.SS1.SSS1.p2.10.m10.1.1" xref="S4.SS1.SSS1.p2.10.m10.1.1.cmml"><mi id="S4.SS1.SSS1.p2.10.m10.1.1.2" xref="S4.SS1.SSS1.p2.10.m10.1.1.2.cmml">P</mi><mrow id="S4.SS1.SSS1.p2.10.m10.1.1.3" xref="S4.SS1.SSS1.p2.10.m10.1.1.3.cmml"><mi id="S4.SS1.SSS1.p2.10.m10.1.1.3.2" xref="S4.SS1.SSS1.p2.10.m10.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.10.m10.1.1.3.1" xref="S4.SS1.SSS1.p2.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.10.m10.1.1.3.3" xref="S4.SS1.SSS1.p2.10.m10.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS1.p2.10.m10.1.1.3.1a" xref="S4.SS1.SSS1.p2.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS1.SSS1.p2.10.m10.1.1.3.4" xref="S4.SS1.SSS1.p2.10.m10.1.1.3.4.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.10.m10.1b"><apply id="S4.SS1.SSS1.p2.10.m10.1.1.cmml" xref="S4.SS1.SSS1.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.10.m10.1.1.1.cmml" xref="S4.SS1.SSS1.p2.10.m10.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p2.10.m10.1.1.2.cmml" xref="S4.SS1.SSS1.p2.10.m10.1.1.2">𝑃</ci><apply id="S4.SS1.SSS1.p2.10.m10.1.1.3.cmml" xref="S4.SS1.SSS1.p2.10.m10.1.1.3"><times id="S4.SS1.SSS1.p2.10.m10.1.1.3.1.cmml" xref="S4.SS1.SSS1.p2.10.m10.1.1.3.1"></times><ci id="S4.SS1.SSS1.p2.10.m10.1.1.3.2.cmml" xref="S4.SS1.SSS1.p2.10.m10.1.1.3.2">𝑡</ci><ci id="S4.SS1.SSS1.p2.10.m10.1.1.3.3.cmml" xref="S4.SS1.SSS1.p2.10.m10.1.1.3.3">𝑎</ci><ci id="S4.SS1.SSS1.p2.10.m10.1.1.3.4.cmml" xref="S4.SS1.SSS1.p2.10.m10.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.10.m10.1c">P_{tar}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.5.1.1" class="ltx_text">IV-A</span>2 </span>Speaker diarisation</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p"><span id="S4.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_bold">DER</span> is the standard metric for evaluating diarisation results between prediction (i.e. hypothesis) and ground truth (i.e. reference).</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">It is computed as:</p>
</div>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.1" class="ltx_Math" alttext="DER=\frac{\texttt{MISS + FA + CONF}}{\texttt{Reference}}" display="block"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mrow id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.2.2" xref="S4.E2.m1.1.1.2.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.1" xref="S4.E2.m1.1.1.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.2.3" xref="S4.E2.m1.1.1.2.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.1a" xref="S4.E2.m1.1.1.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.2.4" xref="S4.E2.m1.1.1.2.4.cmml">R</mi></mrow><mo id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml">=</mo><mfrac id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2a.cmml">MISS + FA + CONF</mtext><mtext class="ltx_mathvariant_monospace" id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3a.cmml">Reference</mtext></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><eq id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"></eq><apply id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2"><times id="S4.E2.m1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.2.1"></times><ci id="S4.E2.m1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.2.2">𝐷</ci><ci id="S4.E2.m1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.2.3">𝐸</ci><ci id="S4.E2.m1.1.1.2.4.cmml" xref="S4.E2.m1.1.1.2.4">𝑅</ci></apply><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><divide id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3"></divide><ci id="S4.E2.m1.1.1.3.2a.cmml" xref="S4.E2.m1.1.1.3.2"><mtext class="ltx_mathvariant_monospace" id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2">MISS + FA + CONF</mtext></ci><ci id="S4.E2.m1.1.1.3.3a.cmml" xref="S4.E2.m1.1.1.3.3"><mtext class="ltx_mathvariant_monospace" id="S4.E2.m1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.3.3">Reference</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">DER=\frac{\texttt{MISS + FA + CONF}}{\texttt{Reference}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.SSS2.p4" class="ltx_para">
<p id="S4.SS1.SSS2.p4.1" class="ltx_p">where <span id="S4.SS1.SSS2.p4.1.1" class="ltx_text ltx_font_typewriter">Reference</span> is the total length of reference, <span id="S4.SS1.SSS2.p4.1.2" class="ltx_text ltx_font_typewriter">MISS</span> is the total length of speech that is present in reference but not in hypothesis and <span id="S4.SS1.SSS2.p4.1.3" class="ltx_text ltx_font_typewriter">FA</span>, false alarm, is the total length of speech that is present in hypothesis but not in reference.
<span id="S4.SS1.SSS2.p4.1.4" class="ltx_text ltx_font_typewriter">CONF</span>, denoting confusion, refers to the total duration of speech which the system incorrectly identifies the speakers.
We applied a forgiveness collar of 0.25 seconds when computing the DER to compensate for small inconsistencies in the annotation.
Overlapping speech was considered during the DER calculation.</p>
</div>
<div id="S4.SS1.SSS2.p5" class="ltx_para">
<p id="S4.SS1.SSS2.p5.2" class="ltx_p"><span id="S4.SS1.SSS2.p5.2.1" class="ltx_text ltx_font_bold">JER</span> was newly introduced in the DIHARD II challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> as another diarisation metric.
It is adopted from the Jaccard similarity index, which is used to evaluate image segmentation.
To compute this, we first construct the mapping between speakers in reference and hypothesis using the Hungarian algorithm.
Then for each reference speaker, we compute <math id="S4.SS1.SSS2.p5.1.m1.1" class="ltx_Math" alttext="JER_{spk}" display="inline"><semantics id="S4.SS1.SSS2.p5.1.m1.1a"><mrow id="S4.SS1.SSS2.p5.1.m1.1.1" xref="S4.SS1.SSS2.p5.1.m1.1.1.cmml"><mi id="S4.SS1.SSS2.p5.1.m1.1.1.2" xref="S4.SS1.SSS2.p5.1.m1.1.1.2.cmml">J</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p5.1.m1.1.1.1" xref="S4.SS1.SSS2.p5.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS2.p5.1.m1.1.1.3" xref="S4.SS1.SSS2.p5.1.m1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p5.1.m1.1.1.1a" xref="S4.SS1.SSS2.p5.1.m1.1.1.1.cmml">​</mo><msub id="S4.SS1.SSS2.p5.1.m1.1.1.4" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.cmml"><mi id="S4.SS1.SSS2.p5.1.m1.1.1.4.2" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.2.cmml">R</mi><mrow id="S4.SS1.SSS2.p5.1.m1.1.1.4.3" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.3.cmml"><mi id="S4.SS1.SSS2.p5.1.m1.1.1.4.3.2" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p5.1.m1.1.1.4.3.1" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.3.1.cmml">​</mo><mi id="S4.SS1.SSS2.p5.1.m1.1.1.4.3.3" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p5.1.m1.1.1.4.3.1a" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.3.1.cmml">​</mo><mi id="S4.SS1.SSS2.p5.1.m1.1.1.4.3.4" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.3.4.cmml">k</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p5.1.m1.1b"><apply id="S4.SS1.SSS2.p5.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1"><times id="S4.SS1.SSS2.p5.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1.1"></times><ci id="S4.SS1.SSS2.p5.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1.2">𝐽</ci><ci id="S4.SS1.SSS2.p5.1.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1.3">𝐸</ci><apply id="S4.SS1.SSS2.p5.1.m1.1.1.4.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p5.1.m1.1.1.4.1.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1.4">subscript</csymbol><ci id="S4.SS1.SSS2.p5.1.m1.1.1.4.2.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.2">𝑅</ci><apply id="S4.SS1.SSS2.p5.1.m1.1.1.4.3.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.3"><times id="S4.SS1.SSS2.p5.1.m1.1.1.4.3.1.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.3.1"></times><ci id="S4.SS1.SSS2.p5.1.m1.1.1.4.3.2.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.3.2">𝑠</ci><ci id="S4.SS1.SSS2.p5.1.m1.1.1.4.3.3.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.3.3">𝑝</ci><ci id="S4.SS1.SSS2.p5.1.m1.1.1.4.3.4.cmml" xref="S4.SS1.SSS2.p5.1.m1.1.1.4.3.4">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p5.1.m1.1c">JER_{spk}</annotation></semantics></math> using Equation <a href="#S4.E3" title="In IV-A2 Speaker diarisation ‣ IV-A Evaluation metrics ‣ IV Challenge mechanics ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> where <span id="S4.SS1.SSS2.p5.2.2" class="ltx_text ltx_font_typewriter">Total</span> is the duration of the union of speaker segments in reference and hypothesis, <span id="S4.SS1.SSS2.p5.2.3" class="ltx_text ltx_font_typewriter">MISS</span> is the duration of speaker segments in the reference which are not present in hypothesis, and <span id="S4.SS1.SSS2.p5.2.4" class="ltx_text ltx_font_typewriter">FA</span> is the duration of speaker segments in the hypothesis which do not exist in reference.
The total JER is the average of <math id="S4.SS1.SSS2.p5.2.m2.1" class="ltx_Math" alttext="JER_{spk}" display="inline"><semantics id="S4.SS1.SSS2.p5.2.m2.1a"><mrow id="S4.SS1.SSS2.p5.2.m2.1.1" xref="S4.SS1.SSS2.p5.2.m2.1.1.cmml"><mi id="S4.SS1.SSS2.p5.2.m2.1.1.2" xref="S4.SS1.SSS2.p5.2.m2.1.1.2.cmml">J</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p5.2.m2.1.1.1" xref="S4.SS1.SSS2.p5.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS1.SSS2.p5.2.m2.1.1.3" xref="S4.SS1.SSS2.p5.2.m2.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p5.2.m2.1.1.1a" xref="S4.SS1.SSS2.p5.2.m2.1.1.1.cmml">​</mo><msub id="S4.SS1.SSS2.p5.2.m2.1.1.4" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.cmml"><mi id="S4.SS1.SSS2.p5.2.m2.1.1.4.2" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.2.cmml">R</mi><mrow id="S4.SS1.SSS2.p5.2.m2.1.1.4.3" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.3.cmml"><mi id="S4.SS1.SSS2.p5.2.m2.1.1.4.3.2" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p5.2.m2.1.1.4.3.1" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.3.1.cmml">​</mo><mi id="S4.SS1.SSS2.p5.2.m2.1.1.4.3.3" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p5.2.m2.1.1.4.3.1a" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.3.1.cmml">​</mo><mi id="S4.SS1.SSS2.p5.2.m2.1.1.4.3.4" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.3.4.cmml">k</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p5.2.m2.1b"><apply id="S4.SS1.SSS2.p5.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p5.2.m2.1.1"><times id="S4.SS1.SSS2.p5.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p5.2.m2.1.1.1"></times><ci id="S4.SS1.SSS2.p5.2.m2.1.1.2.cmml" xref="S4.SS1.SSS2.p5.2.m2.1.1.2">𝐽</ci><ci id="S4.SS1.SSS2.p5.2.m2.1.1.3.cmml" xref="S4.SS1.SSS2.p5.2.m2.1.1.3">𝐸</ci><apply id="S4.SS1.SSS2.p5.2.m2.1.1.4.cmml" xref="S4.SS1.SSS2.p5.2.m2.1.1.4"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p5.2.m2.1.1.4.1.cmml" xref="S4.SS1.SSS2.p5.2.m2.1.1.4">subscript</csymbol><ci id="S4.SS1.SSS2.p5.2.m2.1.1.4.2.cmml" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.2">𝑅</ci><apply id="S4.SS1.SSS2.p5.2.m2.1.1.4.3.cmml" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.3"><times id="S4.SS1.SSS2.p5.2.m2.1.1.4.3.1.cmml" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.3.1"></times><ci id="S4.SS1.SSS2.p5.2.m2.1.1.4.3.2.cmml" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.3.2">𝑠</ci><ci id="S4.SS1.SSS2.p5.2.m2.1.1.4.3.3.cmml" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.3.3">𝑝</ci><ci id="S4.SS1.SSS2.p5.2.m2.1.1.4.3.4.cmml" xref="S4.SS1.SSS2.p5.2.m2.1.1.4.3.4">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p5.2.m2.1c">JER_{spk}</annotation></semantics></math>.</p>
</div>
<div id="S4.SS1.SSS2.p6" class="ltx_para">
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.1" class="ltx_Math" alttext="JER_{spk}=\frac{\texttt{MISS + FA}}{\texttt{Total}}" display="block"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><mrow id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2.cmml"><mi id="S4.E3.m1.1.1.2.2" xref="S4.E3.m1.1.1.2.2.cmml">J</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.2.1" xref="S4.E3.m1.1.1.2.1.cmml">​</mo><mi id="S4.E3.m1.1.1.2.3" xref="S4.E3.m1.1.1.2.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.2.1a" xref="S4.E3.m1.1.1.2.1.cmml">​</mo><msub id="S4.E3.m1.1.1.2.4" xref="S4.E3.m1.1.1.2.4.cmml"><mi id="S4.E3.m1.1.1.2.4.2" xref="S4.E3.m1.1.1.2.4.2.cmml">R</mi><mrow id="S4.E3.m1.1.1.2.4.3" xref="S4.E3.m1.1.1.2.4.3.cmml"><mi id="S4.E3.m1.1.1.2.4.3.2" xref="S4.E3.m1.1.1.2.4.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.2.4.3.1" xref="S4.E3.m1.1.1.2.4.3.1.cmml">​</mo><mi id="S4.E3.m1.1.1.2.4.3.3" xref="S4.E3.m1.1.1.2.4.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.2.4.3.1a" xref="S4.E3.m1.1.1.2.4.3.1.cmml">​</mo><mi id="S4.E3.m1.1.1.2.4.3.4" xref="S4.E3.m1.1.1.2.4.3.4.cmml">k</mi></mrow></msub></mrow><mo id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.cmml">=</mo><mfrac id="S4.E3.m1.1.1.3" xref="S4.E3.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.E3.m1.1.1.3.2" xref="S4.E3.m1.1.1.3.2a.cmml">MISS + FA</mtext><mtext class="ltx_mathvariant_monospace" id="S4.E3.m1.1.1.3.3" xref="S4.E3.m1.1.1.3.3a.cmml">Total</mtext></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><eq id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1.1"></eq><apply id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2"><times id="S4.E3.m1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.2.1"></times><ci id="S4.E3.m1.1.1.2.2.cmml" xref="S4.E3.m1.1.1.2.2">𝐽</ci><ci id="S4.E3.m1.1.1.2.3.cmml" xref="S4.E3.m1.1.1.2.3">𝐸</ci><apply id="S4.E3.m1.1.1.2.4.cmml" xref="S4.E3.m1.1.1.2.4"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.2.4.1.cmml" xref="S4.E3.m1.1.1.2.4">subscript</csymbol><ci id="S4.E3.m1.1.1.2.4.2.cmml" xref="S4.E3.m1.1.1.2.4.2">𝑅</ci><apply id="S4.E3.m1.1.1.2.4.3.cmml" xref="S4.E3.m1.1.1.2.4.3"><times id="S4.E3.m1.1.1.2.4.3.1.cmml" xref="S4.E3.m1.1.1.2.4.3.1"></times><ci id="S4.E3.m1.1.1.2.4.3.2.cmml" xref="S4.E3.m1.1.1.2.4.3.2">𝑠</ci><ci id="S4.E3.m1.1.1.2.4.3.3.cmml" xref="S4.E3.m1.1.1.2.4.3.3">𝑝</ci><ci id="S4.E3.m1.1.1.2.4.3.4.cmml" xref="S4.E3.m1.1.1.2.4.3.4">𝑘</ci></apply></apply></apply><apply id="S4.E3.m1.1.1.3.cmml" xref="S4.E3.m1.1.1.3"><divide id="S4.E3.m1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.3"></divide><ci id="S4.E3.m1.1.1.3.2a.cmml" xref="S4.E3.m1.1.1.3.2"><mtext class="ltx_mathvariant_monospace" id="S4.E3.m1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.3.2">MISS + FA</mtext></ci><ci id="S4.E3.m1.1.1.3.3a.cmml" xref="S4.E3.m1.1.1.3.3"><mtext class="ltx_mathvariant_monospace" id="S4.E3.m1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.3.3">Total</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">JER_{spk}=\frac{\texttt{MISS + FA}}{\texttt{Total}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">How we hosted the VoxSRC challenge</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The challenge started approximately two months before the workshop when the validation and test sets for each track were released.
The submission format differed between speaker verification and diarisation tracks.
For speaker verification tracks, we provided a list of pairs with corresponding audio files and asked the participants to submit the prediction scores for whether each pair was from the same speaker or not.
For speaker diarisation tracks, participants were required to submit their results in RTTM format, which contains channel id, start time, duration and speaker id for each speech segment.
We also provided a validation set per track with ground truth labels to let participants measure their systems’ performance during development.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The challenge was hosted via CodaLab <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
Each year we used the submission server provided by the CodaLab platform except for the last two years when we created our own backend to deal with bugs and errors effectively.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Our challenge had two phases: the <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_italic">challenge</span> phase and the <span id="S4.SS2.p3.1.2" class="ltx_text ltx_font_italic">permanent</span> phase.
The <span id="S4.SS2.p3.1.3" class="ltx_text ltx_font_italic">challenge</span> phase ended before the workshop and only submissions made within this phase were considered for the prizes and certificates at the following workshop.
We also opened a <span id="S4.SS2.p3.1.4" class="ltx_text ltx_font_italic">permanent</span> phase after the workshop ended to let people both in industry and academia evaluate their systems with our test set and compare them against competition winners’ performance.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Participants were allowed to submit only one submission per day.
The total number of submissions allowed was 5 in 2019 and 2020, and then was increased to 10 for future challenges reflecting participants’ feedback.
The limit on submission was to prevent overfitting on the test set.
To prevent the same team from making submissions across multiple CodaLab accounts, we only allowed participants who registered with either academic or industry email accounts to participate. We show the participant statistics each year in the Appendix <a href="#A2" title="Appendix B Participant statistics ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">We also provided baselines with code and models for all tracks to help new participants get started each year.
We mostly used ResNet-34 models adopted from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> for speaker verification.
For diarisation, we adopted the clustering-based approach from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> using pyannote <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> VAD, speaker embedding extractor from either <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> or <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and agglomerative hierachical clustering.
We released both verification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and diarisation baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> in public GitHub repositories.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Results</span>
</h2>

<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>An overview of the winning methods on Tracks 1 and 2 across the VoxSRC challenges.</figcaption>
<div id="S5.T2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:207.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-474.2pt,226.7pt) scale(0.31373839385314,0.31373839385314) ;">
<table id="S5.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Workshop</span></th>
<th id="S5.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Track</span></th>
<th id="S5.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Input feature</span></th>
<th id="S5.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Embedding network architecture</span></th>
<th id="S5.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Data augmentation</span></th>
<th id="S5.T2.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Loss function</span></th>
<th id="S5.T2.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.7.1" class="ltx_text ltx_font_bold">Back-end</span></th>
<th id="S5.T2.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.8.1" class="ltx_text ltx_font_bold">External data</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.2.1" class="ltx_tr">
<td id="S5.T2.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S5.T2.1.1.2.1.1.1" class="ltx_text">
<span id="S5.T2.1.1.2.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.1.1.2.1.1.1.1.1" class="ltx_tr">
<span id="S5.T2.1.1.2.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></span></span>
<span id="S5.T2.1.1.2.1.1.1.1.2" class="ltx_tr">
<span id="S5.T2.1.1.2.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.2.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">2019</span></span></span>
</span></span></td>
<td id="S5.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.2.1.2.1" class="ltx_text ltx_font_bold">1</span></td>
<td id="S5.T2.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.2.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.2.1.3.1.1" class="ltx_tr">
<td id="S5.T2.1.1.2.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">PLP</td>
</tr>
<tr id="S5.T2.1.1.2.1.3.1.2" class="ltx_tr">
<td id="S5.T2.1.1.2.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">log-mel Fbank</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.2.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.2.1.4.1.1" class="ltx_tr">
<td id="S5.T2.1.1.2.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> variants</td>
</tr>
<tr id="S5.T2.1.1.2.1.4.1.2" class="ltx_tr">
<td id="S5.T2.1.1.2.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- TDNN</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.2.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.2.1.5.1.1" class="ltx_tr">
<td id="S5.T2.1.1.2.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- Reverberation</td>
</tr>
<tr id="S5.T2.1.1.2.1.5.1.2" class="ltx_tr">
<td id="S5.T2.1.1.2.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Additive noise with MUSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t">- AAM loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S5.T2.1.1.2.1.7" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.2.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.2.1.7.1.1" class="ltx_tr">
<td id="S5.T2.1.1.2.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- GPLDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.2.1.7.1.2" class="ltx_tr">
<td id="S5.T2.1.1.2.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- AS-norm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.2.1.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">-</td>
</tr>
<tr id="S5.T2.1.1.3.2" class="ltx_tr">
<td id="S5.T2.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.3.2.1.1" class="ltx_text ltx_font_bold">2</span></td>
<td id="S5.T2.1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.3.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.3.2.2.1.1" class="ltx_tr">
<td id="S5.T2.1.1.3.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">PLP</td>
</tr>
<tr id="S5.T2.1.1.3.2.2.1.2" class="ltx_tr">
<td id="S5.T2.1.1.3.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">log-mel Fbank</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.3.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.3.2.3.1.1" class="ltx_tr">
<td id="S5.T2.1.1.3.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> variants</td>
</tr>
<tr id="S5.T2.1.1.3.2.3.1.2" class="ltx_tr">
<td id="S5.T2.1.1.3.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- TDNN</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.3.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.3.2.4.1.1" class="ltx_tr">
<td id="S5.T2.1.1.3.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- Reverberation</td>
</tr>
<tr id="S5.T2.1.1.3.2.4.1.2" class="ltx_tr">
<td id="S5.T2.1.1.3.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Additive noise with MUSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.3.2.5" class="ltx_td ltx_align_left ltx_border_t">- AAM loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S5.T2.1.1.3.2.6" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.3.2.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.3.2.6.1.1" class="ltx_tr">
<td id="S5.T2.1.1.3.2.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- GPLDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.3.2.6.1.2" class="ltx_tr">
<td id="S5.T2.1.1.3.2.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- AS-norm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.3.2.7" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.3.2.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.3.2.7.1.1" class="ltx_tr">
<td id="S5.T2.1.1.3.2.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- Librispeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.3.2.7.1.2" class="ltx_tr">
<td id="S5.T2.1.1.3.2.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Deepmine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T2.1.1.4.3" class="ltx_tr">
<td id="S5.T2.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S5.T2.1.1.4.3.1.1" class="ltx_text">
<span id="S5.T2.1.1.4.3.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.1.1.4.3.1.1.1.1" class="ltx_tr">
<span id="S5.T2.1.1.4.3.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.4.3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></span></span>
<span id="S5.T2.1.1.4.3.1.1.1.2" class="ltx_tr">
<span id="S5.T2.1.1.4.3.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.4.3.1.1.1.2.1.1" class="ltx_text ltx_font_bold">2020</span></span></span>
</span></span></td>
<td id="S5.T2.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.4.3.2.1" class="ltx_text ltx_font_bold">1</span></td>
<td id="S5.T2.1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.4.3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.4.3.3.1.1" class="ltx_tr">
<td id="S5.T2.1.1.4.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">MFCC</td>
</tr>
<tr id="S5.T2.1.1.4.3.3.1.2" class="ltx_tr">
<td id="S5.T2.1.1.4.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">log-mel Fbank</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.4.3.4" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.4.3.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.4.3.4.1.1" class="ltx_tr">
<td id="S5.T2.1.1.4.3.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- ECAPA-TDNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.4.3.4.1.2" class="ltx_tr">
<td id="S5.T2.1.1.4.3.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- SE-ResNet34 variants</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.4.3.5" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.4.3.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.4.3.5.1.1" class="ltx_tr">
<td id="S5.T2.1.1.4.3.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- Reverberation</td>
</tr>
<tr id="S5.T2.1.1.4.3.5.1.2" class="ltx_tr">
<td id="S5.T2.1.1.4.3.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Additive noise with MUSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.4.3.5.1.3" class="ltx_tr">
<td id="S5.T2.1.1.4.3.5.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">- SpecAugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.4.3.5.1.4" class="ltx_tr">
<td id="S5.T2.1.1.4.3.5.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">- Tempo up and down</td>
</tr>
<tr id="S5.T2.1.1.4.3.5.1.5" class="ltx_tr">
<td id="S5.T2.1.1.4.3.5.1.5.1" class="ltx_td ltx_nopad_r ltx_align_left">- Compression using FFmpeg</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.4.3.6" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.4.3.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.4.3.6.1.1" class="ltx_tr">
<td id="S5.T2.1.1.4.3.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- AAM loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.4.3.6.1.2" class="ltx_tr">
<td id="S5.T2.1.1.4.3.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Large-margin Finetuning</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.4.3.7" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.4.3.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.4.3.7.1.1" class="ltx_tr">
<td id="S5.T2.1.1.4.3.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- QMF</td>
</tr>
<tr id="S5.T2.1.1.4.3.7.1.2" class="ltx_tr">
<td id="S5.T2.1.1.4.3.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- AS-norm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.4.3.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">-</td>
</tr>
<tr id="S5.T2.1.1.5.4" class="ltx_tr">
<td id="S5.T2.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.5.4.1.1" class="ltx_text ltx_font_bold">2</span></td>
<td id="S5.T2.1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.5.4.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.5.4.2.1.1" class="ltx_tr">
<td id="S5.T2.1.1.5.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">MFCC</td>
</tr>
<tr id="S5.T2.1.1.5.4.2.1.2" class="ltx_tr">
<td id="S5.T2.1.1.5.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">log-mel Fbank</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.5.4.3" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.5.4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.5.4.3.1.1" class="ltx_tr">
<td id="S5.T2.1.1.5.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- ECAPA-TDNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.5.4.3.1.2" class="ltx_tr">
<td id="S5.T2.1.1.5.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Se-ResNet34 variants</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.5.4.4" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.5.4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.5.4.4.1.1" class="ltx_tr">
<td id="S5.T2.1.1.5.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- Reverberation</td>
</tr>
<tr id="S5.T2.1.1.5.4.4.1.2" class="ltx_tr">
<td id="S5.T2.1.1.5.4.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Additive noise with MUSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.5.4.4.1.3" class="ltx_tr">
<td id="S5.T2.1.1.5.4.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">- SpecAugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.5.4.4.1.4" class="ltx_tr">
<td id="S5.T2.1.1.5.4.4.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">- Tempo up and down</td>
</tr>
<tr id="S5.T2.1.1.5.4.4.1.5" class="ltx_tr">
<td id="S5.T2.1.1.5.4.4.1.5.1" class="ltx_td ltx_nopad_r ltx_align_left">- Compression using FFmpeg</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.5.4.5" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.5.4.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.5.4.5.1.1" class="ltx_tr">
<td id="S5.T2.1.1.5.4.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- AAM loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.5.4.5.1.2" class="ltx_tr">
<td id="S5.T2.1.1.5.4.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Large-margin Finetuning</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.5.4.6" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.5.4.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.5.4.6.1.1" class="ltx_tr">
<td id="S5.T2.1.1.5.4.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- QMF</td>
</tr>
<tr id="S5.T2.1.1.5.4.6.1.2" class="ltx_tr">
<td id="S5.T2.1.1.5.4.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- AS-norm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.5.4.7" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.5.4.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.5.4.7.1.1" class="ltx_tr">
<td id="S5.T2.1.1.5.4.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- VoxCeleb1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.5.4.7.1.2" class="ltx_tr">
<td id="S5.T2.1.1.5.4.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Librispeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.5.4.7.1.3" class="ltx_tr">
<td id="S5.T2.1.1.5.4.7.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">- Deepmine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T2.1.1.6.5" class="ltx_tr">
<td id="S5.T2.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_t">
<table id="S5.T2.1.1.6.5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.6.5.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.6.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.6.5.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></td>
</tr>
<tr id="S5.T2.1.1.6.5.1.1.2" class="ltx_tr">
<td id="S5.T2.1.1.6.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.6.5.1.1.2.1.1" class="ltx_text ltx_font_bold">2021</span></td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.6.5.2.1" class="ltx_text ltx_font_bold">1,2</span></td>
<td id="S5.T2.1.1.6.5.3" class="ltx_td ltx_align_left ltx_border_t">log-mel Fbank</td>
<td id="S5.T2.1.1.6.5.4" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.6.5.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.6.5.4.1.1" class="ltx_tr">
<td id="S5.T2.1.1.6.5.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- RepVGG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.6.5.4.1.2" class="ltx_tr">
<td id="S5.T2.1.1.6.5.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> variants</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.6.5.5" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.6.5.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.6.5.5.1.1" class="ltx_tr">
<td id="S5.T2.1.1.6.5.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- Reverberation</td>
</tr>
<tr id="S5.T2.1.1.6.5.5.1.2" class="ltx_tr">
<td id="S5.T2.1.1.6.5.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Additive noise with MUSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.6.5.5.1.3" class="ltx_tr">
<td id="S5.T2.1.1.6.5.5.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">- White noise</td>
</tr>
<tr id="S5.T2.1.1.6.5.5.1.4" class="ltx_tr">
<td id="S5.T2.1.1.6.5.5.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">- Gain augment</td>
</tr>
<tr id="S5.T2.1.1.6.5.5.1.5" class="ltx_tr">
<td id="S5.T2.1.1.6.5.5.1.5.1" class="ltx_td ltx_nopad_r ltx_align_left">- Time stretch augment</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.6.5.6" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.6.5.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.6.5.6.1.1" class="ltx_tr">
<td id="S5.T2.1.1.6.5.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- AAM loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.6.5.6.1.2" class="ltx_tr">
<td id="S5.T2.1.1.6.5.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Inter-TopK loss</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.6.5.7" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.6.5.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.6.5.7.1.1" class="ltx_tr">
<td id="S5.T2.1.1.6.5.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- QMF</td>
</tr>
<tr id="S5.T2.1.1.6.5.7.1.2" class="ltx_tr">
<td id="S5.T2.1.1.6.5.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- AS-norm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.6.5.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">-</td>
</tr>
<tr id="S5.T2.1.1.7.6" class="ltx_tr">
<td id="S5.T2.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S5.T2.1.1.7.6.1.1" class="ltx_text">
<span id="S5.T2.1.1.7.6.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.1.1.7.6.1.1.1.1" class="ltx_tr">
<span id="S5.T2.1.1.7.6.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.7.6.1.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></span></span>
<span id="S5.T2.1.1.7.6.1.1.1.2" class="ltx_tr">
<span id="S5.T2.1.1.7.6.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.7.6.1.1.1.2.1.1" class="ltx_text ltx_font_bold">2022</span></span></span>
</span></span></td>
<td id="S5.T2.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.7.6.2.1" class="ltx_text ltx_font_bold">1</span></td>
<td id="S5.T2.1.1.7.6.3" class="ltx_td ltx_align_left ltx_border_t">log-mel Fbank</td>
<td id="S5.T2.1.1.7.6.4" class="ltx_td ltx_align_left ltx_border_t">- ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> variants</td>
<td id="S5.T2.1.1.7.6.5" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.7.6.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.7.6.5.1.1" class="ltx_tr">
<td id="S5.T2.1.1.7.6.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- Reverberation</td>
</tr>
<tr id="S5.T2.1.1.7.6.5.1.2" class="ltx_tr">
<td id="S5.T2.1.1.7.6.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Additive noise with MUSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.7.6.5.1.3" class="ltx_tr">
<td id="S5.T2.1.1.7.6.5.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">- Speed augment</td>
</tr>
<tr id="S5.T2.1.1.7.6.5.1.4" class="ltx_tr">
<td id="S5.T2.1.1.7.6.5.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">- SpecAugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.7.6.6" class="ltx_td ltx_align_left ltx_border_t">- AM loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
<td id="S5.T2.1.1.7.6.7" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.7.6.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.7.6.7.1.1" class="ltx_tr">
<td id="S5.T2.1.1.7.6.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- QMF</td>
</tr>
<tr id="S5.T2.1.1.7.6.7.1.2" class="ltx_tr">
<td id="S5.T2.1.1.7.6.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- AS-norm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.7.6.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">-</td>
</tr>
<tr id="S5.T2.1.1.8.7" class="ltx_tr">
<td id="S5.T2.1.1.8.7.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.8.7.1.1" class="ltx_text ltx_font_bold">2</span></td>
<td id="S5.T2.1.1.8.7.2" class="ltx_td ltx_align_left ltx_border_t">log-mel Fbank</td>
<td id="S5.T2.1.1.8.7.3" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.8.7.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.8.7.3.1.1" class="ltx_tr">
<td id="S5.T2.1.1.8.7.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> variants</td>
</tr>
<tr id="S5.T2.1.1.8.7.3.1.2" class="ltx_tr">
<td id="S5.T2.1.1.8.7.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- ECAPA-TDNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> with</td>
</tr>
<tr id="S5.T2.1.1.8.7.3.1.3" class="ltx_tr">
<td id="S5.T2.1.1.8.7.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and Hubert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> features</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.8.7.4" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.8.7.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.8.7.4.1.1" class="ltx_tr">
<td id="S5.T2.1.1.8.7.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- Reverberation</td>
</tr>
<tr id="S5.T2.1.1.8.7.4.1.2" class="ltx_tr">
<td id="S5.T2.1.1.8.7.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Additive noise with MUSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.8.7.4.1.3" class="ltx_tr">
<td id="S5.T2.1.1.8.7.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">- Speed augment</td>
</tr>
<tr id="S5.T2.1.1.8.7.4.1.4" class="ltx_tr">
<td id="S5.T2.1.1.8.7.4.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">- SpecAugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.8.7.5" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.8.7.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.8.7.5.1.1" class="ltx_tr">
<td id="S5.T2.1.1.8.7.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- AM loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.8.7.5.1.2" class="ltx_tr">
<td id="S5.T2.1.1.8.7.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- AAM loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.8.7.6" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.8.7.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.8.7.6.1.1" class="ltx_tr">
<td id="S5.T2.1.1.8.7.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- QMF</td>
</tr>
<tr id="S5.T2.1.1.8.7.6.1.2" class="ltx_tr">
<td id="S5.T2.1.1.8.7.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- AS-norm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.8.7.7" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">Self-VoxCeleb</td>
</tr>
<tr id="S5.T2.1.1.9.8" class="ltx_tr">
<td id="S5.T2.1.1.9.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S5.T2.1.1.9.8.1.1" class="ltx_text">
<span id="S5.T2.1.1.9.8.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T2.1.1.9.8.1.1.1.1" class="ltx_tr">
<span id="S5.T2.1.1.9.8.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.9.8.1.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></span></span>
<span id="S5.T2.1.1.9.8.1.1.1.2" class="ltx_tr">
<span id="S5.T2.1.1.9.8.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.9.8.1.1.1.2.1.1" class="ltx_text ltx_font_bold">2023</span></span></span>
</span></span></td>
<td id="S5.T2.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.1.9.8.2.1" class="ltx_text ltx_font_bold">1</span></td>
<td id="S5.T2.1.1.9.8.3" class="ltx_td ltx_align_left ltx_border_t">log-mel Fbank</td>
<td id="S5.T2.1.1.9.8.4" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.9.8.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.9.8.4.1.1" class="ltx_tr">
<td id="S5.T2.1.1.9.8.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- RepVGG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.9.8.4.1.2" class="ltx_tr">
<td id="S5.T2.1.1.9.8.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> variants</td>
</tr>
<tr id="S5.T2.1.1.9.8.4.1.3" class="ltx_tr">
<td id="S5.T2.1.1.9.8.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">- Multi-query multi-head attention (MQMHA)</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.9.8.5" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.9.8.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.9.8.5.1.1" class="ltx_tr">
<td id="S5.T2.1.1.9.8.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- Reverberation</td>
</tr>
<tr id="S5.T2.1.1.9.8.5.1.2" class="ltx_tr">
<td id="S5.T2.1.1.9.8.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Additive noise with MUSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.9.8.5.1.3" class="ltx_tr">
<td id="S5.T2.1.1.9.8.5.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">- Speed augment</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.9.8.6" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.9.8.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.9.8.6.1.1" class="ltx_tr">
<td id="S5.T2.1.1.9.8.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- AM loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.9.8.6.1.2" class="ltx_tr">
<td id="S5.T2.1.1.9.8.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- AAM loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.9.8.7" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T2.1.1.9.8.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.9.8.7.1.1" class="ltx_tr">
<td id="S5.T2.1.1.9.8.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- QMF</td>
</tr>
<tr id="S5.T2.1.1.9.8.7.1.2" class="ltx_tr">
<td id="S5.T2.1.1.9.8.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- AS-norm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.9.8.8" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">-</td>
</tr>
<tr id="S5.T2.1.1.10.9" class="ltx_tr">
<td id="S5.T2.1.1.10.9.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T2.1.1.10.9.1.1" class="ltx_text ltx_font_bold">2</span></td>
<td id="S5.T2.1.1.10.9.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">log-mel Fbank</td>
<td id="S5.T2.1.1.10.9.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<table id="S5.T2.1.1.10.9.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.10.9.3.1.1" class="ltx_tr">
<td id="S5.T2.1.1.10.9.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> variants</td>
</tr>
<tr id="S5.T2.1.1.10.9.3.1.2" class="ltx_tr">
<td id="S5.T2.1.1.10.9.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- ECAPA-TDNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> with WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>,</td>
</tr>
<tr id="S5.T2.1.1.10.9.3.1.3" class="ltx_tr">
<td id="S5.T2.1.1.10.9.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">Unispeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> and XLSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> features</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.10.9.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<table id="S5.T2.1.1.10.9.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.10.9.4.1.1" class="ltx_tr">
<td id="S5.T2.1.1.10.9.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- Reverberation</td>
</tr>
<tr id="S5.T2.1.1.10.9.4.1.2" class="ltx_tr">
<td id="S5.T2.1.1.10.9.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Additive noise with MUSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.10.9.4.1.3" class="ltx_tr">
<td id="S5.T2.1.1.10.9.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">- SpecAugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.10.9.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<table id="S5.T2.1.1.10.9.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.10.9.5.1.1" class="ltx_tr">
<td id="S5.T2.1.1.10.9.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- AM loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
</tr>
<tr id="S5.T2.1.1.10.9.5.1.2" class="ltx_tr">
<td id="S5.T2.1.1.10.9.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- AAM loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.10.9.6" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<table id="S5.T2.1.1.10.9.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.10.9.6.1.1" class="ltx_tr">
<td id="S5.T2.1.1.10.9.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- CMF score</td>
</tr>
<tr id="S5.T2.1.1.10.9.6.1.2" class="ltx_tr">
<td id="S5.T2.1.1.10.9.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- QMF</td>
</tr>
<tr id="S5.T2.1.1.10.9.6.1.3" class="ltx_tr">
<td id="S5.T2.1.1.10.9.6.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">- AS-norm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T2.1.1.10.9.7" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t">VoxTube <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section discusses the results of each track over the last five years.
We also study the longitudinal progress on verification and diarisation tracks over the years by comparing performance on the VoxSRC 2019 verification<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Included as a subset in all subsequent years’ test sets.</span></span></span> and 2021 diarisation test sets.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Speaker verification – Track 1 and 2</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Table <a href="#S5.T2" title="TABLE II ‣ V Results ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> gives an overview of the winners’ system for Tracks 1 and 2, comparing them over six aspects. The performance of winners each year is shown in Table <a href="#S5.T3" title="TABLE III ‣ V-A Speaker verification – Track 1 and 2 ‣ V Results ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.
As the input, speech waveforms could be directly utilised or could be converted into acoustic features. All teams adopted a DNN-based embedding extractor, which maps a variable length speech segment into a single speaker representation.
Participants trained the DNN with diverse architectures such as ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, ECAPA-TDNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> or RepVGG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and combine several model outputs for final submission.
Commonly leveraged data augmentation techniques included additive noise with MUSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, reverberation using public room impulse response (RIR) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> or spec augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Angular Margin softmax loss (AM loss) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> and Additive Angular Margin softmax loss (AAM loss) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> were the two most commonly used training objectives to learn the embedding extractor.
A few winners adopted additional techniques such as large-margin finetuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> or Inter-TopK penalty <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> to further improve their model performance.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">The scoring procedures included probabilistic linear discriminant analysis (PLDA), and most commonly, direct cosine scoring. Both of them optionally followed by score normalisation.
From the year 2020, the VoxSRC winners started to use the Quality Measure Function (QMF), which has already been explored in the NIST SREs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>.
They include quality metrics such as speech duration or magnitude of non-normalised embeddings to model various conditions of the trial utterances using logistic regression.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">In Track 2, several participants utilised external public data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.
However, it did not show a clear performance gap between Tracks 1 and 2 until VoxSRC 2021.
Both the VoxSRC 2022 winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> and 2023 winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> demonstrated that self-supervised pretrained models generalise well in the new domain with a clear gap between Tracks 1 and 2 submissions (EER 1.49% to 1.21%).
They leveraged self-supervised pretrained models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> to extract features and train another speaker model using these features.
The VoxSRC 2022 and 2023 winner also curated their own datasets, Self-VoxCeleb and VoxTube <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> respectively, using a similar data creation pipeline to that of VoxCeleb.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Comparison of methods on the four workshop test sets of Track 1 and 2. We compare top-2 submissions of the last four years on the 2019 test set. For % EER shown, lower is better. Note that in most years the EER is not the primary evaluation metric. Therefore, in some years the 2nd place have a better EER than the 1st place.</figcaption>
<div id="S5.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:297.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-45.9pt,31.4pt) scale(0.825422760299812,0.825422760299812) ;">
<table id="S5.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Team</span></th>
<th id="S5.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Track</span></th>
<th id="S5.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">2019 test</span></th>
<th id="S5.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.1.4.1" class="ltx_text ltx_font_bold">2020 test</span></th>
<th id="S5.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.1.5.1" class="ltx_text ltx_font_bold">2021 test</span></th>
<th id="S5.T3.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.1.6.1" class="ltx_text ltx_font_bold">2022 test</span></th>
<th id="S5.T3.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.1.7.1" class="ltx_text ltx_font_bold">2023 test</span></th>
</tr>
<tr id="S5.T3.1.1.2.2" class="ltx_tr">
<th id="S5.T3.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</th>
<th id="S5.T3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">-</th>
<th id="S5.T3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">1.29</th>
<th id="S5.T3.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">5.01</th>
<th id="S5.T3.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">5.00</th>
<th id="S5.T3.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">5.26</th>
<th id="S5.T3.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">5.45</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.3.1" class="ltx_tr">
<td id="S5.T3.1.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2019 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S5.T3.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S5.T3.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">1.42</td>
<td id="S5.T3.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T3.1.1.4.2" class="ltx_tr">
<td id="S5.T3.1.1.4.2.1" class="ltx_td ltx_align_left">VoxSRC 2019 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>
</td>
<td id="S5.T3.1.1.4.2.2" class="ltx_td ltx_align_center">1</td>
<td id="S5.T3.1.1.4.2.3" class="ltx_td ltx_align_center">1.54</td>
<td id="S5.T3.1.1.4.2.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.4.2.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.4.2.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.4.2.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T3.1.1.5.3" class="ltx_tr">
<td id="S5.T3.1.1.5.3.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2019 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S5.T3.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_t">2</td>
<td id="S5.T3.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_t">1.26</td>
<td id="S5.T3.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.5.3.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T3.1.1.6.4" class="ltx_tr">
<td id="S5.T3.1.1.6.4.1" class="ltx_td ltx_align_left">VoxSRC 2019 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>
</td>
<td id="S5.T3.1.1.6.4.2" class="ltx_td ltx_align_center">2</td>
<td id="S5.T3.1.1.6.4.3" class="ltx_td ltx_align_center">1.49</td>
<td id="S5.T3.1.1.6.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.6.4.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.6.4.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.6.4.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T3.1.1.7.5" class="ltx_tr">
<td id="S5.T3.1.1.7.5.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2020 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
<td id="S5.T3.1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S5.T3.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_t">0.83</td>
<td id="S5.T3.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_t">3.73</td>
<td id="S5.T3.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.7.5.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T3.1.1.8.6" class="ltx_tr">
<td id="S5.T3.1.1.8.6.1" class="ltx_td ltx_align_left">VoxSRC 2020 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</td>
<td id="S5.T3.1.1.8.6.2" class="ltx_td ltx_align_center">1</td>
<td id="S5.T3.1.1.8.6.3" class="ltx_td ltx_align_center">0.75</td>
<td id="S5.T3.1.1.8.6.4" class="ltx_td ltx_align_center">3.81</td>
<td id="S5.T3.1.1.8.6.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.8.6.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.8.6.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T3.1.1.9.7" class="ltx_tr">
<td id="S5.T3.1.1.9.7.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2020 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
<td id="S5.T3.1.1.9.7.2" class="ltx_td ltx_align_center ltx_border_t">2</td>
<td id="S5.T3.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_t">0.80</td>
<td id="S5.T3.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.9.7.4.1" class="ltx_text ltx_font_bold">3.58</span></td>
<td id="S5.T3.1.1.9.7.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.9.7.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.9.7.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T3.1.1.10.8" class="ltx_tr">
<td id="S5.T3.1.1.10.8.1" class="ltx_td ltx_align_left">VoxSRC 2020 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</td>
<td id="S5.T3.1.1.10.8.2" class="ltx_td ltx_align_center">2</td>
<td id="S5.T3.1.1.10.8.3" class="ltx_td ltx_align_center">0.79</td>
<td id="S5.T3.1.1.10.8.4" class="ltx_td ltx_align_center">3.80</td>
<td id="S5.T3.1.1.10.8.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.10.8.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.10.8.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T3.1.1.11.9" class="ltx_tr">
<td id="S5.T3.1.1.11.9.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2021 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S5.T3.1.1.11.9.2" class="ltx_td ltx_align_center ltx_border_t">1,2</td>
<td id="S5.T3.1.1.11.9.3" class="ltx_td ltx_align_center ltx_border_t">0.57</td>
<td id="S5.T3.1.1.11.9.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.11.9.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.11.9.5.1" class="ltx_text ltx_font_bold">1.85</span></td>
<td id="S5.T3.1.1.11.9.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.11.9.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T3.1.1.12.10" class="ltx_tr">
<td id="S5.T3.1.1.12.10.1" class="ltx_td ltx_align_left">VoxSRC 2021 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
<td id="S5.T3.1.1.12.10.2" class="ltx_td ltx_align_center">1,2</td>
<td id="S5.T3.1.1.12.10.3" class="ltx_td ltx_align_center">0.62</td>
<td id="S5.T3.1.1.12.10.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.12.10.5" class="ltx_td ltx_align_center">2.84</td>
<td id="S5.T3.1.1.12.10.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.12.10.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T3.1.1.13.11" class="ltx_tr">
<td id="S5.T3.1.1.13.11.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2022 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S5.T3.1.1.13.11.2" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S5.T3.1.1.13.11.3" class="ltx_td ltx_align_center ltx_border_t">0.90</td>
<td id="S5.T3.1.1.13.11.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.13.11.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.13.11.6" class="ltx_td ltx_align_center ltx_border_t">1.49</td>
<td id="S5.T3.1.1.13.11.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T3.1.1.14.12" class="ltx_tr">
<td id="S5.T3.1.1.14.12.1" class="ltx_td ltx_align_left">VoxSRC 2022 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
<td id="S5.T3.1.1.14.12.2" class="ltx_td ltx_align_center">1</td>
<td id="S5.T3.1.1.14.12.3" class="ltx_td ltx_align_center">0.65</td>
<td id="S5.T3.1.1.14.12.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.14.12.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.14.12.6" class="ltx_td ltx_align_center">1.40</td>
<td id="S5.T3.1.1.14.12.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T3.1.1.15.13" class="ltx_tr">
<td id="S5.T3.1.1.15.13.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2022 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S5.T3.1.1.15.13.2" class="ltx_td ltx_align_center ltx_border_t">2</td>
<td id="S5.T3.1.1.15.13.3" class="ltx_td ltx_align_center ltx_border_t">0.69</td>
<td id="S5.T3.1.1.15.13.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.15.13.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.15.13.6" class="ltx_td ltx_align_center ltx_border_t">1.21</td>
<td id="S5.T3.1.1.15.13.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T3.1.1.16.14" class="ltx_tr">
<td id="S5.T3.1.1.16.14.1" class="ltx_td ltx_align_left">VoxSRC 2022 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
<td id="S5.T3.1.1.16.14.2" class="ltx_td ltx_align_center">2</td>
<td id="S5.T3.1.1.16.14.3" class="ltx_td ltx_align_center">0.50</td>
<td id="S5.T3.1.1.16.14.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.16.14.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.16.14.6" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.16.14.6.1" class="ltx_text ltx_font_bold">1.12</span></td>
<td id="S5.T3.1.1.16.14.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T3.1.1.17.15" class="ltx_tr">
<td id="S5.T3.1.1.17.15.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2023 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>
</td>
<td id="S5.T3.1.1.17.15.2" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S5.T3.1.1.17.15.3" class="ltx_td ltx_align_center ltx_border_t">0.58</td>
<td id="S5.T3.1.1.17.15.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.17.15.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.17.15.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.17.15.7" class="ltx_td ltx_align_center ltx_border_t">1.59</td>
</tr>
<tr id="S5.T3.1.1.18.16" class="ltx_tr">
<td id="S5.T3.1.1.18.16.1" class="ltx_td ltx_align_left">VoxSRC 2023 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>
</td>
<td id="S5.T3.1.1.18.16.2" class="ltx_td ltx_align_center">1</td>
<td id="S5.T3.1.1.18.16.3" class="ltx_td ltx_align_center">0.59</td>
<td id="S5.T3.1.1.18.16.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.18.16.5" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.18.16.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.18.16.7" class="ltx_td ltx_align_center">1.76</td>
</tr>
<tr id="S5.T3.1.1.19.17" class="ltx_tr">
<td id="S5.T3.1.1.19.17.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2023 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>
</td>
<td id="S5.T3.1.1.19.17.2" class="ltx_td ltx_align_center ltx_border_t">2</td>
<td id="S5.T3.1.1.19.17.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.19.17.3.1" class="ltx_text ltx_font_bold">0.47</span></td>
<td id="S5.T3.1.1.19.17.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.19.17.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.19.17.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.19.17.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.19.17.7.1" class="ltx_text ltx_font_bold">1.30</span></td>
</tr>
<tr id="S5.T3.1.1.20.18" class="ltx_tr">
<td id="S5.T3.1.1.20.18.1" class="ltx_td ltx_align_left ltx_border_bb">VoxSRC 2023 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>
</td>
<td id="S5.T3.1.1.20.18.2" class="ltx_td ltx_align_center ltx_border_bb">2</td>
<td id="S5.T3.1.1.20.18.3" class="ltx_td ltx_align_center ltx_border_bb">0.58</td>
<td id="S5.T3.1.1.20.18.4" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S5.T3.1.1.20.18.5" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S5.T3.1.1.20.18.6" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S5.T3.1.1.20.18.7" class="ltx_td ltx_align_center ltx_border_bb">1.59</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS1.p5" class="ltx_para ltx_noindent">
<p id="S5.SS1.p5.1" class="ltx_p"><span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_bold">Performance progression.</span> The VoxSRC 2019 test set has been included as a subset in all challenges.
We analyse the progress of state of the art by comparing the winning teams’ performances on this set.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.1" class="ltx_p">Results are shown in Figure <a href="#S5.F2" title="Figure 2 ‣ V-A Speaker verification – Track 1 and 2 ‣ V Results ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
By comparing the Track 1 winning methods over the last four years, we see that state-of-the-art performance each year has steadily improved, except for the last two years.
However, when comparing Track 2 submissions, the performance has improved significantly even in the last year due to the use of public self-supervised pretrained models.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2408.14886/assets/x1.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="329" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Top winner’s performance on VoxSRC 2019 test set. We report the 2nd place’s performance for VoxSRC 2020 and 2022, since they are better than the winner on VoxSRC 2019 test set. All other entries are from the 1st place.</figcaption>
</figure>
<div id="S5.SS1.p7" class="ltx_para">
<p id="S5.SS1.p7.1" class="ltx_p">Figure <a href="#S5.F3" title="Figure 3 ‣ V-A Speaker verification – Track 1 and 2 ‣ V Results ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows DET curves from the winners’ performance on the VoxSRC 2019 test set. The closer the curve is to the origin, the better the performance. Note the VoxSRC2019 winners perform worst on Track 1 and Track 2. The VoxSRC2021 winner has the best performance on Track 1, while the VoxSRC2023 winner is the best on Track 2.</p>
</div>
<div id="S5.SS1.p8" class="ltx_para">
<p id="S5.SS1.p8.1" class="ltx_p">We also computed the 95% confidence intervals for the winning submissions on the VoxSRC 2019 test set. Figure <a href="#S5.F4" title="Figure 4 ‣ V-A Speaker verification – Track 1 and 2 ‣ V Results ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the results.
To obtain these intervals, we adopted the conventional sample bootstrap approach, as described in Section 3.1.1 of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>: we generated 1000 samples with replacement for each submission and computed 1000 EER values.
Then, we calculated the quantiles corresponding to the 95% confidence interval.
We decided to use the conventional bootstrap method instead of the user-specific bootstrap subset method for two reasons : (i) we want to preserve the label distribution during sampling and (ii) it aligns with our workshop setting – we accept only one submission for the entire test set, not per enrolled speaker.
The minimum, average, and maximum widths of the confidence intervals are 18.6%, 23.6%, and 31.2% of the EER value, respectively.
The confidence intervals demonstrate the variability in performance across different submissions, highlighting the importance of considering uncertainty in EER measurements.
This is particularly relevant given the close performance between first and second place submissions in both tracks.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2408.14886/assets/x2.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="205" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>DET curves of the top winners each year on the VoxSRC 2019 test set. The circles are the points where the DCF value is minimum.</figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2408.14886/assets/x3.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="276" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Performance confidence intervals (95%) for the first and second places in each year of the VoxSRC2019 test set. The top figure shows the Track 1 submissions and the bottom figure shows the Track 2 submissions.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Self-supervised speaker verification (2020-2021)</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The self-supervised speaker verification track was held for two editions, VoxSRC 2020 and 2021.
The winners of this track employed a similar set of stages in both years: they (i) first trained the network using contrastive learning, (ii) then generated pseudo-labels based on the model from the first stage, and (iii) finally trained the network in a supervised way using these pseudo-labels. Table <a href="#S5.T4" title="TABLE IV ‣ V-B Self-supervised speaker verification (2020-2021) ‣ V Results ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> displays the overall results.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Comparison of methods on test sets of self-supervised track. We compare top-2 submissions of the year 2020 and 2021 on the 2019 test set. For % EER shown, lower is better.</figcaption>
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(0.999770477624664,0.999770477624664) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Team</span></th>
<th id="S5.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T4.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Track</span></th>
<th id="S5.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.1.1.1.1.3.1" class="ltx_text ltx_font_bold">2019 test</span></th>
<th id="S5.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.1.1.1.1.4.1" class="ltx_text ltx_font_bold">2020 test</span></th>
<th id="S5.T4.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.1.1.1.1.5.1" class="ltx_text ltx_font_bold">2021 test</span></th>
</tr>
<tr id="S5.T4.1.1.2.2" class="ltx_tr">
<th id="S5.T4.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>
</th>
<th id="S5.T4.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">-</th>
<th id="S5.T4.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">12.67</th>
<th id="S5.T4.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">29.93</th>
<th id="S5.T4.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">19.74</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.3.1" class="ltx_tr">
<th id="S5.T4.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VoxSRC 2020 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</th>
<th id="S5.T4.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">3</th>
<td id="S5.T4.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">2.26</td>
<td id="S5.T4.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.1.1.3.1.4.1" class="ltx_text ltx_font_bold">7.21</span></td>
<td id="S5.T4.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T4.1.1.4.2" class="ltx_tr">
<th id="S5.T4.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VoxSRC 2020 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>
</th>
<th id="S5.T4.1.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">3</th>
<td id="S5.T4.1.1.4.2.3" class="ltx_td ltx_align_center">6.49</td>
<td id="S5.T4.1.1.4.2.4" class="ltx_td ltx_align_center">12.42</td>
<td id="S5.T4.1.1.4.2.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T4.1.1.5.3" class="ltx_tr">
<th id="S5.T4.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VoxSRC 2021 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>
</th>
<th id="S5.T4.1.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">3</th>
<td id="S5.T4.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.1.1.5.3.3.1" class="ltx_text ltx_font_bold">1.49</span></td>
<td id="S5.T4.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T4.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.1.1.5.3.5.1" class="ltx_text ltx_font_bold">5.59</span></td>
</tr>
<tr id="S5.T4.1.1.6.4" class="ltx_tr">
<th id="S5.T4.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">VoxSRC 2021 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>
</th>
<th id="S5.T4.1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">3</th>
<td id="S5.T4.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_bb">2.40</td>
<td id="S5.T4.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S5.T4.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_bb">6.59</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The VoxSRC 2020 winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> exploited Momentum Contrast (MoCo) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> for the first stage, followed by iterative clustering using both efficient mini-batch k-means and Agglomerative Hierarchical Clustering (AHC) to make pseudo-speaker labels.
A large ECAPA-TDNN was then trained with these labels using a sub-center AAM-softmax <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> layer.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">The VoxSRC 2021 winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> extended their previous two-stage iterative labelling framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, which came second in 2020.
They also leveraged visual data on top of the audio data for the first time in the VoxSRC challenges. <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>This is permitted for the self-supervised track.</span></span></span>
They devised a unique clustering ensemble technique to fuse pseudo-labels from various modalities, which enhances the robustness of the speaker representations extracted.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Since we used the same test set across Tracks 1, 2 and 3, the self-supervised test set also includes the VoxSRC 2019 test set in both years 2020 and 2021.
Therefore, we can show the performance progress of the winners’ method each year by measuring the performance on the VoxSRC 2019 test set.
Table <a href="#S5.T4" title="TABLE IV ‣ V-B Self-supervised speaker verification (2020-2021) ‣ V Results ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> shows the result.
Compared to 2020, both the first and second place in VoxSRC 2021 show lower EER on VoxSRC 2019 test set, 2.26% to 1.49% and 6.49% to 2.40% respectively.
The winner of VoxSRC 2021 demonstrated a performance (EER 1.49%) remarkably similar to the VoxSRC 2019 supervised track winner (EER 1.42% on Track 1).
This highlights the significant advancement of self-supervised methods in the field.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">Note, we only hosted this track until 2021 for two reasons:
first, because most participants used similar techniques, adapting self-supervised methods from the computer vision literature; and second, because the scenario was
somewhat artificial (assuming that there were no labelled data) and we wished to move to the more practical scenario of semi-supervised domain adaptation, described next.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Semi-supervised domain adaptation (2022-2023)</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The semi-supervised domain adaptation track is a new track introduced in VoxSRC 2022.
Here, we describe the winners’ submissions in the years 2022 and 2023.
Each year’s winner performance is reported in Table <a href="#S5.T5" title="TABLE V ‣ V-C Semi-supervised domain adaptation (2022-2023) ‣ V Results ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.
Baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> is the same baseline we used for Tracks 1 and 2, which is only trained with VoxCeleb2 dev set.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Comparison of methods on test sets of semi-supervised track. We compare top-2 submissions of the year 2022 and 2023. For % EER shown, lower is better.</figcaption>
<div id="S5.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:138.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(47.6pt,-15.2pt) scale(1.28164645521231,1.28164645521231) ;">
<table id="S5.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.1.1.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Team</span></th>
<th id="S5.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T5.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Track</span></th>
<th id="S5.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T5.1.1.1.1.3.1" class="ltx_text ltx_font_bold">2022 test</span></th>
<th id="S5.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T5.1.1.1.1.4.1" class="ltx_text ltx_font_bold">2023 test</span></th>
</tr>
<tr id="S5.T5.1.1.2.2" class="ltx_tr">
<th id="S5.T5.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</th>
<th id="S5.T5.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">-</th>
<th id="S5.T5.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">16.88</th>
<th id="S5.T5.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">13.11</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.1.1.3.1" class="ltx_tr">
<td id="S5.T5.1.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2022 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
<td id="S5.T5.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S5.T5.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.1.1.3.1.3.1" class="ltx_text ltx_font_bold">7.03</span></td>
<td id="S5.T5.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T5.1.1.4.2" class="ltx_tr">
<td id="S5.T5.1.1.4.2.1" class="ltx_td ltx_align_left">VoxSRC 2022 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>
</td>
<td id="S5.T5.1.1.4.2.2" class="ltx_td ltx_align_center">3</td>
<td id="S5.T5.1.1.4.2.3" class="ltx_td ltx_align_center">7.14</td>
<td id="S5.T5.1.1.4.2.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T5.1.1.5.3" class="ltx_tr">
<td id="S5.T5.1.1.5.3.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2023 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>
</td>
<td id="S5.T5.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S5.T5.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T5.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.1.1.5.3.4.1" class="ltx_text ltx_font_bold">4.95</span></td>
</tr>
<tr id="S5.T5.1.1.6.4" class="ltx_tr">
<td id="S5.T5.1.1.6.4.1" class="ltx_td ltx_align_left ltx_border_bb">VoxSRC 2023 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>
</td>
<td id="S5.T5.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_bb">3</td>
<td id="S5.T5.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S5.T5.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_bb">8.13</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">The winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> in VoxSRC 2022 used two frameworks, pseudo labelling and self-supervised learning.
A novel sub-graph clustering algorithm was used to generate pseudo-labels based on two Gaussian fitting and multi-model voting.
The model was trained in two stages, first using the labelled source domain data and the pseudo-labelled target domain data, and secondly fine-tuning the CNCeleb data by fixing the VoxCeleb weights of the classification layer using circle loss.
The pseudo-label correction method was then applied and the model was retrained with these new pseudo-labels.
They also explored various types of domain adaptation techniques, such as CORAL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> or CORAL+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">The VoxSRC 2023 winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> introduced a novel pseudo-labelling method based on triple thresholds.
They utilised the well-trained speaker model using the source domain to extract embeddings from the target domain.
They conducted initial clustering using the K-Nearest Neighbours (KNN) algorithm, followed by data cleaning, sub-centre purification and class merging to obtain the pseudo labels.
At the last stage, they finetuned the model using both unlabelled data with pseudo-labels and labelled data.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.5.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.6.2" class="ltx_text ltx_font_italic">Speaker diarisation – Track 4</span>
</h3>

<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>An overview of the winning methods on Track 4 across VoxSRC 2020 to 2023.</figcaption>
<div id="S5.T6.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:85.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-384.6pt,76.0pt) scale(0.36051162048056,0.36051162048056) ;">
<table id="S5.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T6.1.1.1.1" class="ltx_tr">
<th id="S5.T6.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T6.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Workshop</span></th>
<th id="S5.T6.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T6.1.1.1.1.2.1" class="ltx_text ltx_font_bold">VAD</span></th>
<th id="S5.T6.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Embedding network architecture</th>
<th id="S5.T6.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Clustering</th>
<th id="S5.T6.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Fusion method</th>
<th id="S5.T6.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Other models used</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T6.1.1.2.1" class="ltx_tr">
<td id="S5.T6.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">
<table id="S5.T6.1.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.1.2.1.1.1.1" class="ltx_tr">
<td id="S5.T6.1.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T6.1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></td>
</tr>
<tr id="S5.T6.1.1.2.1.1.1.2" class="ltx_tr">
<td id="S5.T6.1.1.2.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T6.1.1.2.1.1.1.2.1.1" class="ltx_text ltx_font_bold">2020</span></td>
</tr>
</table>
</td>
<td id="S5.T6.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> based Speech Separation</td>
<td id="S5.T6.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">Res2Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> variants</td>
<td id="S5.T6.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">- AHC</td>
<td id="S5.T6.1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">Dover-LAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
<td id="S5.T6.1.1.2.1.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">- leakage filtering</td>
</tr>
<tr id="S5.T6.1.1.3.2" class="ltx_tr">
<td id="S5.T6.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t">
<table id="S5.T6.1.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.1.3.2.1.1.1" class="ltx_tr">
<td id="S5.T6.1.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T6.1.1.3.2.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></td>
</tr>
<tr id="S5.T6.1.1.3.2.1.1.2" class="ltx_tr">
<td id="S5.T6.1.1.3.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T6.1.1.3.2.1.1.2.1.1" class="ltx_text ltx_font_bold">2021</span></td>
</tr>
</table>
</td>
<td id="S5.T6.1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t">ResNet + BiLSTM</td>
<td id="S5.T6.1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_t">ResNet34 variant</td>
<td id="S5.T6.1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T6.1.1.3.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.1.3.2.4.1.1" class="ltx_tr">
<td id="S5.T6.1.1.3.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- AHC</td>
</tr>
<tr id="S5.T6.1.1.3.2.4.1.2" class="ltx_tr">
<td id="S5.T6.1.1.3.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Spectral Clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T6.1.1.3.2.5" class="ltx_td ltx_align_left ltx_border_t">Dover-LAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
<td id="S5.T6.1.1.3.2.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<table id="S5.T6.1.1.3.2.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.1.3.2.6.1.1" class="ltx_tr">
<td id="S5.T6.1.1.3.2.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- TS-VAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
</tr>
<tr id="S5.T6.1.1.3.2.6.1.2" class="ltx_tr">
<td id="S5.T6.1.1.3.2.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Overlap speech detection</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T6.1.1.4.3" class="ltx_tr">
<td id="S5.T6.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_t">
<table id="S5.T6.1.1.4.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.1.4.3.1.1.1" class="ltx_tr">
<td id="S5.T6.1.1.4.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T6.1.1.4.3.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></td>
</tr>
<tr id="S5.T6.1.1.4.3.1.1.2" class="ltx_tr">
<td id="S5.T6.1.1.4.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T6.1.1.4.3.1.1.2.1.1" class="ltx_text ltx_font_bold">2022</span></td>
</tr>
</table>
</td>
<td id="S5.T6.1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T6.1.1.4.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.1.4.3.2.1.1" class="ltx_tr">
<td id="S5.T6.1.1.4.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- ResNet based VAD</td>
</tr>
<tr id="S5.T6.1.1.4.3.2.1.2" class="ltx_tr">
<td id="S5.T6.1.1.4.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> based VAD</td>
</tr>
<tr id="S5.T6.1.1.4.3.2.1.3" class="ltx_tr">
<td id="S5.T6.1.1.4.3.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">- pyannote.audio 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</td>
</tr>
<tr id="S5.T6.1.1.4.3.2.1.4" class="ltx_tr">
<td id="S5.T6.1.1.4.3.2.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">- ASR based VAD</td>
</tr>
</table>
</td>
<td id="S5.T6.1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_t">SimAM-ResNet34 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>
</td>
<td id="S5.T6.1.1.4.3.4" class="ltx_td ltx_align_left ltx_border_t">
<table id="S5.T6.1.1.4.3.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.1.4.3.4.1.1" class="ltx_tr">
<td id="S5.T6.1.1.4.3.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- AHC</td>
</tr>
<tr id="S5.T6.1.1.4.3.4.1.2" class="ltx_tr">
<td id="S5.T6.1.1.4.3.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Spectral Clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T6.1.1.4.3.5" class="ltx_td ltx_align_left ltx_border_t">Dover-LAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
<td id="S5.T6.1.1.4.3.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">- TS-VAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
</tr>
<tr id="S5.T6.1.1.5.4" class="ltx_tr">
<td id="S5.T6.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<table id="S5.T6.1.1.5.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.1.5.4.1.1.1" class="ltx_tr">
<td id="S5.T6.1.1.5.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T6.1.1.5.4.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></td>
</tr>
<tr id="S5.T6.1.1.5.4.1.1.2" class="ltx_tr">
<td id="S5.T6.1.1.5.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T6.1.1.5.4.1.1.2.1.1" class="ltx_text ltx_font_bold">2023</span></td>
</tr>
</table>
</td>
<td id="S5.T6.1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<table id="S5.T6.1.1.5.4.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.1.5.4.2.1.1" class="ltx_tr">
<td id="S5.T6.1.1.5.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">- ResNet based VAD</td>
</tr>
<tr id="S5.T6.1.1.5.4.2.1.2" class="ltx_tr">
<td id="S5.T6.1.1.5.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">- Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> based VAD</td>
</tr>
<tr id="S5.T6.1.1.5.4.2.1.3" class="ltx_tr">
<td id="S5.T6.1.1.5.4.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">- ECAPA-TDNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> based VAD</td>
</tr>
</table>
</td>
<td id="S5.T6.1.1.5.4.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">SimAM-ResNet34 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> + ResNet101 + SimAM-ResNet100</td>
<td id="S5.T6.1.1.5.4.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">- AHC</td>
<td id="S5.T6.1.1.5.4.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Dover-LAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
<td id="S5.T6.1.1.5.4.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t">- Seq2Seq-TSVAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S5.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VII: </span>Comparison of methods on test sets of the diarisation track. We compare top-2 submissions of the year 2021, 2022, and 2023. For % DER shown, lower is better.</figcaption>
<div id="S5.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:178.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.7pt,0.7pt) scale(0.992208478539553,0.992208478539553) ;">
<table id="S5.T7.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T7.1.1.1.1" class="ltx_tr">
<th id="S5.T7.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T7.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Team</span></th>
<th id="S5.T7.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T7.1.1.1.1.2.1" class="ltx_text ltx_font_bold">2021 test</span></th>
<th id="S5.T7.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T7.1.1.1.1.3.1" class="ltx_text ltx_font_bold">2021 test</span></th>
<th id="S5.T7.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T7.1.1.1.1.4.1" class="ltx_text ltx_font_bold">2022 test</span></th>
<th id="S5.T7.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T7.1.1.1.1.5.1" class="ltx_text ltx_font_bold">2023 test</span></th>
</tr>
<tr id="S5.T7.1.1.2.2" class="ltx_tr">
<th id="S5.T7.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</th>
<th id="S5.T7.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">8.15</th>
<th id="S5.T7.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">7.57</th>
<th id="S5.T7.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">8.94</th>
<th id="S5.T7.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">8.54</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T7.1.1.3.1" class="ltx_tr">
<td id="S5.T7.1.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2020 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>
</td>
<td id="S5.T7.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T7.1.1.3.1.2.1" class="ltx_text ltx_font_bold">3.71</span></td>
<td id="S5.T7.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T7.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T7.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T7.1.1.4.2" class="ltx_tr">
<td id="S5.T7.1.1.4.2.1" class="ltx_td ltx_align_left">VoxSRC 2020 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>
</td>
<td id="S5.T7.1.1.4.2.2" class="ltx_td ltx_align_center">4.00</td>
<td id="S5.T7.1.1.4.2.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T7.1.1.4.2.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T7.1.1.4.2.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T7.1.1.5.3" class="ltx_tr">
<td id="S5.T7.1.1.5.3.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2021 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>
</td>
<td id="S5.T7.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T7.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_t">5.07</td>
<td id="S5.T7.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T7.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T7.1.1.6.4" class="ltx_tr">
<td id="S5.T7.1.1.6.4.1" class="ltx_td ltx_align_left">VoxSRC 2021 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>
</td>
<td id="S5.T7.1.1.6.4.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T7.1.1.6.4.3" class="ltx_td ltx_align_center">5.15</td>
<td id="S5.T7.1.1.6.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T7.1.1.6.4.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T7.1.1.7.5" class="ltx_tr">
<td id="S5.T7.1.1.7.5.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2022 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>
</td>
<td id="S5.T7.1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T7.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_t">4.16</td>
<td id="S5.T7.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_t">4.75</td>
<td id="S5.T7.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T7.1.1.8.6" class="ltx_tr">
<td id="S5.T7.1.1.8.6.1" class="ltx_td ltx_align_left">VoxSRC 2022 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
<td id="S5.T7.1.1.8.6.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T7.1.1.8.6.3" class="ltx_td ltx_align_center">4.05</td>
<td id="S5.T7.1.1.8.6.4" class="ltx_td ltx_align_center">4.87</td>
<td id="S5.T7.1.1.8.6.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T7.1.1.9.7" class="ltx_tr">
<td id="S5.T7.1.1.9.7.1" class="ltx_td ltx_align_left ltx_border_t">VoxSRC 2023 1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>
</td>
<td id="S5.T7.1.1.9.7.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T7.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T7.1.1.9.7.3.1" class="ltx_text ltx_font_bold">3.74</span></td>
<td id="S5.T7.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T7.1.1.9.7.4.1" class="ltx_text ltx_font_bold">3.94</span></td>
<td id="S5.T7.1.1.9.7.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T7.1.1.9.7.5.1" class="ltx_text ltx_font_bold">4.30</span></td>
</tr>
<tr id="S5.T7.1.1.10.8" class="ltx_tr">
<td id="S5.T7.1.1.10.8.1" class="ltx_td ltx_align_left ltx_border_bb">VoxSRC 2023 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>
</td>
<td id="S5.T7.1.1.10.8.2" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S5.T7.1.1.10.8.3" class="ltx_td ltx_align_center ltx_border_bb">4.06</td>
<td id="S5.T7.1.1.10.8.4" class="ltx_td ltx_align_center ltx_border_bb">4.23</td>
<td id="S5.T7.1.1.10.8.5" class="ltx_td ltx_align_center ltx_border_bb">4.71</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">The speaker diarisation track has been held since 2020.
Table <a href="#S5.T6" title="TABLE VI ‣ V-D Speaker diarisation – Track 4 ‣ V Results ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> compares the winning methods on Track 4.
The winning methods in this track all consist of the following steps: Voice Activity Detection (VAD) to detect the voice regions, speaker embedding extraction by using a sliding window approach over the voice region, and a clustering step to determine the speaker labels.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">Winners used speech separation-based VAD or train their own VAD using ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>, or ECAPA-TDNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> architecture.
External public VAD models, such as the segmentation model from pyannote <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> or word-level timestamps from Kaldi  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> ASR systems were also explored.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">Several embedding networks have been used such as variants of Res2Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> or SimAM-ResNet34 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> trained with VoxCeleb.
VoxSRC 2022 winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> used pseudo-labels from VoxConverse utterances to mitigate the effect of domain mismatch between VoxCeleb and VoxConverse and VoxSRC 2023 winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> included additional data during training.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p">All winners used AHC to cluster the embeddings extracted using the sliding window approach.
Some winners <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> also adopted spectral clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
After these three steps, all winning methods adopted Dover-LAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> to fuse the results from different models.</p>
</div>
<div id="S5.SS4.p5" class="ltx_para">
<p id="S5.SS4.p5.1" class="ltx_p">Multiple methods were used to deal with overlapping speech.
Winners either trained a separate Overlap Speech Detection (OSD) model on their own or used TS-VAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> to detect the overlapping speech regions and integrated the results into their final submission.</p>
</div>
<div id="S5.SS4.p6" class="ltx_para ltx_noindent">
<p id="S5.SS4.p6.1" class="ltx_p"><span id="S5.SS4.p6.1.1" class="ltx_text ltx_font_bold">Performance progression.</span> Table <a href="#S5.T7" title="TABLE VII ‣ V-D Speaker diarisation – Track 4 ‣ V Results ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> shows the winners’ performance on Track 4 test sets.
Since the first year’s diarisation test set has not been included in the next three years, we cannot directly compare the VoxSRC 2020 winner with the winners in 2021, 2022 and 2023.
However, we included the VoxSRC 2021 test set in the VoxSRC 2022 and 2023 test set, and thus, we compare the performances between the last three years.
We could observe a steady performance improvement each year (DER of 5.07% vs 4.05% vs 3.74%).</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Detailed analysis</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we further analyse winners’ submissions.
We add analysis on (i) the verification pairs that winners get wrong, (ii) multi-lingual pairs in the year 2021, (iii) hard positive and negative pairs in the year 2022, and (iv) semi-supervised domain adaptation submissions with several baseline results in the year 2022.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.5.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.6.2" class="ltx_text ltx_font_italic">Verification pairs that winners get wrong</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Over the previous five years, the state-of-the-art performance on the VoxSRC 2019 test set has progressively improved, as shown in Figure <a href="#S5.F2" title="Figure 2 ‣ V-A Speaker verification – Track 1 and 2 ‣ V Results ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Models however still predict differently from the ground truth in some trials.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">Of the 232 pairs that all the winners of VoxSRC 2022 and 2023 (first and second places on both closed / open tracks) get wrong,
16 are positive pairs and the rest are negative pairs.
We manually verify whether these pairs are correctly labelled or not.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">For the positive pairs, 12 out of 16 are label errors.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We fixed these errors and released the new test labels in our official website <a target="_blank" href="https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/workshop.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://mm.kaist.ac.kr/datasets/voxceleb/voxsrc/workshop.html</a>.</span></span></span>
Four of these errors are the result of utterances from different individuals with the same name, a confusion caused by the YouTube scraping process based on video titles.
The remaining eight pairs are utterances from siblings who are mistakenly assigned the same identity due to their inclusion in YouTube videos titled with the name of a well-known celebrity.
These were not caught during the manual verification stage.
Nevertheless, the proportion of erroneous labels is minimal and does not significantly impact the system’s performance, accounting for only 0.006% of the total 208,008 pairs.
The rest of the four pairs without label errors are from the same person but extracted at different ages, which is a challenging setting that we focused on in VoxSRC 2022, and is still an open area of research.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">In the set of 216 negative pairs, no labelling errors are observed.
This can be attributed to our method of random sampling from the speaker pools in the raw data, which significantly reduces the likelihood of labelling errors.
All these pairs consist of different individuals of the same gender, and we do not notice any other significant trends.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.5.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.6.2" class="ltx_text ltx_font_italic">Analysis of performance by utterance length and gender</span>
</h3>

<figure id="S6.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VIII: </span>Analysis of performance by utterance duration. <span id="S6.T8.2.1" class="ltx_text ltx_font_bold">&gt;x sec </span> denotes the subset of the VoxSRC2019 test pairs where both utterances are longer than x seconds. We report the performance of the VoxSRC2023 Track1 and Track2 winners.</figcaption>
<div id="S6.T8.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:76.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-39.7pt,7.0pt) scale(0.845358898815332,0.845358898815332) ;">
<table id="S6.T8.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T8.3.1.1.1" class="ltx_tr">
<th id="S6.T8.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S6.T8.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Eval set</span></th>
<td id="S6.T8.3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S6.T8.3.1.1.1.2.1" class="ltx_text ltx_font_bold">All</span></td>
<td id="S6.T8.3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S6.T8.3.1.1.1.3.1" class="ltx_text ltx_font_bold">&gt;4 sec</span></td>
<td id="S6.T8.3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S6.T8.3.1.1.1.4.1" class="ltx_text ltx_font_bold">&gt;6 sec</span></td>
<td id="S6.T8.3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S6.T8.3.1.1.1.5.1" class="ltx_text ltx_font_bold">&gt;8 sec</span></td>
</tr>
<tr id="S6.T8.3.1.2.2" class="ltx_tr">
<th id="S6.T8.3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S6.T8.3.1.2.2.1.1" class="ltx_text ltx_font_bold"># pairs</span></th>
<td id="S6.T8.3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2">208,008</td>
<td id="S6.T8.3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2">199,680</td>
<td id="S6.T8.3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2">51,621</td>
<td id="S6.T8.3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" colspan="2">16,576</td>
</tr>
<tr id="S6.T8.3.1.3.3" class="ltx_tr">
<th id="S6.T8.3.1.3.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S6.T8.3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.3.1.3.3.2.1" class="ltx_text ltx_font_bold">EER</span></td>
<td id="S6.T8.3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.3.1.3.3.3.1" class="ltx_text ltx_font_bold">minDCF</span></td>
<td id="S6.T8.3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.3.1.3.3.4.1" class="ltx_text ltx_font_bold">EER</span></td>
<td id="S6.T8.3.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.3.1.3.3.5.1" class="ltx_text ltx_font_bold">minDCF</span></td>
<td id="S6.T8.3.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.3.1.3.3.6.1" class="ltx_text ltx_font_bold">EER</span></td>
<td id="S6.T8.3.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.3.1.3.3.7.1" class="ltx_text ltx_font_bold">minDCF</span></td>
<td id="S6.T8.3.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T8.3.1.3.3.8.1" class="ltx_text ltx_font_bold">EER</span></td>
<td id="S6.T8.3.1.3.3.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S6.T8.3.1.3.3.9.1" class="ltx_text ltx_font_bold">minDCF</span></td>
</tr>
<tr id="S6.T8.3.1.4.4" class="ltx_tr">
<th id="S6.T8.3.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S6.T8.3.1.4.4.1.1" class="ltx_text ltx_font_bold">Track 1 winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite></span></th>
<td id="S6.T8.3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">0.58</td>
<td id="S6.T8.3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">0.028</td>
<td id="S6.T8.3.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">0.58</td>
<td id="S6.T8.3.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">0.027</td>
<td id="S6.T8.3.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">0.37</td>
<td id="S6.T8.3.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">0.019</td>
<td id="S6.T8.3.1.4.4.8" class="ltx_td ltx_align_center ltx_border_t">0.11</td>
<td id="S6.T8.3.1.4.4.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.008</td>
</tr>
<tr id="S6.T8.3.1.5.5" class="ltx_tr">
<th id="S6.T8.3.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S6.T8.3.1.5.5.1.1" class="ltx_text ltx_font_bold">Track 2 winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite></span></th>
<td id="S6.T8.3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_bb">0.47</td>
<td id="S6.T8.3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_bb">0.020</td>
<td id="S6.T8.3.1.5.5.4" class="ltx_td ltx_align_center ltx_border_bb">0.45</td>
<td id="S6.T8.3.1.5.5.5" class="ltx_td ltx_align_center ltx_border_bb">0.019</td>
<td id="S6.T8.3.1.5.5.6" class="ltx_td ltx_align_center ltx_border_bb">0.26</td>
<td id="S6.T8.3.1.5.5.7" class="ltx_td ltx_align_center ltx_border_bb">0.014</td>
<td id="S6.T8.3.1.5.5.8" class="ltx_td ltx_align_center ltx_border_bb">0.03</td>
<td id="S6.T8.3.1.5.5.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">0.006</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In this section we analyse the performance on the VoxSRC2019 test set of the Track 1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> and Track 2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> VoxSRC2023 winners against utterance length and gender.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">Table <a href="#S6.T8" title="TABLE VIII ‣ VI-B Analysis of performance by utterance length and gender ‣ VI Detailed analysis ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VIII</span></a> shows the performance as the utterance length is varied, where
<span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_bold"> &gt;x sec </span> denotes the subset of VoxSRC2019 test pairs where both utterances of each pair are longer than x seconds.
Both models show better performance as the utterance length increases. This is
because there is a greater chance that relevant speech signals from the actual speaker are captured, as has been studied in a number of works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>.
Interestingly, the Track 2 winner shows an impressive 0.03% on EER on the <span id="S6.SS2.p2.1.2" class="ltx_text ltx_font_bold"> &gt;8 sec </span> set –
only five out of 16,576 pairs are predicted incorrectly at the EER point when the false positive rate and false alarm rate are the same.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">We provide an analysis of the performance as a function of gender in Table <a href="#S6.T9" title="TABLE IX ‣ VI-B Analysis of performance by utterance length and gender ‣ VI Detailed analysis ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IX</span></a>. <span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_bold">Male</span> and <span id="S6.SS2.p3.1.2" class="ltx_text ltx_font_bold">Female</span> denote the subsets of the VoxSRC2019 test pairs where both utterances are from male and female, respectively.
For both models, performance is better on the male set than the female set.
We assume that this is due to the gender imbalance of the VoxCeleb2 dev set, which is used as our training set : it consists of 61% of male and 39% of female <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. However, since both models already show strong performance, the performance gap between gender is not huge. (0.12% and 0.16% for 1st and 2nd place, respectively)</p>
</div>
<figure id="S6.T9" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IX: </span>Analysis of performance by gender. <span id="S6.T9.3.1" class="ltx_text ltx_font_bold">Male</span> and <span id="S6.T9.4.2" class="ltx_text ltx_font_bold">Female</span> denote the subsets of the VoxSRC2019 test pairs where both utterances are from male and female, respectively. We report the performance of the VoxSRC2023 Track1 and Track2 winners.</figcaption>
<div id="S6.T9.5" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:90.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(1.9pt,-0.4pt) scale(1.0086203497212,1.0086203497212) ;">
<table id="S6.T9.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T9.5.1.1.1" class="ltx_tr">
<th id="S6.T9.5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S6.T9.5.1.1.1.1.1" class="ltx_text ltx_font_bold">Eval set</span></th>
<td id="S6.T9.5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S6.T9.5.1.1.1.2.1" class="ltx_text ltx_font_bold">All</span></td>
<td id="S6.T9.5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S6.T9.5.1.1.1.3.1" class="ltx_text ltx_font_bold">Male</span></td>
<td id="S6.T9.5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S6.T9.5.1.1.1.4.1" class="ltx_text ltx_font_bold">Female</span></td>
</tr>
<tr id="S6.T9.5.1.2.2" class="ltx_tr">
<th id="S6.T9.5.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S6.T9.5.1.2.2.1.1" class="ltx_text ltx_font_bold"># pairs</span></th>
<td id="S6.T9.5.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2">208,008</td>
<td id="S6.T9.5.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2">85,738</td>
<td id="S6.T9.5.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2">78,750</td>
</tr>
<tr id="S6.T9.5.1.3.3" class="ltx_tr">
<th id="S6.T9.5.1.3.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S6.T9.5.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T9.5.1.3.3.2.1" class="ltx_text ltx_font_bold">EER</span></td>
<td id="S6.T9.5.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T9.5.1.3.3.3.1" class="ltx_text ltx_font_bold">minDCF</span></td>
<td id="S6.T9.5.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T9.5.1.3.3.4.1" class="ltx_text ltx_font_bold">EER</span></td>
<td id="S6.T9.5.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T9.5.1.3.3.5.1" class="ltx_text ltx_font_bold">minDCF</span></td>
<td id="S6.T9.5.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T9.5.1.3.3.6.1" class="ltx_text ltx_font_bold">EER</span></td>
<td id="S6.T9.5.1.3.3.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S6.T9.5.1.3.3.7.1" class="ltx_text ltx_font_bold">minDCF</span></td>
</tr>
<tr id="S6.T9.5.1.4.4" class="ltx_tr">
<th id="S6.T9.5.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S6.T9.5.1.4.4.1.1" class="ltx_text ltx_font_bold">Track 1 winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite></span></th>
<td id="S6.T9.5.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">0.58</td>
<td id="S6.T9.5.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">0.028</td>
<td id="S6.T9.5.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">0.57</td>
<td id="S6.T9.5.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">0.026</td>
<td id="S6.T9.5.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">0.69</td>
<td id="S6.T9.5.1.4.4.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.034</td>
</tr>
<tr id="S6.T9.5.1.5.5" class="ltx_tr">
<th id="S6.T9.5.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S6.T9.5.1.5.5.1.1" class="ltx_text ltx_font_bold">Track 2 winner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite></span></th>
<td id="S6.T9.5.1.5.5.2" class="ltx_td ltx_align_center ltx_border_bb">0.47</td>
<td id="S6.T9.5.1.5.5.3" class="ltx_td ltx_align_center ltx_border_bb">0.020</td>
<td id="S6.T9.5.1.5.5.4" class="ltx_td ltx_align_center ltx_border_bb">0.43</td>
<td id="S6.T9.5.1.5.5.5" class="ltx_td ltx_align_center ltx_border_bb">0.018</td>
<td id="S6.T9.5.1.5.5.6" class="ltx_td ltx_align_center ltx_border_bb">0.59</td>
<td id="S6.T9.5.1.5.5.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">0.025</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS3.5.1.1" class="ltx_text">VI-C</span> </span><span id="S6.SS3.6.2" class="ltx_text ltx_font_italic">Multi-lingual focus – VoxSRC 2021</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">The verification tracks in 2021 had a multi-lingual focus, via the inclusion of multi-lingual data in the test set.
We provide some analysis on the performance of the winning methods from the supervised tracks (1st and 2nd places on Track 1) and the provided baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> on the multi-lingual data.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">We measure performance on the identical language subsets for the five most common languages in VoxCeleb.
The results are shown in Figure <a href="#S6.F5" title="Figure 5 ‣ VI-C Multi-lingual focus – VoxSRC 2021 ‣ VI Detailed analysis ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
We calculate a performance measure per language by calculating the EER from the same-language pairs.
For all five languages, there are at least 1000 same-language pairs (both positive and negative) in the test set.
The exact number of same-language pairs per language is: English: 282,039; French: 5,987; German: 1,066; Italian: 1,129; and Spanish: 4,241.
The winning methods perform much better than the baseline across all languages, although there is still considerable variation in performance between different languages.
Interestingly, although English is the most common language in the training set, none of the models performs best on the English language pairs.
However, we note that there is more statistical uncertainty for the languages with a lower number of pairs, <span id="S6.SS3.p2.1.1" class="ltx_text ltx_font_italic">e.g. </span>German with 1,066 pairs, compared to those with many pairs, <span id="S6.SS3.p2.1.2" class="ltx_text ltx_font_italic">e.g. </span>English with 282,039 pairs.
This is due to the error introduced when estimating population statistics using small sample sizes.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><img src="/html/2408.14886/assets/x4.png" id="S6.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="271" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The performances of different models on the pairs only with certain languages in the VoxSRC 2021 test set.</figcaption>
</figure>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS4.5.1.1" class="ltx_text">VI-D</span> </span><span id="S6.SS4.6.2" class="ltx_text ltx_font_italic">Hard positive and negative pairs – VoxSRC 2022</span>
</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">In the year 2022, we introduced new trial types to make the test set harder – hard positive pairs with large age gaps, and hard negative pairs that share the same background noise.
Here we analyse how these pairs affect the winners’ performance.
The VoxSRC 2022 test set consists of four types of trials, (i) hard positive pairs taken from the same speaker at different ages (<span id="S6.SS4.p1.1.1" class="ltx_text ltx_font_bold">P-Hard</span>), (ii) hard negative pairs taken from the same environment (<span id="S6.SS4.p1.1.2" class="ltx_text ltx_font_bold">N-Hard</span>), (iii) positive pairs from VoxSRC 2019 test set (<span id="S6.SS4.p1.1.3" class="ltx_text ltx_font_bold">P-Vox19</span>), and (iv) negative pairs from VoxSRC 2019 test set (<span id="S6.SS4.p1.1.4" class="ltx_text ltx_font_bold">N-Vox19</span>).
We compare the performance of our baseline model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and the top 2 winners of Track 1 on these subsets.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">Table <a href="#S6.T10" title="TABLE X ‣ VI-D Hard positive and negative pairs – VoxSRC 2022 ‣ VI Detailed analysis ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">X</span></a> shows the results.
The 1st <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> and 2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> performed better than our baseline model by a large margin.
Comparing the performance of E-1 to the others shows that both the hard positives and the hard negatives made the challenge more difficult.
For the most challenging set, E-4 with both hard positive and negative pairs, the 1st place method (which achieves an impressive 0.9 % EER on the VoxSRC-19 test set) could only achieve 2.07%.
Interestingly, the second-place method performed better in E-1, E-2 and E-3 than the 1st place but achieved worse results in E-4.
This led to a slightly better EER performance of the second place in 2022 than the first place on the overall VoxSRC2022 test set (1.40% vs 1.49%).
While both use similar training strategies and identical training set in Track 1, the first place uses ResNet100 with a base channel of 128, while the second place uses the variants of ResNet34 with a base channel of 64. The larger capacity of the first place might have led to a slight overfitting to the training set. However, as shown in Table <a href="#S6.T10" title="TABLE X ‣ VI-D Hard positive and negative pairs – VoxSRC 2022 ‣ VI Detailed analysis ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">X</span></a>, the difference is not significant.</p>
</div>
<figure id="S6.T10" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE X: </span>Performance of baseline model and winning methods in VoxSRC 2022 Track 1 on four subsets of the test set. Reported in EER (%). </figcaption>
<div id="S6.T10.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:64.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-86.6pt,12.8pt) scale(0.714605433730393,0.714605433730393) ;">
<table id="S6.T10.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T10.1.1.1.1" class="ltx_tr">
<th id="S6.T10.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T10.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Eval set</span></th>
<th id="S6.T10.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T10.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Positive pairs</span></th>
<th id="S6.T10.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T10.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Negative pairs</span></th>
<th id="S6.T10.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T10.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></span></th>
<th id="S6.T10.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T10.1.1.1.1.5.1" class="ltx_text ltx_font_bold">1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite></span></th>
<th id="S6.T10.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T10.1.1.1.1.6.1" class="ltx_text ltx_font_bold">2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T10.1.1.2.1" class="ltx_tr">
<td id="S6.T10.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T10.1.1.2.1.1.1" class="ltx_text ltx_font_bold">E-1</span></td>
<td id="S6.T10.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">P-Vox19</td>
<td id="S6.T10.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">N-Vox19</td>
<td id="S6.T10.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">1.47</td>
<td id="S6.T10.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.90</td>
<td id="S6.T10.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T10.1.1.2.1.6.1" class="ltx_text ltx_font_bold">0.65</span></td>
</tr>
<tr id="S6.T10.1.1.3.2" class="ltx_tr">
<td id="S6.T10.1.1.3.2.1" class="ltx_td ltx_align_center"><span id="S6.T10.1.1.3.2.1.1" class="ltx_text ltx_font_bold">E-2</span></td>
<td id="S6.T10.1.1.3.2.2" class="ltx_td ltx_align_center">P-Vox19</td>
<td id="S6.T10.1.1.3.2.3" class="ltx_td ltx_align_center">N-Hard</td>
<td id="S6.T10.1.1.3.2.4" class="ltx_td ltx_align_center">3.25</td>
<td id="S6.T10.1.1.3.2.5" class="ltx_td ltx_align_center">1.35</td>
<td id="S6.T10.1.1.3.2.6" class="ltx_td ltx_align_center"><span id="S6.T10.1.1.3.2.6.1" class="ltx_text ltx_font_bold">1.15</span></td>
</tr>
<tr id="S6.T10.1.1.4.3" class="ltx_tr">
<td id="S6.T10.1.1.4.3.1" class="ltx_td ltx_align_center"><span id="S6.T10.1.1.4.3.1.1" class="ltx_text ltx_font_bold">E-3</span></td>
<td id="S6.T10.1.1.4.3.2" class="ltx_td ltx_align_center">P-Hard</td>
<td id="S6.T10.1.1.4.3.3" class="ltx_td ltx_align_center">N-Vox19</td>
<td id="S6.T10.1.1.4.3.4" class="ltx_td ltx_align_center">4.50</td>
<td id="S6.T10.1.1.4.3.5" class="ltx_td ltx_align_center">1.33</td>
<td id="S6.T10.1.1.4.3.6" class="ltx_td ltx_align_center"><span id="S6.T10.1.1.4.3.6.1" class="ltx_text ltx_font_bold">1.18</span></td>
</tr>
<tr id="S6.T10.1.1.5.4" class="ltx_tr">
<td id="S6.T10.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T10.1.1.5.4.1.1" class="ltx_text ltx_font_bold">E-4</span></td>
<td id="S6.T10.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">P-Hard</td>
<td id="S6.T10.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">N-Hard</td>
<td id="S6.T10.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb">9.27</td>
<td id="S6.T10.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T10.1.1.5.4.5.1" class="ltx_text ltx_font_bold">2.07</span></td>
<td id="S6.T10.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_bb">2.28</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS5.5.1.1" class="ltx_text">VI-E</span> </span><span id="S6.SS5.6.2" class="ltx_text ltx_font_italic">Semi-supervised domain adaptation – VoxSRC 2022</span>
</h3>

<div id="S6.SS5.p1" class="ltx_para">
<p id="S6.SS5.p1.1" class="ltx_p">Table <a href="#S6.T11" title="TABLE XI ‣ VI-E Semi-supervised domain adaptation – VoxSRC 2022 ‣ VI Detailed analysis ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XI</span></a> shows the top two teams’ performance on the test set compared to the baselines.
<span id="S6.SS5.p1.1.1" class="ltx_text ltx_font_bold">Baseline 1</span> is the model which is trained only with the VoxCeleb2 dev set, the labelled data in the <span id="S6.SS5.p1.1.2" class="ltx_text ltx_font_italic">source</span> domain (<span id="S6.SS5.p1.1.3" class="ltx_text ltx_font_bold">L-S</span>).
We also provide <span id="S6.SS5.p1.1.4" class="ltx_text ltx_font_bold">Baseline 2</span>, which is trained only with the labelled data in the <span id="S6.SS5.p1.1.5" class="ltx_text ltx_font_italic">target</span> domain (<span id="S6.SS5.p1.1.6" class="ltx_text ltx_font_bold">L-T</span>) from scratch.
<span id="S6.SS5.p1.1.7" class="ltx_text ltx_font_bold">Baseline 3</span> is trained to start from Baseline 1 and finetuned with labelled data in the target domain using a low learning rate (1e-5).
All baseline models are ResNetSE34V2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> with ASP pooling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> and are trained with a combination of angular prototypical loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and cross-entropy loss.
Neither of these baselines takes advantage of the large amount of unlabelled target domain data available to participants.</p>
</div>
<div id="S6.SS5.p2" class="ltx_para">
<p id="S6.SS5.p2.1" class="ltx_p">A comparison of Baselines 1 and 3 shows that including the labelled data in the target domain results in a performance improvement, 2.95% in terms of EER, even though the size of the labelled target domain data is negligible.
However, Baseline 2 shows that using only the labelled target domain data results in a substantial performance decrease due to overfitting.
Finally, the two winners’ performances show that utilising the extensive unlabelled target domain data is essential for performance improvement in the training set, such as in the form of pseudo-labelling during training.</p>
</div>
<figure id="S6.T11" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE XI: </span>Comparison of winning methods in Track3 with baselines. <span id="S6.T11.4.1" class="ltx_text ltx_font_bold">L-S</span> : Labelled data in Source domain, <span id="S6.T11.5.2" class="ltx_text ltx_font_bold">U-T</span>: Unlabelled data in Target domain and <span id="S6.T11.6.3" class="ltx_text ltx_font_bold">L-T</span> Labelled data in Target domain.</figcaption>
<table id="S6.T11.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T11.7.1.1" class="ltx_tr">
<th id="S6.T11.7.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S6.T11.7.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S6.T11.7.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S6.T11.7.1.1.2.1" class="ltx_text ltx_font_bold">Train dataset</span></th>
<td id="S6.T11.7.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T11.7.1.1.3.1" class="ltx_text ltx_font_bold">minDCF</span></td>
<td id="S6.T11.7.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T11.7.1.1.4.1" class="ltx_text ltx_font_bold">EER</span></td>
</tr>
<tr id="S6.T11.7.2.2" class="ltx_tr">
<th id="S6.T11.7.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Baseline 1</th>
<th id="S6.T11.7.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S6.T11.7.2.2.2.1" class="ltx_text ltx_font_bold">L-S</span></th>
<td id="S6.T11.7.2.2.3" class="ltx_td ltx_align_center ltx_border_t">0.823</td>
<td id="S6.T11.7.2.2.4" class="ltx_td ltx_align_center ltx_border_t">16.88</td>
</tr>
<tr id="S6.T11.7.3.3" class="ltx_tr">
<th id="S6.T11.7.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Baseline 2</th>
<th id="S6.T11.7.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S6.T11.7.3.3.2.1" class="ltx_text ltx_font_bold">L-T</span></th>
<td id="S6.T11.7.3.3.3" class="ltx_td ltx_align_center">0.999</td>
<td id="S6.T11.7.3.3.4" class="ltx_td ltx_align_center">32.47</td>
</tr>
<tr id="S6.T11.7.4.4" class="ltx_tr">
<th id="S6.T11.7.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Baseline 3</th>
<th id="S6.T11.7.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<span id="S6.T11.7.4.4.2.1" class="ltx_text ltx_font_bold">L-S</span> + <span id="S6.T11.7.4.4.2.2" class="ltx_text ltx_font_bold">L-T</span>
</th>
<td id="S6.T11.7.4.4.3" class="ltx_td ltx_align_center">0.687</td>
<td id="S6.T11.7.4.4.4" class="ltx_td ltx_align_center">13.93</td>
</tr>
<tr id="S6.T11.7.5.5" class="ltx_tr">
<th id="S6.T11.7.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">1st place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</th>
<th id="S6.T11.7.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">
<span id="S6.T11.7.5.5.2.1" class="ltx_text ltx_font_bold">L-S</span> + <span id="S6.T11.7.5.5.2.2" class="ltx_text ltx_font_bold">U-T</span> + <span id="S6.T11.7.5.5.2.3" class="ltx_text ltx_font_bold">L-T</span>
</th>
<td id="S6.T11.7.5.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T11.7.5.5.3.1" class="ltx_text ltx_font_bold">0.388</span></td>
<td id="S6.T11.7.5.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T11.7.5.5.4.1" class="ltx_text ltx_font_bold">7.03</span></td>
</tr>
<tr id="S6.T11.7.6.6" class="ltx_tr">
<th id="S6.T11.7.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">2nd place <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>
</th>
<th id="S6.T11.7.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">
<span id="S6.T11.7.6.6.2.1" class="ltx_text ltx_font_bold">L-S</span> + <span id="S6.T11.7.6.6.2.2" class="ltx_text ltx_font_bold">U-T</span> + <span id="S6.T11.7.6.6.2.3" class="ltx_text ltx_font_bold">L-T</span>
</th>
<td id="S6.T11.7.6.6.3" class="ltx_td ltx_align_center ltx_border_bb">0.389</td>
<td id="S6.T11.7.6.6.4" class="ltx_td ltx_align_center ltx_border_bb">7.15</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Workshop</span>
</h2>

<figure id="S7.T12" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE XII: </span>Workshop information</figcaption>
<div id="S7.T12.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:154.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-123.0pt,43.8pt) scale(0.638040207122402,0.638040207122402) ;">
<table id="S7.T12.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T12.1.1.1.1" class="ltx_tr">
<th id="S7.T12.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S7.T12.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S7.T12.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Date</span></th>
<th id="S7.T12.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S7.T12.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Location</span></th>
<th id="S7.T12.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S7.T12.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Keynote speech</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T12.1.1.2.1" class="ltx_tr">
<th id="S7.T12.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">
<table id="S7.T12.1.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T12.1.1.2.1.1.1.1" class="ltx_tr">
<td id="S7.T12.1.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S7.T12.1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></td>
</tr>
<tr id="S7.T12.1.1.2.1.1.1.2" class="ltx_tr">
<td id="S7.T12.1.1.2.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S7.T12.1.1.2.1.1.1.2.1.1" class="ltx_text ltx_font_bold">2019</span></td>
</tr>
</table>
</th>
<td id="S7.T12.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">Sep 14th, 2019</td>
<td id="S7.T12.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">Graz, Austria</td>
<td id="S7.T12.1.1.2.1.4" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">Mitchell McLaren, ”Speaker recognition - a retrospective”</td>
</tr>
<tr id="S7.T12.1.1.3.2" class="ltx_tr">
<th id="S7.T12.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<table id="S7.T12.1.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T12.1.1.3.2.1.1.1" class="ltx_tr">
<td id="S7.T12.1.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S7.T12.1.1.3.2.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></td>
</tr>
<tr id="S7.T12.1.1.3.2.1.1.2" class="ltx_tr">
<td id="S7.T12.1.1.3.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S7.T12.1.1.3.2.1.1.2.1.1" class="ltx_text ltx_font_bold">2020</span></td>
</tr>
</table>
</th>
<td id="S7.T12.1.1.3.2.2" class="ltx_td ltx_align_left">Oct 30th, 2020</td>
<td id="S7.T12.1.1.3.2.3" class="ltx_td ltx_align_left">Virtual</td>
<td id="S7.T12.1.1.3.2.4" class="ltx_td ltx_nopad_r ltx_align_left">
<table id="S7.T12.1.1.3.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T12.1.1.3.2.4.1.1" class="ltx_tr">
<td id="S7.T12.1.1.3.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Daniel Garcia-Romero, ”X-vectors: Neural Speech Embeddings for Speaker Recognition”</td>
</tr>
<tr id="S7.T12.1.1.3.2.4.1.2" class="ltx_tr">
<td id="S7.T12.1.1.3.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Shinji Watanabe, ”Tackling Multispeaker Conversation Processing based on Speaker Diarization and Multispeaker Speech Recognition”</td>
</tr>
</table>
</td>
</tr>
<tr id="S7.T12.1.1.4.3" class="ltx_tr">
<th id="S7.T12.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<table id="S7.T12.1.1.4.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T12.1.1.4.3.1.1.1" class="ltx_tr">
<td id="S7.T12.1.1.4.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S7.T12.1.1.4.3.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></td>
</tr>
<tr id="S7.T12.1.1.4.3.1.1.2" class="ltx_tr">
<td id="S7.T12.1.1.4.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S7.T12.1.1.4.3.1.1.2.1.1" class="ltx_text ltx_font_bold">2021</span></td>
</tr>
</table>
</th>
<td id="S7.T12.1.1.4.3.2" class="ltx_td ltx_align_left">Sep 7th, 2021</td>
<td id="S7.T12.1.1.4.3.3" class="ltx_td ltx_align_left">Virtual</td>
<td id="S7.T12.1.1.4.3.4" class="ltx_td ltx_nopad_r ltx_align_left">Andreas Stolcke, ”Speaker Recognition and Diarization for Alexa”</td>
</tr>
<tr id="S7.T12.1.1.5.4" class="ltx_tr">
<th id="S7.T12.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">
<table id="S7.T12.1.1.5.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T12.1.1.5.4.1.1.1" class="ltx_tr">
<td id="S7.T12.1.1.5.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S7.T12.1.1.5.4.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></td>
</tr>
<tr id="S7.T12.1.1.5.4.1.1.2" class="ltx_tr">
<td id="S7.T12.1.1.5.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S7.T12.1.1.5.4.1.1.2.1.1" class="ltx_text ltx_font_bold">2022</span></td>
</tr>
</table>
</th>
<td id="S7.T12.1.1.5.4.2" class="ltx_td ltx_align_left">Sep 22nd, 2022</td>
<td id="S7.T12.1.1.5.4.3" class="ltx_td ltx_align_left">Incheon, Korea</td>
<td id="S7.T12.1.1.5.4.4" class="ltx_td ltx_nopad_r ltx_align_left">Junichi Yamagishi, ”The use of speaker embeddings in neural audio generation”</td>
</tr>
<tr id="S7.T12.1.1.6.5" class="ltx_tr">
<th id="S7.T12.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">
<table id="S7.T12.1.1.6.5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S7.T12.1.1.6.5.1.1.1" class="ltx_tr">
<td id="S7.T12.1.1.6.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S7.T12.1.1.6.5.1.1.1.1.1" class="ltx_text ltx_font_bold">VoxSRC</span></td>
</tr>
<tr id="S7.T12.1.1.6.5.1.1.2" class="ltx_tr">
<td id="S7.T12.1.1.6.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S7.T12.1.1.6.5.1.1.2.1.1" class="ltx_text ltx_font_bold">2023</span></td>
</tr>
</table>
</th>
<td id="S7.T12.1.1.6.5.2" class="ltx_td ltx_align_left ltx_border_bb">Aug 20th, 2023</td>
<td id="S7.T12.1.1.6.5.3" class="ltx_td ltx_align_left ltx_border_bb">Dublin, Ireland</td>
<td id="S7.T12.1.1.6.5.4" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">Wei-Ning Hsu, “Scalable controllable speech generation via explicitly and implicitly disentangled speech representations”</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">VoxSRC workshops were held either in-person or virtually as a Zoom webinar in conjunction with the Interspeech conference.
The workshops were open for all to attend without any charge.
They typically started with an overview of the competition, followed by a keynote speech, an announcement of winners, and presentations by the winners.
The winners were selected only from teams that submitted technical reports to the organisers.
Details such as dates, locations, and the keynote speaker each year can be found in Table <a href="#S7.T12" title="TABLE XII ‣ VII Workshop ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XII</span></a>.
For the winners’ talks, the virtual attendees sent the organisers pre-recorded videos which explained their methods and results, while the in-person attendees gave their talks in the workshop venue, if possible.
Questions were gathered from both the virtual participants as well as those present physically at the venue.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Due to the COVID-19 pandemic, since 2020 we have sent a Zoom link to those who registered for our workshop through EventBrite <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>.
According to statistics gathered from EventBrite,
over the last four years, 776 people in total registered for the workshop virtually from 59 different countries.
Figure <a href="#S7.F6" title="Figure 6 ‣ VII Workshop ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the geographical heatmap of virtual participants per country from 2020 to 2023. The top 5 countries where participants have come from are the United States (119 participants), followed by China (92), India (81), Korea (41) and the United Kingdom (34).</p>
</div>
<figure id="S7.F6" class="ltx_figure"><img src="/html/2408.14886/assets/Figures/map.png" id="S7.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="374" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Geographical heatmap of virtual participants from 2020 to 2023. <span id="S7.F6.2.1" class="ltx_text ltx_font_bold"># participants</span> denotes the number of participants who attended the workshops virtually per country.</figcaption>
</figure>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Discussion and the Future</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this section, we present an analysis of the trends and constraints of the current generation of algorithms, as observed in our workshops over the past five years.
Additionally, we discuss the specific limitations encountered in our workshop setting.
The section concludes with key insights and recommendations intended for future organisers who are interested in hosting similar challenges.</p>
</div>
<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS1.5.1.1" class="ltx_text">VIII-A</span> </span><span id="S8.SS1.6.2" class="ltx_text ltx_font_italic">Analysis of winners’ methods</span>
</h3>

<div id="S8.SS1.p1" class="ltx_para">
<p id="S8.SS1.p1.1" class="ltx_p">Throughout the five series of VoxSRC challenges, there have been significant advances in speaker recognition technology, with
the most recent speaker verification models easily achieving super-human performance (since humans are poor at recognising unfamiliar voices) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>.
Models can now generate representative speaker embeddings that are robust to diverse conditions such as domain and language.
Speaker verification performance on VoxCeleb is now approaching 0% EER, even though conditions are “in the wild” and much shorter (mostly under 10 seconds) than those of a decade ago (minutes or even hours).
Speaker diarisation models have benefited from the progress in speaker verification, as a speaker embedding extractor is a core component of diarisation.</p>
</div>
<div id="S8.SS1.p2" class="ltx_para">
<p id="S8.SS1.p2.1" class="ltx_p">What are the trends of the current generation of methods observed in our workshop?
We consider the trends first for speaker verification and then for diarisation.</p>
</div>
<div id="S8.SS1.p3" class="ltx_para ltx_noindent">
<p id="S8.SS1.p3.1" class="ltx_p"><span id="S8.SS1.p3.1.1" class="ltx_text ltx_font_bold">Trends in verification models.</span>
Although the performance of speaker verification has improved significantly over the five years of the challenge, the underlying methodology remains largely unchanged compared to the approach of the VoxSRC 2019 winner.
As detailed in Table <a href="#S5.T2" title="TABLE II ‣ V Results ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, all winners follow a similar process: (i) inputting the voice as a spectrogram into a CNN-based model, (ii) training the CNN with AM-softmax or AAM-softmax loss, (iii) applying extensive data augmentation or using external data during training, and (iv) employing back-end systems like score normalisation or quality metric-based calibration methods to fuse results from multiple models.
The components of this recipe, present in 2019 systems, have been refined and improved over the past five years – for example, different backbones like TDNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and Conformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> can be used.</p>
</div>
<div id="S8.SS1.p4" class="ltx_para">
<p id="S8.SS1.p4.1" class="ltx_p">Some winners in Track 2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> utilised self-supervised pretrained networks primarily for feature extraction, training an additional network with these features to be used in an ensemble.
Direct finetuning of self-supervised pretrained networks, however, has not led to performance gains.</p>
</div>
<div id="S8.SS1.p5" class="ltx_para">
<p id="S8.SS1.p5.1" class="ltx_p">This raises the question: Can self-supervised front-ends or transformers yield significant improvements as seen in other fields?
Transformers have undoubtedly made remarkable progress in natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite> and computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>.
Yet, all winners adapted large ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>-based models for their submissions or used features from transformer-based models to train additional convolution-based networks.
It is acknowledged that transformers, lacking some inductive biases inherent in CNNs such as translation equivariance and locality, do not generalise well with limited data.
However, with sufficient data, large-scale training can surpass these inductive biases.
With the growing availability of public speaker verification data, it becomes feasible to train transformers leveraging these extensive datasets.
We leave this as a future direction in this field.</p>
</div>
<div id="S8.SS1.p6" class="ltx_para ltx_noindent">
<p id="S8.SS1.p6.1" class="ltx_p"><span id="S8.SS1.p6.1.1" class="ltx_text ltx_font_bold">Trends in diarisation models.</span>
In speaker diarisation, three distinct approaches are used: (i) the use of existing voice activity detection (VAD) combined with a speaker model and clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>, <a href="#bib.bib112" title="" class="ltx_ref">112</a>, <a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>, (ii) the application of End-to-End Neural Diarisation (EEND), which progresses from VAD to speaker assignment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>, <a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite>, and (iii) a combination of the strengths of these two methods by operating locally with end-to-end methods and then globally combining the intermediate results via clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>, <a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>.</p>
</div>
<div id="S8.SS1.p7" class="ltx_para">
<p id="S8.SS1.p7.1" class="ltx_p">Interestingly, all winners in our workshop consistently chose the clustering-based approach (i).
However, this approach inherently faces several limitations, such as difficulty in handling overlapping speech and the challenge of optimising various components for diarisation performance.
In contrast, the EEND method effectively addresses overlapping speech by introducing multiple speaker labels for each timestamp.
It is also capable of directly optimising a single model for diarisation performance.</p>
</div>
<div id="S8.SS1.p8" class="ltx_para">
<p id="S8.SS1.p8.1" class="ltx_p">Of course, EEND also has its limitations: (i) it suffers when processing audio with a large number of speakers or long duration, (ii) it presents challenges for real-time processing as it struggles to match the speakers between the output of audio chunks during block-wise processing, and (iii) it is prone to overfitting to the training data distribution, as noted by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite>.
However, this direction could open up unprecedented opportunities to address challenges in the field of speaker diarisation, such as joint optimisation with speech recognition or speech separation, with overlapping speech.</p>
</div>
<div id="S8.SS1.p9" class="ltx_para">
<p id="S8.SS1.p9.1" class="ltx_p">In recent years, methods that combine EEND models with clustering techniques have gained interest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>.
These methods apply the EEND model on a local scale to effectively manage overlapping speech while employing clustering methods that utilise global speaker embeddings derived from the EEND model.
The objective is to resolve the inter-block label permutation issue that arises during the block-wise processing of long audio recordings.
This approach was employed in a recent submission <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite> and we expect its further improvement in the future.</p>
</div>
</section>
<section id="S8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS2.5.1.1" class="ltx_text">VIII-B</span> </span><span id="S8.SS2.6.2" class="ltx_text ltx_font_italic">Current research challenges</span>
</h3>

<div id="S8.SS2.p1" class="ltx_para">
<p id="S8.SS2.p1.1" class="ltx_p">Although the performance of speaker recognition models saturates on VoxSRC test sets, there still remain several open challenges for future speaker verification workshops.
This section describes outstanding problems that could form possible future challenges for speaker verification workshops.</p>
</div>
<div id="S8.SS2.p2" class="ltx_para ltx_noindent">
<p id="S8.SS2.p2.1" class="ltx_p"><span id="S8.SS2.p2.1.1" class="ltx_text ltx_font_bold">Anti-spoofing.</span>
An important use case of speaker recognition is biometric verification.
However, in real-world scenarios, e.g., online banking, there is a need to safeguard against spoofing attacks – including synthesized utterances, an area where performance is rapidly improving.
Currently, it is well known that even state-of-the-art models are highly vulnerable when fed with spoofed inputs.
The ASVSpoof <span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://www.asvspoof.org/</span></span></span> evaluation series is facilitating advancements in this important topic.</p>
</div>
<div id="S8.SS2.p3" class="ltx_para ltx_noindent">
<p id="S8.SS2.p3.1" class="ltx_p"><span id="S8.SS2.p3.1.1" class="ltx_text ltx_font_bold">Noisy or overlapping scenarios.</span>
Although VoxCeleb is “in the wild” to some extent, it still does not cover extremely noisy or overlapping scenarios.
With these hurdles remaining to be dealt with, we urge the research community to collect and curate a dataset beyond VoxCeleb, and even closer to real-world scenarios which include highly noisy and overlapping inputs.</p>
</div>
<div id="S8.SS2.p4" class="ltx_para ltx_noindent">
<p id="S8.SS2.p4.1" class="ltx_p"><span id="S8.SS2.p4.1.1" class="ltx_text ltx_font_bold">Diversity and scale in data.</span>
While VoxCeleb is recognised as one of the largest speaker verification datasets in the research community, and although it is multi-lingual, it is dominated by English speakers (approximately 84% in VoxCeleb1 and 53% in VoxCeleb2, based on the output of VoxLingua <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> classifier).
In our semi-supervised domain adaptation track, we made efforts to include Chinese data, yet our dataset still lacks representation of more diverse languages, particularly low-resource languages.
Additionally, the gender distribution in VoxCeleb2 is somewhat imbalanced, as discussed in Section <a href="#S6.SS2" title="VI-B Analysis of performance by utterance length and gender ‣ VI Detailed analysis ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-B</span></span></a>.
Diversity in AI systems is important to ensure systems represent the variety of people who will use them.
Additionally, addressing ethical concerns and ensuring fairness in speaker recognition systems is critical.
These issues require attention in any future dataset collection and release.</p>
</div>
<div id="S8.SS2.p5" class="ltx_para">
<p id="S8.SS2.p5.1" class="ltx_p">In terms of scale, while VoxCeleb contains approximately 2,000 hours of speech, the state-of-the-art ASR systems, such as Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>, are typically trained on millions of hours of speech data.
It is well known that training with large amounts of data leads to better performing models.
It would be helpful for the community if challenge organisers could curate and provide large scale training data for participants to push the limits of their models.</p>
</div>
</section>
<section id="S8.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS3.5.1.1" class="ltx_text">VIII-C</span> </span><span id="S8.SS3.6.2" class="ltx_text ltx_font_italic">Lessons for future challenge organisers</span>
</h3>

<div id="S8.SS3.p1" class="ltx_para">
<p id="S8.SS3.p1.1" class="ltx_p">Organising a successful challenge in the field of data science or machine learning requires meticulous planning and foresight.
Based on our experience, we note several key lessons that future challenge organisers should consider:</p>
</div>
<div id="S8.SS3.p2" class="ltx_para ltx_noindent">
<p id="S8.SS3.p2.1" class="ltx_p"><span id="S8.SS3.p2.1.1" class="ltx_text ltx_font_bold">Importance of a robust evaluation platform.</span>
A critical factor in the success of any challenge is the reliability of the evaluation platform.
The platform must be stable to handle a large volume of submissions and a diverse of data types.
Additionally, it should be easily adjustable to accommodate evolving challenge requirements and criteria.
This flexibility ensures that the platform remains relevant and efficient throughout the challenge and beyond.
In recent times, the advent of numerous platforms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>, <a href="#bib.bib123" title="" class="ltx_ref">123</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> for machine learning challenges has significantly aided organisers in hosting such events.
We recommend that future organisers carefully select their platforms, taking into account the diverse features and functionalities that these platforms offer, and promised longevity.</p>
</div>
<div id="S8.SS3.p3" class="ltx_para ltx_noindent">
<p id="S8.SS3.p3.1" class="ltx_p"><span id="S8.SS3.p3.1.1" class="ltx_text ltx_font_bold">Maintain a persistent test set.</span>
Effectively tracking state-of-the-art performance annually is essential in hosting a series of machine learning challenges.
This can be achieved by maintaining a consistent test set.
There are two possible approaches: either keeping the entire test set unchanged or incorporating the test set from previous challenges into the subsequent year’s test set.
This continuity makes it possible to directly compare the methods used by the winners each year and to analyse the progress made by state-of-the-art models.</p>
</div>
<div id="S8.SS3.p4" class="ltx_para ltx_noindent">
<p id="S8.SS3.p4.1" class="ltx_p"><span id="S8.SS3.p4.1.1" class="ltx_text ltx_font_bold">Secure non-overlapping test sets.</span>
As the Internet continues to grow with ever-increasing volumes of data, the challenge is to curate test sets that are truly challenging and reflect real-world scenarios.
A particular challenge is siloing the test set, to ensure that it has not been used for training.
Since 2020, the winners achieved below 1% EER on the original VoxSRC 2019 test set, which we consider as performance saturation. Consequently, each year of the challenge we developed more challenging test sets – and this occupied most of the preparation time before a new challenge was launched.
Future organisers need to be innovative in the way they create harder datasets, ensuring that they test the boundaries of current methodologies.</p>
</div>
<div id="S8.SS3.p5" class="ltx_para ltx_noindent">
<p id="S8.SS3.p5.1" class="ltx_p"><span id="S8.SS3.p5.1.1" class="ltx_text ltx_font_bold">Report individual and ensemble network performance.</span>
It is proven that using an ensemble of several individual models leads to better generalisation performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>.
We observe that all challenge winners fuse predictions from multiple models in order to achieve better performance on the test set.
However, challenge reports should include detailed analyses of the performance of <span id="S8.SS3.p5.1.2" class="ltx_text ltx_font_italic">individual</span> networks as well as their ensemble counterparts.
Addressing both approaches provides a comprehensive view of the strengths and limitations of different models and offers insights into their practical applications.</p>
</div>
<div id="S8.SS3.p6" class="ltx_para ltx_noindent">
<p id="S8.SS3.p6.1" class="ltx_p"><span id="S8.SS3.p6.1.1" class="ltx_text ltx_font_bold">More …</span> We suggest that challenge organisers also read the advice from other challenges, such as the ‘Discussion and Future’ section of the PASCAL VOC retrospective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IX </span><span id="S9.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">This paper presents a review of the VoxSRC challenges and workshops including how we host the challenge, how we create the datasets and methods from challenge participants.
We conclude with a discussion and limitations of our workshop along with our advice for future challenge organisers.
We hope this helps researchers in the speaker recognition and diarisation field to grasp an overall trend in these fields and also people who want to host a similar challenge in the future.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">First, we would like to thank our advisors, Mitchell McLaren and Douglas A. Reynolds for providing valuable comments and feedback.
Second, we thank the annotators who helped us to make all datasets used. We especially thank
Rajan from Elancer and his team for the annotation of the VoxConverse and diarisation test sets. We also thank Bong-Jin Lee, Heesoo Heo, Youngki Kwon, Youjin Kim, Triantafyllos Afouras, Alba Maria Martinez Garcia, Kihyun Nam, Doyeop Kwak and Youngjoon Jang for double-checking the diarisation validation and test set labels.
Third, we are grateful to those who helped maintain our evaluation servers including David Pinto, Ernesto Coto and Abhishek Dutta.
Fourth, we thank the CNCeleb authors for generously providing their hidden dataset as well as additional support.
We also thank Naver Corporation for sponsoring our workshops.</p>
</div>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Statistics of validation and test sets</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Table <a href="#A1.T13" title="TABLE XIII ‣ Appendix A Statistics of validation and test sets ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XIII</span></a> shows the statistics of the verification datasets and Table <a href="#A1.T14" title="TABLE XIV ‣ Appendix A Statistics of validation and test sets ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">XIV</span></a> shows the statistics of the diarisation datasets
for the entire validation and test datasets over the five years of the challenge.
The verification datasets of the verification tracks up to 2021 are constructed using the entire VoxCeleb1, so # of speakers is limited to 1,251, which is the number of speakers in VoxCeleb1.
The # of speakers increased significantly in 2022 when we started to include the utterances of VoxConverse for hard negative pairs.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">For the diarisation datasets, we started to use the entire VoxConverse set as our validation set, so the validation set statistics are identical from 2021 to 2023.
In 2020, there was a significant difference in the dataset distribution between the validation and test sets in terms of the average number of speakers and the average length of the audio.
We fixed this issue from 2021 on.</p>
</div>
<figure id="A1.T13" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE XIII: </span>Statistics of validation and test sets in the verification tracks. Note that validation sets of track 3 in year 2022 and 2023 are identical. <span id="A1.T13.5.1" class="ltx_text ltx_font_bold"># files</span> : total number of audio files, <span id="A1.T13.6.2" class="ltx_text ltx_font_bold"># pairs</span> : total number of pairs, <span id="A1.T13.7.3" class="ltx_text ltx_font_bold"># spks</span> : total number of speakers and <span id="A1.T13.8.4" class="ltx_text ltx_font_bold">Avg length</span> : average length of audio in seconds.</figcaption>
<div id="A1.T13.9" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:145pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.5pt,8.5pt) scale(0.894931804315303,0.894931804315303) ;">
<table id="A1.T13.9.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T13.9.1.1.1" class="ltx_tr">
<td id="A1.T13.9.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="A1.T13.9.1.1.1.1.1" class="ltx_text ltx_font_bold">Year</span></td>
<td id="A1.T13.9.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="A1.T13.9.1.1.1.2.1" class="ltx_text ltx_font_bold">Track</span></td>
<td id="A1.T13.9.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="A1.T13.9.1.1.1.3.1" class="ltx_text ltx_font_bold">Validation set</span></td>
<td id="A1.T13.9.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="A1.T13.9.1.1.1.4.1" class="ltx_text ltx_font_bold">Test set</span></td>
</tr>
<tr id="A1.T13.9.1.2.2" class="ltx_tr">
<td id="A1.T13.9.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T13.9.1.2.2.1.1" class="ltx_text ltx_font_bold"># files</span></td>
<td id="A1.T13.9.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T13.9.1.2.2.2.1" class="ltx_text ltx_font_bold"># pairs</span></td>
<td id="A1.T13.9.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T13.9.1.2.2.3.1" class="ltx_text ltx_font_bold"># spks</span></td>
<td id="A1.T13.9.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T13.9.1.2.2.4.1" class="ltx_text ltx_font_bold">Avg length</span></td>
<td id="A1.T13.9.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T13.9.1.2.2.5.1" class="ltx_text ltx_font_bold"># files</span></td>
<td id="A1.T13.9.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T13.9.1.2.2.6.1" class="ltx_text ltx_font_bold"># pairs</span></td>
<td id="A1.T13.9.1.2.2.7" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T13.9.1.2.2.7.1" class="ltx_text ltx_font_bold"># spks</span></td>
<td id="A1.T13.9.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T13.9.1.2.2.8.1" class="ltx_text ltx_font_bold">Avg length</span></td>
</tr>
<tr id="A1.T13.9.1.3.3" class="ltx_tr">
<td id="A1.T13.9.1.3.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T13.9.1.3.3.1.1" class="ltx_text ltx_font_bold">2019</span></td>
<td id="A1.T13.9.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">1,2</td>
<td id="A1.T13.9.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t">4,715</td>
<td id="A1.T13.9.1.3.3.4" class="ltx_td ltx_align_right ltx_border_t">37,720</td>
<td id="A1.T13.9.1.3.3.5" class="ltx_td ltx_align_right ltx_border_t">40</td>
<td id="A1.T13.9.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">8.3</td>
<td id="A1.T13.9.1.3.3.7" class="ltx_td ltx_align_right ltx_border_t">19,154</td>
<td id="A1.T13.9.1.3.3.8" class="ltx_td ltx_align_right ltx_border_t">208,008</td>
<td id="A1.T13.9.1.3.3.9" class="ltx_td ltx_align_right ltx_border_t">745</td>
<td id="A1.T13.9.1.3.3.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">7.4</td>
</tr>
<tr id="A1.T13.9.1.4.4" class="ltx_tr">
<td id="A1.T13.9.1.4.4.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T13.9.1.4.4.1.1" class="ltx_text ltx_font_bold">2020</span></td>
<td id="A1.T13.9.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">1,2</td>
<td id="A1.T13.9.1.4.4.3" class="ltx_td ltx_align_right ltx_border_t">140,815</td>
<td id="A1.T13.9.1.4.4.4" class="ltx_td ltx_align_right ltx_border_t">263,486</td>
<td id="A1.T13.9.1.4.4.5" class="ltx_td ltx_align_right ltx_border_t">1,251</td>
<td id="A1.T13.9.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">8.2</td>
<td id="A1.T13.9.1.4.4.7" class="ltx_td ltx_align_right ltx_border_t">118,439</td>
<td id="A1.T13.9.1.4.4.8" class="ltx_td ltx_align_right ltx_border_t">1,695,248</td>
<td id="A1.T13.9.1.4.4.9" class="ltx_td ltx_align_right ltx_border_t">1,440</td>
<td id="A1.T13.9.1.4.4.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">5.0</td>
</tr>
<tr id="A1.T13.9.1.5.5" class="ltx_tr">
<td id="A1.T13.9.1.5.5.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T13.9.1.5.5.1.1" class="ltx_text ltx_font_bold">2021</span></td>
<td id="A1.T13.9.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">1,2,3</td>
<td id="A1.T13.9.1.5.5.3" class="ltx_td ltx_align_right ltx_border_t">64,711</td>
<td id="A1.T13.9.1.5.5.4" class="ltx_td ltx_align_right ltx_border_t">60,000</td>
<td id="A1.T13.9.1.5.5.5" class="ltx_td ltx_align_right ltx_border_t">1,251</td>
<td id="A1.T13.9.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">8.1</td>
<td id="A1.T13.9.1.5.5.7" class="ltx_td ltx_align_right ltx_border_t">116,984</td>
<td id="A1.T13.9.1.5.5.8" class="ltx_td ltx_align_right ltx_border_t">476,224</td>
<td id="A1.T13.9.1.5.5.9" class="ltx_td ltx_align_right ltx_border_t">1,441</td>
<td id="A1.T13.9.1.5.5.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">5.0</td>
</tr>
<tr id="A1.T13.9.1.6.6" class="ltx_tr">
<td id="A1.T13.9.1.6.6.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="A1.T13.9.1.6.6.1.1" class="ltx_text ltx_font_bold">2022</span></td>
<td id="A1.T13.9.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">1,2</td>
<td id="A1.T13.9.1.6.6.3" class="ltx_td ltx_align_right ltx_border_t">110,366</td>
<td id="A1.T13.9.1.6.6.4" class="ltx_td ltx_align_right ltx_border_t">305,196</td>
<td id="A1.T13.9.1.6.6.5" class="ltx_td ltx_align_right ltx_border_t">2,307</td>
<td id="A1.T13.9.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">8.4</td>
<td id="A1.T13.9.1.6.6.7" class="ltx_td ltx_align_right ltx_border_t">34,684</td>
<td id="A1.T13.9.1.6.6.8" class="ltx_td ltx_align_right ltx_border_t">317,973</td>
<td id="A1.T13.9.1.6.6.9" class="ltx_td ltx_align_right ltx_border_t">1,974</td>
<td id="A1.T13.9.1.6.6.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">7.4</td>
</tr>
<tr id="A1.T13.9.1.7.7" class="ltx_tr">
<td id="A1.T13.9.1.7.7.1" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="A1.T13.9.1.7.7.2" class="ltx_td ltx_align_right ltx_border_t">2,400</td>
<td id="A1.T13.9.1.7.7.3" class="ltx_td ltx_align_right ltx_border_t">40,000</td>
<td id="A1.T13.9.1.7.7.4" class="ltx_td ltx_align_right ltx_border_t">120</td>
<td id="A1.T13.9.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">9.4</td>
<td id="A1.T13.9.1.7.7.6" class="ltx_td ltx_align_right ltx_border_t">18,377</td>
<td id="A1.T13.9.1.7.7.7" class="ltx_td ltx_align_right ltx_border_t">30,000</td>
<td id="A1.T13.9.1.7.7.8" class="ltx_td ltx_align_right ltx_border_t">56</td>
<td id="A1.T13.9.1.7.7.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">9.1</td>
</tr>
<tr id="A1.T13.9.1.8.8" class="ltx_tr">
<td id="A1.T13.9.1.8.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="A1.T13.9.1.8.8.1.1" class="ltx_text ltx_font_bold">2023</span></td>
<td id="A1.T13.9.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t">1,2</td>
<td id="A1.T13.9.1.8.8.3" class="ltx_td ltx_align_right ltx_border_t">63,516</td>
<td id="A1.T13.9.1.8.8.4" class="ltx_td ltx_align_right ltx_border_t">49,987</td>
<td id="A1.T13.9.1.8.8.5" class="ltx_td ltx_align_right ltx_border_t">2,140</td>
<td id="A1.T13.9.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t">8.2</td>
<td id="A1.T13.9.1.8.8.7" class="ltx_td ltx_align_right ltx_border_t">256,547</td>
<td id="A1.T13.9.1.8.8.8" class="ltx_td ltx_align_right ltx_border_t">825.437</td>
<td id="A1.T13.9.1.8.8.9" class="ltx_td ltx_align_right ltx_border_t">2,532</td>
<td id="A1.T13.9.1.8.8.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">5.3</td>
</tr>
<tr id="A1.T13.9.1.9.9" class="ltx_tr">
<td id="A1.T13.9.1.9.9.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">3</td>
<td id="A1.T13.9.1.9.9.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">2,400</td>
<td id="A1.T13.9.1.9.9.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">40,000</td>
<td id="A1.T13.9.1.9.9.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">120</td>
<td id="A1.T13.9.1.9.9.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">9.4</td>
<td id="A1.T13.9.1.9.9.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">21,997</td>
<td id="A1.T13.9.1.9.9.7" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">80,000</td>
<td id="A1.T13.9.1.9.9.8" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">56</td>
<td id="A1.T13.9.1.9.9.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t">8.9</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="A1.T14" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE XIV: </span>Statistics of track 4 validation and test sets. <span id="A1.T14.4.1" class="ltx_text ltx_font_bold"># files</span> : total number of audio files, <span id="A1.T14.5.2" class="ltx_text ltx_font_bold">Avg # spks</span> : average number of speakers per each audio and <span id="A1.T14.6.3" class="ltx_text ltx_font_bold">Avg length</span> : average length of audio in seconds.</figcaption>
<div id="A1.T14.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:121.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(24.6pt,-6.9pt) scale(1.1281562474878,1.1281562474878) ;">
<table id="A1.T14.7.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T14.7.1.1.1" class="ltx_tr">
<th id="A1.T14.7.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="A1.T14.7.1.1.1.1.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="A1.T14.7.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="A1.T14.7.1.1.1.2.1" class="ltx_text ltx_font_bold">Validation set</span></th>
<th id="A1.T14.7.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="A1.T14.7.1.1.1.3.1" class="ltx_text ltx_font_bold">Test set</span></th>
</tr>
<tr id="A1.T14.7.1.2.2" class="ltx_tr">
<th id="A1.T14.7.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A1.T14.7.1.2.2.1.1" class="ltx_text ltx_font_bold"># files</span></th>
<th id="A1.T14.7.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A1.T14.7.1.2.2.2.1" class="ltx_text ltx_font_bold">Avg # spks</span></th>
<th id="A1.T14.7.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A1.T14.7.1.2.2.3.1" class="ltx_text ltx_font_bold">Avg length</span></th>
<th id="A1.T14.7.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A1.T14.7.1.2.2.4.1" class="ltx_text ltx_font_bold"># files</span></th>
<th id="A1.T14.7.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A1.T14.7.1.2.2.5.1" class="ltx_text ltx_font_bold">Avg # spks</span></th>
<th id="A1.T14.7.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A1.T14.7.1.2.2.6.1" class="ltx_text ltx_font_bold">Avg length</span></th>
</tr>
<tr id="A1.T14.7.1.3.3" class="ltx_tr">
<th id="A1.T14.7.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">2020</th>
<th id="A1.T14.7.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">216</th>
<th id="A1.T14.7.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">4.5</th>
<th id="A1.T14.7.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">338.2</th>
<th id="A1.T14.7.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">232</th>
<th id="A1.T14.7.1.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">6.5</th>
<th id="A1.T14.7.1.3.3.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">675.6</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T14.7.1.4.1" class="ltx_tr">
<td id="A1.T14.7.1.4.1.1" class="ltx_td ltx_align_left ltx_border_t">2021</td>
<td id="A1.T14.7.1.4.1.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="3"><span id="A1.T14.7.1.4.1.2.1" class="ltx_text">448</span></td>
<td id="A1.T14.7.1.4.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="3"><span id="A1.T14.7.1.4.1.3.1" class="ltx_text">5.5</span></td>
<td id="A1.T14.7.1.4.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="3"><span id="A1.T14.7.1.4.1.4.1" class="ltx_text">512.9</span></td>
<td id="A1.T14.7.1.4.1.5" class="ltx_td ltx_align_center ltx_border_t">264</td>
<td id="A1.T14.7.1.4.1.6" class="ltx_td ltx_align_center ltx_border_t">5.6</td>
<td id="A1.T14.7.1.4.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">452.1</td>
</tr>
<tr id="A1.T14.7.1.5.2" class="ltx_tr">
<td id="A1.T14.7.1.5.2.1" class="ltx_td ltx_align_left ltx_border_t">2022</td>
<td id="A1.T14.7.1.5.2.2" class="ltx_td ltx_align_center ltx_border_t">360</td>
<td id="A1.T14.7.1.5.2.3" class="ltx_td ltx_align_center ltx_border_t">5.5</td>
<td id="A1.T14.7.1.5.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">449.2</td>
</tr>
<tr id="A1.T14.7.1.6.3" class="ltx_tr">
<td id="A1.T14.7.1.6.3.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">2023</td>
<td id="A1.T14.7.1.6.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">413</td>
<td id="A1.T14.7.1.6.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">5.8</td>
<td id="A1.T14.7.1.6.3.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t">519.6</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Participant statistics</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">Figure <a href="#A2.F7" title="Figure 7 ‣ Appendix B Participant statistics ‣ The VoxCeleb Speaker Recognition Challenge: A Retrospective" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows a bar chart comparing the participation statistics per year over the course of the challenge.
The left figure shows the progress of the number of teams that participated each year, while the right shows the change of number of submissions each year.
The number of participants increased until 2021 and then decreased.
However, the number of submissions increased until 2022, which means that the number of people participating and submitting their results multiple times before the deadline has increased.
Interestingly, the number of participants in Track 2 increased dramatically from 2020 to 2021 (from 21 to 54), due to the emergence of the self-supervised speaker models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.</p>
</div>
<figure id="A2.F7" class="ltx_figure"><img src="/html/2408.14886/assets/x5.png" id="A2.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="226" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Number of participants (left) and number of submissions (right) over the series of the VoxSRC challenge.</figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Potential risks and societal impacts</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">One potential risk of this work relates to privacy concerns.
Although identities have been anonymised, misuse or unauthorised access to this data could lead to privacy violations or identity theft.
We advise researchers and developers using this dataset to be aware of these risks when training models with this data.
Furthermore, it is important to note that the distribution of identities in our dataset does not represent the global human population.
We urge careful consideration of potential unintended biases – including societal, gender, and racial biases – when training or deploying models using this data for either training or evaluation purposes.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. S. Chung, A. Nagrani, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, and
A. Zisserman, “VoxSRC 2019: The first voxceleb speaker recognition
challenge,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.02522</em>, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie, M. McLaren, D. A.
Reynolds, and A. Zisserman, “VoxSRC 2020: The second voxceleb speaker
recognition challenge,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.06867</em>, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Brown, J. Huh, J. S. Chung, A. Nagrani, and A. Zisserman, “VoxSRC 2021:
The third voxceleb speaker recognition challenge,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2201.04583</em>, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Huh, A. Brown, J.-w. Jung, J. S. Chung, A. Nagrani, D. Garcia-Romero, and
A. Zisserman, “Voxsrc 2022: The fourth voxceleb speaker recognition
challenge,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.10248</em>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M. P. Alvin and A. Martin, “NIST speaker recognition evaluation
chronicles,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proc. Odyssey 2004, The Speaker and Language
Recognition Workshop</em>.   Citeseer, 2004.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
C. S. Greenberg, “The NIST year 2012 speaker recognition evaluation plan,”
<em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">NIST, Technical Report, 2012</em>, 2012.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">NIST 2018 Speaker Recognition Evaluation Plan</em>, 2018 (accessed 31 July
2020),
<a target="_blank" href="https://www.nist.gov/system/files/documents/2018/08/17/sre18_eval_plan_2018-05-31_v6.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nist.gov/system/files/documents/2018/08/17/sre18_eval_plan_2018-05-31_v6.pdf</a>,
See Section 3.1.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. O. Sadjadi, T. Kheyrkhah, A. Tong, C. S. Greenberg, D. A. Reynolds,
E. Singer, L. P. Mason, and J. Hernandez-Cordero, “The 2016 nist speaker
recognition evaluation.” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2017, pp. 1353–1357.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. O. Sadjadi, C. Greenberg, E. Singer, D. Reynolds, L. Mason, and
J. Hernandez-Cordero, “The 2019 nist speaker recognition evaluation cts
challenge,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. Speaker Odyssey</em>, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. McLaren, L. Ferrer, D. Castan, and A. Lawson, “The speakers in the wild
(SITW) speaker recognition database,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2016.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Maciejewski,
V. Manohar, N. Dehak, D. Povey, S. Watanabe <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Diarization is
hard: Some experiences and lessons learned for the jhu team in the inaugural
dihard challenge.” in <em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2018, pp. 2808–2812.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. S. Chung, A. Nagrani, and A. Zisserman, “VoxCeleb2: Deep speaker
recognition,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for
contrastive learning of visual representations,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya,
C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Bootstrap your own latent-a new approach to self-supervised learning,” in
<em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">NeurIPS</em>, vol. 33, 2020, pp. 21 271–21 284.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for
unsupervised visual representation learning,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2020,
pp. 9729–9738.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of
deep bidirectional transformers for language understanding,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proc.
NAACL</em>, Jun 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Language
models are few-shot learners,” <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>, vol. 33, pp. 1877–1901, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
L. Li, R. Liu, J. Kang, Y. Fan, H. Cui, Y. Cai, R. Vipperla, T. F. Zheng, and
D. Wang, “Cn-celeb: multi-genre speaker recognition,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Speech
Communication</em>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A. Nagrani, J. S. Chung, and A. Zisserman, “VoxCeleb: a large-scale speaker
identification dataset,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2017.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, “Voxceleb: Large-scale
speaker verification in the wild,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em>,
vol. 60, p. 101027, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
“Ssd: Single shot multibox detector,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proc. ECCV</em>.   Springer, 2016, pp. 21–37.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, “VGGFace2: A
dataset for recognising faces across pose and age,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proc. Int.
Conf. Autom. Face and Gesture Recog.</em>, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. S. Chung and A. Zisserman, “Out of time: automated lip sync in the wild,”
in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Workshop on Multi-view Lip-reading, ACCV</em>, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Brown, J. Huh, A. Nagrani, J. S. Chung, and A. Zisserman, “Playing a part:
Speaker verification at the movies,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ICASSP</em>, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J. Valk and T. Alumäe, “VoxLingua107: a dataset for spoken language
recognition,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE SLT Workshop</em>, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
<em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">VGG List Annotator (LISA)</em>, 2022,
https://www.robots.ox.ac.uk/ vgg/software/lisa/.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
“Facelib,” <a target="_blank" href="https://github.com/sajjjadayobi/FaceLib" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/sajjjadayobi/FaceLib</a>, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. S. Chung, J. Huh, A. Nagrani, T. Afouras, and A. Zisserman, “Spot the
conversation: speaker diarisation in the wild,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>,
2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J.-w. Jung, H.-S. Heo, B.-J. Lee, J. Huh, A. Brown, Y. Kwon, S. Watanabe, and
J. S. Chung, “In search of strong embedding extractors for speaker
diarisation,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
H. Chen, W. Xie, A. Vedaldi, and A. Zisserman, “Vggsound: A large-scale
audio-visual dataset,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>.   IEEE, 2020, pp. 721–725.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
T. Afouras, J. S. Chung, and A. Zisserman, “The conversation: Deep
audio-visual speech enhancement,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2018.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. S. Chung, J. Huh, S. Mun, M. Lee, H. S. Heo, S. Choe, C. Ham, S. Jung, B.-J.
Lee, and I. Han, “In defence of metric learning for speaker recognition,”
<em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, and
M. Liberman, “The second dihard diarization challenge: Dataset, task, and
baselines,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.07839</em>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
A. Pavao, I. Guyon, A.-C. Letournel, X. Baró, H. Escalante, S. Escalera,
T. Thomas, and Z. Xu, “Codalab competitions: An open source platform to
organize scientific challenges,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Technical report</em>, 2022. [Online].
Available: <a target="_blank" href="https://hal.inria.fr/hal-03629462v1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://hal.inria.fr/hal-03629462v1</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Y. Kwon, H.-S. Heo, B.-J. Lee, and J. S. Chung, “The ins and outs of speaker
recognition: lessons from VoxSRC 2020,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>.   IEEE, 2021, pp. 5809–5813.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Q. Lin, R. Yin, M. Li, H. Bredin, and C. Barras, “Lstm based similarity
measurement with spectral clustering for speaker diarization,” in
<em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
H. Bredin, R. Yin, J. M. Coria, G. Gelly, P. Korshunov, M. Lavechin, D. Fustes,
H. Titeux, W. Bouaziz, and M.-P. Gill, “Pyannote. audio: neural building
blocks for speaker diarization,” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>.   IEEE, 2020, pp. 7124–7128.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
B. Desplanques, J. Thienpondt, and K. Demuynck, “Ecapa-tdnn: Emphasized
channel attention, propagation and aggregation in tdnn based speaker
verification,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2020.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
“Clovaai voxceleb trainer,”
<a target="_blank" href="https://github.com/clovaai/voxceleb_trainer" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/clovaai/voxceleb_trainer</a>, 2024.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
“Simple diarization repository,”
<a target="_blank" href="https://github.com/JaesungHuh/SimpleDiarization" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/JaesungHuh/SimpleDiarization</a>, 2024.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2016.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
D. Snyder, G. Chen, and D. Povey, “Musan: A music, speech, and noise corpus,”
<em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1510.08484</em>, 2015.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular margin
loss for deep face recognition,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
D. Garcia-Romero, X. Zhou, and C. Y. Espy-Wilson, “Multicondition training of
gaussian plda models in i-vector space for noise and reverberation robust
speaker recognition,” in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2012.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
P. Matejka, O. Novotnỳ, O. Plchot, L. Burget, M. D. Sánchez, and
J. Cernockỳ, “Analysis of score normalization in multilingual speaker
recognition.” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2017, pp. 1567–1571.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus
based on public domain audio books,” in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2015.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
H. Zeinali, H. Sameti, and T. Stafylakis, “DeepMine Speech Processing
Database: Text-Dependent and Independent Speaker Verification and Speech
Recognition in Persian and English ,” in <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proc. The Speaker and
Language Recognition Workshop (Odyssey 2018)</em>, 2018.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le,
“Specaugment: A simple data augmentation method for automatic speech
recognition,” in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2019.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
X. Ding, X. Zhang, N. Ma, J. Han, G. Ding, and J. Sun, “Repvgg: Making
vgg-style convnets great again,” in <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2021.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
F. Wang, J. Cheng, W. Liu, and H. Liu, “Additive margin softmax for face
verification,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Letters</em>, 2018.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,
T. Yoshioka, X. Xiao <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Wavlm: Large-scale self-supervised
pre-training for full stack speech processing,” <em id="bib.bib51.2.2" class="ltx_emph ltx_font_italic">IEEE Journal of
Selected Topics in Signal Processing</em>, vol. 16, no. 6, pp. 1505–1518, 2022.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and
A. Mohamed, “Hubert: Self-supervised speech representation learning by
masked prediction of hidden units,” <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio,
Speech, and Language Processing</em>, 2021.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
S. Chen, Y. Wu, C. Wang, Z. Chen, Z. Chen, S. Liu, J. Wu, Y. Qian, F. Wei,
J. Li <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Unispeech-sat: Universal speech representation
learning with speaker aware pre-training,” in <em id="bib.bib53.2.2" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2022.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von
Platen, Y. Saraf, J. Pino <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Xls-r: Self-supervised
cross-lingual speech representation learning at scale,” in <em id="bib.bib54.2.2" class="ltx_emph ltx_font_italic">Proc.
Interspeech</em>, 2022.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
I. Yakovlev, A. Okhotnikov, N. Torgashov, R. Makarov, Y. Voevodin, and
K. Simonchik, “VoxTube: a multilingual speaker recognition dataset,” in
<em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2023.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur, “A study on
data augmentation of reverberant speech for robust speech recognition,” in
<em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>.   IEEE, 2017, pp.
5220–5224.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
J. Thienpondt, B. Desplanques, and K. Demuynck, “The idlab voxsrc-20
submission: Large margin fine-tuning and quality-aware score calibration in
dnn based speaker verification,” in <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2021.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
M. Zhao, Y. Ma, M. Liu, and M. Xu, “The speakin system for voxceleb speaker
recognition challange 2021,” <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.01989</em>, 2021.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
T. Hasan, S. O. Sadjadi, G. Liu, N. Shokouhi, H. Bořil, and J. H. Hansen,
“Crss systems for 2012 nist speaker recognition evaluation,” in <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Proc.
ICASSP</em>, 2013.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
M. I. Mandasari, R. Saeidi, M. McLaren, and D. A. van Leeuwen, “Quality
measure functions for calibration of speaker recognition systems in various
duration conditions,” <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Audio, Speech, and Language
Processing</em>, vol. 21, no. 11, pp. 2425–2438, 2013.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
R. Makarov, N. Torgashov, A. Alenin, I. Yakovlev, and A. Okhotnikov, “Id r&amp;d
system description to voxceleb speaker recognition challenge 2022,”
<a target="_blank" href="https://www.robots.ox.ac.uk/~vgg/data/voxceleb/data_workshop_2022/reports/ravana_idrnd.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.robots.ox.ac.uk/~vgg/data/voxceleb/data_workshop_2022/reports/ravana_idrnd.pdf</a>,
2022.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
N. Torgashov, R. Makarov, I. Yakovlev, P. Malov, A. Balykin, and A. Okhotnikov,
“The id r&amp;d voxceleb speaker recognition challenge 2023 system
description,” <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.08294</em>, 2023.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
H. Zeinali, S. Wang, A. Silnova, P. Matějka, and O. Plchot, “But system
description to voxceleb speaker recognition challenge 2019,” <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1910.12592</em>, 2019.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
D. Garcia-Romero, A. McCree, D. Snyder, and G. Sell, “Jhu-hltcoe system for
the voxsrc speaker recognition challenge,” in <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2020.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
T. Zhou, Y. Zhao, J. Li, Z. Chen, and J. Wu, “Cnn with phonetic attention for
text-independent speaker verification,”
<a target="_blank" href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/data_workshop/VoxSRC_TZ_microsoft.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.robots.ox.ac.uk/~vgg/data/voxceleb/data_workshop/VoxSRC_TZ_microsoft.pdf</a>,
2019.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
X. Xiang, “The xx205 system for the voxceleb speaker recognition challenge
2020,” <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.00200</em>, 2020.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
L. Zhang, H. Zhao, Q. Meng, Y. Chen, M. Liu, and L. Xie, “Beijing zkj-npu
speaker verification system for voxceleb speaker recognition challenge
2021,” <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.03568</em>, 2021.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Q. Cai, G. Hong, Z. Ye, X. Li, and H. Li, “The kriston ai system for the
voxceleb speaker recognition challenge 2022,” <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2209.11433</em>, 2022.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Y. Zheng, Y. Zhang, C. Niu, Y. Zhan, Y. Long, and D. Xu, “Unisound system for
voxceleb speaker recognition challenge 2023,” <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2308.12526</em>, 2023.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
X. Zeng, Z. Yang, S. Wan, W. Deng, and X. Cao, “The bilibili voxceleb speaker
recognition challenge 2023 system description,”
<a target="_blank" href="https://mmai.io/datasets/voxceleb/voxsrc/data_workshop_2023/reports/xg0721_report.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://mmai.io/datasets/voxceleb/voxsrc/data_workshop_2023/reports/xg0721_report.pdf</a>,
2023.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
N. Poh and S. Bengio, “Estimating the confidence interval of expected
performance curve in biometric authentication using joint bootstrap,” in
<em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2007.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
J. Huh, H. S. Heo, J. Kang, S. Watanabe, and J. S. Chung, “Augmentation
adversarial training for unsupervised speaker recognition,” in
<em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Workshop on Self-Supervised Learning for Speech and Audio Processing,
NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
W. Wang, D. Cai, X. Qin, and M. Li, “The DKU-DukeECE systems for voxceleb
speaker recognition challenge 2020,” <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.12731</em>,
2020.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
D. Cai and M. Li, “The dku-dukeece system for the self-supervision speaker
verification task of the 2021 voxceleb speaker recognition challenge,”
<em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.02853</em>, 2021.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
J. Slavíček, A. Swart, M. Klčo, and N. Brümmer, “The
phonexia voxceleb speaker recognition challenge 2021 system description,”
<em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.02052</em>, 2021.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
J. Deng, J. Guo, T. Liu, M. Gong, and S. Zafeiriou, “Sub-center arcface:
Boosting face recognition by large-scale noisy web faces,” in <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">ECCV</em>,
2020.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
D. Cai, W. Wang, and M. Li, “An iterative framework for self-supervised deep
speaker representation learning,” in <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>.   IEEE, 2021, pp. 6728–6732.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Z. Zhao, Z. Li, W. Wang, and P. Zhang, “The hccl system for voxceleb speaker
recognition challenge 2022,” <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.12642</em>, 2023.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
X. Qin, N. Li, Y. Lin, Y. Ding, C. Weng, D. Su, and M. Li, “The dku-tencent
system for the voxceleb speaker recognition challenge 2022,” <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2210.05092</em>, 2022.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Z. Li, Y. Lin, X. Qin, N. Jiang, G. Zhao, and M. Li, “The dku-msxf speaker
verification system for the voxceleb speaker recognition challenge 2023,”
<em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.08766</em>, 2023.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
X. Xiang, “The xx205 voxceleb speaker recognition challenge 2023 system
description,”
<a target="_blank" href="https://mmai.io/datasets/voxceleb/voxsrc/data_workshop_2023/reports/xx205_report.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://mmai.io/datasets/voxceleb/voxsrc/data_workshop_2023/reports/xx205_report.pdf</a>,
2023.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
B. Sun, J. Feng, and K. Saenko, “Correlation alignment for unsupervised domain
adaptation,” in <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">Domain Adaptation in Computer Vision
Applications</em>.   Springer, 2017, pp.
153–171.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
K. A. Lee, Q. Wang, and T. Koshinaka, “The coral+ algorithm for unsupervised
domain adaptation of plda,” in <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2019.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang,
Z. Zhang, Y. Wu <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Conformer: Convolution-augmented transformer
for speech recognition,” <em id="bib.bib84.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.08100</em>, 2020.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
S. Gao, M.-M. Cheng, K. Zhao, X.-Y. Zhang, M.-H. Yang, and P. H. Torr,
“Res2net: A new multi-scale backbone architecture,” <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">IEEE transactions
on pattern analysis and machine intelligence</em>, 2019.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
D. Raj, L. P. Garcia-Perera, Z. Huang, S. Watanabe, D. Povey, A. Stolcke, and
S. Khudanpur, “Dover-lap: A method for combining overlap-aware diarization
outputs,” in <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">Proc. SLT</em>, 2021.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Korenevskaya,
I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko, I. Podluzhny,
A. Laptev, and A. Romanenko, “Target-Speaker Voice Activity Detection: A
Novel Approach for Multi-Speaker Diarization in a Dinner Party Scenario,”
in <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2020.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
X. Qin, N. Li, C. Weng, D. Su, and M. Li, “Simple attention module based
speaker verification with iterative noisy label detection,” in <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Proc.
ICASSP</em>, 2022.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
M. Cheng, W. Wang, Y. Zhang, X. Qin, and M. Li, “Target-speaker voice activity
detection via sequence-to-sequence prediction,” in <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>,
2023.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
X. Xiao, N. Kanda, Z. Chen, T. Zhou, T. Yoshioka, Y. Zhao, G. Liu, J. Wu,
J. Li, and Y. Gong, “Microsoft speaker diarization system for the voxceleb
speaker recognition challenge 2020,” <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11458</em>,
2020.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
F. Landini, O. Glembek, P. Matějka, J. Rohdin, L. Burget, M. Diez, and
A. Silnova, “Analysis of the but diarization system for voxconverse
challenge,” <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11718</em>, 2020.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
W. Wang, D. Cai, Q. Lin, L. Yang, J. Wang, J. Wang, and M. Li, “The
dku-dukeece-lenovo system for the diarization task of the 2021 voxceleb
speaker recognition challenge,” <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.02002</em>,
2021.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
K. Wang, X. Mao, H. Wu, C. Ding, C. Shang, R. Xia, and Y. Wang, “The bytedance
speaker diarization system for the voxceleb speaker recognition challenge
2021,” <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.02047</em>, 2021.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
W. Wang, X. Qin, M. Cheng, Y. Zhang, K. Wang, and M. Li, “The dku-dukeece
diarization system for the voxceleb speaker recognition challenge 2022,”
<em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.01677</em>, 2022.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
M. Cheng, W. Wang, X. Qin, Y. Lin, N. Jiang, G. Zhao, and M. Li, “The dku-msxf
diarization system for the voxceleb speaker recognition challenge 2023,”
<em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.07595</em>, 2023.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
D. Karamyan and G. Kirakosyan, “The krisp diarization system for the voxceleb
speaker recognition challenge 2023,” <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">The VoxCeleb Speaker Recognition
Challenge 2023 (VoxSRC-23)</em>, 2023.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel,
M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The kaldi
speech recognition toolkit,” in <em id="bib.bib97.2.2" class="ltx_emph ltx_font_italic">IEEE 2011 workshop on automatic speech
recognition and understanding</em>, no. CONF.   IEEE Signal Processing Society, 2011.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
R. Vogt and S. Sridharan, “Minimising speaker verification utterance length
through confidence based early verification decisions,” in <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">Advances in
Biometrics: Third International Conference, ICB 2009, Alghero, Italy, June
2-5, 2009. Proceedings 3</em>.   Springer,
2009, pp. 454–463.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
W. Xie, A. Nagrani, J. S. Chung, and A. Zisserman, “Utterance-level
aggregation for speaker recognition in the wild,” in <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">International
Conference on Acoustics, Speech, and Signal Processing</em>, 2019.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
K. Okabe, T. Koshinaka, and K. Shinoda, “Attentive statistics pooling for deep
speaker embedding,” <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.10963, 2018</em>, 2018.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
“Eventbrite webpage,” <a target="_blank" href="https://www.eventbrite.co.uk/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.eventbrite.co.uk/</a>, 2023.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
J. Kreiman and D. Sidtis, <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">Foundations of Voice Studies: An
Interdisciplinary Approach to Voice Production and Perception</em>.   John Wiley &amp; Sons, Ltd, 2011, ch. 7, pp.
237–259.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
C. S. Greenberg, A. F. Martin, G. R. Doddington, and J. J. Godfrey, “Including
human expertise in speaker recognition systems: report on a pilot
evaluation,” in <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2011, pp. 5896–5899.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang, “Phoneme
recognition using time-delay neural networks,” in
<em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">Backpropagation</em>.   Psychology
Press, 2013, pp. 35–61.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
V. Peddinti, D. Povey, and S. Khudanpur, “A time delay neural network
architecture for efficient modeling of long temporal contexts,” in
<em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Sixteenth annual conference of the international speech communication
association</em>, 2015.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
B. Rozière, N. Goyal, E. Hambro, F. Azhar <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Llama: Open
and efficient foundation language models,” <em id="bib.bib106.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,
and N. Houlsby, “An image is worth 16x16 words: Transformers for image
recognition at scale,” <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">Proc. ICLR</em>, 2021.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko,
“End-to-end object detection with transformers,” in <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">Proc. ECCV</em>,
2020.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
A. Askell, P. Mishkin, J. Clark <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Learning transferable visual
models from natural language supervision,” in <em id="bib.bib109.2.2" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2021.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
D. Garcia-Romero, D. Snyder, G. Sell, D. Povey, and A. McCree, “Speaker
diarization using deep neural network embeddings,” in <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>,
2017, pp. 4930–4934.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Q. Wang, C. Downey, L. Wan, P. A. Mansfield, and I. L. Moreno, “Speaker
diarization with lstm,” in <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2018.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully supervised speaker
diarization,” in <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2019.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Y. Kwon, H. S. Heo, J. Huh, B.-J. Lee, and J. S. Chung, “Look who’s not
talking,” in <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">SLT</em>, 2021.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and S. Watanabe, “End-to-end
neural speaker diarization with permutation-free objectives,” <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">Proc.
Interspeech</em>, 2019.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Nagamatsu, “End-to-end
speaker diarization for an unknown number of speakers with encoder-decoder
based attractors,” <em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2020.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
H. Bredin, “pyannote. audio 2.1 speaker diarization pipeline: principle,
benchmark, and recipe,” in <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2023.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
S. Cornell, J.-w. Jung, S. Watanabe, and S. Squartini, “One model to rule them
all? towards end-to-end joint speaker diarization and speech recognition,”
<em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.01688</em>, 2023.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
T. J. Park, N. Kanda, D. Dimitriadis, K. J. Han, S. Watanabe, and S. Narayanan,
“A review of speaker diarization: Recent advances with deep learning,”
<em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em>, vol. 72, p. 101317, 2022.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
K. Kinoshita, M. Delcroix, and N. Tawara, “Integrating end-to-end neural and
clustering-based diarization: Getting the best of both worlds,” in
<em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2021.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
S. Baroudi, H. Bredin, A. Plaquet, and T. Pellegrini, “pyannote.audio speaker
diarization pipeline at voxsrc 2023,”
<a target="_blank" href="https://mmai.io/datasets/voxceleb/voxsrc/data_workshop_2023/reports/pyannote_report.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://mmai.io/datasets/voxceleb/voxsrc/data_workshop_2023/reports/pyannote_report.pdf</a>,
2023.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever,
“Robust speech recognition via large-scale weak supervision,” in
<em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2023.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
“Kaggle website,” <a target="_blank" href="https://www.kaggle.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/</a>.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
D. Yadav, R. Jain, H. Agrawal, P. Chattopadhyay, T. Singh, A. Jain, S. B.
Singh, S. Lee, and D. Batra, “Evalai: Towards better evaluation systems for
ai agents,” <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.03570</em>, 2019.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
M. A. Ganaie, M. Hu, A. Malik, M. Tanveer, and P. Suganthan, “Ensemble deep
learning: A review,” <em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">Engineering Applications of Artificial
Intelligence</em>, vol. 115, p. 105151, 2022.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
M. Everingham, S. M. A. Eslami, L. V. Gool, C. K. I. Williams, J. Winn, and
A. Zisserman, “The pascal visual object classes challenge: A
retrospective,” <em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">IJCV</em>, vol. 111, no. 1, pp. 98–136, Jan 2015.

</span>
</li>
</ul>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Biography Section</h2>

<figure id="A4.tab1" class="ltx_float biography">
<table id="A4.tab1.1" class="ltx_tabular">
<tr id="A4.tab1.1.1" class="ltx_tr">
<td id="A4.tab1.1.1.1" class="ltx_td">
<span id="A4.tab1.1.1.1.1" class="ltx_inline-block">
<span id="A4.tab1.1.1.1.1.1" class="ltx_p"><span id="A4.tab1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Jaesung Huh</span>  is currently working towards DPhil degree in Visual Geometry Group, University of Oxford, supervised by Andrew Zisserman. Before that, he used to be a research engineer at Naver Corporation. His research focuses on audio-visual learning and video understanding. He was one of the main organisers of VoxSRC workshops.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="A4.tab2" class="ltx_float biography">
<table id="A4.tab2.1" class="ltx_tabular">
<tr id="A4.tab2.1.1" class="ltx_tr">
<td id="A4.tab2.1.1.1" class="ltx_td">
<span id="A4.tab2.1.1.1.1" class="ltx_inline-block">
<span id="A4.tab2.1.1.1.1.1" class="ltx_p"><span id="A4.tab2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Joon Son Chung</span> 
is an assistant professor at Korea Advanced Institute of Science and Technology, where he is directing research in speech processing, computer vision and machine learning. He received the D.Phil. in Engineering Science from the University of Oxford.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="A4.tab3" class="ltx_float biography">
<table id="A4.tab3.1" class="ltx_tabular">
<tr id="A4.tab3.1.1" class="ltx_tr">
<td id="A4.tab3.1.1.1" class="ltx_td">
<span id="A4.tab3.1.1.1.1" class="ltx_inline-block">
<span id="A4.tab3.1.1.1.1.1" class="ltx_p"><span id="A4.tab3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Arsha Nagrani</span>  is a Senior Research Scientist at Google Research. She obtained her PhD from the VGG group in the University of Oxford, where her thesis won the ELLIS PhD Award. Her research focuses on cross-modal and multi-modal machine learning techniques for video recognition. Her work has been recognised by a Best Student Paper Award, Outstanding Paper Award, a Google PhD Fellowship and a Townsend Scholarship, and has been covered by major outlets such as The New Scientist, MIT Tech review and Verdict.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="A4.tab4" class="ltx_float biography">
<table id="A4.tab4.1" class="ltx_tabular">
<tr id="A4.tab4.1.1" class="ltx_tr">
<td id="A4.tab4.1.1.1" class="ltx_td">
<span id="A4.tab4.1.1.1.1" class="ltx_inline-block">
<span id="A4.tab4.1.1.1.1.1" class="ltx_p"><span id="A4.tab4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Andrew Brown</span>  completed his PhD in computer vision and machine learning in the University of Oxford’s Visual Geometry Group under the supervision of Professor Andrew Zisserman.
His research during his PhD focused on end-to-end learning and audio-visual human-centric video understanding.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="A4.tab5" class="ltx_float biography">
<table id="A4.tab5.1" class="ltx_tabular">
<tr id="A4.tab5.1.1" class="ltx_tr">
<td id="A4.tab5.1.1.1" class="ltx_td">
<span id="A4.tab5.1.1.1.1" class="ltx_inline-block">
<span id="A4.tab5.1.1.1.1.1" class="ltx_p"><span id="A4.tab5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Jee-weon Jung</span>  is a Postdoctoral Research Associate at Carnegie Mellon University, USA. He received his PhD degree from the University of Seoul, Republic of Korea. Before he joined Carnegie Mellon University, he was a research scientist at Naver Corporation, Republic of Korea. He has worked on speaker recognition, acoustic scene classification, audio spoofing detection, and other tasks. He was the main organiser of the Spoofing-Aware Speaker Verification Challenge, a special session at Interspeech 2022. He is one of the organisers of VoxSRC and ASVspoof since 2022.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="A4.tab6" class="ltx_float biography">
<table id="A4.tab6.1" class="ltx_tabular">
<tr id="A4.tab6.1.1" class="ltx_tr">
<td id="A4.tab6.1.1.1" class="ltx_td">
<span id="A4.tab6.1.1.1.1" class="ltx_inline-block">
<span id="A4.tab6.1.1.1.1.1" class="ltx_p"><span id="A4.tab6.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Daniel Garcia-Romero</span>  is a Principal Scientist at Amazon. Prior to that, he was a Senior Research Scientist at the Human Language Technology Center of Excellence (HLTCOE), Johns Hopkins University. He obtained his PhD in Electrical Engineering at the University of Maryland, College Park. His research interests are in the broad areas of speech processing, deep learning, and multi-modal person identification. For the past few years he has been working on deep neural networks for speaker, language recognition, and diarization. He is co-inventor of the x-vector embeddings that have set the state of the art in these fields.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="A4.tab7" class="ltx_float biography">
<table id="A4.tab7.1" class="ltx_tabular">
<tr id="A4.tab7.1.1" class="ltx_tr">
<td id="A4.tab7.1.1.1" class="ltx_td">
<span id="A4.tab7.1.1.1.1" class="ltx_inline-block">
<span id="A4.tab7.1.1.1.1.1" class="ltx_p"><span id="A4.tab7.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Andrew Zisserman</span>  is a Royal Society Research Professor and the Professor of Computer Vision Engineering at the Department of Engineering Science at the University of Oxford. His research interests have included multiple view geometry, audio and visual recognition, and large scale retrieval in images and video. His papers have won many best paper and test-of-time awards at international conferences.</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.14885" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.14886" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.14886">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.14886" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.14887" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 17:27:51 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
