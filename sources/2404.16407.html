<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.16407] U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF</title><meta property="og:description" content="Scale has opened new frontiers in natural language processing, but at a high cost. In response, by learning to only activate a subset of parameters in training and inference, Mixture-of-Experts (MoE) [1, 2] have been p‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.16407">

<!--Generated on Sun May  5 16:57:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Scale has opened new frontiers in natural language processing, but at a high cost. In response, by learning to only activate a subset of parameters in training and inference, Mixture-of-Experts (MoE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> have been proposed as an energy efficient path to even larger and more capable language models and this shift towards a new generation of foundation models is gaining momentum, particularly within the field of Automatic Speech Recognition (ASR). Recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> that incorporating MoE into ASR models have complex designs such as routing frames via supplementary embedding network, improving multilingual ability for the experts, and utilizing dedicated auxiliary losses for either expert load balancing or specific language handling. We found that delicate designs are not necessary, while an embarrassingly simple substitution of MoE layers for all Feed-Forward Network (FFN) layers is competent for the ASR task. To be more specific, we benchmark our proposed model on a large scale inner-source dataset (160k hours), the results show that we can scale our baseline Conformer (Dense-225M) to its MoE counterparts (MoE-1B) and achieve Dense-1B level Word Error Rate (WER) while maintaining a Dense-225M level Real Time Factor (RTF).
Furthermore, by applying Unified 2-pass framework with bidirectional attention decoders (U2++) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, we achieve the streaming and non-streaming decoding modes in a single MoE based model, which we call U2++ MoE.
We hope that our study can facilitate the research on scaling speech foundation models without sacrificing deployment efficiency.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">‚Äî‚Äâ</span></span>
speech recognition, mixture-of-expert, streaming</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Scaling up neural network models has recently received great attention, given the significant quality improvements on a variety of tasks including natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and speech processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While training massive models on large amounts of data can almost guarantee improved quality, there are two factors affecting their practicality and applicability: (1) <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">training efficiency</span> and (2) <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">inference efficiency</span>. Large dense models are often prohibitively compute-intensive to train, with some models requiring TFlops-days of compute <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. A recent line of work has proposed sparsely-gated Mixture-of-Experts (MoE) layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> as an efficient alternative to dense models in order to address both training and inference efficiency limitations.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">There have been several related Mixture-of-Expert approaches for ASR modeling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. In those models each frame of the input sequence activates a different subset of the experts, hence the computation cost per frame becomes only proportional to the size of the activated sub-network. To avoid collapse to just a few experts while ignoring all others, all of those works use load balancing mechanisms such as dedicated auxiliary losses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Nonetheless, the resulting complex optimization objectives often lead to a large amount of hyper parameter tuning, such as the weight of each auxiliary loss. Moreover, load balancing is designed to address the issue of expert sparsity in the NLP field when routing different tokens. However, this issue may not hold in the speech domain, as there is a high degree of similarity between neighboring speech frames <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Forcing speech frames to be evenly distributed among all experts does not align with intuition, as it conflicts with the natural continuity observed in the relationships between adjacent speech frames.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.16407/assets/pic/u2pp-moe.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="441" height="303" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.8.1.1" class="ltx_text ltx_font_bold">Fig.¬†1</span>: </span>The proposed U2++ MoE, a unified (streaming and non-streaming) two-pass (encoder for 1st pass decoding and decoder for 2nd pass rescoring) joint CTC/AED framework, enhanced with bidirectional decoders and Mixture-of-Experts. For efficient compression of speech frames, we employ 1/8 subsampling and structure our architecture with <math id="S1.F1.4.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S1.F1.4.m1.1b"><mi id="S1.F1.4.m1.1.1" xref="S1.F1.4.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S1.F1.4.m1.1c"><ci id="S1.F1.4.m1.1.1.cmml" xref="S1.F1.4.m1.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m1.1d">M</annotation></semantics></math> encoder layers alongside <math id="S1.F1.5.m2.1" class="ltx_Math" alttext="2N" display="inline"><semantics id="S1.F1.5.m2.1b"><mrow id="S1.F1.5.m2.1.1" xref="S1.F1.5.m2.1.1.cmml"><mn id="S1.F1.5.m2.1.1.2" xref="S1.F1.5.m2.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S1.F1.5.m2.1.1.1" xref="S1.F1.5.m2.1.1.1.cmml">‚Äã</mo><mi id="S1.F1.5.m2.1.1.3" xref="S1.F1.5.m2.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.5.m2.1c"><apply id="S1.F1.5.m2.1.1.cmml" xref="S1.F1.5.m2.1.1"><times id="S1.F1.5.m2.1.1.1.cmml" xref="S1.F1.5.m2.1.1.1"></times><cn type="integer" id="S1.F1.5.m2.1.1.2.cmml" xref="S1.F1.5.m2.1.1.2">2</cn><ci id="S1.F1.5.m2.1.1.3.cmml" xref="S1.F1.5.m2.1.1.3">ùëÅ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.5.m2.1d">2N</annotation></semantics></math> decoder layers, wherein equal divisions of <math id="S1.F1.6.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S1.F1.6.m3.1b"><mi id="S1.F1.6.m3.1.1" xref="S1.F1.6.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S1.F1.6.m3.1c"><ci id="S1.F1.6.m3.1.1.cmml" xref="S1.F1.6.m3.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.6.m3.1d">N</annotation></semantics></math> layers are allocated to both the right-to-left and left-to-right decoders.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Despite several notable successes of speech MoE, wide-spread adoption has been hindered by training complexity and the lack of streaming capabilities. We address these with the introduction of U2++ MoE. We simplify the integration of MoE and preclude the necessity for any auxiliary losses. Our proposed method mitigate the complexities, and we show large sparse models may be trained, for the first time, with unified streaming &amp; non-streaming fashion.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Several mixture-of-expert strategies have been developed for enhancing ASR modeling, but our work differs from them in the following ways.</p>
</div>
<div id="S2.p2" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span> 
<div id="S2.I1.ix1.p1" class="ltx_para">
<p id="S2.I1.ix1.p1.1" class="ltx_p">In contrast to all prior studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, our MoE model do not include any auxiliary losses for expert routing, thus significantly streamlining the training optimization process.</p>
</div>
</li>
<li id="S2.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span> 
<div id="S2.I1.ix2.p1" class="ltx_para">
<p id="S2.I1.ix2.p1.1" class="ltx_p">Compared to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, our MoE study also works without using any shared embedding networks, thereby simplifying the model architecture and enhancing its generality for model scaling.</p>
</div>
</li>
<li id="S2.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3)</span> 
<div id="S2.I1.ix3.p1" class="ltx_para">
<p id="S2.I1.ix3.p1.1" class="ltx_p">Compared to all previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> that exclusively explored the application of MoE layers within the encoder, our study extends this innovation by integrating MoE layers into the decoder‚Äôs FFN as well. Notably, <span id="S2.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">You et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> have also attempted to modify all FFN modules in encoder into MoE layers, but it fails to achieve a better performance (<span id="S2.I1.ix3.p1.1.2" class="ltx_text ltx_font_italic">detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, section 3.2, paragraph 1, last sentence</span>). In contrast, we are the first to demonstrate the effectiveness of MoE layer substitution across both encoder and decoder components.</p>
</div>
</li>
<li id="S2.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4)</span> 
<div id="S2.I1.ix4.p1" class="ltx_para">
<p id="S2.I1.ix4.p1.1" class="ltx_p">We are the pioneers in demonstrating the streaming capability of the MoE. While <span id="S2.I1.ix4.p1.1.1" class="ltx_text ltx_font_italic">Hu et al</span>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> have made attempts to integrate MoE layers into a causal encoder to enable streaming recognition, their approach resulted in a notable deterioration in the average WER (<span id="S2.I1.ix4.p1.1.2" class="ltx_text ltx_font_italic">detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, section 5.1.1, paragraph 2, first sentence</span>). In stark contrast, our approach, which marries the MoE-based Conformer with the U2++ framework, successfully facilitates both streaming and non-streaming decoding modes within a singular MoE-based model.</p>
</div>
</li>
<li id="S2.I1.ix5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5)</span> 
<div id="S2.I1.ix5.p1" class="ltx_para">
<p id="S2.I1.ix5.p1.1" class="ltx_p">Our research primarily emphasizes scaling models without a notable increase on RTF, diverging from prior efforts that predominantly concentrate on enhancing the accuracy of multi-lingual or multi-accent recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. These studies lack a comprehensive analysis of inference latency, such as Dense-1B model v.s. MoE-1B model or Dense-225M model v.s. MoE-1B model. In this paper, however, we demonstrate that a MoE-1B model can achieve the accuracy of a Dense-1B model while maintaining the inference efficiency of a Dense-225M model.</p>
</div>
</li>
</ul>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In summary, our guiding principle has been to <span id="S2.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">keeping MoE model as simple as possible and is thus more generic for scaling up models</span>. Our model do not require any auxiliary losses or any additional embedding networks. By applying 1) an embarrassingly simple replacement of all FFN layers with MoE layers and 2) the U2++ framework to Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, we prove that MoE-1B model can achieve Dense-1B level accuracy with Dense-225M level inference cost, alongside the capability for streaming.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our model uses Conformer (for encoders) and Transformer (for decoders) as the main building block. A Conformer encoder layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> consists of a multi-headed self-attention and a convolution-based layer sandwiched by two FFN. A Transformer decoder layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> consists of a multi-headed self-attention, a multi-headed src-attention and one FFN. As shown in Fig.<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, to incorporate experts, we use an MoE layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to replace all FFN in the encoders and decoders. Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the MoE layer consists of a routing network and multiple experts, each of which is an FFN.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.2" class="ltx_p">We use the joint Connectionist Temporal Classification (CTC) loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and Autoregressive Encoder Decoder (AED) loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> for training the proposed model. The combined loss has two hyper parameters (<math id="S3.p2.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\lambda</annotation></semantics></math> and <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\alpha</annotation></semantics></math>) to balance the importance of different losses (<span id="S3.p2.2.1" class="ltx_text ltx_font_italic">more details can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, section 2.1</span>):</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="L=\lambda L_{CTC}+(1-\lambda)(\alpha L_{AED}^{right2left}+(1-\alpha)L_{AED}^{left2right})" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mi id="S3.E1.m1.2.2.4" xref="S3.E1.m1.2.2.4.cmml">L</mi><mo id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">=</mo><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mrow id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.4.cmml"><mi id="S3.E1.m1.2.2.2.4.2" xref="S3.E1.m1.2.2.2.4.2.cmml">Œª</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.4.1" xref="S3.E1.m1.2.2.2.4.1.cmml">‚Äã</mo><msub id="S3.E1.m1.2.2.2.4.3" xref="S3.E1.m1.2.2.2.4.3.cmml"><mi id="S3.E1.m1.2.2.2.4.3.2" xref="S3.E1.m1.2.2.2.4.3.2.cmml">L</mi><mrow id="S3.E1.m1.2.2.2.4.3.3" xref="S3.E1.m1.2.2.2.4.3.3.cmml"><mi id="S3.E1.m1.2.2.2.4.3.3.2" xref="S3.E1.m1.2.2.2.4.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.4.3.3.1" xref="S3.E1.m1.2.2.2.4.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.4.3.3.3" xref="S3.E1.m1.2.2.2.4.3.3.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.4.3.3.1a" xref="S3.E1.m1.2.2.2.4.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.4.3.3.4" xref="S3.E1.m1.2.2.2.4.3.3.4.cmml">C</mi></mrow></msub></mrow><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">+</mo><mrow id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">Œª</mi></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.3.cmml">‚Äã</mo><mrow id="S3.E1.m1.2.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.2.1.1.3.cmml"><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.2" xref="S3.E1.m1.2.2.2.2.2.1.1.3.2.cmml">Œ±</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.3.1" xref="S3.E1.m1.2.2.2.2.2.1.1.3.1.cmml">‚Äã</mo><msubsup id="S3.E1.m1.2.2.2.2.2.1.1.3.3" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.cmml"><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.2" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.2.cmml">L</mi><mrow id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.cmml"><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.2" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.1" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.3" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.1a" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.4" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.4.cmml">D</mi></mrow><mrow id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.cmml"><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.2" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.3" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1a" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.4" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1b" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.5" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.5.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1c" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.6" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1d" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1.cmml">‚Äã</mo><mn id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.7" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.7.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1e" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.8" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1f" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.9" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1g" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.10" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.10.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1h" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.11" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.11.cmml">t</mi></mrow></msubsup></mrow><mo id="S3.E1.m1.2.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.2.1.1.2.cmml">+</mo><mrow id="S3.E1.m1.2.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.2.1.1.1.cmml"><mrow id="S3.E1.m1.2.2.2.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.cmml"><mn id="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.cmml">‚àí</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.3.cmml">Œ±</mi></mrow><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.2.2.1.1.1.2.cmml">‚Äã</mo><msubsup id="S3.E1.m1.2.2.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.2" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.2.cmml">L</mi><mrow id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.2" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.1" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.3" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.1a" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.4" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.4.cmml">D</mi></mrow><mrow id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.cmml"><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.2" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.3" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1a" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.4" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1b" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.5" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1c" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1.cmml">‚Äã</mo><mn id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.6" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.6.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1d" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.7" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1e" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.8" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1f" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.9" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.9.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1g" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.10" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.10.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1h" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.11" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.11.cmml">t</mi></mrow></msubsup></mrow></mrow><mo stretchy="false" id="S3.E1.m1.2.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><eq id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3"></eq><ci id="S3.E1.m1.2.2.4.cmml" xref="S3.E1.m1.2.2.4">ùêø</ci><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><plus id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></plus><apply id="S3.E1.m1.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.4"><times id="S3.E1.m1.2.2.2.4.1.cmml" xref="S3.E1.m1.2.2.2.4.1"></times><ci id="S3.E1.m1.2.2.2.4.2.cmml" xref="S3.E1.m1.2.2.2.4.2">ùúÜ</ci><apply id="S3.E1.m1.2.2.2.4.3.cmml" xref="S3.E1.m1.2.2.2.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.4.3.1.cmml" xref="S3.E1.m1.2.2.2.4.3">subscript</csymbol><ci id="S3.E1.m1.2.2.2.4.3.2.cmml" xref="S3.E1.m1.2.2.2.4.3.2">ùêø</ci><apply id="S3.E1.m1.2.2.2.4.3.3.cmml" xref="S3.E1.m1.2.2.2.4.3.3"><times id="S3.E1.m1.2.2.2.4.3.3.1.cmml" xref="S3.E1.m1.2.2.2.4.3.3.1"></times><ci id="S3.E1.m1.2.2.2.4.3.3.2.cmml" xref="S3.E1.m1.2.2.2.4.3.3.2">ùê∂</ci><ci id="S3.E1.m1.2.2.2.4.3.3.3.cmml" xref="S3.E1.m1.2.2.2.4.3.3.3">ùëá</ci><ci id="S3.E1.m1.2.2.2.4.3.3.4.cmml" xref="S3.E1.m1.2.2.2.4.3.3.4">ùê∂</ci></apply></apply></apply><apply id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2"><times id="S3.E1.m1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.3"></times><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">ùúÜ</ci></apply><apply id="S3.E1.m1.2.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1"><plus id="S3.E1.m1.2.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.2"></plus><apply id="S3.E1.m1.2.2.2.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3"><times id="S3.E1.m1.2.2.2.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.1"></times><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.2">ùõº</ci><apply id="S3.E1.m1.2.2.2.2.2.1.1.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3">superscript</csymbol><apply id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.2">ùêø</ci><apply id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3"><times id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.1"></times><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.2">ùê¥</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.3">ùê∏</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.4.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.2.3.4">ùê∑</ci></apply></apply><apply id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3"><times id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.1"></times><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.2">ùëü</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.3">ùëñ</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.4.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.4">ùëî</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.5.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.5">‚Ñé</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.6.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.6">ùë°</ci><cn type="integer" id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.7.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.7">2</cn><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.8.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.8">ùëô</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.9.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.9">ùëí</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.10.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.10">ùëì</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.11.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.3.3.3.11">ùë°</ci></apply></apply></apply><apply id="S3.E1.m1.2.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1"><times id="S3.E1.m1.2.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.2"></times><apply id="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.1.1"><minus id="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.2">1</cn><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.1.1.1.3">ùõº</ci></apply><apply id="S3.E1.m1.2.2.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.2">ùêø</ci><apply id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3"><times id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.1"></times><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.2">ùê¥</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.3">ùê∏</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.4.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.2.3.4">ùê∑</ci></apply></apply><apply id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3"><times id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.1"></times><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.2.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.2">ùëô</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.3.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.3">ùëí</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.4.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.4">ùëì</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.5.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.5">ùë°</ci><cn type="integer" id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.6.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.6">2</cn><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.7.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.7">ùëü</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.8.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.8">ùëñ</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.9.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.9">ùëî</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.10.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.10">‚Ñé</ci><ci id="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.11.cmml" xref="S3.E1.m1.2.2.2.2.2.1.1.1.3.3.11">ùë°</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">L=\lambda L_{CTC}+(1-\lambda)(\alpha L_{AED}^{right2left}+(1-\alpha)L_{AED}^{left2right})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Similar to U2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, we adopt the dynamic chunk masking strategy to unify the streaming and non-streaming modes. Firstly, the input is split into several chunks by a fixed chunk size <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">ùê∂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">C</annotation></semantics></math> and every chunk attends on itself and all the previous chunks, so the whole latency for the CTC decoding in the first pass only depends on the chunk size. When the chunk size is limited, it works in a streaming way; otherwise it works in a non-streaming way. Secondly, the chunk size is varied dynamically from 1 to the max length of the current training utterance in the training, so the trained model learns to predict with arbitrary chunk size.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Our training corpus comprises mixed datasets gathered from a variety of application domains, amounting to a substantial 160k hours of large-scale, industrial-level training data. This corpus consists predominantly of Mandarin (90%) with the remainder in English (10%).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">To evaluate the capabilities of the proposed method, we use the most widely used benchmark for the Mandarin ASR task, namely SpeechIO TIOBE ASR Benchmark <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/SpeechColab/Leaderboard</span></span></span>. SpeechIO test sets are carefully curated by SpeechIO authors, crawled from publicly available sources (Youtube, TV programs, Podcast etc), covering various well-known scenarios and topics (TV News, VLog, Documentary and so on), transcribed by payed professional annotators thus is exceptionally suitable for testing a model‚Äôs general speech recognition capabilities. Cumulatively, the 26 publicly available SpeechIO test sets amount to 60.2 hours, averaging 2.3 hours of data across each domain.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In all experiments, we utilize 80-dimensional log-mel filterbank features, computed using a 25ms window that is shifted every 10ms. Each frame undergoes global mean and variance normalization. For modeling Mandarin, we employ character-based representations, whereas for English, we utilize byte-pair encoding (BPE), culminating in a comprehensive vocabulary of 6000 units. All our experiments are conducted in WeNet toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> with DeepSpeed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> enabled, all the models are trained using 8 * NVIDIA 3090 (24GB) GPUs.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.4" class="ltx_p">We have developed three distinct models, as detailed in Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4.2 Training Details ‚Ä£ 4 Experiments ‚Ä£ U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, all of which adopt the parameters <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="Head=8" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mrow id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2.2" xref="S4.SS2.p2.1.m1.1.1.2.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.2.1" xref="S4.SS2.p2.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.p2.1.m1.1.1.2.3" xref="S4.SS2.p2.1.m1.1.1.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.2.1a" xref="S4.SS2.p2.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.p2.1.m1.1.1.2.4" xref="S4.SS2.p2.1.m1.1.1.2.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.2.1b" xref="S4.SS2.p2.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.p2.1.m1.1.1.2.5" xref="S4.SS2.p2.1.m1.1.1.2.5.cmml">d</mi></mrow><mo id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><eq id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></eq><apply id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2"><times id="S4.SS2.p2.1.m1.1.1.2.1.cmml" xref="S4.SS2.p2.1.m1.1.1.2.1"></times><ci id="S4.SS2.p2.1.m1.1.1.2.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2.2">ùêª</ci><ci id="S4.SS2.p2.1.m1.1.1.2.3.cmml" xref="S4.SS2.p2.1.m1.1.1.2.3">ùëí</ci><ci id="S4.SS2.p2.1.m1.1.1.2.4.cmml" xref="S4.SS2.p2.1.m1.1.1.2.4">ùëé</ci><ci id="S4.SS2.p2.1.m1.1.1.2.5.cmml" xref="S4.SS2.p2.1.m1.1.1.2.5">ùëë</ci></apply><cn type="integer" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">Head=8</annotation></semantics></math>, <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="CNN_{kernel}=15" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mrow id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2.2" xref="S4.SS2.p2.2.m2.1.1.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.2.1" xref="S4.SS2.p2.2.m2.1.1.2.1.cmml">‚Äã</mo><mi id="S4.SS2.p2.2.m2.1.1.2.3" xref="S4.SS2.p2.2.m2.1.1.2.3.cmml">N</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.2.1a" xref="S4.SS2.p2.2.m2.1.1.2.1.cmml">‚Äã</mo><msub id="S4.SS2.p2.2.m2.1.1.2.4" xref="S4.SS2.p2.2.m2.1.1.2.4.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2.4.2" xref="S4.SS2.p2.2.m2.1.1.2.4.2.cmml">N</mi><mrow id="S4.SS2.p2.2.m2.1.1.2.4.3" xref="S4.SS2.p2.2.m2.1.1.2.4.3.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2.4.3.2" xref="S4.SS2.p2.2.m2.1.1.2.4.3.2.cmml">k</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.2.4.3.1" xref="S4.SS2.p2.2.m2.1.1.2.4.3.1.cmml">‚Äã</mo><mi id="S4.SS2.p2.2.m2.1.1.2.4.3.3" xref="S4.SS2.p2.2.m2.1.1.2.4.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.2.4.3.1a" xref="S4.SS2.p2.2.m2.1.1.2.4.3.1.cmml">‚Äã</mo><mi id="S4.SS2.p2.2.m2.1.1.2.4.3.4" xref="S4.SS2.p2.2.m2.1.1.2.4.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.2.4.3.1b" xref="S4.SS2.p2.2.m2.1.1.2.4.3.1.cmml">‚Äã</mo><mi id="S4.SS2.p2.2.m2.1.1.2.4.3.5" xref="S4.SS2.p2.2.m2.1.1.2.4.3.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.2.4.3.1c" xref="S4.SS2.p2.2.m2.1.1.2.4.3.1.cmml">‚Äã</mo><mi id="S4.SS2.p2.2.m2.1.1.2.4.3.6" xref="S4.SS2.p2.2.m2.1.1.2.4.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.2.4.3.1d" xref="S4.SS2.p2.2.m2.1.1.2.4.3.1.cmml">‚Äã</mo><mi id="S4.SS2.p2.2.m2.1.1.2.4.3.7" xref="S4.SS2.p2.2.m2.1.1.2.4.3.7.cmml">l</mi></mrow></msub></mrow><mo id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><eq id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></eq><apply id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2"><times id="S4.SS2.p2.2.m2.1.1.2.1.cmml" xref="S4.SS2.p2.2.m2.1.1.2.1"></times><ci id="S4.SS2.p2.2.m2.1.1.2.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2.2">ùê∂</ci><ci id="S4.SS2.p2.2.m2.1.1.2.3.cmml" xref="S4.SS2.p2.2.m2.1.1.2.3">ùëÅ</ci><apply id="S4.SS2.p2.2.m2.1.1.2.4.cmml" xref="S4.SS2.p2.2.m2.1.1.2.4"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.2.4.1.cmml" xref="S4.SS2.p2.2.m2.1.1.2.4">subscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.2.4.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2.4.2">ùëÅ</ci><apply id="S4.SS2.p2.2.m2.1.1.2.4.3.cmml" xref="S4.SS2.p2.2.m2.1.1.2.4.3"><times id="S4.SS2.p2.2.m2.1.1.2.4.3.1.cmml" xref="S4.SS2.p2.2.m2.1.1.2.4.3.1"></times><ci id="S4.SS2.p2.2.m2.1.1.2.4.3.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2.4.3.2">ùëò</ci><ci id="S4.SS2.p2.2.m2.1.1.2.4.3.3.cmml" xref="S4.SS2.p2.2.m2.1.1.2.4.3.3">ùëí</ci><ci id="S4.SS2.p2.2.m2.1.1.2.4.3.4.cmml" xref="S4.SS2.p2.2.m2.1.1.2.4.3.4">ùëü</ci><ci id="S4.SS2.p2.2.m2.1.1.2.4.3.5.cmml" xref="S4.SS2.p2.2.m2.1.1.2.4.3.5">ùëõ</ci><ci id="S4.SS2.p2.2.m2.1.1.2.4.3.6.cmml" xref="S4.SS2.p2.2.m2.1.1.2.4.3.6">ùëí</ci><ci id="S4.SS2.p2.2.m2.1.1.2.4.3.7.cmml" xref="S4.SS2.p2.2.m2.1.1.2.4.3.7">ùëô</ci></apply></apply></apply><cn type="integer" id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">CNN_{kernel}=15</annotation></semantics></math>, <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="\lambda=0.3" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">Œª</mi><mo id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">0.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><eq id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></eq><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">ùúÜ</ci><cn type="float" id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">\lambda=0.3</annotation></semantics></math>, and <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="\alpha=0.3" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mi id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">Œ±</mi><mo id="S4.SS2.p2.4.m4.1.1.1" xref="S4.SS2.p2.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml">0.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><eq id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1.1"></eq><ci id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">ùõº</ci><cn type="float" id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">\alpha=0.3</annotation></semantics></math>. In the context of the MoE layer, we configure it with 8 experts and enable only the top two experts during both the training and inference phases. For the decoding process, the CTC decoder initially generates the N-Best hypotheses during the first pass. Subsequently, these hypotheses are rescored by the attention decoder in the second pass to produce the final outcomes.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.6.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Configuration of different models.</figcaption>
<div id="S4.T1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:205.8pt;height:64.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.4pt,3.6pt) scale(0.9,0.9) ;">
<table id="S4.T1.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.4.4.4" class="ltx_tr">
<th id="S4.T1.4.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">(a) Model</th>
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">(b) <math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mi id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">M</annotation></semantics></math>
</th>
<th id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">(c) <math id="S4.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.T1.2.2.2.2.m1.1a"><mi id="S4.T1.2.2.2.2.m1.1.1" xref="S4.T1.2.2.2.2.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">N</annotation></semantics></math>
</th>
<th id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">(d) <math id="S4.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="d^{ff}" display="inline"><semantics id="S4.T1.3.3.3.3.m1.1a"><msup id="S4.T1.3.3.3.3.m1.1.1" xref="S4.T1.3.3.3.3.m1.1.1.cmml"><mi id="S4.T1.3.3.3.3.m1.1.1.2" xref="S4.T1.3.3.3.3.m1.1.1.2.cmml">d</mi><mrow id="S4.T1.3.3.3.3.m1.1.1.3" xref="S4.T1.3.3.3.3.m1.1.1.3.cmml"><mi id="S4.T1.3.3.3.3.m1.1.1.3.2" xref="S4.T1.3.3.3.3.m1.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.T1.3.3.3.3.m1.1.1.3.1" xref="S4.T1.3.3.3.3.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S4.T1.3.3.3.3.m1.1.1.3.3" xref="S4.T1.3.3.3.3.m1.1.1.3.3.cmml">f</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b"><apply id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.3.3.3.3.m1.1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">superscript</csymbol><ci id="S4.T1.3.3.3.3.m1.1.1.2.cmml" xref="S4.T1.3.3.3.3.m1.1.1.2">ùëë</ci><apply id="S4.T1.3.3.3.3.m1.1.1.3.cmml" xref="S4.T1.3.3.3.3.m1.1.1.3"><times id="S4.T1.3.3.3.3.m1.1.1.3.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1.3.1"></times><ci id="S4.T1.3.3.3.3.m1.1.1.3.2.cmml" xref="S4.T1.3.3.3.3.m1.1.1.3.2">ùëì</ci><ci id="S4.T1.3.3.3.3.m1.1.1.3.3.cmml" xref="S4.T1.3.3.3.3.m1.1.1.3.3">ùëì</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">d^{ff}</annotation></semantics></math>
</th>
<th id="S4.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">(e) <math id="S4.T1.4.4.4.4.m1.1" class="ltx_Math" alttext="d^{att}" display="inline"><semantics id="S4.T1.4.4.4.4.m1.1a"><msup id="S4.T1.4.4.4.4.m1.1.1" xref="S4.T1.4.4.4.4.m1.1.1.cmml"><mi id="S4.T1.4.4.4.4.m1.1.1.2" xref="S4.T1.4.4.4.4.m1.1.1.2.cmml">d</mi><mrow id="S4.T1.4.4.4.4.m1.1.1.3" xref="S4.T1.4.4.4.4.m1.1.1.3.cmml"><mi id="S4.T1.4.4.4.4.m1.1.1.3.2" xref="S4.T1.4.4.4.4.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T1.4.4.4.4.m1.1.1.3.1" xref="S4.T1.4.4.4.4.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S4.T1.4.4.4.4.m1.1.1.3.3" xref="S4.T1.4.4.4.4.m1.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.T1.4.4.4.4.m1.1.1.3.1a" xref="S4.T1.4.4.4.4.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S4.T1.4.4.4.4.m1.1.1.3.4" xref="S4.T1.4.4.4.4.m1.1.1.3.4.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.m1.1b"><apply id="S4.T1.4.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.4.4.4.4.m1.1.1.1.cmml" xref="S4.T1.4.4.4.4.m1.1.1">superscript</csymbol><ci id="S4.T1.4.4.4.4.m1.1.1.2.cmml" xref="S4.T1.4.4.4.4.m1.1.1.2">ùëë</ci><apply id="S4.T1.4.4.4.4.m1.1.1.3.cmml" xref="S4.T1.4.4.4.4.m1.1.1.3"><times id="S4.T1.4.4.4.4.m1.1.1.3.1.cmml" xref="S4.T1.4.4.4.4.m1.1.1.3.1"></times><ci id="S4.T1.4.4.4.4.m1.1.1.3.2.cmml" xref="S4.T1.4.4.4.4.m1.1.1.3.2">ùëé</ci><ci id="S4.T1.4.4.4.4.m1.1.1.3.3.cmml" xref="S4.T1.4.4.4.4.m1.1.1.3.3">ùë°</ci><ci id="S4.T1.4.4.4.4.m1.1.1.3.4.cmml" xref="S4.T1.4.4.4.4.m1.1.1.3.4">ùë°</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.m1.1c">d^{att}</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.4.4.5.1" class="ltx_tr">
<th id="S4.T1.4.4.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Dense-225M</th>
<td id="S4.T1.4.4.5.1.2" class="ltx_td ltx_align_center ltx_border_t">12</td>
<td id="S4.T1.4.4.5.1.3" class="ltx_td ltx_align_center ltx_border_t">3</td>
<td id="S4.T1.4.4.5.1.4" class="ltx_td ltx_align_center ltx_border_t">2880</td>
<td id="S4.T1.4.4.5.1.5" class="ltx_td ltx_align_center ltx_border_t">720</td>
</tr>
<tr id="S4.T1.4.4.6.2" class="ltx_tr">
<th id="S4.T1.4.4.6.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Dense-1B</th>
<td id="S4.T1.4.4.6.2.2" class="ltx_td ltx_align_center">32</td>
<td id="S4.T1.4.4.6.2.3" class="ltx_td ltx_align_center">6</td>
<td id="S4.T1.4.4.6.2.4" class="ltx_td ltx_align_center">4096</td>
<td id="S4.T1.4.4.6.2.5" class="ltx_td ltx_align_center">1024</td>
</tr>
<tr id="S4.T1.4.4.7.3" class="ltx_tr">
<th id="S4.T1.4.4.7.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">MoE-1B</th>
<td id="S4.T1.4.4.7.3.2" class="ltx_td ltx_align_center ltx_border_bb">12</td>
<td id="S4.T1.4.4.7.3.3" class="ltx_td ltx_align_center ltx_border_bb">3</td>
<td id="S4.T1.4.4.7.3.4" class="ltx_td ltx_align_center ltx_border_bb">2880</td>
<td id="S4.T1.4.4.7.3.5" class="ltx_td ltx_align_center ltx_border_bb">720</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Main Results on 160k hours</h3>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Following the scaling law <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, we compare model WERs on a fixed dataset (160k hours) across equal training steps (236k steps) or compute time (25.9 days).</figcaption>
<div id="S4.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:471.6pt;height:418.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-59.0pt,52.2pt) scale(0.8,0.8) ;">
<table id="S4.T2.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.3.1.1.1" class="ltx_tr">
<th id="S4.T2.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">(a) TestSet</th>
<td id="S4.T2.3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">(b) Dense-225M</td>
<td id="S4.T2.3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">(c) Dense-225M</td>
<td id="S4.T2.3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">(d) Dense-1B</td>
<td id="S4.T2.3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">(e) MoE-1B</td>
<td id="S4.T2.3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">(f) MoE-1B</td>
</tr>
<tr id="S4.T2.3.1.2.2" class="ltx_tr">
<th id="S4.T2.3.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S4.T2.3.1.2.2.2" class="ltx_td ltx_align_center">236k steps, 9.3 days</td>
<td id="S4.T2.3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r">657k steps, 25.9 days</td>
<td id="S4.T2.3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r">236k steps, 25.9 days</td>
<td id="S4.T2.3.1.2.2.5" class="ltx_td ltx_align_center">236k steps, 16.8 days</td>
<td id="S4.T2.3.1.2.2.6" class="ltx_td ltx_align_center">364k steps, 25.9 days</td>
</tr>
<tr id="S4.T2.3.1.3.3" class="ltx_tr">
<th id="S4.T2.3.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_001</th>
<td id="S4.T2.3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">1.28</td>
<td id="S4.T2.3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.15</td>
<td id="S4.T2.3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.92</td>
<td id="S4.T2.3.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.95</td>
<td id="S4.T2.3.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">0.90</td>
</tr>
<tr id="S4.T2.3.1.4.4" class="ltx_tr">
<th id="S4.T2.3.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_002</th>
<td id="S4.T2.3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">3.51</td>
<td id="S4.T2.3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.30</td>
<td id="S4.T2.3.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.03</td>
<td id="S4.T2.3.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">3.08</td>
<td id="S4.T2.3.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">2.94</td>
</tr>
<tr id="S4.T2.3.1.5.5" class="ltx_tr">
<th id="S4.T2.3.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_003</th>
<td id="S4.T2.3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">2.34</td>
<td id="S4.T2.3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.11</td>
<td id="S4.T2.3.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.74</td>
<td id="S4.T2.3.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">1.68</td>
<td id="S4.T2.3.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">1.63</td>
</tr>
<tr id="S4.T2.3.1.6.6" class="ltx_tr">
<th id="S4.T2.3.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_004</th>
<td id="S4.T2.3.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">2.05</td>
<td id="S4.T2.3.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.96</td>
<td id="S4.T2.3.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.79</td>
<td id="S4.T2.3.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t">1.87</td>
<td id="S4.T2.3.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">1.93</td>
</tr>
<tr id="S4.T2.3.1.7.7" class="ltx_tr">
<th id="S4.T2.3.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_005</th>
<td id="S4.T2.3.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">2.06</td>
<td id="S4.T2.3.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.92</td>
<td id="S4.T2.3.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.84</td>
<td id="S4.T2.3.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">1.78</td>
<td id="S4.T2.3.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t">1.73</td>
</tr>
<tr id="S4.T2.3.1.8.8" class="ltx_tr">
<th id="S4.T2.3.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_006</th>
<td id="S4.T2.3.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t">7.24</td>
<td id="S4.T2.3.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.69</td>
<td id="S4.T2.3.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.34</td>
<td id="S4.T2.3.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t">6.35</td>
<td id="S4.T2.3.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t">6.34</td>
</tr>
<tr id="S4.T2.3.1.9.9" class="ltx_tr">
<th id="S4.T2.3.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_007</th>
<td id="S4.T2.3.1.9.9.2" class="ltx_td ltx_align_center ltx_border_t">10.23</td>
<td id="S4.T2.3.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.12</td>
<td id="S4.T2.3.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.77</td>
<td id="S4.T2.3.1.9.9.5" class="ltx_td ltx_align_center ltx_border_t">9.67</td>
<td id="S4.T2.3.1.9.9.6" class="ltx_td ltx_align_center ltx_border_t">9.23</td>
</tr>
<tr id="S4.T2.3.1.10.10" class="ltx_tr">
<th id="S4.T2.3.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_008</th>
<td id="S4.T2.3.1.10.10.2" class="ltx_td ltx_align_center ltx_border_t">7.34</td>
<td id="S4.T2.3.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.29</td>
<td id="S4.T2.3.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.78</td>
<td id="S4.T2.3.1.10.10.5" class="ltx_td ltx_align_center ltx_border_t">6.13</td>
<td id="S4.T2.3.1.10.10.6" class="ltx_td ltx_align_center ltx_border_t">5.59</td>
</tr>
<tr id="S4.T2.3.1.11.11" class="ltx_tr">
<th id="S4.T2.3.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_009</th>
<td id="S4.T2.3.1.11.11.2" class="ltx_td ltx_align_center ltx_border_t">3.94</td>
<td id="S4.T2.3.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.67</td>
<td id="S4.T2.3.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.45</td>
<td id="S4.T2.3.1.11.11.5" class="ltx_td ltx_align_center ltx_border_t">3.60</td>
<td id="S4.T2.3.1.11.11.6" class="ltx_td ltx_align_center ltx_border_t">3.52</td>
</tr>
<tr id="S4.T2.3.1.12.12" class="ltx_tr">
<th id="S4.T2.3.1.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_010</th>
<td id="S4.T2.3.1.12.12.2" class="ltx_td ltx_align_center ltx_border_t">4.76</td>
<td id="S4.T2.3.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.68</td>
<td id="S4.T2.3.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.37</td>
<td id="S4.T2.3.1.12.12.5" class="ltx_td ltx_align_center ltx_border_t">4.55</td>
<td id="S4.T2.3.1.12.12.6" class="ltx_td ltx_align_center ltx_border_t">4.49</td>
</tr>
<tr id="S4.T2.3.1.13.13" class="ltx_tr">
<th id="S4.T2.3.1.13.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_011</th>
<td id="S4.T2.3.1.13.13.2" class="ltx_td ltx_align_center ltx_border_t">3.21</td>
<td id="S4.T2.3.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.88</td>
<td id="S4.T2.3.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.31</td>
<td id="S4.T2.3.1.13.13.5" class="ltx_td ltx_align_center ltx_border_t">2.36</td>
<td id="S4.T2.3.1.13.13.6" class="ltx_td ltx_align_center ltx_border_t">2.28</td>
</tr>
<tr id="S4.T2.3.1.14.14" class="ltx_tr">
<th id="S4.T2.3.1.14.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_012</th>
<td id="S4.T2.3.1.14.14.2" class="ltx_td ltx_align_center ltx_border_t">3.39</td>
<td id="S4.T2.3.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.22</td>
<td id="S4.T2.3.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.91</td>
<td id="S4.T2.3.1.14.14.5" class="ltx_td ltx_align_center ltx_border_t">3.01</td>
<td id="S4.T2.3.1.14.14.6" class="ltx_td ltx_align_center ltx_border_t">2.97</td>
</tr>
<tr id="S4.T2.3.1.15.15" class="ltx_tr">
<th id="S4.T2.3.1.15.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_013</th>
<td id="S4.T2.3.1.15.15.2" class="ltx_td ltx_align_center ltx_border_t">4.15</td>
<td id="S4.T2.3.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.81</td>
<td id="S4.T2.3.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.62</td>
<td id="S4.T2.3.1.15.15.5" class="ltx_td ltx_align_center ltx_border_t">3.71</td>
<td id="S4.T2.3.1.15.15.6" class="ltx_td ltx_align_center ltx_border_t">3.69</td>
</tr>
<tr id="S4.T2.3.1.16.16" class="ltx_tr">
<th id="S4.T2.3.1.16.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_014</th>
<td id="S4.T2.3.1.16.16.2" class="ltx_td ltx_align_center ltx_border_t">5.01</td>
<td id="S4.T2.3.1.16.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.45</td>
<td id="S4.T2.3.1.16.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.87</td>
<td id="S4.T2.3.1.16.16.5" class="ltx_td ltx_align_center ltx_border_t">4.06</td>
<td id="S4.T2.3.1.16.16.6" class="ltx_td ltx_align_center ltx_border_t">3.83</td>
</tr>
<tr id="S4.T2.3.1.17.17" class="ltx_tr">
<th id="S4.T2.3.1.17.17.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_015</th>
<td id="S4.T2.3.1.17.17.2" class="ltx_td ltx_align_center ltx_border_t">7.58</td>
<td id="S4.T2.3.1.17.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.77</td>
<td id="S4.T2.3.1.17.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.43</td>
<td id="S4.T2.3.1.17.17.5" class="ltx_td ltx_align_center ltx_border_t">6.69</td>
<td id="S4.T2.3.1.17.17.6" class="ltx_td ltx_align_center ltx_border_t">7.03</td>
</tr>
<tr id="S4.T2.3.1.18.18" class="ltx_tr">
<th id="S4.T2.3.1.18.18.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_016</th>
<td id="S4.T2.3.1.18.18.2" class="ltx_td ltx_align_center ltx_border_t">5.15</td>
<td id="S4.T2.3.1.18.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.46</td>
<td id="S4.T2.3.1.18.18.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.95</td>
<td id="S4.T2.3.1.18.18.5" class="ltx_td ltx_align_center ltx_border_t">4.02</td>
<td id="S4.T2.3.1.18.18.6" class="ltx_td ltx_align_center ltx_border_t">3.82</td>
</tr>
<tr id="S4.T2.3.1.19.19" class="ltx_tr">
<th id="S4.T2.3.1.19.19.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_017</th>
<td id="S4.T2.3.1.19.19.2" class="ltx_td ltx_align_center ltx_border_t">4.11</td>
<td id="S4.T2.3.1.19.19.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.87</td>
<td id="S4.T2.3.1.19.19.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.24</td>
<td id="S4.T2.3.1.19.19.5" class="ltx_td ltx_align_center ltx_border_t">3.52</td>
<td id="S4.T2.3.1.19.19.6" class="ltx_td ltx_align_center ltx_border_t">3.49</td>
</tr>
<tr id="S4.T2.3.1.20.20" class="ltx_tr">
<th id="S4.T2.3.1.20.20.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_018</th>
<td id="S4.T2.3.1.20.20.2" class="ltx_td ltx_align_center ltx_border_t">2.69</td>
<td id="S4.T2.3.1.20.20.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.57</td>
<td id="S4.T2.3.1.20.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.38</td>
<td id="S4.T2.3.1.20.20.5" class="ltx_td ltx_align_center ltx_border_t">2.56</td>
<td id="S4.T2.3.1.20.20.6" class="ltx_td ltx_align_center ltx_border_t">2.44</td>
</tr>
<tr id="S4.T2.3.1.21.21" class="ltx_tr">
<th id="S4.T2.3.1.21.21.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_019</th>
<td id="S4.T2.3.1.21.21.2" class="ltx_td ltx_align_center ltx_border_t">3.91</td>
<td id="S4.T2.3.1.21.21.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.29</td>
<td id="S4.T2.3.1.21.21.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.95</td>
<td id="S4.T2.3.1.21.21.5" class="ltx_td ltx_align_center ltx_border_t">3.05</td>
<td id="S4.T2.3.1.21.21.6" class="ltx_td ltx_align_center ltx_border_t">2.90</td>
</tr>
<tr id="S4.T2.3.1.22.22" class="ltx_tr">
<th id="S4.T2.3.1.22.22.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_020</th>
<td id="S4.T2.3.1.22.22.2" class="ltx_td ltx_align_center ltx_border_t">3.05</td>
<td id="S4.T2.3.1.22.22.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.97</td>
<td id="S4.T2.3.1.22.22.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.33</td>
<td id="S4.T2.3.1.22.22.5" class="ltx_td ltx_align_center ltx_border_t">2.51</td>
<td id="S4.T2.3.1.22.22.6" class="ltx_td ltx_align_center ltx_border_t">2.47</td>
</tr>
<tr id="S4.T2.3.1.23.23" class="ltx_tr">
<th id="S4.T2.3.1.23.23.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_021</th>
<td id="S4.T2.3.1.23.23.2" class="ltx_td ltx_align_center ltx_border_t">2.75</td>
<td id="S4.T2.3.1.23.23.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.89</td>
<td id="S4.T2.3.1.23.23.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.53</td>
<td id="S4.T2.3.1.23.23.5" class="ltx_td ltx_align_center ltx_border_t">2.73</td>
<td id="S4.T2.3.1.23.23.6" class="ltx_td ltx_align_center ltx_border_t">2.73</td>
</tr>
<tr id="S4.T2.3.1.24.24" class="ltx_tr">
<th id="S4.T2.3.1.24.24.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_022</th>
<td id="S4.T2.3.1.24.24.2" class="ltx_td ltx_align_center ltx_border_t">5.55</td>
<td id="S4.T2.3.1.24.24.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.15</td>
<td id="S4.T2.3.1.24.24.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.50</td>
<td id="S4.T2.3.1.24.24.5" class="ltx_td ltx_align_center ltx_border_t">4.86</td>
<td id="S4.T2.3.1.24.24.6" class="ltx_td ltx_align_center ltx_border_t">4.52</td>
</tr>
<tr id="S4.T2.3.1.25.25" class="ltx_tr">
<th id="S4.T2.3.1.25.25.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_023</th>
<td id="S4.T2.3.1.25.25.2" class="ltx_td ltx_align_center ltx_border_t">6.05</td>
<td id="S4.T2.3.1.25.25.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.99</td>
<td id="S4.T2.3.1.25.25.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.89</td>
<td id="S4.T2.3.1.25.25.5" class="ltx_td ltx_align_center ltx_border_t">5.86</td>
<td id="S4.T2.3.1.25.25.6" class="ltx_td ltx_align_center ltx_border_t">5.25</td>
</tr>
<tr id="S4.T2.3.1.26.26" class="ltx_tr">
<th id="S4.T2.3.1.26.26.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_024</th>
<td id="S4.T2.3.1.26.26.2" class="ltx_td ltx_align_center ltx_border_t">5.61</td>
<td id="S4.T2.3.1.26.26.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.19</td>
<td id="S4.T2.3.1.26.26.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.61</td>
<td id="S4.T2.3.1.26.26.5" class="ltx_td ltx_align_center ltx_border_t">4.76</td>
<td id="S4.T2.3.1.26.26.6" class="ltx_td ltx_align_center ltx_border_t">4.78</td>
</tr>
<tr id="S4.T2.3.1.27.27" class="ltx_tr">
<th id="S4.T2.3.1.27.27.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_025</th>
<td id="S4.T2.3.1.27.27.2" class="ltx_td ltx_align_center ltx_border_t">5.76</td>
<td id="S4.T2.3.1.27.27.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.30</td>
<td id="S4.T2.3.1.27.27.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.36</td>
<td id="S4.T2.3.1.27.27.5" class="ltx_td ltx_align_center ltx_border_t">4.83</td>
<td id="S4.T2.3.1.27.27.6" class="ltx_td ltx_align_center ltx_border_t">4.61</td>
</tr>
<tr id="S4.T2.3.1.28.28" class="ltx_tr">
<th id="S4.T2.3.1.28.28.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">speechio_026</th>
<td id="S4.T2.3.1.28.28.2" class="ltx_td ltx_align_center ltx_border_t">4.37</td>
<td id="S4.T2.3.1.28.28.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.01</td>
<td id="S4.T2.3.1.28.28.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.90</td>
<td id="S4.T2.3.1.28.28.5" class="ltx_td ltx_align_center ltx_border_t">4.02</td>
<td id="S4.T2.3.1.28.28.6" class="ltx_td ltx_align_center ltx_border_t">3.84</td>
</tr>
<tr id="S4.T2.3.1.29.29" class="ltx_tr">
<th id="S4.T2.3.1.29.29.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">average</th>
<td id="S4.T2.3.1.29.29.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">4.50</td>
<td id="S4.T2.3.1.29.29.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">4.18</td>
<td id="S4.T2.3.1.29.29.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">3.72</td>
<td id="S4.T2.3.1.29.29.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">3.93</td>
<td id="S4.T2.3.1.29.29.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">3.80</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In Table.<a href="#S4.T2" title="Table 2 ‚Ä£ 4.3 Main Results on 160k hours ‚Ä£ 4 Experiments ‚Ä£ U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we compare the performance of the three models from Table.<a href="#S4.T1" title="Table 1 ‚Ä£ 4.2 Training Details ‚Ä£ 4 Experiments ‚Ä£ U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> under different conditions (such as the same number of training steps or the same training time), with the results indicating:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span> 
<div id="S4.I1.ix1.p1" class="ltx_para">
<p id="S4.I1.ix1.p1.1" class="ltx_p">At the same number of training steps (263k steps), comparing columns (b), (d), and (e) reveals that the WER of the MoE-1B model (3.93) is slightly worse than that of the Dense-1B model (3.72), but both significantly outperform the Dense-225M baseline (4.50).</p>
</div>
</li>
<li id="S4.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span> 
<div id="S4.I1.ix2.p1" class="ltx_para">
<p id="S4.I1.ix2.p1.1" class="ltx_p">With the same training time (25.9 days), comparing columns (c), (d), and (f) shows that the WER of the MoE-1B model (3.80) is very close to that of the Dense-1B model (3.72), and both substantially surpass the Dense-225M model (4.18).</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">These results suggest that on a dataset of 160k hours, a larger number of parameters (from 225M to 1B) leads to better model performance. Moreover, when the number of parameters is the same, MoE models can achieve WER levels comparable to Dense models.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Furthermore, in Table.<a href="#S4.T3" title="Table 3 ‚Ä£ 4.3 Main Results on 160k hours ‚Ä£ 4 Experiments ‚Ä£ U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we compare the inference speeds of the three models, with the results showing:</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span> 
<div id="S4.I2.ix1.p1" class="ltx_para">
<p id="S4.I2.ix1.p1.1" class="ltx_p">Although the MoE-1B and Dense-1B have the same number of parameters, the former is 2.5 times faster than the latter.</p>
</div>
</li>
<li id="S4.I2.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span> 
<div id="S4.I2.ix2.p1" class="ltx_para">
<p id="S4.I2.ix2.p1.1" class="ltx_p">Even though the parameter count of MoE-1B is 4.7 times that of Dense-225M, the absolute difference in RTF between the two is only around 0.03 (for cpu) or 0.0004 (for gpu).</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">Overall, combining the WER and RTF results, we can confirm that <span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">the MoE-1B model can achieve Dense-1B level accuracy with Dense-225M level inference cost</span>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>RTF benchmark. When testing with a CPU, we set the batch size to 1 and perform inference on an int8 quantized model using a single thread on an Intel(R) Core(TM) i5-8400 CPU @ 2.80GHz. For GPU-based evaluations, we set the batch size to 200 and perform inference on an FP16 model using a single NVIDIA 3090. Please note that we do not include GPU RTF for decoder rescoring since the inference time for this process is dominated by the CTC prefix beam search running on the CPU, and therefore, it cannot objectively reflect the inference time on the GPU.</figcaption>
<div id="S4.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:262.5pt;height:61.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.2pt,5.4pt) scale(0.85,0.85) ;">
<table id="S4.T3.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.1.1.1" class="ltx_tr">
<th id="S4.T3.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">(a) Model</th>
<th id="S4.T3.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">(b) ctc greedy decoding</th>
<th id="S4.T3.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">(c) decoder rescoring</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.1.2.1" class="ltx_tr">
<th id="S4.T3.3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Dense-225M</th>
<td id="S4.T3.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.1088 (cpu) / 0.0012 (gpu)</td>
<td id="S4.T3.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.1524 (cpu)</td>
</tr>
<tr id="S4.T3.3.1.3.2" class="ltx_tr">
<th id="S4.T3.3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Dense-1B</th>
<td id="S4.T3.3.1.3.2.2" class="ltx_td ltx_align_center">0.3155 (cpu) / 0.0028 (gpu)</td>
<td id="S4.T3.3.1.3.2.3" class="ltx_td ltx_align_center">0.4515 (cpu)</td>
</tr>
<tr id="S4.T3.3.1.4.3" class="ltx_tr">
<th id="S4.T3.3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">MoE-1B</th>
<td id="S4.T3.3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">0.1299 (cpu) / 0.0016 (gpu)</td>
<td id="S4.T3.3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">0.1826 (cpu)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Streaming Capability</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Empirically, training a large model to accommodate both streaming and non-streaming modes simultaneously could potentially compromise performance. In response, this paper introduces a two-stage training pipeline. Initially, we train a non-streaming base model (such as MoE-1B and Dense-225M that is described in Section <a href="#S4.SS2" title="4.2 Training Details ‚Ä£ 4 Experiments ‚Ä£ U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> and Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4.2 Training Details ‚Ä£ 4 Experiments ‚Ä£ U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), which then serves as the foundation for initializing the proposed U2++-MoE-1B model (and also U2++-Dense-225M, U2++-Dense-1B). The MoE-1B model shares an identical architecture with the U2++-MoE-1B model, with the only distinction lying in their approach to chunk masking. While the MoE-1B employs a full chunk strategy, the U2++-MoE-1B adopts a dynamic chunk method as detailed in section <a href="#S3" title="3 Methodology ‚Ä£ U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. This approach stabilizes the training process for a unified system capable of handling both streaming and non-streaming functionalities.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">In Table <a href="#S4.T4" title="Table 4 ‚Ä£ 4.4 Streaming Capability ‚Ä£ 4 Experiments ‚Ä£ U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, by comparing three different streaming models, we can draw the same conclusion as in the non-streaming models (section <a href="#S4.SS3" title="4.3 Main Results on 160k hours ‚Ä£ 4 Experiments ‚Ä£ U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>), which is that our proposed MoE model significantly outperforms the Dense counterpart in terms of WER while maintaining a similar RTF. Please note that the WER for the U2++-Dense-1B model is not included. This is due to the frequent occurrence of gradient explosions during the training process, which, despite the initialization with a non-streaming Dense-1B model, made the training unsustainable.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text ltx_font_bold">Table 4</span>: </span>Averaged streaming results on SpeechIO test sets: WER Measured with a 640ms chunk size and RTF calculated using the same hardware (cpu) and methodology (decoder rescoring) as in Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.3 Main Results on 160k hours ‚Ä£ 4 Experiments ‚Ä£ U2++ MoE: Scaling 4.7x Parameters with minimal Impact on RTF" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. All models were initialized from their respective non-streaming baselines and subsequently trained for a total of 160k steps.</figcaption>
<div id="S4.T4.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:204.5pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T4.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.3.1.1.1" class="ltx_tr">
<th id="S4.T4.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">(a) Model</th>
<th id="S4.T4.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">(b) WER</th>
<th id="S4.T4.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">(c) RTF</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.3.1.2.1" class="ltx_tr">
<th id="S4.T4.3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">U2++-Dense-225M</th>
<td id="S4.T4.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">6.24</td>
<td id="S4.T4.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.1937</td>
</tr>
<tr id="S4.T4.3.1.3.2" class="ltx_tr">
<th id="S4.T4.3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">U2++-Dense-1B</th>
<td id="S4.T4.3.1.3.2.2" class="ltx_td ltx_align_center">N/A</td>
<td id="S4.T4.3.1.3.2.3" class="ltx_td ltx_align_center">0.6015</td>
</tr>
<tr id="S4.T4.3.1.4.3" class="ltx_tr">
<th id="S4.T4.3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">U2++-MoE-1B</th>
<td id="S4.T4.3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">4.83</td>
<td id="S4.T4.3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">0.2436</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The proposed U2++ MoE provides a clean setup and little task-specific design. Through the straightforward substitution of all FFN layers in the baseline model with MoE FFNs, coupled with the adoption of the U2++ training framework, we attain notable enhancements in WER alongside streaming recognition capabilities, all without a considerable increase in RTF.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We thank Wenpeng Li and Jianwei Niu for their feedbacks on this work.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc¬†V. Le,
Geoffrey¬†E. Hinton, and Jeff Dean,

</span>
<span class="ltx_bibblock">‚ÄúOutrageously large neural networks: The sparsely-gated
mixture-of-experts layer,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">5th International Conference on Learning Representations,
ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings</span>. 2017, OpenReview.net.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen,

</span>
<span class="ltx_bibblock">‚ÄúGshard: Scaling giant models with conditional computation and
automatic sharding,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021</span>. 2021, OpenReview.net.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ke¬†Hu, Bo¬†Li, Tara¬†N. Sainath, Yu¬†Zhang, and Fran√ßoise Beaufays,

</span>
<span class="ltx_bibblock">‚ÄúMixture-of-expert conformer for streaming multilingual ASR,‚Äù

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/2305.15663, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Zhao You, Shulin Feng, Dan Su, and Dong Yu,

</span>
<span class="ltx_bibblock">‚ÄúSpeechmoe: Scaling to large acoustic models with dynamic routing
mixture of experts,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Interspeech 2021, 22nd Annual Conference of the International
Speech Communication Association, Brno, Czechia, 30 August - 3 September
2021</span>, Hynek Hermansky, Honza Cernock√Ω, Luk√°s Burget, Lori Lamel,
Odette Scharenborg, and Petr Motl√≠cek, Eds. 2021, pp. 2077‚Äì2081,
ISCA.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Wenxuan Wang, Guodong Ma, Yuke Li, and Binbin Du,

</span>
<span class="ltx_bibblock">‚ÄúLanguage-routing mixture of experts for multilingual and
code-switching speech recognition,‚Äù

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/2307.05956, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Zhao You, Shulin Feng, Dan Su, and Dong Yu,

</span>
<span class="ltx_bibblock">‚Äú3m: Multi-loss, multi-path and multi-level neural networks for
speech recognition,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">13th International Symposium on Chinese Spoken Language
Processing, ISCSLP 2022, Singapore, December 11-14, 2022</span>, Kong¬†Aik Lee,
Hung-yi Lee, Yanfeng Lu, and Minghui Dong, Eds. 2022, pp. 170‚Äì174, IEEE.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Binbin Zhang, Di¬†Wu, Zhendong Peng, Xingchen Song, Zhuoyuan Yao, Hang Lv, Lei
Xie, Chao Yang, Fuping Pan, and Jianwei Niu,

</span>
<span class="ltx_bibblock">‚ÄúWenet 2.0: More productive end-to-end speech recognition toolkit,‚Äù

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/2203.15455, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
Sutskever, et¬†al.,

</span>
<span class="ltx_bibblock">‚ÄúLanguage models are unsupervised multitask learners,‚Äù

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">OpenAI blog</span>, vol. 1, no. 8, pp. 9, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Tom¬†B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel¬†M. Ziegler, Jeffrey Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei,

</span>
<span class="ltx_bibblock">‚ÄúLanguage models are few-shot learners,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</span>, Hugo Larochelle, Marc‚ÄôAurelio Ranzato, Raia
Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, Eds., 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Alec Radford, Jong¬†Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
Ilya Sutskever,

</span>
<span class="ltx_bibblock">‚ÄúRobust speech recognition via large-scale weak supervision,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning, ICML 2023,
23-29 July 2023, Honolulu, Hawaii, USA</span>, Andreas Krause, Emma Brunskill,
Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, Eds.
2023, vol. 202 of <span id="bib.bib10.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, pp.
28492‚Äì28518, PMLR.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Yu¬†Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin
Chen, Bo¬†Li, Vera Axelrod, Gary Wang, Zhong Meng, Ke¬†Hu, Andrew Rosenberg,
Rohit Prabhavalkar, Daniel¬†S. Park, Parisa Haghani, Jason Riesa, Ginger
Perng, Hagen Soltau, Trevor Strohman, Bhuvana Ramabhadran, Tara¬†N. Sainath,
Pedro¬†J. Moreno, Chung-Cheng Chiu, Johan Schalkwyk, Fran√ßoise
Beaufays, and Yonghui Wu,

</span>
<span class="ltx_bibblock">‚ÄúGoogle USM: scaling automatic speech recognition beyond 100
languages,‚Äù

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/2303.01037, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin,
Minh-Thang Luong, and Orhan Firat,

</span>
<span class="ltx_bibblock">‚ÄúBeyond distillation: Task-level mixture-of-experts for efficient
inference,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November,
2021</span>, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and
Scott¬†Wen-tau Yih, Eds. 2021, pp. 3577‚Äì3599, Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
William Fedus, Barret Zoph, and Noam Shazeer,

</span>
<span class="ltx_bibblock">‚ÄúSwitch transformers: Scaling to trillion parameter models with
simple and efficient sparsity,‚Äù

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">J. Mach. Learn. Res.</span>, vol. 23, pp. 120:1‚Äì120:39, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Yu-An Chung, Wei-Ning Hsu, Hao Tang, and James¬†R. Glass,

</span>
<span class="ltx_bibblock">‚ÄúAn unsupervised autoregressive model for speech representation
learning,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">20th Annual Conference of the International Speech
Communication Association (Interspeech 2019)</span>, Gernot Kubin and Zdravko
Kacic, Eds., Graz, Austria, 2019, pp. 146‚Äì150, ISCA.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu¬†Zhang, Jiahui Yu,
Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang,

</span>
<span class="ltx_bibblock">‚ÄúConformer: Convolution-augmented transformer for speech
recognition,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Interspeech 2020, 21st Annual Conference of the International
Speech Communication Association, Virtual Event, Shanghai, China, 25-29
October 2020</span>, Helen Meng, Bo¬†Xu, and Thomas¬†Fang Zheng, Eds. 2020, pp.
5036‚Äì5040, ISCA.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan¬†N. Gomez, Lukasz Kaiser, and Illia Polosukhin,

</span>
<span class="ltx_bibblock">‚ÄúAttention is all you need,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems (NeurIPS 2017)</span>,
Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna¬†M. Wallach, Rob
Fergus, S.¬†V.¬†N. Vishwanathan, and Roman Garnett, Eds., Long Beach, USA,
2017, pp. 5998‚Äì6008, ACM.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Alex Graves, Santiago Fern√°ndez, Faustino¬†J. Gomez, and J√ºrgen
Schmidhuber,

</span>
<span class="ltx_bibblock">‚ÄúConnectionist temporal classification: labelling unsegmented
sequence data with recurrent neural networks,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">23rd International Conference on Machine Learning (ICML
2006)</span>, William¬†W. Cohen and Andrew¬†W. Moore, Eds., Pittsburgh, USA, 2006,
pp. 369‚Äì376, ACM.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Zhuoyuan Yao, Di¬†Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong
Peng, Xiaoyu Chen, Lei Xie, and Xin Lei,

</span>
<span class="ltx_bibblock">‚ÄúWenet: Production oriented streaming and non-streaming end-to-end
speech recognition toolkit,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Interspeech 2021, 22nd Annual Conference of the International
Speech Communication Association, Brno, Czechia, 30 August - 3 September
2021</span>, Hynek Hermansky, Honza Cernock√Ω, Luk√°s Burget, Lori Lamel,
Odette Scharenborg, and Petr Motl√≠cek, Eds. 2021, pp. 4054‚Äì4058,
ISCA.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He,

</span>
<span class="ltx_bibblock">‚ÄúDeepspeed: System optimizations enable training deep learning
models with over 100 billion parameters,‚Äù

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">KDD ‚Äô20: The 26th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020</span>,
Rajesh Gupta, Yan Liu, Jiliang Tang, and B.¬†Aditya Prakash, Eds. 2020, pp.
3505‚Äì3506, ACM.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom¬†B. Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei,

</span>
<span class="ltx_bibblock">‚ÄúScaling laws for neural language models,‚Äù

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/2001.08361, 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.16406" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.16407" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.16407">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.16407" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.16408" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 16:57:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
