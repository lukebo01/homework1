<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.02565] Sequence-to-sequence models in peer-to-peer learning: A practical application</title><meta property="og:description" content="This paper explores the applicability of sequence-to-sequence (Seq2Seq) models based on LSTM units for Automatic Speech Recognition (ASR) task within peer-to-peer learning environments.
Leveraging two distinct peer-to-‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Sequence-to-sequence models in peer-to-peer learning: A practical application">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Sequence-to-sequence models in peer-to-peer learning: A practical application">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.02565">

<!--Generated on Fri Jul  5 23:25:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
peer-to-peer,  sequence-to-sequence,  Deep Speech 2,  UserLibri,  Automatic speech recognition
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Sequence-to-sequence models in peer-to-peer learning: A practical application</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Robert ≈†ajina1
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Faculty of Informatics
<br class="ltx_break">Juraj Dobrila University of Pula
<br class="ltx_break"></span>Pula, Croatia
<br class="ltx_break">robert.sajina@unipu.hr
<br class="ltx_break">1Corresponding author

</span></span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ivo Ip≈°iƒá
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_font_italic">Faculty of Informatics and Digital Technologies
<br class="ltx_break">University of Rijeka
<br class="ltx_break"></span>Rijeka, Croatia
<br class="ltx_break">ivoi@uniri.hr

</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">This paper explores the applicability of sequence-to-sequence (Seq2Seq) models based on LSTM units for Automatic Speech Recognition (ASR) task within peer-to-peer learning environments.
Leveraging two distinct peer-to-peer learning methods, the study simulates the learning process of agents and evaluates their performance in ASR task using two different ASR datasets.
In a centralized training setting, utilizing a scaled-down variant of the Deep Speech 2 model, a single model achieved a Word Error Rate (WER) of 84% when trained on the UserLibri dataset, and 38% when trained on the LJ Speech dataset.
Conversely, in a peer-to-peer learning scenario involving 55 agents, the WER ranged from 87% to 92% for the UserLibri dataset, and from 52% to 56% for the LJ Speech dataset.
The findings demonstrate the feasibility of employing Seq2Seq models in decentralized settings, albeit with slightly higher Word Error Rates (WER) compared to centralized training methods.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
peer-to-peer, sequence-to-sequence, Deep Speech 2, UserLibri, Automatic speech recognition

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Sequence-to-sequence (Seq2Seq) models have emerged as a powerful framework in various natural language processing (NLP) and sequence generation tasks, ranging from machine translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">23</a>]</cite>, text summarization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">27</a>]</cite> to even automatic speech recognition (ASR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite>.
However, their application in peer-to-peer deep learning scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite>, even within the realm of Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">19</a>]</cite>, remains relatively under explored.
Peer-to-peer and FL environments are comprised of agents, each holding local private data and preforming local computations, with the primary mode of interaction being the sharing of models with outside entities or other agents.
Agent‚Äôs local data is considered as private and must never leave the agent, therefore only the machine learning models are exchanged with other participants in the environment.
In the context of Federated Learning (FL), agents disseminate their trained models solely with a central server, whereas in peer-to-peer settings, agents engage in direct model exchange among themselves, thereby rendering peer-to-peer environments notably more complex scenarios.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">An agent, which may be any edge device such as phone or laptop, owns local data which is most likely generated by the agent or the user using the device.
When considering the applications of peer-to-peer learning, the availability of local data must also be considered.
In this context, local textual exchanges or user-generated written content emerge as potential dataset that can be utilized for training neural network models tailored to tasks like next-word prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">26</a>]</cite>.
In the context of Seq2Seq model applications, an Automatic Speech Recognition (ASR) task presents itself as a viable option.
Users can contribute to the generation of data by orally presenting specific paragraphs or sentences, therefore creating a dataset of audio clips and corresponding textual representations which can be utilized to train a local Seq2Seq model.
The purpose of this study is to investigate if such application of the Seq2Seq model is viable peer-to-peer environment.
Reproducible code is available on a publicly available repository: <a target="_blank" href="https://github.com/rosaj/p2p_bn" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/rosaj/p2p_bn</a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Peer-to-peer learning</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In peer-to-peer learning, agents exchange their models following a network topology which is commonly predetermined <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite>, however, the peer connection between agents can also be established autonomously based on agents‚Äô preferences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite>.
Since the topology often dictates the number of neighbors, it is important to note that the communication can be directed or undirected.
In an example of undirected ring topology, each agent has two neighbors, while in the directed ring topology, each agent has only one neighbor.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Each agent‚Äôs goal is to train its local model by leveraging its local dataset and information received from its neighbors to minimize its average loss function.
An agent uses this local data to train a local model by calculating mini-batch gradient and updating its local model for predetermined number of batch iterations.
The local training step is followed by a communication step in which agents exchange information.
Scenarios involving multiple agent learning and interacting with eachother are often simulated in memory in a computer, in parallel, i.e., each agent performs training and communication steps in a loop.
Different approaches include different message content exchanged between agents; commonly, an agent must send its message to all its out-neighbors and receive all messages from its in-neighbors.
A loss of a message in this phase may stall the overall learning process since an agent may forever wait for a message that never arrives.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In a synchronous approach, a synchronization barrier is implemented, mandating that agents refrain from proceeding with their learning processes until all messages have been both transmitted and received <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite>.
Conversely, an asynchronous approach permits agents to engage in communication and learning activities at their discretion, without significant constraints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">24</a>]</cite>.
Furthermore, variations in the aggregation methodology may arise, with certain techniques involving the aggregation of all received models by computing the average of all model parameters, inclusive of local model parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite>, while in alternative methods, an agent may conduct averaging operations upon each received model in conjunction with its own local model subsequent to reception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Sequence-to-sequence models for ASR</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Seq2Seq models consist of two main components: an encoder and a decoder.
In the context of ASR, the encoder processes input speech signals, typically represented as spectrograms or other time-frequency representations, into a fixed-dimensional vector representation, capturing relevant features of the input audio.
The decoder, often implemented as a recurrent neural network (RNN) or transformer architecture, then generates the corresponding text output based on the encoded information.
The most known model architectures regarding the ASR tasks are the <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">Listen, Attend and Spell</span> (LAS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite> and Deep Speech 2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>.
In this study, Deep Speech 2 model variant will be used to investigate the applicability of the Seq2Seq models in peer-to-peer environments for the ASR task.
In conventional training process (depicted in Figure <a href="#S2.F1" title="Figure 1 ‚Ä£ II-B Sequence-to-sequence models for ASR ‚Ä£ II Background ‚Ä£ Sequence-to-sequence models in peer-to-peer learning: A practical application" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the audio clip is first converted to a spectrogram which is essentially an image, which is then used as an input to the model.
The model outputs a sequence of token probabilities which is then used to assemble the output sentence.
Based on specific needs, the output may be individual characters or words.
Once the sentence is assembled, Connectionist Temporal Classification (CTC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite> loss metric is used to evaluate the similarity of the model output as compared to ground truth text that was spoken in the audio clip.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2406.02565/assets/imgs/02_process.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="987" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Data processing and training scheme.

</span></figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The primary evaluation metric utilized in the assessment of automatic speech recognition systems is the Word Error Rate (WER).
WER quantifies the minimum number of operations, including substitutions, deletions, and insertions, necessary to transform the system‚Äôs transcription (prediction) into the reference transcription (ground truth), divided by the total number of words in the reference.
WER is bounded between 0 and infinity, where a lower value indicates better performance.
Often expressed as a percentage, WER is commonly calculated by multiplying the raw score by 100.
For instance, a WER of 0.15 is equivalently represented as 15%.
Extraction of words from a trained model necessitates the utilization of a decoder, which translates a probability distribution over characters into textual output.
Two primary types of decoders are typically employed with Connectionist Temporal Classification (CTC)-based models: the greedy decoder and the beam search decoder with language model re-scoring.
The greedy decoder selects the most probable character at each time step, facilitating rapid inference and generating transcripts closely resembling the original pronunciation.
However, this approach may introduce numerous minor misspelling errors, which, given the nature of the WER metric, render entire words incorrect with even a single character discrepancy.
Conversely, the beam search decoder with language model re-scoring explores multiple potential decodings concurrently, assigning higher scores to more probable N-grams based on a specified language model.
Integration of the language model aids in rectifying misspelling errors.
Nevertheless, this approach is notably slower in comparison to the greedy decoder.
For simplicity and speed, this study utilizes the greedy decoder.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Related work</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Research in the domain of Seq2Seq models for Automatic Speech Recognition (ASR) has been extensively explored and documented within the context of training a single centralized model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>, <a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite>.
However, the applicability of these models within decentralized systems remains an area requiring further investigation.
In the realm of centralized distributed learning, such as Federated Learning, numerous studies have emerged involving sequence-to-sequence models across various natural language processing (NLP) tasks.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">For instance, Lin <span id="S3.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite> developed FedNLP, a benchmarking framework tailored for evaluating Federated Learning methods on common NLP tasks.
his framework utilizes Transformer-based language models for tasks like text classification, sequence tagging, question answering, and sequence-to-sequence generation.
Similarly, Lu <span id="S3.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>]</cite> proposed the Federated Natural Language Generation (FedNLG) framework, which facilitates the learning of personalized representations from distributed datasets across devices.
FedNLG enables the implementation of personalized dialogue systems by pre-training standard neural conversational models over large dialogue corpora and fine-tuning model parameters and persona embeddings in a federated manner.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Furthermore, recent research by Nguyen <span id="S3.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">21</a>]</cite> specifically addresses the ASR task within the context of Federated Learning.
Their study employs the Wav2vec 2.0 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">3</a>]</cite>, which features a Transformer-influenced architecture, demonstrating the feasibility and effectiveness of leveraging advanced neural network architectures for speech recognition tasks within decentralized learning frameworks.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Related work does not address the applicability of Seq2Seq models based on LSTM cells for the ASR task, especially within the peer-to-peer environments.
Several potential problems may arise from the architecture comprised of LSTMs, as the vanishing or exploding gradients problems associated with RNN cells.
This problems may especially be highlighted due to the nature of local datasets comprised on agents which may contain small amount of data and differ from agent to agent <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">18</a>]</cite>.
This work will analyze the applicability of Seq2Seq model based on architecture utilizing LSTM cells for agents collaborating in peer-to-peer environments.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">The existing literature lacks a comprehensive exploration of the applicability of Seq2Seq models based on LSTM cells for the ASR task, particularly within peer-to-peer environments.
Such architectures pose several potential challenges, including issues related to vanishing or exploding gradients commonly associated with recurrent neural network (RNN) cells.
These challenges may be exacerbated by the nature of local datasets held by individual agents in peer-to-peer settings, which may vary in size and composition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">18</a>]</cite>.
By investigating the performance of such models under two different communication and aggregation techniques, this research seeks to elucidate the potential benefits and limitations of employing LSTM-based Seq2Seq models in decentralized learning settings.
Through empirical evaluations and experimental validations, this work aims to provide insights into the feasibility and effectiveness of utilizing LSTM-based Seq2Seq models for ASR tasks in distributed peer-to-peer environments.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">ASR in peer-to-peer learning</span>
</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Dataset.</span>
UserLibri <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">5</a>]</cite> dataset will be used in the experiments.
This UserLibri dataset is a re-formatted version of the LibriSpeech data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">22</a>]</cite> that is derived from English audiobooks and contains 1000 hours of speech sampled at 16 kHz.
An example of audio-text pair from the UserLibri dataset is shown in Figure <a href="#S4.F2" title="Figure 2 ‚Ä£ IV ASR in peer-to-peer learning ‚Ä£ Sequence-to-sequence models in peer-to-peer learning: A practical application" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
In UserLibri, the data is reorganized into individual <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">user</span> datasets consisting of paired audio-transcript examples and domain-matching text-only data for each user.
Data segregated in this manner allows for a simulation of realistic scenario with user-specific audio clips.
The UserLibri dataset contains 55 unique users (as part of the test-clean split), with average of 47.1 audio clips per user.
The duration of audio samples ranges from 1 to 35 seconds, with an average length of 7.5 seconds, resulting in approximately 5 hours of audio recording in total.
Within these audio clips, the number of spoken words spans from 1 to 96, with an average of 20 words per clip.
Such small amount of local data may cause problems during local training which may lead to poor model performance.
Threfore, an additional LJ Speech dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite> will be used in additional experiments to confirm any findings from the experiments regarding the UserLibri dataset.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2406.02565/assets/imgs/03_userlibri_example.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="210" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">An example of raw and processed audio-text pair from the UserLibri dataset.

</span></figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The LJ Speech dataset consists of 13,100 short audio clips with accompanying transcriptions for clip.
The audio recordings sourced from the LJ Speech dataset exhibit varying durations, spanning from 1 to 10 seconds, with an average duration of 6.5 seconds per clip, resulting in around 24 hours of audio recordings in total.
Within these recordings, the number of spoken words ranges from 1 to 39, with an average of 17 words per clip.
Both datasets were split in a 70%-30% for training and validation purposes.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">To ensure uniformity in input dimensions for training, spectrograms and character tokens underwent a padding process, aligning them to a consistent length.
Specifically, each spectrogram was extended or truncated to a fixed length of 2048 time steps, while character tokens were padded or trimmed to 256 characters.
Spectrograms and token sequences were padded with zeros where necessary. An example of short and long audio-text sequence in Figure <a href="#S4.F3" title="Figure 3 ‚Ä£ IV ASR in peer-to-peer learning ‚Ä£ Sequence-to-sequence models in peer-to-peer learning: A practical application" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.02565/assets/imgs/03_padding_short.png" id="S4.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="169" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.02565/assets/imgs/03_padding_long.png" id="S4.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="177" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.4.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.5.2" class="ltx_text" style="font-size:90%;">Examples of short audio-text pair with added audio and text padding, compared to a long audio-text pair.</span></figcaption>
</figure>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Model.</span>
Model architecture consists of two blocks of 2D Convolutional layers that are followed by a Batch Normalization layer and ReLu activation function.
A single RNN GRU layer with 512 units is used to constrain the number of model parameters. GRU layer is followed by a Fully connected layer, Dropout layer, and the final Fully connected output layer.
The resulting model is comprised of 7.7M model parameters which is of acceptable size for edge devices, and satisfactory for this use case since the goal is to evaluate the applicability of Seq2Seq models in peer-to-peer environments, rather than achiveing state-of-the-art results.
To reduce memory consumption and improve the training process, batch size is set to 8.
Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite> optimizer with a fixed learning rate of 0.0001 is used in all experiments.
Model predicts the probabilities of each individual character of English alphabet and uses the greedy decoder to select the most probable character at each time step.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.9" class="ltx_p"><span id="S4.p5.9.1" class="ltx_text ltx_font_bold">Methodology.</span>
The learning process of agents was emulated in memory using two distinct peer-to-peer learning methodologies: Pull-gossip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite> and P2P-BN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">26</a>]</cite>.
This simulation engenders a cyclical process wherein each local training step is succeeded by a communication step, with these phases iteratively recurring.
A directed <span id="S4.p5.9.2" class="ltx_text ltx_font_italic">sparse</span> network topology with three peer connections was utilized for both methods.
In the Pull-gossip approach, upon pulling models from its peers, an agent forms a new model by aggregating all received models using a weighted summation mechanism: <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="x_{i}=\sum_{\{w_{ij}&gt;0\}}w_{ij}x_{j}" display="inline"><semantics id="S4.p5.1.m1.1a"><mrow id="S4.p5.1.m1.1.2" xref="S4.p5.1.m1.1.2.cmml"><msub id="S4.p5.1.m1.1.2.2" xref="S4.p5.1.m1.1.2.2.cmml"><mi id="S4.p5.1.m1.1.2.2.2" xref="S4.p5.1.m1.1.2.2.2.cmml">x</mi><mi id="S4.p5.1.m1.1.2.2.3" xref="S4.p5.1.m1.1.2.2.3.cmml">i</mi></msub><mo rspace="0.111em" id="S4.p5.1.m1.1.2.1" xref="S4.p5.1.m1.1.2.1.cmml">=</mo><mrow id="S4.p5.1.m1.1.2.3" xref="S4.p5.1.m1.1.2.3.cmml"><msub id="S4.p5.1.m1.1.2.3.1" xref="S4.p5.1.m1.1.2.3.1.cmml"><mo id="S4.p5.1.m1.1.2.3.1.2" xref="S4.p5.1.m1.1.2.3.1.2.cmml">‚àë</mo><mrow id="S4.p5.1.m1.1.1.1.1" xref="S4.p5.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S4.p5.1.m1.1.1.1.1.2" xref="S4.p5.1.m1.1.1.1.2.cmml">{</mo><mrow id="S4.p5.1.m1.1.1.1.1.1" xref="S4.p5.1.m1.1.1.1.1.1.cmml"><msub id="S4.p5.1.m1.1.1.1.1.1.2" xref="S4.p5.1.m1.1.1.1.1.1.2.cmml"><mi id="S4.p5.1.m1.1.1.1.1.1.2.2" xref="S4.p5.1.m1.1.1.1.1.1.2.2.cmml">w</mi><mrow id="S4.p5.1.m1.1.1.1.1.1.2.3" xref="S4.p5.1.m1.1.1.1.1.1.2.3.cmml"><mi id="S4.p5.1.m1.1.1.1.1.1.2.3.2" xref="S4.p5.1.m1.1.1.1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.1.1.1.1.2.3.1" xref="S4.p5.1.m1.1.1.1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S4.p5.1.m1.1.1.1.1.1.2.3.3" xref="S4.p5.1.m1.1.1.1.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="S4.p5.1.m1.1.1.1.1.1.1" xref="S4.p5.1.m1.1.1.1.1.1.1.cmml">&gt;</mo><mn id="S4.p5.1.m1.1.1.1.1.1.3" xref="S4.p5.1.m1.1.1.1.1.1.3.cmml">0</mn></mrow><mo stretchy="false" id="S4.p5.1.m1.1.1.1.1.3" xref="S4.p5.1.m1.1.1.1.2.cmml">}</mo></mrow></msub><mrow id="S4.p5.1.m1.1.2.3.2" xref="S4.p5.1.m1.1.2.3.2.cmml"><msub id="S4.p5.1.m1.1.2.3.2.2" xref="S4.p5.1.m1.1.2.3.2.2.cmml"><mi id="S4.p5.1.m1.1.2.3.2.2.2" xref="S4.p5.1.m1.1.2.3.2.2.2.cmml">w</mi><mrow id="S4.p5.1.m1.1.2.3.2.2.3" xref="S4.p5.1.m1.1.2.3.2.2.3.cmml"><mi id="S4.p5.1.m1.1.2.3.2.2.3.2" xref="S4.p5.1.m1.1.2.3.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.2.3.2.2.3.1" xref="S4.p5.1.m1.1.2.3.2.2.3.1.cmml">‚Äã</mo><mi id="S4.p5.1.m1.1.2.3.2.2.3.3" xref="S4.p5.1.m1.1.2.3.2.2.3.3.cmml">j</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.2.3.2.1" xref="S4.p5.1.m1.1.2.3.2.1.cmml">‚Äã</mo><msub id="S4.p5.1.m1.1.2.3.2.3" xref="S4.p5.1.m1.1.2.3.2.3.cmml"><mi id="S4.p5.1.m1.1.2.3.2.3.2" xref="S4.p5.1.m1.1.2.3.2.3.2.cmml">x</mi><mi id="S4.p5.1.m1.1.2.3.2.3.3" xref="S4.p5.1.m1.1.2.3.2.3.3.cmml">j</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.2.cmml" xref="S4.p5.1.m1.1.2"><eq id="S4.p5.1.m1.1.2.1.cmml" xref="S4.p5.1.m1.1.2.1"></eq><apply id="S4.p5.1.m1.1.2.2.cmml" xref="S4.p5.1.m1.1.2.2"><csymbol cd="ambiguous" id="S4.p5.1.m1.1.2.2.1.cmml" xref="S4.p5.1.m1.1.2.2">subscript</csymbol><ci id="S4.p5.1.m1.1.2.2.2.cmml" xref="S4.p5.1.m1.1.2.2.2">ùë•</ci><ci id="S4.p5.1.m1.1.2.2.3.cmml" xref="S4.p5.1.m1.1.2.2.3">ùëñ</ci></apply><apply id="S4.p5.1.m1.1.2.3.cmml" xref="S4.p5.1.m1.1.2.3"><apply id="S4.p5.1.m1.1.2.3.1.cmml" xref="S4.p5.1.m1.1.2.3.1"><csymbol cd="ambiguous" id="S4.p5.1.m1.1.2.3.1.1.cmml" xref="S4.p5.1.m1.1.2.3.1">subscript</csymbol><sum id="S4.p5.1.m1.1.2.3.1.2.cmml" xref="S4.p5.1.m1.1.2.3.1.2"></sum><set id="S4.p5.1.m1.1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.1.1"><apply id="S4.p5.1.m1.1.1.1.1.1.cmml" xref="S4.p5.1.m1.1.1.1.1.1"><gt id="S4.p5.1.m1.1.1.1.1.1.1.cmml" xref="S4.p5.1.m1.1.1.1.1.1.1"></gt><apply id="S4.p5.1.m1.1.1.1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.p5.1.m1.1.1.1.1.1.2.1.cmml" xref="S4.p5.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.p5.1.m1.1.1.1.1.1.2.2.cmml" xref="S4.p5.1.m1.1.1.1.1.1.2.2">ùë§</ci><apply id="S4.p5.1.m1.1.1.1.1.1.2.3.cmml" xref="S4.p5.1.m1.1.1.1.1.1.2.3"><times id="S4.p5.1.m1.1.1.1.1.1.2.3.1.cmml" xref="S4.p5.1.m1.1.1.1.1.1.2.3.1"></times><ci id="S4.p5.1.m1.1.1.1.1.1.2.3.2.cmml" xref="S4.p5.1.m1.1.1.1.1.1.2.3.2">ùëñ</ci><ci id="S4.p5.1.m1.1.1.1.1.1.2.3.3.cmml" xref="S4.p5.1.m1.1.1.1.1.1.2.3.3">ùëó</ci></apply></apply><cn type="integer" id="S4.p5.1.m1.1.1.1.1.1.3.cmml" xref="S4.p5.1.m1.1.1.1.1.1.3">0</cn></apply></set></apply><apply id="S4.p5.1.m1.1.2.3.2.cmml" xref="S4.p5.1.m1.1.2.3.2"><times id="S4.p5.1.m1.1.2.3.2.1.cmml" xref="S4.p5.1.m1.1.2.3.2.1"></times><apply id="S4.p5.1.m1.1.2.3.2.2.cmml" xref="S4.p5.1.m1.1.2.3.2.2"><csymbol cd="ambiguous" id="S4.p5.1.m1.1.2.3.2.2.1.cmml" xref="S4.p5.1.m1.1.2.3.2.2">subscript</csymbol><ci id="S4.p5.1.m1.1.2.3.2.2.2.cmml" xref="S4.p5.1.m1.1.2.3.2.2.2">ùë§</ci><apply id="S4.p5.1.m1.1.2.3.2.2.3.cmml" xref="S4.p5.1.m1.1.2.3.2.2.3"><times id="S4.p5.1.m1.1.2.3.2.2.3.1.cmml" xref="S4.p5.1.m1.1.2.3.2.2.3.1"></times><ci id="S4.p5.1.m1.1.2.3.2.2.3.2.cmml" xref="S4.p5.1.m1.1.2.3.2.2.3.2">ùëñ</ci><ci id="S4.p5.1.m1.1.2.3.2.2.3.3.cmml" xref="S4.p5.1.m1.1.2.3.2.2.3.3">ùëó</ci></apply></apply><apply id="S4.p5.1.m1.1.2.3.2.3.cmml" xref="S4.p5.1.m1.1.2.3.2.3"><csymbol cd="ambiguous" id="S4.p5.1.m1.1.2.3.2.3.1.cmml" xref="S4.p5.1.m1.1.2.3.2.3">subscript</csymbol><ci id="S4.p5.1.m1.1.2.3.2.3.2.cmml" xref="S4.p5.1.m1.1.2.3.2.3.2">ùë•</ci><ci id="S4.p5.1.m1.1.2.3.2.3.3.cmml" xref="S4.p5.1.m1.1.2.3.2.3.3">ùëó</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">x_{i}=\sum_{\{w_{ij}&gt;0\}}w_{ij}x_{j}</annotation></semantics></math>, where <math id="S4.p5.2.m2.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S4.p5.2.m2.1a"><msub id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml"><mi id="S4.p5.2.m2.1.1.2" xref="S4.p5.2.m2.1.1.2.cmml">x</mi><mi id="S4.p5.2.m2.1.1.3" xref="S4.p5.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><apply id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p5.2.m2.1.1.1.cmml" xref="S4.p5.2.m2.1.1">subscript</csymbol><ci id="S4.p5.2.m2.1.1.2.cmml" xref="S4.p5.2.m2.1.1.2">ùë•</ci><ci id="S4.p5.2.m2.1.1.3.cmml" xref="S4.p5.2.m2.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">x_{i}</annotation></semantics></math> represents the new model of agent <math id="S4.p5.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.p5.3.m3.1a"><mi id="S4.p5.3.m3.1.1" xref="S4.p5.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.p5.3.m3.1b"><ci id="S4.p5.3.m3.1.1.cmml" xref="S4.p5.3.m3.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m3.1c">i</annotation></semantics></math>, <math id="S4.p5.4.m4.1" class="ltx_Math" alttext="x_{j}" display="inline"><semantics id="S4.p5.4.m4.1a"><msub id="S4.p5.4.m4.1.1" xref="S4.p5.4.m4.1.1.cmml"><mi id="S4.p5.4.m4.1.1.2" xref="S4.p5.4.m4.1.1.2.cmml">x</mi><mi id="S4.p5.4.m4.1.1.3" xref="S4.p5.4.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p5.4.m4.1b"><apply id="S4.p5.4.m4.1.1.cmml" xref="S4.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p5.4.m4.1.1.1.cmml" xref="S4.p5.4.m4.1.1">subscript</csymbol><ci id="S4.p5.4.m4.1.1.2.cmml" xref="S4.p5.4.m4.1.1.2">ùë•</ci><ci id="S4.p5.4.m4.1.1.3.cmml" xref="S4.p5.4.m4.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.4.m4.1c">x_{j}</annotation></semantics></math> denotes the received model from peer <math id="S4.p5.5.m5.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.p5.5.m5.1a"><mi id="S4.p5.5.m5.1.1" xref="S4.p5.5.m5.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.p5.5.m5.1b"><ci id="S4.p5.5.m5.1.1.cmml" xref="S4.p5.5.m5.1.1">ùëó</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.5.m5.1c">j</annotation></semantics></math>, and <math id="S4.p5.6.m6.1" class="ltx_Math" alttext="w_{ij}" display="inline"><semantics id="S4.p5.6.m6.1a"><msub id="S4.p5.6.m6.1.1" xref="S4.p5.6.m6.1.1.cmml"><mi id="S4.p5.6.m6.1.1.2" xref="S4.p5.6.m6.1.1.2.cmml">w</mi><mrow id="S4.p5.6.m6.1.1.3" xref="S4.p5.6.m6.1.1.3.cmml"><mi id="S4.p5.6.m6.1.1.3.2" xref="S4.p5.6.m6.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p5.6.m6.1.1.3.1" xref="S4.p5.6.m6.1.1.3.1.cmml">‚Äã</mo><mi id="S4.p5.6.m6.1.1.3.3" xref="S4.p5.6.m6.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p5.6.m6.1b"><apply id="S4.p5.6.m6.1.1.cmml" xref="S4.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S4.p5.6.m6.1.1.1.cmml" xref="S4.p5.6.m6.1.1">subscript</csymbol><ci id="S4.p5.6.m6.1.1.2.cmml" xref="S4.p5.6.m6.1.1.2">ùë§</ci><apply id="S4.p5.6.m6.1.1.3.cmml" xref="S4.p5.6.m6.1.1.3"><times id="S4.p5.6.m6.1.1.3.1.cmml" xref="S4.p5.6.m6.1.1.3.1"></times><ci id="S4.p5.6.m6.1.1.3.2.cmml" xref="S4.p5.6.m6.1.1.3.2">ùëñ</ci><ci id="S4.p5.6.m6.1.1.3.3.cmml" xref="S4.p5.6.m6.1.1.3.3">ùëó</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.6.m6.1c">w_{ij}</annotation></semantics></math> signifies the weight associated with the connection between agents <math id="S4.p5.7.m7.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.p5.7.m7.1a"><mi id="S4.p5.7.m7.1.1" xref="S4.p5.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.p5.7.m7.1b"><ci id="S4.p5.7.m7.1.1.cmml" xref="S4.p5.7.m7.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.7.m7.1c">i</annotation></semantics></math> and <math id="S4.p5.8.m8.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.p5.8.m8.1a"><mi id="S4.p5.8.m8.1.1" xref="S4.p5.8.m8.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.p5.8.m8.1b"><ci id="S4.p5.8.m8.1.1.cmml" xref="S4.p5.8.m8.1.1">ùëó</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.8.m8.1c">j</annotation></semantics></math>.
Conversely, in the P2P-BN method, upon receiving a model from a peer, an agent constructs a new model by averaging the received model with its local model: <math id="S4.p5.9.m9.1" class="ltx_Math" alttext="x_{i}=\frac{x_{i}+x_{j}}{2}" display="inline"><semantics id="S4.p5.9.m9.1a"><mrow id="S4.p5.9.m9.1.1" xref="S4.p5.9.m9.1.1.cmml"><msub id="S4.p5.9.m9.1.1.2" xref="S4.p5.9.m9.1.1.2.cmml"><mi id="S4.p5.9.m9.1.1.2.2" xref="S4.p5.9.m9.1.1.2.2.cmml">x</mi><mi id="S4.p5.9.m9.1.1.2.3" xref="S4.p5.9.m9.1.1.2.3.cmml">i</mi></msub><mo id="S4.p5.9.m9.1.1.1" xref="S4.p5.9.m9.1.1.1.cmml">=</mo><mfrac id="S4.p5.9.m9.1.1.3" xref="S4.p5.9.m9.1.1.3.cmml"><mrow id="S4.p5.9.m9.1.1.3.2" xref="S4.p5.9.m9.1.1.3.2.cmml"><msub id="S4.p5.9.m9.1.1.3.2.2" xref="S4.p5.9.m9.1.1.3.2.2.cmml"><mi id="S4.p5.9.m9.1.1.3.2.2.2" xref="S4.p5.9.m9.1.1.3.2.2.2.cmml">x</mi><mi id="S4.p5.9.m9.1.1.3.2.2.3" xref="S4.p5.9.m9.1.1.3.2.2.3.cmml">i</mi></msub><mo id="S4.p5.9.m9.1.1.3.2.1" xref="S4.p5.9.m9.1.1.3.2.1.cmml">+</mo><msub id="S4.p5.9.m9.1.1.3.2.3" xref="S4.p5.9.m9.1.1.3.2.3.cmml"><mi id="S4.p5.9.m9.1.1.3.2.3.2" xref="S4.p5.9.m9.1.1.3.2.3.2.cmml">x</mi><mi id="S4.p5.9.m9.1.1.3.2.3.3" xref="S4.p5.9.m9.1.1.3.2.3.3.cmml">j</mi></msub></mrow><mn id="S4.p5.9.m9.1.1.3.3" xref="S4.p5.9.m9.1.1.3.3.cmml">2</mn></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.9.m9.1b"><apply id="S4.p5.9.m9.1.1.cmml" xref="S4.p5.9.m9.1.1"><eq id="S4.p5.9.m9.1.1.1.cmml" xref="S4.p5.9.m9.1.1.1"></eq><apply id="S4.p5.9.m9.1.1.2.cmml" xref="S4.p5.9.m9.1.1.2"><csymbol cd="ambiguous" id="S4.p5.9.m9.1.1.2.1.cmml" xref="S4.p5.9.m9.1.1.2">subscript</csymbol><ci id="S4.p5.9.m9.1.1.2.2.cmml" xref="S4.p5.9.m9.1.1.2.2">ùë•</ci><ci id="S4.p5.9.m9.1.1.2.3.cmml" xref="S4.p5.9.m9.1.1.2.3">ùëñ</ci></apply><apply id="S4.p5.9.m9.1.1.3.cmml" xref="S4.p5.9.m9.1.1.3"><divide id="S4.p5.9.m9.1.1.3.1.cmml" xref="S4.p5.9.m9.1.1.3"></divide><apply id="S4.p5.9.m9.1.1.3.2.cmml" xref="S4.p5.9.m9.1.1.3.2"><plus id="S4.p5.9.m9.1.1.3.2.1.cmml" xref="S4.p5.9.m9.1.1.3.2.1"></plus><apply id="S4.p5.9.m9.1.1.3.2.2.cmml" xref="S4.p5.9.m9.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.p5.9.m9.1.1.3.2.2.1.cmml" xref="S4.p5.9.m9.1.1.3.2.2">subscript</csymbol><ci id="S4.p5.9.m9.1.1.3.2.2.2.cmml" xref="S4.p5.9.m9.1.1.3.2.2.2">ùë•</ci><ci id="S4.p5.9.m9.1.1.3.2.2.3.cmml" xref="S4.p5.9.m9.1.1.3.2.2.3">ùëñ</ci></apply><apply id="S4.p5.9.m9.1.1.3.2.3.cmml" xref="S4.p5.9.m9.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.p5.9.m9.1.1.3.2.3.1.cmml" xref="S4.p5.9.m9.1.1.3.2.3">subscript</csymbol><ci id="S4.p5.9.m9.1.1.3.2.3.2.cmml" xref="S4.p5.9.m9.1.1.3.2.3.2">ùë•</ci><ci id="S4.p5.9.m9.1.1.3.2.3.3.cmml" xref="S4.p5.9.m9.1.1.3.2.3.3">ùëó</ci></apply></apply><cn type="integer" id="S4.p5.9.m9.1.1.3.3.cmml" xref="S4.p5.9.m9.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.9.m9.1c">x_{i}=\frac{x_{i}+x_{j}}{2}</annotation></semantics></math>.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">Agent‚Äôs local test accuracy is measured after performing local SGD updates (i.e. an epoch).
As comparison, results of a centrally trained model on pooled data are also analyzed.
Experiments regarding UserLibri dataset utilized all 55 user dataset in a 55 agent environment, while the data from LJ Speech was uniformly divided across 55 agents in the experiments utilizing the LJ Speech dataset.
To assess the performance of agents‚Äô models, the average model CTC loss and Word Error Rate (WER) metrics will be examined.
The computation of average local model performance across all agents using their respective local datasets serves as a pivotal evaluation metric within peer-to-peer environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>, <a href="#bib.bibx26" title="" class="ltx_ref">26</a>, <a href="#bib.bibx25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Experiments on UserLibri dataset</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">To establish a baseline for the optimal model performance, a central model is trained using aggregated data from all 55 users within the UserLibri dataset.
Since batch size of 8 used, this resulted in 226 training batches which showed to be insufficient to train a good model. The onset of overfitting was observed approximately at the 10th epoch, while the lowest WER achieved was 84.34%, with character error rate (CER) at 39.42%.
These suboptimal outcomes indicate that the dataset‚Äôs size may be insufficient to adequately train the model, thereby limiting its capacity to achieve satisfactory performance levels.
For reference, initial experiments conducted on the UserLibri dataset obtained average per-user WER value of around 2.5% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">5</a>]</cite> by using a 86M parameter Conformer Hybrid Autoregressive Transducer (HAT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">28</a>]</cite> model.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The following are some examples of target (correct) and (model) prediction sentences:</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">Example 1</p>
<ul id="S5.I1.i1.I1" class="ltx_itemize">
<li id="S5.I1.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S5.I1.i1.I1.i1.1.1.1" class="ltx_text ltx_font_bold">‚Äì</span></span> 
<div id="S5.I1.i1.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Target</span>: A ROBBER VIKING SAID THE KING AND SCOWLED AT ME</p>
</div>
</li>
<li id="S5.I1.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S5.I1.i1.I1.i2.1.1.1" class="ltx_text ltx_font_bold">‚Äì</span></span> 
<div id="S5.I1.i1.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i1.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Prediction</span>: A R BER FI GENGC AD THE CING AND Y SCOULD IT ME</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">Example 2</p>
<ul id="S5.I1.i2.I1" class="ltx_itemize">
<li id="S5.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S5.I1.i2.I1.i1.1.1.1" class="ltx_text ltx_font_bold">‚Äì</span></span> 
<div id="S5.I1.i2.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i2.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Target</span>: EDISON HELD THAT THE ELECTRICITY SOLD MUST BE MEASURED JUST LIKE GAS OR WATER AND HE PROCEEDED TO DEVELOP A METER</p>
</div>
</li>
<li id="S5.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S5.I1.i2.I1.i2.1.1.1" class="ltx_text ltx_font_bold">‚Äì</span></span> 
<div id="S5.I1.i2.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Prediction</span>: ET ISSUNHO THET HE ULECTRISINTE SID MUST BEIN MISUR JUS LI GIS OR OT ER AND YH PRESUTOD TO DEVU MITER</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">Example 3</p>
<ul id="S5.I1.i3.I1" class="ltx_itemize">
<li id="S5.I1.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S5.I1.i3.I1.i1.1.1.1" class="ltx_text ltx_font_bold">‚Äì</span></span> 
<div id="S5.I1.i3.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i3.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Target</span>: WHETHER OR NOT THIS PRINCIPLE IS LIABLE TO EXCEPTIONS EVERYONE WOULD AGREE THAT IS HAS A BROAD MEASURE OF TRUTH THOUGH THE WORD EXACTLY MIGHT SEEM AN OVERSTATEMENT AND IT MIGHT SEEM MORE CORRECT TO SAY THAT IDEAS APPROXIMATELY REPRESENT IMPRESSIO</p>
</div>
</li>
<li id="S5.I1.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S5.I1.i3.I1.i2.1.1.1" class="ltx_text ltx_font_bold">‚Äì</span></span> 
<div id="S5.I1.i3.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i3.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Prediction</span>: WTHER NOT THESPRENCSPUD IS LBLE TEXKCEPIOANS EVERE ONE WUDA GRETHEK IS HAS ABROUDNMUSURE TRTE BHE HEARED EXSARCCLAY MENEN OVERSSTEGMKT AND IT MASENG MOR CURECT TISSAY THAT IEAD DIS UPROCSIN YD REPRENTUND PTIONS</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">As stated, in the peer-to-peer environment, each user autonomously trains their respective model utilizing locally available data sourced from the UserLibri dataset, subsequently exchanging their locally trained model with peers.
Evidently, the peer-to-peer setting necessitates significantly more training iterations to attain performance levels comparable to those of centrally trained models.
Despite yielding slightly higher WER, with Pull-gossip method achieving 92% and P2P-BN method achieving 87%, the findings indicate that Seq2Seq models can indeed be applied to ASR tasks within peer-to-peer environments.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Experiments on LJ Speech dataset</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">A central model was trained on all training LJ Speech dataset to establish a baseline result.
The model was able to substantially better results as compared to the previous model trained on UserLibri dataset, mainly because of the larger number of samples.
While the model exhibited some form of overfitting, observed by the divergence between training and validation loss, the model still managed to achieve quite low WER value, around 38.61% with CER at 10.59%.
For reference, recent study demonstrated that standard DeepSpeech2 architecture achieved a WER value of 0.1% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p">The following are some examples of target (correct) and (model) prediction sentences:</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p">Example 1</p>
<ul id="S5.I2.i1.I1" class="ltx_itemize">
<li id="S5.I2.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S5.I2.i1.I1.i1.1.1.1" class="ltx_text ltx_font_bold">‚Äì</span></span> 
<div id="S5.I2.i1.I1.i1.p1" class="ltx_para">
<p id="S5.I2.i1.I1.i1.p1.1" class="ltx_p"><span id="S5.I2.i1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Target</span>: NO NIGHT CLUBS OR BOWLING ALLEYS NO PLACES OF RECREATION EXCEPT THE TRADE UNION DANCES I HAVE HAD ENOUGH</p>
</div>
</li>
<li id="S5.I2.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S5.I2.i1.I1.i2.1.1.1" class="ltx_text ltx_font_bold">‚Äì</span></span> 
<div id="S5.I2.i1.I1.i2.p1" class="ltx_para">
<p id="S5.I2.i1.I1.i2.p1.1" class="ltx_p"><span id="S5.I2.i1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Prediction</span>: NOW NHIGH KCLOBS ER BOULLIG ALLIYS NO PLACES OFREACRIATION EXEPT THE TRA UGIN DANCES I HAVD HA A</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p">Example 2</p>
<ul id="S5.I2.i2.I1" class="ltx_itemize">
<li id="S5.I2.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S5.I2.i2.I1.i1.1.1.1" class="ltx_text ltx_font_bold">‚Äì</span></span> 
<div id="S5.I2.i2.I1.i1.p1" class="ltx_para">
<p id="S5.I2.i2.I1.i1.p1.1" class="ltx_p"><span id="S5.I2.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Target</span>: TO AN INFERENCE THAT THE VIOLATION OF THE REGULATION HAD CONTRIBUTED TO THE TRAGIC EVENTS OF NOVEMBER TWENTYTWO</p>
</div>
</li>
<li id="S5.I2.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S5.I2.i2.I1.i2.1.1.1" class="ltx_text ltx_font_bold">‚Äì</span></span> 
<div id="S5.I2.i2.I1.i2.p1" class="ltx_para">
<p id="S5.I2.i2.I1.i2.p1.1" class="ltx_p"><span id="S5.I2.i2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Prediction</span>: TO AN INFRIND S THAT THE VIOLATION OF THE REGULATION AD CONTRIBIUTED TO THE TRAGICGAVANS OF NOVEMBER TWENTYTWO</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S5.I2.i3.p1" class="ltx_para">
<p id="S5.I2.i3.p1.1" class="ltx_p">Example 3</p>
<ul id="S5.I2.i3.I1" class="ltx_itemize">
<li id="S5.I2.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S5.I2.i3.I1.i1.1.1.1" class="ltx_text ltx_font_bold">‚Äì</span></span> 
<div id="S5.I2.i3.I1.i1.p1" class="ltx_para">
<p id="S5.I2.i3.I1.i1.p1.1" class="ltx_p"><span id="S5.I2.i3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Target</span>: UNTIL APRIL NINETEEN SIXTY FBI ACTIVITY CONSISTED OF PLACING IN OSWALD‚ÄôS FILE</p>
</div>
</li>
<li id="S5.I2.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S5.I2.i3.I1.i2.1.1.1" class="ltx_text ltx_font_bold">‚Äì</span></span> 
<div id="S5.I2.i3.I1.i2.p1" class="ltx_para">
<p id="S5.I2.i3.I1.i2.p1.1" class="ltx_p"><span id="S5.I2.i3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Prediction</span>: UNTIL APRIL NINETEEN SIXTY FBI ACTIVITY CONSISTED OF PLACING IN OSWALD‚ÄôS FIL</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">In the peer-to-peer scenario, the LJ Speech dataset was uniformly partitioned among the 55 agents, resulting in an allocation of 166 training examples per agent.
This allocation represents a considerable increase, exceeding threefold, in training data availability compared to the experiment conducted with the UserLibri dataset.
Consistent with prior experiments, peer-to-peer learning necessitated a notable increase in training iterations to achieve model convergence.
However, it is noteworthy that the number of training iterations until convergence remained comparable to previous experiments, suggesting that increase of local data solely enhanced model performance without necessitating a commensurate increase in communication or training iterations.
Within this experiment, agents attained a WER value of 56% for the Pull-gossip and 52% for the P2P-BN method.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In conclusion, this discussion has shed light on the utilization of Seq2Seq models in both centralized and peer-to-peer learning environments for Automatic Speech Recognition (ASR) tasks.
While centralized training on pooled data from all users demonstrated more efficient convergence and lower Word Error Rates (WER), peer-to-peer learning, where each agent independently trains its own model using local data, showcased promising potential despite requiring more training iterations to achieve comparable results.
These findings underscore the applicability of Seq2Seq models in peer-to-peer settings.
However, a critical observation gleaned from the experiments is the pivotal role played by the availability and richness of local training data across decentralized agents.
The correlation between larger local datasets and lower Word Error Rates (WER) on the LJ Speech dataset underscores the importance of data quantity in achieving optimal performance with Seq2Seq models. However, it‚Äôs crucial not to assume this constraint for decentralized agents, as they may not have access to substantial amounts of local data. Therefore, future research endeavors should prioritize the exploration and development of methods that enable robust learning performance for Seq2Seq models, even in scenarios where agents have access to only small quantities of data
Additionally, future research endeavors should focus on addressing challenges such as slow convergence rates in peer-to-peer learning and devising techniques to expedite the learning process, thereby facilitating more efficient and scalable deployment of Seq2Seq models in decentralized settings.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Dario Amodei et al.
</span>
<span class="ltx_bibblock">‚ÄúDeep Speech 2: End-to-End Speech Recognition in English and Mandarin‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">CoRR</em> <span id="bib.bibx1.2.2" class="ltx_text ltx_font_bold">abs/1512.02595</span>, 2015
</span>
<span class="ltx_bibblock">arXiv: <a target="_blank" href="http://arxiv.org/abs/1512.02595" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1512.02595</a>
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Mahmoud Assran, Nicolas Loizou, Nicolas Ballas and Mike Rabbat
</span>
<span class="ltx_bibblock">‚ÄúStochastic Gradient Push for Distributed Deep Learning‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 36th International Conference on Machine Learning</em> <span id="bib.bibx2.2.2" class="ltx_text ltx_font_bold">97</span>, Proceedings of Machine Learning Research
</span>
<span class="ltx_bibblock">PMLR, 2019, pp. 344‚Äì353
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://proceedings.mlr.press/v97/assran19a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v97/assran19a.html</a>
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Alexei Baevski, Henry Zhou, Abdelrahman Mohamed and Michael Auli
</span>
<span class="ltx_bibblock">‚Äúwav2vec 2.0: a framework for self-supervised learning of speech representations‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 34th International Conference on Neural Information Processing Systems</em>, NIPS ‚Äô20
</span>
<span class="ltx_bibblock">¬°conf-loc¬ø, ¬°city¬øVancouver¬°/city¬ø, ¬°state¬øBC¬°/state¬ø, ¬°country¬øCanada¬°/country¬ø, ¬°/conf-loc¬ø: Curran Associates Inc., 2020
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Michael Blot, David Picard, Matthieu Cord and Nicolas Thome
</span>
<span class="ltx_bibblock">‚ÄúGossip training for deep learning‚Äù, 2016
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Theresa Breiner et al.
</span>
<span class="ltx_bibblock">‚ÄúUserLibri: A Dataset for ASR Personalization Using Only Text‚Äù, 2022
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2207.00706" title="" class="ltx_ref ltx_href">2207.00706 [eess.AS]</a>
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">William Chan, Navdeep Jaitly, Quoc V. Le and Oriol Vinyals
</span>
<span class="ltx_bibblock">‚ÄúListen, Attend and Spell‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">CoRR</em> <span id="bib.bibx6.2.2" class="ltx_text ltx_font_bold">abs/1508.01211</span>, 2015
</span>
<span class="ltx_bibblock">arXiv: <a target="_blank" href="http://arxiv.org/abs/1508.01211" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1508.01211</a>
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Zhiyun Fan, Shiyu Zhou and Bo Xu
</span>
<span class="ltx_bibblock">‚ÄúUnsupervised pre-training for sequence to sequence speech recognition‚Äù, 2020
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/1910.12418" title="" class="ltx_ref ltx_href">1910.12418 [cs.SD]</a>
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Alex Graves, Santiago Fern√°ndez, Faustino Gomez and J√ºrgen Schmidhuber
</span>
<span class="ltx_bibblock">‚ÄúConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd International Conference on Machine Learning</em>, ICML ‚Äô06
</span>
<span class="ltx_bibblock">Pittsburgh, Pennsylvania, USA: Association for Computing Machinery, 2006, pp. 369‚Äì376
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1145/1143844.1143891" title="" class="ltx_ref ltx_href">10.1145/1143844.1143891</a>
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Shangwei Guo, Tianwei Zhang, Tao Xiang and Yang Liu
</span>
<span class="ltx_bibblock">‚ÄúDifferentially Private Decentralized Learning‚Äù, 2020
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2006.07817" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2006.07817</a>
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Keith Ito and Linda Johnson
</span>
<span class="ltx_bibblock">‚ÄúThe LJ Speech Dataset‚Äù, <a target="_blank" href="https://keithito.com/LJ-Speech-Dataset/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://keithito.com/LJ-Speech-Dataset/</a>, 2017
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Peter H. Jin, Qiaochu Yuan, Forrest N. Iandola and Kurt Keutzer
</span>
<span class="ltx_bibblock">‚ÄúHow to scale distributed deep learning?‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx11.1.1" class="ltx_emph ltx_font_italic">CoRR</em> <span id="bib.bibx11.2.2" class="ltx_text ltx_font_bold">abs/1611.04581</span>, 2016
</span>
<span class="ltx_bibblock">arXiv: <a target="_blank" href="http://arxiv.org/abs/1611.04581" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1611.04581</a>
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Diederik P. Kingma and Jimmy Ba
</span>
<span class="ltx_bibblock">‚ÄúAdam: A Method for Stochastic Optimization‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</em>, 2015
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/1412.6980" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1412.6980</a>
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Julia Kaiwen Lau et al.
</span>
<span class="ltx_bibblock">‚ÄúSynthesizing Speech Test Cases with Text-to-Speech? An Empirical Study on the False Alarms in Automated Speech Recognition Testing‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis</em>, ISSTA 2023
</span>
<span class="ltx_bibblock">¬°conf-loc¬ø, ¬°city¬øSeattle¬°/city¬ø, ¬°state¬øWA¬°/state¬ø, ¬°country¬øUSA¬°/country¬ø, ¬°/conf-loc¬ø: Association for Computing Machinery, 2023, pp. 1169‚Äì1181
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1145/3597926.3598126" title="" class="ltx_ref ltx_href">10.1145/3597926.3598126</a>
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Xiaoxiao Li et al.
</span>
<span class="ltx_bibblock">‚ÄúFedBN: Federated Learning on Non-IID Features via Local Batch Normalization‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>
</span>
<span class="ltx_bibblock">OpenReview.net, 2021
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Zexi Li et al.
</span>
<span class="ltx_bibblock">‚ÄúTowards Effective Clustered Federated Learning: A Peer-to-peer Framework with Adaptive Neighbor Matching‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Big Data</em>, 2022, pp. 1‚Äì16
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/TBDATA.2022.3222971" title="" class="ltx_ref ltx_href">10.1109/TBDATA.2022.3222971</a>
</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">Bill Yuchen Lin et al.
</span>
<span class="ltx_bibblock">‚ÄúFedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx16.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: NAACL 2022</em>
</span>
<span class="ltx_bibblock">Seattle, United States: Association for Computational Linguistics, 2022, pp. 157‚Äì175
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.18653/v1/2022.findings-naacl.13" title="" class="ltx_ref ltx_href">10.18653/v1/2022.findings-naacl.13</a>
</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Yujie Lu, Chao-Wei Huang, Huanli Zhan and Yong Zhuang
</span>
<span class="ltx_bibblock">‚ÄúFederated Natural Language Generation for Personalized Dialogue System‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">ArXiv</em> <span id="bib.bibx17.2.2" class="ltx_text ltx_font_bold">abs/2110.06419</span>, 2021
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://api.semanticscholar.org/CorpusID:238744124" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:238744124</a>
</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Xiaodong Ma et al.
</span>
<span class="ltx_bibblock">‚ÄúA state-of-the-art survey on solving non-IID data in Federated Learning‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx18.1.1" class="ltx_emph ltx_font_italic">Future Generation Computer Systems</em> <span id="bib.bibx18.2.2" class="ltx_text ltx_font_bold">135</span>, 2022, pp. 244‚Äì258
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/https://doi.org/10.1016/j.future.2022.05.003" title="" class="ltx_ref ltx_href">https://doi.org/10.1016/j.future.2022.05.003</a>
</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">Brendan McMahan et al.
</span>
<span class="ltx_bibblock">‚ÄúCommunication-Efficient Learning of Deep Networks from Decentralized Data‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</em> <span id="bib.bibx19.2.2" class="ltx_text ltx_font_bold">54</span>, Proceedings of Machine Learning Research
</span>
<span class="ltx_bibblock">PMLR, 2017, pp. 1273‚Äì1282
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://proceedings.mlr.press/v54/mcmahan17a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v54/mcmahan17a.html</a>
</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">J. Mills, J. Hu and G. Min
</span>
<span class="ltx_bibblock">‚ÄúMulti-Task Federated Learning for Personalised Deep Neural Networks in Edge Computing‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx20.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</em> <span id="bib.bibx20.2.2" class="ltx_text ltx_font_bold">33.03</span>
</span>
<span class="ltx_bibblock">Los Alamitos, CA, USA: IEEE Computer Society, 2022, pp. 630‚Äì641
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/TPDS.2021.3098467" title="" class="ltx_ref ltx_href">10.1109/TPDS.2021.3098467</a>
</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Tuan Nguyen et al.
</span>
<span class="ltx_bibblock">‚ÄúFederated Learning for ASR Based on wav2vec 2.0‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx21.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023, pp. 1‚Äì5
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/ICASSP49357.2023.10096426" title="" class="ltx_ref ltx_href">10.1109/ICASSP49357.2023.10096426</a>
</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Vassil Panayotov, Guoguo Chen, Daniel Povey and Sanjeev Khudanpur
</span>
<span class="ltx_bibblock">‚ÄúLibrispeech: An ASR corpus based on public domain audio books‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx22.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2015, pp. 5206‚Äì5210
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/ICASSP.2015.7178964" title="" class="ltx_ref ltx_href">10.1109/ICASSP.2015.7178964</a>
</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Diyah Puspitaningrum
</span>
<span class="ltx_bibblock">‚ÄúA Study of English-Indonesian Neural Machine Translation with Attention (Seq2Seq, ConvSeq2Seq, RNN, and MHA): A Comparative Study of NMT on English-Indonesian‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 6th International Conference on Sustainable Information Engineering and Technology</em>, SIET ‚Äô21
</span>
<span class="ltx_bibblock">Malang, Indonesia: Association for Computing Machinery, 2021, pp. 271‚Äì280
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1145/3479645.3479703" title="" class="ltx_ref ltx_href">10.1145/3479645.3479703</a>
</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Robert ≈†ajina, Nikola Tankoviƒá and Darko Etinger
</span>
<span class="ltx_bibblock">‚ÄúDecentralized trustless gossip training of deep neural networks‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx24.1.1" class="ltx_emph ltx_font_italic">2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)</em>, 2020, pp. 1080‚Äì1084
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.23919/MIPRO48935.2020.9245248" title="" class="ltx_ref ltx_href">10.23919/MIPRO48935.2020.9245248</a>
</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Robert ≈†ajina, Nikola Tankoviƒá and Ivo Ip≈°iƒá
</span>
<span class="ltx_bibblock">‚ÄúMulti-task peer-to-peer learning using the BERT transformer model‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx25.1.1" class="ltx_emph ltx_font_italic">Journal</em>, 2023
</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Robert ≈†ajina, Nikola Tankoviƒá and Ivo Ip≈°iƒá
</span>
<span class="ltx_bibblock">‚ÄúPeer-to-peer deep learning with non-IID data‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx26.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em> <span id="bib.bibx26.2.2" class="ltx_text ltx_font_bold">214</span>, 2023, pp. 119159
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/https://doi.org/10.1016/j.eswa.2022.119159" title="" class="ltx_ref ltx_href">https://doi.org/10.1016/j.eswa.2022.119159</a>
</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">Tian Shi, Yaser Keneshloo, Naren Ramakrishnan and Chandan K. Reddy
</span>
<span class="ltx_bibblock">‚ÄúNeural Abstractive Text Summarization with Sequence-to-Sequence Models‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx27.1.1" class="ltx_emph ltx_font_italic">ACM/IMS Trans. Data Sci.</em> <span id="bib.bibx27.2.2" class="ltx_text ltx_font_bold">2.1</span>
</span>
<span class="ltx_bibblock">New York, NY, USA: Association for Computing Machinery, 2021
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1145/3419106" title="" class="ltx_ref ltx_href">10.1145/3419106</a>
</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Ehsan Variani, David Rybach, Cyril Allauzen and Michael Riley
</span>
<span class="ltx_bibblock">‚ÄúHybrid Autoregressive Transducer (HAT)‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx28.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2020, pp. 6139‚Äì6143
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://api.semanticscholar.org/CorpusID:212737031" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:212737031</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.02564" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.02565" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.02565">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.02565" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.02566" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 23:25:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
