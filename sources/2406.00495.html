<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.00495] Audio-visual talker localization in video for spatial sound reproduction</title><meta property="og:description" content="Object-based audio production requires the positional metadata to be defined for each point-source object, including the key elements in the foreground of the sound scene.
In many media production use cases, both camer…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Audio-visual talker localization in video for spatial sound reproduction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Audio-visual talker localization in video for spatial sound reproduction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.00495">

<!--Generated on Sat Jul  6 01:46:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document" style="font-size:90%;">Audio-visual talker localization in video for spatial sound reproduction</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text" style="font-size:90%;">Object-based audio production requires the positional metadata to be defined for each point-source object, including the key elements in the foreground of the sound scene.
In many media production use cases, both cameras and microphones are employed to make recordings, and the human voice is often a key element.
In this research, we detect and locate the active speaker in the video, facilitating the automatic extraction of the positional metadata of the talker relative to the camera’s reference frame.
With the integration of the visual modality, this study expands upon our previous investigation focused solely on audio-based active speaker detection and localization.
Our experiments compare conventional audio-visual approaches for active speaker detection that leverage monaural audio, our previous audio-only method that leverages multichannel recordings from a microphone array, and a novel audio-visual approach integrating vision and multichannel audio.
We found the role of the two modalities to complement each other. Multichannel audio, overcoming the problem of visual occlusions, provides a double-digit reduction in detection error compared to audio-visual methods with single-channel audio. The combination of multichannel audio and vision further enhances spatial accuracy, leading to a four-percentage point increase in F1 score on the Tragic Talkers dataset.
Future investigations will assess the robustness of the model in noisy and highly reverberant environments, as well as tackle the problem of off-screen speakers.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text" style="font-size:90%;">In 3D audio-visual production, meticulous attention is required to accurately align sound sources with the visual events they accompany.
To produce and author an immersive experience, audio sources are generally treated as objects and manually placed in the virtual space by the producer.
This approach to production is often referred to as object-based media (OBM) production </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.4" class="ltx_text" style="font-size:90%;">.
In OBM, each object, spanning audio, video, graphics, text, or other forms of media, is accompanied by its metadata. The metadata associated with an individual object describes specific attributes or desired behaviors of the object, such as its content or position in space over time.
OBM is valued for its adaptability and interactivity, allowing for tailored experiences based on user preferences or device configuration. For example, in the context of producing immersive spatial audio, object-based audio can theoretically be rendered on any loudspeaker configuration, unlike traditional channel-based mixes, which lack such flexibility.</span></p>
</div>
<figure id="S1.F1" class="ltx_figure">
<p id="S1.F1.1" class="ltx_p ltx_align_center"><span id="S1.F1.1.1" class="ltx_text" style="font-size:90%;"><img src="/html/2406.00495/assets/x1.png" id="S1.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="170" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Pipeline for speech signals objectification proposed by Mohd Izhar <em id="S1.F1.14.1" class="ltx_emph ltx_font_italic">et al</em>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and Schweiger <em id="S1.F1.15.2" class="ltx_emph ltx_font_italic">et al</em>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Positional metadata are automatically predicted by leveraging video and microphone array data. These predictions are not only used as final positional information for the spatialization of the objects but also to drive a spatial beamformer. Filtered signals extracted with the beamformer are associated with, and replaced by, the high-quality speech data recorded with Lavalier microphones. This paper focuses on the audio-visual prediction of the speaker’s positional metadata.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="font-size:90%;">However, while it is relatively trivial to manually spatialize synthetic audio-visual assets, this is not true for real scenes. Ideally, the ultimate spatialization should truthfully reflect the positioning and movements of the events. An example of a common audio-visual event is a moving talker.
To tackle this problem, Mohd Izhar </span><em id="S1.p2.1.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al</em><span id="S1.p2.1.3" class="ltx_text" style="font-size:90%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S1.p2.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.6" class="ltx_text" style="font-size:90%;"> and Schweiger </span><em id="S1.p2.1.7" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al</em><span id="S1.p2.1.8" class="ltx_text" style="font-size:90%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.p2.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.11" class="ltx_text" style="font-size:90%;"> proposed the speaker objectification pipeline depicted in Figure </span><a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Audio-visual talker localization in video for spatial sound reproduction" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S1.p2.1.12" class="ltx_text" style="font-size:90%;">.
Automated predictions regarding the locations of active speakers are generated using an audio-visual tracker, which utilizes both video data and audio captured through a microphone array. These predictions drive the spatial listening directivity of a beamformer employed to filter the signals from the microphone array. The resulting filtered speech signals are then associated with, and substituted by, the high-quality speech recordings obtained through Lavalier microphones.</span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text" style="font-size:90%;">This paper addresses the automatic extraction of the speaker positional metadata, corresponding to the green block of the pipeline presented in Figure </span><a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Audio-visual talker localization in video for spatial sound reproduction" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S1.p3.1.2" class="ltx_text" style="font-size:90%;">. Specifically,
we focus on video-based horizontal active speaker detection and localization (ASDL) as videos are a widely consumed form of media, and subjective studies suggest that humans tend to be more spatially sensitive across the azimuth direction than elevation </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S1.p3.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.5" class="ltx_text" style="font-size:90%;">.
The audio-visual localization is performed on the Tragic Talkers dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S1.p3.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.8" class="ltx_text" style="font-size:90%;"> and it expands upon our previous audio-only studies </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S1.p3.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.11" class="ltx_text" style="font-size:90%;">. Our previous works highlighted the benefits of employing multichannel audio captured with a microphone array, outperforming traditional video-based audio-visual approaches for active speaker detection that employ monaural audio and rely on visual face detection.
Multichannel audio provides a higher detection accuracy as it does not suffer from visual occlusions. This paper demonstrates that partnering multichannel audio with the visual modality will improve spatial accuracy while preserving the high detection rate enabled by multichannel audio.</span></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text" style="font-size:90%;">The remainder of this document provides an overview of the related work on video-based active speaker detection and localization, describes the proposed method and the audio-visual network architecture, presents and discusses our experimental results, concludes the paper and suggests future directions for the field.</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="font-size:90%;">ASDL can be addressed through two distinct phases. Initially, the localization subtask is undertaken, wherein a visual face detector is utilized to pre-select a set of candidate speakers. Subsequently, the detected faces undergo classification into active or inactive. In computer vision, this second classification process is usually referred to as Active Speaker Detection (ASD) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.4" class="ltx_text" style="font-size:90%;">.
Researchers usually partner the video stream with the respective (mono) audio signal.
Pioneering the work on video-based active speaker detection, Cutler </span><em id="S2.p1.1.5" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al</em><span id="S2.p1.1.6" class="ltx_text" style="font-size:90%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S2.p1.1.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.9" class="ltx_text" style="font-size:90%;"> proposed to observe the correlation between mouth motion and audio data. Haider </span><em id="S2.p1.1.10" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al</em><span id="S2.p1.1.11" class="ltx_text" style="font-size:90%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.12.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S2.p1.1.13.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.14" class="ltx_text" style="font-size:90%;"> combined lip tracking and voice activity detection (VAD) to predict who is speaking in multiparty dialogue videos.
Chakravarty </span><em id="S2.p1.1.15" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al</em><span id="S2.p1.1.16" class="ltx_text" style="font-size:90%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.17.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S2.p1.1.18.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.19" class="ltx_text" style="font-size:90%;"> proposed to also include head and upper-body motion as additional visual cues to detect the active speaker. They adopted a self-supervised solution to perform ASD by training a visual network under the supervision of its audio counterpart.
Subsequent works </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.20.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S2.p1.1.21.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.22" class="ltx_text" style="font-size:90%;">, adopted a two-step audio-visual co-training for speaker detection and identification. They exploited the co-occurrence of speech and faces in videos to associate clusters generated from speech features with clusters generated from facial features.
With the advance of deep learning techniques and the availability of larger datasets </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.23.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S2.p1.1.24.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.25" class="ltx_text" style="font-size:90%;">, different yet related audio-visual tasks have been introduced, such as audio-visual lip reading </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.26.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S2.p1.1.27.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.28" class="ltx_text" style="font-size:90%;">, lip-voice synchronization </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.29.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S2.p1.1.30.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.31" class="ltx_text" style="font-size:90%;">, and audio-visual speaker separation </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.32.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S2.p1.1.33.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.34" class="ltx_text" style="font-size:90%;">.
What is probably the first, large, annotated dataset for ASD was released for the ActivityNet Challenge (Task B) at CVPR 2019: the AVA-ActiveSpeaker dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.35.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S2.p1.1.36.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.37" class="ltx_text" style="font-size:90%;">.
It provides 38.5 hours of audio-visual face tracks (sequences of consecutive face crops) labeled for speech activity.
Since the face tracks are provided, the challenge task consists of classifying each face as active or inactive, leveraging audio and video signals.
At the 2019 challenge, the first </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.38.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S2.p1.1.39.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.40" class="ltx_text" style="font-size:90%;"> and second </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.41.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S2.p1.1.42.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.43" class="ltx_text" style="font-size:90%;"> positions were achieved by leveraging 3D convolutional neural networks (CNNs).
After that, Alcázar </span><span id="S2.p1.1.44" class="ltx_text ltx_font_italic" style="font-size:90%;">et al.</span><span id="S2.p1.1.45" class="ltx_text" style="font-size:90%;"> proposed a model called Active Speaker in Context (ASC) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.46.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S2.p1.1.47.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.48" class="ltx_text" style="font-size:90%;">. Instead of compute-intensive 3D convolutions or large-scale audio-visual pre-training, ASC leverages context: in assessing the speech activity of a candidate speaker, it looks at any other available faces. Zhang </span><span id="S2.p1.1.49" class="ltx_text ltx_font_italic" style="font-size:90%;">et al.</span><span id="S2.p1.1.50" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.51.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S2.p1.1.52.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.53" class="ltx_text" style="font-size:90%;"> also tackled the ASD task by leveraging contextual information and proposed the UniCon network.
Tao </span><span id="S2.p1.1.54" class="ltx_text ltx_font_italic" style="font-size:90%;">et al.</span><span id="S2.p1.1.55" class="ltx_text" style="font-size:90%;"> introduced TalkNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.56.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S2.p1.1.57.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.58" class="ltx_text" style="font-size:90%;">, an ASD model that leverages short- and long-term features. Additionally, motivated by the call for an ASD system that works properly outside the AVA-ActiveSpeaker dataset domain, they formed a second ASD dataset based on LRS3 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.59.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S2.p1.1.60.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.61" class="ltx_text" style="font-size:90%;"> and VoxCeleb2 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.62.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S2.p1.1.63.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.64" class="ltx_text" style="font-size:90%;"> called TalkSet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.65.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S2.p1.1.66.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.67" class="ltx_text" style="font-size:90%;">.
Recently, Alcázar </span><span id="S2.p1.1.68" class="ltx_text ltx_font_italic" style="font-size:90%;">et al.</span><span id="S2.p1.1.69" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.70.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S2.p1.1.71.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.72" class="ltx_text" style="font-size:90%;"> proposed an end-to-end ASD that unifies audio-visual feature extraction and spatio-temporal context aggregation.</span></p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text" style="font-size:90%;">However, these ASD solutions, focusing solely on the audio-visual classification of the provided face tracks, depend on visual face detection for the localization subtask.
In practice, the speaker can be occluded or facing away from the camera, leading to face detection failures and subsequent degradation of the overall system performance. When this happens, monaural audio is insufficient to compensate for the visual failure, as it lacks the necessary spatial cues required to locate audio sources. In other words, the active speaker is detected only when visible.
In previous studies </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S2.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.4" class="ltx_text" style="font-size:90%;">, we overcame this problem by leveraging multichannel audio to simultaneously address the detection and localization aspects of ASDL. Our model was able to locate the active speaker in the video frames solely from audio inputs, achieving better detection and recall rates.</span></p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text" style="font-size:90%;">Some other recent studies partnered multichannel audio with vision for audio-visual ASDL. For example, Qian </span><span id="S2.p3.1.2" class="ltx_text ltx_font_italic" style="font-size:90%;">et al.</span><span id="S2.p3.1.3" class="ltx_text" style="font-size:90%;"> in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S2.p3.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.6" class="ltx_text" style="font-size:90%;"> and </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S2.p3.1.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.9" class="ltx_text" style="font-size:90%;"> employed visual feature vectors encoding face bounding box coordinates to improve spatial accuracy. The detected face bounding boxes are represented as horizontal and vertical Gaussian-like vectors. Similarly, Wu </span><em id="S2.p3.1.10" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al</em><span id="S2.p3.1.11" class="ltx_text" style="font-size:90%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.12.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S2.p3.1.13.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.14" class="ltx_text" style="font-size:90%;"> adopted Gaussian-like vectors as visual input features and a transformer-based network </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.15.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S2.p3.1.16.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.17" class="ltx_text" style="font-size:90%;"> for their predictions. Zhao </span><em id="S2.p3.1.18" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al</em><span id="S2.p3.1.19" class="ltx_text" style="font-size:90%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.20.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S2.p3.1.21.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.22" class="ltx_text" style="font-size:90%;"> proposed a self-supervised student-teacher knowledge distillation approach to train a multichannel audio network from visual supervision.
In a different study, they explored the audio-visual speaker localization from egocentric views </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.23.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S2.p3.1.24.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.25" class="ltx_text" style="font-size:90%;">. That is, the prediction is performed from the point of view of the device “wearer”, who is free to move in space and the sensors are therefore not stationary.
Research groups from Meta Reality Labs released the EasyCom dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.26.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S2.p3.1.27.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.28" class="ltx_text" style="font-size:90%;">. It consists of a set of videos captured from custom AR glasses with a camera and a microphone array integrated.
Jiang </span><span id="S2.p3.1.29" class="ltx_text ltx_font_italic" style="font-size:90%;">et al.</span><span id="S2.p3.1.30" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.31.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S2.p3.1.32.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.33" class="ltx_text" style="font-size:90%;"> employed EasyCom to perform audio-visual speaker localization in an egocentric setting.
Concurrently, Gurvich </span><em id="S2.p3.1.34" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al</em><span id="S2.p3.1.35" class="ltx_text" style="font-size:90%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.36.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S2.p3.1.37.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.38" class="ltx_text" style="font-size:90%;"> performed the real-time ASDL with a microphone array in 360° videos.
Similarly, in this paper, we integrate multichannel audio and visual data. However, instead of leveraging visual features such as Gaussian-like vectors, we focus on the integration of audio and visual embeddings extracted with pre-trained audio and visual encoders </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.39.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S2.p3.1.40.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.41" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text" style="font-size:90%;">The proposed method extracts audio and visual feature embeddings with an audio and a visual encoder, respectively. The embeddings are then concatenated and fed to an attention-based unit. The output of the attention-based unit represents a joint latent audio-visual representation of the input signals, which is used to generate the final prediction through a feed-forward network. A schematic representation of the proposed model is presented in Figure </span><a href="#S3.F3" title="Figure 3 ‣ 3.2 Network Architecture ‣ 3 Method ‣ Audio-visual talker localization in video for spatial sound reproduction" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.p1.1.2" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset</h3>

<figure id="S3.F2" class="ltx_figure">
<p id="S3.F2.1" class="ltx_p ltx_align_center"><span id="S3.F2.1.1" class="ltx_text" style="font-size:90%;"><img src="/html/2406.00495/assets/x2.png" id="S3.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="176" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>(a) Schematic of camera (blue circles) and microphone (red dots) positions on the AVA Rig. The green square highlights the reference microphone. (b) Photo of an AVA Rig.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">To train and evaluate the proposed method, we employed the Tragic Talkers dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S3.SS1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p1.1.4" class="ltx_text" style="font-size:90%;">.
It offers sequences captured by two Audio-Visual Array (AVA) Rigs. Each AVA Rig is a light-field and sound-field sensing platform consisting of a 16-element microphone array and 11 cameras fixed on a flat perspex baffle as depicted in Figure </span><a href="#S3.F2" title="Figure 2 ‣ 3.1 Dataset ‣ 3 Method ‣ Audio-visual talker localization in video for spatial sound reproduction" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS1.p1.1.5" class="ltx_text" style="font-size:90%;">. Therefore, each sensor is located in a fixed relative position and orientation with respect to the other sensors.
The microphone array has a horizontal aperture of 450 mm and a vertical aperture of only 40 mm. This results in higher resolution when localizing audio sources along the azimuth direction than elevation, which is consistent with human perception </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p1.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S3.SS1.p1.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p1.1.8" class="ltx_text" style="font-size:90%;">.
Horizontally, the microphones are log-spaced for broad frequency coverage from 500 Hz to 8 kHz, to better support the horizontal speech band resolution.
Tragic Talkers was captured in an acoustically treated studio with an average reverberation time of 0.3s in the mid 0.5-2 kHz frequency range and minimal background noise floor (SNR </span><math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mo mathsize="90%" id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><geq id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\geq</annotation></semantics></math><span id="S3.SS1.p1.1.9" class="ltx_text" style="font-size:90%;"> 30 dB).
The dataset does not contain sequences in which the speakers talk simultaneously, off-screen talkers, or external sources of sound other than speech, making it ideal for audio-visual speaker diarization applications too.
Tragic Talkers was specifically designed for OBM production research.
Its content allows the implementation of the speaker objectification pipeline presented in Figure </span><a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Audio-visual talker localization in video for spatial sound reproduction" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S3.SS1.p1.1.10" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p1.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S3.SS1.p1.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p1.1.13" class="ltx_text" style="font-size:90%;">. In fact, it provides high-quality speech signals recorded with Lavalier microphones. Additionally, the scenes are captured against a blue background that facilitates the actors’ silhouette extraction, empowering the producer to extract and position the actors as desired while retaining the natural motion of the performance.</span></p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text" style="font-size:90%;">The method for ASDL proposed in this paper necessitates a single video feed with the microphone array.
Leveraging the multiple views available enables us to extend the network training by choosing the relevant camera perspective.
Consequently, a one-hot vector denoting the selected view is appended to the input data, augmenting the dataset with diverse camera perspectives and enabling the network to learn the correct mapping to the desired viewpoint.
The sequences include one or two actors positioned at a distance of about 3–4 m, engaging in monologues, conversations, and interactive scenes involving movement and occlusion.
Tragic Talkers comprises 30 scenes captured with two AVA Rigs.
Each rig’s audio-visual stream is employed independently, i.e., 16-channel audio is used to predict the speaker’s position within any of the 11 camera perspectives of the rig.
So the dataset’s 30 scenes provide 60 rig sequences, each offering 11 viewpoints.
The dataset is partitioned into a 50-sequence development set and a 10-sequence test set.
TragicTalkers provides ground truth (GT) labels for voice activity and 2D face bounding box. The test set used for evaluation also includes 3D mouth coordinates.</span></p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Network Architecture</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">We tested the proposed architecture in a recent study where we tackled the sound event localization and detection (SELD) task </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S3.SS2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.p1.1.4" class="ltx_text" style="font-size:90%;">. Here, we aim to explore whether a similar network can be extended to the more specific ASDL tasks too.
As depicted in Figure </span><a href="#S3.F3" title="Figure 3 ‣ 3.2 Network Architecture ‣ 3 Method ‣ Audio-visual talker localization in video for spatial sound reproduction" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS2.p1.1.5" class="ltx_text" style="font-size:90%;">, the network presents an audio and a visual encoder to extract audio and visual feature embeddings. The embeddings are then concatenated and fed to an audio-visual Conformer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p1.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a><span id="S3.SS2.p1.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.p1.1.8" class="ltx_text" style="font-size:90%;">. Conformer models were originally proposed for speech recognition but lately, they have achieved state-of-the-art performance in tasks such as SELD too </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p1.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S3.SS2.p1.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.p1.1.11" class="ltx_text" style="font-size:90%;">. They present an architecture similar to the Transformer proposed by Vaswani </span><em id="S3.SS2.p1.1.12" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al</em><span id="S3.SS2.p1.1.13" class="ltx_text" style="font-size:90%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p1.1.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S3.SS2.p1.1.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.p1.1.16" class="ltx_text" style="font-size:90%;">, however, they integrate a convolution module that performs pointwise and depthwise convolutions.
Inspired by Wang </span><em id="S3.SS2.p1.1.17" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al</em><span id="S3.SS2.p1.1.18" class="ltx_text" style="font-size:90%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p1.1.19.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S3.SS2.p1.1.20.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.p1.1.21" class="ltx_text" style="font-size:90%;">, 4 layers are employed with 8 heads each. The size of the kernel for the depthwise convolutions is set to 51, and the dimension of the hidden layer in the feed-forward networks of each Conformer layer is 1024 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p1.1.22.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S3.SS2.p1.1.23.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.p1.1.24" class="ltx_text" style="font-size:90%;">.
The output of the Conformer, i.e., the latent audio-visual representation, is then fed to a feed-forward unit consisting of three fully-connected layers to make the final predictions. A one-hot vector encoding the desired camera view of the AVA Rig is concatenated to the latent audio-visual vector after the second fully-connected layer. It allows mapping the final prediction to the desired camera of the rig.
In output, the network predicts the speaker’s horizontal location and voice activity confidence at a temporal resolution that matches the label and video frame rate (i.e., a position-confidence pair is generated for each video frame).</span></p>
</div>
<figure id="S3.F3" class="ltx_figure">
<p id="S3.F3.1" class="ltx_p ltx_align_center"><span id="S3.F3.1.1" class="ltx_text" style="font-size:90%;"><img src="/html/2406.00495/assets/x3.png" id="S3.F3.1.1.g1" class="ltx_graphics ltx_img_portrait" width="323" height="496" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Proposed network architecture for audio-visual ASDL. An audio encoder based on a CNN extracts an audio embedding from the audio input features. Similarly, an encoder consisting of ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> followed by a Conformer unit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> extracts a visual embedding from the video frames. <math id="S3.F3.3.m1.1" class="ltx_Math" alttext="\otimes" display="inline"><semantics id="S3.F3.3.m1.1b"><mo id="S3.F3.3.m1.1.1" xref="S3.F3.3.m1.1.1.cmml">⊗</mo><annotation-xml encoding="MathML-Content" id="S3.F3.3.m1.1c"><csymbol cd="latexml" id="S3.F3.3.m1.1.1.cmml" xref="S3.F3.3.m1.1.1">tensor-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.m1.1d">\otimes</annotation></semantics></math> denotes the concatenation operation. After concatenation, the audio-visual features and processed by a second Conformer unit. A feed-forward network generates the final prediction. A camera ID one-hot vector is used to regress the speaker’s position to the desired camera view.</figcaption>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Audio Encoder</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.9" class="ltx_p"><span id="S3.SS2.SSS1.p1.9.1" class="ltx_text" style="font-size:90%;">The audio encoder takes as input spatial features extracted from the multichannel audio signals (a detailed description of the audio input features will be provided in </span><a href="#S3.SS3" title="3.3 Audio Input Feature ‣ 3 Method ‣ Audio-visual talker localization in video for spatial sound reproduction" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3.3</span></a><span id="S3.SS2.SSS1.p1.9.2" class="ltx_text" style="font-size:90%;">) with shape </span><math id="S3.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="Ch_{in}\times T_{in}\times F_{in}" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mrow id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><mrow id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.1.m1.1.1.2.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.1.m1.1.1.2.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.1.cmml">​</mo><msub id="S3.SS2.SSS1.p1.1.m1.1.1.2.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.2.cmml">h</mi><mrow id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.3.cmml">n</mi></mrow></msub></mrow><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS2.SSS1.p1.1.m1.1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml">×</mo><msub id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml">T</mi><mrow id="S3.SS2.SSS1.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.cmml">n</mi></mrow></msub><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS2.SSS1.p1.1.m1.1.1.1a" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml">×</mo><msub id="S3.SS2.SSS1.p1.1.m1.1.1.4" xref="S3.SS2.SSS1.p1.1.m1.1.1.4.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.1.m1.1.1.4.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.4.2.cmml">F</mi><mrow id="S3.SS2.SSS1.p1.1.m1.1.1.4.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.4.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.1.m1.1.1.4.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.4.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.1.m1.1.1.4.3.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.4.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS2.SSS1.p1.1.m1.1.1.4.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.4.3.3.cmml">n</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><times id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1"></times><apply id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2"><times id="S3.SS2.SSS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.1"></times><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.2">𝐶</ci><apply id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.2">ℎ</ci><apply id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3"><times id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.1"></times><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.2">𝑖</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.3.3">𝑛</ci></apply></apply></apply><apply id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2">𝑇</ci><apply id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3"><times id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.2">𝑖</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.3.3">𝑛</ci></apply></apply><apply id="S3.SS2.SSS1.p1.1.m1.1.1.4.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.4.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.4">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.4.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.4.2">𝐹</ci><apply id="S3.SS2.SSS1.p1.1.m1.1.1.4.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.4.3"><times id="S3.SS2.SSS1.p1.1.m1.1.1.4.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.4.3.1"></times><ci id="S3.SS2.SSS1.p1.1.m1.1.1.4.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.4.3.2">𝑖</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.4.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.4.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">Ch_{in}\times T_{in}\times F_{in}</annotation></semantics></math><span id="S3.SS2.SSS1.p1.9.3" class="ltx_text" style="font-size:90%;">, where </span><math id="S3.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="Ch_{in}" display="inline"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mrow id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.2.m2.1.1.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.2.m2.1.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.1.cmml">​</mo><msub id="S3.SS2.SSS1.p1.2.m2.1.1.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.2.cmml">h</mi><mrow id="S3.SS2.SSS1.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.2.m2.1.1.3.3.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.2.m2.1.1.3.3.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS2.SSS1.p1.2.m2.1.1.3.3.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3.3.cmml">n</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><apply id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1"><times id="S3.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.1"></times><ci id="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2">𝐶</ci><apply id="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.2">ℎ</ci><apply id="S3.SS2.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3"><times id="S3.SS2.SSS1.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3.1"></times><ci id="S3.SS2.SSS1.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3.2">𝑖</ci><ci id="S3.SS2.SSS1.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">Ch_{in}</annotation></semantics></math><span id="S3.SS2.SSS1.p1.9.4" class="ltx_text" style="font-size:90%;"> corresponds to the number of channels of the input spatial features, </span><math id="S3.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="T_{in}" display="inline"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><msub id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.3.m3.1.1.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml">T</mi><mrow id="S3.SS2.SSS1.p1.3.m3.1.1.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.3.m3.1.1.3.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS2.SSS1.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><apply id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2">𝑇</ci><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3"><times id="S3.SS2.SSS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.1"></times><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2">𝑖</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">T_{in}</annotation></semantics></math><span id="S3.SS2.SSS1.p1.9.5" class="ltx_text" style="font-size:90%;"> the number of temporal bins, and </span><math id="S3.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="F_{in}" display="inline"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><msub id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.4.m4.1.1.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml">F</mi><mrow id="S3.SS2.SSS1.p1.4.m4.1.1.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.4.m4.1.1.3.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS2.SSS1.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.3.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><apply id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2">𝐹</ci><apply id="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3"><times id="S3.SS2.SSS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.1"></times><ci id="S3.SS2.SSS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.2">𝑖</ci><ci id="S3.SS2.SSS1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">F_{in}</annotation></semantics></math><span id="S3.SS2.SSS1.p1.9.6" class="ltx_text" style="font-size:90%;"> the frequency bins.
The audio encoder presents a CNN architecture. The CNN architecture consists of four convolutional blocks, each consisting of two 3</span><math id="S3.SS2.SSS1.p1.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.SSS1.p1.5.m5.1a"><mo mathsize="90%" id="S3.SS2.SSS1.p1.5.m5.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m5.1b"><times id="S3.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m5.1c">\times</annotation></semantics></math><span id="S3.SS2.SSS1.p1.9.7" class="ltx_text" style="font-size:90%;">3 convolutional layers followed by average pooling, batch normalization </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.SSS1.p1.9.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="S3.SS2.SSS1.p1.9.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.SSS1.p1.9.10" class="ltx_text" style="font-size:90%;">, and ReLU activation. The average pooling layer is applied with a stride of 2, halving the temporal and frequency dimension at each block. The resulting tensor of shape </span><math id="S3.SS2.SSS1.p1.6.m6.1" class="ltx_Math" alttext="512\times T_{in}/16\times F_{in}/16" display="inline"><semantics id="S3.SS2.SSS1.p1.6.m6.1a"><mrow id="S3.SS2.SSS1.p1.6.m6.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.cmml"><mrow id="S3.SS2.SSS1.p1.6.m6.1.1.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.cmml"><mrow id="S3.SS2.SSS1.p1.6.m6.1.1.2.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.cmml"><mrow id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.cmml"><mn mathsize="90%" id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.2.cmml">512</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.1.cmml">×</mo><msub id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.2.cmml">T</mi><mrow id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.3.cmml">n</mi></mrow></msub></mrow><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.1.cmml">/</mo><mn mathsize="90%" id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.3.cmml">16</mn></mrow><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS2.SSS1.p1.6.m6.1.1.2.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.1.cmml">×</mo><msub id="S3.SS2.SSS1.p1.6.m6.1.1.2.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.6.m6.1.1.2.3.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3.2.cmml">F</mi><mrow id="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.3.cmml">n</mi></mrow></msub></mrow><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="S3.SS2.SSS1.p1.6.m6.1.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.cmml">/</mo><mn mathsize="90%" id="S3.SS2.SSS1.p1.6.m6.1.1.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.6.m6.1b"><apply id="S3.SS2.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1"><divide id="S3.SS2.SSS1.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1"></divide><apply id="S3.SS2.SSS1.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2"><times id="S3.SS2.SSS1.p1.6.m6.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.1"></times><apply id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2"><divide id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.1"></divide><apply id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2"><times id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.1"></times><cn type="integer" id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.2">512</cn><apply id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3">subscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.2">𝑇</ci><apply id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3"><times id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.1"></times><ci id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.2">𝑖</ci><ci id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.2.3.3.3">𝑛</ci></apply></apply></apply><cn type="integer" id="S3.SS2.SSS1.p1.6.m6.1.1.2.2.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.2.3">16</cn></apply><apply id="S3.SS2.SSS1.p1.6.m6.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.1.1.2.3.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3">subscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.1.1.2.3.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3.2">𝐹</ci><apply id="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3"><times id="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.1"></times><ci id="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.2">𝑖</ci><ci id="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.3.3.3">𝑛</ci></apply></apply></apply><cn type="integer" id="S3.SS2.SSS1.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.6.m6.1c">512\times T_{in}/16\times F_{in}/16</annotation></semantics></math><span id="S3.SS2.SSS1.p1.9.11" class="ltx_text" style="font-size:90%;"> is then reshaped and frequency average pooling is applied to achieve a </span><math id="S3.SS2.SSS1.p1.7.m7.1" class="ltx_Math" alttext="T_{in}/16\times 512" display="inline"><semantics id="S3.SS2.SSS1.p1.7.m7.1a"><mrow id="S3.SS2.SSS1.p1.7.m7.1.1" xref="S3.SS2.SSS1.p1.7.m7.1.1.cmml"><mrow id="S3.SS2.SSS1.p1.7.m7.1.1.2" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.cmml"><msub id="S3.SS2.SSS1.p1.7.m7.1.1.2.2" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.2.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.7.m7.1.1.2.2.2" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.2.2.cmml">T</mi><mrow id="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.2" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.1" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.3" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.3.cmml">n</mi></mrow></msub><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="S3.SS2.SSS1.p1.7.m7.1.1.2.1" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.1.cmml">/</mo><mn mathsize="90%" id="S3.SS2.SSS1.p1.7.m7.1.1.2.3" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.3.cmml">16</mn></mrow><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S3.SS2.SSS1.p1.7.m7.1.1.1" xref="S3.SS2.SSS1.p1.7.m7.1.1.1.cmml">×</mo><mn mathsize="90%" id="S3.SS2.SSS1.p1.7.m7.1.1.3" xref="S3.SS2.SSS1.p1.7.m7.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.7.m7.1b"><apply id="S3.SS2.SSS1.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1"><times id="S3.SS2.SSS1.p1.7.m7.1.1.1.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.1"></times><apply id="S3.SS2.SSS1.p1.7.m7.1.1.2.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.2"><divide id="S3.SS2.SSS1.p1.7.m7.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.1"></divide><apply id="S3.SS2.SSS1.p1.7.m7.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.7.m7.1.1.2.2.1.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.7.m7.1.1.2.2.2.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.2.2">𝑇</ci><apply id="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3"><times id="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.1.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.1"></times><ci id="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.2.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.2">𝑖</ci><ci id="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.3.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.2.3.3">𝑛</ci></apply></apply><cn type="integer" id="S3.SS2.SSS1.p1.7.m7.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.2.3">16</cn></apply><cn type="integer" id="S3.SS2.SSS1.p1.7.m7.1.1.3.cmml" xref="S3.SS2.SSS1.p1.7.m7.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.7.m7.1c">T_{in}/16\times 512</annotation></semantics></math><span id="S3.SS2.SSS1.p1.9.12" class="ltx_text" style="font-size:90%;"> dimensional feature embedding. </span><math id="S3.SS2.SSS1.p1.8.m8.1" class="ltx_Math" alttext="T_{in}" display="inline"><semantics id="S3.SS2.SSS1.p1.8.m8.1a"><msub id="S3.SS2.SSS1.p1.8.m8.1.1" xref="S3.SS2.SSS1.p1.8.m8.1.1.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.8.m8.1.1.2" xref="S3.SS2.SSS1.p1.8.m8.1.1.2.cmml">T</mi><mrow id="S3.SS2.SSS1.p1.8.m8.1.1.3" xref="S3.SS2.SSS1.p1.8.m8.1.1.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.8.m8.1.1.3.2" xref="S3.SS2.SSS1.p1.8.m8.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.8.m8.1.1.3.1" xref="S3.SS2.SSS1.p1.8.m8.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS2.SSS1.p1.8.m8.1.1.3.3" xref="S3.SS2.SSS1.p1.8.m8.1.1.3.3.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.8.m8.1b"><apply id="S3.SS2.SSS1.p1.8.m8.1.1.cmml" xref="S3.SS2.SSS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.8.m8.1.1.1.cmml" xref="S3.SS2.SSS1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.8.m8.1.1.2.cmml" xref="S3.SS2.SSS1.p1.8.m8.1.1.2">𝑇</ci><apply id="S3.SS2.SSS1.p1.8.m8.1.1.3.cmml" xref="S3.SS2.SSS1.p1.8.m8.1.1.3"><times id="S3.SS2.SSS1.p1.8.m8.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.8.m8.1.1.3.1"></times><ci id="S3.SS2.SSS1.p1.8.m8.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.8.m8.1.1.3.2">𝑖</ci><ci id="S3.SS2.SSS1.p1.8.m8.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.8.m8.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.8.m8.1c">T_{in}</annotation></semantics></math><span id="S3.SS2.SSS1.p1.9.13" class="ltx_text" style="font-size:90%;"> is chosen so that </span><math id="S3.SS2.SSS1.p1.9.m9.1" class="ltx_Math" alttext="T_{in}/16" display="inline"><semantics id="S3.SS2.SSS1.p1.9.m9.1a"><mrow id="S3.SS2.SSS1.p1.9.m9.1.1" xref="S3.SS2.SSS1.p1.9.m9.1.1.cmml"><msub id="S3.SS2.SSS1.p1.9.m9.1.1.2" xref="S3.SS2.SSS1.p1.9.m9.1.1.2.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.9.m9.1.1.2.2" xref="S3.SS2.SSS1.p1.9.m9.1.1.2.2.cmml">T</mi><mrow id="S3.SS2.SSS1.p1.9.m9.1.1.2.3" xref="S3.SS2.SSS1.p1.9.m9.1.1.2.3.cmml"><mi mathsize="90%" id="S3.SS2.SSS1.p1.9.m9.1.1.2.3.2" xref="S3.SS2.SSS1.p1.9.m9.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.p1.9.m9.1.1.2.3.1" xref="S3.SS2.SSS1.p1.9.m9.1.1.2.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS2.SSS1.p1.9.m9.1.1.2.3.3" xref="S3.SS2.SSS1.p1.9.m9.1.1.2.3.3.cmml">n</mi></mrow></msub><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="S3.SS2.SSS1.p1.9.m9.1.1.1" xref="S3.SS2.SSS1.p1.9.m9.1.1.1.cmml">/</mo><mn mathsize="90%" id="S3.SS2.SSS1.p1.9.m9.1.1.3" xref="S3.SS2.SSS1.p1.9.m9.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.9.m9.1b"><apply id="S3.SS2.SSS1.p1.9.m9.1.1.cmml" xref="S3.SS2.SSS1.p1.9.m9.1.1"><divide id="S3.SS2.SSS1.p1.9.m9.1.1.1.cmml" xref="S3.SS2.SSS1.p1.9.m9.1.1.1"></divide><apply id="S3.SS2.SSS1.p1.9.m9.1.1.2.cmml" xref="S3.SS2.SSS1.p1.9.m9.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.9.m9.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.9.m9.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.9.m9.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.9.m9.1.1.2.2">𝑇</ci><apply id="S3.SS2.SSS1.p1.9.m9.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.9.m9.1.1.2.3"><times id="S3.SS2.SSS1.p1.9.m9.1.1.2.3.1.cmml" xref="S3.SS2.SSS1.p1.9.m9.1.1.2.3.1"></times><ci id="S3.SS2.SSS1.p1.9.m9.1.1.2.3.2.cmml" xref="S3.SS2.SSS1.p1.9.m9.1.1.2.3.2">𝑖</ci><ci id="S3.SS2.SSS1.p1.9.m9.1.1.2.3.3.cmml" xref="S3.SS2.SSS1.p1.9.m9.1.1.2.3.3">𝑛</ci></apply></apply><cn type="integer" id="S3.SS2.SSS1.p1.9.m9.1.1.3.cmml" xref="S3.SS2.SSS1.p1.9.m9.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.9.m9.1c">T_{in}/16</annotation></semantics></math><span id="S3.SS2.SSS1.p1.9.14" class="ltx_text" style="font-size:90%;"> matches the label frame rate of the Tragic Talkers dataset (30 labels per second).</span></p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Visual Encoder</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p"><span id="S3.SS2.SSS2.p1.1.1" class="ltx_text" style="font-size:90%;">As a visual encoder, the ResNet50 model </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.SSS2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="S3.SS2.SSS2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.SSS2.p1.1.4" class="ltx_text" style="font-size:90%;"> followed by a Conformer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.SSS2.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a><span id="S3.SS2.SSS2.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.SSS2.p1.1.7" class="ltx_text" style="font-size:90%;"> is tested.
Each video frame is fed to ResNet50. Since the video streams of Tragic Talkers are captured at 30 fps, ResNet50 extracts a number of frame embeddings that match the label frame rate as well as the audio embedding temporal resolution. The frame embeddings extracted by ResNet50 are further processed by the Conformer module, which presents the same hyper-parameters utilized for the one employed for the audio-visual fusion.
The video frames used as inputs to the visual encoders are resized to 224x224p. We employed the ResNet50 model available with the torchvision library</span><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://pytorch.org/vision/stable/index.html" title="" class="ltx_ref ltx_href">https://pytorch.org/vision/stable/index.html</a></span></span></span><span id="S3.SS2.SSS2.p1.1.8" class="ltx_text" style="font-size:90%;">, which is pre-trained on image classification on the ImageNet dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.SSS2.p1.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="S3.SS2.SSS2.p1.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.SSS2.p1.1.11" class="ltx_text" style="font-size:90%;">.
Before being fed to the Conformer, the frame embeddings are resized from the original 2048-dimensional vectors generated from ResNet50 to 512 dimensions employing a fully-connected layer to match the size of the audio embeddings.</span></p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p"><span id="S3.SS2.SSS2.p2.1.1" class="ltx_text" style="font-size:90%;">The remainder of this paper will often refer to the visual encoder as ResNet-Conformer and to the attention-based audio-visual fusion as AV-Conformer.</span></p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Audio Input Feature</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text" style="font-size:90%;">This work adopts log-mel spectrograms concatenated with generalized cross-correlation with phase transform (GCC-PHAT) features in log-mel space </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S3.SS3.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS3.p1.1.4" class="ltx_text" style="font-size:90%;"> extracted from the microphone array signals. These audio features were chosen because they achieved good performance and robustness on the large microphone array of the Tragic Talker dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S3.SS3.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS3.p1.1.7" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.8" class="ltx_p"><span id="S3.SS3.p2.8.1" class="ltx_text" style="font-size:90%;">The GCC-PHAT is employed to estimate the time difference of arrival (TDOA) of a sound source at two microphones </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p2.8.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S3.SS3.p2.8.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS3.p2.8.4" class="ltx_text" style="font-size:90%;">. The idea is to find the lag time that maximizes the cross-correlation function between the signals sensed by the two microphones. The generalized cross-correlation (GCC) is computed through the inverse Fast-Fourier Transform (inverse-FFT) of their cross-power spectrum. Phase-transformed GCC, namely the GCC-PHAT, eliminates the influence of the amplitude by leaving only the phase </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p2.8.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S3.SS3.p2.8.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS3.p2.8.7" class="ltx_text" style="font-size:90%;">.
The GCC-PHAT between the </span><span id="S3.SS3.p2.8.8" class="ltx_text ltx_font_italic" style="font-size:90%;">i</span><span id="S3.SS3.p2.8.9" class="ltx_text" style="font-size:90%;">-th and the </span><span id="S3.SS3.p2.8.10" class="ltx_text ltx_font_italic" style="font-size:90%;">j</span><span id="S3.SS3.p2.8.11" class="ltx_text" style="font-size:90%;">-th microphone is defined at each audio frame </span><span id="S3.SS3.p2.8.12" class="ltx_text ltx_font_italic" style="font-size:90%;">t</span><span id="S3.SS3.p2.8.13" class="ltx_text" style="font-size:90%;"> as:</span></p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.12" class="ltx_Math" alttext="GCC_{ij}(t,\tau)=\mathcal{F}^{-1}_{f\rightarrow\tau}\frac{\mathbf{X}_{i}(t,f)\mathbf{X}^{*}_{j}(t,f)}{|\mathbf{X}_{i}(t,f)\mathbf{X}^{*}_{j}(t,f)|}," display="block"><semantics id="S3.E1.m1.12a"><mrow id="S3.E1.m1.12.12.1" xref="S3.E1.m1.12.12.1.1.cmml"><mrow id="S3.E1.m1.12.12.1.1" xref="S3.E1.m1.12.12.1.1.cmml"><mrow id="S3.E1.m1.12.12.1.1.2" xref="S3.E1.m1.12.12.1.1.2.cmml"><mi mathsize="90%" id="S3.E1.m1.12.12.1.1.2.2" xref="S3.E1.m1.12.12.1.1.2.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.12.12.1.1.2.1" xref="S3.E1.m1.12.12.1.1.2.1.cmml">​</mo><mi mathsize="90%" id="S3.E1.m1.12.12.1.1.2.3" xref="S3.E1.m1.12.12.1.1.2.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.12.12.1.1.2.1a" xref="S3.E1.m1.12.12.1.1.2.1.cmml">​</mo><msub id="S3.E1.m1.12.12.1.1.2.4" xref="S3.E1.m1.12.12.1.1.2.4.cmml"><mi mathsize="90%" id="S3.E1.m1.12.12.1.1.2.4.2" xref="S3.E1.m1.12.12.1.1.2.4.2.cmml">C</mi><mrow id="S3.E1.m1.12.12.1.1.2.4.3" xref="S3.E1.m1.12.12.1.1.2.4.3.cmml"><mi mathsize="90%" id="S3.E1.m1.12.12.1.1.2.4.3.2" xref="S3.E1.m1.12.12.1.1.2.4.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.12.12.1.1.2.4.3.1" xref="S3.E1.m1.12.12.1.1.2.4.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E1.m1.12.12.1.1.2.4.3.3" xref="S3.E1.m1.12.12.1.1.2.4.3.3.cmml">j</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.12.12.1.1.2.1b" xref="S3.E1.m1.12.12.1.1.2.1.cmml">​</mo><mrow id="S3.E1.m1.12.12.1.1.2.5.2" xref="S3.E1.m1.12.12.1.1.2.5.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.12.12.1.1.2.5.2.1" xref="S3.E1.m1.12.12.1.1.2.5.1.cmml">(</mo><mi mathsize="90%" id="S3.E1.m1.10.10" xref="S3.E1.m1.10.10.cmml">t</mi><mo mathsize="90%" id="S3.E1.m1.12.12.1.1.2.5.2.2" xref="S3.E1.m1.12.12.1.1.2.5.1.cmml">,</mo><mi mathsize="90%" id="S3.E1.m1.11.11" xref="S3.E1.m1.11.11.cmml">τ</mi><mo maxsize="90%" minsize="90%" id="S3.E1.m1.12.12.1.1.2.5.2.3" xref="S3.E1.m1.12.12.1.1.2.5.1.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E1.m1.12.12.1.1.1" xref="S3.E1.m1.12.12.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.12.12.1.1.3" xref="S3.E1.m1.12.12.1.1.3.cmml"><msubsup id="S3.E1.m1.12.12.1.1.3.2" xref="S3.E1.m1.12.12.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.E1.m1.12.12.1.1.3.2.2.2" xref="S3.E1.m1.12.12.1.1.3.2.2.2.cmml">ℱ</mi><mrow id="S3.E1.m1.12.12.1.1.3.2.3" xref="S3.E1.m1.12.12.1.1.3.2.3.cmml"><mi mathsize="90%" id="S3.E1.m1.12.12.1.1.3.2.3.2" xref="S3.E1.m1.12.12.1.1.3.2.3.2.cmml">f</mi><mo mathsize="90%" stretchy="false" id="S3.E1.m1.12.12.1.1.3.2.3.1" xref="S3.E1.m1.12.12.1.1.3.2.3.1.cmml">→</mo><mi mathsize="90%" id="S3.E1.m1.12.12.1.1.3.2.3.3" xref="S3.E1.m1.12.12.1.1.3.2.3.3.cmml">τ</mi></mrow><mrow id="S3.E1.m1.12.12.1.1.3.2.2.3" xref="S3.E1.m1.12.12.1.1.3.2.2.3.cmml"><mo mathsize="90%" id="S3.E1.m1.12.12.1.1.3.2.2.3a" xref="S3.E1.m1.12.12.1.1.3.2.2.3.cmml">−</mo><mn mathsize="90%" id="S3.E1.m1.12.12.1.1.3.2.2.3.2" xref="S3.E1.m1.12.12.1.1.3.2.2.3.2.cmml">1</mn></mrow></msubsup><mo lspace="0em" rspace="0em" id="S3.E1.m1.12.12.1.1.3.1" xref="S3.E1.m1.12.12.1.1.3.1.cmml">​</mo><mfrac id="S3.E1.m1.9.9" xref="S3.E1.m1.9.9.cmml"><mrow id="S3.E1.m1.4.4.4" xref="S3.E1.m1.4.4.4.cmml"><msub id="S3.E1.m1.4.4.4.6" xref="S3.E1.m1.4.4.4.6.cmml"><mi mathsize="90%" id="S3.E1.m1.4.4.4.6.2" xref="S3.E1.m1.4.4.4.6.2.cmml">𝐗</mi><mi mathsize="90%" id="S3.E1.m1.4.4.4.6.3" xref="S3.E1.m1.4.4.4.6.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.5" xref="S3.E1.m1.4.4.4.5.cmml">​</mo><mrow id="S3.E1.m1.4.4.4.7.2" xref="S3.E1.m1.4.4.4.7.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.4.4.4.7.2.1" xref="S3.E1.m1.4.4.4.7.1.cmml">(</mo><mi mathsize="90%" id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">t</mi><mo mathsize="90%" id="S3.E1.m1.4.4.4.7.2.2" xref="S3.E1.m1.4.4.4.7.1.cmml">,</mo><mi mathsize="90%" id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">f</mi><mo maxsize="90%" minsize="90%" id="S3.E1.m1.4.4.4.7.2.3" xref="S3.E1.m1.4.4.4.7.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.5a" xref="S3.E1.m1.4.4.4.5.cmml">​</mo><msubsup id="S3.E1.m1.4.4.4.8" xref="S3.E1.m1.4.4.4.8.cmml"><mi mathsize="90%" id="S3.E1.m1.4.4.4.8.2.2" xref="S3.E1.m1.4.4.4.8.2.2.cmml">𝐗</mi><mi mathsize="90%" id="S3.E1.m1.4.4.4.8.3" xref="S3.E1.m1.4.4.4.8.3.cmml">j</mi><mo mathsize="90%" id="S3.E1.m1.4.4.4.8.2.3" xref="S3.E1.m1.4.4.4.8.2.3.cmml">∗</mo></msubsup><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.5b" xref="S3.E1.m1.4.4.4.5.cmml">​</mo><mrow id="S3.E1.m1.4.4.4.9.2" xref="S3.E1.m1.4.4.4.9.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.4.4.4.9.2.1" xref="S3.E1.m1.4.4.4.9.1.cmml">(</mo><mi mathsize="90%" id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml">t</mi><mo mathsize="90%" id="S3.E1.m1.4.4.4.9.2.2" xref="S3.E1.m1.4.4.4.9.1.cmml">,</mo><mi mathsize="90%" id="S3.E1.m1.4.4.4.4" xref="S3.E1.m1.4.4.4.4.cmml">f</mi><mo maxsize="90%" minsize="90%" id="S3.E1.m1.4.4.4.9.2.3" xref="S3.E1.m1.4.4.4.9.1.cmml">)</mo></mrow></mrow><mrow id="S3.E1.m1.9.9.9.5" xref="S3.E1.m1.9.9.9.6.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.9.9.9.5.2" xref="S3.E1.m1.9.9.9.6.1.cmml">|</mo><mrow id="S3.E1.m1.9.9.9.5.1" xref="S3.E1.m1.9.9.9.5.1.cmml"><msub id="S3.E1.m1.9.9.9.5.1.2" xref="S3.E1.m1.9.9.9.5.1.2.cmml"><mi mathsize="90%" id="S3.E1.m1.9.9.9.5.1.2.2" xref="S3.E1.m1.9.9.9.5.1.2.2.cmml">𝐗</mi><mi mathsize="90%" id="S3.E1.m1.9.9.9.5.1.2.3" xref="S3.E1.m1.9.9.9.5.1.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.9.9.9.5.1.1" xref="S3.E1.m1.9.9.9.5.1.1.cmml">​</mo><mrow id="S3.E1.m1.9.9.9.5.1.3.2" xref="S3.E1.m1.9.9.9.5.1.3.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.9.9.9.5.1.3.2.1" xref="S3.E1.m1.9.9.9.5.1.3.1.cmml">(</mo><mi mathsize="90%" id="S3.E1.m1.5.5.5.1" xref="S3.E1.m1.5.5.5.1.cmml">t</mi><mo mathsize="90%" id="S3.E1.m1.9.9.9.5.1.3.2.2" xref="S3.E1.m1.9.9.9.5.1.3.1.cmml">,</mo><mi mathsize="90%" id="S3.E1.m1.6.6.6.2" xref="S3.E1.m1.6.6.6.2.cmml">f</mi><mo maxsize="90%" minsize="90%" id="S3.E1.m1.9.9.9.5.1.3.2.3" xref="S3.E1.m1.9.9.9.5.1.3.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.9.9.9.5.1.1a" xref="S3.E1.m1.9.9.9.5.1.1.cmml">​</mo><msubsup id="S3.E1.m1.9.9.9.5.1.4" xref="S3.E1.m1.9.9.9.5.1.4.cmml"><mi mathsize="90%" id="S3.E1.m1.9.9.9.5.1.4.2.2" xref="S3.E1.m1.9.9.9.5.1.4.2.2.cmml">𝐗</mi><mi mathsize="90%" id="S3.E1.m1.9.9.9.5.1.4.3" xref="S3.E1.m1.9.9.9.5.1.4.3.cmml">j</mi><mo mathsize="90%" id="S3.E1.m1.9.9.9.5.1.4.2.3" xref="S3.E1.m1.9.9.9.5.1.4.2.3.cmml">∗</mo></msubsup><mo lspace="0em" rspace="0em" id="S3.E1.m1.9.9.9.5.1.1b" xref="S3.E1.m1.9.9.9.5.1.1.cmml">​</mo><mrow id="S3.E1.m1.9.9.9.5.1.5.2" xref="S3.E1.m1.9.9.9.5.1.5.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.9.9.9.5.1.5.2.1" xref="S3.E1.m1.9.9.9.5.1.5.1.cmml">(</mo><mi mathsize="90%" id="S3.E1.m1.7.7.7.3" xref="S3.E1.m1.7.7.7.3.cmml">t</mi><mo mathsize="90%" id="S3.E1.m1.9.9.9.5.1.5.2.2" xref="S3.E1.m1.9.9.9.5.1.5.1.cmml">,</mo><mi mathsize="90%" id="S3.E1.m1.8.8.8.4" xref="S3.E1.m1.8.8.8.4.cmml">f</mi><mo maxsize="90%" minsize="90%" id="S3.E1.m1.9.9.9.5.1.5.2.3" xref="S3.E1.m1.9.9.9.5.1.5.1.cmml">)</mo></mrow></mrow><mo maxsize="90%" minsize="90%" id="S3.E1.m1.9.9.9.5.3" xref="S3.E1.m1.9.9.9.6.1.cmml">|</mo></mrow></mfrac></mrow></mrow><mo mathsize="90%" id="S3.E1.m1.12.12.1.2" xref="S3.E1.m1.12.12.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.12b"><apply id="S3.E1.m1.12.12.1.1.cmml" xref="S3.E1.m1.12.12.1"><eq id="S3.E1.m1.12.12.1.1.1.cmml" xref="S3.E1.m1.12.12.1.1.1"></eq><apply id="S3.E1.m1.12.12.1.1.2.cmml" xref="S3.E1.m1.12.12.1.1.2"><times id="S3.E1.m1.12.12.1.1.2.1.cmml" xref="S3.E1.m1.12.12.1.1.2.1"></times><ci id="S3.E1.m1.12.12.1.1.2.2.cmml" xref="S3.E1.m1.12.12.1.1.2.2">𝐺</ci><ci id="S3.E1.m1.12.12.1.1.2.3.cmml" xref="S3.E1.m1.12.12.1.1.2.3">𝐶</ci><apply id="S3.E1.m1.12.12.1.1.2.4.cmml" xref="S3.E1.m1.12.12.1.1.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.12.12.1.1.2.4.1.cmml" xref="S3.E1.m1.12.12.1.1.2.4">subscript</csymbol><ci id="S3.E1.m1.12.12.1.1.2.4.2.cmml" xref="S3.E1.m1.12.12.1.1.2.4.2">𝐶</ci><apply id="S3.E1.m1.12.12.1.1.2.4.3.cmml" xref="S3.E1.m1.12.12.1.1.2.4.3"><times id="S3.E1.m1.12.12.1.1.2.4.3.1.cmml" xref="S3.E1.m1.12.12.1.1.2.4.3.1"></times><ci id="S3.E1.m1.12.12.1.1.2.4.3.2.cmml" xref="S3.E1.m1.12.12.1.1.2.4.3.2">𝑖</ci><ci id="S3.E1.m1.12.12.1.1.2.4.3.3.cmml" xref="S3.E1.m1.12.12.1.1.2.4.3.3">𝑗</ci></apply></apply><interval closure="open" id="S3.E1.m1.12.12.1.1.2.5.1.cmml" xref="S3.E1.m1.12.12.1.1.2.5.2"><ci id="S3.E1.m1.10.10.cmml" xref="S3.E1.m1.10.10">𝑡</ci><ci id="S3.E1.m1.11.11.cmml" xref="S3.E1.m1.11.11">𝜏</ci></interval></apply><apply id="S3.E1.m1.12.12.1.1.3.cmml" xref="S3.E1.m1.12.12.1.1.3"><times id="S3.E1.m1.12.12.1.1.3.1.cmml" xref="S3.E1.m1.12.12.1.1.3.1"></times><apply id="S3.E1.m1.12.12.1.1.3.2.cmml" xref="S3.E1.m1.12.12.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.12.12.1.1.3.2.1.cmml" xref="S3.E1.m1.12.12.1.1.3.2">subscript</csymbol><apply id="S3.E1.m1.12.12.1.1.3.2.2.cmml" xref="S3.E1.m1.12.12.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.12.12.1.1.3.2.2.1.cmml" xref="S3.E1.m1.12.12.1.1.3.2">superscript</csymbol><ci id="S3.E1.m1.12.12.1.1.3.2.2.2.cmml" xref="S3.E1.m1.12.12.1.1.3.2.2.2">ℱ</ci><apply id="S3.E1.m1.12.12.1.1.3.2.2.3.cmml" xref="S3.E1.m1.12.12.1.1.3.2.2.3"><minus id="S3.E1.m1.12.12.1.1.3.2.2.3.1.cmml" xref="S3.E1.m1.12.12.1.1.3.2.2.3"></minus><cn type="integer" id="S3.E1.m1.12.12.1.1.3.2.2.3.2.cmml" xref="S3.E1.m1.12.12.1.1.3.2.2.3.2">1</cn></apply></apply><apply id="S3.E1.m1.12.12.1.1.3.2.3.cmml" xref="S3.E1.m1.12.12.1.1.3.2.3"><ci id="S3.E1.m1.12.12.1.1.3.2.3.1.cmml" xref="S3.E1.m1.12.12.1.1.3.2.3.1">→</ci><ci id="S3.E1.m1.12.12.1.1.3.2.3.2.cmml" xref="S3.E1.m1.12.12.1.1.3.2.3.2">𝑓</ci><ci id="S3.E1.m1.12.12.1.1.3.2.3.3.cmml" xref="S3.E1.m1.12.12.1.1.3.2.3.3">𝜏</ci></apply></apply><apply id="S3.E1.m1.9.9.cmml" xref="S3.E1.m1.9.9"><divide id="S3.E1.m1.9.9.10.cmml" xref="S3.E1.m1.9.9"></divide><apply id="S3.E1.m1.4.4.4.cmml" xref="S3.E1.m1.4.4.4"><times id="S3.E1.m1.4.4.4.5.cmml" xref="S3.E1.m1.4.4.4.5"></times><apply id="S3.E1.m1.4.4.4.6.cmml" xref="S3.E1.m1.4.4.4.6"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.6.1.cmml" xref="S3.E1.m1.4.4.4.6">subscript</csymbol><ci id="S3.E1.m1.4.4.4.6.2.cmml" xref="S3.E1.m1.4.4.4.6.2">𝐗</ci><ci id="S3.E1.m1.4.4.4.6.3.cmml" xref="S3.E1.m1.4.4.4.6.3">𝑖</ci></apply><interval closure="open" id="S3.E1.m1.4.4.4.7.1.cmml" xref="S3.E1.m1.4.4.4.7.2"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑡</ci><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">𝑓</ci></interval><apply id="S3.E1.m1.4.4.4.8.cmml" xref="S3.E1.m1.4.4.4.8"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.8.1.cmml" xref="S3.E1.m1.4.4.4.8">subscript</csymbol><apply id="S3.E1.m1.4.4.4.8.2.cmml" xref="S3.E1.m1.4.4.4.8"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.8.2.1.cmml" xref="S3.E1.m1.4.4.4.8">superscript</csymbol><ci id="S3.E1.m1.4.4.4.8.2.2.cmml" xref="S3.E1.m1.4.4.4.8.2.2">𝐗</ci><times id="S3.E1.m1.4.4.4.8.2.3.cmml" xref="S3.E1.m1.4.4.4.8.2.3"></times></apply><ci id="S3.E1.m1.4.4.4.8.3.cmml" xref="S3.E1.m1.4.4.4.8.3">𝑗</ci></apply><interval closure="open" id="S3.E1.m1.4.4.4.9.1.cmml" xref="S3.E1.m1.4.4.4.9.2"><ci id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3">𝑡</ci><ci id="S3.E1.m1.4.4.4.4.cmml" xref="S3.E1.m1.4.4.4.4">𝑓</ci></interval></apply><apply id="S3.E1.m1.9.9.9.6.cmml" xref="S3.E1.m1.9.9.9.5"><abs id="S3.E1.m1.9.9.9.6.1.cmml" xref="S3.E1.m1.9.9.9.5.2"></abs><apply id="S3.E1.m1.9.9.9.5.1.cmml" xref="S3.E1.m1.9.9.9.5.1"><times id="S3.E1.m1.9.9.9.5.1.1.cmml" xref="S3.E1.m1.9.9.9.5.1.1"></times><apply id="S3.E1.m1.9.9.9.5.1.2.cmml" xref="S3.E1.m1.9.9.9.5.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.9.5.1.2.1.cmml" xref="S3.E1.m1.9.9.9.5.1.2">subscript</csymbol><ci id="S3.E1.m1.9.9.9.5.1.2.2.cmml" xref="S3.E1.m1.9.9.9.5.1.2.2">𝐗</ci><ci id="S3.E1.m1.9.9.9.5.1.2.3.cmml" xref="S3.E1.m1.9.9.9.5.1.2.3">𝑖</ci></apply><interval closure="open" id="S3.E1.m1.9.9.9.5.1.3.1.cmml" xref="S3.E1.m1.9.9.9.5.1.3.2"><ci id="S3.E1.m1.5.5.5.1.cmml" xref="S3.E1.m1.5.5.5.1">𝑡</ci><ci id="S3.E1.m1.6.6.6.2.cmml" xref="S3.E1.m1.6.6.6.2">𝑓</ci></interval><apply id="S3.E1.m1.9.9.9.5.1.4.cmml" xref="S3.E1.m1.9.9.9.5.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.9.5.1.4.1.cmml" xref="S3.E1.m1.9.9.9.5.1.4">subscript</csymbol><apply id="S3.E1.m1.9.9.9.5.1.4.2.cmml" xref="S3.E1.m1.9.9.9.5.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.9.5.1.4.2.1.cmml" xref="S3.E1.m1.9.9.9.5.1.4">superscript</csymbol><ci id="S3.E1.m1.9.9.9.5.1.4.2.2.cmml" xref="S3.E1.m1.9.9.9.5.1.4.2.2">𝐗</ci><times id="S3.E1.m1.9.9.9.5.1.4.2.3.cmml" xref="S3.E1.m1.9.9.9.5.1.4.2.3"></times></apply><ci id="S3.E1.m1.9.9.9.5.1.4.3.cmml" xref="S3.E1.m1.9.9.9.5.1.4.3">𝑗</ci></apply><interval closure="open" id="S3.E1.m1.9.9.9.5.1.5.1.cmml" xref="S3.E1.m1.9.9.9.5.1.5.2"><ci id="S3.E1.m1.7.7.7.3.cmml" xref="S3.E1.m1.7.7.7.3">𝑡</ci><ci id="S3.E1.m1.8.8.8.4.cmml" xref="S3.E1.m1.8.8.8.4">𝑓</ci></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.12c">GCC_{ij}(t,\tau)=\mathcal{F}^{-1}_{f\rightarrow\tau}\frac{\mathbf{X}_{i}(t,f)\mathbf{X}^{*}_{j}(t,f)}{|\mathbf{X}_{i}(t,f)\mathbf{X}^{*}_{j}(t,f)|},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.7" class="ltx_p"><span id="S3.SS3.p2.7.1" class="ltx_text" style="font-size:90%;">where </span><math id="S3.SS3.p2.1.m1.2" class="ltx_Math" alttext="\mathbf{X}_{i}(t,f)" display="inline"><semantics id="S3.SS3.p2.1.m1.2a"><mrow id="S3.SS3.p2.1.m1.2.3" xref="S3.SS3.p2.1.m1.2.3.cmml"><msub id="S3.SS3.p2.1.m1.2.3.2" xref="S3.SS3.p2.1.m1.2.3.2.cmml"><mi mathsize="90%" id="S3.SS3.p2.1.m1.2.3.2.2" xref="S3.SS3.p2.1.m1.2.3.2.2.cmml">𝐗</mi><mi mathsize="90%" id="S3.SS3.p2.1.m1.2.3.2.3" xref="S3.SS3.p2.1.m1.2.3.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.2.3.1" xref="S3.SS3.p2.1.m1.2.3.1.cmml">​</mo><mrow id="S3.SS3.p2.1.m1.2.3.3.2" xref="S3.SS3.p2.1.m1.2.3.3.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS3.p2.1.m1.2.3.3.2.1" xref="S3.SS3.p2.1.m1.2.3.3.1.cmml">(</mo><mi mathsize="90%" id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">t</mi><mo mathsize="90%" id="S3.SS3.p2.1.m1.2.3.3.2.2" xref="S3.SS3.p2.1.m1.2.3.3.1.cmml">,</mo><mi mathsize="90%" id="S3.SS3.p2.1.m1.2.2" xref="S3.SS3.p2.1.m1.2.2.cmml">f</mi><mo maxsize="90%" minsize="90%" id="S3.SS3.p2.1.m1.2.3.3.2.3" xref="S3.SS3.p2.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.2b"><apply id="S3.SS3.p2.1.m1.2.3.cmml" xref="S3.SS3.p2.1.m1.2.3"><times id="S3.SS3.p2.1.m1.2.3.1.cmml" xref="S3.SS3.p2.1.m1.2.3.1"></times><apply id="S3.SS3.p2.1.m1.2.3.2.cmml" xref="S3.SS3.p2.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.2.3.2.1.cmml" xref="S3.SS3.p2.1.m1.2.3.2">subscript</csymbol><ci id="S3.SS3.p2.1.m1.2.3.2.2.cmml" xref="S3.SS3.p2.1.m1.2.3.2.2">𝐗</ci><ci id="S3.SS3.p2.1.m1.2.3.2.3.cmml" xref="S3.SS3.p2.1.m1.2.3.2.3">𝑖</ci></apply><interval closure="open" id="S3.SS3.p2.1.m1.2.3.3.1.cmml" xref="S3.SS3.p2.1.m1.2.3.3.2"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝑡</ci><ci id="S3.SS3.p2.1.m1.2.2.cmml" xref="S3.SS3.p2.1.m1.2.2">𝑓</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.2c">\mathbf{X}_{i}(t,f)</annotation></semantics></math><span id="S3.SS3.p2.7.2" class="ltx_text" style="font-size:90%;"> is the Short-Time Fourier Transform (STFT) of the </span><span id="S3.SS3.p2.7.3" class="ltx_text ltx_font_italic" style="font-size:90%;">i</span><span id="S3.SS3.p2.7.4" class="ltx_text" style="font-size:90%;">-th channel, </span><math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{F}^{-1}_{f\rightarrow\tau}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msubsup id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.SS3.p2.2.m2.1.1.2.2" xref="S3.SS3.p2.2.m2.1.1.2.2.cmml">ℱ</mi><mrow id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml"><mi mathsize="90%" id="S3.SS3.p2.2.m2.1.1.3.2" xref="S3.SS3.p2.2.m2.1.1.3.2.cmml">f</mi><mo mathsize="90%" stretchy="false" id="S3.SS3.p2.2.m2.1.1.3.1" xref="S3.SS3.p2.2.m2.1.1.3.1.cmml">→</mo><mi mathsize="90%" id="S3.SS3.p2.2.m2.1.1.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.cmml">τ</mi></mrow><mrow id="S3.SS3.p2.2.m2.1.1.2.3" xref="S3.SS3.p2.2.m2.1.1.2.3.cmml"><mo mathsize="90%" id="S3.SS3.p2.2.m2.1.1.2.3a" xref="S3.SS3.p2.2.m2.1.1.2.3.cmml">−</mo><mn mathsize="90%" id="S3.SS3.p2.2.m2.1.1.2.3.2" xref="S3.SS3.p2.2.m2.1.1.2.3.2.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><apply id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.2.1.cmml" xref="S3.SS3.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2.2">ℱ</ci><apply id="S3.SS3.p2.2.m2.1.1.2.3.cmml" xref="S3.SS3.p2.2.m2.1.1.2.3"><minus id="S3.SS3.p2.2.m2.1.1.2.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.2.3"></minus><cn type="integer" id="S3.SS3.p2.2.m2.1.1.2.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2.3.2">1</cn></apply></apply><apply id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3"><ci id="S3.SS3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.1">→</ci><ci id="S3.SS3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2">𝑓</ci><ci id="S3.SS3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3">𝜏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\mathcal{F}^{-1}_{f\rightarrow\tau}</annotation></semantics></math><span id="S3.SS3.p2.7.5" class="ltx_text" style="font-size:90%;"> the inverse-FFT from the frequency domain </span><math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi mathsize="90%" id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">f</annotation></semantics></math><span id="S3.SS3.p2.7.6" class="ltx_text" style="font-size:90%;"> to the lag-time domain </span><math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mi mathsize="90%" id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\tau</annotation></semantics></math><span id="S3.SS3.p2.7.7" class="ltx_text" style="font-size:90%;">, and </span><math id="S3.SS3.p2.5.m5.1" class="ltx_math_unparsed" alttext="(.)^{*}" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><msup id="S3.SS3.p2.5.m5.1.1"><mrow id="S3.SS3.p2.5.m5.1.1.2"><mo maxsize="90%" minsize="90%" id="S3.SS3.p2.5.m5.1.1.2.1">(</mo><mo lspace="0em" mathsize="90%" rspace="0.167em" id="S3.SS3.p2.5.m5.1.1.2.2">.</mo><mo maxsize="90%" minsize="90%" id="S3.SS3.p2.5.m5.1.1.2.3">)</mo></mrow><mo mathsize="90%" id="S3.SS3.p2.5.m5.1.1.3">∗</mo></msup><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1b">(.)^{*}</annotation></semantics></math><span id="S3.SS3.p2.7.8" class="ltx_text" style="font-size:90%;"> denotes the complex conjugate. The TDOA can be estimated as the lag-time </span><math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="\Delta\tau" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><mrow id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mi mathsize="90%" mathvariant="normal" id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.6.m6.1.1.1" xref="S3.SS3.p2.6.m6.1.1.1.cmml">​</mo><mi mathsize="90%" id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml">τ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><times id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1.1"></times><ci id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2">Δ</ci><ci id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3">𝜏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">\Delta\tau</annotation></semantics></math><span id="S3.SS3.p2.7.9" class="ltx_text" style="font-size:90%;"> that maximizes </span><math id="S3.SS3.p2.7.m7.2" class="ltx_Math" alttext="GCC_{ij}(t,\tau)" display="inline"><semantics id="S3.SS3.p2.7.m7.2a"><mrow id="S3.SS3.p2.7.m7.2.3" xref="S3.SS3.p2.7.m7.2.3.cmml"><mi mathsize="90%" id="S3.SS3.p2.7.m7.2.3.2" xref="S3.SS3.p2.7.m7.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.7.m7.2.3.1" xref="S3.SS3.p2.7.m7.2.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS3.p2.7.m7.2.3.3" xref="S3.SS3.p2.7.m7.2.3.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.7.m7.2.3.1a" xref="S3.SS3.p2.7.m7.2.3.1.cmml">​</mo><msub id="S3.SS3.p2.7.m7.2.3.4" xref="S3.SS3.p2.7.m7.2.3.4.cmml"><mi mathsize="90%" id="S3.SS3.p2.7.m7.2.3.4.2" xref="S3.SS3.p2.7.m7.2.3.4.2.cmml">C</mi><mrow id="S3.SS3.p2.7.m7.2.3.4.3" xref="S3.SS3.p2.7.m7.2.3.4.3.cmml"><mi mathsize="90%" id="S3.SS3.p2.7.m7.2.3.4.3.2" xref="S3.SS3.p2.7.m7.2.3.4.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.7.m7.2.3.4.3.1" xref="S3.SS3.p2.7.m7.2.3.4.3.1.cmml">​</mo><mi mathsize="90%" id="S3.SS3.p2.7.m7.2.3.4.3.3" xref="S3.SS3.p2.7.m7.2.3.4.3.3.cmml">j</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p2.7.m7.2.3.1b" xref="S3.SS3.p2.7.m7.2.3.1.cmml">​</mo><mrow id="S3.SS3.p2.7.m7.2.3.5.2" xref="S3.SS3.p2.7.m7.2.3.5.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.SS3.p2.7.m7.2.3.5.2.1" xref="S3.SS3.p2.7.m7.2.3.5.1.cmml">(</mo><mi mathsize="90%" id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml">t</mi><mo mathsize="90%" id="S3.SS3.p2.7.m7.2.3.5.2.2" xref="S3.SS3.p2.7.m7.2.3.5.1.cmml">,</mo><mi mathsize="90%" id="S3.SS3.p2.7.m7.2.2" xref="S3.SS3.p2.7.m7.2.2.cmml">τ</mi><mo maxsize="90%" minsize="90%" id="S3.SS3.p2.7.m7.2.3.5.2.3" xref="S3.SS3.p2.7.m7.2.3.5.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.2b"><apply id="S3.SS3.p2.7.m7.2.3.cmml" xref="S3.SS3.p2.7.m7.2.3"><times id="S3.SS3.p2.7.m7.2.3.1.cmml" xref="S3.SS3.p2.7.m7.2.3.1"></times><ci id="S3.SS3.p2.7.m7.2.3.2.cmml" xref="S3.SS3.p2.7.m7.2.3.2">𝐺</ci><ci id="S3.SS3.p2.7.m7.2.3.3.cmml" xref="S3.SS3.p2.7.m7.2.3.3">𝐶</ci><apply id="S3.SS3.p2.7.m7.2.3.4.cmml" xref="S3.SS3.p2.7.m7.2.3.4"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m7.2.3.4.1.cmml" xref="S3.SS3.p2.7.m7.2.3.4">subscript</csymbol><ci id="S3.SS3.p2.7.m7.2.3.4.2.cmml" xref="S3.SS3.p2.7.m7.2.3.4.2">𝐶</ci><apply id="S3.SS3.p2.7.m7.2.3.4.3.cmml" xref="S3.SS3.p2.7.m7.2.3.4.3"><times id="S3.SS3.p2.7.m7.2.3.4.3.1.cmml" xref="S3.SS3.p2.7.m7.2.3.4.3.1"></times><ci id="S3.SS3.p2.7.m7.2.3.4.3.2.cmml" xref="S3.SS3.p2.7.m7.2.3.4.3.2">𝑖</ci><ci id="S3.SS3.p2.7.m7.2.3.4.3.3.cmml" xref="S3.SS3.p2.7.m7.2.3.4.3.3">𝑗</ci></apply></apply><interval closure="open" id="S3.SS3.p2.7.m7.2.3.5.1.cmml" xref="S3.SS3.p2.7.m7.2.3.5.2"><ci id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1">𝑡</ci><ci id="S3.SS3.p2.7.m7.2.2.cmml" xref="S3.SS3.p2.7.m7.2.2">𝜏</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.2c">GCC_{ij}(t,\tau)</annotation></semantics></math><span id="S3.SS3.p2.7.10" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.4" class="ltx_p"><span id="S3.SS3.p3.4.1" class="ltx_text" style="font-size:90%;">With time bins (</span><math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi mathsize="90%" id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">t</annotation></semantics></math><span id="S3.SS3.p3.4.2" class="ltx_text" style="font-size:90%;">) on the </span><math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi mathsize="90%" id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">x</annotation></semantics></math><span id="S3.SS3.p3.4.3" class="ltx_text" style="font-size:90%;">-axis and time-lags (</span><math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><mi mathsize="90%" id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><ci id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\tau</annotation></semantics></math><span id="S3.SS3.p3.4.4" class="ltx_text" style="font-size:90%;">) on the </span><math id="S3.SS3.p3.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><mi mathsize="90%" id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><ci id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">y</annotation></semantics></math><span id="S3.SS3.p3.4.5" class="ltx_text" style="font-size:90%;">-axis, the GCC-PHAT can be concatenated with the log-mel spectrograms extracted from the channels of the microphone array, as indicated by Cao </span><em id="S3.SS3.p3.4.6" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al</em><span id="S3.SS3.p3.4.7" class="ltx_text" style="font-size:90%;">. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p3.4.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S3.SS3.p3.4.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS3.p3.4.10" class="ltx_text" style="font-size:90%;">. The concatenation of GCC-PHAT features and log-mel spectrograms provides the network with a unified representation that enables both detection and localization subtasks. We compute the GCC-PHAT between a reference microphone and the other microphones of the array. As the reference microphone, we select the first channel of the lower sub-array, as highlighted in Figure </span><a href="#S3.F2" title="Figure 2 ‣ 3.1 Dataset ‣ 3 Method ‣ Audio-visual talker localization in video for spatial sound reproduction" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS3.p3.4.11" class="ltx_text" style="font-size:90%;"> (a). From the same channel, we also extract a single log-mel spectrogram for the concatenation.</span></p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Loss Function</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p"><span id="S3.SS4.p1.1.1" class="ltx_text" style="font-size:90%;">For each input segment, the loss function </span><math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\mathcal{L}</annotation></semantics></math><span id="S3.SS4.p1.1.2" class="ltx_text" style="font-size:90%;"> is determined as the sum of the individual frame losses. A sum-squared error loss </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS4.p1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib46" title="" class="ltx_ref">46</a><span id="S3.SS4.p1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS4.p1.1.5" class="ltx_text" style="font-size:90%;"> is computed at each output frame and is comprised of a regression and a voice activity confidence loss:</span></p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\mathcal{L}=\sum_{i=1}^{T_{in}/16}\mathbbm{1}_{i}(x_{i}-\hat{x}_{i})^{2}+(C_{i}-\hat{C}_{i})^{2}" display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.E2.m1.2.2.4" xref="S3.E2.m1.2.2.4.cmml">ℒ</mi><mo mathsize="90%" rspace="0.111em" id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml">=</mo><mrow id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><munderover id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" movablelimits="false" stretchy="true" id="S3.E2.m1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml"><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo mathsize="90%" id="S3.E2.m1.1.1.1.1.2.2.3.1" xref="S3.E2.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn mathsize="90%" id="S3.E2.m1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml"><msub id="S3.E2.m1.1.1.1.1.2.3.2" xref="S3.E2.m1.1.1.1.1.2.3.2.cmml"><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.2.3.2.2" xref="S3.E2.m1.1.1.1.1.2.3.2.2.cmml">T</mi><mrow id="S3.E2.m1.1.1.1.1.2.3.2.3" xref="S3.E2.m1.1.1.1.1.2.3.2.3.cmml"><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.2.3.2.3.2" xref="S3.E2.m1.1.1.1.1.2.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2.3.2.3.1" xref="S3.E2.m1.1.1.1.1.2.3.2.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.2.3.2.3.3" xref="S3.E2.m1.1.1.1.1.2.3.2.3.3.cmml">n</mi></mrow></msub><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="S3.E2.m1.1.1.1.1.2.3.1" xref="S3.E2.m1.1.1.1.1.2.3.1.cmml">/</mo><mn mathsize="90%" id="S3.E2.m1.1.1.1.1.2.3.3" xref="S3.E2.m1.1.1.1.1.2.3.3.cmml">16</mn></mrow></munderover><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml"><mn mathsize="90%" id="S3.E2.m1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.3.2.cmml">𝟙</mn><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">​</mo><msup id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml"><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">x</mi><mo mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo maxsize="90%" minsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow><mo mathsize="90%" id="S3.E2.m1.2.2.2.3" xref="S3.E2.m1.2.2.2.3.cmml">+</mo><msup id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml"><mrow id="S3.E2.m1.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E2.m1.2.2.2.2.1.1.2" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.2.2.1.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml"><msub id="S3.E2.m1.2.2.2.2.1.1.1.2" xref="S3.E2.m1.2.2.2.2.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E2.m1.2.2.2.2.1.1.1.2.2" xref="S3.E2.m1.2.2.2.2.1.1.1.2.2.cmml">C</mi><mi mathsize="90%" id="S3.E2.m1.2.2.2.2.1.1.1.2.3" xref="S3.E2.m1.2.2.2.2.1.1.1.2.3.cmml">i</mi></msub><mo mathsize="90%" id="S3.E2.m1.2.2.2.2.1.1.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.1.cmml">−</mo><msub id="S3.E2.m1.2.2.2.2.1.1.1.3" xref="S3.E2.m1.2.2.2.2.1.1.1.3.cmml"><mover accent="true" id="S3.E2.m1.2.2.2.2.1.1.1.3.2" xref="S3.E2.m1.2.2.2.2.1.1.1.3.2.cmml"><mi mathsize="90%" id="S3.E2.m1.2.2.2.2.1.1.1.3.2.2" xref="S3.E2.m1.2.2.2.2.1.1.1.3.2.2.cmml">C</mi><mo mathsize="90%" id="S3.E2.m1.2.2.2.2.1.1.1.3.2.1" xref="S3.E2.m1.2.2.2.2.1.1.1.3.2.1.cmml">^</mo></mover><mi mathsize="90%" id="S3.E2.m1.2.2.2.2.1.1.1.3.3" xref="S3.E2.m1.2.2.2.2.1.1.1.3.3.cmml">i</mi></msub></mrow><mo maxsize="90%" minsize="90%" id="S3.E2.m1.2.2.2.2.1.1.3" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow><mn mathsize="90%" id="S3.E2.m1.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><eq id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3"></eq><ci id="S3.E2.m1.2.2.4.cmml" xref="S3.E2.m1.2.2.4">ℒ</ci><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><plus id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.3"></plus><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3"><eq id="S3.E2.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3"><divide id="S3.E2.m1.1.1.1.1.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.3.1"></divide><apply id="S3.E2.m1.1.1.1.1.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.2.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.3.2.2">𝑇</ci><apply id="S3.E2.m1.1.1.1.1.2.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3.2.3"><times id="S3.E2.m1.1.1.1.1.2.3.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.3.2.3.1"></times><ci id="S3.E2.m1.1.1.1.1.2.3.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.3.2.3.2">𝑖</ci><ci id="S3.E2.m1.1.1.1.1.2.3.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3.2.3.3">𝑛</ci></apply></apply><cn type="integer" id="S3.E2.m1.1.1.1.1.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3.3">16</cn></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3">subscript</csymbol><cn type="integer" id="S3.E2.m1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2">1</cn><ci id="S3.E2.m1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2"><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.1">^</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.2">𝑥</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">2</cn></apply></apply></apply><apply id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">superscript</csymbol><apply id="S3.E2.m1.2.2.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1"><minus id="S3.E2.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.1"></minus><apply id="S3.E2.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.2.2">𝐶</ci><ci id="S3.E2.m1.2.2.2.2.1.1.1.2.3.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E2.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3">subscript</csymbol><apply id="S3.E2.m1.2.2.2.2.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3.2"><ci id="S3.E2.m1.2.2.2.2.1.1.1.3.2.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3.2.1">^</ci><ci id="S3.E2.m1.2.2.2.2.1.1.1.3.2.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3.2.2">𝐶</ci></apply><ci id="S3.E2.m1.2.2.2.2.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3.3">𝑖</ci></apply></apply><cn type="integer" id="S3.E2.m1.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\mathcal{L}=\sum_{i=1}^{T_{in}/16}\mathbbm{1}_{i}(x_{i}-\hat{x}_{i})^{2}+(C_{i}-\hat{C}_{i})^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p1.9" class="ltx_p"><span id="S3.SS4.p1.9.1" class="ltx_text" style="font-size:90%;">where </span><math id="S3.SS4.p1.2.m1.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S3.SS4.p1.2.m1.1a"><msub id="S3.SS4.p1.2.m1.1.1" xref="S3.SS4.p1.2.m1.1.1.cmml"><mi mathsize="90%" id="S3.SS4.p1.2.m1.1.1.2" xref="S3.SS4.p1.2.m1.1.1.2.cmml">x</mi><mi mathsize="90%" id="S3.SS4.p1.2.m1.1.1.3" xref="S3.SS4.p1.2.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m1.1b"><apply id="S3.SS4.p1.2.m1.1.1.cmml" xref="S3.SS4.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m1.1.1.1.cmml" xref="S3.SS4.p1.2.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.2.m1.1.1.2.cmml" xref="S3.SS4.p1.2.m1.1.1.2">𝑥</ci><ci id="S3.SS4.p1.2.m1.1.1.3.cmml" xref="S3.SS4.p1.2.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m1.1c">x_{i}</annotation></semantics></math><span id="S3.SS4.p1.9.2" class="ltx_text" style="font-size:90%;"> and </span><math id="S3.SS4.p1.3.m2.1" class="ltx_Math" alttext="\hat{x}_{i}" display="inline"><semantics id="S3.SS4.p1.3.m2.1a"><msub id="S3.SS4.p1.3.m2.1.1" xref="S3.SS4.p1.3.m2.1.1.cmml"><mover accent="true" id="S3.SS4.p1.3.m2.1.1.2" xref="S3.SS4.p1.3.m2.1.1.2.cmml"><mi mathsize="90%" id="S3.SS4.p1.3.m2.1.1.2.2" xref="S3.SS4.p1.3.m2.1.1.2.2.cmml">x</mi><mo mathsize="90%" id="S3.SS4.p1.3.m2.1.1.2.1" xref="S3.SS4.p1.3.m2.1.1.2.1.cmml">^</mo></mover><mi mathsize="90%" id="S3.SS4.p1.3.m2.1.1.3" xref="S3.SS4.p1.3.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m2.1b"><apply id="S3.SS4.p1.3.m2.1.1.cmml" xref="S3.SS4.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.3.m2.1.1.1.cmml" xref="S3.SS4.p1.3.m2.1.1">subscript</csymbol><apply id="S3.SS4.p1.3.m2.1.1.2.cmml" xref="S3.SS4.p1.3.m2.1.1.2"><ci id="S3.SS4.p1.3.m2.1.1.2.1.cmml" xref="S3.SS4.p1.3.m2.1.1.2.1">^</ci><ci id="S3.SS4.p1.3.m2.1.1.2.2.cmml" xref="S3.SS4.p1.3.m2.1.1.2.2">𝑥</ci></apply><ci id="S3.SS4.p1.3.m2.1.1.3.cmml" xref="S3.SS4.p1.3.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m2.1c">\hat{x}_{i}</annotation></semantics></math><span id="S3.SS4.p1.9.3" class="ltx_text" style="font-size:90%;"> are respectively the predicted and target positions of the speaker along the horizontal axis of the </span><math id="S3.SS4.p1.4.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS4.p1.4.m3.1a"><mi mathsize="90%" id="S3.SS4.p1.4.m3.1.1" xref="S3.SS4.p1.4.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m3.1b"><ci id="S3.SS4.p1.4.m3.1.1.cmml" xref="S3.SS4.p1.4.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m3.1c">i</annotation></semantics></math><span id="S3.SS4.p1.9.4" class="ltx_text" style="font-size:90%;">-th video frame, while </span><math id="S3.SS4.p1.5.m4.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.SS4.p1.5.m4.1a"><msub id="S3.SS4.p1.5.m4.1.1" xref="S3.SS4.p1.5.m4.1.1.cmml"><mi mathsize="90%" id="S3.SS4.p1.5.m4.1.1.2" xref="S3.SS4.p1.5.m4.1.1.2.cmml">C</mi><mi mathsize="90%" id="S3.SS4.p1.5.m4.1.1.3" xref="S3.SS4.p1.5.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m4.1b"><apply id="S3.SS4.p1.5.m4.1.1.cmml" xref="S3.SS4.p1.5.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.5.m4.1.1.1.cmml" xref="S3.SS4.p1.5.m4.1.1">subscript</csymbol><ci id="S3.SS4.p1.5.m4.1.1.2.cmml" xref="S3.SS4.p1.5.m4.1.1.2">𝐶</ci><ci id="S3.SS4.p1.5.m4.1.1.3.cmml" xref="S3.SS4.p1.5.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m4.1c">C_{i}</annotation></semantics></math><span id="S3.SS4.p1.9.5" class="ltx_text" style="font-size:90%;"> and </span><math id="S3.SS4.p1.6.m5.1" class="ltx_Math" alttext="\hat{C}_{i}" display="inline"><semantics id="S3.SS4.p1.6.m5.1a"><msub id="S3.SS4.p1.6.m5.1.1" xref="S3.SS4.p1.6.m5.1.1.cmml"><mover accent="true" id="S3.SS4.p1.6.m5.1.1.2" xref="S3.SS4.p1.6.m5.1.1.2.cmml"><mi mathsize="90%" id="S3.SS4.p1.6.m5.1.1.2.2" xref="S3.SS4.p1.6.m5.1.1.2.2.cmml">C</mi><mo mathsize="90%" id="S3.SS4.p1.6.m5.1.1.2.1" xref="S3.SS4.p1.6.m5.1.1.2.1.cmml">^</mo></mover><mi mathsize="90%" id="S3.SS4.p1.6.m5.1.1.3" xref="S3.SS4.p1.6.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m5.1b"><apply id="S3.SS4.p1.6.m5.1.1.cmml" xref="S3.SS4.p1.6.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.6.m5.1.1.1.cmml" xref="S3.SS4.p1.6.m5.1.1">subscript</csymbol><apply id="S3.SS4.p1.6.m5.1.1.2.cmml" xref="S3.SS4.p1.6.m5.1.1.2"><ci id="S3.SS4.p1.6.m5.1.1.2.1.cmml" xref="S3.SS4.p1.6.m5.1.1.2.1">^</ci><ci id="S3.SS4.p1.6.m5.1.1.2.2.cmml" xref="S3.SS4.p1.6.m5.1.1.2.2">𝐶</ci></apply><ci id="S3.SS4.p1.6.m5.1.1.3.cmml" xref="S3.SS4.p1.6.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m5.1c">\hat{C}_{i}</annotation></semantics></math><span id="S3.SS4.p1.9.6" class="ltx_text" style="font-size:90%;"> are the predicted and target confidences. The voice activity confidence loss is trivially achieved using the voice activity annotations: </span><math id="S3.SS4.p1.7.m6.1" class="ltx_Math" alttext="\hat{C}_{i}" display="inline"><semantics id="S3.SS4.p1.7.m6.1a"><msub id="S3.SS4.p1.7.m6.1.1" xref="S3.SS4.p1.7.m6.1.1.cmml"><mover accent="true" id="S3.SS4.p1.7.m6.1.1.2" xref="S3.SS4.p1.7.m6.1.1.2.cmml"><mi mathsize="90%" id="S3.SS4.p1.7.m6.1.1.2.2" xref="S3.SS4.p1.7.m6.1.1.2.2.cmml">C</mi><mo mathsize="90%" id="S3.SS4.p1.7.m6.1.1.2.1" xref="S3.SS4.p1.7.m6.1.1.2.1.cmml">^</mo></mover><mi mathsize="90%" id="S3.SS4.p1.7.m6.1.1.3" xref="S3.SS4.p1.7.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m6.1b"><apply id="S3.SS4.p1.7.m6.1.1.cmml" xref="S3.SS4.p1.7.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.7.m6.1.1.1.cmml" xref="S3.SS4.p1.7.m6.1.1">subscript</csymbol><apply id="S3.SS4.p1.7.m6.1.1.2.cmml" xref="S3.SS4.p1.7.m6.1.1.2"><ci id="S3.SS4.p1.7.m6.1.1.2.1.cmml" xref="S3.SS4.p1.7.m6.1.1.2.1">^</ci><ci id="S3.SS4.p1.7.m6.1.1.2.2.cmml" xref="S3.SS4.p1.7.m6.1.1.2.2">𝐶</ci></apply><ci id="S3.SS4.p1.7.m6.1.1.3.cmml" xref="S3.SS4.p1.7.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m6.1c">\hat{C}_{i}</annotation></semantics></math><span id="S3.SS4.p1.9.7" class="ltx_text" style="font-size:90%;"> is set to 1 when the frame is active and 0 when silent.
The masking term </span><math id="S3.SS4.p1.8.m7.1" class="ltx_Math" alttext="\mathbbm{1}_{i}" display="inline"><semantics id="S3.SS4.p1.8.m7.1a"><msub id="S3.SS4.p1.8.m7.1.1" xref="S3.SS4.p1.8.m7.1.1.cmml"><mn mathsize="90%" id="S3.SS4.p1.8.m7.1.1.2" xref="S3.SS4.p1.8.m7.1.1.2.cmml">𝟙</mn><mi mathsize="90%" id="S3.SS4.p1.8.m7.1.1.3" xref="S3.SS4.p1.8.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.8.m7.1b"><apply id="S3.SS4.p1.8.m7.1.1.cmml" xref="S3.SS4.p1.8.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.8.m7.1.1.1.cmml" xref="S3.SS4.p1.8.m7.1.1">subscript</csymbol><cn type="integer" id="S3.SS4.p1.8.m7.1.1.2.cmml" xref="S3.SS4.p1.8.m7.1.1.2">1</cn><ci id="S3.SS4.p1.8.m7.1.1.3.cmml" xref="S3.SS4.p1.8.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.8.m7.1c">\mathbbm{1}_{i}</annotation></semantics></math><span id="S3.SS4.p1.9.8" class="ltx_text" style="font-size:90%;"> is 1 only when voice activity GT is true. It is set to 0 otherwise. So, when the frame is silent, the network is only penalized by the voice activity confidence loss and not by the regression loss. The target position of the speaker, </span><math id="S3.SS4.p1.9.m8.1" class="ltx_Math" alttext="\hat{x}_{i}" display="inline"><semantics id="S3.SS4.p1.9.m8.1a"><msub id="S3.SS4.p1.9.m8.1.1" xref="S3.SS4.p1.9.m8.1.1.cmml"><mover accent="true" id="S3.SS4.p1.9.m8.1.1.2" xref="S3.SS4.p1.9.m8.1.1.2.cmml"><mi mathsize="90%" id="S3.SS4.p1.9.m8.1.1.2.2" xref="S3.SS4.p1.9.m8.1.1.2.2.cmml">x</mi><mo mathsize="90%" id="S3.SS4.p1.9.m8.1.1.2.1" xref="S3.SS4.p1.9.m8.1.1.2.1.cmml">^</mo></mover><mi mathsize="90%" id="S3.SS4.p1.9.m8.1.1.3" xref="S3.SS4.p1.9.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.9.m8.1b"><apply id="S3.SS4.p1.9.m8.1.1.cmml" xref="S3.SS4.p1.9.m8.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.9.m8.1.1.1.cmml" xref="S3.SS4.p1.9.m8.1.1">subscript</csymbol><apply id="S3.SS4.p1.9.m8.1.1.2.cmml" xref="S3.SS4.p1.9.m8.1.1.2"><ci id="S3.SS4.p1.9.m8.1.1.2.1.cmml" xref="S3.SS4.p1.9.m8.1.1.2.1">^</ci><ci id="S3.SS4.p1.9.m8.1.1.2.2.cmml" xref="S3.SS4.p1.9.m8.1.1.2.2">𝑥</ci></apply><ci id="S3.SS4.p1.9.m8.1.1.3.cmml" xref="S3.SS4.p1.9.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.9.m8.1c">\hat{x}_{i}</annotation></semantics></math><span id="S3.SS4.p1.9.9" class="ltx_text" style="font-size:90%;">, corresponds to the horizontal position of the center of the face bounding box of the active speaker, normalized by the size of the video frame to be in the range [0, 1].</span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation Details Evaluation Metrics</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">The network is trained with a 5-fold cross-validation approach: each validation fold sets aside 10 unseen sequences from the 50 sequences of the development set. This cross-validation approach is used to find suitable hyper-parameters for the network. Once found, the model is retrained using the entire 50-sequence training set with these values.
The network is trained for 50 epochs using batches of 32 audio feature inputs and Adam optimizer. The learning rate is fixed for the first 30 epochs, then reduced by 10% each epoch, as in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S4.SS1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.4" class="ltx_text" style="font-size:90%;">.
The initial learning rate determined in the cross-validation is </span><math id="S4.SS1.p1.1.m1.3" class="ltx_Math" alttext="{10}^{-4}" display="inline"><semantics id="S4.SS1.p1.1.m1.3a"><mrow id="S4.SS1.p1.1.m1.3.3.3" xref="S4.SS1.p1.1.m1.3.3.3.cmml"><mi id="S4.SS1.p1.1.m1.1.1.1.1.1.1.1" xref="S4.SS1.p1.1.m1.3.3.3.cmml"></mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.2.2.2.2.2.2.2" xref="S4.SS1.p1.1.m1.3.3.3.cmml">​</mo><msup id="S4.SS1.p1.1.m1.3.3.3.3.3.3.3" xref="S4.SS1.p1.1.m1.3.3.3.cmml"><mn mathsize="90%" id="S4.SS1.p1.1.m1.3.3.3.3.3.3.3.2" xref="S4.SS1.p1.1.m1.3.3.3.cmml">10</mn><mrow id="S4.SS1.p1.1.m1.3.3.3.3.3.3.3.3.2" xref="S4.SS1.p1.1.m1.3.3.3.cmml"><mo mathsize="90%" id="S4.SS1.p1.1.m1.3.3.3.3.3.3.3.3.2a" xref="S4.SS1.p1.1.m1.3.3.3.cmml">−</mo><mn mathsize="90%" id="S4.SS1.p1.1.m1.3.3.3.3.3.3.3.3.2.2" xref="S4.SS1.p1.1.m1.3.3.3.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.3b"><csymbol cd="latexml" id="S4.SS1.p1.1.m1.3.3.3.cmml" xref="S4.SS1.p1.1.m1.3.3.3">E-4</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.3c">{10}^{-4}</annotation></semantics></math><span id="S4.SS1.p1.1.5" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.3" class="ltx_p"><span id="S4.SS1.p2.3.1" class="ltx_text" style="font-size:90%;">The audio stream of the Tragic Talkers dataset is sampled at 48 kHz. The dataset is discretized into audio-visual segments of 2 seconds. The label frame rate is consistent with the video frame rate (30 fps).
To align the output temporal resolution (</span><math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="T_{out}=T_{in}/16" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><msub id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml"><mi mathsize="90%" id="S4.SS1.p2.1.m1.1.1.2.2" xref="S4.SS1.p2.1.m1.1.1.2.2.cmml">T</mi><mrow id="S4.SS1.p2.1.m1.1.1.2.3" xref="S4.SS1.p2.1.m1.1.1.2.3.cmml"><mi mathsize="90%" id="S4.SS1.p2.1.m1.1.1.2.3.2" xref="S4.SS1.p2.1.m1.1.1.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.1.m1.1.1.2.3.1" xref="S4.SS1.p2.1.m1.1.1.2.3.1.cmml">​</mo><mi mathsize="90%" id="S4.SS1.p2.1.m1.1.1.2.3.3" xref="S4.SS1.p2.1.m1.1.1.2.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.1.m1.1.1.2.3.1a" xref="S4.SS1.p2.1.m1.1.1.2.3.1.cmml">​</mo><mi mathsize="90%" id="S4.SS1.p2.1.m1.1.1.2.3.4" xref="S4.SS1.p2.1.m1.1.1.2.3.4.cmml">t</mi></mrow></msub><mo mathsize="90%" id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml"><msub id="S4.SS1.p2.1.m1.1.1.3.2" xref="S4.SS1.p2.1.m1.1.1.3.2.cmml"><mi mathsize="90%" id="S4.SS1.p2.1.m1.1.1.3.2.2" xref="S4.SS1.p2.1.m1.1.1.3.2.2.cmml">T</mi><mrow id="S4.SS1.p2.1.m1.1.1.3.2.3" xref="S4.SS1.p2.1.m1.1.1.3.2.3.cmml"><mi mathsize="90%" id="S4.SS1.p2.1.m1.1.1.3.2.3.2" xref="S4.SS1.p2.1.m1.1.1.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.1.m1.1.1.3.2.3.1" xref="S4.SS1.p2.1.m1.1.1.3.2.3.1.cmml">​</mo><mi mathsize="90%" id="S4.SS1.p2.1.m1.1.1.3.2.3.3" xref="S4.SS1.p2.1.m1.1.1.3.2.3.3.cmml">n</mi></mrow></msub><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="S4.SS1.p2.1.m1.1.1.3.1" xref="S4.SS1.p2.1.m1.1.1.3.1.cmml">/</mo><mn mathsize="90%" id="S4.SS1.p2.1.m1.1.1.3.3" xref="S4.SS1.p2.1.m1.1.1.3.3.cmml">16</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><eq id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></eq><apply id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.2.1.cmml" xref="S4.SS1.p2.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2.2">𝑇</ci><apply id="S4.SS1.p2.1.m1.1.1.2.3.cmml" xref="S4.SS1.p2.1.m1.1.1.2.3"><times id="S4.SS1.p2.1.m1.1.1.2.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.2.3.1"></times><ci id="S4.SS1.p2.1.m1.1.1.2.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2.3.2">𝑜</ci><ci id="S4.SS1.p2.1.m1.1.1.2.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.2.3.3">𝑢</ci><ci id="S4.SS1.p2.1.m1.1.1.2.3.4.cmml" xref="S4.SS1.p2.1.m1.1.1.2.3.4">𝑡</ci></apply></apply><apply id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3"><divide id="S4.SS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3.1"></divide><apply id="S4.SS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.3.2.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.3.2.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2.2">𝑇</ci><apply id="S4.SS1.p2.1.m1.1.1.3.2.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2.3"><times id="S4.SS1.p2.1.m1.1.1.3.2.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2.3.1"></times><ci id="S4.SS1.p2.1.m1.1.1.3.2.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2.3.2">𝑖</ci><ci id="S4.SS1.p2.1.m1.1.1.3.2.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2.3.3">𝑛</ci></apply></apply><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3.3">16</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">T_{out}=T_{in}/16</annotation></semantics></math><span id="S4.SS1.p2.3.2" class="ltx_text" style="font-size:90%;">) with the labels frame rate, i.e., generating 60 activity-regression pair predictions for the 2-second input, an STFT with Hann window is applied at hop steps of 100 samples. Thus, the 2-second (96k-sample) audio chunk is discretized into 960 temporal bins (96k</span><math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="/" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">/</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><divide id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"></divide></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">/</annotation></semantics></math><span id="S4.SS1.p2.3.3" class="ltx_text" style="font-size:90%;">100), which correspond to </span><math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="T_{in}" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><msub id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mi mathsize="90%" id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">T</mi><mrow id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml"><mi mathsize="90%" id="S4.SS1.p2.3.m3.1.1.3.2" xref="S4.SS1.p2.3.m3.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.1.1.3.1" xref="S4.SS1.p2.3.m3.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.SS1.p2.3.m3.1.1.3.3" xref="S4.SS1.p2.3.m3.1.1.3.3.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">𝑇</ci><apply id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3"><times id="S4.SS1.p2.3.m3.1.1.3.1.cmml" xref="S4.SS1.p2.3.m3.1.1.3.1"></times><ci id="S4.SS1.p2.3.m3.1.1.3.2.cmml" xref="S4.SS1.p2.3.m3.1.1.3.2">𝑖</ci><ci id="S4.SS1.p2.3.m3.1.1.3.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">T_{in}</annotation></semantics></math><span id="S4.SS1.p2.3.4" class="ltx_text" style="font-size:90%;">. The Hann window presents a size of 512 samples, as in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p2.3.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib47" title="" class="ltx_ref">47</a><span id="S4.SS1.p2.3.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p2.3.7" class="ltx_text" style="font-size:90%;">.
To compute the log-mel spectrogram used in the concatenation with the GCC-PHAT features, the frequency resolution of the spectrogram is down-sampled over 64 mel-frequency bins and the logarithm operation is applied.
The number of time-lags for the GCC-PHAT is also set to 64 to enable the concatenation.</span></p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.7" class="ltx_p"><span id="S4.SS1.p3.7.1" class="ltx_text" style="font-size:90%;">The evaluation is performed on the TragicTalkers test set, labeled for 2D speaker mouth positions.
A frame prediction is considered positive, i.e. the network predicts the presence of speech, when the predicted voice activity confidence is above a threshold </span><math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="Th" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi mathsize="90%" id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">​</mo><mi mathsize="90%" id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><times id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></times><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">𝑇</ci><ci id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">Th</annotation></semantics></math><span id="S4.SS1.p3.7.2" class="ltx_text" style="font-size:90%;">, and a positive detection is true when the localization error is within a predefined spatial tolerance </span><math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mi mathsize="90%" id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">S</annotation></semantics></math><span id="S4.SS1.p3.7.3" class="ltx_text" style="font-size:90%;">.
The precision and recall rates are computed by varying the confidence threshold </span><math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="Th" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mrow id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mi mathsize="90%" id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p3.3.m3.1.1.1" xref="S4.SS1.p3.3.m3.1.1.1.cmml">​</mo><mi mathsize="90%" id="S4.SS1.p3.3.m3.1.1.3" xref="S4.SS1.p3.3.m3.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><times id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1"></times><ci id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2">𝑇</ci><ci id="S4.SS1.p3.3.m3.1.1.3.cmml" xref="S4.SS1.p3.3.m3.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">Th</annotation></semantics></math><span id="S4.SS1.p3.7.4" class="ltx_text" style="font-size:90%;"> from 0% to 100% sampling the thresholds from a Sigmoid-spaced distribution to provide more data points for high and low confidence values.
The average precision (AP) was computed as the numerical integration of the precision-recall curve, as indicated in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p3.7.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib48" title="" class="ltx_ref">48</a><span id="S4.SS1.p3.7.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p3.7.7" class="ltx_text" style="font-size:90%;">.
We set a spatial tolerance </span><math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><mi mathsize="90%" id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><ci id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">S</annotation></semantics></math><span id="S4.SS1.p3.7.8" class="ltx_text" style="font-size:90%;"> of </span><math id="S4.SS1.p3.5.m5.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.SS1.p3.5.m5.1a"><mo mathsize="90%" id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><csymbol cd="latexml" id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.1c">\pm</annotation></semantics></math><span id="S4.SS1.p3.7.9" class="ltx_text" style="font-size:90%;">2° along the azimuth according to human auditory perception </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p3.7.10.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S4.SS1.p3.7.11.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p3.7.12" class="ltx_text" style="font-size:90%;">, the minimum audible angle (MAA), corresponding to </span><math id="S4.SS1.p3.6.m6.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.SS1.p3.6.m6.1a"><mo mathsize="90%" id="S4.SS1.p3.6.m6.1.1" xref="S4.SS1.p3.6.m6.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m6.1b"><csymbol cd="latexml" id="S4.SS1.p3.6.m6.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m6.1c">\pm</annotation></semantics></math><span id="S4.SS1.p3.7.13" class="ltx_text" style="font-size:90%;">89 pixels on the image plane. From the precision and recall rates, the F1 score is computed too. To independently evaluate the localization and the speaker detection subtasks, we define the average distance (aD) and the detection error (Det Err %) metrics. The former represents the average distance error between the active detections and the GT speaker locations. It is computed in pixels on the image frame and then converted to angle units leveraging the camera calibration data.
The detection error corresponds to the percentage of frames incorrectly classified as active or inactive when a threshold </span><math id="S4.SS1.p3.7.m7.1" class="ltx_Math" alttext="Th=0.5" display="inline"><semantics id="S4.SS1.p3.7.m7.1a"><mrow id="S4.SS1.p3.7.m7.1.1" xref="S4.SS1.p3.7.m7.1.1.cmml"><mrow id="S4.SS1.p3.7.m7.1.1.2" xref="S4.SS1.p3.7.m7.1.1.2.cmml"><mi mathsize="90%" id="S4.SS1.p3.7.m7.1.1.2.2" xref="S4.SS1.p3.7.m7.1.1.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p3.7.m7.1.1.2.1" xref="S4.SS1.p3.7.m7.1.1.2.1.cmml">​</mo><mi mathsize="90%" id="S4.SS1.p3.7.m7.1.1.2.3" xref="S4.SS1.p3.7.m7.1.1.2.3.cmml">h</mi></mrow><mo mathsize="90%" id="S4.SS1.p3.7.m7.1.1.1" xref="S4.SS1.p3.7.m7.1.1.1.cmml">=</mo><mn mathsize="90%" id="S4.SS1.p3.7.m7.1.1.3" xref="S4.SS1.p3.7.m7.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.7.m7.1b"><apply id="S4.SS1.p3.7.m7.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1"><eq id="S4.SS1.p3.7.m7.1.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1.1"></eq><apply id="S4.SS1.p3.7.m7.1.1.2.cmml" xref="S4.SS1.p3.7.m7.1.1.2"><times id="S4.SS1.p3.7.m7.1.1.2.1.cmml" xref="S4.SS1.p3.7.m7.1.1.2.1"></times><ci id="S4.SS1.p3.7.m7.1.1.2.2.cmml" xref="S4.SS1.p3.7.m7.1.1.2.2">𝑇</ci><ci id="S4.SS1.p3.7.m7.1.1.2.3.cmml" xref="S4.SS1.p3.7.m7.1.1.2.3">ℎ</ci></apply><cn type="float" id="S4.SS1.p3.7.m7.1.1.3.cmml" xref="S4.SS1.p3.7.m7.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.7.m7.1c">Th=0.5</annotation></semantics></math><span id="S4.SS1.p3.7.14" class="ltx_text" style="font-size:90%;"> is set.</span></p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Methods and Baselines</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">The proposed audio-visual method with multichannel audio is compared with two traditional audio-visual approaches for active speaker detection that employ single-channel audio and rely on face detection: Active Speaker in Context (ASC) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.SS2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p1.1.4" class="ltx_text" style="font-size:90%;"> and TalkNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.SS2.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p1.1.7" class="ltx_text" style="font-size:90%;">.
We also report the results achieved in our previous multichannel audio-only study </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S4.SS2.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p1.1.10" class="ltx_text" style="font-size:90%;">. The audio-only approach proposed in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S4.SS2.p1.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p1.1.13" class="ltx_text" style="font-size:90%;"> leverages a convolutional recurrent neural network (CRNN) with an architecture similar to the audio encoder proposed in the present paper. However, instead of the Conformer unit, the CRNN presents two bidirectional gated recurrent units (biGRUs).
Additionally, as a baseline system to highlight the advantages introduced by multichannel audio, we report the results achieved with a single-channel audio-only network (Mono). The input to the Mono baseline is a log mel spectrogram extracted from the central microphone of the array.</span></p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>ASDL results on the test set of the Tragic Talkers dataset and modality potential. The results for the proposed approach are achieved with audio-visual inputs and multichannel audio (AV-M). The table includes the results achieved by a single-channel audio-only network (A-S), two audio-visual systems that employ single-channel audio (AV-S), and an audio-only method that leverages multichannel audio (A-M).
</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.3.1.1" class="ltx_tr">
<th id="S4.T1.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></th>
<th id="S4.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Mod</span></th>
<th id="S4.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">DetErr</span></th>
<th id="S4.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">aD</span></th>
<th id="S4.T1.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">AP</span></th>
<th id="S4.T1.3.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.3.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.3.2.1" class="ltx_tr">
<td id="S4.T1.3.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.2.1.1.1" class="ltx_text" style="font-size:90%;">Mono</span></td>
<td id="S4.T1.3.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.2.1.2.1" class="ltx_text" style="font-size:90%;">A-S</span></td>
<td id="S4.T1.3.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2.7%</span></td>
<td id="S4.T1.3.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.2.1.4.1" class="ltx_text" style="font-size:90%;">210p, 4.7°</span></td>
<td id="S4.T1.3.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.3.2.1.5.1" class="ltx_text" style="font-size:90%;">10%</span></td>
<td id="S4.T1.3.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.2.1.6.1" class="ltx_text" style="font-size:90%;">30.0</span></td>
</tr>
<tr id="S4.T1.3.3.2" class="ltx_tr">
<td id="S4.T1.3.3.2.1" class="ltx_td ltx_align_center ltx_border_r">
<span id="S4.T1.3.3.2.1.1" class="ltx_text" style="font-size:90%;">ASC </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.3.3.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.T1.3.3.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.3.3.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.3.2.2.1" class="ltx_text" style="font-size:90%;">AV-S</span></td>
<td id="S4.T1.3.3.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.3.2.3.1" class="ltx_text" style="font-size:90%;">43%</span></td>
<td id="S4.T1.3.3.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.3.2.4.1" class="ltx_text" style="font-size:90%;">50p, 1.1°</span></td>
<td id="S4.T1.3.3.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.3.2.5.1" class="ltx_text" style="font-size:90%;">59%</span></td>
<td id="S4.T1.3.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T1.3.3.2.6.1" class="ltx_text" style="font-size:90%;">67.6</span></td>
</tr>
<tr id="S4.T1.3.4.3" class="ltx_tr">
<td id="S4.T1.3.4.3.1" class="ltx_td ltx_align_center ltx_border_r">
<span id="S4.T1.3.4.3.1.1" class="ltx_text" style="font-size:90%;">TalkNet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.3.4.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T1.3.4.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.3.4.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.4.3.2.1" class="ltx_text" style="font-size:90%;">AV-S</span></td>
<td id="S4.T1.3.4.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.4.3.3.1" class="ltx_text" style="font-size:90%;">14%</span></td>
<td id="S4.T1.3.4.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.4.3.4.1" class="ltx_text" style="font-size:90%;">35p, 0.79°</span></td>
<td id="S4.T1.3.4.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.4.3.5.1" class="ltx_text" style="font-size:90%;">82%</span></td>
<td id="S4.T1.3.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.3.6.1" class="ltx_text" style="font-size:90%;">84.9</span></td>
</tr>
<tr id="S4.T1.3.5.4" class="ltx_tr">
<td id="S4.T1.3.5.4.1" class="ltx_td ltx_align_center ltx_border_r">
<span id="S4.T1.3.5.4.1.1" class="ltx_text" style="font-size:90%;">CRNN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.3.5.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S4.T1.3.5.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.3.5.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.5.4.2.1" class="ltx_text" style="font-size:90%;">A-M</span></td>
<td id="S4.T1.3.5.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.5.4.3.1" class="ltx_text" style="font-size:90%;">3.2%</span></td>
<td id="S4.T1.3.5.4.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.5.4.4.1" class="ltx_text" style="font-size:90%;">39p, 0.88°</span></td>
<td id="S4.T1.3.5.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.3.5.4.5.1" class="ltx_text" style="font-size:90%;">87%</span></td>
<td id="S4.T1.3.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.4.6.1" class="ltx_text" style="font-size:90%;">90.9</span></td>
</tr>
<tr id="S4.T1.3.6.5" class="ltx_tr">
<td id="S4.T1.3.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.6.5.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Proposed</span></td>
<td id="S4.T1.3.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.6.5.2.1" class="ltx_text" style="font-size:90%;">AV-M</span></td>
<td id="S4.T1.3.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.6.5.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">2.7%</span></td>
<td id="S4.T1.3.6.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.6.5.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">32p, 0.72°</span></td>
<td id="S4.T1.3.6.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.6.5.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">92%</span></td>
<td id="S4.T1.3.6.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.3.6.5.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">94.9</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F4" class="ltx_figure">
<p id="S4.F4.1" class="ltx_p ltx_align_center"><span id="S4.F4.1.1" class="ltx_text" style="font-size:90%;"><img src="/html/2406.00495/assets/x4.png" id="S4.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="317" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparison of precision versus recall curves for different ASDL methods. The plot includes audio-visual methods with single-channel audio (AV-S), i.e., ASC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and TalkNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, the multichannel audio-only CRNN (M-S) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and the proposed audio-visual approach with multichannel audio (AV-M). The combination of precision and recall rates that achieves the highest F1 score is marked on each curve.</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text" style="font-size:90%;">The experimental results are presented in Table </span><a href="#S4.T1" title="Table 1 ‣ 4.3 Results ‣ 4 Experiments ‣ Audio-visual talker localization in video for spatial sound reproduction" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS3.p1.1.2" class="ltx_text" style="font-size:90%;">.
The audio-visual multichannel (AV-M) method significantly improves the performance of the audio-only systems across nearly all metrics.</span></p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text" style="font-size:90%;">In the Mono baseline, the detection subtask is accomplished with a detection error of only 2.7%. However, the position of the speaker is not accurately predicted due to the absence of spatial cues. To minimize the error, the model locates the speaker in the central area of the frame.</span></p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text" style="font-size:90%;">Conventional active speaker detectors, such as ASC </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.SS3.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p3.1.4" class="ltx_text" style="font-size:90%;"> and TalkNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p3.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.SS3.p3.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p3.1.7" class="ltx_text" style="font-size:90%;">, employ the audio modality only to classify the pre-extracted faces. The localisation subtask is performed by the visual face detector, yielding high spatial accuracy. However, since the average distance is achieved as the average of the predicted positions, the final average includes true and false positive predictions (false positive predictions happen when the detector classifies the silent actor as active).
False positive detections are mainly present in the detections of ASC </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p3.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.SS3.p3.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p3.1.10" class="ltx_text" style="font-size:90%;">, penalizing its spatial accuracy as well as its detection.
In the audio-visual methods with single-channel audio, the horizontal coordinate of the center of the bounding box is used as prediction, while the ground truth used for evaluation refers to the actual mouth position of the speaker. Therefore, the aD achieved is slightly overestimated due to the offset between the two representations. For example, when the speaker is captured in profile and his/her mouth is closer to the edge of the bounding box.
Additionally, the AV-S methods fail when the face of the active speaker is not detected by the face detector. This causes a higher detection error and, consequently, a lower recall rate, as shown in Figure </span><a href="#S4.F4" title="Figure 4 ‣ 4.3 Results ‣ 4 Experiments ‣ Audio-visual talker localization in video for spatial sound reproduction" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S4.SS3.p3.1.11" class="ltx_text" style="font-size:90%;">.
At its best precision-recall pair TalkNet presents a recall rate of 79.7%, while ASC only of 58.5%. As a consequence, their overall F1 scores are affected too.
In contrast, the multichannel audio method achieves a lower detection error as speech activity can be sensed even when the speaker is visually occluded.
In fact, the double-digit detection errors of the AV-S methods are reduced to 3.2% with the A-M approach. Figure </span><a href="#S4.F4" title="Figure 4 ‣ 4.3 Results ‣ 4 Experiments ‣ Audio-visual talker localization in video for spatial sound reproduction" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S4.SS3.p3.1.12" class="ltx_text" style="font-size:90%;"> shows how the gap in recall rate generated by TalkNet is halved with the multichannel audio method (90.6% recall rate).
This produces an AP and F1 score higher than the AV-S systems.</span></p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text" style="font-size:90%;">When multichannel audio is partnered with vision, beneficial effects involve both detection and localization accuracy.
The detection error decreases by 0.5 percentage points to 2.7%, while the aD outperforms even the audio-visual TalkNet model.
The F1 score is 4 percentage points higher than the A-M approach and 10 points greater than TalkNet.</span></p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p"><span id="S4.SS3.p5.1.1" class="ltx_text" style="font-size:90%;">The residual error in the F1 score for the proposed AV-M method is only 5%.
In the future, this error might be further narrowed by implementing visually guided predictions post-processing </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib49" title="" class="ltx_ref">49</a><span id="S4.SS3.p5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p5.1.4" class="ltx_text" style="font-size:90%;">. For example, pose detection could rectify spatial predictions using the mouth key-point coordinates. This would further improve the localization accuracy and consequently increase the number of true positive detections that fall within the spatial tolerance.
Another aspect to consider is the consistency between training and testing labels. The GT labels employed to train the model correspond to center of the face bounding boxes, whereas the evaluation is based on GT mouth positions. This disparity introduces a subtle domain bias between the training and inference phases, potentially resulting in residual errors regardless of the quality of the model.</span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:90%;">This paper proposes an audio-visual approach for active speaker detection and localization that leverages multichannel audio on the Tragic Talkers dataset. The approach extracts audio and visual embedding leveraging audio and visual encoders. Then, the embeddings are concatenated and processed by an AV-Conformer.
The proposed method outperforms conventional audio-visual approaches for active speaker detection that rely on visual face detection as well as our previous audio-only multichannel work. This highlights the importance of the input modalities.
Active speaker detection and localization can be employed for the automatic extraction of speaker positional metadata useful in immersive audio productions.</span></p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgments</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text" style="font-size:90%;">This research was funded by EPSRC-BBC Prosperity Partnership ‘AI4ME: Future personalised object-based media experiences delivered at scale anywhere’ (EP/V038087/1).
For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) license to any Author Accepted Manuscript version arising.
Data supporting this study are available from </span><a target="_blank" href="https://cvssp.org/data/TragicTalkers" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://cvssp.org/data/TragicTalkers</a><span id="S6.p1.1.2" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Philip Coleman, Andreas Franck, Jon Francombe, Qingju Liu, Teofilo de Campos, Richard J. Hughes, Dylan Menzies, Marcos F. Simón Gálvez, Yan Tang, James Woodcock, Philip J. B. Jackson, Frank Melchior, Chris Pike, Filippo Maria Fazi, Trevor J. Cox, and Adrian Hilton,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">“An audio-visual system for object-based audio: From recording to listening,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, vol. 20, no. 8, pp. 1919–1931, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Chris Pike, Richard Taylor, Tom Parnell, and Frank Melchior,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">“Object-based 3d audio production for virtual reality using the audio definition model,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of the Audio Engineering Society</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, september 2016.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Mohd Azri Mohd Izhar, Marco Volino, Adrian Hilton, and Philip J. B. Jackson,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">“Tracking sound sources for object-based spatial audio in 3D audio-visual production,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Forum Acusticum</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 2051–2058.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Florian Schweiger, Chris Pike, Tom Nixon, Matt Firth, Bruce Weir, Paul Golds, Marco Volino, Craig Cieciura, Mohd Izhar, Nick Graham-Rack, Philip J. B. Jackson, and Alex Ang,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">“Tools for 6-DoF immersive audio-visual content capture and production,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Broadcasting Convention</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Thomas Strybel and Ken Fujimoto,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">“Minimum audible angles in the horizontal and vertical planes: Effects of stimulus onset asynchrony and burst duration,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of the Acoustical Society of America</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, vol. 108 6, pp. 3092–5, 2000.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Davide Berghi, Marco Volino, and Philip J. B. Jackson,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">“Tragic Talkers: A Shakespearean sound- and light-field dataset for audio-visual machine learning research,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Visual Media Production</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Davide Berghi and Philip J. B. Jackson,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">“Leveraging visual supervision for array-based active speaker detection and localization,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, vol. 32, pp. 984–995, 2024.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Davide Berghi, Adrian Hilton, and Philip J. B. Jackson,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">“Visually supervised speaker detection and localization via microphone array,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Workshop on Multimedia Signal Processing</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Radhika Marvin, Andrew Gallagher, Liat Kaver, Sharadh Ramaswamy, Arkadiusz Stopczynski, Cordelia Schmid, Zhonghua Xi, and Caroline Pantofaru,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">“AVA active speaker: An audio-visual dataset for active speaker detection,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 4492–4496.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Yuanhang Zhang, Susan Liang, Shuang Yang, Xiao Liu, Zhongqin Wu, Shiguang Shan, and Xilin Chen,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">“UniCon: Unified context network for robust active speaker detection,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Multimedia</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Ruijie Tao, Zexu Pan, Rohan Kumar Das, Xinyuan Qian, Mike Zheng Shou, and Haizhou Li,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">“Is someone speaking? Exploring long-term temporal features for audio-visual active speaker detection,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM International Conference on Multimedia</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2021, p. 3927–3935.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Juan León Alcázar, Fabian Caba, Long Mai, Federico Perazzi, Joon-Young Lee, Pablo Arbeláez, and Bernard Ghanem,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">“Active speakers in context,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 12462–12471.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Punarjay Chakravarty, Jeroen Zegers, Tinne Tuytelaars, and Hugo Van hamme,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">“Active speaker detection with audio-visual co-training,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Multimodal Interaction</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2016, p. 312–316.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Ross Cutler and Larry Davis,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">“Look who’s talking: Speaker detection using video and audio correlation,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Multimedia and Expo</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2000, pp. 1589–1592.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Fasih Haider and Samer Al Moubayed,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">“Towards speaker detection using lips movements for human-machine multiparty dialogue,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Fonetik</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Punarjay Chakravarty, Sayeh Mirzaei, Tinne Tuytelaars, and Hugo Van hamme,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">“Who’s speaking? Audio-supervised classification of active speakers in video,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Multimodal Interaction</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2015, p. 87–90.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Ken Hoover, Sourish Chaudhuri, Caroline Pantofaru, Ian Sturdy, and Malcolm Slaney,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">“Using audio-visual information to understand speaker activity: Tracking active speakers on and off screen,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Confonference on Acoustics, Speech and Signal Processing</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 6558–6562.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew Zisserman,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">“VoxCeleb: Large-scale speaker verification in the wild,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Speech &amp; Language</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, vol. 60, pp. 101027, 2020.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Joon Son Chung and Andrew Zisserman,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">“Lip reading in the wild,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Asian Conference on Computer Vision</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">“ASR is all you need: Cross-modal distillation for lip reading,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 2143–2147.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Soo-Whan Chung, Joon Son Chung, and Hong-Goo Kang,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">“Perfect match: Improved cross-modal embeddings for audio-visual synchronisation,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 3965–3969.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">“The conversation: Deep audio-visual speech enhancement,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">INTERSPEECH</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Joon Son Chung,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">“Naver at ActivityNet Challenge 2019 - Task B Active speaker detection (AVA),”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, vol. abs/1906.10555, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Yuan-Hang Zhang, Jing-Yun Xiao, Shuang Yang, and Shiguang Shan,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">“Multi-task learning for audio-visual active speaker detection,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Techical Report AVA-ActiveSpeaker Challenge</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">“LRS3-TED: a large-scale dataset for visual speech recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, vol. abs/1809.00496, 2018.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Joon Son Chung, Arsha Nagrani, and Andrew Zisserman,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">“VoxCeleb2: Deep speaker recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">INTERSPEECH</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Juan León Alcázar, Moritz Cordes, Chen Zhao, and Bernard Ghanem,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">“End-to-end active speaker detection,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2022, p. 126–143.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Xinyuan Qian, Maulik Madhavi, Zexu Pan, Jiadong Wang, and Haizhou Li,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">“Multi-target DoA estimation with an audio-visual fusion mechanism,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2021, pp. 4280–4284.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Xinyuan Qian, Zhengdong Wang, Jiadong Wang, Guohui Guan, and Haizhou Li,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">“Audio-visual cross-attention network for robotic speaker tracking,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, vol. 31, pp. 550–562, 2023.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Yulin Wu, Ruimin Hu, Xiaochen Wang, and Shanfa Ke,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">“Multi-speaker DoA estimation using audio and visual modality,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural Processing Letters</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, vol. 55, no. 7, pp. 8887–8901, 2023.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">“Attention is all you need,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Neural Information Processing Systems</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2017, p. 6000–6010.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Jinzheng Zhao, Peipei Wu, Shidrokh Goudarzi, Xubo Liu, Jianyuan Sun, Yong Xu, and Wenwu Wang,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">“Visually assisted self-supervised audio speaker localization and tracking,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Signal Processing Conference</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2022, pp. 787–791.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Jinzheng Zhao, Yong Xu, Xinyuan Qian, and Wenwu Wang,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">“Audio visual speaker localization from egocentric views,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, vol. abs/2309.16308, 2023.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Jacob Donley, Vladimir Tourbabin, Jung-Suk Lee, Mark Broyles, Hao Jiang, Jie Shen, Maja Pantic, Vamsi Krishna Ithapu, and Ravish Mehra,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">“Easycom: An augmented reality dataset to support algorithms for easy communication in noisy environments,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, vol. 2107.04174, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Hao Jiang, Calvin Murdock, and Vamsi Krishna Ithapu,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">“Egocentric deep multi-channel audio-visual active speaker localization,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, pp. 10534–10542, 2022.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Ilya Gurvich, Ido Leichter, Dharmendar Reddy Palle, Yossi Asher, Alon Vinnikov, Igor Abramovski, Vishak Gopal, Ross Cutler, and Eyal Krupka,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">“A real-time active speaker detection system integrating an audio-visual signal with a spatial querying mechanism,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, vol. abs/2309.08295, 2023.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Davide Berghi, Peipei Wu, Jinzheng Zhao, Wenwu Wang, and Philip J. B. Jackson,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">“Fusion of audio and visual embeddings for sound event localization and detection,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">“Conformer: Convolution-augmented transformer for speech recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, vol. abs/2005.08100, 2020.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Qing Wang, Jun Du, Hua-Xin Wu, Jia Pan, Feng Ma, and Chin-Hui Lee,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">“A four-stage data augmentation approach to resnet-conformer based acoustic modeling for sound event localization and detection,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, vol. 31, pp. 1251–1264, 2023.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">“Deep residual learning for image recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2016, pp. 770–778.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Sergey Ioffe and Christian Szegedy,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">“Batch normalization: Accelerating deep network training by reducing internal covariate shift,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2015, vol. 37, pp. 448–456.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">“ImageNet: A large-scale hierarchical image database,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:90%;">, pp. 248–255, 2009.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Charles Knapp and G. Clifford Carter,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">“The generalized correlation method for estimation of time delay,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Acoustics, Speech, and Signal Processing</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, vol. 24, no. 4, pp. 320–327, 1976.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Yin Cao, Qiuqiang Kong, Turab Iqbal, Fengyan An, Wenwu Wang, and Mark D. Plumbley,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">“Polyphonic sound event detection and localization using a two-stage strategy,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Detection and Classification of Acoustic Scenes and Events Workshop</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Davide Berghi and Philip J. B. Jackson,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">“Audio inputs for active speaker detection and localization via microphone array,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">“You only look once: Unified, real-time object detection,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2016, pp. 779–788.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Thi Ngoc Tho Nguyen, Karn N. Watcharasupat, Ngoc Khanh Nguyen, Douglas L. Jones, and Woon-Seng Gan,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">“SALSA: Spatial cue-augmented log-spectrogram features for polyphonic sound event localization and detection,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/ACM Trans. on Audio, Speech, and Language Processing</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, vol. 30, pp. 1749–1762, 2022.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">“The PASCAL visual object classes challenge: A retrospective,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib48.4.2" class="ltx_text" style="font-size:90%;">, vol. 111, no. 1, pp. 98–136, 2015.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Qing Wang, Ya Jiang, Shi Cheng, Maocheng Hu, Zhaoxu Nian, Pengfei Hu, Zeyan Liu, Yuxuan Dong, Mingqi Cai, Jun Du, and Chin-Hui Lee,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">“The NERC-SLIP system for sound event localization and detection of DCASE2023 challenge,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Techical Report of DCASE Challenge</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="Davide Berghi and Philip J. B. Jackson"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="Proceedings of the 27th International Conference on Digital Audio Effects (DAFx24)"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="Audio-visual talker localization in video for spatial sound reproduction"></div>

<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.00494" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.00495" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.00495">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.00495" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.00497" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Jul  6 01:46:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
