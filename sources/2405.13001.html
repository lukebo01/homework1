<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.13001] Large Language Models for Education: A Survey</title><meta property="og:description" content="Artificial intelligence (AI) has a profound impact on traditional education. In recent years, large language models (LLMs) have been increasingly used in various applications such as natural language processing, comput…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Large Language Models for Education: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Large Language Models for Education: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.13001">

<!--Generated on Wed Jun  5 13:35:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\cortext</span>
<p id="p1.2" class="ltx_p">[cor1]Corresponding author
<span id="p1.2.1" class="ltx_ERROR undefined">\cormark</span>[1]</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\cormark</span>
<p id="p2.2" class="ltx_p">[1]</p>
</div>
<h1 class="ltx_title ltx_title_document">Large Language Models for Education: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hanyi Xu
</span><span class="ltx_author_notes">xhyzhiyi@gmail.com
<span class="ltx_contact ltx_role_address">College of Cyber Security, Jinan University, Guangzhou 510632, China
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wensheng Gan
</span><span class="ltx_author_notes">wsgan001@gmail.com</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhenlian Qi
</span><span class="ltx_author_notes">qzlhit@gmail.com
<span class="ltx_contact ltx_role_address">School of Information Engineering, Guangdong Eco-Engineering Polytechnic, Guangzhou 510520, China
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiayang Wu
</span><span class="ltx_author_notes">csjywu1@jnu.edu.cn</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Philip S. Yu
</span><span class="ltx_author_notes">psyu@uic.edu
<span class="ltx_contact ltx_role_address">Department of Computer Science, University of Illinois Chicago, Chicago, USA
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Artificial intelligence (AI) has a profound impact on traditional education. In recent years, large language models (LLMs) have been increasingly used in various applications such as natural language processing, computer vision, speech recognition, and autonomous driving. LLMs have also been applied in many fields, including recommendation, finance, government, education, legal affairs, and finance. As powerful auxiliary tools, LLMs incorporate various technologies such as deep learning, pre-training, fine-tuning, and reinforcement learning. The use of LLMs for smart education (LLMEdu) has been a significant strategic direction for countries worldwide. While LLMs have shown great promise in improving teaching quality, changing education models, and modifying teacher roles, the technologies are still facing several challenges. In this paper, we conduct a systematic review of LLMEdu, focusing on current technologies, challenges, and future developments. We first summarize the current state of LLMEdu and then introduce the characteristics of LLMs and education, as well as the benefits of integrating LLMs into education. We also review the process of integrating LLMs into the education industry, as well as the introduction of related technologies. Finally, we discuss the challenges and problems faced by LLMEdu, as well as prospects for future optimization of LLMEdu.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
artificial intelligence
<br class="ltx_break">smart education 
<br class="ltx_break">LLMs 
<br class="ltx_break">applications 
<br class="ltx_break">challenges 
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Artificial intelligence (AI) has developed rapidly in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>, <a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite>, thanks to the continuous improvements in Web 3.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, Internet of Behaviors (IoB) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>, data mining <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, deep learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite>, and language processing technologies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. LLMs have shown excellent performance in various industries with the optimization of pre-training models and the continuous adjustment of related technologies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite>. LLM is mainly based on many AI technologies, e.g., natural language processing (NLP), and was used to understand and generate massive texts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. They perform self-supervised learning on a large-scale corpus to obtain the statistical laws of language <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and then convert it into logical natural language text. Its basic framework is shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. LLMs have demonstrated strong versatility and logical reasoning capabilities, leading to their widespread model-as-a-service (MaaS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> in various industries, including finance, education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, law <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, robotics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite>, and government affairs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>. Creating a scenario-based user experience is a key advantage for most digital companies, and it also happens to be a development need for LLM.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2405.13001/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="147" height="80" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Framework of LLMs.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The concept of education has been around for centuries, dating back to the theory of biological origins. In primitive societies, education was limited to the use of primary production tools, whereas ancient societies relied on oral transmission and practice to pass knowledge down to future generations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. With the development of science and technology in modern society, education and AI have become inseparable <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, including intelligent teacher assistants, voice assistants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>, AI writing creation platforms, etc. The fourth industrial revolution, represented by the intelligent revolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, can bring the education industry to a new level with the help of LLMs. Education is essentially about knowledge transfer, instant feedback, and emotional interaction. LLMs mainly enhance the “immediate feedback" process in education. They have the potential to revolutionize the education industry by providing personalized, adaptive learning experiences for students. By infusing knowledge into their models, LLMs can gradually build a deep understanding of the world, surpassing human learning in some aspects. They can generate high-quality text content, comprehend natural language, extract information, and answer questions across various fields <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>. LLMs can also do complex mathematical reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite>, which helps the education sector show that they are good at self-supervision, intelligent adaptive teaching, and multi-modal interaction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. With their ability to adapt the individual students’ needs and learning styles, LLMs can provide a more effective and engaging learning experience.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text ltx_font_bold">Research gaps</span>: There are already many educators and researchers who have shown a lot of thinking about AI in education. Examples are as follows: Some research has been conducted on the paradigm shift in AI in education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> and on the impact of AI in management, teaching, and learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Some studies explain AI in education and show how they work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>. Due to the rapid iteration and update of AI, many new educational AI technologies have been spawned, but there is a lack of summary and analysis of emerging technological means. LLMs, as one of these technologies, have significantly advanced AI development to a new stage. LLMs are the latest technological means to support intelligent education. The integration of education and LLMs particularly highlights the development and application characteristics of LLMs. There has been one brief review of LLMs for education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, while many characteristics of LLMEdu and key technologies are not discussed in detail.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text ltx_font_bold">Contributions</span>: To examine the potential of LLMEdu and promote its development, this paper provides an in-depth analysis of the development process and technical structure of LLMEdu and forms a comprehensive summary. This review aims to help readers gain a deeper understanding of LLMEdu and encourages us to invent and consider LLMEdu applications. The specific contributions are as follows:</p>
</div>
<div id="S1.p5" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We take a closer look at the connection between LLMs and education, aiming to achieve smart education.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We demonstrate the development process of LLMEdu through the process of applying LLMs to education and the key technologies of LLMs.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We review the implementation of LLMEdu from the perspective of LLMs empowering education, focusing on exploring the development potential of LLMEdu.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We highlight the problems and challenges existing in LLMEdu in detail, aiming to trigger some insight, critical thinking, and exploration.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_bold">Roadmap</span>: In Section <a href="#S2" title="2 Characteristics of LLM in Education ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we briefly introduce the characteristics of LLMs and the education industry, as well as the characteristics of LLMs integrated into education. In Section <a href="#S3" title="3 How to Gradually Integrate LLMs into Education ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we conduct an in-depth analysis of the process of applying LLMs to education. In Section <a href="#S4" title="4 Key Technologies for LLMEdu ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we explain the key technologies related to LLMs. In Section <a href="#S5" title="5 Implementation of LLMEdu ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we provide the implementation of LLMEdu from the perspective of empowering education with LLMs. In Section <a href="#S6" title="6 Issues and Challenges ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we highlight some of the main issues and challenges in LLMEdu. Finally, in Section <a href="#S7" title="7 Conclusion ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we summarize LLMEdu and propose expectations for the development of future LLMs. Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> describes some basic symbols in this article.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of symbols and their explanations</figcaption>
<table id="S1.T1.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S1.T1.3.1" class="ltx_tr">
<td id="S1.T1.3.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Symbol</span></td>
<td id="S1.T1.3.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.3.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Definition</span></td>
</tr>
<tr id="S1.T1.3.2" class="ltx_tr">
<td id="S1.T1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.3.2.1.1" class="ltx_text" style="font-size:90%;">AI</span></td>
<td id="S1.T1.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.3.2.2.1" class="ltx_text" style="font-size:90%;">Artificial Intelligence</span></td>
</tr>
<tr id="S1.T1.3.3" class="ltx_tr">
<td id="S1.T1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.3.3.1.1" class="ltx_text" style="font-size:90%;">AIGC</span></td>
<td id="S1.T1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.3.3.2.1" class="ltx_text" style="font-size:90%;">AI-Generated Content</span></td>
</tr>
<tr id="S1.T1.3.4" class="ltx_tr">
<td id="S1.T1.3.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.3.4.1.1" class="ltx_text" style="font-size:90%;">ChatGPT</span></td>
<td id="S1.T1.3.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.3.4.2.1" class="ltx_text" style="font-size:90%;">Chat Generative Pre-Training Transformer</span></td>
</tr>
<tr id="S1.T1.3.5" class="ltx_tr">
<td id="S1.T1.3.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.3.5.1.1" class="ltx_text" style="font-size:90%;">CV</span></td>
<td id="S1.T1.3.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.3.5.2.1" class="ltx_text" style="font-size:90%;">Computer Vision</span></td>
</tr>
<tr id="S1.T1.3.6" class="ltx_tr">
<td id="S1.T1.3.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.3.6.1.1" class="ltx_text" style="font-size:90%;">DNNs</span></td>
<td id="S1.T1.3.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.3.6.2.1" class="ltx_text" style="font-size:90%;">Deep Neural Networks</span></td>
</tr>
<tr id="S1.T1.3.7" class="ltx_tr">
<td id="S1.T1.3.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.3.7.1.1" class="ltx_text" style="font-size:90%;">GPT</span></td>
<td id="S1.T1.3.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.3.7.2.1" class="ltx_text" style="font-size:90%;">Generative Pre-trained Transformer</span></td>
</tr>
<tr id="S1.T1.3.8" class="ltx_tr">
<td id="S1.T1.3.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.3.8.1.1" class="ltx_text" style="font-size:90%;">HFRL</span></td>
<td id="S1.T1.3.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.3.8.2.1" class="ltx_text" style="font-size:90%;">Human Feedback Reinforcement Learning</span></td>
</tr>
<tr id="S1.T1.3.9" class="ltx_tr">
<td id="S1.T1.3.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.3.9.1.1" class="ltx_text" style="font-size:90%;">LLMEdu</span></td>
<td id="S1.T1.3.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.3.9.2.1" class="ltx_text" style="font-size:90%;">Large Language Models for Education</span></td>
</tr>
<tr id="S1.T1.3.10" class="ltx_tr">
<td id="S1.T1.3.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.3.10.1.1" class="ltx_text" style="font-size:90%;">LLMs</span></td>
<td id="S1.T1.3.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.3.10.2.1" class="ltx_text" style="font-size:90%;">Large Language Models</span></td>
</tr>
<tr id="S1.T1.3.11" class="ltx_tr">
<td id="S1.T1.3.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.3.11.1.1" class="ltx_text" style="font-size:90%;">LMs</span></td>
<td id="S1.T1.3.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S1.T1.3.11.2.1" class="ltx_text" style="font-size:90%;">Language Models</span></td>
</tr>
<tr id="S1.T1.3.12" class="ltx_tr">
<td id="S1.T1.3.12.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.3.12.1.1" class="ltx_text" style="font-size:90%;">NLP</span></td>
<td id="S1.T1.3.12.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S1.T1.3.12.2.1" class="ltx_text" style="font-size:90%;">Natural Language Processing</span></td>
</tr>
</table>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Characteristics of LLM in Education</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we discuss the key characteristics of LLMs, the key characteristics of education, the limitations of traditional education, and the combinations between LLMs and education, as depicted in Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Characteristics of LLM in Education ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2405.13001/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="186" height="100" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The characteristics of LLMEdu.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Characteristics of LLMs</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_bold">Large-scale</span>. The term
“large" in LLMs can be interpreted in two ways. Firstly, LLMs possess an enormous number of parameters, with the parameter count increasing exponentially from billions to trillions in just a few years. For instance, Google’s BERT had 300 million parameters in 2018, GPT-2 had 1.5 billion parameters in 2019, and GPT-3 had 175 billion parameters in 2021 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>. In 2022, the Switch Transformer reached an impressive 1.6 trillion parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>. Furthermore, LLMs are trained on vast amounts of data from diverse sources, including the web, academic literature, and conversations. This large-scale corpus of data enables the models to learn and represent complex patterns and relationships in language, leading to improved performance in various NLP tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">General-purpose</span>. LLMs have a wide range of applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>. In addition to excelling in specific domains, they are adept at handling various types of tasks, including NLP, CV, speech recognition, and even cross-modal tasks. In other words, LLMs possess powerful generalization capabilities, and achieving such capabilities requires training on massive amounts of data.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Pre-training and fine-tuning</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite>. The core of the model training process lies in the use of pre-training followed by fine-tuning. Initially, pre-training is performed on a large-scale unlabeled text corpus to acquire the model’s basic language knowledge. Subsequently, fine-tuning is conducted on specific tasks in a particular domain to better understand and generate language specific to that domain, such as legal, educational, or medical texts.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p"><span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_bold">Emergent ability: unpredictability</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>. The emergent ability of LLMs refers to their capacity to generate coherent and logically consistent text without explicit human intervention, as they have learned from their training process. When the amount of data reaches a sufficiently large scale, the model’s learning and feedback capabilities can experience a substantial increase, resulting in improved performance.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p"><span id="S2.SS1.p5.1.1" class="ltx_text ltx_font_bold">Fragmentation</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>. The current AI landscape is characterized by diverse business scenarios across various industries, resulting in fragmented and diversified AI demands. The development process of AI models involves several stages, including development, hyperparameter tuning, optimization, and iterative deployment for eventual application. Each stage requires significant investment, and in high-cost situations, catering to customized market demands can be challenging.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p"><span id="S2.SS1.p6.1.1" class="ltx_text ltx_font_bold">Potential for breaking accuracy limitations</span>. The development of deep learning has taken a long time. The improvement in accuracy through architectural changes appears to have reached a bottleneck as neural network design techniques have matured and converged. However, LLM development has shown that increasing the scale of both the model and the data can help break through accuracy limitations. Research experiments have consistently demonstrated that scaling up the model and data leads to improved model accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>.
<span id="S2.SS1.p6.1.2" class="ltx_text ltx_font_bold">High complexity and investment costs</span>. LLMs are becoming increasingly complex, with single-step computation time growing by more than 10 times <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. For high-traffic businesses, a training experiment that used to take a few hours now takes several days, with the expectation that tests will remain within a one-day timeframe as a basic requirement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>. Moreover, training a general-purpose large model is expensive, and if subsequent optimization, updates, and deployment are included, it will cost even more. For example, the core infrastructure of ChatGPT, the Azure AI, required an investment of nearly $1 billion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>. Moreover, ChatGPT has high requirements for the number of GPU chips used for data processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Characteristics of education</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">According to its definition, education is a deliberate and conscious social practice that aims to nurture individuals. Its fundamental characteristic is its process-oriented nature, indicating that education exists and evolves through a series of steps. With a focus on individuals, education ultimately aims to facilitate their holistic and enduring growth. Education encompasses knowledge transmission, immediate feedback, and emotional interaction. Error correction, knowledge reinforcement, and rapid training consolidation are some parts of educational behavior. Furthermore, the education system is highly intricate, marked by the distinctiveness of its subjects, diverse requirements, and intricate interactions.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Educational development process</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p"><span id="S2.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">Low entry barriers.</span> On one hand, the accessibility of starting an educational institution is relatively easy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, resulting in lower operating and investment costs for both teachers and institutions. However, this has also led to a disparity in teacher qualifications, contributing to issues such as disorder in the education and training industry, misleading advertisements, exaggerated titles for teachers, and ineffective offline one-on-one teaching. These have subsequently led to an increase in complaints. On the other hand, there has been a reduction in barriers to education for learners, leading to greater equality of educational opportunities across different regions and a stronger emphasis on the right to education.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p"><span id="S2.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Large capacity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>.</span> The education industry encompasses a significant number of students and teachers, making it crucial to consider the implications of a large population. Moreover, there exists a diverse array of educational settings, including public schools as well as numerous private educational institutions. There is an abundance of educational materials available, and the advent of the internet has made access to educational resources easier. This development has transcended the confines of traditional textbook-based teaching, breaking down information barriers and expanding the horizons of education.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<p id="S2.SS2.SSS1.p3.1" class="ltx_p"><span id="S2.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Well-developed system.</span> The expansion of education has been propelled by economic development <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, leading to a surge in investment in the education sector. This growth encompasses a wide range of educational institutions at different levels. Moreover, the education system encompasses diverse forms of education, such as social life education, family education, and school education. It also encompasses a variety of disciplines, including mathematics, languages, and physical education.</p>
</div>
<div id="S2.SS2.SSS1.p4" class="ltx_para">
<p id="S2.SS2.SSS1.p4.1" class="ltx_p"><span id="S2.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Rise of online education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. </span> Since the late 1990s, emerging technologies have made significant inroads into the education industry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. This transformation has propelled education through various stages, including traditional education, digital education, internet-based education, mobile-based education, and intelligent education. The advancement of information technology has played a pivotal role in facilitating education development by overcoming time and space constraints, making knowledge acquisition more convenient and rapid.</p>
</div>
<div id="S2.SS2.SSS1.p5" class="ltx_para">
<p id="S2.SS2.SSS1.p5.1" class="ltx_p"><span id="S2.SS2.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Education at a younger age.</span> The development of the internet has dismantled barriers to education, resulting in heightened parental concerns and an increased focus on early education. Under the influence of globalization, the significance of early education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>, particularly in language and logic development, has been recognized. In conjunction with the surge of online education, early childhood education has become more readily available. A wide range of tutoring classes and early learning programs have become commonplace.</p>
</div>
<div id="S2.SS2.SSS1.p6" class="ltx_para">
<p id="S2.SS2.SSS1.p6.1" class="ltx_p"><span id="S2.SS2.SSS1.p6.1.1" class="ltx_text ltx_font_bold">Intelligent, precise, and personalized education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</span> With the rapid advancement of AI, technology has significantly enhanced production methods and raised people’s living standards. As a result, society’s demand for education has escalated, leading to a more targeted approach to talent development. Education is currently transforming the integration and innovation of “AI + education" in smart education.</p>
</div>
<div id="S2.SS2.SSS1.p7" class="ltx_para">
<p id="S2.SS2.SSS1.p7.1" class="ltx_p">Although education has integrated AI to a significant extent, the nature of human education and machine education fundamentally differs in a two-tier manner. These two forms of education vary in their sequence: human education primarily focuses on shaping values, followed by systematic knowledge acquisition, and ultimately engaging in real-world experiences to foster learning. In contrast, machine education begins by processing vast amounts of data, subsequently discerning between right and wrong (learning values), incorporating human feedback, and ultimately attaining practicality. When it comes to learning, the most notable distinction between humans and machines lies in the limited energy humans possess to acquire knowledge within a fixed period, whereas machines have a relatively unlimited learning capacity. Embracing AI, formulating education strategies that align with the current era, and achieving a comprehensive digital transformation of education are the central points of contemporary educational development.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Impact on teachers</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p"><span id="S2.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_bold">Instructional method’s development</span>. Digital education provides a wider range of teaching methods and tools <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. It requires teachers to adapt and become proficient in utilizing these innovative approaches and technologies. This includes leveraging online learning platforms, educational applications, and virtual classrooms to effectively impart knowledge and engage with students. To cater to student’s diverse learning needs, teachers must acquire familiarity with and expertise in using these technologies.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p"><span id="S2.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Personalized and self-directed learning support</span>. Digital education has the potential to better support personalized and self-directed learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Teachers can leverage technology to gain insights into student’s learning styles, interests, and needs. They also provide tailored instructional content and learning plans. This shift in education will see teachers adopt more of a guide and mentor role. They encourage students to take an active role in their learning and self-development.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p"><span id="S2.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Data-driven instructional decision-making</span>. Digital education yields a wealth of learning data, including student’s performance, interests, and progress <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>. Teachers can leverage this data to make informed instructional decisions and provide personalized guidance. By analyzing student’s data, teachers can identify areas of difficulty and weakness and offer targeted support and feedback to help students overcome these challenges and improve their learning outcomes.</p>
</div>
<div id="S2.SS2.SSS2.p4" class="ltx_para">
<p id="S2.SS2.SSS2.p4.1" class="ltx_p"><span id="S2.SS2.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Collaboration and cross-border teaching</span>. Digital education has the power to break down geographical barriers, enabling teachers to engage in cross-border teaching and collaboration with students from all over the world. This allows for the sharing of instructional resources, experiences, and best practices among educators, promoting professional development and collaboration within the teaching community.</p>
</div>
<div id="S2.SS2.SSS2.p5" class="ltx_para">
<p id="S2.SS2.SSS2.p5.1" class="ltx_p"><span id="S2.SS2.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Cultivating 21st-century skills</span>. In the digital age, it’s essential for students to develop skills such as creative thinking, digital literacy, collaboration, and problem-solving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. Teachers play a vital role in guiding students to cultivate these skills and providing relevant educational support and guidance. By exploring and applying new technologies together with students, teachers can foster student’s innovation and adaptability, preparing them for success in an ever-changing digital landscape.</p>
</div>
<div id="S2.SS2.SSS2.p6" class="ltx_para">
<p id="S2.SS2.SSS2.p6.1" class="ltx_p">Teachers are indispensable in the digital transformation of education, as they play a multifaceted role in shaping student’s academic, emotional, and social development. While technology can provide access to vast knowledge and resources, it cannot replace the personalized guidance, emotional support, and values-based education that teachers offer. The expertise, interpersonal relationships, and educational wisdom of teachers are still essential elements in the digital transformation of education, ensuring that students receive a well-rounded education that prepares them for success in the 21st century.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Educational challenges</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p"><span id="S2.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_bold">Personalized learning needs. </span> In contemporary education, students have diverse learning needs, styles, interests, and aspirations. The traditional one-size-fits-all approach may not cater to each student’s unique requirements, and personalized learning is essential to addressing these differences effectively. Therefore, implementing personalized learning is a significant challenge that educators and administrators must address to ensure that every student receives an education tailored to their individual needs and abilities.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para">
<p id="S2.SS2.SSS3.p2.1" class="ltx_p"><span id="S2.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Insufficient educational resources.</span> Despite the advancements in technology, there are still areas where schools lack modern technology infrastructure, resulting in a digital divide that hinders student’s access to online learning and digital education resources. Moreover, the number of students worldwide continues to rise, putting immense pressure on the education industry. Some regions face the challenge of insufficient educational resources, including teachers, classrooms, and learning materials, leading to disparities in educational opportunities.</p>
</div>
<div id="S2.SS2.SSS3.p3" class="ltx_para">
<p id="S2.SS2.SSS3.p3.1" class="ltx_p"><span id="S2.SS2.SSS3.p3.1.1" class="ltx_text ltx_font_bold">Education quality and standards.</span> Inconsistencies in education quality pose a significant challenge. In some regions, an exam-oriented approach to education may lead to a narrow focus on standardized testing, resulting in a simplified curriculum and a lack of support for students’ personal interests and development. Ensuring high-quality, standardized education is crucial to enhance student’s academic performance and overall quality. This can be achieved by implementing a well-rounded curriculum that fosters critical thinking, creativity, and problem-solving skills while also providing individualized support for student’s unique needs and interests.</p>
</div>
<div id="S2.SS2.SSS3.p4" class="ltx_para">
<p id="S2.SS2.SSS3.p4.1" class="ltx_p"><span id="S2.SS2.SSS3.p4.1.1" class="ltx_text ltx_font_bold">Diverse educational technology.</span> The integration of big data, AI, virtual reality, and other educational technologies has the potential to revolutionize the education sector. However, it also poses new challenges, such as management, security, and privacy considerations. Effective integration and utilization of these technologies are crucial to enhance the learning experience and achieve optimal educational outcomes. This requires a well-thought-out strategy that takes into account the unique needs and constraints of the education sector.</p>
</div>
<div id="S2.SS2.SSS3.p5" class="ltx_para">
<p id="S2.SS2.SSS3.p5.1" class="ltx_p"><span id="S2.SS2.SSS3.p5.1.1" class="ltx_text ltx_font_bold">Challenges in implementing new educational concepts.</span> The rapid pace of technological and economic advancements, coupled with improvements in living standards and quality, has led to the emergence of new educational concepts. One such concept is “Science Technology Engineer Art Math (STEAM)" education, which emphasizes interdisciplinary approaches and hands-on practice. However, implementing these cutting-edge educational concepts and cultivating the next generation of socially conscious talents pose a significant challenge for the education sector. Effective strategies and innovative approaches are needed to address these challenges and ensure that students are well-equipped to thrive in an ever-changing world.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Characteristics of LLMEdu</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The integration of AI into the education industry has accelerated rapidly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>, transforming teaching methods and enhancing learning outcomes. From computer-assisted teaching to personalized adaptive learning and content generation, AI has revolutionized the education sector, catering to diverse age groups and fields of study. In the era of intelligence, the primary objective of education is to convert knowledge into intelligence and nurture intelligent individuals. LLMs, with natural language technology at their core, align seamlessly with the education industry’s development and adapt to the vast changes in intelligent education. These models have the potential to support and enhance various aspects of the learning experience, making education more accessible, engaging, and effective.</p>
</div>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Specific embodiment of “LLMs + education"</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">Reasons for integrating LLM into education are shown in Figure <a href="#S2.F3" title='Figure 3 ‣ 2.3.1 Specific embodiment of “LLMs + education" ‣ 2.3 Characteristics of LLMEdu ‣ 2 Characteristics of LLM in Education ‣ Large Language Models for Education: A Survey' class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2405.13001/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="182" height="66" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Reasons for integrating LLM into education.</figcaption>
</figure>
<div id="S2.SS3.SSS1.p2" class="ltx_para">
<p id="S2.SS3.SSS1.p2.1" class="ltx_p"><span id="S2.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Interdisciplinary teaching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>.</span> The training of LLMs with vast amounts of data gives them a significant advantage in knowledge integration. They can provide diverse learning support based on different subjects and boast excellent interdisciplinary capabilities. For instance, the “Ziyue" large model<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.bing.com/ck/a?!&amp;&amp;p=684b97a33aeec65fJmltdHM9MTY5OTE0MjQwMCZpZ3VpZD0xODBlMmVkZC1jNGM5LTYxYWEtMGNmYi0zZGMxYzUxYjYwN2YmaW5zaWQ9NTIwOQ&amp;ptn=3&amp;hsh=3&amp;fclid=180e2edd-c4c9-61aa-0cfb-3dc1c51b607f&amp;psq=%e5%ad%90%e6%9b%b0%e6%95%99%e8%82%b2%e5%a4%a7%e6%a8%a1%e5%9e%8b%e8%8b%b1%e6%96%87%e5%90%8d&amp;u=a1aHR0cHM6Ly9haWNlbnRlci55b3VkYW8uY29tLw&amp;ntb=1" title="" class="ltx_ref ltx_href">https://aicenter.youdao.com</a> </span></span></span> prioritizes a “scenario-first" approach, while the iFLYTEK “Spark Desk"<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://passport.xfyun.cn/login" title="" class="ltx_ref ltx_href"> (xfyun.cn)</a> </span></span></span> can conduct human-like interactive learning in various fields, including mathematics, English oral practice, essay correction, and more. These models have the potential to revolutionize the way we learn and teach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S2.SS3.SSS1.p3" class="ltx_para">
<p id="S2.SS3.SSS1.p3.1" class="ltx_p"><span id="S2.SS3.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Precise identification of personalized needs.</span> LLMs possess advanced language understanding and generation capabilities, enabling them to provide adaptive learning guidance tailored to individual users’ age, learning stage, and learning environment. For example, the iFlytek learning machine based on LLMs can provide customized teaching for traditional subjects, such as oral teaching, Chinese and English composition correction, interactive supplementary mathematics, and so on, providing students with personalized one-to-one mentoring experiences. Furthermore, the learning machine can help parents answer questions through one-to-one dialogue, provide suggestions, and assist in parent-child communication, parent-child interaction, behavioral habits, and so on.</p>
</div>
<div id="S2.SS3.SSS1.p4" class="ltx_para">
<p id="S2.SS3.SSS1.p4.1" class="ltx_p"><span id="S2.SS3.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Guided learning.</span> LLMs are shifting towards a more human-like approach, providing authentic conversational teaching experiences in various scenarios instead of simply giving answers. This is particularly noticeable in subjects like physics and mathematics, where LLMs simulate a teacher’s role and ask questions to encourage critical thinking and independent exploration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. By fostering a self-learning environment, LLMs can help students develop their problem-solving skills and become more effective learners <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>. For example, OpenAI collaborated with the educational organization Khan Academy to produce Khanmigo, an LLM-based educational tool. As students complete the exercises, Khanmigo can guide them to get answers on their own by asking a lot of questions.</p>
</div>
<div id="S2.SS3.SSS1.p5" class="ltx_para">
<p id="S2.SS3.SSS1.p5.1" class="ltx_p"><span id="S2.SS3.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Integration of three modes.</span> Tool-based, companion-based, and information-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite>. The tool-based mode primarily involves using data to construct a knowledge base, which becomes a large-scale query repository. The companion-based mode is exemplified by virtual teachers and assistants, providing virtual teaching and online assistance through human-like conversations. The informatization-based mode mainly refers to educational informatization, accelerating the development of an “internet + education" platform.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Impact of “LLMs + education"</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">“LLMs + education" will have far-reaching and profound impacts. Here are 10 areas where these impacts can be observed, along with detailed explanations.</p>
</div>
<div id="S2.SS3.SSS2.p2" class="ltx_para">
<p id="S2.SS3.SSS2.p2.1" class="ltx_p"><span id="S2.SS3.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Personalized learning support.</span> LLMs can provide customized learning support based on students’ personalized needs. By deeply understanding students learning characteristics, interests, and learning styles, LLMs can tailor teaching content and learning plans for each student. For example, in mathematics learning, LLMs can provide targeted guidance for students’ weak points in mathematics by interacting with them in dialogue, helping them overcome difficulties, and improving their mathematical abilities. LLMs can design adaptive tests that adjust the difficulty of questions based on students’ responses, accurately assessing students’ knowledge levels and ensuring they are educated at the appropriate level <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S2.SS3.SSS2.p3" class="ltx_para">
<p id="S2.SS3.SSS2.p3.1" class="ltx_p"><span id="S2.SS3.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Personalized assessment and feedback.</span> LLMs can provide personalized assessment and feedback based on students’ learning performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. By analyzing student’s answers, understanding levels, and error patterns during the learning process, LLMs can provide targeted assessment results and improvement suggestions. For example, when students encounter difficulties in writing, LLMs can analyze the structure, grammar, and expression of their writing pieces and provide detailed guidance and suggestions to help students improve their writing skills <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>. Some commercial auxiliary tools based on OpenAI’s LLM technology, MagicSchool, and Eduaide, can participate in the assessment of students’ homework and give feedback <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>.</p>
</div>
<div id="S2.SS3.SSS2.p4" class="ltx_para">
<p id="S2.SS3.SSS2.p4.1" class="ltx_p"><span id="S2.SS3.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Wide coverage of subject knowledge.</span> LLMs have extensive knowledge coverage and can encompass knowledge content from multiple subject areas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>. Students can engage in dialogue with LLMs to acquire knowledge and information across various subject domains. For instance, when students encounter problems in history learning, LLMs can provide detailed explanations and in-depth discussions of historical events, figures, and backgrounds, helping students better understand historical knowledge. According to statistics, the latest model has 13 trillion tokens of carefully selected pre-training knowledge data, which is equivalent to 5 million sets of four major classics. In addition, 1.8 trillion "knowledge fragments" are extracted during training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S2.SS3.SSS2.p5" class="ltx_para">
<p id="S2.SS3.SSS2.p5.1" class="ltx_p"><span id="S2.SS3.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Interdisciplinary learning.</span> LLMs have excellent interdisciplinary capabilities, enabling students to engage in integrated learning and cultivate interdisciplinary thinking skills <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite>. Through interactions with LLMs, students can integrate and apply knowledge from different subject areas. For example, when conducting scientific experiments, students can have conversations with LLMs to discuss experimental principles, data analysis, and scientific reasoning, promoting integrated learning between science and mathematics, logical thinking, and other disciplines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S2.SS3.SSS2.p6" class="ltx_para">
<p id="S2.SS3.SSS2.p6.1" class="ltx_p"><span id="S2.SS3.SSS2.p6.1.1" class="ltx_text ltx_font_bold">Real-time problem-solving and tutoring.</span> LLMs can provide real-time problem-solving and tutoring support for students. When students encounter confusion or questions during the learning process, they can ask LLMs at any time and receive immediate answers and solutions. A survey report in the first half of this year pointed out that 89% of American students surveyed were using ChatGPT to complete homework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite>. Additionally, when students encounter comprehension difficulties while reading literary works, they can engage in dialogue with LLMs to explore the themes, plots, and character images of literary works, helping students better understand and analyze literary works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite>.</p>
</div>
<div id="S2.SS3.SSS2.p7" class="ltx_para">
<p id="S2.SS3.SSS2.p7.1" class="ltx_p"><span id="S2.SS3.SSS2.p7.1.1" class="ltx_text ltx_font_bold">Opportunities for learning across time and space.</span> The existence of LLMs allows students to learn anytime and anywhere. Students can interact with LLMs through mobile devices or computers, without being constrained by traditional classroom time and location. For example, students can utilize evening or weekend time to engage in online learning with LLMs, improving their academic abilities and knowledge levels. Online learning platforms, which utilize LLMs, provide students with access to a wide range of courses and disciplines via the Internet. The LLMs support the implementation of virtual classrooms and distance education, and students talk to the LLMs in real time to solve problems.</p>
</div>
<div id="S2.SS3.SSS2.p8" class="ltx_para">
<p id="S2.SS3.SSS2.p8.1" class="ltx_p"><span id="S2.SS3.SSS2.p8.1.1" class="ltx_text ltx_font_bold">Provision of learning resources and tools.</span> LLMs can serve as rich learning resources and tools, providing a wide range of educational materials and tools for student’s learning needs. For instance, LLMs can offer textbooks, educational videos, interactive exercises, and other learning materials to support student’s learning in various subjects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Additionally, there are some subject-specific tools, such as MathGPT. MathGPT has an accuracy rate of 60.34% in the benchmark test AGIEval, which can help students solve mathematical problems efficiently <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite>.</p>
</div>
<div id="S2.SS3.SSS2.p9" class="ltx_para">
<p id="S2.SS3.SSS2.p9.1" class="ltx_p"><span id="S2.SS3.SSS2.p9.1.1" class="ltx_text ltx_font_bold">Promotion of critical thinking.</span> LLMs can guide students in developing critical thinking and problem-solving skills <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. By engaging in dialogue and posing thought-provoking questions, LLMs can foster a thinking atmosphere that encourages students to explore answers, enhancing their self-learning abilities and critical thinking skills. For example, LLMs can simulate a teacher’s role in a physics class, asking students questions about concepts, principles, and problem-solving strategies, encouraging them to think critically and develop problem-solving skills <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>.</p>
</div>
<div id="S2.SS3.SSS2.p10" class="ltx_para">
<p id="S2.SS3.SSS2.p10.1" class="ltx_p"><span id="S2.SS3.SSS2.p10.1.1" class="ltx_text ltx_font_bold">Professional development for educators.</span> LLMs can support the professional development of educators by providing them with access to a vast amount of educational resources, best practices, and innovative teaching approaches. Educators can interact with LLMs to enhance their teaching methods and explore new ways to engage students <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>. For example, teachers can engage in dialogue with LLMs to discuss teaching strategies, classroom management techniques, and approaches to address student’s individual needs, improving their teaching effectiveness and professional growth.</p>
</div>
<div id="S2.SS3.SSS2.p11" class="ltx_para">
<p id="S2.SS3.SSS2.p11.1" class="ltx_p"><span id="S2.SS3.SSS2.p11.1.1" class="ltx_text ltx_font_bold">Accessibility and inclusivity in education.</span> LLMs can contribute to making education more accessible and inclusive. They can provide learning support for students with different learning styles, abilities, and backgrounds, ensuring that all students have equitable access to quality education. For example, LLMs can offer alternative explanations, visual aids, and interactive learning experiences to accommodate diverse learners, including students with learning disabilities or language barriers, making education more inclusive and supportive. Additionally, through multicultural training, LLMs can better understand and respect students from different cultural backgrounds and create a learning environment that is inclusive and respectful of diversity.</p>
</div>
<div id="S2.SS3.SSS2.p12" class="ltx_para">
<p id="S2.SS3.SSS2.p12.1" class="ltx_p">In summary, the integration of LLMs with education will revolutionize the learning experience by providing personalized support, expanding knowledge coverage, promoting critical thinking, and enhancing the accessibility and inclusivity of education. It will empower students and educators alike, transforming the way knowledge is acquired, shared, and applied in the digital age.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>How to Gradually Integrate LLMs into Education</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The integration of AI into the education industry has been progressing step by step, from machine learning (implementing the ability to store and calculate) to deep learning (implementing the ability to see and hear), and now to LLMs (capable of understanding and creating) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>. In the current era, the vigorous development of quality education by the entire population and the active deployment of educational intelligent hardware nationwide represent the active transformation of educational training enterprises <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>. In the long-standing coexistence and collaboration between teachers and AI models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>, as well as the highly homogeneous hardware background, LLMs have emerged as one of the most important technologies in human intelligence.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Reasons why LLMs for education</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">LLMs’ excellent characteristics make their application in the education industry very reasonable. NLP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, data analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite>, and text generation capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> align well with the fundamental processes of learning, questioning, and feedback in education. The iterative optimization process of “development-deployment" suits the application process in the education industry. User testing and feedback data lay the foundation for further optimization. Taking the development of LLMs in China as an example, the Spark Desk by iFLYTEK<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://xinghuo.xfyun.cn/" title="" class="ltx_ref ltx_href">https://xinghuo.xfyun.cn/</a></span></span></span>, the ERNIE Bot by Baidu<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://yiyan.baidu.com/" title="" class="ltx_ref ltx_href">baidu.com</a> </span></span></span>, and the “MathGPT" by TAL<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://math-gpt.org/" title="" class="ltx_ref ltx_href">MathGPT — AI Math Calculator (math-gpt.org)</a> </span></span></span> have accumulated data from years of experience in the education industry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite>. During their usage, these LLMs can collect more data from the education industry, leading to further technology optimization.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The “AI + education" model has already formed, and the gradual maturity of AI technology has paved the way for the entry of LLMs into the education industry. Smart classrooms, voice-assisted teaching, intelligent problem-solving, and other AI applications have become routine in the education industry, leading to high acceptance of LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>. It is important to recognize that LLMs are the latest technological achievements that gather human collective intelligence, rather than only technological achievements. However, LLMs’ development potential and influence are gradually increasing.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Education companies implement their own LLMEdu development strategies. LLMs require massive amounts of data and significant investments to support them. In terms of data, looking at various education companies, long-term experience data accumulation, technology accumulation, and an objective combination of their development conditions have differentiated the educational application of LLMs. They focus on LLM research and strive to maximize their benefits, cater to current development trends, and reduce development costs. In terms of funding, consumers in the education industry have a strong willingness to consume. As people’s living standards and education levels improve, the world strengthens the education industry and injects large amounts of funding to provide a solid foundation for LLM research, development, and application.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">ChatGPT makes practical changes to the integration of technology and education. Learning is an exploration process, and LLMs play an exploratory role in education. Because of interactive questions and answers, people’s roles are changing from passive recipients of knowledge to active explorers. Because of the existence of machine hallucinations, scholars need to have a skeptical and judgmental attitude towards generated knowledge and treat LLMs from a dialectical perspective. Intelligent technology stimulates human creativity, allowing people to continuously expand their breadth of learning, thus leading to scientific and technological progress.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">LLMs support the sustainable development of education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Innovation is the core of technological development and the premise of long-term application. By fully utilizing AI technologies such as ChatGPT, the application process in education can transition from a search mode to a content generation mode personalized for individuals. This enables the development of diverse, scalable, tangible application scenarios, as well as a series of differentiated and highly experiential educational products and services. It provides excellent environments and resources for educators and education recipients, supporting education’s sustainable development.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">Nowadays, general language models (LMs) leverage extensive data memory to shift from dedicated to universal application models. They rely on text generation capabilities, transitioning the application process from distribution to generation. This allows them to achieve multi-modality and transform application scenarios from single to multiple <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. Multi-modal LLMs, which combine pre-training and downstream tasks, can efficiently complete downstream task adaptation with relatively small amounts of data and can be used in small sample learning and natural language question answering. In education, three typical applications are realized: automatic generation of teaching resources, human-machine collaborative process support <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite>, and intelligent teaching assistance for teachers. Multi-modal LMs combine the three fields of reinforcement learning, CV, and NLP. They attempt to extend the concept of LMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">What’s more, we demonstrate the development of the GPT models, as shown in Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Reasons why LLMs for education ‣ 3 How to Gradually Integrate LLMs into Education ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Iteration and comparison of LLMs</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:89.3pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-279.2pt,63.6pt) scale(0.411383733165042,0.411383733165042) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">LLMs</span></td>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Publish time</span></td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;"><span id="S3.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Parameter quantity</span></td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;"><span id="S3.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Pre-training data size</span></td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;"><span id="S3.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">Training paradigm</span></td>
<td id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;"><span id="S3.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">Feature</span></td>
</tr>
<tr id="S3.T2.1.1.2" class="ltx_tr">
<td id="S3.T2.1.1.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">GPT</td>
<td id="S3.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">2018.7</td>
<td id="S3.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">120 million</td>
<td id="S3.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">5G</td>
<td id="S3.T2.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Pre-training + fine-tuning</td>
<td id="S3.T2.1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Reflection of the advantages of self-attention structure</td>
</tr>
<tr id="S3.T2.1.1.3" class="ltx_tr">
<td id="S3.T2.1.1.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">GPT-2<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://openai.com/research/gpt-2-1-5b-release</span></span></span>
</td>
<td id="S3.T2.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">2019.2</td>
<td id="S3.T2.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">1.5 billion</td>
<td id="S3.T2.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">40G</td>
<td id="S3.T2.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Prompt paradigm based on Tunning-free: Zero Shot Prompt</td>
<td id="S3.T2.1.1.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Open the exploration of the Prompt paradigm</td>
</tr>
<tr id="S3.T2.1.1.4" class="ltx_tr">
<td id="S3.T2.1.1.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">GPT-3<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://openai.com/blog/gpt-3-apps</span></span></span>
</td>
<td id="S3.T2.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">2020.6</td>
<td id="S3.T2.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">175 billion</td>
<td id="S3.T2.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">45TB</td>
<td id="S3.T2.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Prompt paradigm based on Tunning-free: In-Context Learning</td>
<td id="S3.T2.1.1.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Deepen the exploration of the Prompt paradigm</td>
</tr>
<tr id="S3.T2.1.1.5" class="ltx_tr">
<td id="S3.T2.1.1.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">InstructGPT<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>https://openai.com/research/instruction-following</span></span></span>
</td>
<td id="S3.T2.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">2022.3</td>
<td id="S3.T2.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">175 billion</td>
<td id="S3.T2.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">45TB</td>
<td id="S3.T2.1.1.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Prompt paradigm of Instruction Tuning</td>
<td id="S3.T2.1.1.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Start paying attention to human preferences</td>
</tr>
<tr id="S3.T2.1.1.6" class="ltx_tr">
<td id="S3.T2.1.1.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">ChatGPT<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>https://openai.com/chatgpt</span></span></span>
</td>
<td id="S3.T2.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">2022.11</td>
<td id="S3.T2.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">175 billion</td>
<td id="S3.T2.1.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">45TB</td>
<td id="S3.T2.1.1.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Reinforcement learning from human feedback</td>
<td id="S3.T2.1.1.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Aligned with human preferences</td>
</tr>
<tr id="S3.T2.1.1.7" class="ltx_tr">
<td id="S3.T2.1.1.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">GPT-4<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>https://openai.com/gpt-4</span></span></span>
</td>
<td id="S3.T2.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">2023.3</td>
<td id="S3.T2.1.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Nearly 2 trillion</td>
<td id="S3.T2.1.1.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">-</td>
<td id="S3.T2.1.1.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Reinforcement learning from human feedback</td>
<td id="S3.T2.1.1.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Multimodal processing and getting closer to the bionic human brain</td>
</tr>
<tr id="S3.T2.1.1.8" class="ltx_tr">
<td id="S3.T2.1.1.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">LaMDA<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>https://openai.com/gpt-4https://blog.google/technology/ai/lamda/</span></span></span>
</td>
<td id="S3.T2.1.1.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">2021</td>
<td id="S3.T2.1.1.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">137 billion</td>
<td id="S3.T2.1.1.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">150TB</td>
<td id="S3.T2.1.1.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Pre-training + fine-tuning</td>
<td id="S3.T2.1.1.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Introduce external information retrieval system</td>
</tr>
<tr id="S3.T2.1.1.9" class="ltx_tr">
<td id="S3.T2.1.1.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">BARD<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>https://bard.google.com/</span></span></span>
</td>
<td id="S3.T2.1.1.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">2023.2</td>
<td id="S3.T2.1.1.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">137 billion</td>
<td id="S3.T2.1.1.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">-</td>
<td id="S3.T2.1.1.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Join ChromeOS as a search engine</td>
<td id="S3.T2.1.1.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Using LaMDA as a base</td>
</tr>
<tr id="S3.T2.1.1.10" class="ltx_tr">
<td id="S3.T2.1.1.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">PaLM</td>
<td id="S3.T2.1.1.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">2022.4</td>
<td id="S3.T2.1.1.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">540 billion</td>
<td id="S3.T2.1.1.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">-</td>
<td id="S3.T2.1.1.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">PathWay distributed training framework</td>
<td id="S3.T2.1.1.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Large scale, multi-lingual</td>
</tr>
<tr id="S3.T2.1.1.11" class="ltx_tr">
<td id="S3.T2.1.1.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Claude<span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>https://claude.ai/</span></span></span>
</td>
<td id="S3.T2.1.1.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">2023.3</td>
<td id="S3.T2.1.1.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">52 billion</td>
<td id="S3.T2.1.1.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">-</td>
<td id="S3.T2.1.1.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Join the RLAIF training paradigm</td>
<td id="S3.T2.1.1.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Longer and more natural text editing than ChatGPT</td>
</tr>
<tr id="S3.T2.1.1.12" class="ltx_tr">
<td id="S3.T2.1.1.12.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">BlenderBot3<span id="footnote14" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>https://blenderbot.ai/</span></span></span>
</td>
<td id="S3.T2.1.1.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">2022.8</td>
<td id="S3.T2.1.1.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">175 billion</td>
<td id="S3.T2.1.1.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">-</td>
<td id="S3.T2.1.1.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Instruction fine-tuning</td>
<td id="S3.T2.1.1.12.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:4pt;padding-bottom:4pt;">Text generation, question answering</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Fusion strategies</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Cooperating with the education and training community.</span> LLM technology engages with schools, online education platforms, and educational technology companies to collectively explore and develop the application of LLMs in education. Partnering to provide actual educational scenarios and resources can help customize models to meet educational needs and accelerate the implementation of LLMEdu. For example, Baidu launched “ERNIE Bot" <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite>, Alibaba Group Holding Limited launched “Tongyi Qianwen"<span id="footnote15" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a target="_blank" href="https://qianwen.aliyun.com/" title="" class="ltx_ref ltx_href">aliyun.com</a> </span></span></span>, and universities like Tsinghua University launched "ChatGLM"<span id="footnote16" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><a target="_blank" href="https://chatglm.cn/main/detail" title="" class="ltx_ref ltx_href"> chatglm.cn</a> </span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>, etc.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Form customized content generation to enhance competitiveness.</span> LLMs require high-quality and large data sets, so the education and training community can use LLMs to generate high-quality educational content, such as course materials, textbooks, exercises, and tests. For example, Baidu’s “ERNIE Bot" has a certain accuracy in answering knowledge questions because it uses the Baidu Encyclopedia as training material. ChatGPT can also generate some framework lesson plans for teaching.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Provide popular educational functions.</span> Some educational technology companies develop an intelligent tutoring system, use LLMs to answer students’ questions, provide answers and feedback, provide logical responses to open-ended questions, and provide guided responses to calculation questions. For example, MathGPT, developed by TAL, provides high-quality problem-solving tutoring in the field of mathematics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>. Some use LLMs to develop speech recognition and dialogue systems, making speech education and interaction easier to implement, enabling language teaching and situational dialogue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Integrate LLMs into online education platforms.</span> Based on the learning model combined with the Internet and the rapid development of big data, integrating LLMs into online education platforms can provide students with richer learning resources, tools, and more comprehensive applications. For example, the Coursera online education platform<span id="footnote17" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span><a target="_blank" href="https://www.coursera.org/" title="" class="ltx_ref ltx_href">Coursera — Degrees, Certificates, &amp; Free Online Courses</a> </span></span></span> uses LLMs to implement functions such as data collection and course recommendations. Duolingo<span id="footnote18" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span><a target="_blank" href="https://www.duolingo.com/" title="" class="ltx_ref ltx_href">duolingo.com</a> </span></span></span> uses LLMs to upgrade language functions. Chegg<span id="footnote19" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span><a target="_blank" href="https://www.chegg.com/" title="" class="ltx_ref ltx_href">Chegg - Get 24/7 Homework Help — Rent Textbooks</a></span></span></span> uses LLMs to optimize the homework tutoring process.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Participate in optimizing the educational work training process.</span> First, provide training and support to educators so that they can effectively use LLMs and related tools. For example, we learn how to integrate models into teaching, as well as how to interpret and use the data and recommendations generated by the models. Second, we use LLMs to analyze student data to provide educators with insights about student progress and needs, thereby optimizing their teaching methods, such as timely feedback features.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p"><span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_bold">Continuous improvement and research.</span> The gradual integration of LLMs into the education industry requires time and resources. During this process, the performance, application, and potential risks of LLMs are continuously monitored and improved, and data privacy and security regulations are observed, considering the educational needs of different regions and cultures, which can maximize the role of LLMs in the education industry.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Key Technologies for LLMEdu</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The technologies behind LLMs support their rapid development, as shown in Figure <a href="#S4.F4" title="Figure 4 ‣ 4 Key Technologies for LLMEdu ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The combination of these technologies enables LLMs to achieve excellent performance in a variety of NLP tasks, such as text generation, machine translation, sentiment analysis, and text classification. They already play an important role in various applications such as virtual assistants, intelligent search, automatic summary generation, and natural language understanding, which promotes the development of LLMEdu.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison between generative AI and discriminative AI</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:30.1pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-160.9pt,12.2pt) scale(0.548142434192697,0.548142434192697) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;"></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Core</span></td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;"><span id="S4.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Data learning</span></td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;"><span id="S4.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">Development process</span></td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;"><span id="S4.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">Application</span></td>
</tr>
<tr id="S4.T3.1.1.2" class="ltx_tr">
<td id="S4.T3.1.1.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;"><span id="S4.T3.1.1.2.1.1" class="ltx_text ltx_font_bold">Discriminant/Analytical AI</span></td>
<td id="S4.T3.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;">Analysis</td>
<td id="S4.T3.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;">Conditional probability distribution</td>
<td id="S4.T3.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;">Mature technology and widely used</td>
<td id="S4.T3.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;">Recommendation systems, CV, NLP</td>
</tr>
<tr id="S4.T3.1.1.3" class="ltx_tr">
<td id="S4.T3.1.1.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;"><span id="S4.T3.1.1.3.1.1" class="ltx_text ltx_font_bold">Generative AI</span></td>
<td id="S4.T3.1.1.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;">Creation</td>
<td id="S4.T3.1.1.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;">Joint probability distribution</td>
<td id="S4.T3.1.1.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;">Exponential explosion</td>
<td id="S4.T3.1.1.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:8.5pt;padding-bottom:8.5pt;">AIGC, text generation, audio generation</td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Language model.</span> It learns from a corpus and predicts word sequences based on probability distributions. Two main technologies used to train a language model are next-token prediction and masked language modeling. Next-token prediction predicts the next word based on its context, and masked language modeling learns the statistical structure of language, like word order and usage patterns <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>. However, there is still a significant gap between predicting text and mastering more advanced representations in LMs, so training strategies for LMs can be inconsistent and may not correctly reach the ultimate goal. The prediction ability reflects the large model’s learning ability, which determines whether the LLM can form a coherent and logical text when answering questions. So the language model is LLMEdu’s foundation.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Human feedback reinforcement learning (HFRL).</span> It is a method used in the training of LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>. By incorporating human feedback, it reduces distorted and meaningless outputs, helping ChatGPT overcome the issues present in GPT-3, such as consistency problems. It includes supervised fine-tuning, simulating human preferences, and proximal policy optimization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite>. i) In supervised fine-tuning, a small amount of annotated data is fine-tuned by first performing next-token prediction to improve the injected data, then integrating the results, and finally decoding operations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. ii) Developing a reward model that simulates human preferences to rank the decoded results, and constructing a ranking sequence to obtain a scoring model. To ensure consistent annotation results, the ranking process uses ordinal ranking for data annotation, resulting in a new dataset composed of comparative data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. iii) Proximal policy optimization aims to learn a policy that maximizes the cumulative reward obtained during training. The algorithm involves an actor, which outputs the probability distribution for the next action, and a critic, which estimates the expected cumulative reward for a given state. By iteratively optimizing the reward signal output, the model learns from experience, adapts to new situations, continuously adjusts its policy, and improves the LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>. HFRL improves LLMEdu’s accuracy, making the output results more concise, accurate, and in line with the human thinking process.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Deep neural networks (DNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.</span> Before explaining DNNs, it is necessary to introduce deep learning. It refers to the learning of the underlying patterns and hierarchical representations of sample data, aiming to achieve the goal of machine learning with analytical capabilities similar to humans. DNNs consist of multiple layers of interconnected neurons, typically including an input layer, several hidden layers, and an output layer. The connectivity between neurons is similar to the connections between biological neural cells. DNNs have advantages in processing large-scale educational data, including students’ academic performance, learning behavior, problem-solving abilities, etc. By analyzing these data, LLM can provide insights for educational decision-making and improve teaching methods and personalized education strategies.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Self-supervised learning. </span> To produce the desired results, a model or machine needs to be trained with the given materials. Machine learning can be categorized into supervised learning, unsupervised learning, and reinforcement learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>. Self-supervised learning falls under unsupervised learning, where the model learns general feature representations for specific tasks. Unlike supervised learning, which requires a large amount of manually annotated data for training, self-supervised learning completes self-training by replacing human annotations with the intrinsic structural features of the data itself, using unlabeled datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite>. It gradually trains the parameters from scratch in a progressive manner, using part of the input as the supervisory signal and the rest as input. This approach significantly reduces the cost of manual annotation in terms of high cost, long cycles, and low accuracy, resulting in a lower development cost. Through self-supervised learning, LLMs can learn advanced representations of language data and deep cognition of language skills. This enables them to better understand and generate education-related content, including textbooks, exercises, solutions, and study materials.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Transformer model.</span> From a structural perspective, LMs have evolved from statistical LMs to neural network LMs, and now to LLMs. Statistical LMs focus on transforming sentences into probability distributions, but the lack of computational power limits their ability to match massive amounts of data. Neural network LMs, such as recurrent neural networks, use recursion and convolutional neural networks to transform language sequences. Recurrent neural networks require considering the input-output order for computation and cannot handle examples in batches efficiently, resulting in slow speed. The Transformer model, widely used in LLMs, overcomes these limitations. The transformer model is essentially an encoder-decoder architecture that includes encoding and decoding components. It employs attention mechanisms to capture global dependencies between inputs and outputs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, without considering the distance within input or output sequences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. This approach transforms the growth rate of required data for operations on related signals from linear or logarithmic to constant, showcasing high parallelism, which is beneficial for fast model iterations. Compared to previous models, the Transformer model has a richer structure, stronger adaptability to various scenarios, and better performance. The Transformer model improves the compatibility and practicality of LLMs, as well as its ability to cope with diverse and rich teaching contents and educational scenarios.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p"><span id="S4.p7.1.1" class="ltx_text ltx_font_bold">LLM diagnostics and application evaluation.</span> Existing interdisciplinary evaluation systems assess LLMs from two perspectives: diagnostics during LLM training and the effectiveness of LLM applications. “ChatbotArena"<span id="footnote20" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span><a target="_blank" href="https://chatbotarena.com/" title="" class="ltx_ref ltx_href">24/7 Digital AI Assistant - Modern Chatbot (chatbotarena.com)</a> </span></span></span> is a benchmark platform for LLMs that conduct anonymous and random adversarial evaluations, where the system randomly selects two different LLMs to chat with users, who then rate the interactions. “SuperCLUE"<span id="footnote21" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span><a target="_blank" href="https://www.superclueai.com/" title="" class="ltx_ref ltx_href">SuperCLUE (superclueai.com)</a> </span></span></span> is a benchmark for evaluating general-purpose LMs in Chinese, examining multidimensional capabilities in terms of basic abilities, professional abilities, and Chinese-specific abilities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite>. “The C-Eval project" <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, jointly carried out by Shanghai Jiao Tong University, Tsinghua University, and the University of Edinburgh, constructs a multidisciplinary benchmark list to assist Chinese LLM research. “FlagEval" <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, built by multiple universities, adopts a three-dimensional approach to evaluating LLMs, including factuality, safety, and inclusivity. These evaluation frameworks are designed to comprehensively assess LLMEdu’s performance, ethical impact, and potential bias, as well as promote the improvement of LLMEdu’s capabilities and technology optimization.</p>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p"><span id="S4.p8.1.1" class="ltx_text ltx_font_bold">Prompt engineering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>.</span> It refers to the ability to interact with LLMs. Machines match corresponding results through prompts, thereby increasing productivity. Good prompts can enhance the intelligence of LLMs and increase the value of feedback results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite>, increasing the use value of LLMEdu. Moreover, poor prompts may lead to erroneous conclusions. In the field of education, especially rigorous science, the correctness of answers is always given priority, so optimizing prompt words is also important to deal with LLM’s nonsense when answering academic questions. Different LMs, such as ChatGPT, ERNIE Bot, and MathGPT, have independent underlying training mechanisms, and their prompts are different. This can be likened to communication with individuals with different personalities.</p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p"><span id="S4.p9.1.1" class="ltx_text ltx_font_bold">Learning cognitive mechanisms.</span> Learning cognitive mechanisms, which were developed in cognitive ethics, serve as the foundation for intelligent instructional design. It studies the process of knowledge construction in learners, integrating new knowledge into existing knowledge structures, and adjusting and updating the overall structure. Prior to ChatGPT, AI primarily focused on computation and reasoning. With AI’s rapid development, its cognitive intelligence has gradually emerged and can even match human intelligence. There are two main cognitive approaches: one involves simulating human learning processes through computer models, and the other utilizes non-invasive brain imaging techniques such as functional magnetic resonance imaging. LLMs primarily simulate human learning processes, where pre-training can be likened to acquiring new knowledge and constructing knowledge.</p>
</div>
<div id="S4.p10" class="ltx_para">
<p id="S4.p10.1" class="ltx_p">By adding plug-ins, the latest LLM GPT-4 can address real-time problems, such as solving the lag problem of pre-training data. GPT-4 can also better solve logic problems because it introduces the mathematical problem data sets MATH and GSM-8K into the training data set, which greatly improves its mathematical reasoning capabilities. Moreover, GPT-4 can also complete creative text creation because it is connected to the API, and users can customize the AI character and complete simulated writing, reducing deviations and over-correction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2405.13001/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="191" height="96" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Key technologies of the LLMs </figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Implementation of LLMEdu</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this article, many products of LLMEdu are introduced, and the summary is shown in Figure <a href="#S5.F5" title="Figure 5 ‣ 5 Implementation of LLMEdu ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Moreover, this part will focus on the implementation process of LMs from two aspects: LLMs empowering education and specifically LLMs empowering the field of mathematics. Finally, we use a unified framework to organize and compare the application of LLM in the field of education. The details are shown in Table <a href="#S5.T4" title="Table 4 ‣ 5.2 LLMs in Mathematics ‣ 5 Implementation of LLMEdu ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2405.13001/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="183" height="71" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Examples of LLMEdu.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>LLMs-empowered education</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Improve teacher effectiveness.</span> LLM can help teachers access a wealth of teaching resources, allowing them to conduct classroom instruction more effectively. Before class, LLM can serve as a helpful assistant for lesson preparation. Through interactive question-and-answer sessions, LLM can provide ideas for teacher’s lesson planning, assist in designing teaching outlines and curriculum plans, and help teachers quickly identify the highlights and challenges of a lesson. In the classroom, LLM can act as an AI teaching assistant, providing an instant feedback platform for both teachers and students and enhancing classroom engagement, interest, and appeal. After class, LLM can assist teachers in generating homework assignments and exam questions, enabling teachers to better assess students’ understanding of the subject matter. In daily work, LLM is also a valuable assistant for teachers, capable of drafting meeting invitations, writing work plans, summaries, reports, and more. When used properly, LLM can help alleviate teachers’ workload and promote their professional development <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite>. For example, a survey pointed out that during the paper revision process, 57.4% of users believed that the feedback generated by LLM was helpful and could help them improve their research process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Promote student progress and growth.</span> In terms of learning assistance, LLM is a powerful tool that can understand complex concepts, solve difficult problems, and provide corresponding learning advice. In language learning, LLM offers scenario-based dialogue training, greatly enhancing student’s oral and written abilities. In terms of cultivating thinking skills, LLM sometimes exhibits “serious nonsense". Teachers and parents can utilize this phenomenon to cultivate students’ critical thinking and enhance their information literacy. In terms of learning ability development, the process of using LLM requires students to ask questions. In this process, students have to learn how to translate their questions into effective questions and how to obtain useful information, which cultivates students’ self-learning ability and summary ability. Taking college students as an example, data shows that more than 20% of the users of one of LLM’s latest products, the iFlytek Spark model, are college students, and it helps them improve in English speaking practice, mock interviews, and after-school homework.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">Answer professional and academic questions, accelerating research progress.</span> LLM is capable of writing academic experiment codes, building experimental models, quickly and accurately searching for literature materials, and extracting and integrating relevant information. This reduces the tedious process of manual research and accumulation, saving a significant amount of time. As a result, researchers can invest more energy into subsequent research, thereby improving research efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Additionally, the report findings show that LLMs in universities, as an important research platform in the field of AI, have achieved remarkable results. Chinese universities’ research on LLMs mainly focuses on CV, NLP, speech recognition, and other fields. Research results in these fields not only provide a good academic atmosphere for teachers and students in universities but also provide strong support for the development of different AI industries.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">Promote the evolution of educational consciousness and form new learning paradigms.</span> The existing educational system is primarily focused on inheritance, and students often approach knowledge with inertial thinking inherited from their learning experiences. There is a lack of creative awareness. However, with the advancement of AI technologies such as ChatGPT, the existing learning paradigms are no longer sufficient for the future. Faced with the challenges posed by technologies like ChatGPT, it is necessary to cultivate higher consciousness and exercise thinking skills with a high level of awareness, forming new learning paradigms while improving perception and cognition to better understand the world. For example, the high-consciousness generative learning paradigm reflected in ChatGPT involves establishing connections between new and old knowledge, incorporating reflection and introspection, and innovating new concepts and understandings. To advance the high-consciousness generative learning paradigm, collaboration between educational designers and implementers is required to build adaptive learning environments and foster a positive learning atmosphere <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p"><span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_bold">Create highly contextualized and intelligent learning experiences.</span> In subject learning, generative AI like LLM, with its vast amount of data, can provide students with abundant information and knowledge, streamlining the process of finding learning materials and assisting students in finding answers and solving problems across various subjects. In language learning, LLM can offer real-time dialogue training, enabling students to immerse themselves in scenario-based learning and improve their conversational and writing skills. In terms of temporal and spatial aspects of learning, as an online tool, LLM can be accessed by students anytime and anywhere, providing great flexibility. Currently, LLMs are constantly improving their technologies and capabilities to achieve intelligent learning. For example, in the language understanding task, the ultra-large-scale Chinese pre-trained language model PLUG broke the Chinese GLUE classification list record with a score of 80.179. In the language generation task, it improved by an average of more than 8% compared with the previous best results in multiple datasets.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.1" class="ltx_p"><span id="S5.SS1.p6.1.1" class="ltx_text ltx_font_bold">Promoting high-quality development in education enhances educational management and decision-making capabilities.</span> LLMs represent the latest technological means supporting intelligent education, and their development process reflects the synchronized progress of AI and humans. This embodies a new era of educational style that aims to create intelligence, cultivate wisdom, and create more efficient intelligence. Moreover, the data transparency involved in LLMs can make educational development decisions more precise and scientific, transforming educational decision-making from experiential patterns to evidence-based patterns and thereby enhancing educational governance capabilities. Finally, educational practitioners can use AI technologies like ChatGPT to conduct scenario-based assessments of students, resulting in a digital transformation of educational evaluation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. LLMs can help teachers judge student’s progress in learning and understand student’s learning status. Notice that the multi-dimensional data collected by LLMs through evaluation is helpful for educators to study student’s learning logic and development rules, adjust teaching content on time, and provide students with personalized growth services.</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p id="S5.SS1.p7.1" class="ltx_p"><span id="S5.SS1.p7.1.1" class="ltx_text ltx_font_bold">Driving in-depth research in the education system.</span> The research paradigms in education have evolved from the traditional observation and summary of scientific experiment experience, the construction of theoretical models and derivations, and computer simulation to the scientific research paradigm of large-scale data collection, analysis, and processing. The educational research paradigm is constantly changing. However, as time progresses, the old research paradigms no longer meet the requirements. The emergence of content-generative AI, represented by LLMs, has given rise to a new paradigm, "The Fifth Paradigm" of "AI for Science," enabling humans to delve further into the exploration of the education system. This paradigm shift involves the transition from simple imitation of humans to cognitive understanding and transformation, creating a new world of AI and education. According to a survey by Study.com<span id="footnote22" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_tag ltx_tag_note">22</span><a target="_blank" href="https://study.com/" title="" class="ltx_ref ltx_href">Study.com — Take Online Courses. Earn College Credit. Research Schools, Degrees &amp; Careers</a> </span></span></span>, 21<math id="S5.SS1.p7.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS1.p7.1.m1.1a"><mo id="S5.SS1.p7.1.m1.1.1" xref="S5.SS1.p7.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p7.1.m1.1.1.cmml" xref="S5.SS1.p7.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.1.m1.1c">\%</annotation></semantics></math> of teachers outside China have begun to use ChatGPT to assist their teaching work. Chegg, a listed American education and training company, also said that after launching the LLM-based learning assistance platform, it has affected the user growth of its original business, and students’ interest in ChatGPT has greatly increased.</p>
</div>
<div id="S5.SS1.p8" class="ltx_para">
<p id="S5.SS1.p8.1" class="ltx_p"><span id="S5.SS1.p8.1.1" class="ltx_text ltx_font_bold">Promote the development of AI from fragmentation to scalability, thereby enhancing its generalization capabilities in education.</span> LLMs accurately capture knowledge from massive datasets through the process of pre-training an LLM and fine-tuning it for downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. This knowledge is stored in a large number of parameters and then fine-tuned for specific tasks. Finally, it can be flexibly applied to various scenarios. In other words, a single set of techniques can be used to address different tasks, greatly improving development efficiency. For example, in the field of education, LLMs share data to solve common problems and are widely applied in dialogue question-answering, language translation, text generation, and other scenarios. Some open-source LLMs, such as ChatGLM, Baichuan, InternLM, Qwen-7B, and Qwen-14B, are all manifestations of the generalization of LLMs, and Qwen-14B among them already has an accuracy of more than 70<math id="S5.SS1.p8.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.SS1.p8.1.m1.1a"><mo id="S5.SS1.p8.1.m1.1.1" xref="S5.SS1.p8.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p8.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p8.1.m1.1.1.cmml" xref="S5.SS1.p8.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p8.1.m1.1c">\%</annotation></semantics></math>, which shows that these degrees are constantly improving.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>LLMs in Mathematics</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">AI has been pursuing mathematical research and applications since its inception. Mathematics is a challenging subject in education, and proficiency in math represents a significant milestone in the intelligence level of LLMs. The successful handling of mathematical problems by LLMs will mark a new era in AI.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Applications in mathematics can reflect the imitation ability of LLMs.</span> Mathematics is an abstract discipline that requires logical reasoning and critical thinking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>. Currently, LLMs are unable to genuinely comprehend the essence of mathematics and demonstrate independent thought. Therefore, when addressing mathematical problems, these LLM models rely heavily on the mathematical concepts and rules embedded in their training data. For instance, when solving algebraic problems, LLMs apply algebraic rules by mimicking the way humans learn and apply algebra <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Improvement of computational performance of LLMs in mathematics.</span> The essence of LLMs is to predict future outputs based on data correlation. However, errors may occur for symbols that are rarely or never encountered in the pre-training stage. For example, because the size of numbers is infinite and the scale of LLMs is limited, arithmetic operations on large numbers are likely to go wrong. To solve this problem, fine-tune the LLM on synthetic arithmetic problems and use special training and inference strategies to further improve numerical computing performance.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">Optimize the logical reasoning process.</span> One is to optimize the human logical reasoning process through LLMs. For example, some scholars have applied LLMs to the proof of theorems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, because LLMs can provide a large amount of relevant materials to make up for the lack of information or omissions, making the reasoning more complete. The second goal is to improve LLMs’ logical reasoning abilities. The logical reasoning ability of LLMs is a key indicator for evaluating LLMs. Because LLMs usually have problems such as excessive parameter space and severe data sparseness, LLMs perform poorly on robust and rigorous reasoning tasks. Relevant research has proposed optimization methods for LLM logical reasoning problems. For example, OpenAI<span id="footnote23" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup><span class="ltx_tag ltx_tag_note">23</span>https://openai.com/research/improving-mathematical-reasoning-with-process-supervision</span></span></span> studies a process-based supervision model to improve the logical reasoning capabilities of GPT-4. Moreover, some research institutions use the method of continuous pre-prediction on large-scale mathematical corpora, which improves model performance on mathematical reasoning tasks.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p"><span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_bold">Interaction with external tools to improve LLMs’ mathematical capabilities.</span> 1) LLMs interact with language conversion tools, such as lean language <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, which can convert mathematical language into computer language, thereby improving the rigor of model reasoning. This is an innovative way to bridge the gap between human reasoning and machine reasoning. This could allow models to better understand and process complex mathematical concepts. 2) LLMs interact with information retrieval systems, such as the large dialogue model LaMDA proposed by Google, which connects to the information retrieval system and allows the model to learn to retrieve and use calculators and translation engines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>. 3) LLMs directly interact with the calculation engine, such as MathGPT, which improves calculation accuracy by interacting with the calculation engine. This allows models to take advantage of calculators’ powerful computing capabilities and perform complex mathematical calculations with greater accuracy. 4) LLMs enable themselves to determine the interactive tools, such as Meta’s toolformer model, which can determine the use of external tools by itself <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>. This gives models the flexibility to adapt to different situations and choose the most appropriate tools to solve a problem, much like humans do.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.1" class="ltx_p"><span id="S5.SS2.p6.1.1" class="ltx_text ltx_font_bold">Future development of LLMs in mathematics.</span> Specifically, the first is a cutting-edge exploration with scientific research at the core, such as the research and improvement of LLMs’ capabilities in mathematics, including computing capabilities, reasoning capabilities, robustness, and so on. The second is to improve inclusive education and basic education for the general public. This entails studying how to use models to improve learning experiences and effects, as well as enhance mathematical education for students of all ages and backgrounds. By leveraging the power of LLMs, it may be possible to create personalized learning experiences that cater to individual student’s needs and learning styles, making mathematics education more accessible and effective for a broader range of people. In terms of development potential, the expansion of LLMs’ ability to solve mathematical problems could have far-reaching implications for other technical and educational fields. For example, LLMs could be used to improve the accuracy and efficiency of scientific simulations, enhance the effectiveness of machine learning algorithms, or even aid in the development of new technologies such as quantum computing. Ultimately, the development of LLMs in mathematics could drive the development of a new generation of education models that are more inclusive, effective, and efficient.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison between generative AI and discriminative AI</figcaption>
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:138.3pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-224.6pt,75.1pt) scale(0.478362924915206,0.478362924915206) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;"><span id="S5.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Application</span></td>
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;"><span id="S5.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Advantage</span></td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;"><span id="S5.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">Disadvantage</span></td>
<td id="S5.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;"><span id="S5.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">Challenge</span></td>
<td id="S5.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;"><span id="S5.T4.1.1.1.5.1" class="ltx_text ltx_font_bold">Future development</span></td>
</tr>
<tr id="S5.T4.1.1.2" class="ltx_tr">
<td id="S5.T4.1.1.2.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Save time and costs</td>
<td id="S5.T4.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Data privacy issues</td>
<td id="S5.T4.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Expand the corpus</td>
<td id="S5.T4.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Develop personalized applications</td>
</tr>
<tr id="S5.T4.1.1.3" class="ltx_tr">
<td id="S5.T4.1.1.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Personalized learning</td>
<td id="S5.T4.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Precise teaching</td>
<td id="S5.T4.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Information bias</td>
<td id="S5.T4.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Information accuracy</td>
<td id="S5.T4.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Information extraction technology update</td>
</tr>
<tr id="S5.T4.1.1.4" class="ltx_tr">
<td id="S5.T4.1.1.4.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Good interactivity</td>
<td id="S5.T4.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">The learning process is opaque</td>
<td id="S5.T4.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Update corpus in real time</td>
<td id="S5.T4.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Integration of various technologies</td>
</tr>
<tr id="S5.T4.1.1.5" class="ltx_tr">
<td id="S5.T4.1.1.5.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Improve problem-solving abilities</td>
<td id="S5.T4.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Marginalized teachers</td>
<td id="S5.T4.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Social impact</td>
<td id="S5.T4.1.1.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Training with more accurate data</td>
</tr>
<tr id="S5.T4.1.1.6" class="ltx_tr">
<td id="S5.T4.1.1.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Guided learning</td>
<td id="S5.T4.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Encourage critical thinking</td>
<td id="S5.T4.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Misleading information</td>
<td id="S5.T4.1.1.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Emotional understanding</td>
<td id="S5.T4.1.1.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Integrate with personalized experiences</td>
</tr>
<tr id="S5.T4.1.1.7" class="ltx_tr">
<td id="S5.T4.1.1.7.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Cultivate interest in learning</td>
<td id="S5.T4.1.1.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Lack of emotional resonance</td>
<td id="S5.T4.1.1.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Unemployment Risk</td>
<td id="S5.T4.1.1.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Develop policies to address social impacts</td>
</tr>
<tr id="S5.T4.1.1.8" class="ltx_tr">
<td id="S5.T4.1.1.8.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Provide diverse learning support</td>
<td id="S5.T4.1.1.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Insufficient training data support</td>
<td id="S5.T4.1.1.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Logic optimization</td>
<td id="S5.T4.1.1.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Integration of multidisciplinary and LLM</td>
</tr>
<tr id="S5.T4.1.1.9" class="ltx_tr">
<td id="S5.T4.1.1.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Interdisciplinary learning</td>
<td id="S5.T4.1.1.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Cultivate interdisciplinary thinking skills</td>
<td id="S5.T4.1.1.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Lack of domain knowledge</td>
<td id="S5.T4.1.1.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Accuracy of knowledge integration</td>
<td id="S5.T4.1.1.9.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Revolutionize the way we learn and teach</td>
</tr>
<tr id="S5.T4.1.1.10" class="ltx_tr">
<td id="S5.T4.1.1.10.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Boast excellent interdisciplinary capabilities</td>
<td id="S5.T4.1.1.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Disciplinary bias</td>
<td id="S5.T4.1.1.10.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Algorithm optimization</td>
<td id="S5.T4.1.1.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Filter useful training data</td>
</tr>
<tr id="S5.T4.1.1.11" class="ltx_tr">
<td id="S5.T4.1.1.11.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Reduce teacher stress</td>
<td id="S5.T4.1.1.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Machine hallucination</td>
<td id="S5.T4.1.1.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Multiple text associations</td>
<td id="S5.T4.1.1.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Standardize technology use</td>
</tr>
<tr id="S5.T4.1.1.12" class="ltx_tr">
<td id="S5.T4.1.1.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Real-time problem-solving</td>
<td id="S5.T4.1.1.12.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Improved learning efficiency</td>
<td id="S5.T4.1.1.12.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Over-reliance on technology</td>
<td id="S5.T4.1.1.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Text extraction</td>
<td id="S5.T4.1.1.12.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Acceleration of model inference</td>
</tr>
<tr id="S5.T4.1.1.13" class="ltx_tr">
<td id="S5.T4.1.1.13.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.13.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Teaching assistance upgrade</td>
<td id="S5.T4.1.1.13.3" class="ltx_td ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.13.4" class="ltx_td ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.13.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Diversified technical assistance</td>
</tr>
<tr id="S5.T4.1.1.14" class="ltx_tr">
<td id="S5.T4.1.1.14.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Guide mathematics learning</td>
<td id="S5.T4.1.1.14.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Promote mathematical research</td>
<td id="S5.T4.1.1.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:4.1pt;padding-bottom:4.1pt;">Pay attention to thinking guidance</td>
</tr>
<tr id="S5.T4.1.1.15" class="ltx_tr">
<td id="S5.T4.1.1.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Applications in mathematics</td>
<td id="S5.T4.1.1.15.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Improve math learning efficiency</td>
<td id="S5.T4.1.1.15.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Math terminology learning</td>
<td id="S5.T4.1.1.15.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Improved logical reasoning ability</td>
<td id="S5.T4.1.1.15.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Mathematics research and teaching</td>
</tr>
<tr id="S5.T4.1.1.16" class="ltx_tr">
<td id="S5.T4.1.1.16.1" class="ltx_td ltx_border_b ltx_border_l ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.16.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Show the fusion of AI and mathematics</td>
<td id="S5.T4.1.1.16.3" class="ltx_td ltx_border_b ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;"></td>
<td id="S5.T4.1.1.16.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Understand number relationships</td>
<td id="S5.T4.1.1.16.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:4.1pt;padding-bottom:4.1pt;">Adequate language modeling</td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Issues and Challenges</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In practical applications, LLMs for education still face many issues and challenges, including but not limited to, as shown in Figure <a href="#S6.F6" title="Figure 6 ‣ 6 Issues and Challenges ‣ Large Language Models for Education: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S6.F6" class="ltx_figure"><img src="/html/2405.13001/assets/x6.png" id="S6.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="190" height="85" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Some challenges and issues of LLMEdu.</figcaption>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Main issues</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p"><span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_bold">Risk of widespread false knowledge.</span> As an imperfect intelligent technology, LLMs such as ChatGPT still have many flaws. The biggest drawback is the potential for generating incorrect information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. As many people have noticed, LLM sometimes exhibits machine hallucination <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. For example, a computer scientist in California tried different methods to check the output of the GPT robots and found that GPT-3.5 and GPT-4 were full of errors when testing physics, chemistry, and mathematics questions selected from college textbooks and exams. Moreover, since LLM’s training data largely consists of English corpora, it often struggles to understand and provide correct answers to personalized Chinese questions. In the short term, these errors can cause disruptions in students’ knowledge learning, and students with weaker discernment abilities are highly likely to acquire erroneous knowledge without realizing it. In the long term, if the corresponding technology is not improved promptly, LLM may contribute further to the proliferation of false knowledge. There are many examples of actively dealing with machine hallucinations. For example, the retrieval-augmented generation method (RAG) can integrate LLM with a rigorously verified external key knowledge corpus.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p"><span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_bold">Lack of clear operating rules in the education system.</span> Due to the complexity of education itself, representing the education system using specific symbols and algorithms is an extremely challenging process that current LLMs cannot achieve. Education behaviors, such as emotional interaction, effective communication, and leading by example, are currently beyond the capabilities of LLMs. LLMs learn from a large amount of data and provide feedback, representing subjective educational information with data and providing rational reflections of human thinking. The goal of anthropomorphizing LLMs is to enable NLP models, such as Word2Vec, to convert words into vectors, facilitating the computer’s processing of textual data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. GPT-1 and BERT, based on the self-attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, further enhance performance. GPT-3 achieves another leap in performance on zero-shot learning tasks with its significantly increased parameter scale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>. ChatGPT’s HFRL, code pretraining, and instruction fine-tuning improve the model’s inference capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>. GPT-4, an ultra-large-scale multimodal pre-trained model, possesses multimodal understanding and multi-type content generation capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. These examples show ideas for solving the problem of anthropomorphizing LLMs, gradually approaching human-like capabilities through continuous optimization and development, thereby alleviating the limitations of the abstraction and ambiguity of educational rules.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p"><span id="S6.SS1.p3.1.1" class="ltx_text ltx_font_bold">Some drawbacks when students use LLMs.</span> The occasional inaccuracies in LLM’s answers can mislead students who lack critical thinking skills. The great convenience of LLM may reduce students’ desire for independent learning and innovation, leading to intellectual laziness. As LLM involves massive amounts of data, students who lack awareness of data security may unknowingly leak their personal data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>. While LLM provides interactive dialogue scenarios and opportunities for AI communication with students, it reduces real interpersonal conversations, and the way of discussing problems may shift from online to one-sided questioning of the machine, affecting the development of student’s social skills. In response to these problems, educators need to actively guide students to adapt to the characteristics of LLM-assisted education and enhance the cultivation of privacy and security awareness.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p"><span id="S6.SS1.p4.1.1" class="ltx_text ltx_font_bold">Insufficient integration of LLMs in collaborative teaching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>. </span>Although LLM has achieved some level of one-on-one dialogue and communication, its integration with education in real life is still limited. The ability to solve higher-order reasoning problems and complex problems still needs improvement. For example, while GPT-4 performs reasonably well in some exams, it fails to demonstrate significant advantages in logical reasoning problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. Most LLMs have high accuracy rates (up to 95%) for reasoning with a small number of steps, but as the number of steps increases, reaching 20 or more, the accuracy drops significantly to 36%, indicating a significant disparity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>. As a result, it is necessary to develop chain-of-thought technology to improve LLMs’ reasoning ability and ability to solve complex problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>, thereby promoting the integration of large models and collaborative education.</p>
</div>
<div id="S6.SS1.p5" class="ltx_para">
<p id="S6.SS1.p5.1" class="ltx_p"><span id="S6.SS1.p5.1.1" class="ltx_text ltx_font_bold">Limitations of LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>.</span> Firstly, in pre-training, models that simultaneously satisfy the reasonable model size, advanced few-shot learning capability, and advanced fine-tuning capability have not been achieved yet. For example, GPT-3 lacks a reasonable model size and is relatively large in scale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Furthermore, the high complexity and strong data dependency of LLMs may be exploited by malicious data to affect their training process and generation results, as well as output uncertainty and other factors. The lack of interpretability in LLMs’ technology makes their internal mechanisms unclear. The widespread application of LMs requires interpretability to ensure application security, overcome performance limitations, and control societal impact, which has triggered corresponding considerations regarding these issues. In the future, LLM’s technology still needs optimization and innovation, and researchers need to consider the interpretability of the model more based on the user’s situation.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Main challenges</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p"><span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_bold">Technological challenges.</span> The application of LLMEdu relies on AI-based technologies, which are complex and challenging. If the technology is not perfected, it becomes difficult to provide high-quality educational services. The availability of high-quality data sources is one important factor influencing the improvement of LLM technology. High-quality data transformation involves capture and conversion processes. It is necessary to consider how to expand the perception of the educational field to capture dynamic performance data from any learning activity in educational subjects and how to improve the quality of the data through efficient processing. Moreover, LLMEdu faces technological challenges such as speech recognition, NLP, AIGC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>, multimodal LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite>, and other aspects. The above-mentioned issues require researchers to always pay attention to the development of other technologies in the AI field and actively integrate them into LLM to bring a better experience to the education industry.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p"><span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_bold">Artificial intelligence security.</span> The intelligence level of LLMs continues to improve, and security issues have become more severe. The first is the LLMs’ biased cognition. Some studies have pointed out that when LLMs are tested using gender bias data sets, their answers will reflect gender bias <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. Therefore, when training an LLM, the data should be filtered. The second is the lack of correct social, moral, and ethical values. For some issues that violate social ethics, LLMs are unable to judge, which increases the risk of crime. Therefore, the country should formulate a more complete legal system to regulate the use of LLMs. The third is the most common issue among artificial intelligence ethical issues: "AI replaces human activities". AI has limitations in education. While AI has great potential in education, it cannot replace the role of teachers, such as encouraging critical thinking, solving complex problems, and providing psychological and social support. However, humans should also flexibly adjust their roles, regulate and guide the development of AI from an ethical perspective, and maintain their dominant position.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p"><span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_bold">Education quality.</span> The use of LLMEdu provides many opportunities for smart education, but it also presents challenges in terms of quality. If LLMEdu cannot provide high-quality educational services, it will be difficult to gain recognition from students and teachers. Furthermore, educational institutions that use LMs must strike a balance between educational quality and technological innovation. Otherwise, there may be an overreliance on technology, neglecting the quality of education itself. Therefore, to ensure the quality of education, the first consideration is to ensure the educational content, which requires educators to adjust reasonable teaching content and clarify the auxiliary functions of LLMs. Then, technology developers are required to ensure that the technology of LLMs is steadily progressing.</p>
</div>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p"><span id="S6.SS2.p4.1.1" class="ltx_text ltx_font_bold">Technological dependence. </span> Note that the future LLMEdu should be human-centric but not technology-centric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite>. Overreliance on AI may reduce students’ ability for independent learning and innovative thinking, and it may even lead to cheating and academic misconduct, such as using ChatGPT to complete assignments and papers. It is necessary to prevent the passive application of LLMs, as seen in the examples in reality. While using AI, the student should be encouraged to think independently, explore problems, and find answers. Furthermore, students should be educated on time management, ensuring sufficient time for other important activities while using AI, and avoiding excessive dependence on it.</p>
</div>
<div id="S6.SS2.p5" class="ltx_para">
<p id="S6.SS2.p5.1" class="ltx_p"><span id="S6.SS2.p5.1.1" class="ltx_text ltx_font_bold">Technical accessibility and training.</span> The introduction of AI technology requires corresponding hardware infrastructure and network support. In resource-limited areas, this can be a challenge. Combined with the pressures and entrenched thinking that fear is being replaced <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>, there is a phenomenon of fear and refusal to use AI in education, in other words, cognitive limitations. In such cases, technical access and training become difficult. Therefore, efforts should be made to promote the long-term advantages of AI in the education industry, guide teachers and students to receive appropriate training, better understand the application ideas and specific methods of intelligent technology, enhance willingness to use, and better adapt to and utilize these tools.</p>
</div>
<div id="S6.SS2.p6" class="ltx_para">
<p id="S6.SS2.p6.1" class="ltx_p"><span id="S6.SS2.p6.1.1" class="ltx_text ltx_font_bold">Equity issues.</span> Although AI has the potential to improve the quality and efficiency of education, its use can lead to unfairness among students. For example, some families may not be able to afford AI learning tools, or in certain areas, students may lack access to the necessary technological facilities for tools like ChatGPT. Educational equity is the cornerstone of social development, and interventions are needed to address the examples mentioned above effectively. For instance, when designing and optimizing LLMs, efforts should be made to balance characteristics such as race, gender, and age, reducing the digital divide and gender gap.</p>
</div>
<div id="S6.SS2.p7" class="ltx_para">
<p id="S6.SS2.p7.1" class="ltx_p"><span id="S6.SS2.p7.1.1" class="ltx_text ltx_font_bold">Data privacy and security <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>.</span> Data privacy, including privacy protection, is a significant concern in the application of LLMs. LLMs involve collecting personal information and learning data from students and teachers. Therefore, privacy protection becomes an important issue in LLM applications. Educational institutions need to ensure the effective protection of student’s and teacher’s privacy while also ensuring the security and reliability of the data. Parents and teachers should focus on cultivating children’s awareness of data privacy and security, as well as educating students to avoid privacy risks associated with the use of LLMs. Moreover, when collecting and processing student’s learning data, it is essential to ensure that this information is properly protected to avoid data breaches or improper use.</p>
</div>
<div id="S6.SS2.p8" class="ltx_para">
<p id="S6.SS2.p8.1" class="ltx_p">In the future, following the development characteristics of the era of integrating intelligence and education, while continuing to optimize core technologies and technological innovations, LLMs such as ChatGPT, GPT-4, and MathGPT will continue to empower the education field. Moreover, based on the existing LLMs, we must continue to look for more effective training methods to more efficiently train models with large-scale parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this article, we have introduced the development and application of LLMs in the field of education as comprehensively as possible. There are still some technologies that have not been included, as well as other issues that have not been discussed in depth. It is hoped that the technology introduced in this article and the thinking presented can help scholars and researchers better develop and optimize educational LLMs. This article summarizes the process of integrating education and LLMs. LLMs have excellent language generation and interactive capabilities that cannot be provided by traditional book-based teaching. It demonstrates the creative role of AI in education, as well as teachers, and the changing roles of parents and students. For smart education, we call for more mature education and AI development standards, technical specifications, and data security guidelines to focus on more practical issues. How to ensure data security? How can we limit the behavior that relies too much on AI technology? How to cultivate students’ active exploration abilities?
LLMs and education complement each other. The application of LLMs in education makes education more intelligent and efficient, and the data accumulated over many years in education can help optimize LLM training. More attention should be paid to these development conditions. How can we create more valuable LLMEdu application scenarios? We look forward to the future of LLMEdu.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgments</span> This research was supported in part by the National Natural Science Foundation of China (No. 62272196), the Natural Science Foundation of Guangdong Province (No. 2022A1515011861), Guangzhou Basic and Applied Basic Research Foundation (No. 2024A04J9971).</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p"><span id="S7.p3.1.1" class="ltx_text ltx_font_bold">Author contributions</span> Hanyi Xu: paper reading and review, writing original draft. Wensheng Gan: conceptualization, review and editing, supervisor. Zhenlian Qi: conceptualization, review and editing. Jiayang Wu: writing original draft. Philip S. Yu: review and editing.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p"><span id="S7.p4.1.1" class="ltx_text ltx_font_bold">Data availability</span> This is a review paper, and no data was generated during the study.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p"><span id="S7.p5.1.1" class="ltx_text ltx_font_bold">Conflict of interest</span> The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
</div>
<div id="S7.p6" class="ltx_para">
<span id="S7.p6.1" class="ltx_ERROR undefined">\printcredits</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmad et al. [2023]</span>
<span class="ltx_bibblock">
Ahmad, N., Murugesan, S., Kshetri, N., 2023.

</span>
<span class="ltx_bibblock">Generative Artificial Intelligence and the Education Sector.

</span>
<span class="ltx_bibblock">Computer 56, 72–76.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al-Garaady and Mahyoob [2023]</span>
<span class="ltx_bibblock">
Al-Garaady, J., Mahyoob, M., 2023.

</span>
<span class="ltx_bibblock">ChatGPT’s Capabilities in Spotting and Analyzing Writing Errors Experienced by EFL Learners.

</span>
<span class="ltx_bibblock">Arab World English Journals .

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amer-Yahia et al. [2023]</span>
<span class="ltx_bibblock">
Amer-Yahia, S., Bonifati, A., Chen, L., Li, G., Shim, K., Xu, J., Yang, X., 2023.

</span>
<span class="ltx_bibblock">From Large Language Models to Databases and Back: A Discussion on Research and Education.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2306.01388.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amin et al. [2023]</span>
<span class="ltx_bibblock">
Amin, M.M., Cambria, E., Schuller, B.W., 2023.

</span>
<span class="ltx_bibblock">Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2303.03186.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahrami and Srinivasan [2023]</span>
<span class="ltx_bibblock">
Bahrami, M., Srinivasan, R., 2023.

</span>
<span class="ltx_bibblock">Examining LLM’s Awareness of the United Nations Sustainable Development Goals, in: ICLR Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai and Shrivastava [2010]</span>
<span class="ltx_bibblock">
Bai, K., Shrivastava, A., 2010.

</span>
<span class="ltx_bibblock">Heap Data Management for Limited Local Memory Multi-Core Processors, in: Proceedings of the Eighth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis, ACM. p. 317–326.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baidoo-Anu and Ansah [2023]</span>
<span class="ltx_bibblock">
Baidoo-Anu, D., Ansah, L.O., 2023.

</span>
<span class="ltx_bibblock">Education in the Era of Generative Artificial Intelligence: Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning.

</span>
<span class="ltx_bibblock">Journal of AI 7, 52–62.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bakker et al. [2022]</span>
<span class="ltx_bibblock">
Bakker, M., Chadwick, M., Sheahan, H., Tessler, M., Campbell-Gillingham, L., Balaguer, J., McAleese, N., Glaese, A., Aslanides, J., Botvinick, M., et al., 2022.

</span>
<span class="ltx_bibblock">Fine-tuning Language Models to Find Agreement among Humans with Diverse Preferences.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 35, 38176–38189.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al. [2020]</span>
<span class="ltx_bibblock">
Bao, H., Dong, L., Wei, F., Wang, W., Yang, N., Liu, X., Wang, Y., Gao, J., Piao, S., Zhou, M., et al., 2020.

</span>
<span class="ltx_bibblock">UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training, in: International Conference on Machine Learning, PMLR. pp. 642–652.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beck et al. [1996]</span>
<span class="ltx_bibblock">
Beck, J., Stern, M., Haugsjaa, E., 1996.

</span>
<span class="ltx_bibblock">Applications of AI in Education.

</span>
<span class="ltx_bibblock">XRDS: Crossroads, The ACM Magazine for Students 3, 11–15.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bender et al. [2021]</span>
<span class="ltx_bibblock">
Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S., 2021.

</span>
<span class="ltx_bibblock">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?, in: ACM Conference on Fairness, Accountability, and Transparency, pp. 610–623.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhutoria [2022]</span>
<span class="ltx_bibblock">
Bhutoria, A., 2022.

</span>
<span class="ltx_bibblock">Personalized Education and Artificial Intelligence in the United States, China, and India: A Systematic Review Using A Human-in-the-loop Model.

</span>
<span class="ltx_bibblock">Computers and Education: Artificial Intelligence 3, 100068.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biggs et al. [2022]</span>
<span class="ltx_bibblock">
Biggs, J., Tang, C., Kennedy, G., 2022.

</span>
<span class="ltx_bibblock">Ebook: Teaching for Quality Learning at University 5e.

</span>
<span class="ltx_bibblock">McGraw-hill education (UK).

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et al. [2022]</span>
<span class="ltx_bibblock">
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche, G.B., Lespiau, J.B., Damoc, B., Clark, A., et al., 2022.

</span>
<span class="ltx_bibblock">Improving Language Models by Retrieving from Trillions of Tokens, in: International Conference on Machine Learning, PMLR. pp. 2206–2240.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brem et al. [2021]</span>
<span class="ltx_bibblock">
Brem, A., Giones, F., Werle, M., 2021.

</span>
<span class="ltx_bibblock">The AI Digital Revolution in Innovation: A Conceptual Framework of Artificial Intelligence Technologies for the Management of Innovation.

</span>
<span class="ltx_bibblock">IEEE Transactions on Engineering Management 70, 770–776.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. [2020]</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al., 2020.

</span>
<span class="ltx_bibblock">Language Models are Few-shot lLarners.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 33, 1877–1901.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Budiharso and Tarman [2020]</span>
<span class="ltx_bibblock">
Budiharso, T., Tarman, B., 2020.

</span>
<span class="ltx_bibblock">Improving Quality Education through Better Working Conditions of Academic Institutes.

</span>
<span class="ltx_bibblock">Journal of Ethnic and Cultural Studies 7, 99–115.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bunnell et al. [2020]</span>
<span class="ltx_bibblock">
Bunnell, T., Courtois, A., Donnelly, M., 2020.

</span>
<span class="ltx_bibblock">British Elite Private Schools and Their Overseas Branches: Unexpected Actors in the Global Education Industry.

</span>
<span class="ltx_bibblock">British Journal of Educational Studies 68, 691–712.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Butcher and Sumner [2011]</span>
<span class="ltx_bibblock">
Butcher, K.R., Sumner, T., 2011.

</span>
<span class="ltx_bibblock">Self-Directed Learning and the Sensemaking Paradox.

</span>
<span class="ltx_bibblock">Human–Computer Interaction 26, 123–159.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. [2023]</span>
<span class="ltx_bibblock">
Chang, Y., Wang, X., Wang, J., Wu, Y., Zhu, K., Chen, H., Yang, L., Yi, X., Wang, C., Wang, Y., et al., 2023.

</span>
<span class="ltx_bibblock">A Survey on Evaluation of Large Language Models.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2307.03109.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020a]</span>
<span class="ltx_bibblock">
Chen, L., Chen, P., Lin, Z., 2020a.

</span>
<span class="ltx_bibblock">Artificial Intelligence in Education: A Review.

</span>
<span class="ltx_bibblock">IEEE Access 8, 75264–75278.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020b]</span>
<span class="ltx_bibblock">
Chen, X., Xie, H., Hwang, G.J., 2020b.

</span>
<span class="ltx_bibblock">A Multi-perspective Study on Artificial Intelligence in Education: Grants, Conferences, Journals, Software Tools, Institutions, and Researchers.

</span>
<span class="ltx_bibblock">Computers and Education: Artificial Intelligence 1, 100005.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020c]</span>
<span class="ltx_bibblock">
Chen, X., Xie, H., Zou, D., Hwang, G.J., 2020c.

</span>
<span class="ltx_bibblock">Application and Theory Gaps During the Rise of Artificial Intelligence in Education.

</span>
<span class="ltx_bibblock">Computers and Education: Artificial Intelligence 1, 100002.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. [2023]</span>
<span class="ltx_bibblock">
Cheng, X., Jiao, F., Ji, G., Tian, Y., 2023.

</span>
<span class="ltx_bibblock">The Artificial Intelligence Revolution Led by ChatGPT, in: International Seminar on Computer Science and Engineering Technology, IEEE. pp. 360–363.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. [2021]</span>
<span class="ltx_bibblock">
Chung, Y.A., Zhang, Y., Han, W., Chiu, C.C., Qin, J., Pang, R., Wu, Y., 2021.

</span>
<span class="ltx_bibblock">W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-supervised Speech Pre-training, in: IEEE Automatic Speech Recognition and Understanding Workshop, IEEE. pp. 244–250.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. [2023]</span>
<span class="ltx_bibblock">
Deng, Y., Liu, X., Meng, L., Jiang, W., Dong, Y., Liu, C., 2023.

</span>
<span class="ltx_bibblock">Multi-Modal Information Fusion for Action Unit Detection in the Wild, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 5855–5862.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeRose et al. [2020]</span>
<span class="ltx_bibblock">
DeRose, J.F., Wang, J., Berger, M., 2020.

</span>
<span class="ltx_bibblock">Attention flows: Analyzing and Comparing Attention Mechanisms in Language Models.

</span>
<span class="ltx_bibblock">IEEE Transactions on Visualization and Computer Graphics 27, 1160–1170.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dillenbourg [2016]</span>
<span class="ltx_bibblock">
Dillenbourg, P., 2016.

</span>
<span class="ltx_bibblock">The Evolution of Research on Digital Education.

</span>
<span class="ltx_bibblock">International Journal of Artificial Intelligence in Education 26, 544–560.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. [2023]</span>
<span class="ltx_bibblock">
Dong, L., Jiang, F., Peng, Y., Wang, K., Yang, K., Pan, C., Schober, R., 2023.

</span>
<span class="ltx_bibblock">LAMBO: Large Language Model Empowered Edge Intelligence.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2308.15078.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edyko et al. [2023]</span>
<span class="ltx_bibblock">
Edyko, K., Petryla, P., Ostafin, K., Minkner, M., Bieńkowski, B., Feja, K., Suwała, Z., Rektor, N., Łuczak, E., Marchewka, U., 2023.

</span>
<span class="ltx_bibblock">Utilizing Artificial Intelligence Tools Using the GPT Chatbot in Medicine-A Review of Flaws, Advantages, and Limitations.

</span>
<span class="ltx_bibblock">Journal of Education, Health and Sport 46, 122–133.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elnaggar et al. [2021]</span>
<span class="ltx_bibblock">
Elnaggar, A., Heinzinger, M., Dallago, C., Rehawi, G., Wang, Y., Jones, L., Gibbs, T., Feher, T., Angerer, C., Steinegger, M., et al., 2021.

</span>
<span class="ltx_bibblock">ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning.

</span>
<span class="ltx_bibblock">IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 7112–7127.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. [2023a]</span>
<span class="ltx_bibblock">
Fan, W., Zhao, Z., Li, J., Liu, Y., Mei, X., Wang, Y., Tang, J., Li, Q., 2023a.

</span>
<span class="ltx_bibblock">Recommender Systems in the Era of Large Language Models (LLMs).

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2307.02046.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. [2023b]</span>
<span class="ltx_bibblock">
Fan, Y., Jiang, F., Li, P., Li, H., 2023b.

</span>
<span class="ltx_bibblock">GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning, in: Natural Language Processing and Chinese Computing, Springer Nature Switzerland. pp. 69–80.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. [2023a]</span>
<span class="ltx_bibblock">
Gan, W., Lin, J.C.W., Chao, H.C., Yu, P.S., 2023a.

</span>
<span class="ltx_bibblock">Discovering high utility episodes in sequences.

</span>
<span class="ltx_bibblock">IEEE Transactions on Artificial Intelligence 4, 473–486.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. [2021]</span>
<span class="ltx_bibblock">
Gan, W., Lin, J.C.W., Fournier-Viger, P., Chao, H.C., Tseng, V.S., Yu, P.S., 2021.

</span>
<span class="ltx_bibblock">A Survey of Utility-oriented Pattern Mining.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering 33, 1306–1327.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. [2023b]</span>
<span class="ltx_bibblock">
Gan, W., Qi, Z., Wu, J., Lin, J.C.W., 2023b.

</span>
<span class="ltx_bibblock">Large Language Models in Education: Vision and Opportunities, in: IEEE International Conference on Big Data, IEEE. pp. 4776–4785.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. [2023c]</span>
<span class="ltx_bibblock">
Gan, W., Wan, S., Yu, P.S., 2023c.

</span>
<span class="ltx_bibblock">Model-as-a-Service (MaaS): A Survey, in: IEEE International Conference on Big Data, IEEE. pp. 4636–4645.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. [2023d]</span>
<span class="ltx_bibblock">
Gan, W., Ye, Z., Wan, S., Yu, P.S., 2023d.

</span>
<span class="ltx_bibblock">Web 3.0: The Future of Internet, in: Companion Proceedings of the ACM Web Conference, pp. 1266–1275.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2020]</span>
<span class="ltx_bibblock">
Gao, B., Cai, K., Qu, T., Hu, Y., Chen, H., 2020.

</span>
<span class="ltx_bibblock">Personalized Adaptive Cruise Control Based on Online Driving Style Recognition Technology and Model Predictive Control.

</span>
<span class="ltx_bibblock">IEEE Transactions on Vehicular Technology 69, 12482–12496.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghojogh and Ghodsi [2020]</span>
<span class="ltx_bibblock">
Ghojogh, B., Ghodsi, A., 2020.

</span>
<span class="ltx_bibblock">Attention mechanism, transformers, bert, and gpt: tutorial and survey .

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. [2021]</span>
<span class="ltx_bibblock">
Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J., Poon, H., 2021.

</span>
<span class="ltx_bibblock">Domain-specific Language Model Pretraining for Biomedical Natural Language Processing.

</span>
<span class="ltx_bibblock">ACM Transactions on Computing for Healthcare 3, 1–23.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et al. [2020]</span>
<span class="ltx_bibblock">
Guu, K., Lee, K., Tung, Z., Pasupat, P., Chang, M., 2020.

</span>
<span class="ltx_bibblock">Retrieval Augmented Language Model Pre-Training, in: International Conference on Machine Learning, PMLR. pp. 3929–3938.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. [2023]</span>
<span class="ltx_bibblock">
Han, J., Zhang, R., Shao, W., Gao, P., Xu, P., Xiao, H., Zhang, K., Liu, C., Wen, S., Guo, Z., et al., 2023.

</span>
<span class="ltx_bibblock">ImageBind-LLM: Multi-modality Instruction Tuning.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2309.03905.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. [2021]</span>
<span class="ltx_bibblock">
Han, J.M., Rute, J., Wu, Y., Ayers, E.W., Polu, S., 2021.

</span>
<span class="ltx_bibblock">Proof Artifact Co-training for Theorem Proving with Language Models.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2102.06203.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hawley and Allen [2018]</span>
<span class="ltx_bibblock">
Hawley, R., Allen, C., 2018.

</span>
<span class="ltx_bibblock">Student-generated Video Creation for Assessment: Can It Transform Assessment Within Higher Education?

</span>
<span class="ltx_bibblock">International Journal for Transformative Research 5, 1–11.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. [2019]</span>
<span class="ltx_bibblock">
Hsu, H.P., Wenting, Z., Hughes, J.E., 2019.

</span>
<span class="ltx_bibblock">Developing Elementary Students’ Digital Literacy Through Augmented Reality Creation: Insights From a Longitudinal Analysis of Questionnaires, Interviews, and Projects.

</span>
<span class="ltx_bibblock">Journal of Educational Computing Research 57, 1400–1435.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2023]</span>
<span class="ltx_bibblock">
Hu, L., Liu, Z., Zhao, Z., Hou, L., Nie, L., Li, J., 2023.

</span>
<span class="ltx_bibblock">A Survey of Knowledge Enhanced Pre-trained Language Models.

</span>
<span class="ltx_bibblock">IEEE Transactions on Knowledge and Data Engineering , 1–19.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2023a]</span>
<span class="ltx_bibblock">
Huang, G., Gan, W., Weng, J., Yu, P.S., 2023a.

</span>
<span class="ltx_bibblock">US-Rule: Discovering Utility-driven Sequential Rules.

</span>
<span class="ltx_bibblock">ACM Transactions on Knowledge Discovery from Data 17, 1–22.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2023b]</span>
<span class="ltx_bibblock">
Huang, H., Zheng, O., Wang, D., Yin, J., Wang, Z., Ding, S., Yin, H., Xu, C., Yang, R., Zheng, Q., et al., 2023b.

</span>
<span class="ltx_bibblock">ChatGPT for Shaping the Future of Dentistry: the Potential of Multi-modal Large Language Model.

</span>
<span class="ltx_bibblock">International Journal of Oral Science 15, 29.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Chang [2022]</span>
<span class="ltx_bibblock">
Huang, J., Chang, K.C.C., 2022.

</span>
<span class="ltx_bibblock">Towards Reasoning in Large Language Models: A Survey.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2212.10403.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2023c]</span>
<span class="ltx_bibblock">
Huang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu, J., Lv, C., Zhang, Y., Lei, J., et al., 2023c.

</span>
<span class="ltx_bibblock">C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2305.08322.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ivanov and Soliman [2023]</span>
<span class="ltx_bibblock">
Ivanov, S., Soliman, M., 2023.

</span>
<span class="ltx_bibblock">Game of Algorithms: ChatGPT Implications for the Future of Tourism Education and Research.

</span>
<span class="ltx_bibblock">Journal of Tourism Futures 9, 214–221.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeon and Lee [2023]</span>
<span class="ltx_bibblock">
Jeon, J., Lee, S., 2023.

</span>
<span class="ltx_bibblock">Large Language Models in Education: A Focus on the Complementary Relationship between Human Teachers and ChatGPT.

</span>
<span class="ltx_bibblock">Education and Information Technologies 28, 15873–15892.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. [2022]</span>
<span class="ltx_bibblock">
Kim, J.W., Yoon, H., Jung, H.Y., 2022.

</span>
<span class="ltx_bibblock">Improved Spoken Language Representation for Intent Understanding in a Task-Oriented Dialogue System.

</span>
<span class="ltx_bibblock">Sensors 22, 1509.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koksal [2020]</span>
<span class="ltx_bibblock">
Koksal, I., 2020.

</span>
<span class="ltx_bibblock">The Rise of Online Learning.

</span>
<span class="ltx_bibblock">FORBES .

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kopnina [2020]</span>
<span class="ltx_bibblock">
Kopnina, H., 2020.

</span>
<span class="ltx_bibblock">Education for the Future? Critical Evaluation of Education for Sustainable Development Goals.

</span>
<span class="ltx_bibblock">The Journal of Environmental Education 51, 280–291.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kotek et al. [2023]</span>
<span class="ltx_bibblock">
Kotek, H., Dockum, R., Sun, D., 2023.

</span>
<span class="ltx_bibblock">Gender Bias and Stereotypes in Large Language Models, in: The ACM Collective Intelligence Conference, pp. 12–24.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. [2023]</span>
<span class="ltx_bibblock">
Lai, J., Gan, W., Wu, J., Qi, Z., Yu, P.S., 2023.

</span>
<span class="ltx_bibblock">Large Language Models in Law: A survey.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2312.03718 .

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Latif et al. [2023]</span>
<span class="ltx_bibblock">
Latif, E., Mai, G., Nyaaba, M., Wu, X., Liu, N., Lu, G., Li, S., Liu, T., Zhai, X., 2023.

</span>
<span class="ltx_bibblock">Artificial General Intelligence for Education.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2304.12479.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li [2020]</span>
<span class="ltx_bibblock">
Li, L., 2020.

</span>
<span class="ltx_bibblock">Education Supply Chain in the Era of Industry 4.0.

</span>
<span class="ltx_bibblock">Systems Research and Behavioral Science 37, 579–592.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Challoo [2006]</span>
<span class="ltx_bibblock">
Li, S., Challoo, R., 2006.

</span>
<span class="ltx_bibblock">Restructuring An Electric Machinery Course with An Integrative Approach and Computer-assisted Teaching Methodology.

</span>
<span class="ltx_bibblock">IEEE Transactions on Education 49, 16–28.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023]</span>
<span class="ltx_bibblock">
Li, Y., Hu, B., Chen, X., Ma, L., Xu, Y., Zhang, M., 2023.

</span>
<span class="ltx_bibblock">LMEye: An Interactive Perception Network for Large Language Models.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2305.03701.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023]</span>
<span class="ltx_bibblock">
Li, Y., Zhao, J., Zheng, D., Hu, Z.Y., Chen, Z., Su, X., Huang, Y., Huang, S., Lin, D., Lyu, M.R., et al., 2023.

</span>
<span class="ltx_bibblock">CLEVA: Chinese Language Models EVAluation Platform.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2308.04813.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. [2023]</span>
<span class="ltx_bibblock">
Liang, W., Zhang, Y., Cao, H., Wang, B., Ding, D., Yang, X., Vodrahalli, K., He, S., Smith, D., Yin, Y., McFarland, D., Zou, J., 2023.

</span>
<span class="ltx_bibblock">Can Large Language Models Provide Useful Feedback on Research Papers? A Large-scale Empirical Analysis.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2310.01783.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lim et al. [2023]</span>
<span class="ltx_bibblock">
Lim, J., Sa, I., MacDonald, B., Ahn, H.S., 2023.

</span>
<span class="ltx_bibblock">A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM.

</span>
<span class="ltx_bibblock">ArXiv EA-prints , arXiv:2309.16898.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2022]</span>
<span class="ltx_bibblock">
Lin, H., Wan, S., Gan, W., Chen, J., Chao, H.C., 2022.

</span>
<span class="ltx_bibblock">Metaverse in Education: Vision, Opportunities, and Challenges, in: IEEE International Conference on Big Data, IEEE. pp. 2857–2866.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2021]</span>
<span class="ltx_bibblock">
Lin, J., Yang, A., Bai, J., Zhou, C., Jiang, L., Jia, X., Wang, A., Zhang, J., Li, Y., Lin, W., et al., 2021.

</span>
<span class="ltx_bibblock">M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2110.03888.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2015]</span>
<span class="ltx_bibblock">
Lin, J.C.W., Gan, W., Fournier-Viger, P., Hong, T.P., 2015.

</span>
<span class="ltx_bibblock">Mining High-utility Itemsets with Multiple Minimum Utility Thresholds, in: The Eighth International C* Conference on Computer Science &amp; Software Engineering, pp. 9–17.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023]</span>
<span class="ltx_bibblock">
Liu, C., Jin, R., Ren, Y., Yu, L., Dong, T., Peng, X., Zhang, S., Peng, J., Zhang, P., Lyu, Q., et al., 2023.

</span>
<span class="ltx_bibblock">M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2305.10263.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023]</span>
<span class="ltx_bibblock">
Liu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., Zhang, Y., 2023.

</span>
<span class="ltx_bibblock">Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2304.03439.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023]</span>
<span class="ltx_bibblock">
Liu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., He, H., Li, A., He, M., Liu, Z., et al., 2023.

</span>
<span class="ltx_bibblock">Summary of ChatGPT-Related Research and Perspective towards the Future of Large Language Models.

</span>
<span class="ltx_bibblock">Meta-Radiology 1, 100017.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luckin and Holmes [2016]</span>
<span class="ltx_bibblock">
Luckin, R., Holmes, W., 2016.

</span>
<span class="ltx_bibblock">Intelligence Unleashed: An Argument for AI in Education .

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lv et al. [2020]</span>
<span class="ltx_bibblock">
Lv, Z., Han, Y., Singh, A.K., Manogaran, G., Lv, H., 2020.

</span>
<span class="ltx_bibblock">Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence.

</span>
<span class="ltx_bibblock">IEEE Transactions on Industrial Informatics 17, 1496–1504.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. [2023]</span>
<span class="ltx_bibblock">
Lyu, C., Xu, J., Wang, L., 2023.

</span>
<span class="ltx_bibblock">New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2305.01181.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2023]</span>
<span class="ltx_bibblock">
Ma, X., Fang, G., Wang, X., 2023.

</span>
<span class="ltx_bibblock">LLM-Pruner: On the Structural Pruning of Large Language Models.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2305.11627.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maddigan and Susnjak [2023]</span>
<span class="ltx_bibblock">
Maddigan, P., Susnjak, T., 2023.

</span>
<span class="ltx_bibblock">Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models.

</span>
<span class="ltx_bibblock">IEEE Access 11, 45181–45193.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malodia et al. [2021]</span>
<span class="ltx_bibblock">
Malodia, S., Islam, N., Kaur, P., Dhir, A., 2021.

</span>
<span class="ltx_bibblock">Why Do People Use Artificial Intelligence-Enabled Voice Assistants?

</span>
<span class="ltx_bibblock">IEEE Transactions on Engineering Management , 1–15.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al. [2020]</span>
<span class="ltx_bibblock">
Meng, Y., Zhang, Y., Huang, J., Xiong, C., Ji, H., Zhang, C., Han, J., 2020.

</span>
<span class="ltx_bibblock">Text Classification Using Label Names Only: A Language Model Self-Training Approach.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2010.07245.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mhlanga [2023]</span>
<span class="ltx_bibblock">
Mhlanga, D., 2023.

</span>
<span class="ltx_bibblock">Open AI in Education, the Responsible and Ethical Use of ChatGPT Towards Lifelong Learning, in: FinTech and Artificial Intelligence for Sustainable Development: The Role of Smart Technologies in Achieving Development Goals. Springer, pp. 387–409.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morales and Escalante [2022]</span>
<span class="ltx_bibblock">
Morales, E.F., Escalante, H.J., 2022.

</span>
<span class="ltx_bibblock">A Brief Introduction to Supervised, Unsupervised, and Reinforcement Learning, in: Biosignal Processing and Classification Using Computational Learning and Intelligence. Academic Press, pp. 111–129.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moura and Ullrich [2021]</span>
<span class="ltx_bibblock">
Moura, L.d., Ullrich, S., 2021.

</span>
<span class="ltx_bibblock">The Lean 4 Theorem Prover and Programming Language, in: Automated Deduction – CADE 28, Springer International Publishing. pp. 625–635.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan et al. [2021]</span>
<span class="ltx_bibblock">
Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., et al., 2021.

</span>
<span class="ltx_bibblock">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM, in: The International Conference for High Performance Computing, Networking, Storage and Analysis, ACM. pp. 1–15.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naseem et al. [2021]</span>
<span class="ltx_bibblock">
Naseem, U., Razzak, I., Khan, S.K., Prasad, M., 2021.

</span>
<span class="ltx_bibblock">A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models.

</span>
<span class="ltx_bibblock">Transactions on Asian and Low-Resource Language Information Processing 20, 1–35.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ng et al. [2023]</span>
<span class="ltx_bibblock">
Ng, E., Subramanian, S., Klein, D., Kanazawa, A., Darrell, T., Ginosar, S., 2023.

</span>
<span class="ltx_bibblock">Can Language Models Learn to Listen?, in: The IEEE/CVF International Conference on Computer Vision, pp. 10083–10093.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang and Jiao [2021]</span>
<span class="ltx_bibblock">
Ouyang, F., Jiao, P., 2021.

</span>
<span class="ltx_bibblock">Artificial Intelligence in Education: The Three Paradigms.

</span>
<span class="ltx_bibblock">Computers and Education: Artificial Intelligence 2, 100020.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. [2022]</span>
<span class="ltx_bibblock">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al., 2022.

</span>
<span class="ltx_bibblock">Training Language Models to Follow Instructions with Human Feedback.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 35, 27730–27744.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">P [2020]</span>
<span class="ltx_bibblock">
P, D., 2020.

</span>
<span class="ltx_bibblock">AI in the Wild: Sustainability in the Age of Artificial Intelligence.

</span>
<span class="ltx_bibblock">MIT Press.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. [2023]</span>
<span class="ltx_bibblock">
Pan, S., Luo, L., Wang, Y., Chen, C., Wang, J., Wu, X., 2023.

</span>
<span class="ltx_bibblock">Unifying Large Language Models and Knowledge Graphs: A Roadmap.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2306.08302.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pankiewicz and Baker [2023]</span>
<span class="ltx_bibblock">
Pankiewicz, M., Baker, R.S., 2023.

</span>
<span class="ltx_bibblock">Large Language Models (GPT) for Automating Feedback on Programming Assignments.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2307.00150.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paranjape et al. [2023]</span>
<span class="ltx_bibblock">
Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettlemoyer, L., Tulio Ribeiro, M., 2023.

</span>
<span class="ltx_bibblock">ART: Automatic Multi-step Reasoning and Tool-use for Large Language Models.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2303.09014.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Philippe et al. [2020]</span>
<span class="ltx_bibblock">
Philippe, S., Souchet, A.D., Lameras, P., Petridis, P., Caporal, J., Coldeboeuf, G., Duzan, H., 2020.

</span>
<span class="ltx_bibblock">Multimodal Teaching, Learning and Training in Virtual Reality: A Review and Case Study.

</span>
<span class="ltx_bibblock">Virtual Reality &amp; Intelligent Hardware 2, 421–442.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qidwai et al. [2020]</span>
<span class="ltx_bibblock">
Qidwai, U., Kashem, S.B.A., Conor, O., 2020.

</span>
<span class="ltx_bibblock">Humanoid Robot as a Teacher’s Assistant: Helping Children with Autism to Learn Social and Academic Skills.

</span>
<span class="ltx_bibblock">Journal of Intelligent &amp; Robotic Systems 98, 759–770.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajbhandari et al. [2020]</span>
<span class="ltx_bibblock">
Rajbhandari, S., Rasley, J., Ruwase, O., He, Y., 2020.

</span>
<span class="ltx_bibblock">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models, in: SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE. pp. 1–16.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rawte et al. [2023]</span>
<span class="ltx_bibblock">
Rawte, V., Sheth, A., Das, A., 2023.

</span>
<span class="ltx_bibblock">A Survey of Hallucination in Large Foundation Models.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2309.05922.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rudovic et al. [2019]</span>
<span class="ltx_bibblock">
Rudovic, O., Zhang, M., Schuller, B., Picard, R., 2019.

</span>
<span class="ltx_bibblock">Multi-Modal Active Learning From Human Data: A Deep Reinforcement Learning Approach, in: International Conference on Multimodal Interaction, pp. 6–15.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saini and Goel [2019]</span>
<span class="ltx_bibblock">
Saini, M.K., Goel, N., 2019.

</span>
<span class="ltx_bibblock">How Smart Are Smart Classrooms? A Review of Smart Classroom Technologies.

</span>
<span class="ltx_bibblock">ACM Computing Survey 52, 1–28.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scarlatos and Lan [2023]</span>
<span class="ltx_bibblock">
Scarlatos, A., Lan, A., 2023.

</span>
<span class="ltx_bibblock">Tree-Based Representation and Generation of Natural and Mathematical Language.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2302.07974.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al. [2023]</span>
<span class="ltx_bibblock">
Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., Scialom, T., 2023.

</span>
<span class="ltx_bibblock">Toolformer: Language Models Can Teach Themselves to Use Tools.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2302.04761.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schlexer Lamoureux et al. [2019]</span>
<span class="ltx_bibblock">
Schlexer Lamoureux, P., Winther, K.T., Garrido Torres, J.A., Streibel, V., Zhao, M., Bajdich, M., Abild-Pedersen, F., Bligaard, T., 2019.

</span>
<span class="ltx_bibblock">Machine Learning for Computational Heterogeneous Catalysis.

</span>
<span class="ltx_bibblock">ChemCatChem 11, 3581–3601.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwartz et al. [2020]</span>
<span class="ltx_bibblock">
Schwartz, R., Dodge, J., Smith, N.A., Etzioni, O., 2020.

</span>
<span class="ltx_bibblock">Green AI.

</span>
<span class="ltx_bibblock">Communications of the ACM 63, 54–63.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srinivas Tida and Hsu [2022]</span>
<span class="ltx_bibblock">
Srinivas Tida, V., Hsu, S., 2022.

</span>
<span class="ltx_bibblock">Universal Spam Detection using Transfer Learning of BERT Model.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2202.03480.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. [2016]</span>
<span class="ltx_bibblock">
Su, H.F.H., Ricci, F.A., Mnatsakanian, M., 2016.

</span>
<span class="ltx_bibblock">Mathematical Teaching Strategies: Pathways to Critical Thinking and Metacognition.

</span>
<span class="ltx_bibblock">International Journal of Research in Education and Science 2, 190–200.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2023]</span>
<span class="ltx_bibblock">
Sun, J., Gan, W., Chao, H.C., Yu, P.S., Ding, W., 2023.

</span>
<span class="ltx_bibblock">Internet of Behaviors: A Survey.

</span>
<span class="ltx_bibblock">IEEE Internet of Things Journal 10, 11117–11134.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Le [2019]</span>
<span class="ltx_bibblock">
Tan, M., Le, Q., 2019.

</span>
<span class="ltx_bibblock">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, in: The 36th International Conference on Machine Learning, PMLR. pp. 6105–6114.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2020]</span>
<span class="ltx_bibblock">
Tang, Y., Liang, J., Hare, R., Wang, F.Y., 2020.

</span>
<span class="ltx_bibblock">A Personalized Learning System for Parallel Intelligent Education.

</span>
<span class="ltx_bibblock">IEEE Transactions on Computational Social Systems 7, 352–361.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tao et al. [2021]</span>
<span class="ltx_bibblock">
Tao, S., Qiu, R., Ping, Y., Ma, H., 2021.

</span>
<span class="ltx_bibblock">Multi-modal Knowledge-aware Reinforcement Learning Network for Explainable Recommendation.

</span>
<span class="ltx_bibblock">Knowledge-Based Systems 227, 107217.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thirunavukarasu et al. [2023]</span>
<span class="ltx_bibblock">
Thirunavukarasu, A.J., Ting, D.S.J., Elangovan, K., Gutierrez, L., Tan, T.F., Ting, D.S.W., 2023.

</span>
<span class="ltx_bibblock">Large language models in medicine.

</span>
<span class="ltx_bibblock">Nature Medicine 29, 1930–1940.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thoppilan et al. [2022]</span>
<span class="ltx_bibblock">
Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.T., Jin, A., Bos, T., Baker, L., Du, Y., et al., 2022.

</span>
<span class="ltx_bibblock">Language Models for Dialog Applications.

</span>
<span class="ltx_bibblock">arXiv preprint, arXiv:2201.08239 .

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tirumala et al. [2022]</span>
<span class="ltx_bibblock">
Tirumala, K., Markosyan, A., Zettlemoyer, L., Aghajanyan, A., 2022.

</span>
<span class="ltx_bibblock">Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 35, 38274–38290.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Valverde Valencia [2023]</span>
<span class="ltx_bibblock">
Valverde Valencia, À., 2023.

</span>
<span class="ltx_bibblock">An Interdisciplinary and Applied Approach to Generative Artificial Intelligence in Secondary School for the Development of Communicative Competencies .

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020a]</span>
<span class="ltx_bibblock">
Wang, C.X., Di Renzo, M., Stanczak, S., Wang, S., Larsson, E.G., 2020a.

</span>
<span class="ltx_bibblock">Artificial Intelligence Enabled Wireless Networking for 5G and Beyond: Recent Advances and Future Challenge.

</span>
<span class="ltx_bibblock">IEEE Wireless Communications 27, 16–23.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019]</span>
<span class="ltx_bibblock">
Wang, D., Weisz, J.D., Muller, M., Ram, P., Geyer, W., Dugan, C., Tausczik, Y., Samulowitz, H., Gray, A., 2019.

</span>
<span class="ltx_bibblock">Human-AI Collaboration in Data Science: Exploring Data Scientists’ Perceptions of Automated AI.

</span>
<span class="ltx_bibblock">The ACM on Human-Computer Interaction 3, 1–24.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Yeung [2020]</span>
<span class="ltx_bibblock">
Wang, H., Yeung, D.Y., 2020.

</span>
<span class="ltx_bibblock">A Survey on Bayesian Deep Learning.

</span>
<span class="ltx_bibblock">ACM Computing Survey 53, 1–37.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020b]</span>
<span class="ltx_bibblock">
Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., Zhou, M., 2020b.

</span>
<span class="ltx_bibblock">MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 33, 5776–5788.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023]</span>
<span class="ltx_bibblock">
Wang, Y., Chu, Z., Ouyang, X., Wang, S., Hao, H., Shen, Y., Gu, J., Xue, S., Zhang, J.Y., Cui, Q., et al., 2023.

</span>
<span class="ltx_bibblock">Enhancing Recommender Systems with Large Language Model Reasoning Graphs.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2308.10835.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. [2021]</span>
<span class="ltx_bibblock">
Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M., Le, Q.V., 2021.

</span>
<span class="ltx_bibblock">Finetuned Language Models Are Zero-Shot Learners.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2109.01652.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. [2022]</span>
<span class="ltx_bibblock">
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al., 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 35, 24824–24837.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williamson et al. [2023]</span>
<span class="ltx_bibblock">
Williamson, B., Macgilchrist, F., Potter, J., 2023.

</span>
<span class="ltx_bibblock">Re-examining AI, Automation and Datafication in Education.

</span>
<span class="ltx_bibblock">Learning, Media and Technology 48, 1–5.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023a]</span>
<span class="ltx_bibblock">
Wu, J., Gan, W., Chen, Z., Wan, S., Lin, H., 2023a.

</span>
<span class="ltx_bibblock">AI-Generated Content (AIGC): A Survey.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2304.06632 .

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023b]</span>
<span class="ltx_bibblock">
Wu, J., Gan, W., Chen, Z., Wan, S., Yu, P.S., 2023b.

</span>
<span class="ltx_bibblock">Multimodal Large Language Models: A Survey, in: IEEE International Conference on Big Data, IEEE. pp. 2247–2256.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023c]</span>
<span class="ltx_bibblock">
Wu, T., Zhu, B., Zhang, R., Wen, Z., Ramchandran, K., Jiao, J., 2023c.

</span>
<span class="ltx_bibblock">Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2310.00212 .

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. [2021]</span>
<span class="ltx_bibblock">
Xie, H., Qin, Z., Li, G.Y., Juang, B.H., 2021.

</span>
<span class="ltx_bibblock">Deep Learning Enabled Semantic Communication Systems.

</span>
<span class="ltx_bibblock">IEEE Transactions on Signal Processing 69, 2663–2675.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu [2023]</span>
<span class="ltx_bibblock">
Xu, H., 2023.

</span>
<span class="ltx_bibblock">No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2309.03224.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023]</span>
<span class="ltx_bibblock">
Xu, L., Li, A., Zhu, L., Xue, H., Zhu, C., Zhao, K., He, H., Zhang, X., Kang, Q., Lan, Z., 2023.

</span>
<span class="ltx_bibblock">SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2307.15020.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. [2022]</span>
<span class="ltx_bibblock">
Yan, K., Cai, J., Jin, D., Miao, S., Guo, D., Harrison, A.P., Tang, Y., Xiao, J., Lu, J., Lu, L., 2022.

</span>
<span class="ltx_bibblock">Self-Supervised Learning of Pixel-Wise Anatomical Embeddings in Radiological Images.

</span>
<span class="ltx_bibblock">IEEE Transactions on Medical Imaging 41, 2658–2669.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. [2024]</span>
<span class="ltx_bibblock">
Yan, L., Sha, L., Zhao, L., Li, Y., Martinez-Maldonado, R., Chen, G., Li, X., Jin, Y., Gašević, D., 2024.

</span>
<span class="ltx_bibblock">Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review.

</span>
<span class="ltx_bibblock">British Journal of Educational Technology 55, 90–112.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2023]</span>
<span class="ltx_bibblock">
Yang, R., Li, L., Gan, W., Chen, Z., Qi, Z., 2023.

</span>
<span class="ltx_bibblock">The Human-centric Metaverse: A Survey, in: Companion Proceedings of the ACM Web Conference, pp. 1296–1306.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Li [2019]</span>
<span class="ltx_bibblock">
Yang, W., Li, H., 2019.

</span>
<span class="ltx_bibblock">Changing Culture, Changing Curriculum: A Case Study of Early Childhood Curriculum Innovations in Two Chinese Kindergartens.

</span>
<span class="ltx_bibblock">The Curriculum Journal 30, 279–297.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2023]</span>
<span class="ltx_bibblock">
Yu, Z., Wu, Y., Zhang, N., Wang, C., Vorobeychik, Y., Xiao, C., 2023.

</span>
<span class="ltx_bibblock">CodeIPPrompt: Intellectual Property Infringement Assessment of Code Language Models, in: International Conference on Machine Learning, PMLR. pp. 40373–40389.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zamfirescu-Pereira et al. [2023]</span>
<span class="ltx_bibblock">
Zamfirescu-Pereira, J., Wong, R.Y., Hartmann, B., Yang, Q., 2023.

</span>
<span class="ltx_bibblock">Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts, in: CHI Conference on Human Factors in Computing Systems, Curran Associates, Inc.. pp. 1–21.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. [2023a]</span>
<span class="ltx_bibblock">
Zeng, F., Gan, W., Wang, Y., Liu, N., Yu, P.S., 2023a.

</span>
<span class="ltx_bibblock">Large Language Models for Robotics: A Survey.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2311.07226 .

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. [2023b]</span>
<span class="ltx_bibblock">
Zeng, F., Gan, W., Wang, Y., Yu, P.S., 2023b.

</span>
<span class="ltx_bibblock">Distributed Training of Large Language Models, in: IEEE 29th International Conference on Parallel and Distributed Systems, IEEE. pp. 840–847.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng [2023]</span>
<span class="ltx_bibblock">
Zeng, H., 2023.

</span>
<span class="ltx_bibblock">Measuring Massive Multitask Chinese Understanding.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2304.12986.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng and Mahmud [2023]</span>
<span class="ltx_bibblock">
Zeng, Y., Mahmud, T., 2023.

</span>
<span class="ltx_bibblock">ChatGPT in English Class: Perspectives of Students and Teachers from Swedish Upper Secondary Schools.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023a]</span>
<span class="ltx_bibblock">
Zhang, C., Dai, Q., Du, Z., Gan, W., Weng, J., Yu, P.S., 2023a.

</span>
<span class="ltx_bibblock">TUSQ: Targeted High-Utility Sequence Querying.

</span>
<span class="ltx_bibblock">IEEE Transactions on Big Data 9, 512–527.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023b]</span>
<span class="ltx_bibblock">
Zhang, C., Zhang, C., Zheng, S., Qiao, Y., Li, C., Zhang, M., Dam, S.K., Thwal, C.M., Tun, Y.L., Huy, L.L., et al., 2023b.

</span>
<span class="ltx_bibblock">A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2303.11717.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Li [2021]</span>
<span class="ltx_bibblock">
Zhang, M., Li, J., 2021.

</span>
<span class="ltx_bibblock">A Commentary of GPT-3 in MIT Technology Review.

</span>
<span class="ltx_bibblock">Fundamental Research 1, 831–833.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao [2022]</span>
<span class="ltx_bibblock">
Zhao, L., 2022.

</span>
<span class="ltx_bibblock">A Study on Data-Driven Teaching Decision Optimization of Distance Education Platforms.

</span>
<span class="ltx_bibblock">International Journal of Emerging Technologies in Learning 17.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2020]</span>
<span class="ltx_bibblock">
Zhao, S., Blaabjerg, F., Wang, H., 2020.

</span>
<span class="ltx_bibblock">An Overview of Artificial Intelligence Applications for Power Electronics.

</span>
<span class="ltx_bibblock">IEEE Transactions on Power Electronics 36, 4633–4658.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2023]</span>
<span class="ltx_bibblock">
Zheng, R., Dou, S., Gao, S., Shen, W., Wang, B., Liu, Y., Jin, S., Liu, Q., Xiong, L., Chen, L., et al., 2023.

</span>
<span class="ltx_bibblock">Secrets of RLHF in Large Language Models Part I: PPO.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2307.04964.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhipeng et al. [2019]</span>
<span class="ltx_bibblock">
Zhipeng, G., Yi, X., Sun, M., Li, W., Yang, C., Liang, J., Chen, H., Zhang, Y., Li, R., 2019.

</span>
<span class="ltx_bibblock">Jiuge: A Human-Machine Collaborative Chinese Classical Poetry Generation System , 25–30.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. [2023]</span>
<span class="ltx_bibblock">
Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., Duan, N., 2023.

</span>
<span class="ltx_bibblock">AGIEval: A Human-centric Benchmark for Evaluating Foundation Models.

</span>
<span class="ltx_bibblock">ArXiv E-prints , arXiv:2304.06364.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al. [2021]</span>
<span class="ltx_bibblock">
Zou, L., Zhang, S., Cai, H., Ma, D., Cheng, S., Wang, S., Shi, D., Cheng, Z., Yin, D., 2021.

</span>
<span class="ltx_bibblock">Pre-Trained Language Model Based Ranking in Baidu Search, in: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, ACM. pp. 4014–4022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.13000" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.13001" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.13001">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.13001" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.13002" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 13:35:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
