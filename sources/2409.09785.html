<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.09785] Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition</title><meta property="og:description" content="Given recent advances in generative AI technology, a key question is how large language models (LLMs) can enhance acoustic modeling tasks using text decoding results from a frozen, pretrained automatic speech recogniti‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.09785">

<!--Generated on Sat Oct  5 19:26:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Large Language Model Based Generative Error Correction: 
<br class="ltx_break">A Challenge and Baselines for
Speech Recognition, Speaker Tagging, and Emotion Recognition</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Given recent advances in generative AI technology, a key question is how large language models (LLMs) can enhance acoustic modeling tasks using text decoding results from a frozen, pretrained automatic speech recognition (ASR) model. To explore new capabilities in language modeling for speech processing, we introduce the generative speech transcription error correction (GenSEC) challenge. This challenge comprises three post-ASR language modeling tasks: (i) post-ASR transcription correction, (ii) speaker tagging, and (iii) emotion recognition. These tasks aim to emulate future LLM-based agents handling voice-based interfaces while remaining accessible to a broad audience by utilizing open pretrained language models or agent-based APIs. We also discuss insights from baseline evaluations, as well as lessons learned for designing future evaluations.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">‚Äî‚Äâ</span></span>
Language modeling, speech recognition postprocessing, speaker tagging, speech emotion recognition.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Early statistical ASR systems based on the noisy channel model were conceived as utilizing two model components: acoustic model and language model (LM)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. First-pass decoding results could be subjected to postprocessing, or <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">rescoring</span>, to apply more powerful LMs or additional knowledge sources<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. With the introduction of end-to-end (E2E) ASR systems in the early 2020s, language modeling for post-ASR has become more complex, e.g., by also modeling the implicit <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">internal LM</span> of E2E ASR systems<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>). With the advent of LLMs, however, post-ASR processing has become very attractive again, given the capacity of LLMs to model linguistic patterns, contextual influences, and even world knowledge as reflected in language.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">More recently, LLMs and speech/language-model alignment methods have sparked considerable interest in new methods, such as cascaded LLM correction¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, for ASR and speech translation. LLM-based text-to-text generative error correction¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> has shown accuracy improvements over baseline rescoring methods, even surpassing <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S1.p2.1.m1.1a"><mi id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">n</annotation></semantics></math>-best oracle performance, by bringing external knowledge to bear in ASR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, speech translation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and image captioning¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While a text-based ASR-LLM interface limits the richness of information utilized in post-processing, such as acoustic-prosodic expressions of speaker identity and paralinguistic properties, ASR outputs will still be sensitive to such information, especially when multiple hypotheses are output, effectively providing a text-based feature map that weakly reflects acoustic information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.09785/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="161" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text ltx_font_bold">Fig.¬†1</span>: </span>The framework for LLM postprocessing of ‚Äútext representation of speech‚Äù via ASR-decoded information, for three tasks: speech recognition, speaker diarization, and emotion recognition.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Inspired by this observation, as well as by the early LLM-based studies cited, our challenge task aims to push research in two directions: (1) how large <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">performance gains</span> could be achieved by applying cascaded ASR-LLMs, and (2) how well cascaded LLMs could perform <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">tasks beyond word-level transcription</span>, such as recovering speaker and paralinguistic information. In other words, by leveraging LLMs, even text-only output from first-pass ASR system (such as available from a black-box API) might be enriched with paralinguistic and meta-information that is commonly thought to be encoded principally in acoustic-prosodic features. Fig.¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the three challenge tasks, highlighting the assumption that ASR hypotheses in textual form contain sufficient implicit acoustic information to perform these tasks.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The short-term goal of the challenge is to introduce new ASR-LM tasks to the speech community that leverage the latest developments in LLM-based post-ASR text modeling, potentially benefiting the design of voice-interface LLM agents that use only text-based encodings. Through this initiative, we aim to advance the understanding of LLM capabilities in implicit acoustic modeling within the spoken language processing community. In the longer term, our goal is to highlight the importance of audio and other modalities for speech processing and understanding, setting the stage for future challenges that go beyond the initial text-only framework.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">By promoting ongoing cooperation and knowledge sharing, we seek to catalyze significant progress in the field of LLM-based voice interfaces, driving innovation and opening new avenues for research, development, and practical applications of ASR-LLM. This includes error type analysis and cross-lingual variants of these tasks. In the following sections we give more background and introduce the specifics of the challenge tasks.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Post-ASR Text Modeling</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">This challenge considers an agent-based LLM application scenario with a fixed ASR interface. The focus is on characterizing how LLMs can enhance speech processing by leveraging the textual N-best hypotheses without explicit encoding of acoustic information. Additionally, we encourage participants to ‚Äúpush the limits of language modeling for ASR‚Äù given a setup based on a black-box ASR interface, which would be readily accessible through APIs, at modest cost.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The success of probabilistic language modeling systems in ASR can be traced back to several influential tools and frameworks, including SRILM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, CNTK¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, and the LM components within Kaldi¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. These tools have significantly contributed to the development and enhancement of speech recognition technology by providing robust methods for handling the complexities of speech processing.
LLMs, on the other hand, have also benefited from democratized model inference (e.g., Claude) and open-source models (e.g., LLaMA) to establish end-to-end agent learning-based interfaces, such as AudioGPT¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. For instance, work on task-activating prompting (TAP)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> illustrates that instruction-prompted LLMs for ASR can correct recognition errors by inferring phonetic confusions or grammatical variants from the ASR-decoded text.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">To investigate this form of ASR-LLM pipeline, we introduce three tasks based on ASR-decoded text, which have been studied previously and were shown to benefit from combined acoustic and language modeling. In this challenge, participants can explore a training-free setup by optimizing instruction prompts for the speech tasks, or by hosting LLMs in their own compute environment. To examine the limits of the text-only modality for speech processing, we limit the first SLT challenge to text-to-text modeling without access to acoustic embeddings. The acoustic information will be accessed through ASR hypotheses ranked by acoustic confidence scores and error word-level or utterance-level error attributions. We hope this simple setup will entice researchers without a speech background to become active in speech processing through language modeling.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Open Topics in LLM-based Speech Modeling</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">To avoid test set data that may have leaked into the pretrained LLMs, we prepare a non-public test set for each challenge subtask. While LLMs hold promise for post-ASR correction, they are not without problems. One concern is the potential for introducing biases reflected in the training data, which could affect the accuracy and fairness of the corrected transcripts. Additionally, LLMs could produce enriched text that diverges from the intended meaning or introduces new types of errors, necessitating novel error analysis methodologies. These potential issues highlight the need for ongoing research and future challenges that assess the reliability and effectiveness of LLMs in ASR postprocessing.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Cross-modal setups will be incorporated into future versions of the challenge by providing acoustic embeddings¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> or raw waveforms. By connecting the latest research and developments in speech language modeling with practical applications, the challenge promotes implementation, adoption, and understanding of cutting-edge language technologies, including in scarce-training or low-compute scenarios. Participants can expect to foster the development of innovative solutions at the intersection of speech and language technology.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Challenge Description</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The GenSEC challenge at IEEE SLT 2024 consists of three tasks for post-ASR language modeling:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Task 1: Post-ASR Output Correction by LLM</span></p>
</div>
<div id="S3.I1.i1.p2" class="ltx_para">
<p id="S3.I1.i1.p2.1" class="ltx_p">The goal of this task is to map from N-best <span id="S3.I1.i1.p2.1.1" class="ltx_text ltx_font_bold">H</span>ypotheses to ground <span id="S3.I1.i1.p2.1.2" class="ltx_text ltx_font_bold">T</span>ruth transcriptions (H2T), similar to the setup in Yang et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. The training set includes recognition scores from various pretrained end-to-end ASR models and N-best hypotheses. Participants are allowed to use N-best hypotheses and their scores for re-ranking or generative correction to produce final transcriptions.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p2" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Task 2: Post-ASR Speaker Tagging Correction</span></p>
</div>
<div id="S3.I2.i1.p2" class="ltx_para">
<p id="S3.I2.i1.p2.1" class="ltx_p">This task aims at correcting the speaker tags in the output of a speaker-attributed (multi-speaker) ASR system. Speaker tagging in Task 2 refers to the speaker indices or anonymized speaker names (e.g., ‚Äúspeaker-A‚Äù, ‚Äúspeaker-2‚Äù) used to identify who spoke which words. We will provide errorful speaker-attributed transcripts produced by a multi-speaker ASR system. Participants in Task 2 are asked to submit corrected versions of the transcripts with accurate speaker tagging. A metric that gauges both speaker tagging and ASR accuracy will be used for evaluation. Similar to the other tasks, the current version of the Track-2 challenge allows use of the text modality only.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p3" class="ltx_para">
<ul id="S3.I3" class="ltx_itemize">
<li id="S3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I3.i1.p1" class="ltx_para">
<p id="S3.I3.i1.p1.1" class="ltx_p"><span id="S3.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Task 3: Post-ASR Speech Emotion Recognition</span></p>
</div>
<div id="S3.I3.i1.p2" class="ltx_para">
<p id="S3.I3.i1.p2.1" class="ltx_p">This task aims to achieve utterance-level speech emotion recognition (SER) based on errorful ASR transcripts. Participants will develop ASR error correction methods combined with traditional deep-learning-based SER models, design novel prompt templates utilizing LLMs for SER, or utilize any other methods based on text input. Participants are encouraged to use the conversation as context to predict the emotion of a target utterance.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.09785/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text ltx_font_bold">Fig.¬†2</span>: </span>Example Task 1 approach: post-speech recognition error correction with different techniques on LLMs.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Task Description</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Task 1 on LLM for Post-ASR Output Correction</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Background:</span> Language model (LM) rescoring has been employed widely and for a variety of ASR technologies to improve ASR results, usually achieving good performance gains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. In this approach, an external LM is trained separately and used to re-score the N-best hypotheses generated by the ASR system. While text error correction (TEC) has been explored <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, ASR error correction is distinct due to the variability and distinct patterns of spoken language <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Neural models have been used widely with E2E models for text error correction or normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. These models often use beam search to generate new estimates, and can usually handle text normalization and denormalization of spelling errors.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Motivation:</span> As shown in Fig.¬†<a href="#S3.F2" title="Figure 2 ‚Ä£ 3 Challenge Description ‚Ä£ Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, with Task 1 we aim to explore the limits of ASR-LLM error correction, as well as how best to utilize the ambiguity conveyed by N-best output.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.5" class="ltx_p"><span id="S4.SS1.p3.5.1" class="ltx_text ltx_font_bold">N-best dataset:</span> The N-best open source corpus HyPoradise¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> will be made open-source under the MIT license. This includes HyPoradise training sets (<math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="316.8" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mn id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">316.8</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><cn type="float" id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">316.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">316.8</annotation></semantics></math>k pairs), development sets such as Librispeech-test-clean (<math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="2.6" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mn id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">2.6</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><cn type="float" id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">2.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">2.6</annotation></semantics></math>k pairs) and WSJ-dev93 (<math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="503" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mn id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">503</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><cn type="integer" id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">503</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">503</annotation></semantics></math> pairs), and evaluation sets including Librispeech-test-other (<math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="2.9k" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><mrow id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml"><mn id="S4.SS1.p3.4.m4.1.1.2" xref="S4.SS1.p3.4.m4.1.1.2.cmml">2.9</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p3.4.m4.1.1.1" xref="S4.SS1.p3.4.m4.1.1.1.cmml">‚Äã</mo><mi id="S4.SS1.p3.4.m4.1.1.3" xref="S4.SS1.p3.4.m4.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><apply id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"><times id="S4.SS1.p3.4.m4.1.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1.1"></times><cn type="float" id="S4.SS1.p3.4.m4.1.1.2.cmml" xref="S4.SS1.p3.4.m4.1.1.2">2.9</cn><ci id="S4.SS1.p3.4.m4.1.1.3.cmml" xref="S4.SS1.p3.4.m4.1.1.3">ùëò</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">2.9k</annotation></semantics></math> pairs) and WSJ-dev93 (<math id="S4.SS1.p3.5.m5.1" class="ltx_Math" alttext="333" display="inline"><semantics id="S4.SS1.p3.5.m5.1a"><mn id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml">333</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><cn type="integer" id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1">333</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.1c">333</annotation></semantics></math> pairs).</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Baseline:</span> We provide pretrained 1st-pass and 2nd-pass models. The details of existing engineering pipelines are listed below. Training code has been released<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/Hypotheses-Paradise/Hypo2Trans</span></span></span></span> and the pretrained LLaMA2-7B model<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://huggingface.co/GenSEC-LLM</span>.</span></span></span>
has been released.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">Evaluation:</span> The challenge participants are allowed to apply their own 2nd-pass model to the provided ASR hypotheses decoded by beam search using Whisper.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.5.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Task-1 WER (<math id="S4.T1.2.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T1.2.m1.1b"><mo id="S4.T1.2.m1.1.1" xref="S4.T1.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.m1.1c"><csymbol cd="latexml" id="S4.T1.2.m1.1.1.cmml" xref="S4.T1.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.m1.1d">\%</annotation></semantics></math>) of post-ASR LM correction on the HyPoradise¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> dataset.</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.3.2.1" class="ltx_tr">
<td id="S4.T1.3.2.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S4.T1.3.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.3.2.1.2.1" class="ltx_text ltx_font_bold">train</span></th>
<th id="S4.T1.3.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.3.2.1.3.1" class="ltx_text ltx_font_bold">test</span></th>
</tr>
<tr id="S4.T1.3.3.2" class="ltx_tr">
<th id="S4.T1.3.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">Whisper-1.5B (first-pass) w/o LM</th>
<th id="S4.T1.3.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">10.43</th>
<th id="S4.T1.3.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">11.82</th>
</tr>
<tr id="S4.T1.3.1" class="ltx_tr">
<th id="S4.T1.3.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">
<math id="S4.T1.3.1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.T1.3.1.1.m1.1a"><mi id="S4.T1.3.1.1.m1.1.1" xref="S4.T1.3.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.T1.3.1.1.m1.1b"><ci id="S4.T1.3.1.1.m1.1.1.cmml" xref="S4.T1.3.1.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.1.1.m1.1c">N</annotation></semantics></math>-best Oracle</th>
<th id="S4.T1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">9.61</th>
<th id="S4.T1.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">9.32</th>
</tr>
<tr id="S4.T1.3.4.3" class="ltx_tr">
<td id="S4.T1.3.4.3.1" class="ltx_td ltx_align_right ltx_border_tt">Reranking LM: T5-750M</td>
<td id="S4.T1.3.4.3.2" class="ltx_td ltx_align_center ltx_border_tt">9.90</td>
<td id="S4.T1.3.4.3.3" class="ltx_td ltx_align_center ltx_border_tt">9.74</td>
</tr>
<tr id="S4.T1.3.5.4" class="ltx_tr">
<td id="S4.T1.3.5.4.1" class="ltx_td ltx_align_right ltx_border_t">Correction LM: T5-750M</td>
<td id="S4.T1.3.5.4.2" class="ltx_td ltx_align_center ltx_border_t">9.21</td>
<td id="S4.T1.3.5.4.3" class="ltx_td ltx_align_center ltx_border_t">9.05</td>
</tr>
<tr id="S4.T1.3.6.5" class="ltx_tr">
<td id="S4.T1.3.6.5.1" class="ltx_td ltx_align_right ltx_border_t">Correction LM: LLaMA-13B</td>
<td id="S4.T1.3.6.5.2" class="ltx_td ltx_align_center ltx_border_t">8.62</td>
<td id="S4.T1.3.6.5.3" class="ltx_td ltx_align_center ltx_border_t">8.63</td>
</tr>
<tr id="S4.T1.3.7.6" class="ltx_tr">
<td id="S4.T1.3.7.6.1" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">Correction LM: LLaMaA2-7B</td>
<td id="S4.T1.3.7.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">8.71</td>
<td id="S4.T1.3.7.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">8.33</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">The WER of the corrected hypotheses is used for evaluation. This WER is compared to two
‚Äúoracle‚Äù WERs calculated from the N-best inputs, namely, 1) the lowest WER achievable by picking the best hypothesis from each N-best list, and 2) the compositional oracle method ocp: the achievable WER using ‚Äúall tokens‚Äù in the N-best hypothesis list. The former can be viewed as a lower bound on re-ranking methods, while the latter denotes the lower bound using elements already occurring in the list. To understand the effect of text normalization (punctuation and capitalization, P&amp;C) on ASR performance, both normalized and unnormalized (P&amp;C) WERs are reported.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Task-1 dataset statistics: number of hypothesis-transcription pairs and average utterance length.
</figcaption>
<div id="S4.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:233.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.2pt,9.3pt) scale(0.926317222345672,0.926317222345672) ;">
<table id="S4.T2.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.3.1.1.1" class="ltx_tr">
<th id="S4.T2.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" colspan="2">Domain</th>
<td id="S4.T2.3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.3.1.1.1.2.1" class="ltx_text">Training Set</span></td>
<td id="S4.T2.3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.3.1.1.1.3.1" class="ltx_text"># Pairs</span></td>
<td id="S4.T2.3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T2.3.1.1.1.4.1" class="ltx_text">Length</span></td>
<td id="S4.T2.3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.3.1.1.1.5.1" class="ltx_text">Test Set</span></td>
<td id="S4.T2.3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.3.1.1.1.6.1" class="ltx_text"># Pairs</span></td>
<td id="S4.T2.3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.3.1.1.1.7.1" class="ltx_text">Length</span></td>
</tr>
<tr id="S4.T2.3.1.2.2" class="ltx_tr">
<th id="S4.T2.3.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Source</th>
<th id="S4.T2.3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Category</th>
</tr>
<tr id="S4.T2.3.1.3.3" class="ltx_tr">
<th id="S4.T2.3.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T2.3.1.3.3.1.1" class="ltx_text">LibriSpeech</span></th>
<th id="S4.T2.3.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.3.1.3.3.2.1" class="ltx_text">Audiobooks</span></th>
<td id="S4.T2.3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T2.3.1.3.3.3.1" class="ltx_text"><em id="S4.T2.3.1.3.3.3.1.1" class="ltx_emph ltx_font_italic">train-960</em></span></td>
<td id="S4.T2.3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T2.3.1.3.3.4.1" class="ltx_text">88,200</span></td>
<td id="S4.T2.3.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.3.1.3.3.5.1" class="ltx_text">33.7</span></td>
<td id="S4.T2.3.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.3.3.6.1" class="ltx_emph ltx_font_italic">test-clean</em></td>
<td id="S4.T2.3.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">2,620</td>
<td id="S4.T2.3.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">20.1</td>
</tr>
<tr id="S4.T2.3.1.4.4" class="ltx_tr">
<td id="S4.T2.3.1.4.4.1" class="ltx_td ltx_align_center"><em id="S4.T2.3.1.4.4.1.1" class="ltx_emph ltx_font_italic">test-other</em></td>
<td id="S4.T2.3.1.4.4.2" class="ltx_td ltx_align_center">2,939</td>
<td id="S4.T2.3.1.4.4.3" class="ltx_td ltx_align_center">17.8</td>
</tr>
<tr id="S4.T2.3.1.5.5" class="ltx_tr">
<th id="S4.T2.3.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">CHiME4</th>
<th id="S4.T2.3.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Noise</th>
<td id="S4.T2.3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.5.5.3.1" class="ltx_emph ltx_font_italic">train</em></td>
<td id="S4.T2.3.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">8,738</td>
<td id="S4.T2.3.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.0</td>
<td id="S4.T2.3.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.5.5.6.1" class="ltx_emph ltx_font_italic">test-real</em></td>
<td id="S4.T2.3.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">1,320</td>
<td id="S4.T2.3.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t">16.4</td>
</tr>
<tr id="S4.T2.3.1.6.6" class="ltx_tr">
<th id="S4.T2.3.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T2.3.1.6.6.1.1" class="ltx_text">WSJ</span></th>
<th id="S4.T2.3.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.3.1.6.6.2.1" class="ltx_text">Business news</span></th>
<td id="S4.T2.3.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T2.3.1.6.6.3.1" class="ltx_text"><em id="S4.T2.3.1.6.6.3.1.1" class="ltx_emph ltx_font_italic">train-si284</em></span></td>
<td id="S4.T2.3.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T2.3.1.6.6.4.1" class="ltx_text">37,514</span></td>
<td id="S4.T2.3.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.3.1.6.6.5.1" class="ltx_text">17.5</span></td>
<td id="S4.T2.3.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.6.6.6.1" class="ltx_emph ltx_font_italic">dev93</em></td>
<td id="S4.T2.3.1.6.6.7" class="ltx_td ltx_align_center ltx_border_t">503</td>
<td id="S4.T2.3.1.6.6.8" class="ltx_td ltx_align_center ltx_border_t">16.7</td>
</tr>
<tr id="S4.T2.3.1.7.7" class="ltx_tr">
<td id="S4.T2.3.1.7.7.1" class="ltx_td ltx_align_center"><em id="S4.T2.3.1.7.7.1.1" class="ltx_emph ltx_font_italic">eval92</em></td>
<td id="S4.T2.3.1.7.7.2" class="ltx_td ltx_align_center">333</td>
<td id="S4.T2.3.1.7.7.3" class="ltx_td ltx_align_center">17.3</td>
</tr>
<tr id="S4.T2.3.1.8.8" class="ltx_tr">
<th id="S4.T2.3.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">SwitchBoard</th>
<th id="S4.T2.3.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Telephone</th>
<td id="S4.T2.3.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.8.8.3.1" class="ltx_emph ltx_font_italic">train</em></td>
<td id="S4.T2.3.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t">36,539</td>
<td id="S4.T2.3.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.8</td>
<td id="S4.T2.3.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.8.8.6.1" class="ltx_emph ltx_font_italic">eval2000</em></td>
<td id="S4.T2.3.1.8.8.7" class="ltx_td ltx_align_center ltx_border_t">2,000</td>
<td id="S4.T2.3.1.8.8.8" class="ltx_td ltx_align_center ltx_border_t">11.8</td>
</tr>
<tr id="S4.T2.3.1.9.9" class="ltx_tr">
<th id="S4.T2.3.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">CommonVoice</th>
<th id="S4.T2.3.1.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Accented English</th>
<td id="S4.T2.3.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.9.9.3.1" class="ltx_emph ltx_font_italic">train-accent</em></td>
<td id="S4.T2.3.1.9.9.4" class="ltx_td ltx_align_center ltx_border_t">49,758</td>
<td id="S4.T2.3.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.5</td>
<td id="S4.T2.3.1.9.9.6" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.9.9.6.1" class="ltx_emph ltx_font_italic">test-accent</em></td>
<td id="S4.T2.3.1.9.9.7" class="ltx_td ltx_align_center ltx_border_t">2,000</td>
<td id="S4.T2.3.1.9.9.8" class="ltx_td ltx_align_center ltx_border_t">10.5</td>
</tr>
<tr id="S4.T2.3.1.10.10" class="ltx_tr">
<th id="S4.T2.3.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Tedlium-3</th>
<th id="S4.T2.3.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">TED talk</th>
<td id="S4.T2.3.1.10.10.3" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.10.10.3.1" class="ltx_emph ltx_font_italic">train</em></td>
<td id="S4.T2.3.1.10.10.4" class="ltx_td ltx_align_center ltx_border_t">47,500</td>
<td id="S4.T2.3.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.6</td>
<td id="S4.T2.3.1.10.10.6" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.10.10.6.1" class="ltx_emph ltx_font_italic">test</em></td>
<td id="S4.T2.3.1.10.10.7" class="ltx_td ltx_align_center ltx_border_t">2,500</td>
<td id="S4.T2.3.1.10.10.8" class="ltx_td ltx_align_center ltx_border_t">12.6</td>
</tr>
<tr id="S4.T2.3.1.11.11" class="ltx_tr">
<th id="S4.T2.3.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">LRS2</th>
<th id="S4.T2.3.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">BBC audio</th>
<td id="S4.T2.3.1.11.11.3" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.11.11.3.1" class="ltx_emph ltx_font_italic">train</em></td>
<td id="S4.T2.3.1.11.11.4" class="ltx_td ltx_align_center ltx_border_t">42,940</td>
<td id="S4.T2.3.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.6</td>
<td id="S4.T2.3.1.11.11.6" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.11.11.6.1" class="ltx_emph ltx_font_italic">test</em></td>
<td id="S4.T2.3.1.11.11.7" class="ltx_td ltx_align_center ltx_border_t">2,259</td>
<td id="S4.T2.3.1.11.11.8" class="ltx_td ltx_align_center ltx_border_t">7.6</td>
</tr>
<tr id="S4.T2.3.1.12.12" class="ltx_tr">
<th id="S4.T2.3.1.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">ATIS</th>
<th id="S4.T2.3.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Airline info.</th>
<td id="S4.T2.3.1.12.12.3" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.12.12.3.1" class="ltx_emph ltx_font_italic">train</em></td>
<td id="S4.T2.3.1.12.12.4" class="ltx_td ltx_align_center ltx_border_t">3,964</td>
<td id="S4.T2.3.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.4</td>
<td id="S4.T2.3.1.12.12.6" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.12.12.6.1" class="ltx_emph ltx_font_italic">test</em></td>
<td id="S4.T2.3.1.12.12.7" class="ltx_td ltx_align_center ltx_border_t">809</td>
<td id="S4.T2.3.1.12.12.8" class="ltx_td ltx_align_center ltx_border_t">11.3</td>
</tr>
<tr id="S4.T2.3.1.13.13" class="ltx_tr">
<th id="S4.T2.3.1.13.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">CORAAL</th>
<th id="S4.T2.3.1.13.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Interview</th>
<td id="S4.T2.3.1.13.13.3" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.13.13.3.1" class="ltx_emph ltx_font_italic">train</em></td>
<td id="S4.T2.3.1.13.13.4" class="ltx_td ltx_align_center ltx_border_t">1,728</td>
<td id="S4.T2.3.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.2</td>
<td id="S4.T2.3.1.13.13.6" class="ltx_td ltx_align_center ltx_border_t"><em id="S4.T2.3.1.13.13.6.1" class="ltx_emph ltx_font_italic">test</em></td>
<td id="S4.T2.3.1.13.13.7" class="ltx_td ltx_align_center ltx_border_t">100</td>
<td id="S4.T2.3.1.13.13.8" class="ltx_td ltx_align_center ltx_border_t">24.0</td>
</tr>
<tr id="S4.T2.3.1.14.14" class="ltx_tr">
<th id="S4.T2.3.1.14.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" colspan="2">Total</th>
<td id="S4.T2.3.1.14.14.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><em id="S4.T2.3.1.14.14.2.1" class="ltx_emph ltx_font_italic">train</em></td>
<td id="S4.T2.3.1.14.14.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">316,881</td>
<td id="S4.T2.3.1.14.14.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">18.1</td>
<td id="S4.T2.3.1.14.14.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><em id="S4.T2.3.1.14.14.5.1" class="ltx_emph ltx_font_italic">test</em></td>
<td id="S4.T2.3.1.14.14.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">17,383</td>
<td id="S4.T2.3.1.14.14.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">14.1</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Task 2: Post-ASR Speaker Tagging Correction</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Background:</span> While the use of lexical cues in speaker diarization, speaker turn detection, and speaker segmentation has been explored previously, it is still less commonly used than acoustic-only speaker diarization. Early studies in this area, such as those presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, utilized linguistic patterns to identify speakers during the diarization process.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Several studies have enhanced speaker segmentation and clustering accuracy by integrating ASR output to leverage lexical cues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Furthermore, lexical cues can be incorporated into speaker diarization by combining speaker turn probabilities based on both audio and text during the clustering phase <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Alternatively, spoken words and speaker channels can be decoded jointly, thus utilizing lexical cues implicitly for diarization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">More recently, the study presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> introduced semantic information through neural embeddings generated by a spoken language processing (SLP) unit. Subsequently, a multimodal (audio-text) speaker change detector was proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, along with a speaker error correction (SEC) system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> based on a pretrained language model.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Due to the recent popularity of LLMs, the multi-speaker ASR and speaker diarization community has also begun employing LLMs to enhance performance. One framework established for this purpose fine-tunes PaLM 2-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> to correct speaker diarization errors from GCP‚Äôs Universal Speech Model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, which uses Turn-to-Diarize <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for speaker diarization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. More recently, an ensemble of LLMs has been proposed to correct speaker diarization outputs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Motivation:</span>
As discussed for Task 1, LM rescoring for ASR has been widely studied and adopted, as external language models can be trained on relatively larger text-only datasets. Numerous studies have demonstrated the benefits of using LLMs for speaker diarization correction. Despite an abundance of research, there has been no standardized evaluation of multi-speaker error correction systems. We believe that our GenSEC Challenge Task 2 is timely in filling this gap. To lower the bar for entry, we focus on the text modality, excluding acoustic or visual modalities in this first round. Therefore, in Task 2, we employ the system proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> without utilizing acoustic information from the (acoustic-only) speaker diarization system.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2409.09785/assets/figs/bsd_example_pic.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text ltx_font_bold">Fig.¬†3</span>: </span>Example Task 2 approach based on beam-search decoding for speaker tagging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite></figcaption>
</figure>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">Datasets:</span> The DiPCo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, Mixer6 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, AMI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, and CallHome American English Speech (CHAES) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> corpora have been divided into training, development, and evaluation sets. The session names have been anonymized to prevent participants from exploiting the publicly available ground-truth data. Additionally, the evaluation scripts display the total word count and the count of erroneous words, to verify whether the output transcripts have altered the total number of words. In total, there are 222 training samples, 13 development samples, and 11 evaluation samples. The dataset is accessible through Huggingface.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://huggingface.co/datasets/GenSEC-LLM/SLT-Task2-Post-ASR-Speaker-Tagging</span></span></span></span></p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>Task-2 cpWER (%) of the source files and the baseline system for text-only speaker recognition.</figcaption>
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.1.1" class="ltx_tr">
<th id="S4.T3.3.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt">System</th>
<th id="S4.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.3.1.1.2.1" class="ltx_text ltx_font_bold">dev</span></th>
<th id="S4.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.3.1.1.3.1" class="ltx_text ltx_font_bold">eval</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.2.1" class="ltx_tr">
<th id="S4.T3.3.2.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">Source Transcript</th>
<td id="S4.T3.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t">24.65</td>
<td id="S4.T3.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t">28.45</td>
</tr>
<tr id="S4.T3.3.3.2" class="ltx_tr">
<th id="S4.T3.3.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t">Task-2 Baseline</th>
<td id="S4.T3.3.3.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">24.54</td>
<td id="S4.T3.3.3.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">28.37</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2409.09785/assets/figs/overall_dataflow.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text ltx_font_bold">Fig.¬†4</span>: </span>Dataflow for the Task 2 baseline. Note that the acoustic-only diarization probability values are set to fixed values.</figcaption>
</figure>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p"><span id="S4.SS2.p7.1.1" class="ltx_text ltx_font_bold">Baseline:</span>
We generated the speaker-annotated transcripts from the system proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> as a baseline system,<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/tango4j/llm_speaker_tagging</span></span></span></span> based on NeMo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> speaker diarization and NeMo ASR models. We provide the postprocessing method proposed in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, where we replace the LLM with n-gram language models. Since the n-gram-based system shows similar results and has low computational demands, we use this n-gram baseline to gauge the performance of beam-search-based speaker tag correction. Fig.¬†<a href="#S4.F3" title="Figure 3 ‚Ä£ 4.2 Task 2: Post-ASR Speaker Tagging Correction ‚Ä£ 4 Task Description ‚Ä£ Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows how beam search decoding can correct the speaker tagging. To mask out acoustic information, the speaker probability values in Fig.¬†<a href="#S4.F4" title="Figure 4 ‚Ä£ 4.2 Task 2: Post-ASR Speaker Tagging Correction ‚Ä£ 4 Task Description ‚Ä£ Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> are all fixed at 0.96. Table¬†<a href="#S4.T3" title="Table 3 ‚Ä£ 4.2 Task 2: Post-ASR Speaker Tagging Correction ‚Ä£ 4 Task Description ‚Ä£ Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the accuracy of the baseline for development and evaluation sets.</p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p"><span id="S4.SS2.p8.1.1" class="ltx_text ltx_font_bold">Evaluation:</span> We employ concatenated minimum permutation word error rate (cpWER), as presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. cpWER is calculated by concatenating the speaker-wise transcripts for every label permutation and selecting the permutation that results in the lowest WER, using the open-source and publicly available MeetEval¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> multi-speaker ASR evaluation toolkit. Additionally, we provide a Hugging Face-style leaderboard<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://huggingface.co/spaces/GenSEC-LLM/task2_speaker_tagging_leaderboard</span></span></span></span> for challenge participants to upload and evaluate their submissions.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Task 3: Post-ASR Speech Emotion Recognition</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Background:</span> Text-based SER has advanced significantly over the past decade. However, its use in real-world applications remains rare. One reason is that the majority of SER research relies on human annotation, i.e., manual transcripts. In contrast, even for elicited emotion corpora, transcripts from a state-of-the-art ASR system can result in high WERs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, meaning that few findings obtained in the lab can be replicated in the wild. Moreover, SER on ASR transcripts is an understudied topic. Traditionally, researchers have considered confidence scores of recognized words <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, ASR error correction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, as well as fusion with audio information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> to mitigate the side effects of ASR errors. Still, there is a lack of comprehensive studies covering diverse situations (i.e., corpora, metrics, WERs, fusion techniques). With the rise of LLMs, it has become feasible to perform SER on ASR transcripts with simple prompting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. This emerging approach, however, has not been established as a reliable solution given the uneven performance with different prompting templates and the general lack of explainability of LLM outputs.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Motivation:</span> Using text input only, without access to acoustic information, is a good starting point for exploring LLMs for SER, especially given that most LLMs are text-based. Insights gained about text-based SER, including handling of ASR errors, can be a foundation for future evaluations incorporating acoustic features. Potential future tasks could include ASR-error-robust multimodal fusion, enhancing word embeddings from ASR transcripts with discrete speech units, and developing ASR-integrated multimodal LLMs based on spoken language.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Dataset:</span> We use the public IEMOCAP dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. Speech transcripts from eleven ASR models (Wav2vec2, HuBERT, Whisper, etc.) are provided for each audio segment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. We ask participants to predict four emotion classes: angry, happy (combined with excited), neutral, and sad, for each segment. All segments are presented in the order of the conversation based on the timestamp. Participants are encouraged to use conversational context to improve emotion prediction.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">An exemplary data entry is shown in Fig.¬†<a href="#S4.F5" title="Figure 5 ‚Ä£ 4.3 Task 3: Post-ASR Speech Emotion Recognition ‚Ä£ 4 Task Description ‚Ä£ Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, where <span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_italic">need_prediction</span> indicates whether this utterance should be included in the prediction procedure. ‚Äúyes‚Äù denotes the utterances labeled with the four emotion classes and ‚Äúno‚Äù denotes all other utterances. Note that we have removed the utterances that have no human annotations. The key <span id="S4.SS3.p4.1.2" class="ltx_text ltx_font_italic">emotion</span> indicates the emotion label of the utterance. The key <span id="S4.SS3.p4.1.3" class="ltx_text ltx_font_italic">id</span> indicates the utterance ID, which is also the name of the audio file in IEMOCAP dataset. The ID is exactly the same as the raw ID in IEMOCAP. The key <span id="S4.SS3.p4.1.4" class="ltx_text ltx_font_italic">speaker</span> indicates the speaker of the utterance.
The key <span id="S4.SS3.p4.1.5" class="ltx_text ltx_font_italic">groundtruth</span> indicates the original human transcription provided by IEMOCAP while the remaining ten keys indicate the ASR transcription generated by the various ASR models.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2409.09785/assets/x3.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="276" height="293" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text ltx_font_bold">Fig.¬†5</span>: </span>Example of a data entry of Task 3.</figcaption>
</figure>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p"><span id="S4.SS3.p5.1.1" class="ltx_text ltx_font_bold">Baseline:</span> We provide two performance baselines with ASR transcripts from Whisper-tiny used as the text input: one with an LLM-based approach using GPT-3.5-turbo,<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Version: GPT-3.5-turbo-0125; Context window: 16,385 tokens; Training data: up to Sep 2021</span></span></span> the other with a traditional approach based on a deep learning model. For the GPT-3.5-turbo approach, we performed zero-shot prediction with a context window of three (only previous utterances allowed), with code available<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/YuanGongND/llm_speech_emotion_challenge</span></span></span></span> to participants as a reference. For the deep learning-based model, a two-layer feed-forward network was trained following the standard five-fold cross-validation of IEMOCAP. The first layer encodes RoBERTa output of dimension 768 into hidden states of dimension 128, and the second further encodes it into a dimension of 16. ReLU is used as the activation function between the layers. The dataset statistics and our baseline results are given in Table¬†<a href="#S4.T4" title="Table 4 ‚Ä£ 4.3 Task 3: Post-ASR Speech Emotion Recognition ‚Ä£ 4 Task Description ‚Ä£ Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text ltx_font_bold">Table 4</span>: </span>Task-3 dataset statistics and baseline unweighted accuracies (%).</figcaption>
<div id="S4.T4.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:138.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(76.2pt,-24.4pt) scale(1.54236707210873,1.54236707210873) ;">
<table id="S4.T4.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.3.1.1.1" class="ltx_tr">
<th id="S4.T4.3.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T4.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.3.1.1.1.2.1" class="ltx_text ltx_font_bold">Training set</span></th>
<th id="S4.T4.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.3.1.1.1.3.1" class="ltx_text ltx_font_bold">Test set</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.3.1.2.1" class="ltx_tr">
<th id="S4.T4.3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.3.1.2.1.1.1" class="ltx_text ltx_font_bold">Number of samples (all)</span></th>
<td id="S4.T4.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">5,525</td>
<td id="S4.T4.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">4,730</td>
</tr>
<tr id="S4.T4.3.1.3.2" class="ltx_tr">
<th id="S4.T4.3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.3.1.3.2.1.1" class="ltx_text ltx_font_bold">Number of samples (four emotions)</span></th>
<td id="S4.T4.3.1.3.2.2" class="ltx_td ltx_align_center">2,577</td>
<td id="S4.T4.3.1.3.2.3" class="ltx_td ltx_align_center">2,923</td>
</tr>
<tr id="S4.T4.3.1.4.3" class="ltx_tr">
<th id="S4.T4.3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.3.1.4.3.1.1" class="ltx_text ltx_font_bold">Baseline accuracy (GPT3.5-turbo)</span></th>
<td id="S4.T4.3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">44.70</td>
<td id="S4.T4.3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">55.18</td>
</tr>
<tr id="S4.T4.3.1.5.4" class="ltx_tr">
<th id="S4.T4.3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T4.3.1.5.4.1.1" class="ltx_text ltx_font_bold">Baseline accuracy (traditional)</span></th>
<td id="S4.T4.3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">62.34</td>
<td id="S4.T4.3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">51.08</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS3.p6" class="ltx_para">
<p id="S4.SS3.p6.1" class="ltx_p">For the method based on GPT3.5-turbo, the accuracy is 44.70% on the training set and 55.18% on the test set. This significant discrepancy is reasonable since the training set contains scripted dialogs (which may not align with emotion labels), while the test set is more spontaneous. For the traditional deep learning approach, the accuracy is 62.34% on the training set and 51.08% on the test set. This discrepancy is also plausible, considering duplicate textual scripts in the training set, which results in overlap between the training and development subsets of the training set.</p>
</div>
<div id="S4.SS3.p7" class="ltx_para">
<p id="S4.SS3.p7.1" class="ltx_p"><span id="S4.SS3.p7.1.1" class="ltx_text ltx_font_bold">Evaluation:</span> We use unweighted four-class accuracy (number of correctly predicted samples / total number of samples). We will release a training set and a test set. Participants can use the training set to develop their methods and tune hyperparameters. The test set does not come with emotion labels or ground-truth transcription and is strictly disallowed for use in model development. We will rank the models based on their accuracy on the test set, but will also further evaluate the models with a separate unpublished test set to assess generalization. Participants are free to use any LLM (such as GPT or LLaMA) or non-LLM methods (traditional text-based emotion classifiers) based on the provided ASR transcriptions. For fairness considerations, participants should not use any audio data (including audio waveforms or acoustic features) or transcribe speech using their own ASR model. Participants are allowed to use additional training datasets, as long as they are specified and publicly available, but they must not include IEMOCAP. To encourage innovation, we do not place any other restrictions on the methods used, as long as they are automated.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have created the GenSEC challenge to probe the capabilities of large language models for post-processing of ASR outputs. By standardizing the tasks, datasets and metrics, we hope to create a community of researchers that will advance the state of the art in speech processing systems by loose coupling of off-the-shelf ASR systems and techniques based on generative LMs, such as instruction prompting, text generation and in-context learning. We propose three tasks that go beyond speech transcription correction and include text-based speaker diarization correction and emotion recognition.
Unlike traditional models, LLMs provide the potential for improving these tasks by leveraging linguistic and world knowledge learned during pretraining, and by taking advantage of long conversational context.
</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Frederick Jelinek,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">‚ÄúContinuous speech recognition by statistical methods,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 1976.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Ostendorf, A.¬†Kannan, S.¬†Austin, O.¬†Kimball, R.¬†Schwartz, and J.¬†R. Rohlicek,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">‚ÄúIntegration of diverse recognition methodologies through reevaluation of N-best sentence hypotheses,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Workshop on Speech and Natural Language</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 1991.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">‚ÄúInternal language model estimation for domain-adaptive end-to-end speech recognition,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">SLT</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Mohammadreza Ghodsi, Xiaofeng Liu, James Apfel, Rodrigo Cabrera, and Eugene Weinstein,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">‚ÄúRnn-transducer with stateless prediction network,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Yukun Ma, Chong Zhang, Qian Chen, Wen Wang, and Bin Ma,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">‚ÄúTuning large language model for speech recognition with mixed-scale re-tokenization,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Signal Processing Letters</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Yu¬†Yu, Chao-Han¬†Huck Yang, Jari Kolehmainen, et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">‚ÄúLow-rank adaptation of large language model rescoring for parameter-efficient speech recognition,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ASRU</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2023, pp. 1‚Äì8.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Chao-Han¬†Huck Yang, Yile Gu, Yi-Chieh Liu, Shalini Ghosh, Ivan Bulyko, and Andreas Stolcke,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">‚ÄúGenerative speech recognition error correction with large language models and task-activating prompting,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ASRU</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Srijith Radhakrishnan, Chao-Han Yang, Sumeer Khan, Rohit Kumar, Narsis Kiani, David Gomez-Cabrero, and Jesper Tegn√©r,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">‚ÄúWhispering LLaMA: A cross-modal generative error correction framework for speech recognition,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Chen Chen, Yuchen Hu, Chao-Han¬†Huck Yang, Sabato¬†Marco Siniscalchi, Pin-Yu Chen, and Eng-Siong Chng,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">‚ÄúHyPoradise: An open baseline for generative speech recognition with large language models,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Yuchen Hu, Chen Chen, Chao-Han¬†Huck Yang, Ruizhe Li, Dong Zhang, Zhehuai Chen, and Eng¬†Siong Chng,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">‚ÄúGenTranslate: Large language models are generative multilingual speech and machine translators,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
David Chan, Austin Myers, Sudheendra Vijayanarasimhan, David Ross, and John Canny,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">‚ÄúIC3: Image captioning by committee consensus,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Yusuke Hirota, Ryo Hachiuma, Chao-Han¬†Huck Yang, and Yuta Nakashima,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">‚ÄúFrom descriptive richness to bias: Unveiling the dark side of generative image caption enrichment,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2406.13912</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Andreas Stolcke et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">‚ÄúSRILM-an extensible language modeling toolkit.,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">INTERSPEECH</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2002.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Frank Seide and Amit Agarwal,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">‚ÄúCNTK: Microsoft‚Äôs open-source deep-learning toolkit,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">KDD</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">‚ÄúThe Kaldi speech recognition toolkit,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ASRU</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2011.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">‚ÄúAudioGPT: Understanding and generating speech, music, sound, and talking head,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Andreas Stolcke, Barry Chen, Horacio Franco, Venkata Ramana¬†Rao Gadde, Martin Graciarena, Mei-Yuh Hwang, Katrin Kirchhoff, Arindam Mandal, Nelson Morgan, Xin Lei, et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">‚ÄúRecent innovations in speech-to-text transcription at SRI-ICSI-UW,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Audio, Speech, and Language Processing</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2006.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Ebru Arisoy, Abhinav Sethy, Bhuvana Ramabhadran, and Stanley Chen,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">‚ÄúBidirectional recurrent neural network language models for automatic speech recognition,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2015.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Michael¬†L Seltzer, Andreas Stolcke, Dong Yu, and Geoffrey Zweig,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">‚ÄúToward human parity in conversational speech recognition,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Daniel Dahlmeier and Hwee¬†Tou Ng,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">‚ÄúBetter evaluation for grammatical error correction,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NAACL-HLT</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Al√´na Aks√´nova, Daan van Esch, James Flynn, and Pavel Golik,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">‚ÄúHow might we create better benchmarks for speech recognition?,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">1st Workshop on Benchmarking: Past, Present and Future</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Hao Zhang, Richard Sproat, Axel¬†H Ng, Felix Stahlberg, Xiaochang Peng, Kyle Gorman, and Brian Roark,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">‚ÄúNeural models of text normalization for speech applications,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computational Linguistics</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Jinxi Guo, Tara¬†N Sainath, and Ron¬†J Weiss,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">‚ÄúA spelling correction model for end-to-end speech recognition,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Rao Ma, Mark¬†JF Gales, Kate¬†M Knill, and Mengjie Qian,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">‚ÄúN-best t5: Robust asr error correction using multiple input hypotheses and constrained decoding space,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.00456</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Leonardo Canseco-Rodriguez, Lori Lamel, and Jean-Luc Gauvain,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">‚ÄúSpeaker diarization from speech transcripts,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICSLP</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2004.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Leonardo Canseco, Lori Lamel, and J-L Gauvain,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">‚ÄúA comparative study using manual and automatic transcriptions for diarization,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ASRU</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2005.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Tae¬†Jin Park and Panayiotis Georgiou,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">‚ÄúMultimodal speaker segmentation and diarization using lexical and acoustic cues via sequence to sequence neural networks,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">INTERSPEECH</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Wei Xia, Han Lu, Quan Wang, Anshuman Tripathi, Yiling Huang, Ignacio¬†Lopez Moreno, and Hasim Sak,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">‚ÄúTurn-to-diarize: Online speaker diarization constrained by transformer transducer speaker turn detection,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Aparna Khare, Eunjung Han, Yuguang Yang, and Andreas Stolcke,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">‚ÄúASR-aware end-to-end neural diarization,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2022, pp. 8092‚Äì8096.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Tae¬†Jin Park, Kyu¬†J Han, Jing Huang, Xiaodong He, Bowen Zhou, Panayiotis Georgiou, and Shrikanth Narayanan,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">‚ÄúSpeaker diarization with lexical information,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">INTERSPEECH</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Laurent El¬†Shafey, Hagen Soltau, and Izhak Shafran,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">‚ÄúJoint speech recognition and speaker diarization via sequence transduction,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">INTERSPEECH</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Naoyuki Kanda, Xiong Xiao, Yashesh Gaur, Xiaofei Wang, Zhong Meng, Zhuo Chen, and Takuya Yoshioka,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">‚ÄúTranscribe-to-diarize: Neural speaker diarization for unlimited number of speakers using end-to-end speaker-attributed ASR,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2022, pp. 8082‚Äì8086.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Luyao Cheng, Siqi Zheng, Zhang Qinglin, Hui Wang, Yafeng Chen, and Qian Chen,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">‚ÄúExploring speaker-related information in spoken language understanding for better speaker diarization,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Findings of ACL 2023</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Jee-weon Jung et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">‚ÄúEncoder-decoder multimodal speaker change detection,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">INTERSPEECH</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Rohit Paturi et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">‚ÄúLexical speaker error correction: Leveraging language models for speaker diarization error correction,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">INTERSPEECH</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung¬†Won Chung, Charles Sutton, Sebastian Gehrmann, et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">‚ÄúPalm: Scaling language modeling with pathways,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, vol. 24, no. 240, pp. 1‚Äì113, 2023.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Yu¬†Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo¬†Li, Vera Axelrod, Gary Wang, et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">‚ÄúGoogle USM: Scaling automatic speech recognition beyond 100 languages,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.01037</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Quan Wang, Yiling Huang, Guanlong Zhao, Evan Clark, Wei Xia, and Hank Liao,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">‚ÄúDiarizationLM: Speaker diarization post-processing with large language models,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2401.03506</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Georgios Efstathiadis, Vijay Yadav, and Anzar Abbas,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">‚ÄúLLM-based speaker diarization correction: A generalizable approach,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2406.04927</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Tae¬†Jin Park, Kunal Dhawan, Nithin Koluguri, and Jagadeesh Balam,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">‚ÄúEnhancing speaker diarization with large language models: A contextual beam search approach,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Maarten Van¬†Segbroeck, Ahmed Zaid, Ksenia Kutsenko, Cirenia Huerta, Tinh Nguyen, Xuewen Luo, Bj√∂rn Hoffmeister, Jan Trmal, Maurizio Omologo, and Roland Maas,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">‚ÄúDiPCo‚ÄìDinner Party corpus,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.13447</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Linda Brandschain, David Graff, Christopher Cieri, Kevin Walker, Chris Caruso, and A¬†Neely,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">‚ÄúThe Mixer 6 corpus: Resources for cross-channel and text independent speaker recognition,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. of LREC</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Jean Carletta, Simone Ashby, Sebastien Bourban, Mike Flynn, Mael Guillemot, Thomas Hain, Jaroslav Kadlec, Vasilis Karaiskos, Wessel Kraaij, Melissa Kronenthal, et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">‚ÄúThe AMI meeting corpus: A pre-announcement,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International workshop on machine learning for multimodal interaction</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">. Springer, 2005.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Alexandra Canavan, David Graff, and George Zipperlen,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">‚ÄúCALLHOME American English speech,‚Äù https://catalog.ldc.upenn.edu/LDC97S42, 1997.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Tae¬†Jin Park, He¬†Huang, Ante Jukic, Kunal Dhawan, Krishna¬†C Puvvada, Nithin Koluguri, Nikolay Karpov, Aleksandr Laptev, Jagadeesh Balam, and Boris Ginsburg,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">‚ÄúThe CHiME-7 challenge: System description and performance of nemo team‚Äôs dasr system,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.12378</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">‚ÄúNeMo: a toolkit for building AI applications using neural modules,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.09577</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Shinji Watanabe, Michael Mandel, Jon Barker, et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">‚ÄúCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CHiME Workshop</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Thilo von Neumann, Christoph Boeddeker, Marc Delcroix, and Reinhold Haeb-Umbach,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">‚ÄúMeetEval: A meeting transcription evaluation toolkit,‚Äù </span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:90%;">https://github.com/fgnt/meeteval</span><span id="bib.bib48.3.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Yuanchao Li, Zeyu Zhao, Ondrej Klejch, Peter Bell, and Catherine Lai,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">‚ÄúASR and emotional speech: A word-level investigation of the mutual impact of speech and emotion recognition,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">INTERSPEECH</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Jennifer Santoso, Takeshi Yamada, Shoji Makino, Kenkichi Ishizuka, and Takekatsu Hiramura,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">‚ÄúSpeech emotion recognition based on attention weight correction using word-level confidence measure.,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">INTERSPEECH</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Yuanchao Li, Pinzhen Chen, Peter Bell, and Catherine Lai,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">‚ÄúCrossmodal ASR error correction with discrete speech units,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">SLT</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2024.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Jiajun He, Xiaohan Shi, Xingfeng Li, and Tomoki Toda,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">‚ÄúMF-AED-AEC: Speech emotion recognition by leveraging multimodal fusion, ASR error detection, and ASR error correction,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2401.13260</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Jeremy Ang, Rajdip Dhillon, Ashley Krupski, Elizabeth Shriberg, and Andreas Stolcke,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">‚ÄúProsody-based automatic detection of annoyance and frustration in human-computer dialog,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">INTERSPEECH</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2002.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Yuanchao Li, Peter Bell, and Catherine Lai,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">‚ÄúFusing ASR outputs in joint training for speech emotion recognition,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Tiantian Feng and Shrikanth Narayanan,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">‚ÄúFoundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2024.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette¬†N Chang, Sungbok Lee, and Shrikanth¬†S Narayanan,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">‚ÄúIEMOCAP: Interactive emotional dyadic motion capture database,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">LREC</span><span id="bib.bib56.4.2" class="ltx_text" style="font-size:90%;">, 2008.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Yuanchao Li, Peter Bell, and Catherine Lai,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">‚ÄúSpeech emotion recognition with ASR transcripts: A comprehensive study on word error rate and fusion techniques,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">SLT</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2024.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.09784" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.09785" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.09785">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.09785" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.09786" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 19:26:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
