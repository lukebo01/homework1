<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.02111] Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions</title><meta property="og:description" content="Deep learning has revolutionized artificial intelligence (AI), achieving remarkable progress in fields such as computer vision, speech recognition, and natural language processing. Moreover, the recent success of large…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.02111">

<!--Generated on Sat Oct  5 22:32:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Spiking neural networks (SNNs),  large-scale neural networks,  deep neural networks (DNNs),  spiking transformer
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yangfan Hu,
Qian Zheng,  
Guoqi Li,  
Huajin Tang, 
and Gang Pan
</span><span class="ltx_author_notes">
This work was supported in part by Natural Science Foundation of China under Grant 61925603, in part by the STI 2030 Major Projects under Grant 2021ZD0200400, and in part by the National Key Research and Development Program of China under Grant 2022YFB4500100. (Corresponding author: Gang Pan.) Yangfan Hu, Qian Zheng, Huajin Tang and Gang Pan are with College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China. They are also with the State Key Lab of Brain-Machine Intelligence, Zhejiang University, Hangzhou 310027, China. 
<br class="ltx_break">E-mail:huyangfan@zju.edu.cn; qianzheng@zju.edu.cn; htang@zju.edu.cn;
<br class="ltx_break"> gpan@zju.edu.cn
Guoqi Li is with Institute of Automation, Chinese Academy of Sciences, China. 
<br class="ltx_break">E-mail: guoqi.li@ia.ac.cn
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Deep learning has revolutionized artificial intelligence (AI), achieving remarkable progress in fields such as computer vision, speech recognition, and natural language processing. Moreover, the recent success of large language models (LLMs) has fueled a surge in research on large-scale neural networks. However, the escalating demand for computing resources and energy consumption has prompted the search for energy-efficient alternatives. Inspired by the human brain, spiking neural networks (SNNs) promise energy-efficient computation with event-driven spikes. To provide future directions toward building energy-efficient large SNN models, we present a survey of existing methods for developing deep spiking neural networks, with a focus on emerging Spiking Transformers. Our main contributions are as follows: (1) an overview of learning methods for deep spiking neural networks, categorized by ANN-to-SNN conversion and direct training with surrogate gradients; (2) an overview of network architectures for deep spiking neural networks, categorized by deep convolutional neural networks (DCNNs) and Transformer architecture; and (3) a comprehensive comparison of state-of-the-art deep SNNs with a focus on emerging Spiking Transformers. We then further discuss and outline future directions toward large-scale SNNs.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Spiking neural networks (SNNs), large-scale neural networks, deep neural networks (DNNs), spiking transformer

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deep learning has achieved significant accomplishments over the last decade <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, demonstrating promising results that match or even surpass human performance across various fields such as computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, speech recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, natural language processing (NLP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and go <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Recently, large language models (LLMs), i.e., very deep neural networks based on Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> that contain hundreds of billions of parameters have attracted worldwide interest. Fueled by the success of ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> (a large language model with remarkable communication abilities), the artificial intelligence (AI) community has witnessed a rapid expansion of research on large-scale neural networks throughout 2022 and 2023.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Although deep neural networks (DNNs) demonstrate promising capabilities, the increasing demand for memory and computing resources poses a significant challenge to the development and application of DNNs, especially in resource-constrained environments such as edge computing applications. Additionally, the growing carbon footprint of DNNs also contributes to environmental problems such as global warming. For instance, GPT-3 reportedly used 1,287 MWh during training and OpenAI consumes approximately 564 MWh per day to run ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. In contrast, the human brain can perform a series of complex tasks with a power budget of about 20 Watts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. To address the bottleneck of deep learning, researchers have drawn inspiration from human brain and proposed spiking neural networks (SNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, which hold promise for achieving high energy efficiency.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text ltx_font_italic">Spiking Neural Networks (SNNs).</span> Different from traditional artificial neural networks (ANNs), SNNs are neural networks composed of spiking neurons that exchange information via discrete spikes (events that are either 0 or 1) rather than real-valued activations. Leveraging an event-driven computing model, spiking neurons in SNNs only update asynchronously upon the arrival of spikes. Additionally, compared to DNNs that heavily rely on multiply-and-accumulate (MAC) operations, SNNs employ less costly accumulate (AC) operations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Along with emerging neuromorphic hardware (e.g., TrueNorth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, Loihi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, and Darwin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>), SNNs hold promise for addressing the von Neumann bottleneck and achieving energy-efficient machine intelligence with massively parallel processing driven by spikes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text ltx_font_italic">Development.</span> Due to the discontinuity of spikes, training SNNs has been challenging for powerful gradient descent algorithms are not directly applicable. In early works (e.g., SpikeProp <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, Tempotron <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, ReSuMe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and unsupervised STDP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>), SNNs had limited capabilities due to the lack of effective learning algorithms. Inspired by the success of deep learning, researchers have developed various learning algorithms based on deep convolutional neural networks (DCNNs) since 2015, leading to significant improvements in complex tasks such as ImageNet classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Recently, inspired by the success of LLMs, a new trend in SNN research has emerged: building deep SNNs based on Transformer architecture. Since Transformer blocks are a crucial and constant part of most LLM frameworks, combining Spiking Transformers with neuromorphic hardware could make significant progress toward alleviating the energy bottleneck of LLM inference by implementing large SNN models.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text ltx_font_italic">Scope.</span> Focusing on deep neural networks, we limit the scope of our study to deep spiking neural networks, <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">i.e., SNNs capable of performing complex tasks such as image classification on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite></span>. To this end, we primarily examine two aspects that are heavily studied and of great importance: learning rules and network architecture. For learning rules, we focus on two popular approaches: ANN-to-SNN conversion and direct training with surrogate gradients. For SNNs built with local plasticity rules such as STDP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>), please refer to other surveys such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. For network architectures, we focus on two popular categories: DCNNs and Spiking Transformers.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_italic">Related Work.</span> Spiking neural networks, especially their training methods, have been the subject of several recent surveys <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, Yi et al. describe a range of learning rules for SNNs. Focusing on direct learning methods, Guo et al. present a survey on methods for accuracy improvement, efficiency enhancement, and temporal dynamics utilization. Dampfhoffer et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, concentrating on deep SNNs, review ANN-to-SNN conversion and backpropagation methods with a taxonomy of spatial, spatiotemporal, and single-spike approaches. Similarly, Eshraghian et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> explore how SNNs could leverage deep learning technologies. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, Rathi et al. provide a systematic review of SNNs, covering both algorithms and hardware. However, none of these works provide a survey on emerging Spiking Transformer architectures, which hold the potential for achieving large-scale SNN models.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text ltx_font_italic">Paper Overview.</span> First, Section <a href="#S2.SS1" title="2.1 Learning Rules ‣ 2 Deep Spiking Neural Networks ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a> surveys learning methods for building deep SNNs. Section <a href="#S2.SS2" title="2.2 Network Architectures in Large Spiking Neural Networks ‣ 2 Deep Spiking Neural Networks ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a> surveys network architectures for deep SNNs (e.g., DCNNs and Spiking Transformers). Section <a href="#S2.SS3" title="2.3 Benchmarking ‣ 2 Deep Spiking Neural Networks ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a> compares state-of-the-art deep SNNs on the ImageNet benchmark. Section <a href="#S3" title="3 Future Directions ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> discusses challenges and future directions toward building large-scale spiking neural networks. Section <a href="#S4" title="4 Conclusion ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> provides the conclusion.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Deep Spiking Neural Networks</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span id="S2.SS1.1.1" class="ltx_text ltx_font_italic">Learning Rules</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In this section, we present an overview of learning rules in deep spiking neural networks grouped into two popular approaches: ANN-to-SNN conversion and direct training with surrogate gradients.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>ANN-to-SNN Conversion</h4>

<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of Recent ANN-to-SNN Conversion Algorithms</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Work</span></th>
<th id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Architecture</span></th>
<th id="S2.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.5.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Time</span></td>
</tr>
<tr id="S2.T1.1.1.1.5.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">Steps</span></td>
</tr>
</table>
</th>
<th id="S2.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.6.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.1.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">ANN</span></td>
</tr>
<tr id="S2.T1.1.1.1.6.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.1.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold">Acc. (%)</span></td>
</tr>
</table>
</th>
<th id="S2.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.1.1.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.7.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.1.1.1.7.1.1.1.1" class="ltx_text ltx_font_bold">SNN</span></td>
</tr>
<tr id="S2.T1.1.1.1.7.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.1.1.1.7.1.2.1.1" class="ltx_text ltx_font_bold">Acc. (%)</span></td>
</tr>
</table>
</th>
<th id="S2.T1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.1.1.1.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.8.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.1.1.1.8.1.1.1.1" class="ltx_text ltx_font_bold">SNN - ANN</span></td>
</tr>
<tr id="S2.T1.1.1.1.8.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T1.1.1.1.8.1.2.1.1" class="ltx_text ltx_font_bold">Acc. (%)</span></td>
</tr>
</table>
</th>
</tr>
<tr id="S2.T1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_tt" rowspan="8"><span id="S2.T1.1.2.2.1.1" class="ltx_text">
<span id="S2.T1.1.2.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:44.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:44.4pt;transform:translate(-18.81pt,-18.81pt) rotate(-90deg) ;">
<span id="S2.T1.1.2.2.1.1.1.1" class="ltx_p"><span id="S2.T1.1.2.2.1.1.1.1.1" class="ltx_text ltx_font_bold">CIFAR-10</span></span>
</span></span></span></td>
<td id="S2.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">2021</td>
<td id="S2.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_tt">Clamp and Quantization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S2.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_tt">VGG-19</td>
<td id="S2.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_tt">1000</td>
<td id="S2.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_tt">93.50</td>
<td id="S2.T1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_tt">93.44</td>
<td id="S2.T1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_tt">-0.06</td>
</tr>
<tr id="S2.T1.1.3.3" class="ltx_tr">
<td id="S2.T1.1.3.3.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T1.1.3.3.2" class="ltx_td ltx_align_center">Spiking ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S2.T1.1.3.3.3" class="ltx_td ltx_align_center">ResNet-110</td>
<td id="S2.T1.1.3.3.4" class="ltx_td ltx_align_center">350</td>
<td id="S2.T1.1.3.3.5" class="ltx_td ltx_align_center">93.47</td>
<td id="S2.T1.1.3.3.6" class="ltx_td ltx_align_center">93.07</td>
<td id="S2.T1.1.3.3.7" class="ltx_td ltx_align_center">-0.45</td>
</tr>
<tr id="S2.T1.1.4.4" class="ltx_tr">
<td id="S2.T1.1.4.4.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T1.1.4.4.2" class="ltx_td ltx_align_center">Progressive Tandeom Learning<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S2.T1.1.4.4.3" class="ltx_td ltx_align_center">VGG-11</td>
<td id="S2.T1.1.4.4.4" class="ltx_td ltx_align_center">16</td>
<td id="S2.T1.1.4.4.5" class="ltx_td ltx_align_center">90.59</td>
<td id="S2.T1.1.4.4.6" class="ltx_td ltx_align_center">91.24</td>
<td id="S2.T1.1.4.4.7" class="ltx_td ltx_align_center">+0.65</td>
</tr>
<tr id="S2.T1.1.5.5" class="ltx_tr">
<td id="S2.T1.1.5.5.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T1.1.5.5.2" class="ltx_td ltx_align_center">Threshold ReLU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S2.T1.1.5.5.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T1.1.5.5.4" class="ltx_td ltx_align_center">512</td>
<td id="S2.T1.1.5.5.5" class="ltx_td ltx_align_center">92.09</td>
<td id="S2.T1.1.5.5.6" class="ltx_td ltx_align_center">92.03</td>
<td id="S2.T1.1.5.5.7" class="ltx_td ltx_align_center">-0.06</td>
</tr>
<tr id="S2.T1.1.6.6" class="ltx_tr">
<td id="S2.T1.1.6.6.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T1.1.6.6.2" class="ltx_td ltx_align_center">Network Calibration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</td>
<td id="S2.T1.1.6.6.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T1.1.6.6.4" class="ltx_td ltx_align_center">32</td>
<td id="S2.T1.1.6.6.5" class="ltx_td ltx_align_center">95.72</td>
<td id="S2.T1.1.6.6.6" class="ltx_td ltx_align_center">93.71</td>
<td id="S2.T1.1.6.6.7" class="ltx_td ltx_align_center">-2.01</td>
</tr>
<tr id="S2.T1.1.7.7" class="ltx_tr">
<td id="S2.T1.1.7.7.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T1.1.7.7.2" class="ltx_td ltx_align_center">Potential Initialization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S2.T1.1.7.7.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T1.1.7.7.4" class="ltx_td ltx_align_center">32</td>
<td id="S2.T1.1.7.7.5" class="ltx_td ltx_align_center">94.57</td>
<td id="S2.T1.1.7.7.6" class="ltx_td ltx_align_center">94.20</td>
<td id="S2.T1.1.7.7.7" class="ltx_td ltx_align_center">-0.37</td>
</tr>
<tr id="S2.T1.1.8.8" class="ltx_tr">
<td id="S2.T1.1.8.8.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T1.1.8.8.2" class="ltx_td ltx_align_center">clip-floor-shift <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="S2.T1.1.8.8.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T1.1.8.8.4" class="ltx_td ltx_align_center">32</td>
<td id="S2.T1.1.8.8.5" class="ltx_td ltx_align_center">95.52</td>
<td id="S2.T1.1.8.8.6" class="ltx_td ltx_align_center">95.54</td>
<td id="S2.T1.1.8.8.7" class="ltx_td ltx_align_center">+0.02</td>
</tr>
<tr id="S2.T1.1.9.9" class="ltx_tr">
<td id="S2.T1.1.9.9.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T1.1.9.9.2" class="ltx_td ltx_align_center">Fast-SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S2.T1.1.9.9.3" class="ltx_td ltx_align_center">ResNet-18</td>
<td id="S2.T1.1.9.9.4" class="ltx_td ltx_align_center">3</td>
<td id="S2.T1.1.9.9.5" class="ltx_td ltx_align_center">95.51</td>
<td id="S2.T1.1.9.9.6" class="ltx_td ltx_align_center">95.42</td>
<td id="S2.T1.1.9.9.7" class="ltx_td ltx_align_center">-0.09</td>
</tr>
<tr id="S2.T1.1.10.10" class="ltx_tr">
<td id="S2.T1.1.10.10.1" class="ltx_td"></td>
<td id="S2.T1.1.10.10.2" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T1.1.10.10.3" class="ltx_td ltx_align_center">Parameter Calibration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</td>
<td id="S2.T1.1.10.10.4" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T1.1.10.10.5" class="ltx_td ltx_align_center">4</td>
<td id="S2.T1.1.10.10.6" class="ltx_td ltx_align_center">95.60</td>
<td id="S2.T1.1.10.10.7" class="ltx_td ltx_align_center">94.75</td>
<td id="S2.T1.1.10.10.8" class="ltx_td ltx_align_center">-0.85</td>
</tr>
<tr id="S2.T1.1.11.11" class="ltx_tr">
<td id="S2.T1.1.11.11.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="8"><span id="S2.T1.1.11.11.1.1" class="ltx_text">
<span id="S2.T1.1.11.11.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:42.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:42.2pt;transform:translate(-16.72pt,-15.75pt) rotate(-90deg) ;">
<span id="S2.T1.1.11.11.1.1.1.1" class="ltx_p"><span id="S2.T1.1.11.11.1.1.1.1.1" class="ltx_text ltx_font_bold">ImageNet</span></span>
</span></span></span></td>
<td id="S2.T1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_t">2021</td>
<td id="S2.T1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_t">Spiking ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S2.T1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_t">ResNet-50</td>
<td id="S2.T1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_t">350</td>
<td id="S2.T1.1.11.11.6" class="ltx_td ltx_align_center ltx_border_t">75.45</td>
<td id="S2.T1.1.11.11.7" class="ltx_td ltx_align_center ltx_border_t">73.77</td>
<td id="S2.T1.1.11.11.8" class="ltx_td ltx_align_center ltx_border_t">-1.68</td>
</tr>
<tr id="S2.T1.1.12.12" class="ltx_tr">
<td id="S2.T1.1.12.12.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T1.1.12.12.2" class="ltx_td ltx_align_center">Progressive Tandeom Learning<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S2.T1.1.12.12.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T1.1.12.12.4" class="ltx_td ltx_align_center">16</td>
<td id="S2.T1.1.12.12.5" class="ltx_td ltx_align_center">71.65</td>
<td id="S2.T1.1.12.12.6" class="ltx_td ltx_align_center">65.08</td>
<td id="S2.T1.1.12.12.7" class="ltx_td ltx_align_center">-6.57</td>
</tr>
<tr id="S2.T1.1.13.13" class="ltx_tr">
<td id="S2.T1.1.13.13.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T1.1.13.13.2" class="ltx_td ltx_align_center">Threshold ReLU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S2.T1.1.13.13.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T1.1.13.13.4" class="ltx_td ltx_align_center">512</td>
<td id="S2.T1.1.13.13.5" class="ltx_td ltx_align_center">72.40</td>
<td id="S2.T1.1.13.13.6" class="ltx_td ltx_align_center">72.34</td>
<td id="S2.T1.1.13.13.7" class="ltx_td ltx_align_center">-0.06</td>
</tr>
<tr id="S2.T1.1.14.14" class="ltx_tr">
<td id="S2.T1.1.14.14.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T1.1.14.14.2" class="ltx_td ltx_align_center">Network Calibration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</td>
<td id="S2.T1.1.14.14.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T1.1.14.14.4" class="ltx_td ltx_align_center">32</td>
<td id="S2.T1.1.14.14.5" class="ltx_td ltx_align_center">75.36</td>
<td id="S2.T1.1.14.14.6" class="ltx_td ltx_align_center">63.64</td>
<td id="S2.T1.1.14.14.7" class="ltx_td ltx_align_center">-11.72</td>
</tr>
<tr id="S2.T1.1.15.15" class="ltx_tr">
<td id="S2.T1.1.15.15.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T1.1.15.15.2" class="ltx_td ltx_align_center">Potential Initialization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S2.T1.1.15.15.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T1.1.15.15.4" class="ltx_td ltx_align_center">32</td>
<td id="S2.T1.1.15.15.5" class="ltx_td ltx_align_center">74.85</td>
<td id="S2.T1.1.15.15.6" class="ltx_td ltx_align_center">64.70</td>
<td id="S2.T1.1.15.15.7" class="ltx_td ltx_align_center">-10.15</td>
</tr>
<tr id="S2.T1.1.16.16" class="ltx_tr">
<td id="S2.T1.1.16.16.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T1.1.16.16.2" class="ltx_td ltx_align_center">clip-floor-shift <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="S2.T1.1.16.16.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T1.1.16.16.4" class="ltx_td ltx_align_center">32</td>
<td id="S2.T1.1.16.16.5" class="ltx_td ltx_align_center">74.29</td>
<td id="S2.T1.1.16.16.6" class="ltx_td ltx_align_center">68.47</td>
<td id="S2.T1.1.16.16.7" class="ltx_td ltx_align_center">-5.82</td>
</tr>
<tr id="S2.T1.1.17.17" class="ltx_tr">
<td id="S2.T1.1.17.17.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T1.1.17.17.2" class="ltx_td ltx_align_center">Fast-SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S2.T1.1.17.17.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T1.1.17.17.4" class="ltx_td ltx_align_center">3</td>
<td id="S2.T1.1.17.17.5" class="ltx_td ltx_align_center">71.91</td>
<td id="S2.T1.1.17.17.6" class="ltx_td ltx_align_center">71.31</td>
<td id="S2.T1.1.17.17.7" class="ltx_td ltx_align_center">-0.60</td>
</tr>
<tr id="S2.T1.1.18.18" class="ltx_tr">
<td id="S2.T1.1.18.18.1" class="ltx_td ltx_align_center ltx_border_bb">2024</td>
<td id="S2.T1.1.18.18.2" class="ltx_td ltx_align_center ltx_border_bb">Parameter Calibration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</td>
<td id="S2.T1.1.18.18.3" class="ltx_td ltx_align_center ltx_border_bb">VGG-16</td>
<td id="S2.T1.1.18.18.4" class="ltx_td ltx_align_center ltx_border_bb">75.36</td>
<td id="S2.T1.1.18.18.5" class="ltx_td ltx_align_center ltx_border_bb">16</td>
<td id="S2.T1.1.18.18.6" class="ltx_td ltx_align_center ltx_border_bb">65.02</td>
<td id="S2.T1.1.18.18.7" class="ltx_td ltx_align_center ltx_border_bb">-10.34</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2409.02111/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="241" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>ANN-to-SNN conversion, a mapping between real-valued activation neurons and spiking neurons.</figcaption>
</figure>
<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">ANN-to-SNN conversion facilitates the efficient utilization of pre-trained models, enabling compatibility with existing frameworks and reducing resource demands during training and inference. This conversion method promotes transfer learning and fine-tuning while enhancing the biological plausibility of neural networks. The inherent sparsity and event-driven processing of SNNs align well with hardware implementations, fostering scalability and energy efficiency in neuromorphic computing.</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.8" class="ltx_p">Based on the assumption that ANN activations approximate SNN firing rates, researchers proposed various conversion methods to exploit the advantages of deep neural networks and build deep SNNs by mapping real-valued activation neurons into discrete spiking neurons (Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.1.1 ANN-to-SNN Conversion ‣ 2.1 Learning Rules ‣ 2 Deep Spiking Neural Networks ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Cao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> first proposed to map CNNs with ReLU activations and no biases into SNNs of integrate-and-fire (IF) neurons. The ReLU function is defined by</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.4" class="ltx_Math" alttext="y(x)=\max(0,\sum W_{i}x_{i})," display="block"><semantics id="S2.E1.m1.4a"><mrow id="S2.E1.m1.4.4.1" xref="S2.E1.m1.4.4.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1" xref="S2.E1.m1.4.4.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1.3" xref="S2.E1.m1.4.4.1.1.3.cmml"><mi id="S2.E1.m1.4.4.1.1.3.2" xref="S2.E1.m1.4.4.1.1.3.2.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.1.1.3.1" xref="S2.E1.m1.4.4.1.1.3.1.cmml">​</mo><mrow id="S2.E1.m1.4.4.1.1.3.3.2" xref="S2.E1.m1.4.4.1.1.3.cmml"><mo stretchy="false" id="S2.E1.m1.4.4.1.1.3.3.2.1" xref="S2.E1.m1.4.4.1.1.3.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S2.E1.m1.4.4.1.1.3.3.2.2" xref="S2.E1.m1.4.4.1.1.3.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.4.4.1.1.2" xref="S2.E1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.4.4.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.2.cmml"><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">max</mi><mo id="S2.E1.m1.4.4.1.1.1.1a" xref="S2.E1.m1.4.4.1.1.1.2.cmml">⁡</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.4.4.1.1.1.1.1.2" xref="S2.E1.m1.4.4.1.1.1.2.cmml">(</mo><mn id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">0</mn><mo rspace="0em" id="S2.E1.m1.4.4.1.1.1.1.1.3" xref="S2.E1.m1.4.4.1.1.1.2.cmml">,</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.cmml"><mo movablelimits="false" id="S2.E1.m1.4.4.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.cmml">∑</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.cmml"><msub id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.2.cmml">W</mi><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.1.1.1.1.1.1.2.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.1.cmml">​</mo><msub id="S2.E1.m1.4.4.1.1.1.1.1.1.2.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.2.3.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.3.2.cmml">x</mi><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.2.3.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow></mrow><mo stretchy="false" id="S2.E1.m1.4.4.1.1.1.1.1.4" xref="S2.E1.m1.4.4.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.4.4.1.2" xref="S2.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.4b"><apply id="S2.E1.m1.4.4.1.1.cmml" xref="S2.E1.m1.4.4.1"><eq id="S2.E1.m1.4.4.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.2"></eq><apply id="S2.E1.m1.4.4.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.3"><times id="S2.E1.m1.4.4.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.3.1"></times><ci id="S2.E1.m1.4.4.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.3.2">𝑦</ci><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝑥</ci></apply><apply id="S2.E1.m1.4.4.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1"><max id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"></max><cn type="integer" id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">0</cn><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1"><sum id="S2.E1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1"></sum><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2"><times id="S2.E1.m1.4.4.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.1"></times><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.2">𝑊</ci><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.3.2">𝑥</ci><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.2.3.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.4c">y(x)=\max(0,\sum W_{i}x_{i}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.SSS1.p2.2" class="ltx_p">where <math id="S2.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S2.SS1.SSS1.p2.1.m1.1a"><mi id="S2.SS1.SSS1.p2.1.m1.1.1" xref="S2.SS1.SSS1.p2.1.m1.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.1.m1.1b"><ci id="S2.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p2.1.m1.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.1.m1.1c">W</annotation></semantics></math> denotes weight and <math id="S2.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.SSS1.p2.2.m2.1a"><mi id="S2.SS1.SSS1.p2.2.m2.1.1" xref="S2.SS1.SSS1.p2.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.2.m2.1b"><ci id="S2.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS1.SSS1.p2.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.2.m2.1c">x</annotation></semantics></math> denotes input activation. The IF neuron is defined by</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.2" class="ltx_Math" alttext="s(t)=\Theta(\vartheta(t-1)+\sum W_{i}s_{i}(t-1)-\theta)," display="block"><semantics id="S2.E2.m1.2a"><mrow id="S2.E2.m1.2.2.1" xref="S2.E2.m1.2.2.1.1.cmml"><mrow id="S2.E2.m1.2.2.1.1" xref="S2.E2.m1.2.2.1.1.cmml"><mrow id="S2.E2.m1.2.2.1.1.3" xref="S2.E2.m1.2.2.1.1.3.cmml"><mi id="S2.E2.m1.2.2.1.1.3.2" xref="S2.E2.m1.2.2.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.3.1" xref="S2.E2.m1.2.2.1.1.3.1.cmml">​</mo><mrow id="S2.E2.m1.2.2.1.1.3.3.2" xref="S2.E2.m1.2.2.1.1.3.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.3.3.2.1" xref="S2.E2.m1.2.2.1.1.3.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">t</mi><mo stretchy="false" id="S2.E2.m1.2.2.1.1.3.3.2.2" xref="S2.E2.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.2.2.1.1.2" xref="S2.E2.m1.2.2.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.2.2.1.1.1" xref="S2.E2.m1.2.2.1.1.1.cmml"><mi mathvariant="normal" id="S2.E2.m1.2.2.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.3.cmml">Θ</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.cmml"><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">ϑ</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">t</mi><mo id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.055em" id="S2.E2.m1.2.2.1.1.1.1.1.1.2.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.3.cmml">+</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.cmml"><mo movablelimits="false" id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.cmml"><msub id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3.cmml"><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3.2.cmml">W</mi><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.2.cmml">​</mo><msub id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4.cmml"><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4.2.cmml">s</mi><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.2a" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.2.cmml">​</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.cmml"><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.2.cmml">t</mi><mo id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.1.cmml">−</mo><mn id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E2.m1.2.2.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.3.cmml">−</mo><mi id="S2.E2.m1.2.2.1.1.1.1.1.1.4" xref="S2.E2.m1.2.2.1.1.1.1.1.1.4.cmml">θ</mi></mrow><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E2.m1.2.2.1.2" xref="S2.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.2b"><apply id="S2.E2.m1.2.2.1.1.cmml" xref="S2.E2.m1.2.2.1"><eq id="S2.E2.m1.2.2.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.2"></eq><apply id="S2.E2.m1.2.2.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.3"><times id="S2.E2.m1.2.2.1.1.3.1.cmml" xref="S2.E2.m1.2.2.1.1.3.1"></times><ci id="S2.E2.m1.2.2.1.1.3.2.cmml" xref="S2.E2.m1.2.2.1.1.3.2">𝑠</ci><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑡</ci></apply><apply id="S2.E2.m1.2.2.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1"><times id="S2.E2.m1.2.2.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.2"></times><ci id="S2.E2.m1.2.2.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.3">Θ</ci><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1"><minus id="S2.E2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.3"></minus><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2"><plus id="S2.E2.m1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.3"></plus><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1"><times id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2"></times><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.3">italic-ϑ</ci><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1"><minus id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2"><sum id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.2"></sum><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1"><times id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.2"></times><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3.2">𝑊</ci><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.3.3">𝑖</ci></apply><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4.2">𝑠</ci><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.4.3">𝑖</ci></apply><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1"><minus id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.1"></minus><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.2">𝑡</ci><cn type="integer" id="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.2.2.1.1.1.1.3">1</cn></apply></apply></apply></apply><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.4.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.4">𝜃</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.2c">s(t)=\Theta(\vartheta(t-1)+\sum W_{i}s_{i}(t-1)-\theta),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.SSS1.p2.6" class="ltx_p">where <math id="S2.SS1.SSS1.p2.3.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.SS1.SSS1.p2.3.m1.1a"><mi id="S2.SS1.SSS1.p2.3.m1.1.1" xref="S2.SS1.SSS1.p2.3.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.3.m1.1b"><ci id="S2.SS1.SSS1.p2.3.m1.1.1.cmml" xref="S2.SS1.SSS1.p2.3.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.3.m1.1c">s</annotation></semantics></math> denotes output, <math id="S2.SS1.SSS1.p2.4.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.SSS1.p2.4.m2.1a"><mi id="S2.SS1.SSS1.p2.4.m2.1.1" xref="S2.SS1.SSS1.p2.4.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.4.m2.1b"><ci id="S2.SS1.SSS1.p2.4.m2.1.1.cmml" xref="S2.SS1.SSS1.p2.4.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.4.m2.1c">\theta</annotation></semantics></math> denotes the firing threshold, <math id="S2.SS1.SSS1.p2.5.m3.1" class="ltx_Math" alttext="\vartheta" display="inline"><semantics id="S2.SS1.SSS1.p2.5.m3.1a"><mi id="S2.SS1.SSS1.p2.5.m3.1.1" xref="S2.SS1.SSS1.p2.5.m3.1.1.cmml">ϑ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.5.m3.1b"><ci id="S2.SS1.SSS1.p2.5.m3.1.1.cmml" xref="S2.SS1.SSS1.p2.5.m3.1.1">italic-ϑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.5.m3.1c">\vartheta</annotation></semantics></math> denotes the membrane potential, and <math id="S2.SS1.SSS1.p2.6.m4.1" class="ltx_Math" alttext="\Theta" display="inline"><semantics id="S2.SS1.SSS1.p2.6.m4.1a"><mi mathvariant="normal" id="S2.SS1.SSS1.p2.6.m4.1.1" xref="S2.SS1.SSS1.p2.6.m4.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.6.m4.1b"><ci id="S2.SS1.SSS1.p2.6.m4.1.1.cmml" xref="S2.SS1.SSS1.p2.6.m4.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.6.m4.1c">\Theta</annotation></semantics></math> is the Heaviside step function:</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.5" class="ltx_Math" alttext="\Theta(x)=\begin{cases}1&amp;\text{if }x\geq 0\\
0&amp;\text{else.}\\
\end{cases}" display="block"><semantics id="S2.E3.m1.5a"><mrow id="S2.E3.m1.5.6" xref="S2.E3.m1.5.6.cmml"><mrow id="S2.E3.m1.5.6.2" xref="S2.E3.m1.5.6.2.cmml"><mi mathvariant="normal" id="S2.E3.m1.5.6.2.2" xref="S2.E3.m1.5.6.2.2.cmml">Θ</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.5.6.2.1" xref="S2.E3.m1.5.6.2.1.cmml">​</mo><mrow id="S2.E3.m1.5.6.2.3.2" xref="S2.E3.m1.5.6.2.cmml"><mo stretchy="false" id="S2.E3.m1.5.6.2.3.2.1" xref="S2.E3.m1.5.6.2.cmml">(</mo><mi id="S2.E3.m1.5.5" xref="S2.E3.m1.5.5.cmml">x</mi><mo stretchy="false" id="S2.E3.m1.5.6.2.3.2.2" xref="S2.E3.m1.5.6.2.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.5.6.1" xref="S2.E3.m1.5.6.1.cmml">=</mo><mrow id="S2.E3.m1.4.4" xref="S2.E3.m1.5.6.3.1.cmml"><mo id="S2.E3.m1.4.4.5" xref="S2.E3.m1.5.6.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S2.E3.m1.4.4.4" xref="S2.E3.m1.5.6.3.1.cmml"><mtr id="S2.E3.m1.4.4.4a" xref="S2.E3.m1.5.6.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.4.4.4b" xref="S2.E3.m1.5.6.3.1.cmml"><mn id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.4.4.4c" xref="S2.E3.m1.5.6.3.1.cmml"><mrow id="S2.E3.m1.2.2.2.2.2.1" xref="S2.E3.m1.2.2.2.2.2.1.cmml"><mrow id="S2.E3.m1.2.2.2.2.2.1.2" xref="S2.E3.m1.2.2.2.2.2.1.2.cmml"><mtext id="S2.E3.m1.2.2.2.2.2.1.2.2" xref="S2.E3.m1.2.2.2.2.2.1.2.2a.cmml">if </mtext><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.2.2.2.1.2.1" xref="S2.E3.m1.2.2.2.2.2.1.2.1.cmml">​</mo><mi id="S2.E3.m1.2.2.2.2.2.1.2.3" xref="S2.E3.m1.2.2.2.2.2.1.2.3.cmml">x</mi></mrow><mo id="S2.E3.m1.2.2.2.2.2.1.1" xref="S2.E3.m1.2.2.2.2.2.1.1.cmml">≥</mo><mn id="S2.E3.m1.2.2.2.2.2.1.3" xref="S2.E3.m1.2.2.2.2.2.1.3.cmml">0</mn></mrow></mtd></mtr><mtr id="S2.E3.m1.4.4.4d" xref="S2.E3.m1.5.6.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.4.4.4e" xref="S2.E3.m1.5.6.3.1.cmml"><mn id="S2.E3.m1.3.3.3.3.1.1" xref="S2.E3.m1.3.3.3.3.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.4.4.4f" xref="S2.E3.m1.5.6.3.1.cmml"><mtext id="S2.E3.m1.4.4.4.4.2.1" xref="S2.E3.m1.4.4.4.4.2.1a.cmml">else.</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.5b"><apply id="S2.E3.m1.5.6.cmml" xref="S2.E3.m1.5.6"><eq id="S2.E3.m1.5.6.1.cmml" xref="S2.E3.m1.5.6.1"></eq><apply id="S2.E3.m1.5.6.2.cmml" xref="S2.E3.m1.5.6.2"><times id="S2.E3.m1.5.6.2.1.cmml" xref="S2.E3.m1.5.6.2.1"></times><ci id="S2.E3.m1.5.6.2.2.cmml" xref="S2.E3.m1.5.6.2.2">Θ</ci><ci id="S2.E3.m1.5.5.cmml" xref="S2.E3.m1.5.5">𝑥</ci></apply><apply id="S2.E3.m1.5.6.3.1.cmml" xref="S2.E3.m1.4.4"><csymbol cd="latexml" id="S2.E3.m1.5.6.3.1.1.cmml" xref="S2.E3.m1.4.4.5">cases</csymbol><cn type="integer" id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1">1</cn><apply id="S2.E3.m1.2.2.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.2.2.1"><geq id="S2.E3.m1.2.2.2.2.2.1.1.cmml" xref="S2.E3.m1.2.2.2.2.2.1.1"></geq><apply id="S2.E3.m1.2.2.2.2.2.1.2.cmml" xref="S2.E3.m1.2.2.2.2.2.1.2"><times id="S2.E3.m1.2.2.2.2.2.1.2.1.cmml" xref="S2.E3.m1.2.2.2.2.2.1.2.1"></times><ci id="S2.E3.m1.2.2.2.2.2.1.2.2a.cmml" xref="S2.E3.m1.2.2.2.2.2.1.2.2"><mtext id="S2.E3.m1.2.2.2.2.2.1.2.2.cmml" xref="S2.E3.m1.2.2.2.2.2.1.2.2">if </mtext></ci><ci id="S2.E3.m1.2.2.2.2.2.1.2.3.cmml" xref="S2.E3.m1.2.2.2.2.2.1.2.3">𝑥</ci></apply><cn type="integer" id="S2.E3.m1.2.2.2.2.2.1.3.cmml" xref="S2.E3.m1.2.2.2.2.2.1.3">0</cn></apply><cn type="integer" id="S2.E3.m1.3.3.3.3.1.1.cmml" xref="S2.E3.m1.3.3.3.3.1.1">0</cn><ci id="S2.E3.m1.4.4.4.4.2.1a.cmml" xref="S2.E3.m1.4.4.4.4.2.1"><mtext id="S2.E3.m1.4.4.4.4.2.1.cmml" xref="S2.E3.m1.4.4.4.4.2.1">else.</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.5c">\Theta(x)=\begin{cases}1&amp;\text{if }x\geq 0\\
0&amp;\text{else.}\\
\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.SSS1.p2.9" class="ltx_p">They demonstrated that the rectified linear unit (ReLU) function is functionally equivalent to integrate-and-fire (IF) neuron, i.e., LIF neuron with no leaky factor or refractory period:</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.3" class="ltx_Math" alttext="y(x)\approx\dfrac{\sum_{i=1}^{T}s(t)}{T}," display="block"><semantics id="S2.E4.m1.3a"><mrow id="S2.E4.m1.3.3.1" xref="S2.E4.m1.3.3.1.1.cmml"><mrow id="S2.E4.m1.3.3.1.1" xref="S2.E4.m1.3.3.1.1.cmml"><mrow id="S2.E4.m1.3.3.1.1.2" xref="S2.E4.m1.3.3.1.1.2.cmml"><mi id="S2.E4.m1.3.3.1.1.2.2" xref="S2.E4.m1.3.3.1.1.2.2.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.3.3.1.1.2.1" xref="S2.E4.m1.3.3.1.1.2.1.cmml">​</mo><mrow id="S2.E4.m1.3.3.1.1.2.3.2" xref="S2.E4.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.3.3.1.1.2.3.2.1" xref="S2.E4.m1.3.3.1.1.2.cmml">(</mo><mi id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml">x</mi><mo stretchy="false" id="S2.E4.m1.3.3.1.1.2.3.2.2" xref="S2.E4.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.3.3.1.1.1" xref="S2.E4.m1.3.3.1.1.1.cmml">≈</mo><mfrac id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml"><mrow id="S2.E4.m1.1.1.1" xref="S2.E4.m1.1.1.1.cmml"><msubsup id="S2.E4.m1.1.1.1.2" xref="S2.E4.m1.1.1.1.2.cmml"><mo id="S2.E4.m1.1.1.1.2.2.2" xref="S2.E4.m1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S2.E4.m1.1.1.1.2.2.3" xref="S2.E4.m1.1.1.1.2.2.3.cmml"><mi id="S2.E4.m1.1.1.1.2.2.3.2" xref="S2.E4.m1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.E4.m1.1.1.1.2.2.3.1" xref="S2.E4.m1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E4.m1.1.1.1.2.2.3.3" xref="S2.E4.m1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E4.m1.1.1.1.2.3" xref="S2.E4.m1.1.1.1.2.3.cmml">T</mi></msubsup><mrow id="S2.E4.m1.1.1.1.3" xref="S2.E4.m1.1.1.1.3.cmml"><mi id="S2.E4.m1.1.1.1.3.2" xref="S2.E4.m1.1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.1.1.1.3.1" xref="S2.E4.m1.1.1.1.3.1.cmml">​</mo><mrow id="S2.E4.m1.1.1.1.3.3.2" xref="S2.E4.m1.1.1.1.3.cmml"><mo stretchy="false" id="S2.E4.m1.1.1.1.3.3.2.1" xref="S2.E4.m1.1.1.1.3.cmml">(</mo><mi id="S2.E4.m1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.E4.m1.1.1.1.3.3.2.2" xref="S2.E4.m1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mi id="S2.E4.m1.1.1.3" xref="S2.E4.m1.1.1.3.cmml">T</mi></mfrac></mrow><mo id="S2.E4.m1.3.3.1.2" xref="S2.E4.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.3b"><apply id="S2.E4.m1.3.3.1.1.cmml" xref="S2.E4.m1.3.3.1"><approx id="S2.E4.m1.3.3.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1"></approx><apply id="S2.E4.m1.3.3.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.2"><times id="S2.E4.m1.3.3.1.1.2.1.cmml" xref="S2.E4.m1.3.3.1.1.2.1"></times><ci id="S2.E4.m1.3.3.1.1.2.2.cmml" xref="S2.E4.m1.3.3.1.1.2.2">𝑦</ci><ci id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2">𝑥</ci></apply><apply id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1"><divide id="S2.E4.m1.1.1.2.cmml" xref="S2.E4.m1.1.1"></divide><apply id="S2.E4.m1.1.1.1.cmml" xref="S2.E4.m1.1.1.1"><apply id="S2.E4.m1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.2.1.cmml" xref="S2.E4.m1.1.1.1.2">superscript</csymbol><apply id="S2.E4.m1.1.1.1.2.2.cmml" xref="S2.E4.m1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.2.2.1.cmml" xref="S2.E4.m1.1.1.1.2">subscript</csymbol><sum id="S2.E4.m1.1.1.1.2.2.2.cmml" xref="S2.E4.m1.1.1.1.2.2.2"></sum><apply id="S2.E4.m1.1.1.1.2.2.3.cmml" xref="S2.E4.m1.1.1.1.2.2.3"><eq id="S2.E4.m1.1.1.1.2.2.3.1.cmml" xref="S2.E4.m1.1.1.1.2.2.3.1"></eq><ci id="S2.E4.m1.1.1.1.2.2.3.2.cmml" xref="S2.E4.m1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S2.E4.m1.1.1.1.2.2.3.3.cmml" xref="S2.E4.m1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E4.m1.1.1.1.2.3.cmml" xref="S2.E4.m1.1.1.1.2.3">𝑇</ci></apply><apply id="S2.E4.m1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.3"><times id="S2.E4.m1.1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.1.3.1"></times><ci id="S2.E4.m1.1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.1.3.2">𝑠</ci><ci id="S2.E4.m1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1">𝑡</ci></apply></apply><ci id="S2.E4.m1.1.1.3.cmml" xref="S2.E4.m1.1.1.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.3c">y(x)\approx\dfrac{\sum_{i=1}^{T}s(t)}{T},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.SSS1.p2.7" class="ltx_p">where <math id="S2.SS1.SSS1.p2.7.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS1.SSS1.p2.7.m1.1a"><mi id="S2.SS1.SSS1.p2.7.m1.1.1" xref="S2.SS1.SSS1.p2.7.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.7.m1.1b"><ci id="S2.SS1.SSS1.p2.7.m1.1.1.cmml" xref="S2.SS1.SSS1.p2.7.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.7.m1.1c">T</annotation></semantics></math> denotes the total number of time steps.</p>
</div>
<div id="S2.SS1.SSS1.p3" class="ltx_para">
<p id="S2.SS1.SSS1.p3.1" class="ltx_p">To improve the performance of converted SNNs, Diehl et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> examined the conversion process and reported over-/under-activation of spiking neurons that distorts the approximation between ANN activations and SNN firing rates. To address this problem, they proposed weight normalization and threshold balancing, which are mathematically equivalent.</p>
</div>
<div id="S2.SS1.SSS1.p4" class="ltx_para">
<p id="S2.SS1.SSS1.p4.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, Rueckauer et al. performed a detailed analysis of ANN-to-SNN conversion. They found information loss due to the reset of spiking neurons and proposed using reset-by-subtraction or soft reset to replace the original reset-by-zero method. They further identified that quantization resulting from residual membrane potentials not integrated into spikes is a major factor degrading the performance of converted SNNs. To address this issue, they improved weight normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> by using the 99th or 99.9th percentile of activations instead of the maximum. Additionally, they implemented spiking versions of common operations in modern DCNNs (e.g., batch normalization), enabling the conversion of deeper CNNs.</p>
</div>
<div id="S2.SS1.SSS1.p5" class="ltx_para">
<p id="S2.SS1.SSS1.p5.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, several novel normalization methods have emerged to mitigate performance degradation after conversion. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, Sengupta et al. proposed a dynamic threshold balancing strategy that normalizes SNNs at runtime. Building on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, Han et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> proposed scaling the threshold by the fan-in and fan-out of the IF neuron. Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> introduced channel-wise weight normalization to eliminate extremely small activations and implemented Spiking-YOLO for object detection, which incorporates negative spikes to represent negative activations.</p>
</div>
<div id="S2.SS1.SSS1.p6" class="ltx_para">
<p id="S2.SS1.SSS1.p6.1" class="ltx_p">To improve the performance of converted SNNs, several interesting works have utilized fine-tuning after conversion. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, Yan et al. proposed a framework to adjust pre-trained ANNs by incorporating knowledge of temporal quantization in SNNs. They introduced a residual term in ANNs to emulate the residual membrane potential in SNNs and reduce quantization error. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, Wu et al. proposed a hybrid framework called progressive tandem learning to fine-tune full-precision floating-point ANNs with knowledge of temporal quantization.</p>
</div>
<div id="S2.SS1.SSS1.p7" class="ltx_para">
<p id="S2.SS1.SSS1.p7.1" class="ltx_p">Aiming to mitigate conversion errors that degrade the performance and increase the inference latency of converted SNNs, several works have further analyzed the conversion process and developed methods to facilitate ANN-to-SNN conversion. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, Hu et al. proposed countering the accumulating error by increasing the firing rate of neurons in deeper layers based on statistically estimated error. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, Deng et al. suggested training ANNs using capped ReLU functions, i.e., ReLU1 and ReLU2, and then applying a scaling factor to normalize the firing thresholds by the maximum activation of the capped ReLU function. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, Li et al. introduced layer-wise calibration to optimize the weights of SNNs, correcting conversion errors layer by layer. Instead of optimizing synaptic weights, Bu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> proposed optimizing the initial membrane potential to reduce conversion errors. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, Bu et al. introduced a quantization clip-floor-shift activation function to replace ReLU, achieving ultra-low latency (4 time steps) for converted SNNs. Through an analysis of the equivalence between ANN quantization and SNN spike firing, Hu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> proposed a mapping framework that facilitates conversion from quantized ANNs to SNNs. They also demonstrated a signed IF neuron model and a layer-wise fine-tuning scheme to address sequential errors in low-latency SNNs. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, Li et al. proposed a set of layer-wise parameter calibration algorithms to tackle activation mismatch.</p>
</div>
<div id="S2.SS1.SSS1.p8" class="ltx_para">
<p id="S2.SS1.SSS1.p8.1" class="ltx_p">In Table <a href="#S2.T1" title="TABLE I ‣ 2.1.1 ANN-to-SNN Conversion ‣ 2.1 Learning Rules ‣ 2 Deep Spiking Neural Networks ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we summarize the state-of-the-art results of ANN-to-SNN conversion methods on the CIFAR-10, and ImageNet datasets.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Direct Training with Surrogate Gradients</h4>

<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison of Recent Direct Training Algorithms</figcaption>
<table id="S2.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.1.1.1" class="ltx_tr">
<td id="S2.T2.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S2.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S2.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Work</span></th>
<th id="S2.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Architecture</span></th>
<th id="S2.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T2.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.1.1.5.1.1" class="ltx_tr">
<td id="S2.T2.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T2.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Time</span></td>
</tr>
<tr id="S2.T2.1.1.1.5.1.2" class="ltx_tr">
<td id="S2.T2.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T2.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">Steps</span></td>
</tr>
</table>
</th>
<th id="S2.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T2.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.1.1.6.1.1" class="ltx_tr">
<td id="S2.T2.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T2.1.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">SNN</span></td>
</tr>
<tr id="S2.T2.1.1.1.6.1.2" class="ltx_tr">
<td id="S2.T2.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T2.1.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold">Acc. (%)</span></td>
</tr>
</table>
</th>
</tr>
<tr id="S2.T2.1.2.2" class="ltx_tr">
<td id="S2.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_tt" rowspan="14"><span id="S2.T2.1.2.2.1.1" class="ltx_text">
<span id="S2.T2.1.2.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:44.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:44.4pt;transform:translate(-18.81pt,-18.81pt) rotate(-90deg) ;">
<span id="S2.T2.1.2.2.1.1.1.1" class="ltx_p"><span id="S2.T2.1.2.2.1.1.1.1.1" class="ltx_text ltx_font_bold">CIFAR-10</span></span>
</span></span></span></td>
<td id="S2.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">2021</td>
<td id="S2.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_tt">PLIF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S2.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_tt">CNN</td>
<td id="S2.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_tt">8</td>
<td id="S2.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_tt">93.50</td>
</tr>
<tr id="S2.T2.1.3.3" class="ltx_tr">
<td id="S2.T2.1.3.3.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T2.1.3.3.2" class="ltx_td ltx_align_center">BNTT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
<td id="S2.T2.1.3.3.3" class="ltx_td ltx_align_center">VGG-9</td>
<td id="S2.T2.1.3.3.4" class="ltx_td ltx_align_center">25</td>
<td id="S2.T2.1.3.3.5" class="ltx_td ltx_align_center">90.3</td>
</tr>
<tr id="S2.T2.1.4.4" class="ltx_tr">
<td id="S2.T2.1.4.4.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T2.1.4.4.2" class="ltx_td ltx_align_center">STBP-tdBN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S2.T2.1.4.4.3" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.4.4.4" class="ltx_td ltx_align_center">6</td>
<td id="S2.T2.1.4.4.5" class="ltx_td ltx_align_center">93.16</td>
</tr>
<tr id="S2.T2.1.5.5" class="ltx_tr">
<td id="S2.T2.1.5.5.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T2.1.5.5.2" class="ltx_td ltx_align_center">Diet-SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S2.T2.1.5.5.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T2.1.5.5.4" class="ltx_td ltx_align_center">5</td>
<td id="S2.T2.1.5.5.5" class="ltx_td ltx_align_center">92.70</td>
</tr>
<tr id="S2.T2.1.6.6" class="ltx_tr">
<td id="S2.T2.1.6.6.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T2.1.6.6.2" class="ltx_td ltx_align_center">Dspike <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
<td id="S2.T2.1.6.6.3" class="ltx_td ltx_align_center">ResNet-18</td>
<td id="S2.T2.1.6.6.4" class="ltx_td ltx_align_center">6</td>
<td id="S2.T2.1.6.6.5" class="ltx_td ltx_align_center">94.25</td>
</tr>
<tr id="S2.T2.1.7.7" class="ltx_tr">
<td id="S2.T2.1.7.7.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.7.7.2" class="ltx_td ltx_align_center">TET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S2.T2.1.7.7.3" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.7.7.4" class="ltx_td ltx_align_center">6</td>
<td id="S2.T2.1.7.7.5" class="ltx_td ltx_align_center">94.50</td>
</tr>
<tr id="S2.T2.1.8.8" class="ltx_tr">
<td id="S2.T2.1.8.8.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.8.8.2" class="ltx_td ltx_align_center">IM-loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S2.T2.1.8.8.3" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.8.8.4" class="ltx_td ltx_align_center">6</td>
<td id="S2.T2.1.8.8.5" class="ltx_td ltx_align_center">95.49</td>
</tr>
<tr id="S2.T2.1.9.9" class="ltx_tr">
<td id="S2.T2.1.9.9.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.9.9.2" class="ltx_td ltx_align_center">TEBN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
<td id="S2.T2.1.9.9.3" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.9.9.4" class="ltx_td ltx_align_center">6</td>
<td id="S2.T2.1.9.9.5" class="ltx_td ltx_align_center">94.71</td>
</tr>
<tr id="S2.T2.1.10.10" class="ltx_tr">
<td id="S2.T2.1.10.10.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.10.10.2" class="ltx_td ltx_align_center">LTMD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</td>
<td id="S2.T2.1.10.10.3" class="ltx_td ltx_align_center">DenseNet</td>
<td id="S2.T2.1.10.10.4" class="ltx_td ltx_align_center">4</td>
<td id="S2.T2.1.10.10.5" class="ltx_td ltx_align_center">94.19</td>
</tr>
<tr id="S2.T2.1.11.11" class="ltx_tr">
<td id="S2.T2.1.11.11.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.11.11.2" class="ltx_td ltx_align_center">GLIF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</td>
<td id="S2.T2.1.11.11.3" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.11.11.4" class="ltx_td ltx_align_center">6</td>
<td id="S2.T2.1.11.11.5" class="ltx_td ltx_align_center">95.03</td>
</tr>
<tr id="S2.T2.1.12.12" class="ltx_tr">
<td id="S2.T2.1.12.12.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.12.12.2" class="ltx_td ltx_align_center">RecDis-SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</td>
<td id="S2.T2.1.12.12.3" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.12.12.4" class="ltx_td ltx_align_center">6</td>
<td id="S2.T2.1.12.12.5" class="ltx_td ltx_align_center">95.55</td>
</tr>
<tr id="S2.T2.1.13.13" class="ltx_tr">
<td id="S2.T2.1.13.13.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T2.1.13.13.2" class="ltx_td ltx_align_center">LSG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
<td id="S2.T2.1.13.13.3" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.13.13.4" class="ltx_td ltx_align_center">6</td>
<td id="S2.T2.1.13.13.5" class="ltx_td ltx_align_center">95.52</td>
</tr>
<tr id="S2.T2.1.14.14" class="ltx_tr">
<td id="S2.T2.1.14.14.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T2.1.14.14.2" class="ltx_td ltx_align_center">MPBN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
<td id="S2.T2.1.14.14.3" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.14.14.4" class="ltx_td ltx_align_center">2</td>
<td id="S2.T2.1.14.14.5" class="ltx_td ltx_align_center">96.47</td>
</tr>
<tr id="S2.T2.1.15.15" class="ltx_tr">
<td id="S2.T2.1.15.15.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T2.1.15.15.2" class="ltx_td ltx_align_center">KDSNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>
</td>
<td id="S2.T2.1.15.15.3" class="ltx_td ltx_align_center">ResNet-18</td>
<td id="S2.T2.1.15.15.4" class="ltx_td ltx_align_center">4</td>
<td id="S2.T2.1.15.15.5" class="ltx_td ltx_align_center">93.41</td>
</tr>
<tr id="S2.T2.1.16.16" class="ltx_tr">
<td id="S2.T2.1.16.16.1" class="ltx_td"></td>
<td id="S2.T2.1.16.16.2" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T2.1.16.16.3" class="ltx_td ltx_align_center">IM-LIF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
<td id="S2.T2.1.16.16.4" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.16.16.5" class="ltx_td ltx_align_center">3</td>
<td id="S2.T2.1.16.16.6" class="ltx_td ltx_align_center">95.29</td>
</tr>
<tr id="S2.T2.1.17.17" class="ltx_tr">
<td id="S2.T2.1.17.17.1" class="ltx_td"></td>
<td id="S2.T2.1.17.17.2" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T2.1.17.17.3" class="ltx_td ltx_align_center">LocalZO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S2.T2.1.17.17.4" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.17.17.5" class="ltx_td ltx_align_center">2</td>
<td id="S2.T2.1.17.17.6" class="ltx_td ltx_align_center">95.03</td>
</tr>
<tr id="S2.T2.1.18.18" class="ltx_tr">
<td id="S2.T2.1.18.18.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="9"><span id="S2.T2.1.18.18.1.1" class="ltx_text">
<span id="S2.T2.1.18.18.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:42.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:42.2pt;transform:translate(-16.72pt,-15.75pt) rotate(-90deg) ;">
<span id="S2.T2.1.18.18.1.1.1.1" class="ltx_p"><span id="S2.T2.1.18.18.1.1.1.1.1" class="ltx_text ltx_font_bold">ImageNet</span></span>
</span></span></span></td>
<td id="S2.T2.1.18.18.2" class="ltx_td ltx_align_center ltx_border_t">2021</td>
<td id="S2.T2.1.18.18.3" class="ltx_td ltx_align_center ltx_border_t">STBP-tdBN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S2.T2.1.18.18.4" class="ltx_td ltx_align_center ltx_border_t">ResNet-50</td>
<td id="S2.T2.1.18.18.5" class="ltx_td ltx_align_center ltx_border_t">6</td>
<td id="S2.T2.1.18.18.6" class="ltx_td ltx_align_center ltx_border_t">64.88</td>
</tr>
<tr id="S2.T2.1.19.19" class="ltx_tr">
<td id="S2.T2.1.19.19.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T2.1.19.19.2" class="ltx_td ltx_align_center">Diet-SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S2.T2.1.19.19.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T2.1.19.19.4" class="ltx_td ltx_align_center">5</td>
<td id="S2.T2.1.19.19.5" class="ltx_td ltx_align_center">69.00</td>
</tr>
<tr id="S2.T2.1.20.20" class="ltx_tr">
<td id="S2.T2.1.20.20.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.20.20.2" class="ltx_td ltx_align_center">TET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S2.T2.1.20.20.3" class="ltx_td ltx_align_center">ResNet-34</td>
<td id="S2.T2.1.20.20.4" class="ltx_td ltx_align_center">6</td>
<td id="S2.T2.1.20.20.5" class="ltx_td ltx_align_center">64.79</td>
</tr>
<tr id="S2.T2.1.21.21" class="ltx_tr">
<td id="S2.T2.1.21.21.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.21.21.2" class="ltx_td ltx_align_center">IM-loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S2.T2.1.21.21.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T2.1.21.21.4" class="ltx_td ltx_align_center">5</td>
<td id="S2.T2.1.21.21.5" class="ltx_td ltx_align_center">70.65</td>
</tr>
<tr id="S2.T2.1.22.22" class="ltx_tr">
<td id="S2.T2.1.22.22.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.22.22.2" class="ltx_td ltx_align_center">TEBN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
<td id="S2.T2.1.22.22.3" class="ltx_td ltx_align_center">ResNet-34</td>
<td id="S2.T2.1.22.22.4" class="ltx_td ltx_align_center">4</td>
<td id="S2.T2.1.22.22.5" class="ltx_td ltx_align_center">64.29</td>
</tr>
<tr id="S2.T2.1.23.23" class="ltx_tr">
<td id="S2.T2.1.23.23.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.23.23.2" class="ltx_td ltx_align_center">GLIF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</td>
<td id="S2.T2.1.23.23.3" class="ltx_td ltx_align_center">ResNet-34</td>
<td id="S2.T2.1.23.23.4" class="ltx_td ltx_align_center">4</td>
<td id="S2.T2.1.23.23.5" class="ltx_td ltx_align_center">67.52</td>
</tr>
<tr id="S2.T2.1.24.24" class="ltx_tr">
<td id="S2.T2.1.24.24.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.24.24.2" class="ltx_td ltx_align_center">RecDis-SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</td>
<td id="S2.T2.1.24.24.3" class="ltx_td ltx_align_center">ResNet-34</td>
<td id="S2.T2.1.24.24.4" class="ltx_td ltx_align_center">6</td>
<td id="S2.T2.1.24.24.5" class="ltx_td ltx_align_center">67.33</td>
</tr>
<tr id="S2.T2.1.25.25" class="ltx_tr">
<td id="S2.T2.1.25.25.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T2.1.25.25.2" class="ltx_td ltx_align_center">MPBN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
<td id="S2.T2.1.25.25.3" class="ltx_td ltx_align_center">ResNet-34</td>
<td id="S2.T2.1.25.25.4" class="ltx_td ltx_align_center">4</td>
<td id="S2.T2.1.25.25.5" class="ltx_td ltx_align_center">64.71</td>
</tr>
<tr id="S2.T2.1.26.26" class="ltx_tr">
<td id="S2.T2.1.26.26.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T2.1.26.26.2" class="ltx_td ltx_align_center">Attention SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
</td>
<td id="S2.T2.1.26.26.3" class="ltx_td ltx_align_center">ResNet-34</td>
<td id="S2.T2.1.26.26.4" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.1.26.26.5" class="ltx_td ltx_align_center">69.15</td>
</tr>
<tr id="S2.T2.1.27.27" class="ltx_tr">
<td id="S2.T2.1.27.27.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="15"><span id="S2.T2.1.27.27.1.1" class="ltx_text">
<span id="S2.T2.1.27.27.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:64.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:64.9pt;transform:translate(-29.01pt,-29.01pt) rotate(-90deg) ;">
<span id="S2.T2.1.27.27.1.1.1.1" class="ltx_p"><span id="S2.T2.1.27.27.1.1.1.1.1" class="ltx_text ltx_font_bold">DVS CIFAR10</span></span>
</span></span></span></td>
<td id="S2.T2.1.27.27.2" class="ltx_td ltx_align_center ltx_border_t">2021</td>
<td id="S2.T2.1.27.27.3" class="ltx_td ltx_align_center ltx_border_t">PLIF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S2.T2.1.27.27.4" class="ltx_td ltx_align_center ltx_border_t">CNN</td>
<td id="S2.T2.1.27.27.5" class="ltx_td ltx_align_center ltx_border_t">20</td>
<td id="S2.T2.1.27.27.6" class="ltx_td ltx_align_center ltx_border_t">74.80</td>
</tr>
<tr id="S2.T2.1.28.28" class="ltx_tr">
<td id="S2.T2.1.28.28.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T2.1.28.28.2" class="ltx_td ltx_align_center">BNTT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
</td>
<td id="S2.T2.1.28.28.3" class="ltx_td ltx_align_center">VGG-9</td>
<td id="S2.T2.1.28.28.4" class="ltx_td ltx_align_center">N/A</td>
<td id="S2.T2.1.28.28.5" class="ltx_td ltx_align_center">63.2</td>
</tr>
<tr id="S2.T2.1.29.29" class="ltx_tr">
<td id="S2.T2.1.29.29.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T2.1.29.29.2" class="ltx_td ltx_align_center">STBP-tdBN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S2.T2.1.29.29.3" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.29.29.4" class="ltx_td ltx_align_center">10</td>
<td id="S2.T2.1.29.29.5" class="ltx_td ltx_align_center">67.8</td>
</tr>
<tr id="S2.T2.1.30.30" class="ltx_tr">
<td id="S2.T2.1.30.30.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T2.1.30.30.2" class="ltx_td ltx_align_center">Dspike <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
<td id="S2.T2.1.30.30.3" class="ltx_td ltx_align_center">ResNet-18</td>
<td id="S2.T2.1.30.30.4" class="ltx_td ltx_align_center">10</td>
<td id="S2.T2.1.30.30.5" class="ltx_td ltx_align_center">75.4</td>
</tr>
<tr id="S2.T2.1.31.31" class="ltx_tr">
<td id="S2.T2.1.31.31.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T2.1.31.31.2" class="ltx_td ltx_align_center">TA-SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S2.T2.1.31.31.3" class="ltx_td ltx_align_center">CNN</td>
<td id="S2.T2.1.31.31.4" class="ltx_td ltx_align_center">10</td>
<td id="S2.T2.1.31.31.5" class="ltx_td ltx_align_center">72.00</td>
</tr>
<tr id="S2.T2.1.32.32" class="ltx_tr">
<td id="S2.T2.1.32.32.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.32.32.2" class="ltx_td ltx_align_center">TET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S2.T2.1.32.32.3" class="ltx_td ltx_align_center">VGGSNN</td>
<td id="S2.T2.1.32.32.4" class="ltx_td ltx_align_center">6</td>
<td id="S2.T2.1.32.32.5" class="ltx_td ltx_align_center">77.33</td>
</tr>
<tr id="S2.T2.1.33.33" class="ltx_tr">
<td id="S2.T2.1.33.33.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.33.33.2" class="ltx_td ltx_align_center">IM-loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S2.T2.1.33.33.3" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.33.33.4" class="ltx_td ltx_align_center">10</td>
<td id="S2.T2.1.33.33.5" class="ltx_td ltx_align_center">72.60</td>
</tr>
<tr id="S2.T2.1.34.34" class="ltx_tr">
<td id="S2.T2.1.34.34.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.34.34.2" class="ltx_td ltx_align_center">TEBN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
<td id="S2.T2.1.34.34.3" class="ltx_td ltx_align_center">CNN</td>
<td id="S2.T2.1.34.34.4" class="ltx_td ltx_align_center">4</td>
<td id="S2.T2.1.34.34.5" class="ltx_td ltx_align_center">75.10</td>
</tr>
<tr id="S2.T2.1.35.35" class="ltx_tr">
<td id="S2.T2.1.35.35.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.35.35.2" class="ltx_td ltx_align_center">STSC-SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S2.T2.1.35.35.3" class="ltx_td ltx_align_center">CNN</td>
<td id="S2.T2.1.35.35.4" class="ltx_td ltx_align_center">10</td>
<td id="S2.T2.1.35.35.5" class="ltx_td ltx_align_center">81.40</td>
</tr>
<tr id="S2.T2.1.36.36" class="ltx_tr">
<td id="S2.T2.1.36.36.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.36.36.2" class="ltx_td ltx_align_center">LTMD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</td>
<td id="S2.T2.1.36.36.3" class="ltx_td ltx_align_center">DenseNet</td>
<td id="S2.T2.1.36.36.4" class="ltx_td ltx_align_center">4</td>
<td id="S2.T2.1.36.36.5" class="ltx_td ltx_align_center">73.30</td>
</tr>
<tr id="S2.T2.1.37.37" class="ltx_tr">
<td id="S2.T2.1.37.37.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.37.37.2" class="ltx_td ltx_align_center">GLIF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</td>
<td id="S2.T2.1.37.37.3" class="ltx_td ltx_align_center">7B-wideNet</td>
<td id="S2.T2.1.37.37.4" class="ltx_td ltx_align_center">16</td>
<td id="S2.T2.1.37.37.5" class="ltx_td ltx_align_center">78.10</td>
</tr>
<tr id="S2.T2.1.38.38" class="ltx_tr">
<td id="S2.T2.1.38.38.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T2.1.38.38.2" class="ltx_td ltx_align_center">RecDis-SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</td>
<td id="S2.T2.1.38.38.3" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.38.38.4" class="ltx_td ltx_align_center">10</td>
<td id="S2.T2.1.38.38.5" class="ltx_td ltx_align_center">72.42</td>
</tr>
<tr id="S2.T2.1.39.39" class="ltx_tr">
<td id="S2.T2.1.39.39.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T2.1.39.39.2" class="ltx_td ltx_align_center">LSG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
<td id="S2.T2.1.39.39.3" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.39.39.4" class="ltx_td ltx_align_center">2</td>
<td id="S2.T2.1.39.39.5" class="ltx_td ltx_align_center">77.50</td>
</tr>
<tr id="S2.T2.1.40.40" class="ltx_tr">
<td id="S2.T2.1.40.40.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T2.1.40.40.2" class="ltx_td ltx_align_center">MPBN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
<td id="S2.T2.1.40.40.3" class="ltx_td ltx_align_center">ResNet-19</td>
<td id="S2.T2.1.40.40.4" class="ltx_td ltx_align_center">10</td>
<td id="S2.T2.1.40.40.5" class="ltx_td ltx_align_center">74.40</td>
</tr>
<tr id="S2.T2.1.41.41" class="ltx_tr">
<td id="S2.T2.1.41.41.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T2.1.41.41.2" class="ltx_td ltx_align_center">IM-LIF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
<td id="S2.T2.1.41.41.3" class="ltx_td ltx_align_center">VGGSNN</td>
<td id="S2.T2.1.41.41.4" class="ltx_td ltx_align_center">10</td>
<td id="S2.T2.1.41.41.5" class="ltx_td ltx_align_center">80.50</td>
</tr>
<tr id="S2.T2.1.42.42" class="ltx_tr">
<td id="S2.T2.1.42.42.1" class="ltx_td ltx_border_bb"></td>
<td id="S2.T2.1.42.42.2" class="ltx_td ltx_align_center ltx_border_bb">2024</td>
<td id="S2.T2.1.42.42.3" class="ltx_td ltx_align_center ltx_border_bb">LocalZO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S2.T2.1.42.42.4" class="ltx_td ltx_align_center ltx_border_bb">VGGSNN</td>
<td id="S2.T2.1.42.42.5" class="ltx_td ltx_align_center ltx_border_bb">10</td>
<td id="S2.T2.1.42.42.6" class="ltx_td ltx_align_center ltx_border_bb">79.86</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">Direct training of spiking neural networks (SNNs) with surrogate gradients enables the use of standard optimization algorithms like stochastic gradient descent (SGD) or Adam by providing smooth approximations. This offers streamlined end-to-end learning, simplifying the training process of SNNs.</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.9" class="ltx_p">To address the discontinuous spiking function, researchers employ surrogate gradients (derivatives of continuously differentiable functions) to approximate the derivative of the spiking nonlinearity. For deep spiking neural networks, a popular method is to treat SNNs as recurrent neural networks (RNNs) with binary outputs and use backpropagation-through-time (BPTT) to train SNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Similar to the iterative application of the chain rule in RNNs, BPTT unrolls SNNs and propagates gradients from the loss function to all descendants. For example, synaptic weights can be updated using the following rule:</p>
<table id="S2.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E5.m1.5" class="ltx_Math" alttext="\dfrac{\partial L}{\partial W}=\sum_{t=1}^{T}\dfrac{\partial L}{\partial s(t)}\dfrac{\partial s(t)}{\partial\vartheta(t)}\dfrac{\partial\vartheta(t)}{\partial W}," display="block"><semantics id="S2.E5.m1.5a"><mrow id="S2.E5.m1.5.5.1" xref="S2.E5.m1.5.5.1.1.cmml"><mrow id="S2.E5.m1.5.5.1.1" xref="S2.E5.m1.5.5.1.1.cmml"><mfrac id="S2.E5.m1.5.5.1.1.2" xref="S2.E5.m1.5.5.1.1.2.cmml"><mrow id="S2.E5.m1.5.5.1.1.2.2" xref="S2.E5.m1.5.5.1.1.2.2.cmml"><mo rspace="0em" id="S2.E5.m1.5.5.1.1.2.2.1" xref="S2.E5.m1.5.5.1.1.2.2.1.cmml">∂</mo><mi id="S2.E5.m1.5.5.1.1.2.2.2" xref="S2.E5.m1.5.5.1.1.2.2.2.cmml">L</mi></mrow><mrow id="S2.E5.m1.5.5.1.1.2.3" xref="S2.E5.m1.5.5.1.1.2.3.cmml"><mo rspace="0em" id="S2.E5.m1.5.5.1.1.2.3.1" xref="S2.E5.m1.5.5.1.1.2.3.1.cmml">∂</mo><mi id="S2.E5.m1.5.5.1.1.2.3.2" xref="S2.E5.m1.5.5.1.1.2.3.2.cmml">W</mi></mrow></mfrac><mo rspace="0.111em" id="S2.E5.m1.5.5.1.1.1" xref="S2.E5.m1.5.5.1.1.1.cmml">=</mo><mrow id="S2.E5.m1.5.5.1.1.3" xref="S2.E5.m1.5.5.1.1.3.cmml"><munderover id="S2.E5.m1.5.5.1.1.3.1" xref="S2.E5.m1.5.5.1.1.3.1.cmml"><mo movablelimits="false" id="S2.E5.m1.5.5.1.1.3.1.2.2" xref="S2.E5.m1.5.5.1.1.3.1.2.2.cmml">∑</mo><mrow id="S2.E5.m1.5.5.1.1.3.1.2.3" xref="S2.E5.m1.5.5.1.1.3.1.2.3.cmml"><mi id="S2.E5.m1.5.5.1.1.3.1.2.3.2" xref="S2.E5.m1.5.5.1.1.3.1.2.3.2.cmml">t</mi><mo id="S2.E5.m1.5.5.1.1.3.1.2.3.1" xref="S2.E5.m1.5.5.1.1.3.1.2.3.1.cmml">=</mo><mn id="S2.E5.m1.5.5.1.1.3.1.2.3.3" xref="S2.E5.m1.5.5.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E5.m1.5.5.1.1.3.1.3" xref="S2.E5.m1.5.5.1.1.3.1.3.cmml">T</mi></munderover><mrow id="S2.E5.m1.5.5.1.1.3.2" xref="S2.E5.m1.5.5.1.1.3.2.cmml"><mfrac id="S2.E5.m1.1.1" xref="S2.E5.m1.1.1.cmml"><mrow id="S2.E5.m1.1.1.3" xref="S2.E5.m1.1.1.3.cmml"><mo rspace="0em" id="S2.E5.m1.1.1.3.1" xref="S2.E5.m1.1.1.3.1.cmml">∂</mo><mi id="S2.E5.m1.1.1.3.2" xref="S2.E5.m1.1.1.3.2.cmml">L</mi></mrow><mrow id="S2.E5.m1.1.1.1" xref="S2.E5.m1.1.1.1.cmml"><mo rspace="0em" id="S2.E5.m1.1.1.1.2" xref="S2.E5.m1.1.1.1.2.cmml">∂</mo><mrow id="S2.E5.m1.1.1.1.3" xref="S2.E5.m1.1.1.1.3.cmml"><mi id="S2.E5.m1.1.1.1.3.2" xref="S2.E5.m1.1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.1.3.1" xref="S2.E5.m1.1.1.1.3.1.cmml">​</mo><mrow id="S2.E5.m1.1.1.1.3.3.2" xref="S2.E5.m1.1.1.1.3.cmml"><mo stretchy="false" id="S2.E5.m1.1.1.1.3.3.2.1" xref="S2.E5.m1.1.1.1.3.cmml">(</mo><mi id="S2.E5.m1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.E5.m1.1.1.1.3.3.2.2" xref="S2.E5.m1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em" id="S2.E5.m1.5.5.1.1.3.2.1" xref="S2.E5.m1.5.5.1.1.3.2.1.cmml">​</mo><mfrac id="S2.E5.m1.3.3" xref="S2.E5.m1.3.3.cmml"><mrow id="S2.E5.m1.2.2.1" xref="S2.E5.m1.2.2.1.cmml"><mo rspace="0em" id="S2.E5.m1.2.2.1.2" xref="S2.E5.m1.2.2.1.2.cmml">∂</mo><mrow id="S2.E5.m1.2.2.1.3" xref="S2.E5.m1.2.2.1.3.cmml"><mi id="S2.E5.m1.2.2.1.3.2" xref="S2.E5.m1.2.2.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.1.3.1" xref="S2.E5.m1.2.2.1.3.1.cmml">​</mo><mrow id="S2.E5.m1.2.2.1.3.3.2" xref="S2.E5.m1.2.2.1.3.cmml"><mo stretchy="false" id="S2.E5.m1.2.2.1.3.3.2.1" xref="S2.E5.m1.2.2.1.3.cmml">(</mo><mi id="S2.E5.m1.2.2.1.1" xref="S2.E5.m1.2.2.1.1.cmml">t</mi><mo stretchy="false" id="S2.E5.m1.2.2.1.3.3.2.2" xref="S2.E5.m1.2.2.1.3.cmml">)</mo></mrow></mrow></mrow><mrow id="S2.E5.m1.3.3.2" xref="S2.E5.m1.3.3.2.cmml"><mo rspace="0em" id="S2.E5.m1.3.3.2.2" xref="S2.E5.m1.3.3.2.2.cmml">∂</mo><mrow id="S2.E5.m1.3.3.2.3" xref="S2.E5.m1.3.3.2.3.cmml"><mi id="S2.E5.m1.3.3.2.3.2" xref="S2.E5.m1.3.3.2.3.2.cmml">ϑ</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.3.3.2.3.1" xref="S2.E5.m1.3.3.2.3.1.cmml">​</mo><mrow id="S2.E5.m1.3.3.2.3.3.2" xref="S2.E5.m1.3.3.2.3.cmml"><mo stretchy="false" id="S2.E5.m1.3.3.2.3.3.2.1" xref="S2.E5.m1.3.3.2.3.cmml">(</mo><mi id="S2.E5.m1.3.3.2.1" xref="S2.E5.m1.3.3.2.1.cmml">t</mi><mo stretchy="false" id="S2.E5.m1.3.3.2.3.3.2.2" xref="S2.E5.m1.3.3.2.3.cmml">)</mo></mrow></mrow></mrow></mfrac><mo lspace="0em" rspace="0em" id="S2.E5.m1.5.5.1.1.3.2.1a" xref="S2.E5.m1.5.5.1.1.3.2.1.cmml">​</mo><mfrac id="S2.E5.m1.4.4" xref="S2.E5.m1.4.4.cmml"><mrow id="S2.E5.m1.4.4.1" xref="S2.E5.m1.4.4.1.cmml"><mo rspace="0em" id="S2.E5.m1.4.4.1.2" xref="S2.E5.m1.4.4.1.2.cmml">∂</mo><mrow id="S2.E5.m1.4.4.1.3" xref="S2.E5.m1.4.4.1.3.cmml"><mi id="S2.E5.m1.4.4.1.3.2" xref="S2.E5.m1.4.4.1.3.2.cmml">ϑ</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.4.4.1.3.1" xref="S2.E5.m1.4.4.1.3.1.cmml">​</mo><mrow id="S2.E5.m1.4.4.1.3.3.2" xref="S2.E5.m1.4.4.1.3.cmml"><mo stretchy="false" id="S2.E5.m1.4.4.1.3.3.2.1" xref="S2.E5.m1.4.4.1.3.cmml">(</mo><mi id="S2.E5.m1.4.4.1.1" xref="S2.E5.m1.4.4.1.1.cmml">t</mi><mo stretchy="false" id="S2.E5.m1.4.4.1.3.3.2.2" xref="S2.E5.m1.4.4.1.3.cmml">)</mo></mrow></mrow></mrow><mrow id="S2.E5.m1.4.4.3" xref="S2.E5.m1.4.4.3.cmml"><mo rspace="0em" id="S2.E5.m1.4.4.3.1" xref="S2.E5.m1.4.4.3.1.cmml">∂</mo><mi id="S2.E5.m1.4.4.3.2" xref="S2.E5.m1.4.4.3.2.cmml">W</mi></mrow></mfrac></mrow></mrow></mrow><mo id="S2.E5.m1.5.5.1.2" xref="S2.E5.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.5b"><apply id="S2.E5.m1.5.5.1.1.cmml" xref="S2.E5.m1.5.5.1"><eq id="S2.E5.m1.5.5.1.1.1.cmml" xref="S2.E5.m1.5.5.1.1.1"></eq><apply id="S2.E5.m1.5.5.1.1.2.cmml" xref="S2.E5.m1.5.5.1.1.2"><divide id="S2.E5.m1.5.5.1.1.2.1.cmml" xref="S2.E5.m1.5.5.1.1.2"></divide><apply id="S2.E5.m1.5.5.1.1.2.2.cmml" xref="S2.E5.m1.5.5.1.1.2.2"><partialdiff id="S2.E5.m1.5.5.1.1.2.2.1.cmml" xref="S2.E5.m1.5.5.1.1.2.2.1"></partialdiff><ci id="S2.E5.m1.5.5.1.1.2.2.2.cmml" xref="S2.E5.m1.5.5.1.1.2.2.2">𝐿</ci></apply><apply id="S2.E5.m1.5.5.1.1.2.3.cmml" xref="S2.E5.m1.5.5.1.1.2.3"><partialdiff id="S2.E5.m1.5.5.1.1.2.3.1.cmml" xref="S2.E5.m1.5.5.1.1.2.3.1"></partialdiff><ci id="S2.E5.m1.5.5.1.1.2.3.2.cmml" xref="S2.E5.m1.5.5.1.1.2.3.2">𝑊</ci></apply></apply><apply id="S2.E5.m1.5.5.1.1.3.cmml" xref="S2.E5.m1.5.5.1.1.3"><apply id="S2.E5.m1.5.5.1.1.3.1.cmml" xref="S2.E5.m1.5.5.1.1.3.1"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.1.1.3.1.1.cmml" xref="S2.E5.m1.5.5.1.1.3.1">superscript</csymbol><apply id="S2.E5.m1.5.5.1.1.3.1.2.cmml" xref="S2.E5.m1.5.5.1.1.3.1"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.1.1.3.1.2.1.cmml" xref="S2.E5.m1.5.5.1.1.3.1">subscript</csymbol><sum id="S2.E5.m1.5.5.1.1.3.1.2.2.cmml" xref="S2.E5.m1.5.5.1.1.3.1.2.2"></sum><apply id="S2.E5.m1.5.5.1.1.3.1.2.3.cmml" xref="S2.E5.m1.5.5.1.1.3.1.2.3"><eq id="S2.E5.m1.5.5.1.1.3.1.2.3.1.cmml" xref="S2.E5.m1.5.5.1.1.3.1.2.3.1"></eq><ci id="S2.E5.m1.5.5.1.1.3.1.2.3.2.cmml" xref="S2.E5.m1.5.5.1.1.3.1.2.3.2">𝑡</ci><cn type="integer" id="S2.E5.m1.5.5.1.1.3.1.2.3.3.cmml" xref="S2.E5.m1.5.5.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S2.E5.m1.5.5.1.1.3.1.3.cmml" xref="S2.E5.m1.5.5.1.1.3.1.3">𝑇</ci></apply><apply id="S2.E5.m1.5.5.1.1.3.2.cmml" xref="S2.E5.m1.5.5.1.1.3.2"><times id="S2.E5.m1.5.5.1.1.3.2.1.cmml" xref="S2.E5.m1.5.5.1.1.3.2.1"></times><apply id="S2.E5.m1.1.1.cmml" xref="S2.E5.m1.1.1"><divide id="S2.E5.m1.1.1.2.cmml" xref="S2.E5.m1.1.1"></divide><apply id="S2.E5.m1.1.1.3.cmml" xref="S2.E5.m1.1.1.3"><partialdiff id="S2.E5.m1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.3.1"></partialdiff><ci id="S2.E5.m1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.3.2">𝐿</ci></apply><apply id="S2.E5.m1.1.1.1.cmml" xref="S2.E5.m1.1.1.1"><partialdiff id="S2.E5.m1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.2"></partialdiff><apply id="S2.E5.m1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.3"><times id="S2.E5.m1.1.1.1.3.1.cmml" xref="S2.E5.m1.1.1.1.3.1"></times><ci id="S2.E5.m1.1.1.1.3.2.cmml" xref="S2.E5.m1.1.1.1.3.2">𝑠</ci><ci id="S2.E5.m1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1">𝑡</ci></apply></apply></apply><apply id="S2.E5.m1.3.3.cmml" xref="S2.E5.m1.3.3"><divide id="S2.E5.m1.3.3.3.cmml" xref="S2.E5.m1.3.3"></divide><apply id="S2.E5.m1.2.2.1.cmml" xref="S2.E5.m1.2.2.1"><partialdiff id="S2.E5.m1.2.2.1.2.cmml" xref="S2.E5.m1.2.2.1.2"></partialdiff><apply id="S2.E5.m1.2.2.1.3.cmml" xref="S2.E5.m1.2.2.1.3"><times id="S2.E5.m1.2.2.1.3.1.cmml" xref="S2.E5.m1.2.2.1.3.1"></times><ci id="S2.E5.m1.2.2.1.3.2.cmml" xref="S2.E5.m1.2.2.1.3.2">𝑠</ci><ci id="S2.E5.m1.2.2.1.1.cmml" xref="S2.E5.m1.2.2.1.1">𝑡</ci></apply></apply><apply id="S2.E5.m1.3.3.2.cmml" xref="S2.E5.m1.3.3.2"><partialdiff id="S2.E5.m1.3.3.2.2.cmml" xref="S2.E5.m1.3.3.2.2"></partialdiff><apply id="S2.E5.m1.3.3.2.3.cmml" xref="S2.E5.m1.3.3.2.3"><times id="S2.E5.m1.3.3.2.3.1.cmml" xref="S2.E5.m1.3.3.2.3.1"></times><ci id="S2.E5.m1.3.3.2.3.2.cmml" xref="S2.E5.m1.3.3.2.3.2">italic-ϑ</ci><ci id="S2.E5.m1.3.3.2.1.cmml" xref="S2.E5.m1.3.3.2.1">𝑡</ci></apply></apply></apply><apply id="S2.E5.m1.4.4.cmml" xref="S2.E5.m1.4.4"><divide id="S2.E5.m1.4.4.2.cmml" xref="S2.E5.m1.4.4"></divide><apply id="S2.E5.m1.4.4.1.cmml" xref="S2.E5.m1.4.4.1"><partialdiff id="S2.E5.m1.4.4.1.2.cmml" xref="S2.E5.m1.4.4.1.2"></partialdiff><apply id="S2.E5.m1.4.4.1.3.cmml" xref="S2.E5.m1.4.4.1.3"><times id="S2.E5.m1.4.4.1.3.1.cmml" xref="S2.E5.m1.4.4.1.3.1"></times><ci id="S2.E5.m1.4.4.1.3.2.cmml" xref="S2.E5.m1.4.4.1.3.2">italic-ϑ</ci><ci id="S2.E5.m1.4.4.1.1.cmml" xref="S2.E5.m1.4.4.1.1">𝑡</ci></apply></apply><apply id="S2.E5.m1.4.4.3.cmml" xref="S2.E5.m1.4.4.3"><partialdiff id="S2.E5.m1.4.4.3.1.cmml" xref="S2.E5.m1.4.4.3.1"></partialdiff><ci id="S2.E5.m1.4.4.3.2.cmml" xref="S2.E5.m1.4.4.3.2">𝑊</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.5c">\dfrac{\partial L}{\partial W}=\sum_{t=1}^{T}\dfrac{\partial L}{\partial s(t)}\dfrac{\partial s(t)}{\partial\vartheta(t)}\dfrac{\partial\vartheta(t)}{\partial W},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.SSS2.p2.6" class="ltx_p">where <math id="S2.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.SS1.SSS2.p2.1.m1.1a"><mi id="S2.SS1.SSS2.p2.1.m1.1.1" xref="S2.SS1.SSS2.p2.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.1.m1.1b"><ci id="S2.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p2.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.1.m1.1c">L</annotation></semantics></math> denotes the loss, <math id="S2.SS1.SSS2.p2.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.SS1.SSS2.p2.2.m2.1a"><mi id="S2.SS1.SSS2.p2.2.m2.1.1" xref="S2.SS1.SSS2.p2.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.2.m2.1b"><ci id="S2.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.2.m2.1c">s</annotation></semantics></math> denotes the output, <math id="S2.SS1.SSS2.p2.3.m3.1" class="ltx_Math" alttext="\vartheta" display="inline"><semantics id="S2.SS1.SSS2.p2.3.m3.1a"><mi id="S2.SS1.SSS2.p2.3.m3.1.1" xref="S2.SS1.SSS2.p2.3.m3.1.1.cmml">ϑ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.3.m3.1b"><ci id="S2.SS1.SSS2.p2.3.m3.1.1.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1">italic-ϑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.3.m3.1c">\vartheta</annotation></semantics></math> denotes the membrane potential, <math id="S2.SS1.SSS2.p2.4.m4.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S2.SS1.SSS2.p2.4.m4.1a"><mi id="S2.SS1.SSS2.p2.4.m4.1.1" xref="S2.SS1.SSS2.p2.4.m4.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.4.m4.1b"><ci id="S2.SS1.SSS2.p2.4.m4.1.1.cmml" xref="S2.SS1.SSS2.p2.4.m4.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.4.m4.1c">W</annotation></semantics></math> denotes the synaptic weight, <math id="S2.SS1.SSS2.p2.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS1.SSS2.p2.5.m5.1a"><mi id="S2.SS1.SSS2.p2.5.m5.1.1" xref="S2.SS1.SSS2.p2.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.5.m5.1b"><ci id="S2.SS1.SSS2.p2.5.m5.1.1.cmml" xref="S2.SS1.SSS2.p2.5.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.5.m5.1c">T</annotation></semantics></math> denotes the total number of time steps. To circumvent the non-differentiable spiking mechanism, <math id="S2.SS1.SSS2.p2.6.m6.2" class="ltx_Math" alttext="\partial s(t)/\partial\vartheta(t)" display="inline"><semantics id="S2.SS1.SSS2.p2.6.m6.2a"><mrow id="S2.SS1.SSS2.p2.6.m6.2.3" xref="S2.SS1.SSS2.p2.6.m6.2.3.cmml"><mo rspace="0em" id="S2.SS1.SSS2.p2.6.m6.2.3.1" xref="S2.SS1.SSS2.p2.6.m6.2.3.1.cmml">∂</mo><mrow id="S2.SS1.SSS2.p2.6.m6.2.3.2" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.cmml"><mrow id="S2.SS1.SSS2.p2.6.m6.2.3.2.2" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.2.cmml"><mi id="S2.SS1.SSS2.p2.6.m6.2.3.2.2.2" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p2.6.m6.2.3.2.2.1" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.2.1.cmml">​</mo><mrow id="S2.SS1.SSS2.p2.6.m6.2.3.2.2.3.2" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.2.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p2.6.m6.2.3.2.2.3.2.1" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.2.cmml">(</mo><mi id="S2.SS1.SSS2.p2.6.m6.1.1" xref="S2.SS1.SSS2.p2.6.m6.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.SSS2.p2.6.m6.2.3.2.2.3.2.2" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.2.cmml">)</mo></mrow></mrow><mo id="S2.SS1.SSS2.p2.6.m6.2.3.2.1" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.1.cmml">/</mo><mrow id="S2.SS1.SSS2.p2.6.m6.2.3.2.3" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.3.cmml"><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p2.6.m6.2.3.2.3.1" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.3.1.cmml">∂</mo><mrow id="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.cmml"><mi id="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.2" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.2.cmml">ϑ</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.1" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.1.cmml">​</mo><mrow id="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.3.2" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.3.2.1" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.cmml">(</mo><mi id="S2.SS1.SSS2.p2.6.m6.2.2" xref="S2.SS1.SSS2.p2.6.m6.2.2.cmml">t</mi><mo stretchy="false" id="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.3.2.2" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.6.m6.2b"><apply id="S2.SS1.SSS2.p2.6.m6.2.3.cmml" xref="S2.SS1.SSS2.p2.6.m6.2.3"><partialdiff id="S2.SS1.SSS2.p2.6.m6.2.3.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.2.3.1"></partialdiff><apply id="S2.SS1.SSS2.p2.6.m6.2.3.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.2.3.2"><divide id="S2.SS1.SSS2.p2.6.m6.2.3.2.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.1"></divide><apply id="S2.SS1.SSS2.p2.6.m6.2.3.2.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.2"><times id="S2.SS1.SSS2.p2.6.m6.2.3.2.2.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.2.1"></times><ci id="S2.SS1.SSS2.p2.6.m6.2.3.2.2.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.2.2">𝑠</ci><ci id="S2.SS1.SSS2.p2.6.m6.1.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.1.1">𝑡</ci></apply><apply id="S2.SS1.SSS2.p2.6.m6.2.3.2.3.cmml" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.3"><partialdiff id="S2.SS1.SSS2.p2.6.m6.2.3.2.3.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.3.1"></partialdiff><apply id="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2"><times id="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.1.cmml" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.1"></times><ci id="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.2.3.2.3.2.2">italic-ϑ</ci><ci id="S2.SS1.SSS2.p2.6.m6.2.2.cmml" xref="S2.SS1.SSS2.p2.6.m6.2.2">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.6.m6.2c">\partial s(t)/\partial\vartheta(t)</annotation></semantics></math> is typically replaced by a differential surrogate gradient function to facilitate gradient backpropagation. Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.1.2 Direct Training with Surrogate Gradients ‣ 2.1 Learning Rules ‣ 2 Deep Spiking Neural Networks ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> demonstrates a linear function for generating surrogate gradients:</p>
<table id="S2.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E6.m1.7" class="ltx_Math" alttext="y(x)=\begin{cases}x-\theta+c,&amp;\text{if }\theta\leq x\leq\theta+c,\\
-x+\theta+c,&amp;\text{else if }\theta-c\leq x&lt;\theta,\\
0,&amp;\text{else,}\end{cases}" display="block"><semantics id="S2.E6.m1.7a"><mrow id="S2.E6.m1.7.8" xref="S2.E6.m1.7.8.cmml"><mrow id="S2.E6.m1.7.8.2" xref="S2.E6.m1.7.8.2.cmml"><mi id="S2.E6.m1.7.8.2.2" xref="S2.E6.m1.7.8.2.2.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.E6.m1.7.8.2.1" xref="S2.E6.m1.7.8.2.1.cmml">​</mo><mrow id="S2.E6.m1.7.8.2.3.2" xref="S2.E6.m1.7.8.2.cmml"><mo stretchy="false" id="S2.E6.m1.7.8.2.3.2.1" xref="S2.E6.m1.7.8.2.cmml">(</mo><mi id="S2.E6.m1.7.7" xref="S2.E6.m1.7.7.cmml">x</mi><mo stretchy="false" id="S2.E6.m1.7.8.2.3.2.2" xref="S2.E6.m1.7.8.2.cmml">)</mo></mrow></mrow><mo id="S2.E6.m1.7.8.1" xref="S2.E6.m1.7.8.1.cmml">=</mo><mrow id="S2.E6.m1.6.6" xref="S2.E6.m1.7.8.3.1.cmml"><mo id="S2.E6.m1.6.6.7" xref="S2.E6.m1.7.8.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S2.E6.m1.6.6.6" xref="S2.E6.m1.7.8.3.1.cmml"><mtr id="S2.E6.m1.6.6.6a" xref="S2.E6.m1.7.8.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E6.m1.6.6.6b" xref="S2.E6.m1.7.8.3.1.cmml"><mrow id="S2.E6.m1.1.1.1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E6.m1.1.1.1.1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E6.m1.1.1.1.1.1.1.1.1.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mo id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.cmml">−</mo><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.2.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.3.cmml">θ</mi></mrow><mo id="S2.E6.m1.1.1.1.1.1.1.1.1.1" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S2.E6.m1.1.1.1.1.1.1.1.1.3" xref="S2.E6.m1.1.1.1.1.1.1.1.1.3.cmml">c</mi></mrow><mo id="S2.E6.m1.1.1.1.1.1.1.1.2" xref="S2.E6.m1.1.1.1.1.1.1.1.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E6.m1.6.6.6c" xref="S2.E6.m1.7.8.3.1.cmml"><mrow id="S2.E6.m1.2.2.2.2.2.1.1" xref="S2.E6.m1.2.2.2.2.2.1.1.1.cmml"><mrow id="S2.E6.m1.2.2.2.2.2.1.1.1" xref="S2.E6.m1.2.2.2.2.2.1.1.1.cmml"><mrow id="S2.E6.m1.2.2.2.2.2.1.1.1.2" xref="S2.E6.m1.2.2.2.2.2.1.1.1.2.cmml"><mtext id="S2.E6.m1.2.2.2.2.2.1.1.1.2.2" xref="S2.E6.m1.2.2.2.2.2.1.1.1.2.2a.cmml">if </mtext><mo lspace="0em" rspace="0em" id="S2.E6.m1.2.2.2.2.2.1.1.1.2.1" xref="S2.E6.m1.2.2.2.2.2.1.1.1.2.1.cmml">​</mo><mi id="S2.E6.m1.2.2.2.2.2.1.1.1.2.3" xref="S2.E6.m1.2.2.2.2.2.1.1.1.2.3.cmml">θ</mi></mrow><mo id="S2.E6.m1.2.2.2.2.2.1.1.1.3" xref="S2.E6.m1.2.2.2.2.2.1.1.1.3.cmml">≤</mo><mi id="S2.E6.m1.2.2.2.2.2.1.1.1.4" xref="S2.E6.m1.2.2.2.2.2.1.1.1.4.cmml">x</mi><mo id="S2.E6.m1.2.2.2.2.2.1.1.1.5" xref="S2.E6.m1.2.2.2.2.2.1.1.1.5.cmml">≤</mo><mrow id="S2.E6.m1.2.2.2.2.2.1.1.1.6" xref="S2.E6.m1.2.2.2.2.2.1.1.1.6.cmml"><mi id="S2.E6.m1.2.2.2.2.2.1.1.1.6.2" xref="S2.E6.m1.2.2.2.2.2.1.1.1.6.2.cmml">θ</mi><mo id="S2.E6.m1.2.2.2.2.2.1.1.1.6.1" xref="S2.E6.m1.2.2.2.2.2.1.1.1.6.1.cmml">+</mo><mi id="S2.E6.m1.2.2.2.2.2.1.1.1.6.3" xref="S2.E6.m1.2.2.2.2.2.1.1.1.6.3.cmml">c</mi></mrow></mrow><mo id="S2.E6.m1.2.2.2.2.2.1.1.2" xref="S2.E6.m1.2.2.2.2.2.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S2.E6.m1.6.6.6d" xref="S2.E6.m1.7.8.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E6.m1.6.6.6e" xref="S2.E6.m1.7.8.3.1.cmml"><mrow id="S2.E6.m1.3.3.3.3.1.1.1" xref="S2.E6.m1.3.3.3.3.1.1.1.1.cmml"><mrow id="S2.E6.m1.3.3.3.3.1.1.1.1" xref="S2.E6.m1.3.3.3.3.1.1.1.1.cmml"><mrow id="S2.E6.m1.3.3.3.3.1.1.1.1.2" xref="S2.E6.m1.3.3.3.3.1.1.1.1.2.cmml"><mo id="S2.E6.m1.3.3.3.3.1.1.1.1.2a" xref="S2.E6.m1.3.3.3.3.1.1.1.1.2.cmml">−</mo><mi id="S2.E6.m1.3.3.3.3.1.1.1.1.2.2" xref="S2.E6.m1.3.3.3.3.1.1.1.1.2.2.cmml">x</mi></mrow><mo id="S2.E6.m1.3.3.3.3.1.1.1.1.1" xref="S2.E6.m1.3.3.3.3.1.1.1.1.1.cmml">+</mo><mi id="S2.E6.m1.3.3.3.3.1.1.1.1.3" xref="S2.E6.m1.3.3.3.3.1.1.1.1.3.cmml">θ</mi><mo id="S2.E6.m1.3.3.3.3.1.1.1.1.1a" xref="S2.E6.m1.3.3.3.3.1.1.1.1.1.cmml">+</mo><mi id="S2.E6.m1.3.3.3.3.1.1.1.1.4" xref="S2.E6.m1.3.3.3.3.1.1.1.1.4.cmml">c</mi></mrow><mo id="S2.E6.m1.3.3.3.3.1.1.1.2" xref="S2.E6.m1.3.3.3.3.1.1.1.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E6.m1.6.6.6f" xref="S2.E6.m1.7.8.3.1.cmml"><mrow id="S2.E6.m1.4.4.4.4.2.1.1" xref="S2.E6.m1.4.4.4.4.2.1.1.1.cmml"><mrow id="S2.E6.m1.4.4.4.4.2.1.1.1" xref="S2.E6.m1.4.4.4.4.2.1.1.1.cmml"><mrow id="S2.E6.m1.4.4.4.4.2.1.1.1.2" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.cmml"><mrow id="S2.E6.m1.4.4.4.4.2.1.1.1.2.2" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.cmml"><mtext id="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.2" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.2a.cmml">else if </mtext><mo lspace="0em" rspace="0em" id="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.1" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.1.cmml">​</mo><mi id="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.3" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.3.cmml">θ</mi></mrow><mo id="S2.E6.m1.4.4.4.4.2.1.1.1.2.1" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.1.cmml">−</mo><mi id="S2.E6.m1.4.4.4.4.2.1.1.1.2.3" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.3.cmml">c</mi></mrow><mo id="S2.E6.m1.4.4.4.4.2.1.1.1.3" xref="S2.E6.m1.4.4.4.4.2.1.1.1.3.cmml">≤</mo><mi id="S2.E6.m1.4.4.4.4.2.1.1.1.4" xref="S2.E6.m1.4.4.4.4.2.1.1.1.4.cmml">x</mi><mo id="S2.E6.m1.4.4.4.4.2.1.1.1.5" xref="S2.E6.m1.4.4.4.4.2.1.1.1.5.cmml">&lt;</mo><mi id="S2.E6.m1.4.4.4.4.2.1.1.1.6" xref="S2.E6.m1.4.4.4.4.2.1.1.1.6.cmml">θ</mi></mrow><mo id="S2.E6.m1.4.4.4.4.2.1.1.2" xref="S2.E6.m1.4.4.4.4.2.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S2.E6.m1.6.6.6g" xref="S2.E6.m1.7.8.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E6.m1.6.6.6h" xref="S2.E6.m1.7.8.3.1.cmml"><mrow id="S2.E6.m1.5.5.5.5.1.1.3" xref="S2.E6.m1.7.8.3.1.cmml"><mn id="S2.E6.m1.5.5.5.5.1.1.1" xref="S2.E6.m1.5.5.5.5.1.1.1.cmml">0</mn><mo id="S2.E6.m1.5.5.5.5.1.1.3.1" xref="S2.E6.m1.7.8.3.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E6.m1.6.6.6i" xref="S2.E6.m1.7.8.3.1.cmml"><mtext id="S2.E6.m1.6.6.6.6.2.1" xref="S2.E6.m1.6.6.6.6.2.1a.cmml">else,</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E6.m1.7b"><apply id="S2.E6.m1.7.8.cmml" xref="S2.E6.m1.7.8"><eq id="S2.E6.m1.7.8.1.cmml" xref="S2.E6.m1.7.8.1"></eq><apply id="S2.E6.m1.7.8.2.cmml" xref="S2.E6.m1.7.8.2"><times id="S2.E6.m1.7.8.2.1.cmml" xref="S2.E6.m1.7.8.2.1"></times><ci id="S2.E6.m1.7.8.2.2.cmml" xref="S2.E6.m1.7.8.2.2">𝑦</ci><ci id="S2.E6.m1.7.7.cmml" xref="S2.E6.m1.7.7">𝑥</ci></apply><apply id="S2.E6.m1.7.8.3.1.cmml" xref="S2.E6.m1.6.6"><csymbol cd="latexml" id="S2.E6.m1.7.8.3.1.1.cmml" xref="S2.E6.m1.6.6.7">cases</csymbol><apply id="S2.E6.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1"><plus id="S2.E6.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.1"></plus><apply id="S2.E6.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2"><minus id="S2.E6.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.1"></minus><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.2.3">𝜃</ci></apply><ci id="S2.E6.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E6.m1.1.1.1.1.1.1.1.1.3">𝑐</ci></apply><apply id="S2.E6.m1.2.2.2.2.2.1.1.1.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1"><and id="S2.E6.m1.2.2.2.2.2.1.1.1a.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1"></and><apply id="S2.E6.m1.2.2.2.2.2.1.1.1b.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1"><leq id="S2.E6.m1.2.2.2.2.2.1.1.1.3.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1.1.3"></leq><apply id="S2.E6.m1.2.2.2.2.2.1.1.1.2.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1.1.2"><times id="S2.E6.m1.2.2.2.2.2.1.1.1.2.1.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1.1.2.1"></times><ci id="S2.E6.m1.2.2.2.2.2.1.1.1.2.2a.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1.1.2.2"><mtext id="S2.E6.m1.2.2.2.2.2.1.1.1.2.2.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1.1.2.2">if </mtext></ci><ci id="S2.E6.m1.2.2.2.2.2.1.1.1.2.3.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1.1.2.3">𝜃</ci></apply><ci id="S2.E6.m1.2.2.2.2.2.1.1.1.4.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1.1.4">𝑥</ci></apply><apply id="S2.E6.m1.2.2.2.2.2.1.1.1c.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1"><leq id="S2.E6.m1.2.2.2.2.2.1.1.1.5.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1.1.5"></leq><share href="#S2.E6.m1.2.2.2.2.2.1.1.1.4.cmml" id="S2.E6.m1.2.2.2.2.2.1.1.1d.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1"></share><apply id="S2.E6.m1.2.2.2.2.2.1.1.1.6.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1.1.6"><plus id="S2.E6.m1.2.2.2.2.2.1.1.1.6.1.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1.1.6.1"></plus><ci id="S2.E6.m1.2.2.2.2.2.1.1.1.6.2.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1.1.6.2">𝜃</ci><ci id="S2.E6.m1.2.2.2.2.2.1.1.1.6.3.cmml" xref="S2.E6.m1.2.2.2.2.2.1.1.1.6.3">𝑐</ci></apply></apply></apply><apply id="S2.E6.m1.3.3.3.3.1.1.1.1.cmml" xref="S2.E6.m1.3.3.3.3.1.1.1"><plus id="S2.E6.m1.3.3.3.3.1.1.1.1.1.cmml" xref="S2.E6.m1.3.3.3.3.1.1.1.1.1"></plus><apply id="S2.E6.m1.3.3.3.3.1.1.1.1.2.cmml" xref="S2.E6.m1.3.3.3.3.1.1.1.1.2"><minus id="S2.E6.m1.3.3.3.3.1.1.1.1.2.1.cmml" xref="S2.E6.m1.3.3.3.3.1.1.1.1.2"></minus><ci id="S2.E6.m1.3.3.3.3.1.1.1.1.2.2.cmml" xref="S2.E6.m1.3.3.3.3.1.1.1.1.2.2">𝑥</ci></apply><ci id="S2.E6.m1.3.3.3.3.1.1.1.1.3.cmml" xref="S2.E6.m1.3.3.3.3.1.1.1.1.3">𝜃</ci><ci id="S2.E6.m1.3.3.3.3.1.1.1.1.4.cmml" xref="S2.E6.m1.3.3.3.3.1.1.1.1.4">𝑐</ci></apply><apply id="S2.E6.m1.4.4.4.4.2.1.1.1.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1"><and id="S2.E6.m1.4.4.4.4.2.1.1.1a.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1"></and><apply id="S2.E6.m1.4.4.4.4.2.1.1.1b.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1"><leq id="S2.E6.m1.4.4.4.4.2.1.1.1.3.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1.1.3"></leq><apply id="S2.E6.m1.4.4.4.4.2.1.1.1.2.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2"><minus id="S2.E6.m1.4.4.4.4.2.1.1.1.2.1.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.1"></minus><apply id="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.2"><times id="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.1.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.1"></times><ci id="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.2a.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.2"><mtext id="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.2.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.2">else if </mtext></ci><ci id="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.3.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.2.3">𝜃</ci></apply><ci id="S2.E6.m1.4.4.4.4.2.1.1.1.2.3.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1.1.2.3">𝑐</ci></apply><ci id="S2.E6.m1.4.4.4.4.2.1.1.1.4.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1.1.4">𝑥</ci></apply><apply id="S2.E6.m1.4.4.4.4.2.1.1.1c.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1"><lt id="S2.E6.m1.4.4.4.4.2.1.1.1.5.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1.1.5"></lt><share href="#S2.E6.m1.4.4.4.4.2.1.1.1.4.cmml" id="S2.E6.m1.4.4.4.4.2.1.1.1d.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1"></share><ci id="S2.E6.m1.4.4.4.4.2.1.1.1.6.cmml" xref="S2.E6.m1.4.4.4.4.2.1.1.1.6">𝜃</ci></apply></apply><cn type="integer" id="S2.E6.m1.5.5.5.5.1.1.1.cmml" xref="S2.E6.m1.5.5.5.5.1.1.1">0</cn><ci id="S2.E6.m1.6.6.6.6.2.1a.cmml" xref="S2.E6.m1.6.6.6.6.2.1"><mtext id="S2.E6.m1.6.6.6.6.2.1.cmml" xref="S2.E6.m1.6.6.6.6.2.1">else,</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E6.m1.7c">y(x)=\begin{cases}x-\theta+c,&amp;\text{if }\theta\leq x\leq\theta+c,\\
-x+\theta+c,&amp;\text{else if }\theta-c\leq x&lt;\theta,\\
0,&amp;\text{else,}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.SSS2.p2.8" class="ltx_p">where <math id="S2.SS1.SSS2.p2.7.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.SSS2.p2.7.m1.1a"><mi id="S2.SS1.SSS2.p2.7.m1.1.1" xref="S2.SS1.SSS2.p2.7.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.7.m1.1b"><ci id="S2.SS1.SSS2.p2.7.m1.1.1.cmml" xref="S2.SS1.SSS2.p2.7.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.7.m1.1c">\theta</annotation></semantics></math> is the firing threshold and <math id="S2.SS1.SSS2.p2.8.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.SS1.SSS2.p2.8.m2.1a"><mi id="S2.SS1.SSS2.p2.8.m2.1.1" xref="S2.SS1.SSS2.p2.8.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.8.m2.1b"><ci id="S2.SS1.SSS2.p2.8.m2.1.1.cmml" xref="S2.SS1.SSS2.p2.8.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.8.m2.1c">c</annotation></semantics></math> is a constant.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2409.02111/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="253" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Surrogate gradient function (linear) for backpropagation.</figcaption>
</figure>
<div id="S2.SS1.SSS2.p3" class="ltx_para">
<p id="S2.SS1.SSS2.p3.1" class="ltx_p">In early works, Zenke and Ganguli <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> proposed a nonlinear voltage-based three-factor online learning rule using a fast sigmoid function as the surrogate gradient function. Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> introduced a spatio-temporal backpropagation (STBP) framework to simultaneously consider both the spatial and timing-dependent temporal domains during network training. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, Gu et al. proposed spatio-temporal credit assignment (STCA) for BPTT with a temporal-based loss function.</p>
</div>
<div id="S2.SS1.SSS2.p4" class="ltx_para">
<p id="S2.SS1.SSS2.p4.1" class="ltx_p">Researchers have proposed various methods to address the slow convergence and performance degradation caused by the mismatch between surrogate gradients and true gradients. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, Li et al. introduced a differentiable spike function with four parameters to control its shape, based on the estimated loss of the finite difference gradient (FDG). Guo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> proposed adapting the shape of the surrogate gradient function during training by minimizing the information maximization loss (IM-Loss). Guo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> developed RecDis-SNN, which rectifies the membrane potential distribution (MPD) to better align with the surrogate gradient function. Lian et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> introduced the learnable surrogate gradient (LSG), which adjusts the width of the surrogate gradient according to the distribution of the membrane potentials.</p>
</div>
<div id="S2.SS1.SSS2.p5" class="ltx_para">
<p id="S2.SS1.SSS2.p5.1" class="ltx_p">To exploit neuron dynamics and enhance the performance of SNNs, several works have introduced learnable parameters into neuron models. Rathi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> introduced the leakage and threshold parameters in the leaky integrate-and-fire (LIF) neuron model for optimization. Fang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> introduced the Parametric Leaky Integrate-and-Fire (PLIF) neuron, which incorporates the time constant as a learnable parameter. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> proposed learnable thresholding to optimize threshold values during training. Yao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> developed the gated LIF model (GLIF), which integrates bio-inspired features into neuronal behavior.</p>
</div>
<div id="S2.SS1.SSS2.p6" class="ltx_para">
<p id="S2.SS1.SSS2.p6.1" class="ltx_p">To facilitate error backpropagation, several works have introduced normalization techniques. Inspired by batch normalization in CNNs, Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> enhanced spatio-temporal backpropagation (STBP) with a neuron normalization technique. Zheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> proposed threshold-dependent batch normalization (tdBN) for STBP. Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> introduced a batch normalization through time (BNTT) technique. Duan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> developed temporal efficient batch normalization (TEBN), which rescales presynaptic inputs using time-specific learnable weights. Guo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> proposed membrane potential batch normalization (MPBN), which adds an additional batch normalization layer before the firing function to normalize the membrane potential.</p>
</div>
<div id="S2.SS1.SSS2.p7" class="ltx_para">
<p id="S2.SS1.SSS2.p7.1" class="ltx_p">To better extract temporal features, several works have proposed attention mechanisms for SNNs. Yao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> introduced a temporal-wise attention SNN (TA-SNN) to estimate the saliency of each frame and process event streams efficiently. Yu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> proposed a Spatio-Temporal Synaptic Connection SNN (STSC-SNN) model that incorporates temporal convolution and attention mechanisms for synaptic filtering and gating functions. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, Yao et al. presented a multi-dimensional attention module that infers attention weights along the temporal, channel, and spatial dimensions, either separately or simultaneously. Lian et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> introduced an IM-LIF neuron model that utilizes a temporal-wise attention mechanism to adjust the synaptic current equation.</p>
</div>
<div id="S2.SS1.SSS2.p8" class="ltx_para">
<p id="S2.SS1.SSS2.p8.1" class="ltx_p">Demonstrating that the incorrect surrogate gradient makes the SNN easily trapped into a local minimum with poor generalization, Deng et al. TET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> proposed temporal efficient training (TET) to compensate for the loss of momentum in the gradient descent with surrogate gradient. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, Mukhoty et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> proposed to address the loss of gradient information with zeroth-order technique at the local or neuron level.</p>
</div>
<div id="S2.SS1.SSS2.p9" class="ltx_para">
<p id="S2.SS1.SSS2.p9.1" class="ltx_p">To exploit the knowledge of ANNs, researchers have proposed hybrid training and knowledge distillation. Hybrid training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> circumvents the high training costs of backpropagation by using SNNs converted from ANNs for initialization. This method allows for the training of deep SNNs with limited resources and achieves high performance more quickly than random initialization. In contrast, knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> employs teacher ANNs to enhance the performance of SNNs trained with surrogate gradients.</p>
</div>
<div id="S2.SS1.SSS2.p10" class="ltx_para">
<p id="S2.SS1.SSS2.p10.1" class="ltx_p">In Table <a href="#S2.T2" title="TABLE II ‣ 2.1.2 Direct Training with Surrogate Gradients ‣ 2.1 Learning Rules ‣ 2 Deep Spiking Neural Networks ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we summarize the state-of-the-art results of direct training methods on the CIFAR-10, DVS CIFAR10, and ImageNet datasets.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.02111/assets/x3.png" id="S2.F3.sf1.g1" class="ltx_graphics ltx_img_portrait" width="92" height="183" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Spiking-ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.02111/assets/x4.png" id="S2.F3.sf2.g1" class="ltx_graphics ltx_img_portrait" width="92" height="183" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>SEW-ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.02111/assets/x5.png" id="S2.F3.sf3.g1" class="ltx_graphics ltx_img_portrait" width="92" height="183" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>MS-ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Different residual connections in SNNs.</figcaption>
</figure>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span id="S2.SS2.1.1" class="ltx_text ltx_font_italic">Network Architectures in Large Spiking Neural Networks</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In the past decade, deep convolutional neural networks (DCNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> have achieved significant success across various applications. Building on these advancements, the development of deep SNNs has incorporated lessons learned from DCNNs. Recently, ANNs with Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> have set new benchmarks in performance. Large language models based on Transformer backbones have demonstrated remarkable capabilities, generating substantial interest in the neuromorphic computing community. As a result, SNNs incorporating Transformer architecture have become a research hotspot. In this section, we summarize network architectures in deep spiking neural networks, categorizing them into two groups: DCNN Architectures and Transformer Architectures.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>DCNN Architectures</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">In early works, Cao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> demonstrated that convolutional neural networks (CNNs) with ReLU activation functions can be mapped to spiking neural networks (SNNs) with integrate-and-fire (IF) neurons. In this framework, convolution and pooling operations in artificial neural networks (ANNs) can be interpreted as different patterns of synaptic connections in SNNs. Consequently, SNNs can be seen as CNNs with spiking neurons serving as activation functions, which paved the way for building deep SNNs with DCNN architectures. Esser et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> further showed that batch normalization (BN) can be integrated into the firing function during inference. This development facilitates the construction of deep SNNs with DCNN architectures, as batch normalization is a commonly used technique for training DCNNs efficiently. Consequently, popular ANN architectures such as AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, VGG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>, and ResNets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> have become widely employed in SNNs.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">In the search for deep SNN architectures, the ResNet architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> has garnered attention for its effective mitigation of the gradient exploding/vanishing problem. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, Hu et al. demonstrated an ANN-to-SNN conversion method for converting the residual structure and reported that ResNets facilitate conversion by generating lower conversion errors compared to plain networks of the same depth. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, Fang et al. proposed the spike-element-wise ResNet (SEW-ResNet), which replaces the standard residual structure with an activation-before-addition approach, allowing spiking neurons to fire positive integer spikes. While this modification enhances the representation capability of spikes, it also diminishes the advantages of event-driven computation. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, Hu et al. introduced the membrane-shortcut ResNet (MS-ResNet), incorporating the pre-activation structure found in ANNs. This approach features a shortcut path that directly propagates the full-precision membrane potential of spiking neurons to all subsequent residual blocks. However, this hybrid structure of ANNs and SNNs also reduces the benefits of event-driven computation. Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.1.2 Direct Training with Surrogate Gradients ‣ 2.1 Learning Rules ‣ 2 Deep Spiking Neural Networks ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> visualizes these three different implementations of shortcuts.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<p id="S2.SS2.SSS1.p3.1" class="ltx_p">In contrast to the manually designed architectures mentioned above, several works have proposed using neural architecture search (NAS) to automatically discover optimal architectures for SNNs. Kim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> introduced SNASNet, which simultaneously searches for both feed-forward and backward connections. Na et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> developed AutoSNN, a spike-aware NAS framework designed to effectively explore SNNs within a defined energy-efficient search space. Yan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> proposed encoding candidate architectures in a branchless spiking supernet to address long search times, along with a Synaptic Operation (SynOps)-aware optimization to reduce computational requirements.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Transformer Architectures</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Inspired by the impressive performance of transformer networks, researchers have proposed incorporating Transformer architectures into spiking neural networks (SNNs) to bridge the performance gap between state-of-the-art artificial neural networks (ANNs) and SNNs. With the recent success of large language models (LLMs), research on deep SNNs with transformer architectures has become a focal point in the neuromorphic computing community.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p"><span id="S2.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">1) Vanilla Self-attention:</span> Early works often utilized hybrid structures combining ANN-based self-attention modules with spiking components. For instance, Mueller et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> proposed a spiking transformer using the conversion method by Rueckauer et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> introduced a spiking transformer for event-based single object tracking, employing SNNs for feature extraction while retaining real-valued transformers. Similarly, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> developed a model integrating transformers to estimate monocular depth from continuous spike streams generated by spiking cameras. However, these methods, using vanilla self-attention mechanisms, face challenges in fully leveraging the event-driven nature of SNNs and in reducing resource consumption.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2409.02111/assets/x6.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="105" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>An example of spiking self attention.</figcaption>
</figure>
<figure id="S2.T3" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Summary of Spiking Neural Networks with Transformer Architectures (Spiking Transformers)</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S2.T3.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T3.1.2.1" class="ltx_tr">
<th id="S2.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T3.1.2.1.1.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S2.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T3.1.2.1.2.1" class="ltx_text ltx_font_bold">Work</span></th>
<th id="S2.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T3.1.2.1.3.1" class="ltx_text ltx_font_bold">Training<sup id="S2.T3.1.2.1.3.1.1" class="ltx_sup"><span id="S2.T3.1.2.1.3.1.1.1" class="ltx_text ltx_font_medium">a</span></sup></span></th>
<th id="S2.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T3.1.2.1.4.1" class="ltx_text ltx_font_bold">Tasks</span></th>
<th id="S2.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T3.1.2.1.5.1" class="ltx_text ltx_font_bold">Datasets</span></th>
<th id="S2.T3.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T3.1.2.1.6.1" class="ltx_text ltx_font_bold">Metrics Used</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T3.1.3.1" class="ltx_tr">
<td id="S2.T3.1.3.1.1" class="ltx_td ltx_align_center ltx_border_tt">2021</td>
<td id="S2.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_tt">Spiking Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>
</td>
<td id="S2.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_tt">C</td>
<td id="S2.T3.1.3.1.4" class="ltx_td ltx_align_center ltx_border_tt">[SC, IC]</td>
<td id="S2.T3.1.3.1.5" class="ltx_td ltx_align_center ltx_border_tt">[IMDB, MNIST]</td>
<td id="S2.T3.1.3.1.6" class="ltx_td ltx_align_center ltx_border_tt">Acc.</td>
</tr>
<tr id="S2.T3.1.4.2" class="ltx_tr">
<td id="S2.T3.1.4.2.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T3.1.4.2.2" class="ltx_td ltx_align_center">STNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>
</td>
<td id="S2.T3.1.4.2.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.4.2.4" class="ltx_td ltx_align_center">[OT]</td>
<td id="S2.T3.1.4.2.5" class="ltx_td ltx_align_center">[FE240hz, EED]</td>
<td id="S2.T3.1.4.2.6" class="ltx_td ltx_align_center">RSR, OP, RPR</td>
</tr>
<tr id="S2.T3.1.1" class="ltx_tr">
<td id="S2.T3.1.1.2" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T3.1.1.3" class="ltx_td ltx_align_center">Spike-T <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>
</td>
<td id="S2.T3.1.1.4" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.1.5" class="ltx_td ltx_align_center">[MED]</td>
<td id="S2.T3.1.1.6" class="ltx_td ltx_align_center">[DENSE-spike]</td>
<td id="S2.T3.1.1.1" class="ltx_td ltx_align_center">
<table id="S2.T3.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.1.1.1.2" class="ltx_tr">
<td id="S2.T3.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Abs Rel, Sq Rel,</td>
</tr>
<tr id="S2.T3.1.1.1.1.3" class="ltx_tr">
<td id="S2.T3.1.1.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">MAE, RMSE log,</td>
</tr>
<tr id="S2.T3.1.1.1.1.1" class="ltx_tr">
<td id="S2.T3.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Acc.<math id="S2.T3.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S2.T3.1.1.1.1.1.1.m1.1a"><mi id="S2.T3.1.1.1.1.1.1.m1.1.1" xref="S2.T3.1.1.1.1.1.1.m1.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S2.T3.1.1.1.1.1.1.m1.1b"><ci id="S2.T3.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T3.1.1.1.1.1.1.m1.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.1.1.1.1.1.1.m1.1c">\delta</annotation></semantics></math>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.5.3" class="ltx_tr">
<td id="S2.T3.1.5.3.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T3.1.5.3.2" class="ltx_td ltx_align_center">Spikformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
<td id="S2.T3.1.5.3.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.5.3.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.5.3.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.5.3.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.5.3.5.1.1" class="ltx_tr">
<td id="S2.T3.1.5.3.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, ImageNet,</td>
</tr>
<tr id="S2.T3.1.5.3.5.1.2" class="ltx_tr">
<td id="S2.T3.1.5.3.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">DVS CIFAR10, DVS128 Gesture]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.5.3.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.6.4" class="ltx_tr">
<td id="S2.T3.1.6.4.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.6.4.2" class="ltx_td ltx_align_center">SMMT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>
</td>
<td id="S2.T3.1.6.4.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.6.4.4" class="ltx_td ltx_align_center">[AVC]</td>
<td id="S2.T3.1.6.4.5" class="ltx_td ltx_align_center">[CIFAR-10 AV]</td>
<td id="S2.T3.1.6.4.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.7.5" class="ltx_tr">
<td id="S2.T3.1.7.5.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.7.5.2" class="ltx_td ltx_align_center">Spike-driven Tr. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>
</td>
<td id="S2.T3.1.7.5.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.7.5.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.7.5.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.7.5.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.7.5.5.1.1" class="ltx_tr">
<td id="S2.T3.1.7.5.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, ImageNet,</td>
</tr>
<tr id="S2.T3.1.7.5.5.1.2" class="ltx_tr">
<td id="S2.T3.1.7.5.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">DVS CIFAR10, DVS128 Gesture]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.7.5.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.8.6" class="ltx_tr">
<td id="S2.T3.1.8.6.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.8.6.2" class="ltx_td ltx_align_center">SST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>
</td>
<td id="S2.T3.1.8.6.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.8.6.4" class="ltx_td ltx_align_center">[HPT]</td>
<td id="S2.T3.1.8.6.5" class="ltx_td ltx_align_center">[MMHPSD, SynEventHPD, DHP19]</td>
<td id="S2.T3.1.8.6.6" class="ltx_td ltx_align_center">
<table id="S2.T3.1.8.6.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.8.6.6.1.1" class="ltx_tr">
<td id="S2.T3.1.8.6.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">MPJPE, PEL-MPJPE</td>
</tr>
<tr id="S2.T3.1.8.6.6.1.2" class="ltx_tr">
<td id="S2.T3.1.8.6.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">PA-MPJPE</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.9.7" class="ltx_tr">
<td id="S2.T3.1.9.7.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.9.7.2" class="ltx_td ltx_align_center">Spikformer-LT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>
</td>
<td id="S2.T3.1.9.7.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.9.7.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.9.7.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.9.7.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.9.7.5.1.1" class="ltx_tr">
<td id="S2.T3.1.9.7.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, DVS CIFAR10,</td>
</tr>
<tr id="S2.T3.1.9.7.5.1.2" class="ltx_tr">
<td id="S2.T3.1.9.7.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">DVS128 Gesture]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.9.7.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.10.8" class="ltx_tr">
<td id="S2.T3.1.10.8.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.10.8.2" class="ltx_td ltx_align_center">Spiking CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>
</td>
<td id="S2.T3.1.10.8.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.10.8.4" class="ltx_td ltx_align_center">[IC, ZSC]</td>
<td id="S2.T3.1.10.8.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.10.8.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.10.8.5.1.1" class="ltx_tr">
<td id="S2.T3.1.10.8.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, Flowers102,</td>
</tr>
<tr id="S2.T3.1.10.8.5.1.2" class="ltx_tr">
<td id="S2.T3.1.10.8.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">OxfordIIITPet, Caltech101, STL10]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.10.8.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.11.9" class="ltx_tr">
<td id="S2.T3.1.11.9.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.11.9.2" class="ltx_td ltx_align_center">Spike-BERT-SSA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>
</td>
<td id="S2.T3.1.11.9.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.11.9.4" class="ltx_td ltx_align_center">[TC]</td>
<td id="S2.T3.1.11.9.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.11.9.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.11.9.5.1.1" class="ltx_tr">
<td id="S2.T3.1.11.9.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[MR, SST-2, SST-5,</td>
</tr>
<tr id="S2.T3.1.11.9.5.1.2" class="ltx_tr">
<td id="S2.T3.1.11.9.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Subj., ChnSenti, Waimai]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.11.9.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.12.10" class="ltx_tr">
<td id="S2.T3.1.12.10.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.12.10.2" class="ltx_td ltx_align_center">Spiking-BERT-SA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>
</td>
<td id="S2.T3.1.12.10.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.12.10.4" class="ltx_td ltx_align_center">[SR, NLI]</td>
<td id="S2.T3.1.12.10.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.12.10.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.12.10.5.1.1" class="ltx_tr">
<td id="S2.T3.1.12.10.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[QQP, MNLI-m, SST-2, QNLI,</td>
</tr>
<tr id="S2.T3.1.12.10.5.1.2" class="ltx_tr">
<td id="S2.T3.1.12.10.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">RTE, MRPC, STS-B]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.12.10.6" class="ltx_td ltx_align_center">
<table id="S2.T3.1.12.10.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.12.10.6.1.1" class="ltx_tr">
<td id="S2.T3.1.12.10.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Acc., F1 scores,</td>
</tr>
<tr id="S2.T3.1.12.10.6.1.2" class="ltx_tr">
<td id="S2.T3.1.12.10.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">PCC, SCC</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.13.11" class="ltx_tr">
<td id="S2.T3.1.13.11.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.13.11.2" class="ltx_td ltx_align_center">Spike-GPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
<td id="S2.T3.1.13.11.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.13.11.4" class="ltx_td ltx_align_center">[NLG, NLU]</td>
<td id="S2.T3.1.13.11.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.13.11.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.13.11.5.1.1" class="ltx_tr">
<td id="S2.T3.1.13.11.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[Enwik8, WikiText-2, WikiText103,</td>
</tr>
<tr id="S2.T3.1.13.11.5.1.2" class="ltx_tr">
<td id="S2.T3.1.13.11.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">MR, SST-2, SST-5, Subj.]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.13.11.6" class="ltx_td ltx_align_center">BPC, PPL, Acc.</td>
</tr>
<tr id="S2.T3.1.14.12" class="ltx_tr">
<td id="S2.T3.1.14.12.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.14.12.2" class="ltx_td ltx_align_center">Spiking ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
<td id="S2.T3.1.14.12.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.14.12.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.14.12.5" class="ltx_td ltx_align_center">[CIFAR10/100, ImageNet]</td>
<td id="S2.T3.1.14.12.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.15.13" class="ltx_tr">
<td id="S2.T3.1.15.13.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.15.13.2" class="ltx_td ltx_align_center">STS-Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>
</td>
<td id="S2.T3.1.15.13.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.15.13.4" class="ltx_td ltx_align_center">[IC, SR]</td>
<td id="S2.T3.1.15.13.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.15.13.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.15.13.5.1.1" class="ltx_tr">
<td id="S2.T3.1.15.13.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[DVS CIFAR10, DVS128 Gesture,</td>
</tr>
<tr id="S2.T3.1.15.13.5.1.2" class="ltx_tr">
<td id="S2.T3.1.15.13.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">GSC V1, GSC V2]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.15.13.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.16.14" class="ltx_tr">
<td id="S2.T3.1.16.14.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.16.14.2" class="ltx_td ltx_align_center">MAST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>
</td>
<td id="S2.T3.1.16.14.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.16.14.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.16.14.5" class="ltx_td ltx_align_center">[CIFAR-10, DVS CIFAR10, DVS128 Gesture]</td>
<td id="S2.T3.1.16.14.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.17.15" class="ltx_tr">
<td id="S2.T3.1.17.15.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.17.15.2" class="ltx_td ltx_align_center">SSTFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>
</td>
<td id="S2.T3.1.17.15.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.17.15.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.17.15.5" class="ltx_td ltx_align_center">[HAR-DVS, PokerEvents]</td>
<td id="S2.T3.1.17.15.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.18.16" class="ltx_tr">
<td id="S2.T3.1.18.16.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.18.16.2" class="ltx_td ltx_align_center">AutoST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>
</td>
<td id="S2.T3.1.18.16.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.18.16.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.18.16.5" class="ltx_td ltx_align_center">[CIFAR-10/100, ImageNet]</td>
<td id="S2.T3.1.18.16.6" class="ltx_td ltx_align_center">Acc,</td>
</tr>
<tr id="S2.T3.1.19.17" class="ltx_tr">
<td id="S2.T3.1.19.17.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.19.17.2" class="ltx_td ltx_align_center">MST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>
</td>
<td id="S2.T3.1.19.17.3" class="ltx_td ltx_align_center">C</td>
<td id="S2.T3.1.19.17.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.19.17.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.19.17.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.19.17.5.1.1" class="ltx_tr">
<td id="S2.T3.1.19.17.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, ImageNet,</td>
</tr>
<tr id="S2.T3.1.19.17.5.1.2" class="ltx_tr">
<td id="S2.T3.1.19.17.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">N-Caltech101, N-CARS, AR, ASL-DVS]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.19.17.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.20.18" class="ltx_tr">
<td id="S2.T3.1.20.18.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.20.18.2" class="ltx_td ltx_align_center">Spikingformer-RL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>
</td>
<td id="S2.T3.1.20.18.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.20.18.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.20.18.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.20.18.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.20.18.5.1.1" class="ltx_tr">
<td id="S2.T3.1.20.18.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, ImageNet,</td>
</tr>
<tr id="S2.T3.1.20.18.5.1.2" class="ltx_tr">
<td id="S2.T3.1.20.18.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">DVS CIFAR10, DVS128 Gesture]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.20.18.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.21.19" class="ltx_tr">
<td id="S2.T3.1.21.19.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.21.19.2" class="ltx_td ltx_align_center">Spikingformer-CML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>
</td>
<td id="S2.T3.1.21.19.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.21.19.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.21.19.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.21.19.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.21.19.5.1.1" class="ltx_tr">
<td id="S2.T3.1.21.19.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, ImageNet,</td>
</tr>
<tr id="S2.T3.1.21.19.5.1.2" class="ltx_tr">
<td id="S2.T3.1.21.19.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">DVS CIFAR10, DVS128 Gesture]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.21.19.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.22.20" class="ltx_tr">
<td id="S2.T3.1.22.20.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T3.1.22.20.2" class="ltx_td ltx_align_center">DISTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>
</td>
<td id="S2.T3.1.22.20.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.22.20.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.22.20.5" class="ltx_td ltx_align_center">[CIFAR-10/100, DVS CIFAR10]</td>
<td id="S2.T3.1.22.20.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.23.21" class="ltx_tr">
<td id="S2.T3.1.23.21.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.23.21.2" class="ltx_td ltx_align_center">TIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>
</td>
<td id="S2.T3.1.23.21.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.23.21.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.23.21.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.23.21.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.23.21.5.1.1" class="ltx_tr">
<td id="S2.T3.1.23.21.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[DVS CIFAR10, N-Caltech101,</td>
</tr>
<tr id="S2.T3.1.23.21.5.1.2" class="ltx_tr">
<td id="S2.T3.1.23.21.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">N-CARS, UCF101-DVS, HMDB51-DVS]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.23.21.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.24.22" class="ltx_tr">
<td id="S2.T3.1.24.22.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.24.22.2" class="ltx_td ltx_align_center">Spikformer V2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>
</td>
<td id="S2.T3.1.24.22.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.24.22.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.24.22.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.24.22.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.24.22.5.1.1" class="ltx_tr">
<td id="S2.T3.1.24.22.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, ImageNet,</td>
</tr>
<tr id="S2.T3.1.24.22.5.1.2" class="ltx_tr">
<td id="S2.T3.1.24.22.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">DVS CIFAR10, DVS128 Gesture]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.24.22.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.25.23" class="ltx_tr">
<td id="S2.T3.1.25.23.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.25.23.2" class="ltx_td ltx_align_center">Spike-driven Tr. V2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>
</td>
<td id="S2.T3.1.25.23.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.25.23.4" class="ltx_td ltx_align_center">[IC, HAR, OD, SS]</td>
<td id="S2.T3.1.25.23.5" class="ltx_td ltx_align_center">[ImageNet, HAR-DVS, COCO, ADE20K]</td>
<td id="S2.T3.1.25.23.6" class="ltx_td ltx_align_center">Acc., mAP, mIoU</td>
</tr>
<tr id="S2.T3.1.26.24" class="ltx_tr">
<td id="S2.T3.1.26.24.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.26.24.2" class="ltx_td ltx_align_center">Spiking-PhysFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>
</td>
<td id="S2.T3.1.26.24.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.26.24.4" class="ltx_td ltx_align_center">[RP]</td>
<td id="S2.T3.1.26.24.5" class="ltx_td ltx_align_center">[PURE, UBFC-rPPG, UBFC-Phys, MMPD]</td>
<td id="S2.T3.1.26.24.6" class="ltx_td ltx_align_center">
<table id="S2.T3.1.26.24.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.26.24.6.1.1" class="ltx_tr">
<td id="S2.T3.1.26.24.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">MAE, MAPE, PCC</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.27.25" class="ltx_tr">
<td id="S2.T3.1.27.25.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.27.25.2" class="ltx_td ltx_align_center">Spikeformer-CT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>
</td>
<td id="S2.T3.1.27.25.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.27.25.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.27.25.5" class="ltx_td ltx_align_center">[ImageNet, DVS CIFAR10, DVS128 Gesture]</td>
<td id="S2.T3.1.27.25.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.28.26" class="ltx_tr">
<td id="S2.T3.1.28.26.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.28.26.2" class="ltx_td ltx_align_center">SDiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>
</td>
<td id="S2.T3.1.28.26.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.28.26.4" class="ltx_td ltx_align_center">[IG]</td>
<td id="S2.T3.1.28.26.5" class="ltx_td ltx_align_center">[MNIST, Fashion-MNIST, CIFAR-10]</td>
<td id="S2.T3.1.28.26.6" class="ltx_td ltx_align_center">FID</td>
</tr>
<tr id="S2.T3.1.29.27" class="ltx_tr">
<td id="S2.T3.1.29.27.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.29.27.2" class="ltx_td ltx_align_center">Spiking Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>
</td>
<td id="S2.T3.1.29.27.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.29.27.4" class="ltx_td ltx_align_center">[ESDP]</td>
<td id="S2.T3.1.29.27.5" class="ltx_td ltx_align_center">[CHB-MIT]</td>
<td id="S2.T3.1.29.27.6" class="ltx_td ltx_align_center">SENS, SPEC, Acc.</td>
</tr>
<tr id="S2.T3.1.30.28" class="ltx_tr">
<td id="S2.T3.1.30.28.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.30.28.2" class="ltx_td ltx_align_center">RevSResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>
</td>
<td id="S2.T3.1.30.28.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.30.28.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.30.28.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.30.28.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.30.28.5.1.1" class="ltx_tr">
<td id="S2.T3.1.30.28.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, DVS CIFAR10,</td>
</tr>
<tr id="S2.T3.1.30.28.5.1.2" class="ltx_tr">
<td id="S2.T3.1.30.28.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">DVS128 Gesture]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.30.28.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.31.29" class="ltx_tr">
<td id="S2.T3.1.31.29.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.31.29.2" class="ltx_td ltx_align_center">QKFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>
</td>
<td id="S2.T3.1.31.29.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.31.29.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.31.29.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.31.29.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.31.29.5.1.1" class="ltx_tr">
<td id="S2.T3.1.31.29.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, ImageNet,</td>
</tr>
<tr id="S2.T3.1.31.29.5.1.2" class="ltx_tr">
<td id="S2.T3.1.31.29.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">DVS CIFAR10, DVS128 Gesture]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.31.29.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.32.30" class="ltx_tr">
<td id="S2.T3.1.32.30.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.32.30.2" class="ltx_td ltx_align_center">SpikingResformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>
</td>
<td id="S2.T3.1.32.30.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.32.30.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.32.30.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.32.30.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.32.30.5.1.1" class="ltx_tr">
<td id="S2.T3.1.32.30.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, ImageNet,</td>
</tr>
<tr id="S2.T3.1.32.30.5.1.2" class="ltx_tr">
<td id="S2.T3.1.32.30.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">DVS CIFAR10, DVS128 Gesture]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.32.30.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.33.31" class="ltx_tr">
<td id="S2.T3.1.33.31.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.33.31.2" class="ltx_td ltx_align_center">SpikingLLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>
</td>
<td id="S2.T3.1.33.31.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.33.31.4" class="ltx_td ltx_align_center">[LG, CSR]</td>
<td id="S2.T3.1.33.31.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.33.31.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.33.31.5.1.1" class="ltx_tr">
<td id="S2.T3.1.33.31.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[WikiText2, C4 , PIQA, ARC-easy, BoolQ,</td>
</tr>
<tr id="S2.T3.1.33.31.5.1.2" class="ltx_tr">
<td id="S2.T3.1.33.31.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ARC-challenge, HellaSwag, Winogrande ]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.33.31.6" class="ltx_td ltx_align_center">PPL., Acc.</td>
</tr>
<tr id="S2.T3.1.34.32" class="ltx_tr">
<td id="S2.T3.1.34.32.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.34.32.2" class="ltx_td ltx_align_center">STA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>
</td>
<td id="S2.T3.1.34.32.3" class="ltx_td ltx_align_center">C</td>
<td id="S2.T3.1.34.32.4" class="ltx_td ltx_align_center">[ZSC]</td>
<td id="S2.T3.1.34.32.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.34.32.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.34.32.5.1.1" class="ltx_tr">
<td id="S2.T3.1.34.32.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100/10.1/10.2, ImageNet-200]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.34.32.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.35.33" class="ltx_tr">
<td id="S2.T3.1.35.33.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.35.33.2" class="ltx_td ltx_align_center">SpikeZIP-TF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>
</td>
<td id="S2.T3.1.35.33.3" class="ltx_td ltx_align_center">C</td>
<td id="S2.T3.1.35.33.4" class="ltx_td ltx_align_center">[IC, NLU]</td>
<td id="S2.T3.1.35.33.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.35.33.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.35.33.5.1.1" class="ltx_tr">
<td id="S2.T3.1.35.33.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR10/100, ImageNet,</td>
</tr>
<tr id="S2.T3.1.35.33.5.1.2" class="ltx_tr">
<td id="S2.T3.1.35.33.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">CIFAR10-DVS, MR, Subj, SST-2,</td>
</tr>
<tr id="S2.T3.1.35.33.5.1.3" class="ltx_tr">
<td id="S2.T3.1.35.33.5.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">SST-5, ChnSenti, Waimai]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.35.33.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.36.34" class="ltx_tr">
<td id="S2.T3.1.36.34.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.36.34.2" class="ltx_td ltx_align_center">ECMT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>
</td>
<td id="S2.T3.1.36.34.3" class="ltx_td ltx_align_center">C</td>
<td id="S2.T3.1.36.34.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.36.34.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.36.34.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.36.34.5.1.1" class="ltx_tr">
<td id="S2.T3.1.36.34.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[ImageNet]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.36.34.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.37.35" class="ltx_tr">
<td id="S2.T3.1.37.35.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.37.35.2" class="ltx_td ltx_align_center">SpikingMiniLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite>
</td>
<td id="S2.T3.1.37.35.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.37.35.4" class="ltx_td ltx_align_center">[NLU]</td>
<td id="S2.T3.1.37.35.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.37.35.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.37.35.5.1.1" class="ltx_tr">
<td id="S2.T3.1.37.35.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[GLUE]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.37.35.6" class="ltx_td ltx_align_center">
<table id="S2.T3.1.37.35.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.37.35.6.1.1" class="ltx_tr">
<td id="S2.T3.1.37.35.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Acc., F1 scores,</td>
</tr>
<tr id="S2.T3.1.37.35.6.1.2" class="ltx_tr">
<td id="S2.T3.1.37.35.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">MCC, PCC</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T3.1.38.36" class="ltx_tr">
<td id="S2.T3.1.38.36.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.38.36.2" class="ltx_td ltx_align_center">SGLFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>
</td>
<td id="S2.T3.1.38.36.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.38.36.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.38.36.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.38.36.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.38.36.5.1.1" class="ltx_tr">
<td id="S2.T3.1.38.36.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, ImageNet,</td>
</tr>
<tr id="S2.T3.1.38.36.5.1.2" class="ltx_tr">
<td id="S2.T3.1.38.36.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">CIFAR10-DVS, DVS128-Gesture]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.38.36.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.39.37" class="ltx_tr">
<td id="S2.T3.1.39.37.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.39.37.2" class="ltx_td ltx_align_center">OST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>
</td>
<td id="S2.T3.1.39.37.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.39.37.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.39.37.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.39.37.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.39.37.5.1.1" class="ltx_tr">
<td id="S2.T3.1.39.37.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, ImageNet,</td>
</tr>
<tr id="S2.T3.1.39.37.5.1.2" class="ltx_tr">
<td id="S2.T3.1.39.37.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">CIFAR10-DVS, DVS128-Gesture]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.39.37.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.40.38" class="ltx_tr">
<td id="S2.T3.1.40.38.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T3.1.40.38.2" class="ltx_td ltx_align_center">TE-Spikformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>
</td>
<td id="S2.T3.1.40.38.3" class="ltx_td ltx_align_center">DT</td>
<td id="S2.T3.1.40.38.4" class="ltx_td ltx_align_center">[IC]</td>
<td id="S2.T3.1.40.38.5" class="ltx_td ltx_align_center">
<table id="S2.T3.1.40.38.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.40.38.5.1.1" class="ltx_tr">
<td id="S2.T3.1.40.38.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[DVS128 Gesture, CIFAR10-DVS,</td>
</tr>
<tr id="S2.T3.1.40.38.5.1.2" class="ltx_tr">
<td id="S2.T3.1.40.38.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">N-Caltech101]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.40.38.6" class="ltx_td ltx_align_center">Acc.</td>
</tr>
<tr id="S2.T3.1.41.39" class="ltx_tr">
<td id="S2.T3.1.41.39.1" class="ltx_td ltx_align_center ltx_border_bb">2024</td>
<td id="S2.T3.1.41.39.2" class="ltx_td ltx_align_center ltx_border_bb">SWformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>
</td>
<td id="S2.T3.1.41.39.3" class="ltx_td ltx_align_center ltx_border_bb">DT</td>
<td id="S2.T3.1.41.39.4" class="ltx_td ltx_align_center ltx_border_bb">[IC]</td>
<td id="S2.T3.1.41.39.5" class="ltx_td ltx_align_center ltx_border_bb">
<table id="S2.T3.1.41.39.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.1.41.39.5.1.1" class="ltx_tr">
<td id="S2.T3.1.41.39.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">[CIFAR-10/100, ImageNet, CIFAR10-DVS,</td>
</tr>
<tr id="S2.T3.1.41.39.5.1.2" class="ltx_tr">
<td id="S2.T3.1.41.39.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">N-Caltech101, N-Cars, ActionRecognition,</td>
</tr>
<tr id="S2.T3.1.41.39.5.1.3" class="ltx_tr">
<td id="S2.T3.1.41.39.5.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">ASL-DVS, NavGesture]</td>
</tr>
</table>
</td>
<td id="S2.T3.1.41.39.6" class="ltx_td ltx_align_center ltx_border_bb">Acc.</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S2.I1" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">a</span> 
<div id="S2.I1.ix1.p1" class="ltx_para">
<p id="S2.I1.ix1.p1.1" class="ltx_p">C - Conversion; DT - Direct Training</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p"><span id="S2.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">2) Spiking Self-attention:</span> A notable breakthrough was made by Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>. For the first time, they introduced a spiking self-attention mechanism and proposed a framework, i.e., Spikformer, to build deep SNNs with a transformer architecture. In contrast to vanilla self-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, spiking self-attention (Fig. <a href="#S2.F4" title="Figure 4 ‣ 2.2.2 Transformer Architectures ‣ 2.2 Network Architectures in Large Spiking Neural Networks ‣ 2 Deep Spiking Neural Networks ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) discards the complex softmax operation, which is difficult to replace with spiking operations, and performs matrix dot-product on the spike forms of Query (Q), Key (K), and Value (V). On ImageNet, Spikformer achieves an accuracy of 74.81% with Spikformer-8-768 architecture and 4 time steps. However, there is still a performance gap between ANNs (Transformer-8-512 achieves an accuracy of 80.80%) and SNNs (Spikformer-8-512 achieves an accuracy of 73.38%).</p>
</div>
<div id="S2.SS2.SSS2.p4" class="ltx_para">
<p id="S2.SS2.SSS2.p4.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, several works have explored the implementation of self attention mechanism in spiking transformers. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, Yao el al. introduced Spike-driven Transformer and Spike-Driven Self-Attention (SDSA) that exploits mask and addition operations only for implementing self-attention. Shi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> proposed Dual Spike Self-Attention (DSSA) that is compatible with SNNs to efficiently handle multi-scale feature maps. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>, zhou et al. developed Q-K attention mechanism that only adopts two spike-form components: Query (<span id="S2.SS2.SSS2.p4.1.1" class="ltx_text ltx_font_italic">Q</span>) and Key (<span id="S2.SS2.SSS2.p4.1.2" class="ltx_text ltx_font_italic">K</span>).</p>
</div>
<div id="S2.SS2.SSS2.p5" class="ltx_para">
<p id="S2.SS2.SSS2.p5.1" class="ltx_p">Aiming to enhance spiking transformers with spatio-temporal attention, several studies have introduced spatio-temporal self-attention mechanisms. Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> proposed the Denoising Spiking Transformer with Intrinsic Plasticity and Spatio-Temporal Attention (DISTA), which integrates neuron-level and network-level spatiotemporal attention. They also introduced a non-linear denoising layer to mitigate noisy signals within the computed spatiotemporal attention map. To exploit both temporal and spatial information, Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> developed Spatial-Temporal Self-Attention (STSA), enabling spiking transformers to capture features from both domains. They incorporated a spatial-temporal relative position bias (STRPB) to infuse the spatiotemporal position of spikes, integrating STSA into their Spatial-Temporal Spiking Transformer (STS-Transformer). For tracking human poses from purely event-based data, Zou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> proposed an architecture combining a Spike-Element-Wise (SEW) ResNet as the backbone with a Spiking Spatiotemporal Transformer based on spiking spatiotemporal attention. To better exploit temporal information, Shen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> introduced a Temporal Interaction Module (TIM) that integrates into the Spikformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> framework, improving performance with minimal additional parameters through a one-dimensional convolution. Gao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> also proposed capturing meaningful temporal information with a Spike Spatio-Temporal Attention (SSTA) module and replacing Batch Normalization (BN) with Batch Group Normalization (BGN) to balance firing rates across temporal steps.</p>
</div>
<div id="S2.SS2.SSS2.p6" class="ltx_para">
<p id="S2.SS2.SSS2.p6.1" class="ltx_p">Aiming to exploit frequency representations, Fang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> proposed the Spiking Wavelet Transformer (SWformer). This model incorporates negative spikes and a Frequency-Aware Token Mixer (FATM) designed to extract both spatial and frequency features effectively.</p>
</div>
<div id="S2.SS2.SSS2.p7" class="ltx_para">
<p id="S2.SS2.SSS2.p7.1" class="ltx_p"><span id="S2.SS2.SSS2.p7.1.1" class="ltx_text ltx_font_bold ltx_font_italic">3) Enhancing Performance:</span></p>
</div>
<div id="S2.SS2.SSS2.p8" class="ltx_para">
<p id="S2.SS2.SSS2.p8.1" class="ltx_p">To improve network performance, several works have focused on optimizing network structure. Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> proposed Spikingformer, which modifies Spike-Element-Wise (SEW) shortcuts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> to use membrane shortcuts, avoiding integer spikes. Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> introduced ConvBN-MaxPooling-LIF (CML) to enhance downsampling modules in deep SNNs, facilitating gradient backpropagation compared to ConvBN-LIF-MaxPooling. To further improve Spikformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> developed Spikformer V2, incorporating a Spiking Convolutional Stem (SCS). Similarly, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> proposed a Convolutional Tokenizer (CT) module for patch embedding. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, Yao et al. introduced Spike-driven Transformer V2 with a meta-architecture to enhance performance and versatility. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite> proposed the Spiking Global-Local-Fusion Transformer (SGLFormer), designed to efficiently process information on both global and local scales, and introduced a new max pooling module and classification head.</p>
</div>
<div id="S2.SS2.SSS2.p9" class="ltx_para">
<p id="S2.SS2.SSS2.p9.1" class="ltx_p">There are also works focusing on reducing complexity. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> proposed AutoST, a training-free neural architecture search method for identifying optimal spiking transformer architectures. By emphasizing FLOPs, this method provides a standardized and objective assessment of model efficiency and computational complexity. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> aimed to reduce time complexity in Spikformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> by replacing spiking self-attention with unparameterized Linear Transforms (LTs), such as Fourier and Wavelet transforms. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> introduced the Masked Spiking Transformer (MST) framework, incorporating a Random Spike Masking (RSM) method to reduce the number of spikes. Datta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> proposed a training framework that dynamically allocates the number of time steps to each Vision Transformer (ViT) module based on a trainable score assigned to each timestep, hypothesizing that each ViT block has varying sensitivity to the number of time steps. To address the spatio-temporal overhead of Spiking Transformers, Song et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> introduced a Time Domain Compression and Compensation (TDCC) component and Spiking Linear Transformation (SLT) for implementing the One-step Spiking Transformer (OST).</p>
</div>
<div id="S2.SS2.SSS2.p10" class="ltx_para">
<p id="S2.SS2.SSS2.p10.1" class="ltx_p">To avoid the burdensome cost of training from scratch, some works employ ANN-to-SNN conversion to build Spiking Transformers. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> proposed to build Spiking Transformers based on ANN-to-SNN conversion with quantization clip-floor-shift (QCFS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. To address non-linear mechanisms like self-attention and test-time normalization in vanilla Transformers, Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> proposed Spatio-Temporal Approximation (STA) to approximate floating-point values in ANNs by introducing new spiking operators and layers. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>, You et al. proposed to facilitate Transformer-based ANN-to-SNN conversions with quantized ANNs that incorporate SNN-friendly operators. To preserve the accuracy after conversion, Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> proposed a expectation compensation module that uses information from the previous T time steps to calculate the expected output at time step T. In addition, they also introduced a multi-threshold neuron model and a corresponding parallel parameter normalization to reduce time steps needed for high accuracy. On ImageNet, they reported 88.60% accuracy under 4 time steps, using a model with 1 billion parameters.</p>
</div>
<div id="S2.SS2.SSS2.p11" class="ltx_para">
<p id="S2.SS2.SSS2.p11.1" class="ltx_p"><span id="S2.SS2.SSS2.p11.1.1" class="ltx_text ltx_font_bold ltx_font_italic">3) Spiking Transformers for Natural Language Processing</span>
In pursuit of spiking large language models (LLMs), several works explored Spiking Transformers for natural language processing (NLP). Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> proposed SpikingGPT for language generation based on Receptance Weighted Key Value (RWKV) language model. Replacing self attention with RWKV, they proposed a structure that employs the Spiking Receptance Weighted Key Value (Spiking RWKV) as a token-mixer and the Spiking Receptance Feed-Forward Networks (Spiking RFFN) as a channel mixer. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, Bal et al. proposed SpikingBERT by combine spiking transformers with BERT. To effectively train their SpikingBERT, they proposed to employ a pre-trained BERT model as “teacher” to train their “student” spiking architecture. Similarly, Lv et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> modified Spikformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> with respect to BERT and introduced SpikeBERT. To improve performance of Spiking BERT, they also proposed a two-stage, “pre-training + task-specific” knowledge distillation method to transfer knowledge from BERTs to SpikeBERT. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite>, Zhange el al. also proposed a SpikingMiniLM based on BERT with a parameter initialization and ANN-to-SNN distillation method to achieve fast convergence. With a novel spike-driven quantization framework named Optimal Brain Spiking, Xing et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite> proposed bio-plausible SpikeLLM that supports 7 70 billion parameters.</p>
</div>
<div id="S2.SS2.SSS2.p12" class="ltx_para">
<p id="S2.SS2.SSS2.p12.1" class="ltx_p"><span id="S2.SS2.SSS2.p12.1.1" class="ltx_text ltx_font_bold ltx_font_italic">4) Beyond Image Classification and Natural Language Processing</span>
Currently, computer vision is the most explored field for Spiking Transformers. Although most works focus on image classification to design and evaluate Spiking Transformers, there are also works exploring their versatility. In computer vision, researchers have applied Spiking Transformers to object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, zero-shot classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>, image generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>, etc.</p>
</div>
<div id="S2.SS2.SSS2.p13" class="ltx_para">
<p id="S2.SS2.SSS2.p13.1" class="ltx_p">In addition, researchers have also explored multidisciplinary applications for Spiking Transformers. For example, Guo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> proposed Spiking Multi-Model Transformer (SMMT) for multimodel classification. Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> proposed Spiking-PhysFormer for remote photoplethysmography. Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> proposed Spiking Conformer to detect and predict epileptic seizure segments from scalped long-term electroencephalogram (EEG) recordings.</p>
</div>
<div id="S2.SS2.SSS2.p14" class="ltx_para">
<p id="S2.SS2.SSS2.p14.1" class="ltx_p">In Table <a href="#S2.T3" title="TABLE III ‣ 2.2.2 Transformer Architectures ‣ 2.2 Network Architectures in Large Spiking Neural Networks ‣ 2 Deep Spiking Neural Networks ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we summarize the evaluation tasks and datasets of existing Spiking Transformers. This table provides an overview of how these models have been assessed across various domains and applications.</p>
</div>
<figure id="S2.T4" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Performance Comparison on ImageNet Dataset</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S2.T4.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T4.1.1.1" class="ltx_tr">
<th id="S2.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S2.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Work</span></th>
<th id="S2.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Architecture</th>
<th id="S2.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">Learning Rule</span></th>
<th id="S2.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T4.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T4.1.1.1.5.1.1" class="ltx_tr">
<td id="S2.T4.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T4.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Param</span></td>
</tr>
<tr id="S2.T4.1.1.1.5.1.2" class="ltx_tr">
<td id="S2.T4.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T4.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">(M)</span></td>
</tr>
</table>
</th>
<th id="S2.T4.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T4.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T4.1.1.1.6.1.1" class="ltx_tr">
<td id="S2.T4.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T4.1.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">Time</span></td>
</tr>
<tr id="S2.T4.1.1.1.6.1.2" class="ltx_tr">
<td id="S2.T4.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T4.1.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold">Steps</span></td>
</tr>
</table>
</th>
<th id="S2.T4.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T4.1.1.1.7.1" class="ltx_text ltx_font_bold">Accuracy (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T4.1.2.1" class="ltx_tr">
<td id="S2.T4.1.2.1.1" class="ltx_td ltx_align_center ltx_border_tt">2021</td>
<td id="S2.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">Spiking ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S2.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">ResNet-50</td>
<td id="S2.T4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">Conversion</td>
<td id="S2.T4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">25.56</td>
<td id="S2.T4.1.2.1.6" class="ltx_td ltx_align_center ltx_border_tt">350</td>
<td id="S2.T4.1.2.1.7" class="ltx_td ltx_align_center ltx_border_tt">73.77</td>
</tr>
<tr id="S2.T4.1.3.2" class="ltx_tr">
<td id="S2.T4.1.3.2.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T4.1.3.2.2" class="ltx_td ltx_align_center">Tandeom <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S2.T4.1.3.2.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T4.1.3.2.4" class="ltx_td ltx_align_center">Conversion</td>
<td id="S2.T4.1.3.2.5" class="ltx_td ltx_align_center">138.42</td>
<td id="S2.T4.1.3.2.6" class="ltx_td ltx_align_center">16</td>
<td id="S2.T4.1.3.2.7" class="ltx_td ltx_align_center">65.08</td>
</tr>
<tr id="S2.T4.1.4.3" class="ltx_tr">
<td id="S2.T4.1.4.3.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T4.1.4.3.2" class="ltx_td ltx_align_center">Threshold ReLU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S2.T4.1.4.3.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T4.1.4.3.4" class="ltx_td ltx_align_center">Conversion</td>
<td id="S2.T4.1.4.3.5" class="ltx_td ltx_align_center">138.42</td>
<td id="S2.T4.1.4.3.6" class="ltx_td ltx_align_center">512</td>
<td id="S2.T4.1.4.3.7" class="ltx_td ltx_align_center">72.34</td>
</tr>
<tr id="S2.T4.1.5.4" class="ltx_tr">
<td id="S2.T4.1.5.4.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T4.1.5.4.2" class="ltx_td ltx_align_center">Calibration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</td>
<td id="S2.T4.1.5.4.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T4.1.5.4.4" class="ltx_td ltx_align_center">Conversion</td>
<td id="S2.T4.1.5.4.5" class="ltx_td ltx_align_center">138.42</td>
<td id="S2.T4.1.5.4.6" class="ltx_td ltx_align_center">32</td>
<td id="S2.T4.1.5.4.7" class="ltx_td ltx_align_center">63.64</td>
</tr>
<tr id="S2.T4.1.6.5" class="ltx_tr">
<td id="S2.T4.1.6.5.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T4.1.6.5.2" class="ltx_td ltx_align_center">Initialization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S2.T4.1.6.5.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T4.1.6.5.4" class="ltx_td ltx_align_center">Conversion</td>
<td id="S2.T4.1.6.5.5" class="ltx_td ltx_align_center">138.42</td>
<td id="S2.T4.1.6.5.6" class="ltx_td ltx_align_center">32</td>
<td id="S2.T4.1.6.5.7" class="ltx_td ltx_align_center">63.64</td>
</tr>
<tr id="S2.T4.1.7.6" class="ltx_tr">
<td id="S2.T4.1.7.6.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T4.1.7.6.2" class="ltx_td ltx_align_center">clip-floor-shift <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="S2.T4.1.7.6.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T4.1.7.6.4" class="ltx_td ltx_align_center">Conversion</td>
<td id="S2.T4.1.7.6.5" class="ltx_td ltx_align_center">138.42</td>
<td id="S2.T4.1.7.6.6" class="ltx_td ltx_align_center">32</td>
<td id="S2.T4.1.7.6.7" class="ltx_td ltx_align_center">68.47</td>
</tr>
<tr id="S2.T4.1.8.7" class="ltx_tr">
<td id="S2.T4.1.8.7.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T4.1.8.7.2" class="ltx_td ltx_align_center">Fast-SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S2.T4.1.8.7.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T4.1.8.7.4" class="ltx_td ltx_align_center">Conversion</td>
<td id="S2.T4.1.8.7.5" class="ltx_td ltx_align_center">138.42</td>
<td id="S2.T4.1.8.7.6" class="ltx_td ltx_align_center">3</td>
<td id="S2.T4.1.8.7.7" class="ltx_td ltx_align_center">71.91</td>
</tr>
<tr id="S2.T4.1.9.8" class="ltx_tr">
<td id="S2.T4.1.9.8.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T4.1.9.8.2" class="ltx_td ltx_align_center">Dspike <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
<td id="S2.T4.1.9.8.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T4.1.9.8.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.9.8.5" class="ltx_td ltx_align_center">138.42</td>
<td id="S2.T4.1.9.8.6" class="ltx_td ltx_align_center">5</td>
<td id="S2.T4.1.9.8.7" class="ltx_td ltx_align_center">71.24</td>
</tr>
<tr id="S2.T4.1.10.9" class="ltx_tr">
<td id="S2.T4.1.10.9.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T4.1.10.9.2" class="ltx_td ltx_align_center">IM-loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S2.T4.1.10.9.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T4.1.10.9.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.10.9.5" class="ltx_td ltx_align_center">138.42</td>
<td id="S2.T4.1.10.9.6" class="ltx_td ltx_align_center">5</td>
<td id="S2.T4.1.10.9.7" class="ltx_td ltx_align_center">70.65</td>
</tr>
<tr id="S2.T4.1.11.10" class="ltx_tr">
<td id="S2.T4.1.11.10.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T4.1.11.10.2" class="ltx_td ltx_align_center">Diet-SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S2.T4.1.11.10.3" class="ltx_td ltx_align_center">VGG-16</td>
<td id="S2.T4.1.11.10.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.11.10.5" class="ltx_td ltx_align_center">138.42</td>
<td id="S2.T4.1.11.10.6" class="ltx_td ltx_align_center">5</td>
<td id="S2.T4.1.11.10.7" class="ltx_td ltx_align_center">69.00</td>
</tr>
<tr id="S2.T4.1.12.11" class="ltx_tr">
<td id="S2.T4.1.12.11.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T4.1.12.11.2" class="ltx_td ltx_align_center">STBP-tdBN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S2.T4.1.12.11.3" class="ltx_td ltx_align_center">ResNet-50</td>
<td id="S2.T4.1.12.11.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.12.11.5" class="ltx_td ltx_align_center">25.56</td>
<td id="S2.T4.1.12.11.6" class="ltx_td ltx_align_center">6</td>
<td id="S2.T4.1.12.11.7" class="ltx_td ltx_align_center">64.88</td>
</tr>
<tr id="S2.T4.1.13.12" class="ltx_tr">
<td id="S2.T4.1.13.12.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T4.1.13.12.2" class="ltx_td ltx_align_center">TEBN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S2.T4.1.13.12.3" class="ltx_td ltx_align_center">ResNet-34</td>
<td id="S2.T4.1.13.12.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.13.12.5" class="ltx_td ltx_align_center">21.79</td>
<td id="S2.T4.1.13.12.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.13.12.7" class="ltx_td ltx_align_center">64.29</td>
</tr>
<tr id="S2.T4.1.14.13" class="ltx_tr">
<td id="S2.T4.1.14.13.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T4.1.14.13.2" class="ltx_td ltx_align_center">GLIF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</td>
<td id="S2.T4.1.14.13.3" class="ltx_td ltx_align_center">ResNet-34</td>
<td id="S2.T4.1.14.13.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.14.13.5" class="ltx_td ltx_align_center">21.79</td>
<td id="S2.T4.1.14.13.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.14.13.7" class="ltx_td ltx_align_center">67.52</td>
</tr>
<tr id="S2.T4.1.15.14" class="ltx_tr">
<td id="S2.T4.1.15.14.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T4.1.15.14.2" class="ltx_td ltx_align_center">Recdis-SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</td>
<td id="S2.T4.1.15.14.3" class="ltx_td ltx_align_center">ResNet-34</td>
<td id="S2.T4.1.15.14.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.15.14.5" class="ltx_td ltx_align_center">21.79</td>
<td id="S2.T4.1.15.14.6" class="ltx_td ltx_align_center">6</td>
<td id="S2.T4.1.15.14.7" class="ltx_td ltx_align_center">67.33</td>
</tr>
<tr id="S2.T4.1.16.15" class="ltx_tr">
<td id="S2.T4.1.16.15.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T4.1.16.15.2" class="ltx_td ltx_align_center">TET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S2.T4.1.16.15.3" class="ltx_td ltx_align_center">ResNet-34</td>
<td id="S2.T4.1.16.15.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.16.15.5" class="ltx_td ltx_align_center">21.79</td>
<td id="S2.T4.1.16.15.6" class="ltx_td ltx_align_center">6</td>
<td id="S2.T4.1.16.15.7" class="ltx_td ltx_align_center">64.79</td>
</tr>
<tr id="S2.T4.1.17.16" class="ltx_tr">
<td id="S2.T4.1.17.16.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T4.1.17.16.2" class="ltx_td ltx_align_center">SEW-ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
<td id="S2.T4.1.17.16.3" class="ltx_td ltx_align_center">SEW-ResNet-152</td>
<td id="S2.T4.1.17.16.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.17.16.5" class="ltx_td ltx_align_center">60.19</td>
<td id="S2.T4.1.17.16.6" class="ltx_td ltx_align_center">5</td>
<td id="S2.T4.1.17.16.7" class="ltx_td ltx_align_center">69.26</td>
</tr>
<tr id="S2.T4.1.18.17" class="ltx_tr">
<td id="S2.T4.1.18.17.1" class="ltx_td ltx_align_center">2021</td>
<td id="S2.T4.1.18.17.2" class="ltx_td ltx_align_center">MS-ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
<td id="S2.T4.1.18.17.3" class="ltx_td ltx_align_center">MS-ResNet-104</td>
<td id="S2.T4.1.18.17.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.18.17.5" class="ltx_td ltx_align_center">78.37</td>
<td id="S2.T4.1.18.17.6" class="ltx_td ltx_align_center">5</td>
<td id="S2.T4.1.18.17.7" class="ltx_td ltx_align_center">74.21</td>
</tr>
<tr id="S2.T4.1.19.18" class="ltx_tr">
<td id="S2.T4.1.19.18.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T4.1.19.18.2" class="ltx_td ltx_align_center">MPBN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
<td id="S2.T4.1.19.18.3" class="ltx_td ltx_align_center">ResNet-34</td>
<td id="S2.T4.1.19.18.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.19.18.5" class="ltx_td ltx_align_center">21.79</td>
<td id="S2.T4.1.19.18.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.19.18.7" class="ltx_td ltx_align_center">64.71</td>
</tr>
<tr id="S2.T4.1.20.19" class="ltx_tr">
<td id="S2.T4.1.20.19.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T4.1.20.19.2" class="ltx_td ltx_align_center">Attention SNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
</td>
<td id="S2.T4.1.20.19.3" class="ltx_td ltx_align_center">ResNet-34</td>
<td id="S2.T4.1.20.19.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.20.19.5" class="ltx_td ltx_align_center">22.11</td>
<td id="S2.T4.1.20.19.6" class="ltx_td ltx_align_center">1</td>
<td id="S2.T4.1.20.19.7" class="ltx_td ltx_align_center">69.15</td>
</tr>
<tr id="S2.T4.1.21.20" class="ltx_tr">
<td id="S2.T4.1.21.20.1" class="ltx_td ltx_align_center">2022</td>
<td id="S2.T4.1.21.20.2" class="ltx_td ltx_align_center">Spikformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
<td id="S2.T4.1.21.20.3" class="ltx_td ltx_align_center">Spikformer-8-768</td>
<td id="S2.T4.1.21.20.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.21.20.5" class="ltx_td ltx_align_center">66.34</td>
<td id="S2.T4.1.21.20.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.21.20.7" class="ltx_td ltx_align_center">74.81</td>
</tr>
<tr id="S2.T4.1.22.21" class="ltx_tr">
<td id="S2.T4.1.22.21.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T4.1.22.21.2" class="ltx_td ltx_align_center">Spike-driven Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>
</td>
<td id="S2.T4.1.22.21.3" class="ltx_td ltx_align_center">Spiking Transformer-10-512</td>
<td id="S2.T4.1.22.21.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.22.21.5" class="ltx_td ltx_align_center">36.01</td>
<td id="S2.T4.1.22.21.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.22.21.7" class="ltx_td ltx_align_center">74.66</td>
</tr>
<tr id="S2.T4.1.23.22" class="ltx_tr">
<td id="S2.T4.1.23.22.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T4.1.23.22.2" class="ltx_td ltx_align_center">Spiking ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
<td id="S2.T4.1.23.22.3" class="ltx_td ltx_align_center">Spikformer-8-512</td>
<td id="S2.T4.1.23.22.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.23.22.5" class="ltx_td ltx_align_center">29.68</td>
<td id="S2.T4.1.23.22.6" class="ltx_td ltx_align_center">1.3</td>
<td id="S2.T4.1.23.22.7" class="ltx_td ltx_align_center">68.04</td>
</tr>
<tr id="S2.T4.1.24.23" class="ltx_tr">
<td id="S2.T4.1.24.23.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T4.1.24.23.2" class="ltx_td ltx_align_center">AutoST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>
</td>
<td id="S2.T4.1.24.23.3" class="ltx_td ltx_align_center">AutoST-base</td>
<td id="S2.T4.1.24.23.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.24.23.5" class="ltx_td ltx_align_center">34.44</td>
<td id="S2.T4.1.24.23.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.24.23.7" class="ltx_td ltx_align_center">74.54</td>
</tr>
<tr id="S2.T4.1.25.24" class="ltx_tr">
<td id="S2.T4.1.25.24.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T4.1.25.24.2" class="ltx_td ltx_align_center">MST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>
</td>
<td id="S2.T4.1.25.24.3" class="ltx_td ltx_align_center">Swin-T (BN)</td>
<td id="S2.T4.1.25.24.4" class="ltx_td ltx_align_center">Conversion</td>
<td id="S2.T4.1.25.24.5" class="ltx_td ltx_align_center">28.5</td>
<td id="S2.T4.1.25.24.6" class="ltx_td ltx_align_center">512</td>
<td id="S2.T4.1.25.24.7" class="ltx_td ltx_align_center">78.5</td>
</tr>
<tr id="S2.T4.1.26.25" class="ltx_tr">
<td id="S2.T4.1.26.25.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T4.1.26.25.2" class="ltx_td ltx_align_center">Spikingformer-RL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>
</td>
<td id="S2.T4.1.26.25.3" class="ltx_td ltx_align_center">Spikingformer-8-768</td>
<td id="S2.T4.1.26.25.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.26.25.5" class="ltx_td ltx_align_center">66.34</td>
<td id="S2.T4.1.26.25.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.26.25.7" class="ltx_td ltx_align_center">75.85</td>
</tr>
<tr id="S2.T4.1.27.26" class="ltx_tr">
<td id="S2.T4.1.27.26.1" class="ltx_td ltx_align_center">2023</td>
<td id="S2.T4.1.27.26.2" class="ltx_td ltx_align_center">Spikingformer-CML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>
</td>
<td id="S2.T4.1.27.26.3" class="ltx_td ltx_align_center">Spikingformer-8-768</td>
<td id="S2.T4.1.27.26.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.27.26.5" class="ltx_td ltx_align_center">66.34</td>
<td id="S2.T4.1.27.26.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.27.26.7" class="ltx_td ltx_align_center">77.64</td>
</tr>
<tr id="S2.T4.1.28.27" class="ltx_tr">
<td id="S2.T4.1.28.27.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T4.1.28.27.2" class="ltx_td ltx_align_center">Spikeformer-CT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>
</td>
<td id="S2.T4.1.28.27.3" class="ltx_td ltx_align_center">Spikeformer-7L/3 × 2 × 4</td>
<td id="S2.T4.1.28.27.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.28.27.5" class="ltx_td ltx_align_center">38.75</td>
<td id="S2.T4.1.28.27.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.28.27.7" class="ltx_td ltx_align_center">75.89</td>
</tr>
<tr id="S2.T4.1.29.28" class="ltx_tr">
<td id="S2.T4.1.29.28.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T4.1.29.28.2" class="ltx_td ltx_align_center">Spikformer V2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>
</td>
<td id="S2.T4.1.29.28.3" class="ltx_td ltx_align_center">Spikformer V2-8-512</td>
<td id="S2.T4.1.29.28.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.29.28.5" class="ltx_td ltx_align_center">51.55</td>
<td id="S2.T4.1.29.28.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.29.28.7" class="ltx_td ltx_align_center">80.38</td>
</tr>
<tr id="S2.T4.1.30.29" class="ltx_tr">
<td id="S2.T4.1.30.29.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T4.1.30.29.2" class="ltx_td ltx_align_center">Spike-driven Transformer V2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>
</td>
<td id="S2.T4.1.30.29.3" class="ltx_td ltx_align_center">Meta-SpikeFormer</td>
<td id="S2.T4.1.30.29.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.30.29.5" class="ltx_td ltx_align_center">55.4</td>
<td id="S2.T4.1.30.29.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.30.29.7" class="ltx_td ltx_align_center">79.7</td>
</tr>
<tr id="S2.T4.1.31.30" class="ltx_tr">
<td id="S2.T4.1.31.30.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T4.1.31.30.2" class="ltx_td ltx_align_center">QKFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>
</td>
<td id="S2.T4.1.31.30.3" class="ltx_td ltx_align_center">HST-10-768</td>
<td id="S2.T4.1.31.30.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.31.30.5" class="ltx_td ltx_align_center">64.96</td>
<td id="S2.T4.1.31.30.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.31.30.7" class="ltx_td ltx_align_center">84.22</td>
</tr>
<tr id="S2.T4.1.32.31" class="ltx_tr">
<td id="S2.T4.1.32.31.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T4.1.32.31.2" class="ltx_td ltx_align_center">SpikingResformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>
</td>
<td id="S2.T4.1.32.31.3" class="ltx_td ltx_align_center">SpikingResformer-L</td>
<td id="S2.T4.1.32.31.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.32.31.5" class="ltx_td ltx_align_center">60.38</td>
<td id="S2.T4.1.32.31.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.32.31.7" class="ltx_td ltx_align_center">78.77</td>
</tr>
<tr id="S2.T4.1.33.32" class="ltx_tr">
<td id="S2.T4.1.33.32.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T4.1.33.32.2" class="ltx_td ltx_align_center">SGLFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>
</td>
<td id="S2.T4.1.33.32.3" class="ltx_td ltx_align_center">SGLFormer-8-768</td>
<td id="S2.T4.1.33.32.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.33.32.5" class="ltx_td ltx_align_center">64.02</td>
<td id="S2.T4.1.33.32.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.33.32.7" class="ltx_td ltx_align_center">83.73</td>
</tr>
<tr id="S2.T4.1.34.33" class="ltx_tr">
<td id="S2.T4.1.34.33.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T4.1.34.33.2" class="ltx_td ltx_align_center">OST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>
</td>
<td id="S2.T4.1.34.33.3" class="ltx_td ltx_align_center">OST-8-512</td>
<td id="S2.T4.1.34.33.4" class="ltx_td ltx_align_center">Direct Training</td>
<td id="S2.T4.1.34.33.5" class="ltx_td ltx_align_center">33.87</td>
<td id="S2.T4.1.34.33.6" class="ltx_td ltx_align_center">4(1)<sup id="S2.T4.1.34.33.6.1" class="ltx_sup">a</sup>
</td>
<td id="S2.T4.1.34.33.7" class="ltx_td ltx_align_center">74.97</td>
</tr>
<tr id="S2.T4.1.35.34" class="ltx_tr">
<td id="S2.T4.1.35.34.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T4.1.35.34.2" class="ltx_td ltx_align_center">SpikeZIP-TF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>
</td>
<td id="S2.T4.1.35.34.3" class="ltx_td ltx_align_center">SViT-L-32Level</td>
<td id="S2.T4.1.35.34.4" class="ltx_td ltx_align_center">Conversion</td>
<td id="S2.T4.1.35.34.5" class="ltx_td ltx_align_center">304.33</td>
<td id="S2.T4.1.35.34.6" class="ltx_td ltx_align_center">64</td>
<td id="S2.T4.1.35.34.7" class="ltx_td ltx_align_center">83.82</td>
</tr>
<tr id="S2.T4.1.36.35" class="ltx_tr">
<td id="S2.T4.1.36.35.1" class="ltx_td ltx_align_center">2024</td>
<td id="S2.T4.1.36.35.2" class="ltx_td ltx_align_center">ECMT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>
</td>
<td id="S2.T4.1.36.35.3" class="ltx_td ltx_align_center">EVA</td>
<td id="S2.T4.1.36.35.4" class="ltx_td ltx_align_center">Conversion</td>
<td id="S2.T4.1.36.35.5" class="ltx_td ltx_align_center">1074</td>
<td id="S2.T4.1.36.35.6" class="ltx_td ltx_align_center">4</td>
<td id="S2.T4.1.36.35.7" class="ltx_td ltx_align_center">88.60</td>
</tr>
<tr id="S2.T4.1.37.36" class="ltx_tr">
<td id="S2.T4.1.37.36.1" class="ltx_td ltx_align_center ltx_border_bb">2024</td>
<td id="S2.T4.1.37.36.2" class="ltx_td ltx_align_center ltx_border_bb">SWformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>
</td>
<td id="S2.T4.1.37.36.3" class="ltx_td ltx_align_center ltx_border_bb">Transformer-8-512</td>
<td id="S2.T4.1.37.36.4" class="ltx_td ltx_align_center ltx_border_bb">Direct Training</td>
<td id="S2.T4.1.37.36.5" class="ltx_td ltx_align_center ltx_border_bb">23.14</td>
<td id="S2.T4.1.37.36.6" class="ltx_td ltx_align_center ltx_border_bb">4</td>
<td id="S2.T4.1.37.36.7" class="ltx_td ltx_align_center ltx_border_bb">75.29</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S2.I2" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S2.I2.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">a</span> 
<div id="S2.I2.ix1.p1" class="ltx_para">
<p id="S2.I2.ix1.p1.1" class="ltx_p">Inputs are spike trains of 4 time steps, compressed to 1 time step inside the pipeline.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<figure id="S2.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Implementation Comparison of Spiking Transformers Trained from Scratch</figcaption>
<table id="S2.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T5.1.1.1" class="ltx_tr">
<th id="S2.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S2.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S2.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">Work</span></th>
<th id="S2.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T5.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T5.1.1.1.3.1.1" class="ltx_tr">
<td id="S2.T5.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T5.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Stacked /</span></td>
</tr>
<tr id="S2.T5.1.1.1.3.1.2" class="ltx_tr">
<td id="S2.T5.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T5.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">MLP</span></td>
</tr>
</table>
</th>
<th id="S2.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T5.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T5.1.1.1.4.1.1" class="ltx_tr">
<td id="S2.T5.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T5.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Attention</span></td>
</tr>
<tr id="S2.T5.1.1.1.4.1.2" class="ltx_tr">
<td id="S2.T5.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T5.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">Module</span></td>
</tr>
</table>
</th>
<th id="S2.T5.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T5.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T5.1.1.1.5.1.1" class="ltx_tr">
<td id="S2.T5.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T5.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Residual</span></td>
</tr>
<tr id="S2.T5.1.1.1.5.1.2" class="ltx_tr">
<td id="S2.T5.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T5.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">Connections</span></td>
</tr>
</table>
</th>
<th id="S2.T5.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T5.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T5.1.1.1.6.1.1" class="ltx_tr">
<td id="S2.T5.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T5.1.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">Patch</span></td>
</tr>
<tr id="S2.T5.1.1.1.6.1.2" class="ltx_tr">
<td id="S2.T5.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T5.1.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold">Embedding</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T5.1.2.1" class="ltx_tr">
<th id="S2.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">2022</th>
<td id="S2.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">Spikformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
<td id="S2.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">Addition</td>
<td id="S2.T5.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">Addition</td>
<td id="S2.T5.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">Integers</td>
<td id="S2.T5.1.2.1.6" class="ltx_td ltx_align_center ltx_border_tt">Addition</td>
</tr>
<tr id="S2.T5.1.3.2" class="ltx_tr">
<th id="S2.T5.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2023</th>
<td id="S2.T5.1.3.2.2" class="ltx_td ltx_align_center">Spike-driven Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>
</td>
<td id="S2.T5.1.3.2.3" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.3.2.4" class="ltx_td ltx_align_center">Mask &amp; Addition</td>
<td id="S2.T5.1.3.2.5" class="ltx_td ltx_align_center">Real-values</td>
<td id="S2.T5.1.3.2.6" class="ltx_td ltx_align_center">Addition &amp; Real-valued Max-Pool</td>
</tr>
<tr id="S2.T5.1.4.3" class="ltx_tr">
<th id="S2.T5.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2023</th>
<td id="S2.T5.1.4.3.2" class="ltx_td ltx_align_center">Spiking ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
<td id="S2.T5.1.4.3.3" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.4.3.4" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.4.3.5" class="ltx_td ltx_align_center">Integer</td>
<td id="S2.T5.1.4.3.6" class="ltx_td ltx_align_center">Addition</td>
</tr>
<tr id="S2.T5.1.5.4" class="ltx_tr">
<th id="S2.T5.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2023</th>
<td id="S2.T5.1.5.4.2" class="ltx_td ltx_align_center">AutoST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>
</td>
<td id="S2.T5.1.5.4.3" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.5.4.4" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.5.4.5" class="ltx_td ltx_align_center">Real-values</td>
<td id="S2.T5.1.5.4.6" class="ltx_td ltx_align_center">Addition &amp; Multiplication</td>
</tr>
<tr id="S2.T5.1.6.5" class="ltx_tr">
<th id="S2.T5.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2023</th>
<td id="S2.T5.1.6.5.2" class="ltx_td ltx_align_center">Spikingformer-RL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>
</td>
<td id="S2.T5.1.6.5.3" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.6.5.4" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.6.5.5" class="ltx_td ltx_align_center">Real-values</td>
<td id="S2.T5.1.6.5.6" class="ltx_td ltx_align_center">Addition</td>
</tr>
<tr id="S2.T5.1.7.6" class="ltx_tr">
<th id="S2.T5.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2023</th>
<td id="S2.T5.1.7.6.2" class="ltx_td ltx_align_center">Spikingformer-CML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>
</td>
<td id="S2.T5.1.7.6.3" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.7.6.4" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.7.6.5" class="ltx_td ltx_align_center">Real-values</td>
<td id="S2.T5.1.7.6.6" class="ltx_td ltx_align_center">Addition &amp; Real-valued Max-Pool</td>
</tr>
<tr id="S2.T5.1.8.7" class="ltx_tr">
<th id="S2.T5.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2024</th>
<td id="S2.T5.1.8.7.2" class="ltx_td ltx_align_center">Spikeformer-CT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>
</td>
<td id="S2.T5.1.8.7.3" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.8.7.4" class="ltx_td ltx_align_center">Addition &amp; Multiplication</td>
<td id="S2.T5.1.8.7.5" class="ltx_td ltx_align_center">Real-values</td>
<td id="S2.T5.1.8.7.6" class="ltx_td ltx_align_center">Addition &amp; Multiplication</td>
</tr>
<tr id="S2.T5.1.9.8" class="ltx_tr">
<th id="S2.T5.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2024</th>
<td id="S2.T5.1.9.8.2" class="ltx_td ltx_align_center">Spikformer V2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>
</td>
<td id="S2.T5.1.9.8.3" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.9.8.4" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.9.8.5" class="ltx_td ltx_align_center">Integers</td>
<td id="S2.T5.1.9.8.6" class="ltx_td ltx_align_center">Addition</td>
</tr>
<tr id="S2.T5.1.10.9" class="ltx_tr">
<th id="S2.T5.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2024</th>
<td id="S2.T5.1.10.9.2" class="ltx_td ltx_align_center">Spike-driven Transformer V2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>
</td>
<td id="S2.T5.1.10.9.3" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.10.9.4" class="ltx_td ltx_align_center">Mask &amp; Addition</td>
<td id="S2.T5.1.10.9.5" class="ltx_td ltx_align_center">Real-values</td>
<td id="S2.T5.1.10.9.6" class="ltx_td ltx_align_center">Addition</td>
</tr>
<tr id="S2.T5.1.11.10" class="ltx_tr">
<th id="S2.T5.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2024</th>
<td id="S2.T5.1.11.10.2" class="ltx_td ltx_align_center">QKFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>
</td>
<td id="S2.T5.1.11.10.3" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.11.10.4" class="ltx_td ltx_align_center">Mask &amp; Addition</td>
<td id="S2.T5.1.11.10.5" class="ltx_td ltx_align_center">
<table id="S2.T5.1.11.10.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T5.1.11.10.5.1.1" class="ltx_tr">
<td id="S2.T5.1.11.10.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Learnable</td>
</tr>
<tr id="S2.T5.1.11.10.5.1.2" class="ltx_tr">
<td id="S2.T5.1.11.10.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Weights</td>
</tr>
</table>
</td>
<td id="S2.T5.1.11.10.6" class="ltx_td ltx_align_center">
<table id="S2.T5.1.11.10.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T5.1.11.10.6.1.1" class="ltx_tr">
<td id="S2.T5.1.11.10.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Addition &amp; Real-valued Max-Pool</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T5.1.12.11" class="ltx_tr">
<th id="S2.T5.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2024</th>
<td id="S2.T5.1.12.11.2" class="ltx_td ltx_align_center">SGLFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>
</td>
<td id="S2.T5.1.12.11.3" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.12.11.4" class="ltx_td ltx_align_center">Addition</td>
<td id="S2.T5.1.12.11.5" class="ltx_td ltx_align_center">Integer</td>
<td id="S2.T5.1.12.11.6" class="ltx_td ltx_align_center">Addition &amp; Real-valued Max-Pool</td>
</tr>
<tr id="S2.T5.1.13.12" class="ltx_tr">
<th id="S2.T5.1.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">2024</th>
<td id="S2.T5.1.13.12.2" class="ltx_td ltx_align_center ltx_border_bb">SWformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>
</td>
<td id="S2.T5.1.13.12.3" class="ltx_td ltx_align_center ltx_border_bb">Addition</td>
<td id="S2.T5.1.13.12.4" class="ltx_td ltx_align_center ltx_border_bb">Addition</td>
<td id="S2.T5.1.13.12.5" class="ltx_td ltx_align_center ltx_border_bb">Real-values</td>
<td id="S2.T5.1.13.12.6" class="ltx_td ltx_align_center ltx_border_bb">Addition</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span><span id="S2.SS3.1.1" class="ltx_text ltx_font_italic">Benchmarking</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In this section, we present a comparison of methods for building deep SNNs on ImageNet, one of the most widely used benchmarks for image classification.</p>
</div>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Configurations</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p"><span id="S2.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_italic">Dataset.</span> We survey image classification task on ILSVRC2012, which is also known as ImageNet or ImageNet-1k. This dataset comprises 1.2 million training images, 50,000 validation images, and 100,000 test images in 1,000 classes. For a fair comparison, we only compare results with an inference input resolution of 224 <math id="S2.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.SS3.SSS1.p1.1.m1.1a"><mo id="S2.SS3.SSS1.p1.1.m1.1.1" xref="S2.SS3.SSS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.1.m1.1b"><times id="S2.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.1.m1.1c">\times</annotation></semantics></math> 224.</p>
</div>
<div id="S2.SS3.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS1.p2.1" class="ltx_p"><span id="S2.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_italic">Evaluation Metrics.</span> To measure the performance of deep SNNs, we employ classification accuracy and time steps (latency) as two main metrics for comparison. In addition, we also include number of parameters in our comparison</p>
</div>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2409.02111/assets/x7.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="323" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>ImageNet classification results for SOTA Spiking Transformers.</figcaption>
</figure>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Comparision</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">In Table <a href="#S2.T4" title="TABLE IV ‣ 2.2.2 Transformer Architectures ‣ 2.2 Network Architectures in Large Spiking Neural Networks ‣ 2 Deep Spiking Neural Networks ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, we summarize methods for building deep SNNs on ImageNet, comparing performance reported in corresponding papers. For each method, we present the result obtained by the model with highest classification accuracy. As is shown in the table, the performance of deep SNNs has improved significantly over the past few years, approaching the state-of-the-art performance of ANNs. In early years, pioneering deep SNNs on ImageNet usually employ ANN-to-SNN conversion methods to build deep models. However, although methods such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> can achieve comparable performance to their ANN counter parts, they also require a latency of thousands of time steps that would effectively negate the energy advantage of SNNs and obstruct the potential application of SNNs in real-time scenarios. To address this problem, researchers have developed effective ANN-to-SNN conversion methods and direct training methods that can infer in just several time steps while maintaining comparable performance to ANNs. Meanwhile, it is also worth noting that there is still a performance gap between SNNs with a single time step for inference, which is analog to binary neural networks (BNNs) and ANNs. Besides, modern deep SNNs usually achieve state-of-the-art performance with 4 time steps, which effectively resembles a 2-bit quantization and coincides with findings in network quantization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite>. Therefore, how to conserve maximal information with a minimal latency in SNNs remains a valuable research topic. Ignited by Spikformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, research on deep SNNs with Transformer architecture is also booming. Although Spikformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> suffers a performance gap between SNNs and their corresponding ANNs (73.38% accuracy of SNN Spikformer-8-512 vs. 80.80% accuracy of ANN Transformer-8-512), performance of deep SNNs is catching up with state-of-the-art ANNs. In Fig. <a href="#S2.F5" title="Figure 5 ‣ 2.3.1 Configurations ‣ 2.3 Benchmarking ‣ 2 Deep Spiking Neural Networks ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we further demonstrate the performance of typical Spiking Transformers with respect to the size of models. The recent announced QKFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> achieves 84.22% accuracy, which is comparable to an ANN baseline reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> that employs no external data nor distillation. In Table <a href="#S2.T5" title="TABLE V ‣ 2.2.2 Transformer Architectures ‣ 2.2 Network Architectures in Large Spiking Neural Networks ‣ 2 Deep Spiking Neural Networks ‣ Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, we also summarize the implementation details of typical Spiking Transformers trained from scratch.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Future Directions</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span id="S3.SS1.1.1" class="ltx_text ltx_font_italic">Learning Rules for Deep SNNs</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Deep Spiking Neural Networks (SNNs), particularly those incorporating Transformer architectures, have achieved impressive results in recent years. However, developing deep SNNs with capabilities comparable to state-of-the-art Artificial Neural Networks (ANNs) remains challenging. To address this issue, several potential directions for future research can be explored.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Backpropagation Gradient Rules</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">One direction is to learn from the success of deep learning with modern ANNs. This approach involves leveraging powerful architectures and training techniques from ANNs to develop their spiking counterparts. Currently, most gradient backpropagation-based SNN methods fall into this category, including ANN-to-SNN conversion and surrogate gradient direct training. Recently, the development of Spiking Transformers, which incorporate spiking self-attention, has significantly advanced this field. However, issues such as information loss and gradient vanishing in deep layers still limit the scalability of deep SNNs due to binary spike signals. Additionally, there is a lack of methods to efficiently utilize the temporal gradient information inherent in the recurrent nature of SNNs. We can anticipate many research efforts aimed at addressing these challenges.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Non-Backpropagation Gradient Rules</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">Another direction is to explore gradient rules that do not rely on traditional backpropagation mechanisms. Observing biological neurons, which transmit information through axons without a secondary mechanism for gradient backpropagation, suggests that non-backpropagation rules may align better with the nature of SNNs. Potential alternatives to conventional backpropagation include equilibrium propagation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite> and the Forward-Forward approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite>. For instance, equilibrium propagation utilizes the dynamics of the system for learning, while the Forward-Forward approach employs contrastive learning by feeding both the sample and its corresponding target into the network. Although still in its early stages, equilibrium propagation has already been applied in deep SNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>. These novel learning paradigms hold promise for fully realizing the potential of SNNs.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Biology-inspired Rules</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">A third direction is to leverage the inherent biological plasticity of SNNs and integrate insights from neuroscience to develop deep SNNs. Although the widely used Integrate-and-Fire (IF) and Leaky Integrate-and-Fire (LIF) neuron models are valued for their simplicity, they are limited in their biological plasticity. Research into neuron modeling, structural plasticity, and the role of dendrites shows promise for incorporating more sophisticated brain-like behaviors. Additionally, while gradient backpropagation relies on global plasticity, this contrasts with the local plasticity rules observed in neurobiology. Introducing local synaptic plasticity could enhance the learning capabilities of SNNs. However, such research may necessitate the co-design of neuron models and learning algorithms to achieve an optimal balance between complexity and learnability.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_italic">Towards Large Models</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">An important future research direction for deep SNNs is to develop large models with state-of-the-art capabilities that can perform a variety of tasks. However, several challenges need to be addressed compared to state-of-the-art large language models (LLMs).</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Scalability</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Compared to state-of-the-art artificial neural networks (ANNs) that have billions of parameters, deep spiking neural networks (SNNs) are still limited in the number of parameters they can effectively utilize. Deep SNNs typically employ millions of parameters due to the challenges associated with training. To overcome this limitation, substantial research efforts are needed to explore new learning rules, architectures, and training techniques for scaling up SNN models.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Multi-modal Models</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">In the future, large models are expected to process a wide variety of data types, including images, videos, audio, text, sensor data, and more. However, current research on deep spiking neural networks (SNNs) primarily focuses on images. Advancing research on spiking multi-modal models will require novel architectural designs and evaluation methods. Additionally, exploring how to enhance spiking multi-modal models with neuromorphic sensor data is a particularly interesting avenue. Spiking multi-modal models hold great promise for unlocking the potential of large spiking models across a variety of tasks.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Attention/Post-attention Architectural Paradigms</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Although spiking self-attention has shown success in constructing Spiking Transformers, it has reduced capabilities compared to vanilla self-attention due to the removal of non-linearity. To effectively extract information for large-scale SNN models, more efficient attention architectures should be developed in conjunction with the spiking mechanism. Additionally, exploring alternatives to attention architectures, especially for handling longer contexts, is a promising area of research. Advancements in these areas could lead to the development of more efficient architectures, enabling the creation of large SNN models.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this article, we have reviewed the learning and architectural paradigms toward developing large-scale spiking neural networks with a particular focus on the emerging Spiking Transformers. Delving into the state-of-the-art approaches of constructing deep spiking neural networks, this study demonstrates the potential of large-scale SNNs in achieving energy-efficient machine intelligence systems. We hope this study will help researchers efficiently grasp the core techniques employed in the emerging Spiking Transformers. Our study also identified key challenges toward developing large-scale spiking neural networks, including optimizing training algorithms, enhancing model scalability, etc. These challenges call for more powerful algorithms, larger models and further exploration in this domain.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Nature</em>, vol. 521, no. 7553, pp. 436–444, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 25, pp. 1097–1105, 2012.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,” <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>, vol. 29, no. 6, pp. 82–97, 2012.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, “Natural language processing (almost) from scratch,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Sesearch</em>, vol. 12, pp. 2493–2537, 2011.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Mastering the game of go with deep neural networks and tree search,” <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">Nature</em>, vol. 529, no. 7587, pp. 484–489, 2016.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, and A. a. Bolton, “Mastering the game of go without human knowledge,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Nature</em>, vol. 550, no. 7676, pp. 354–359, 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 30, pp. 6000–6010, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Language models are few-shot learners,” <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 33, pp. 1877–1901, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. de Vries, “The growing energy footprint of artificial intelligence,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Joule</em>, vol. 7, no. 10, pp. 2191–2194, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
K. Roy, A. Jaiswal, and P. Panda, “Towards spike-based machine intelligence with neuromorphic computing,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Nature</em>, vol. 575, no. 7784, pp. 607–617, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
W. Maass, “Networks of spiking neurons: the third generation of neural network models,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Neural Networks</em>, vol. 10, no. 9, pp. 1659–1671, 1997.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
P. A. Merolla, J. V. Arthur, R. Alvarez-Icaza, A. S. Cassidy, J. Sawada, F. Akopyan, B. L. Jackson, N. Imam, C. Guo, Y. Nakamura <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A million spiking-neuron integrated circuit with a scalable communication network and interface,” <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">Science</em>, vol. 345, no. 6197, pp. 668–673, 2014.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Davies, N. Srinivasa, T.-H. Lin, G. Chinya, Y. Cao, S. H. Choday, G. Dimou, P. Joshi, N. Imam, S. Jain <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Loihi: A neuromorphic manycore processor with on-chip learning,” <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">IEEE Micro</em>, vol. 38, no. 1, pp. 82–99, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
D. Ma, X. Jin, S. Sun, Y. Li, X. Wu, Y. Hu, F. Yang, H. Tang, X. Zhu, P. Lin, and G. Pan, “Darwin3: a large-scale neuromorphic chip with a novel ISA and on-chip learning,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">National Science Review</em>, vol. 11, no. 5, 2024.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
C. D. Schuman, S. R. Kulkarni, M. Parsa, J. P. Mitchell, P. Date, and B. Kay, “Opportunities for neuromorphic computing algorithms and applications,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Nature Computational Science</em>, vol. 2, no. 1, pp. 10–19, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. M. Bohte, J. N. Kok, and J. A. La Poutré, “Spikeprop: backpropagation for networks of spiking neurons.” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Symposium on Artificial Neural Networks</em>, vol. 48, 2000, pp. 419–424.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
R. Gütig and H. Sompolinsky, “The tempotron: a neuron that learns spike timing–based decisions,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Nature neuroscience</em>, vol. 9, no. 3, pp. 420–428, 2006.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
F. Ponulak and A. Kasiński, “Supervised learning in spiking neural networks with resume: sequence learning, classification, and spike shifting,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Neural Computation</em>, vol. 22, no. 2, pp. 467–510, 2010.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T. Masquelier and S. J. Thorpe, “Unsupervised learning of visual features through spike timing dependent plasticity,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">PLoS Computational Biology</em>, vol. 3, no. 2, p. e31, 2007.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, vol. 115, no. 3, pp. 211–252, 2015.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
G.-q. Bi and M.-m. Poo, “Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Journal of Neuroscience</em>, vol. 18, no. 24, pp. 10 464–10 472, 1998.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
P. U. Diehl, D. Neil, J. Binas, M. Cook, S.-C. Liu, and M. Pfeiffer, “Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Joint Conference on Neural Networks</em>, 2015, pp. 1–8.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Vigneron and J. Martinet, “A critical survey of STDP in spiking neural networks for pattern recognition,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Joint Conference on Neural Networks</em>, 2020, pp. 1–9.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Z. Yi, J. Lian, Q. Liu, H. Zhu, D. Liang, and J. Liu, “Learning rules in spiking neural networks: A survey,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, vol. 531, pp. 163–179, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y. Guo, X. Huang, and Z. Ma, “Direct learning-based deep spiking neural networks: a review,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Frontiers in Neuroscience</em>, vol. 17, p. 1209795, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M. Dampfhoffer, T. Mesquida, A. Valentian, and L. Anghel, “Backpropagation-based learning techniques for deep spiking neural networks: A survey,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. K. Eshraghian, M. Ward, E. O. Neftci, X. Wang, G. Lenz, G. Dwivedi, M. Bennamoun, D. S. Jeong, and W. D. Lu, “Training spiking neural networks using lessons from deep learning,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, vol. 111, no. 9, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
N. Rathi, I. Chakraborty, A. Kosta, A. Sengupta, A. Ankit, P. Panda, and K. Roy, “Exploring neuromorphic computing based on spiking neural networks: Algorithms to hardware,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, vol. 55, no. 12, pp. 1–49, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Z. Yan, J. Zhou, and W.-F. Wong, “Near lossless transfer learning for spiking neural networks,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 35, no. 12, 2021, pp. 10 577–10 584.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Y. Hu, H. Tang, and G. Pan, “Spiking deep residual networks,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, vol. 34, no. 8, pp. 5200–5205, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J. Wu, C. Xu, X. Han, D. Zhou, M. Zhang, H. Li, and K. C. Tan, “Progressive tandem learning for pattern recognition with deep spiking neural networks,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 44, no. 11, pp. 7824–7840, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
S. Deng and S. Gu, “Optimal conversion of conventional artificial neural networks to spiking neural networks,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedigns of the International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Y. Li, S. Deng, X. Dong, R. Gong, and S. Gu, “A free lunch from ANN: Towards efficient, accurate spiking neural networks calibration,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Machine Learning</em>, 2021, pp. 6316–6325.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
T. Bu, J. Ding, Z. Yu, and T. Huang, “Optimized potential initialization for low-latency spiking neural networks,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 36, no. 1, 2022, pp. 11–20.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
T. Bu, W. Fang, J. Ding, P. Dai, Z. Yu, and T. Huang, “Optimal ANN-SNN conversion for high-accuracy and ultra-low-latency spiking neural networks,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning Representations</em>, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Y. Hu, Q. Zheng, X. Jiang, and G. Pan, “Fast-SNN: Fast spiking neural network by converting quantized ANN,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 45, no. 12, pp. 14 546–14 562, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Y. Li, S. Deng, X. Dong, and S. Gu, “Error-aware conversion from ANN to SNN via post-training parameter calibration,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, pp. 1–24, 2024.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Y. Cao, Y. Chen, and D. Khosla, “Spiking deep convolutional neural networks for energy-efficient object recognition,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, vol. 113, no. 1, pp. 54–66, 2015.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
B. Rueckauer, I.-A. Lungu, Y. Hu, M. Pfeiffer, and S.-C. Liu, “Conversion of continuous-valued deep networks to efficient event-driven networks for image classification,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Frontiers in Neuroscience</em>, vol. 11, p. 682, 2017.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
A. Sengupta, Y. Ye, R. Wang, C. Liu, and K. Roy, “Going deeper in spiking neural networks: VGG and residual architectures,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Frontiers in Neuroscience</em>, vol. 13, p. 95, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
B. Han, G. Srinivasan, and K. Roy, “RMP-SNN: Residual membrane potential neuron for enabling deeper high-accuracy and low-latency spiking neural network,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020, pp. 13 558–13 567.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
S. Kim, S. Park, B. Na, and S. Yoon, “Spiking-YOLO: Spiking neural network for energy-efficient object detection,” in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 34, no. 07, 2020, pp. 11 270–11 277.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
W. Fang, Z. Yu, Y. Chen, T. Masquelier, T. Huang, and Y. Tian, “Incorporating learnable membrane time constant to enhance learning of spiking neural networks,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 2661–2671.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Y. Kim and P. Panda, “Revisiting batch normalization for training low-latency deep spiking neural networks from scratch,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Frontiers in Neuroscience</em>, p. 1638, 2021.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
H. Zheng, Y. Wu, L. Deng, Y. Hu, and G. Li, “Going deeper with directly-trained larger spiking neural networks,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 35, no. 12, 2021, pp. 11 062–11 070.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
N. Rathi and K. Roy, “DIET-SNN: A low-latency spiking neural network with direct input encoding and leakage and threshold optimization,” <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, vol. 34, no. 6, pp. 3174–3182, 2021.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Y. Li, Y. Guo, S. Zhang, S. Deng, Y. Hai, and S. Gu, “Differentiable spike: Rethinking gradient-descent for training spiking neural networks,” <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 34, pp. 23 426–23 439, 2021.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
S. Deng, Y. Li, S. Zhang, and S. Gu, “Temporal efficient training of spiking neural network via gradient re-weighting,” in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning Representations</em>, 2022.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Y. Guo, Y. Chen, L. Zhang, X. Liu, Y. Wang, X. Huang, and Z. Ma, “Im-loss: information maximization loss for spiking neural networks,” <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 156–166, 2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
C. Duan, J. Ding, S. Chen, Z. Yu, and T. Huang, “Temporal effective batch normalization in spiking neural networks,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 34 377–34 390, 2022.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
S. Wang, T. H. Cheng, and M.-H. Lim, “LTMD: Learning improvement of spiking neural networks with learnable thresholding neurons and moderate dropout,” <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 28 350–28 362, 2022.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
X. Yao, F. Li, Z. Mo, and J. Cheng, “GLIF: A unified gated leaky integrate-and-fire neuron for spiking neural networks,” <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 32 160–32 171, 2022.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Y. Guo, X. Tong, Y. Chen, L. Zhang, X. Liu, Z. Ma, and X. Huang, “RecDis-SNN: Rectifying membrane potential distribution for directly training spiking neural networks,” in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 326–335.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
S. Lian, J. Shen, Q. Liu, Z. Wang, R. Yan, and H. Tang, “Learnable surrogate gradient for direct training spiking neural networks,” in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Joint Conference on Artificial Intelligence</em>, 2023, pp. 3002–3010.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Y. Guo, Y. Zhang, Y. Chen, W. Peng, X. Liu, L. Zhang, X. Huang, and Z. Ma, “Membrane potential batch normalization for spiking neural networks,” in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023, pp. 19 420–19 430.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Q. Xu, Y. Li, J. Shen, J. K. Liu, H. Tang, and G. Pan, “Constructing deep spiking neural networks from artificial neural networks with knowledge distillation,” in <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 7886–7895.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
S. Lian, J. Shen, Z. Wang, and H. Tang, “IM-LIF: Improved neuronal dynamics with attention mechanism for direct training deep spiking neural network,” <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Emerging Topics in Computational Intelligence</em>, 2024.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
B. Mukhoty, V. Bojkovic, W. de Vazelhes, X. Zhao, G. De Masi, H. Xiong, and B. Gu, “Direct training of snn using local zeroth order method,” <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 36, pp. 18 994–19 014, 2024.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
M. Yao, G. Zhao, H. Zhang, Y. Hu, L. Deng, Y. Tian, B. Xu, and G. Li, “Attention spiking neural networks,” <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 45, no. 8, pp. 9393–9410, 2023.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
M. Yao, H. Gao, G. Zhao, D. Wang, Y. Lin, Z. Yang, and G. Li, “Temporal-wise attention spiking neural networks for event streams classification,” in <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 10 221–10 230.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
C. Yu, Z. Gu, D. Li, G. Wang, A. Wang, and E. Li, “STSC-SNN: Spatio-temporal synaptic connection with temporal convolution and attention for spiking neural networks,” <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Frontiers in Neuroscience</em>, vol. 16, p. 1079357, 2022.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
E. O. Neftci, H. Mostafa, and F. Zenke, “Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks,” <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>, vol. 36, no. 6, pp. 51–63, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
F. Zenke and S. Ganguli, “SuperSpike: Supervised learning in multilayer spiking neural networks,” <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Neural Computation</em>, vol. 30, no. 6, pp. 1514–1541, 2018.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Y. Wu, L. Deng, G. Li, J. Zhu, and L. Shi, “Spatio-temporal backpropagation for training high-performance spiking neural networks,” <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Frontiers in Neuroscience</em>, vol. 12, p. 331, 2018.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
P. Gu, R. Xiao, G. Pan, and H. Tang, “STCA: Spatio-temporal credit assignment with delayed feedback in deep spiking neural networks.” in <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Joint Conference on Artificial Intelligence</em>, 2019, pp. 1366–1372.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Y. Wu, L. Deng, G. Li, J. Zhu, Y. Xie, and L. Shi, “Direct training for spiking neural networks: Faster, larger, better,” in <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 33, no. 01, 2019, pp. 1311–1318.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
W. Fang, Z. Yu, Y. Chen, T. Huang, T. Masquelier, and Y. Tian, “Deep residual learning in spiking neural networks,” <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 34, pp. 21 056–21 069, 2021.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Y. Hu, L. Deng, Y. Wu, M. Yao, and G. Li, “Advancing spiking neural networks toward deep residual learning,” <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, 2024.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
S. K. Esser, P. A. Merollaa, J. V. Arthura, A. S. Cassidya, R. Appuswamya, A. Andreopoulosa, D. J. Berga, J. L. McKinstrya, T. Melanoa, D. R. Barcha <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Convolutional networks for fast energy-efficient neuromorphic computing,” <em id="bib.bib69.2.2" class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences of the United States of America</em>, vol. 113, no. 41, pp. 11 441–11 446, 2016.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning Representations</em>, 2015.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Y. Kim, Y. Li, H. Park, Y. Venkatesha, and P. Panda, “Neural architecture search for spiking neural networks,” in <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision</em>, 2022, pp. 36–56.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
B. Na, J. Mok, S. Park, D. Lee, H. Choe, and S. Yoon, “AutoSNN: Towards energy-efficient spiking neural networks,” in <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Machine Learning</em>, 2022, pp. 16 253–16 269.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
J. Yan, Q. Liu, M. Zhang, L. Feng, D. Ma, H. Li, and G. Pan, “Efficient spiking neural network design via neural architecture search,” <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">Neural Networks</em>, p. 106172, 2024.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
E. Mueller, V. Studenyak, D. Auge, and A. Knoll, “Spiking transformer networks: A rate coded approach for processing sequential data,” in <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Systems and Informatics</em>.   IEEE, 2021, pp. 1–5.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
J. Zhang, B. Dong, H. Zhang, J. Ding, F. Heide, B. Yin, and X. Yang, “Spiking transformers for event-based single object tracking,” in <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 8801–8810.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
J. Zhang, L. Tang, Z. Yu, J. Lu, and T. Huang, “Spike transformer: Monocular depth estimation for spiking camera,” in <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision</em>, 2022, pp. 34–52.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Z. Zhou, Y. Zhu, C. He, Y. Wang, S. Yan, Y. Tian, and L. Yuan, “Spikformer: When spiking neural network meets transformer,” in <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
L. Guo, Z. Gao, J. Qu, S. Zheng, R. Jiang, Y. Lu, and H. Qiao, “Transformer-based spiking neural networks for multimodal audio-visual classification,” <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Cognitive and Developmental Systems</em>, vol. 16, no. 3, pp. 1077–1086, 2023.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
M. Yao, J. Hu, Z. Zhou, L. Yuan, Y. Tian, B. Xu, and G. Li, “Spike-driven transformer,” <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 36, pp. 64 043–64 058, 2024.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
S. Zou, Y. Mu, X. Zuo, S. Wang, and L. Cheng, “Event-based human pose tracking by spiking spatiotemporal transformer,” <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.09681</em>, 2023.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Q. Wang, D. Zhang, T. Zhang, and B. Xu, “Attention-free spikformer: Mixing spike sequences with simple linear transforms,” <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.02557</em>, 2023.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
T. Li, W. Liu, C. Lv, J. Xu, C. Zhang, M. Wu, X. Zheng, and X. Huang, “SpikeCLIP: A contrastive language-image pretrained spiking neural network,” <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06488</em>, 2023.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
C. Lv, T. Li, J. Xu, C. Gu, Z. Ling, C. Zhang, X. Zheng, and X. Huang, “SpikeBERT: A language spikformer trained with two-stage knowledge distillation from BERT,” <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.15122</em>, 2023.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
M. Bal and A. Sengupta, “SpikingBERT: Distilling bert to train spiking language models using implicit differentiation,” in <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 38, no. 10, 2024, pp. 10 998–11 006.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
R.-J. Zhu, Q. Zhao, and J. K. Eshraghian, “SpikeGPT: Generative pre-trained language model with spiking neural networks,” <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13939</em>, 2023.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
G. Datta, Z. Liu, A. Li, and P. A. Beerel, “Spiking neural networks with dynamic time steps for vision transformers,” <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.16456</em>, 2023.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Y. Wang, K. Shi, C. Lu, Y. Liu, M. Zhang, and H. Qu, “Spatial-temporal self-attention for asynchronous spiking neural networks,” in <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Joint Conference on Artificial Intelligence</em>, vol. 8, 2023, pp. 3085–3093.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
L. Li and Y. Liu, “Multi-dimensional attention spiking transformer for event-based image classification,” in <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Communications, Information System and Computer Engineering</em>, 2023, pp. 359–362.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
X. Wang, Z. Wu, Y. Rong, L. Zhu, B. Jiang, J. Tang, and Y. Tian, “SSTFormer: bridging spiking neural network and memory support transformer for frame-event based recognition,” <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.04369</em>, 2023.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Z. Wang, Q. Zhao, J. Cui, X. Liu, and D. Xu, “AutoST: Training-free neural architecture search for spiking transformers,” in <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</em>, 2024, pp. 3455–3459.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Z. Wang, Y. Fang, J. Cao, Q. Zhang, Z. Wang, and R. Xu, “Masked spiking transformer,” in <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023, pp. 1761–1771.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
C. Zhou, L. Yu, Z. Zhou, H. Zhang, Z. Ma, H. Zhou, and Y. Tian, “Spikingformer: Spike-driven residual learning for transformer-based spiking neural network,” <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.11954</em>, 2023.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
C. Zhou, H. Zhang, Z. Zhou, L. Yu, Z. Ma, H. Zhou, X. Fan, and Y. Tian, “Enhancing the performance of transformer-based spiking neural networks by improved downsampling with precise gradient backpropagation,” <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.05954</em>, 2023.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
B. Xu, H. Geng, Y. Yin, and P. Li, “DISTA: Denoising spiking transformer with intrinsic plasticity and spatiotemporal attention,” <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.09376</em>, 2023.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
S. Shen, D. Zhao, G. Shen, and Y. Zeng, “TIM: An efficient temporal interaction module for spiking transformer,” <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.11687</em>, 2024.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Z. Zhou, K. Che, W. Fang, K. Tian, Y. Zhu, S. Yan, Y. Tian, and L. Yuan, “Spikformer v2: Join the high accuracy club on ImageNet with an SNN ticket,” <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.02020</em>, 2024.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
M. Yao, J. Hu, T. Hu, Y. Xu, Z. Zhou, Y. Tian, B. Xu, and G. Li, “Spike-driven transformer v2: Meta spiking neural network architecture inspiring the design of next-generation neuromorphic chips,” in <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning Representation</em>, 2024.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
M. Liu, J. Tang, H. Li, J. Qi, S. Li, K. Wang, Y. Wang, and H. Chen, “Spiking-PhysFormer: Camera-based remote photoplethysmography with parallel spike-driven transformer,” <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.04798</em>, 2024.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Y. Li, Y. Lei, and X. Yang, “Spikeformer: Training high-performance spiking neural network with transformer,” <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, vol. 574, p. 127279, 2024.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
S. Yang, H. Ma, C. Yu, A. Wang, and E.-P. Li, “SDiT: Spiking diffusion model with transformer,” <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.11588</em>, 2024.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Q. Chen, C. Sun, C. Gao, and S.-C. Liu, “Epilepsy seizure detection and prediction using an approximate spiking convolutional transformer,” <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.09424</em>, 2024.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
H. Zhang and Y. Zhang, “Memory-efficient reversible spiking neural networks,” in <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 38, no. 15, 2024, pp. 16 759–16 767.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
C. Zhou, H. Zhang, Z. Zhou, L. Yu, L. Huang, X. Fan, L. Yuan, Z. Ma, H. Zhou, and Y. Tian, “QKFormer: Hierarchical spiking transformer using qk attention,” <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.16552</em>, 2024.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
X. Shi, Z. Hao, and Z. Yu, “SpikingResformer: Bridging resnet and vision transformer in spiking neural networks,” in <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024, pp. 5610–5619.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
X. Xing, B. Gao, Z. Zhang, D. A. Clifton, S. Xiao, L. Du, G. Li, and J. Zhang, “SpikeLLM: Scaling up spiking neural network to large language models via saliency-based spiking,” <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.04752</em>, 2024.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Y. Jiang, K. Hu, T. Zhang, H. Gao, Y. Liu, Y. Fang, and F. Chen, “Spatio-temporal approximation: A training-free snn conversion for transformers,” in <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
K. You, Z. Xu, C. Nie, Z. Deng, Q. Guo, X. Wang, and Z. He, “SpikeZIP-TF: Conversion is all you need for transformer-based SNN,” in <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Machine Learning</em>, 2024.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Z. Huang, X. Shi, Z. Hao, T. Bu, J. Ding, Z. Yu, and T. Huang, “Towards high-performance spiking transformers from ANN to SNN conversion,” in <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM Multimedia</em>, 2024.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
J. Zhang, J. Shen, Z. Wang, Q. Guo, R. Yan, G. Pan, and H. Tang, “SpikingMiniLM: Energy-efficient spiking transformer for natural language understanding,” <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">Science China Information Sciences</em>, 2024.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
H. Zhang, C. Zhou, L. Yu, L. Huang, Z. Ma, X. Fan, H. Zhou, and Y. Tian, “SGLFormer: Spiking global-local-fusion transformer with high performance,” <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">Frontiers in Neuroscience</em>, vol. 18, p. 1371290, 2024.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
X. Song, A. Song, R. Xiao, and Y. Sun, “One-step spiking transformer with a linear complexity,” in <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Joint Conference on Artificial Intelligence</em>, 2024.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
S. Gao, X. Fan, X. Deng, Z. Hong, H. Zhou, and Z. Zhu, “TE-Spikformer: Temporal-enhanced spiking neural network with transformer,” <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, p. 128268, 2024.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Y. Fang, Z. Wang, L. Zhang, J. Cao, H. Chen, and R. Xu, “Spiking wavelet transformer,” <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.11138</em>, 2024.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, “Binarized neural networks,” <em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 29, pp. 4114–4122, 2016.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
W. Yu, C. Si, P. Zhou, M. Luo, Y. Zhou, J. Feng, S. Yan, and X. Wang, “Metaformer baselines for vision,” <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 46, no. 2, pp. 896–912, 2023.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
B. Scellier and Y. Bengio, “Equilibrium propagation: Bridging the gap between energy-based models and backpropagation,” <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">Frontiers in Computational Neuroscience</em>, vol. 11, p. 24, 2017.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
G. Hinton, “The forward-forward algorithm: Some preliminary investigations,” <em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.13345</em>, 2022.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
P. O’Connor, E. Gavves, and M. Welling, “Training a spiking neural network with equilibrium propagation,” in <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Artificial Intelligence and Statistics</em>, 2019, pp. 1516–1523.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.02110" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.02111" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.02111">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.02111" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.02113" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 22:32:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
