<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.07851] SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition</title><meta property="og:description" content="Speech emotion recognition (SER) has made significant strides with the advent of powerful self-supervised learning (SSL) models. However, the generalization of these models to diverse languages and emotional expression…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.07851">

<!--Generated on Thu Sep  5 16:57:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.4" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.5" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">Mohamed Osman<sup id="p1.3.1" class="ltx_sup">1</sup>, Daniel Z. Kaplan<sup id="p1.3.2" class="ltx_sup">2</sup>, Tamer Nadeem<sup id="p1.3.3" class="ltx_sup">1</sup>



</p>
</div>
<h1 class="ltx_title ltx_title_document">SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Speech emotion recognition (SER) has made significant strides with the advent of powerful self-supervised learning (SSL) models. However, the generalization of these models to diverse languages and emotional expressions remains a challenge. We propose a large-scale benchmark to evaluate the robustness and adaptability of state-of-the-art SER models in both in-domain and out-of-domain settings. Our benchmark includes a diverse set of multilingual datasets, focusing on less commonly used corpora to assess generalization to new data. We employ logit adjustment to account for varying class distributions and establish a single dataset cluster for systematic evaluation. Surprisingly, we find that the Whisper model, primarily designed for automatic speech recognition, outperforms dedicated SSL models in cross-lingual SER. Our results highlight the need for more robust and generalizable SER models, and our benchmark serves as a valuable resource to drive future research in this direction.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>speech recognition, human-computer interaction, computational paralinguistics
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Speech emotion recognition has garnered significant attention due to its potential to enable more natural and empathetic human-computer interaction. Recent advancements in self-supervised learning have led to powerful speech representation models like wav2vec2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, which have shown impressive performance on various speech processing tasks. However, the generalization of these models to diverse languages and emotional expressions remains a critical challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Existing SER benchmarks often focus on a limited set of well-studied datasets, which may not accurately reflect real-world scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Moreover, the emphasis on in-domain evaluation fails to capture the crucial aspect of out-of-domain generalization, which is essential for deploying SER systems in practical applications. For our paper's purpose, we define in-domain as evaluating on the same data distribution seen in training, and out-of-domain as evaluating on a different data distribution. This can manifest as different speakers, tones, decision boundaries, etc. To address these limitations, we propose a large-scale benchmark that evaluates SER models on a diverse collection of multilingual datasets, emphasizing zero-shot performance.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Our benchmark focuses on less commonly used datasets to mitigate overfitting and encourage the development of more robust and adaptable models. We employ state-of-the-art speech representation models, including Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, an automatic speech recognition model, and CLAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, a contrastive learning model, to analyze their performance in cross-lingual SER. Interestingly, our results show that Whisper consistently outperforms dedicated SSL models across most datasets, challenging the common belief that ASR models are suboptimal for SER due to their focus on phoneme recognition.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The main contributions of this work are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce a large-scale benchmark for evaluating the robustness and generalization of SER models across diverse languages and emotional expressions.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We curate a collection of multilingual datasets and establish targetted subsets for systematic in-domain and out-of-domain evaluation.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We employ logit adjustment<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to account for varying class distributions and ensure fair comparisons across datasets.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We conduct extensive experiments with state-of-the-art speech representation models and provide insights into their cross-lingual SER performance.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p">We open source our entire code base, our full un-reduced results and training logs, as well as all implementation details at the following url: <a target="_blank" href="https://github.com/spaghettiSystems/serval" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/spaghettiSystems/serval</a>.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Multilingual datasets used in our benchmark. Values reflect the datasets after the class mapping.</figcaption>
<div id="S1.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:356.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.8pt,2.3pt) scale(0.987356548636128,0.987356548636128) ;">
<table id="S1.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Dataset</span></th>
<th id="S1.T1.1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.1.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Classes</span></th>
<th id="S1.T1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.1.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Speakers</span></th>
<th id="S1.T1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.1.1.1.1.4.1" class="ltx_text" style="font-size:70%;">Language</span></th>
<th id="S1.T1.1.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.1.1.1.1.5.1" class="ltx_text" style="font-size:70%;">Samples</span></th>
<th id="S1.T1.1.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.1.1.1.1.6.1" class="ltx_text" style="font-size:70%;">Avg Duration (s)</span></th>
<th id="S1.T1.1.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.1.1.1.1.7.1" class="ltx_text" style="font-size:70%;">OOD Eligible</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.2.1" class="ltx_tr">
<td id="S1.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S1.T1.1.1.2.1.1.1" class="ltx_text" style="font-size:70%;">URDUDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.2.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S1.T1.1.1.2.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S1.T1.1.1.2.1.2.1" class="ltx_text" style="font-size:70%;">4</span></td>
<td id="S1.T1.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S1.T1.1.1.2.1.3.1" class="ltx_text" style="font-size:70%;">6</span></td>
<td id="S1.T1.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S1.T1.1.1.2.1.4.1" class="ltx_text" style="font-size:70%;">Urdu</span></td>
<td id="S1.T1.1.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S1.T1.1.1.2.1.5.1" class="ltx_text" style="font-size:70%;">400</span></td>
<td id="S1.T1.1.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S1.T1.1.1.2.1.6.1" class="ltx_text" style="font-size:70%;">2.5</span></td>
<td id="S1.T1.1.1.2.1.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S1.T1.1.1.2.1.7.1" class="ltx_text" style="font-size:70%;">No</span></td>
</tr>
<tr id="S1.T1.1.1.3.2" class="ltx_tr">
<td id="S1.T1.1.1.3.2.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.3.2.1.1" class="ltx_text" style="font-size:70%;">EmoDBDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.3.2.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S1.T1.1.1.3.2.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.3.2.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.3.2.2.1" class="ltx_text" style="font-size:70%;">6</span></td>
<td id="S1.T1.1.1.3.2.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.3.2.3.1" class="ltx_text" style="font-size:70%;">10</span></td>
<td id="S1.T1.1.1.3.2.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.3.2.4.1" class="ltx_text" style="font-size:70%;">German</span></td>
<td id="S1.T1.1.1.3.2.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.3.2.5.1" class="ltx_text" style="font-size:70%;">535</span></td>
<td id="S1.T1.1.1.3.2.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.3.2.6.1" class="ltx_text" style="font-size:70%;">2.8</span></td>
<td id="S1.T1.1.1.3.2.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.3.2.7.1" class="ltx_text" style="font-size:70%;">No</span></td>
</tr>
<tr id="S1.T1.1.1.4.3" class="ltx_tr">
<td id="S1.T1.1.1.4.3.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.4.3.1.1" class="ltx_text" style="font-size:70%;">EMOVODataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.4.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S1.T1.1.1.4.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.4.3.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.4.3.2.1" class="ltx_text" style="font-size:70%;">7</span></td>
<td id="S1.T1.1.1.4.3.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.4.3.3.1" class="ltx_text" style="font-size:70%;">10</span></td>
<td id="S1.T1.1.1.4.3.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.4.3.4.1" class="ltx_text" style="font-size:70%;">Italian</span></td>
<td id="S1.T1.1.1.4.3.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.4.3.5.1" class="ltx_text" style="font-size:70%;">588</span></td>
<td id="S1.T1.1.1.4.3.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.4.3.6.1" class="ltx_text" style="font-size:70%;">3.1</span></td>
<td id="S1.T1.1.1.4.3.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.4.3.7.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
</tr>
<tr id="S1.T1.1.1.5.4" class="ltx_tr">
<td id="S1.T1.1.1.5.4.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.5.4.1.1" class="ltx_text" style="font-size:70%;">eNTERFACEDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.5.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S1.T1.1.1.5.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.5.4.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.5.4.2.1" class="ltx_text" style="font-size:70%;">6</span></td>
<td id="S1.T1.1.1.5.4.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.5.4.3.1" class="ltx_text" style="font-size:70%;">42</span></td>
<td id="S1.T1.1.1.5.4.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.5.4.4.1" class="ltx_text" style="font-size:70%;">English</span></td>
<td id="S1.T1.1.1.5.4.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.5.4.5.1" class="ltx_text" style="font-size:70%;">1293</span></td>
<td id="S1.T1.1.1.5.4.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.5.4.6.1" class="ltx_text" style="font-size:70%;">2.9</span></td>
<td id="S1.T1.1.1.5.4.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.5.4.7.1" class="ltx_text" style="font-size:70%;">No</span></td>
</tr>
<tr id="S1.T1.1.1.6.5" class="ltx_tr">
<td id="S1.T1.1.1.6.5.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.6.5.1.1" class="ltx_text" style="font-size:70%;">MESDDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.6.5.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S1.T1.1.1.6.5.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.6.5.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.6.5.2.1" class="ltx_text" style="font-size:70%;">6</span></td>
<td id="S1.T1.1.1.6.5.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.6.5.3.1" class="ltx_text" style="font-size:70%;">6</span></td>
<td id="S1.T1.1.1.6.5.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.6.5.4.1" class="ltx_text" style="font-size:70%;">Spanish</span></td>
<td id="S1.T1.1.1.6.5.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.6.5.5.1" class="ltx_text" style="font-size:70%;">1150</span></td>
<td id="S1.T1.1.1.6.5.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.6.5.6.1" class="ltx_text" style="font-size:70%;">0.7</span></td>
<td id="S1.T1.1.1.6.5.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.6.5.7.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
</tr>
<tr id="S1.T1.1.1.7.6" class="ltx_tr">
<td id="S1.T1.1.1.7.6.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.7.6.1.1" class="ltx_text" style="font-size:70%;">MASCDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.7.6.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S1.T1.1.1.7.6.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.7.6.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.7.6.2.1" class="ltx_text" style="font-size:70%;">5</span></td>
<td id="S1.T1.1.1.7.6.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.7.6.3.1" class="ltx_text" style="font-size:70%;">68</span></td>
<td id="S1.T1.1.1.7.6.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.7.6.4.1" class="ltx_text" style="font-size:70%;">Mandarin</span></td>
<td id="S1.T1.1.1.7.6.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.7.6.5.1" class="ltx_text" style="font-size:70%;">25636</span></td>
<td id="S1.T1.1.1.7.6.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.7.6.6.1" class="ltx_text" style="font-size:70%;">1.9</span></td>
<td id="S1.T1.1.1.7.6.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.7.6.7.1" class="ltx_text" style="font-size:70%;">No</span></td>
</tr>
<tr id="S1.T1.1.1.8.7" class="ltx_tr">
<td id="S1.T1.1.1.8.7.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.8.7.1.1" class="ltx_text" style="font-size:70%;">DEMoSDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.8.7.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S1.T1.1.1.8.7.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.8.7.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.8.7.2.1" class="ltx_text" style="font-size:70%;">7</span></td>
<td id="S1.T1.1.1.8.7.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.8.7.3.1" class="ltx_text" style="font-size:70%;">68</span></td>
<td id="S1.T1.1.1.8.7.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.8.7.4.1" class="ltx_text" style="font-size:70%;">Italian</span></td>
<td id="S1.T1.1.1.8.7.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.8.7.5.1" class="ltx_text" style="font-size:70%;">9697</span></td>
<td id="S1.T1.1.1.8.7.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.8.7.6.1" class="ltx_text" style="font-size:70%;">2.9</span></td>
<td id="S1.T1.1.1.8.7.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.8.7.7.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
</tr>
<tr id="S1.T1.1.1.9.8" class="ltx_tr">
<td id="S1.T1.1.1.9.8.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.9.8.1.1" class="ltx_text" style="font-size:70%;">CASIADataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.9.8.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S1.T1.1.1.9.8.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.9.8.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.9.8.2.1" class="ltx_text" style="font-size:70%;">6</span></td>
<td id="S1.T1.1.1.9.8.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.9.8.3.1" class="ltx_text" style="font-size:70%;">4</span></td>
<td id="S1.T1.1.1.9.8.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.9.8.4.1" class="ltx_text" style="font-size:70%;">Mandarin</span></td>
<td id="S1.T1.1.1.9.8.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.9.8.5.1" class="ltx_text" style="font-size:70%;">1200</span></td>
<td id="S1.T1.1.1.9.8.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.9.8.6.1" class="ltx_text" style="font-size:70%;">1.9</span></td>
<td id="S1.T1.1.1.9.8.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.9.8.7.1" class="ltx_text" style="font-size:70%;">No</span></td>
</tr>
<tr id="S1.T1.1.1.10.9" class="ltx_tr">
<td id="S1.T1.1.1.10.9.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.10.9.1.1" class="ltx_text" style="font-size:70%;">AESDDDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.10.9.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S1.T1.1.1.10.9.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.10.9.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.10.9.2.1" class="ltx_text" style="font-size:70%;">5</span></td>
<td id="S1.T1.1.1.10.9.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.10.9.3.1" class="ltx_text" style="font-size:70%;">6</span></td>
<td id="S1.T1.1.1.10.9.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.10.9.4.1" class="ltx_text" style="font-size:70%;">Greek</span></td>
<td id="S1.T1.1.1.10.9.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.10.9.5.1" class="ltx_text" style="font-size:70%;">604</span></td>
<td id="S1.T1.1.1.10.9.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.10.9.6.1" class="ltx_text" style="font-size:70%;">4.1</span></td>
<td id="S1.T1.1.1.10.9.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.10.9.7.1" class="ltx_text" style="font-size:70%;">No</span></td>
</tr>
<tr id="S1.T1.1.1.11.10" class="ltx_tr">
<td id="S1.T1.1.1.11.10.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.11.10.1.1" class="ltx_text" style="font-size:70%;">BAUMDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.11.10.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S1.T1.1.1.11.10.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.11.10.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.11.10.2.1" class="ltx_text" style="font-size:70%;">8</span></td>
<td id="S1.T1.1.1.11.10.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.11.10.3.1" class="ltx_text" style="font-size:70%;">31</span></td>
<td id="S1.T1.1.1.11.10.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.11.10.4.1" class="ltx_text" style="font-size:70%;">Turkish</span></td>
<td id="S1.T1.1.1.11.10.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.11.10.5.1" class="ltx_text" style="font-size:70%;">1398</span></td>
<td id="S1.T1.1.1.11.10.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.11.10.6.1" class="ltx_text" style="font-size:70%;">4.6</span></td>
<td id="S1.T1.1.1.11.10.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.11.10.7.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
</tr>
<tr id="S1.T1.1.1.12.11" class="ltx_tr">
<td id="S1.T1.1.1.12.11.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.12.11.1.1" class="ltx_text" style="font-size:70%;">EEKKDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.12.11.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S1.T1.1.1.12.11.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.12.11.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.12.11.2.1" class="ltx_text" style="font-size:70%;">4</span></td>
<td id="S1.T1.1.1.12.11.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.12.11.3.1" class="ltx_text" style="font-size:70%;">10</span></td>
<td id="S1.T1.1.1.12.11.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.12.11.4.1" class="ltx_text" style="font-size:70%;">Estonian</span></td>
<td id="S1.T1.1.1.12.11.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.12.11.5.1" class="ltx_text" style="font-size:70%;">1164</span></td>
<td id="S1.T1.1.1.12.11.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.12.11.6.1" class="ltx_text" style="font-size:70%;">3.4</span></td>
<td id="S1.T1.1.1.12.11.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.12.11.7.1" class="ltx_text" style="font-size:70%;">No</span></td>
</tr>
<tr id="S1.T1.1.1.13.12" class="ltx_tr">
<td id="S1.T1.1.1.13.12.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.13.12.1.1" class="ltx_text" style="font-size:70%;">ThorstenDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.13.12.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S1.T1.1.1.13.12.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.13.12.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.13.12.2.1" class="ltx_text" style="font-size:70%;">6</span></td>
<td id="S1.T1.1.1.13.12.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.13.12.3.1" class="ltx_text" style="font-size:70%;">1</span></td>
<td id="S1.T1.1.1.13.12.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.13.12.4.1" class="ltx_text" style="font-size:70%;">German</span></td>
<td id="S1.T1.1.1.13.12.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.13.12.5.1" class="ltx_text" style="font-size:70%;">2399</span></td>
<td id="S1.T1.1.1.13.12.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.13.12.6.1" class="ltx_text" style="font-size:70%;">4.4</span></td>
<td id="S1.T1.1.1.13.12.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.13.12.7.1" class="ltx_text" style="font-size:70%;">No</span></td>
</tr>
<tr id="S1.T1.1.1.14.13" class="ltx_tr">
<td id="S1.T1.1.1.14.13.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.14.13.1.1" class="ltx_text" style="font-size:70%;">RESDDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.14.13.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S1.T1.1.1.14.13.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.14.13.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.14.13.2.1" class="ltx_text" style="font-size:70%;">7</span></td>
<td id="S1.T1.1.1.14.13.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.14.13.3.1" class="ltx_text" style="font-size:70%;">200</span></td>
<td id="S1.T1.1.1.14.13.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.14.13.4.1" class="ltx_text" style="font-size:70%;">Russian</span></td>
<td id="S1.T1.1.1.14.13.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.14.13.5.1" class="ltx_text" style="font-size:70%;">1396</span></td>
<td id="S1.T1.1.1.14.13.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.14.13.6.1" class="ltx_text" style="font-size:70%;">6.0</span></td>
<td id="S1.T1.1.1.14.13.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.14.13.7.1" class="ltx_text" style="font-size:70%;">No</span></td>
</tr>
<tr id="S1.T1.1.1.15.14" class="ltx_tr">
<td id="S1.T1.1.1.15.14.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.15.14.1.1" class="ltx_text" style="font-size:70%;">MELDDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.15.14.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S1.T1.1.1.15.14.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.15.14.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.15.14.2.1" class="ltx_text" style="font-size:70%;">7</span></td>
<td id="S1.T1.1.1.15.14.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.15.14.3.1" class="ltx_text" style="font-size:70%;">407</span></td>
<td id="S1.T1.1.1.15.14.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.15.14.4.1" class="ltx_text" style="font-size:70%;">English</span></td>
<td id="S1.T1.1.1.15.14.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.15.14.5.1" class="ltx_text" style="font-size:70%;">12924</span></td>
<td id="S1.T1.1.1.15.14.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.15.14.6.1" class="ltx_text" style="font-size:70%;">3.2</span></td>
<td id="S1.T1.1.1.15.14.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.15.14.7.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
</tr>
<tr id="S1.T1.1.1.16.15" class="ltx_tr">
<td id="S1.T1.1.1.16.15.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.16.15.1.1" class="ltx_text" style="font-size:70%;">MEADDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.16.15.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S1.T1.1.1.16.15.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.16.15.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.16.15.2.1" class="ltx_text" style="font-size:70%;">7</span></td>
<td id="S1.T1.1.1.16.15.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.16.15.3.1" class="ltx_text" style="font-size:70%;">60</span></td>
<td id="S1.T1.1.1.16.15.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.16.15.4.1" class="ltx_text" style="font-size:70%;">English</span></td>
<td id="S1.T1.1.1.16.15.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.16.15.5.1" class="ltx_text" style="font-size:70%;">31724</span></td>
<td id="S1.T1.1.1.16.15.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.16.15.6.1" class="ltx_text" style="font-size:70%;">4.2</span></td>
<td id="S1.T1.1.1.16.15.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.16.15.7.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
</tr>
<tr id="S1.T1.1.1.17.16" class="ltx_tr">
<td id="S1.T1.1.1.17.16.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.17.16.1.1" class="ltx_text" style="font-size:70%;">CaFEDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.17.16.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S1.T1.1.1.17.16.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.17.16.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.17.16.2.1" class="ltx_text" style="font-size:70%;">7</span></td>
<td id="S1.T1.1.1.17.16.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.17.16.3.1" class="ltx_text" style="font-size:70%;">12</span></td>
<td id="S1.T1.1.1.17.16.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.17.16.4.1" class="ltx_text" style="font-size:70%;">French</span></td>
<td id="S1.T1.1.1.17.16.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.17.16.5.1" class="ltx_text" style="font-size:70%;">936</span></td>
<td id="S1.T1.1.1.17.16.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.17.16.6.1" class="ltx_text" style="font-size:70%;">4.4</span></td>
<td id="S1.T1.1.1.17.16.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.17.16.7.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
</tr>
<tr id="S1.T1.1.1.18.17" class="ltx_tr">
<td id="S1.T1.1.1.18.17.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.18.17.1.1" class="ltx_text" style="font-size:70%;">ExpressoDataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.18.17.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S1.T1.1.1.18.17.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.18.17.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.18.17.2.1" class="ltx_text" style="font-size:70%;">8</span></td>
<td id="S1.T1.1.1.18.17.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.18.17.3.1" class="ltx_text" style="font-size:70%;">4</span></td>
<td id="S1.T1.1.1.18.17.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.18.17.4.1" class="ltx_text" style="font-size:70%;">English</span></td>
<td id="S1.T1.1.1.18.17.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.18.17.5.1" class="ltx_text" style="font-size:70%;">11954</span></td>
<td id="S1.T1.1.1.18.17.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.18.17.6.1" class="ltx_text" style="font-size:70%;">4.2</span></td>
<td id="S1.T1.1.1.18.17.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.18.17.7.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
</tr>
<tr id="S1.T1.1.1.19.18" class="ltx_tr">
<td id="S1.T1.1.1.19.18.1" class="ltx_td ltx_align_left">
<span id="S1.T1.1.1.19.18.1.1" class="ltx_text" style="font-size:70%;">ShEMODataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.19.18.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S1.T1.1.1.19.18.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.19.18.2" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.19.18.2.1" class="ltx_text" style="font-size:70%;">6</span></td>
<td id="S1.T1.1.1.19.18.3" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.19.18.3.1" class="ltx_text" style="font-size:70%;">87</span></td>
<td id="S1.T1.1.1.19.18.4" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.19.18.4.1" class="ltx_text" style="font-size:70%;">Persian</span></td>
<td id="S1.T1.1.1.19.18.5" class="ltx_td ltx_align_right"><span id="S1.T1.1.1.19.18.5.1" class="ltx_text" style="font-size:70%;">3000</span></td>
<td id="S1.T1.1.1.19.18.6" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.19.18.6.1" class="ltx_text" style="font-size:70%;">4.1</span></td>
<td id="S1.T1.1.1.19.18.7" class="ltx_td ltx_align_left"><span id="S1.T1.1.1.19.18.7.1" class="ltx_text" style="font-size:70%;">No</span></td>
</tr>
<tr id="S1.T1.1.1.20.19" class="ltx_tr">
<td id="S1.T1.1.1.20.19.1" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S1.T1.1.1.20.19.1.1" class="ltx_text" style="font-size:70%;">SUBESCODataset</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.1.1.20.19.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S1.T1.1.1.20.19.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S1.T1.1.1.20.19.2" class="ltx_td ltx_align_right ltx_border_bb"><span id="S1.T1.1.1.20.19.2.1" class="ltx_text" style="font-size:70%;">7</span></td>
<td id="S1.T1.1.1.20.19.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="S1.T1.1.1.20.19.3.1" class="ltx_text" style="font-size:70%;">20</span></td>
<td id="S1.T1.1.1.20.19.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S1.T1.1.1.20.19.4.1" class="ltx_text" style="font-size:70%;">Bangla</span></td>
<td id="S1.T1.1.1.20.19.5" class="ltx_td ltx_align_right ltx_border_bb"><span id="S1.T1.1.1.20.19.5.1" class="ltx_text" style="font-size:70%;">7000</span></td>
<td id="S1.T1.1.1.20.19.6" class="ltx_td ltx_align_left ltx_border_bb"><span id="S1.T1.1.1.20.19.6.1" class="ltx_text" style="font-size:70%;">4.0</span></td>
<td id="S1.T1.1.1.20.19.7" class="ltx_td ltx_align_left ltx_border_bb"><span id="S1.T1.1.1.20.19.7.1" class="ltx_text" style="font-size:70%;">Yes</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Self-supervised learning has revolutionized speech representation learning, enabling models to capture rich acoustic features without relying on labeled data. Models like wav2vec 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> have achieved state-of-the-art performance on various speech processing tasks, including speech recognition, speaker identification, and emotion recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Cross-lingual SER has gained attention as a means to develop models that can generalize across languages. Several studies have explored the use of SSL models for cross-lingual SER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. However, these works often focus on a limited set of languages and datasets, making it difficult to assess the true generalization capabilities of the models.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Existing well-known SER benchmarks, such as IEMOCAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and MSP-Podcast <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, have played a crucial role in advancing the field. However, these benchmarks often emphasize in-domain evaluation and may not adequately capture the challenges of real-world deployment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Our work aims to address these limitations by introducing a large-scale benchmark that focuses on out-of-domain generalization and includes a diverse set of multilingual datasets.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Recent works such as EMO-SUPERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and SERAB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> have made notable contributions to the field of Speech Emotion Recognition (SER). However, these works have limitations in terms of the diversity of languages, datasets, and the emphasis on out-of-domain generalization.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Our benchmark significantly advances the state-of-the-art in SER evaluation by addressing these limitations. We curate an extensive collection of multilingual datasets, carefully selected to cover diverse linguistic and cultural contexts, ensuring a thorough evaluation of SER models in real-world scenarios. Moreover, our benchmark places a strong emphasis on out-of-domain generalization, a crucial aspect that has been largely overlooked in previous works. We evaluate SER models in both in-domain and out-of-domain settings, providing valuable insights into their ability to adapt to unseen data distributions. This focus on generalizability is essential for developing SER models that can be effectively deployed in real-world applications, where the variability in speech patterns, emotions, and recording conditions is vast.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The primary objectives of this section are to detail the dataset selection and preprocessing steps, introduce the backbone models employed, describe the model architecture and training process, explain the logit adjustment technique, and outline the evaluation protocol. The methodology is designed to ensure a comprehensive and fair evaluation of state-of-the-art SER models across diverse languages and emotional expressions.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2408.07851/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="492" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of our benchmark's methodology.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset Selection and Preprocessing</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We curate a diverse collection of multilingual datasets for our benchmark, covering various languages and emotional expressions. Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides an overview of the datasets used in our evaluation. We focus on less commonly used datasets to mitigate overfitting and encourage the development of more robust models.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The datasets are preprocessed to ensure consistency and compatibility with our evaluation protocol. We set the maximum audio length to 30 seconds, and process the audios with appropriately for each backbone model we test (detailed in the next section). We rely on the Huggingface library for model preprocessing and inference implementations. Additionally, we remap the label space by mapping the original emotion labels to a unified eight-class space, facilitating cross-dataset comparisons. Due to complexity, detailing the exact remapping for each dataset is relegated to the open-source code.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The datasets used for out-of-domain evaluations are matched by having the same classes (excluding 'other') and their eligibility is indicated in the 'OOD Eligible' column of Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. These datasets were found to have the same exact classes after the class mapping, making them eligible for out-of-domain testing. When calculating out-of-domain metrics, samples with the 'other' label were discarded, and models were banned from predicting the 'other' class.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Backbone Models</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We employ state-of-the-art speech representation models as backbones for our benchmark, as listed in Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Backbone Models ‣ 3 Methodology ‣ SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. These models are selected based on their strong performance on various speech processing tasks and their ability to capture rich acoustic features.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Backbone models used in our benchmark. All checkpoints are from Huggingface.</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:149.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-44.6pt,15.4pt) scale(0.829276037920624,0.829276037920624) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Checkpoint name</th>
<th id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Training Dataset Hours</th>
<th id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"># Params</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.2.1" class="ltx_tr">
<td id="S3.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">facebook/w2v-bert-2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S3.T2.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">4500k</td>
<td id="S3.T2.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">580M</td>
</tr>
<tr id="S3.T2.1.1.3.2" class="ltx_tr">
<td id="S3.T2.1.1.3.2.1" class="ltx_td ltx_align_left">facebook/hubert-large-ll60k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S3.T2.1.1.3.2.2" class="ltx_td ltx_align_left">60k</td>
<td id="S3.T2.1.1.3.2.3" class="ltx_td ltx_align_left">315M</td>
</tr>
<tr id="S3.T2.1.1.4.3" class="ltx_tr">
<td id="S3.T2.1.1.4.3.1" class="ltx_td ltx_align_left">microsoft/wavlm-large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S3.T2.1.1.4.3.2" class="ltx_td ltx_align_left">94k</td>
<td id="S3.T2.1.1.4.3.3" class="ltx_td ltx_align_left">315M</td>
</tr>
<tr id="S3.T2.1.1.5.4" class="ltx_tr">
<td id="S3.T2.1.1.5.4.1" class="ltx_td ltx_align_left">laion/larger_clap_music_and_speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S3.T2.1.1.5.4.2" class="ltx_td ltx_align_left">&gt;10k</td>
<td id="S3.T2.1.1.5.4.3" class="ltx_td ltx_align_left">193M</td>
</tr>
<tr id="S3.T2.1.1.6.5" class="ltx_tr">
<td id="S3.T2.1.1.6.5.1" class="ltx_td ltx_align_left">m-a-p/MERT-v1-330M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S3.T2.1.1.6.5.2" class="ltx_td ltx_align_left">160k</td>
<td id="S3.T2.1.1.6.5.3" class="ltx_td ltx_align_left">315M</td>
</tr>
<tr id="S3.T2.1.1.7.6" class="ltx_tr">
<td id="S3.T2.1.1.7.6.1" class="ltx_td ltx_align_left">openai/whisper-medium <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S3.T2.1.1.7.6.2" class="ltx_td ltx_align_left">680k</td>
<td id="S3.T2.1.1.7.6.3" class="ltx_td ltx_align_left">307M</td>
</tr>
<tr id="S3.T2.1.1.8.7" class="ltx_tr">
<td id="S3.T2.1.1.8.7.1" class="ltx_td ltx_align_left">openai/whisper-large-v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S3.T2.1.1.8.7.2" class="ltx_td ltx_align_left">680k</td>
<td id="S3.T2.1.1.8.7.3" class="ltx_td ltx_align_left">636M</td>
</tr>
<tr id="S3.T2.1.1.9.8" class="ltx_tr">
<td id="S3.T2.1.1.9.8.1" class="ltx_td ltx_align_left">openai/whisper-large-v3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S3.T2.1.1.9.8.2" class="ltx_td ltx_align_left">5000k</td>
<td id="S3.T2.1.1.9.8.3" class="ltx_td ltx_align_left">636M</td>
</tr>
<tr id="S3.T2.1.1.10.9" class="ltx_tr">
<td id="S3.T2.1.1.10.9.1" class="ltx_td ltx_align_left ltx_border_bb">openai/whisper-large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S3.T2.1.1.10.9.2" class="ltx_td ltx_align_left ltx_border_bb">680k</td>
<td id="S3.T2.1.1.10.9.3" class="ltx_td ltx_align_left ltx_border_bb">636M</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In addition to the SSL models, we also evaluate MERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, a music recognition model, and CLAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, a contrastive learning model. Including these models allows us to assess the effectiveness of different learning paradigms for cross-lingual SER. Lastly, we evaluate the Whisper<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> encoder which is trained under an encoder-decoder setup for ASR.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model Architecture and Training</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We employ a simple multilayer perceptron (MLP) architecture with approximately 500K parameters for emotion classification. The MLP consists of two hidden layers and is trained for 100 epochs. Due to the small parameter size and shallow depth, we do not expect substantial overfitting. We apply label smoothing with a factor of 0.1 to improve generalization.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Instead of the typical approach of averaging the features before classification, we execute the MLP on every feature frame and then take the mean of the predictions. We find that this approach preserves more information and leads to stronger and more consistent results.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Logit Adjustment</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">To account for the varying class distributions across datasets, we employ logit adjustment during evaluation. This technique adjusts the model's output logits based on the difference between the training and testing dataset distributions, mitigating the impact of class imbalance and enabling fair comparisons.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Evaluation Protocol</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Methodology ‣ SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides an overview of our benchmark's methodology. As we described in Subsection <a href="#S3.SS1" title="3.1 Dataset Selection and Preprocessing ‣ 3 Methodology ‣ SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, we establish a subset of our datasets as OOD eligible, which have the same exact classes after the class mapping. Effectively, all datasets are accounted in in-domain tests. Only OOD-eligible datasets are accounted for our out of domain metrics.
</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">For each model, we construct a performance matrix where the rows represent the training datasets and the columns represent the evaluation datasets. When the training and evaluation datasets are the same (diagonal elements), it indicates in-domain performance. Off-diagonal elements correspond to out-of-domain zero-shot performance.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">We assess the quality of the backbone models based on three key metrics:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">In-domain separability: We compute the mean of the diagonal elements to measure how well the features learned by a model can separate emotions within a dataset.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Out-of-domain performance given training dataset: We calculate the mean of each row, excluding the diagonal element, to evaluate the model's ability to generalize to unseen datasets when trained on a specific dataset.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Average performance on unseen datasets: We compute the mean of each column, excluding the diagonal element, to assess the average performance on a dataset when the model is not trained on it.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p">All metrics are reported in terms of macro-averaged F1 score to account for class imbalance.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The results of our benchmark provide valuable insights into the performance and generalization capabilities of state-of-the-art SER models across diverse languages and emotional expressions.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>In-domain separability</h3>

<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Summary of key performance metrics for the evaluated models.</figcaption>
<div id="S4.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:229.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(29.9pt,-15.8pt) scale(1.15979848578241,1.15979848578241) ;">
<table id="S4.T3.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.3.1.1.1" class="ltx_tr">
<th id="S4.T3.3.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T3.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T3.3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">In-Domain (ID) Performance</span></th>
<th id="S4.T3.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T3.3.1.1.1.3.1" class="ltx_text" style="font-size:70%;">Out-of-Domain (OOD) Performance</span></th>
<td id="S4.T3.3.1.1.1.4" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S4.T3.3.1.2.2" class="ltx_tr">
<th id="S4.T3.3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S4.T3.3.1.2.2.1.1" class="ltx_text" style="font-size:70%;">Model</span></th>
<th id="S4.T3.3.1.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column"><span id="S4.T3.3.1.2.2.2.1" class="ltx_text" style="font-size:70%;">Average</span></th>
<th id="S4.T3.3.1.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column"><span id="S4.T3.3.1.2.2.3.1" class="ltx_text" style="font-size:70%;">Standard Deviation</span></th>
<th id="S4.T3.3.1.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_column"><span id="S4.T3.3.1.2.2.4.1" class="ltx_text" style="font-size:70%;">Average</span></th>
<th id="S4.T3.3.1.2.2.5" class="ltx_td ltx_align_right ltx_th ltx_th_column"><span id="S4.T3.3.1.2.2.5.1" class="ltx_text" style="font-size:70%;">Standard Deviation</span></th>
<th id="S4.T3.3.1.2.2.6" class="ltx_td ltx_align_right ltx_th ltx_th_column"><span id="S4.T3.3.1.2.2.6.1" class="ltx_text" style="font-size:70%;">Weighted Average</span></th>
</tr>
<tr id="S4.T3.3.1.3.3" class="ltx_tr">
<th id="S4.T3.3.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.3.1.3.3.1.1" class="ltx_text" style="font-size:70%;">Whisper-Large-v2</span></th>
<td id="S4.T3.3.1.3.3.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T3.3.1.3.3.2.1" class="ltx_text" style="font-size:70%;">0.781942</span></td>
<td id="S4.T3.3.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T3.3.1.3.3.3.1" class="ltx_text" style="font-size:70%;">0.194716</span></td>
<td id="S4.T3.3.1.3.3.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T3.3.1.3.3.4.1" class="ltx_text" style="font-size:70%;">0.194250</span></td>
<td id="S4.T3.3.1.3.3.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T3.3.1.3.3.5.1" class="ltx_text" style="font-size:70%;">0.089993</span></td>
<td id="S4.T3.3.1.3.3.6" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T3.3.1.3.3.6.1" class="ltx_text" style="font-size:70%;">0.345741</span></td>
</tr>
<tr id="S4.T3.3.1.4.4" class="ltx_tr">
<th id="S4.T3.3.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.1.4.4.1.1" class="ltx_text" style="font-size:70%;">Whisper-Large</span></th>
<td id="S4.T3.3.1.4.4.2" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.4.4.2.1" class="ltx_text" style="font-size:70%;">0.781314</span></td>
<td id="S4.T3.3.1.4.4.3" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.4.4.3.1" class="ltx_text" style="font-size:70%;">0.203542</span></td>
<td id="S4.T3.3.1.4.4.4" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.4.4.4.1" class="ltx_text" style="font-size:70%;">0.197882</span></td>
<td id="S4.T3.3.1.4.4.5" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.4.4.5.1" class="ltx_text" style="font-size:70%;">0.085657</span></td>
<td id="S4.T3.3.1.4.4.6" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.4.4.6.1" class="ltx_text" style="font-size:70%;">0.344999</span></td>
</tr>
<tr id="S4.T3.3.1.5.5" class="ltx_tr">
<th id="S4.T3.3.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.1.5.5.1.1" class="ltx_text" style="font-size:70%;">Whisper-Large-v3</span></th>
<td id="S4.T3.3.1.5.5.2" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.5.5.2.1" class="ltx_text" style="font-size:70%;">0.776689</span></td>
<td id="S4.T3.3.1.5.5.3" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.5.5.3.1" class="ltx_text" style="font-size:70%;">0.201399</span></td>
<td id="S4.T3.3.1.5.5.4" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.5.5.4.1" class="ltx_text" style="font-size:70%;">0.192961</span></td>
<td id="S4.T3.3.1.5.5.5" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.5.5.5.1" class="ltx_text" style="font-size:70%;">0.083822</span></td>
<td id="S4.T3.3.1.5.5.6" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.5.5.6.1" class="ltx_text" style="font-size:70%;">0.342214</span></td>
</tr>
<tr id="S4.T3.3.1.6.6" class="ltx_tr">
<th id="S4.T3.3.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.1.6.6.1.1" class="ltx_text" style="font-size:70%;">Whisper-medium</span></th>
<td id="S4.T3.3.1.6.6.2" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.6.6.2.1" class="ltx_text" style="font-size:70%;">0.756563</span></td>
<td id="S4.T3.3.1.6.6.3" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.6.6.3.1" class="ltx_text" style="font-size:70%;">0.200798</span></td>
<td id="S4.T3.3.1.6.6.4" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.6.6.4.1" class="ltx_text" style="font-size:70%;">0.196831</span></td>
<td id="S4.T3.3.1.6.6.5" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.6.6.5.1" class="ltx_text" style="font-size:70%;">0.087710</span></td>
<td id="S4.T3.3.1.6.6.6" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.6.6.6.1" class="ltx_text" style="font-size:70%;">0.332443</span></td>
</tr>
<tr id="S4.T3.3.1.7.7" class="ltx_tr">
<th id="S4.T3.3.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.1.7.7.1.1" class="ltx_text" style="font-size:70%;">WavLM-Large</span></th>
<td id="S4.T3.3.1.7.7.2" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.7.7.2.1" class="ltx_text" style="font-size:70%;">0.765474</span></td>
<td id="S4.T3.3.1.7.7.3" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.7.7.3.1" class="ltx_text" style="font-size:70%;">0.206174</span></td>
<td id="S4.T3.3.1.7.7.4" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.7.7.4.1" class="ltx_text" style="font-size:70%;">0.161098</span></td>
<td id="S4.T3.3.1.7.7.5" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.7.7.5.1" class="ltx_text" style="font-size:70%;">0.068182</span></td>
<td id="S4.T3.3.1.7.7.6" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.7.7.6.1" class="ltx_text" style="font-size:70%;">0.326108</span></td>
</tr>
<tr id="S4.T3.3.1.8.8" class="ltx_tr">
<th id="S4.T3.3.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.1.8.8.1.1" class="ltx_text" style="font-size:70%;">CLAP Music &amp; Speech</span></th>
<td id="S4.T3.3.1.8.8.2" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.8.8.2.1" class="ltx_text" style="font-size:70%;">0.743248</span></td>
<td id="S4.T3.3.1.8.8.3" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.8.8.3.1" class="ltx_text" style="font-size:70%;">0.215404</span></td>
<td id="S4.T3.3.1.8.8.4" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.8.8.4.1" class="ltx_text" style="font-size:70%;">0.148090</span></td>
<td id="S4.T3.3.1.8.8.5" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.8.8.5.1" class="ltx_text" style="font-size:70%;">0.055976</span></td>
<td id="S4.T3.3.1.8.8.6" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.8.8.6.1" class="ltx_text" style="font-size:70%;">0.309980</span></td>
</tr>
<tr id="S4.T3.3.1.9.9" class="ltx_tr">
<th id="S4.T3.3.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.1.9.9.1.1" class="ltx_text" style="font-size:70%;">Hubert Large</span></th>
<td id="S4.T3.3.1.9.9.2" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.9.9.2.1" class="ltx_text" style="font-size:70%;">0.733036</span></td>
<td id="S4.T3.3.1.9.9.3" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.9.9.3.1" class="ltx_text" style="font-size:70%;">0.207492</span></td>
<td id="S4.T3.3.1.9.9.4" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.9.9.4.1" class="ltx_text" style="font-size:70%;">0.156504</span></td>
<td id="S4.T3.3.1.9.9.5" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.9.9.5.1" class="ltx_text" style="font-size:70%;">0.066509</span></td>
<td id="S4.T3.3.1.9.9.6" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.9.9.6.1" class="ltx_text" style="font-size:70%;">0.307769</span></td>
</tr>
<tr id="S4.T3.3.1.10.10" class="ltx_tr">
<th id="S4.T3.3.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.1.10.10.1.1" class="ltx_text" style="font-size:70%;">MERT v1 330M</span></th>
<td id="S4.T3.3.1.10.10.2" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.10.10.2.1" class="ltx_text" style="font-size:70%;">0.707891</span></td>
<td id="S4.T3.3.1.10.10.3" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.10.10.3.1" class="ltx_text" style="font-size:70%;">0.211471</span></td>
<td id="S4.T3.3.1.10.10.4" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.10.10.4.1" class="ltx_text" style="font-size:70%;">0.127485</span></td>
<td id="S4.T3.3.1.10.10.5" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.10.10.5.1" class="ltx_text" style="font-size:70%;">0.049841</span></td>
<td id="S4.T3.3.1.10.10.6" class="ltx_td ltx_align_right"><span id="S4.T3.3.1.10.10.6.1" class="ltx_text" style="font-size:70%;">0.287032</span></td>
</tr>
<tr id="S4.T3.3.1.11.11" class="ltx_tr">
<th id="S4.T3.3.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T3.3.1.11.11.1.1" class="ltx_text" style="font-size:70%;">w2v-bert-2.0</span></th>
<td id="S4.T3.3.1.11.11.2" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T3.3.1.11.11.2.1" class="ltx_text" style="font-size:70%;">0.668253</span></td>
<td id="S4.T3.3.1.11.11.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T3.3.1.11.11.3.1" class="ltx_text" style="font-size:70%;">0.211685</span></td>
<td id="S4.T3.3.1.11.11.4" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T3.3.1.11.11.4.1" class="ltx_text" style="font-size:70%;">0.141581</span></td>
<td id="S4.T3.3.1.11.11.5" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T3.3.1.11.11.5.1" class="ltx_text" style="font-size:70%;">0.061820</span></td>
<td id="S4.T3.3.1.11.11.6" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T3.3.1.11.11.6.1" class="ltx_text" style="font-size:70%;">0.268165</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The second and third column in Table <a href="#S4.T3" title="Table 3 ‣ 4.1 In-domain separability ‣ 4 Results and Discussion ‣ SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> present the in-domain separability performance of various models, focusing on their ability to distinguish between different emotional states in speech. Performance is quantified by two metrics: the mean average performance across datasets (Mean) and the variability of performance across these datasets (Standard Deviation). From the table, Whisper-Large-v2 leads the evaluated models in in-domain SER performance, with the highest mean accuracy and low variability across datasets, closely followed by the original Whisper-Large. Other models like Whisper-Large-v3, Whisper-medium, WavLM-Large, and CLAP Music &amp; Speech show competent but slightly more variable performances. Conversely, Hubert Large, MERT v1 330M, and w2v-bert-2.0 exhibit the lowest accuracies with higher fluctuations in their effectiveness across different datasets, indicating potential limitations in generalization capabilities for speech emotion contexts.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The outcome of this evaluation highlights a clear hierarchy among the models in terms of both accuracy and consistency in emotion recognition within the same domain. Whisper-Large variants stand out as the most effective, with their newer versions, particularly Whisper-Large-v2, slightly improving upon the original's already high benchmark. Lower-ranked models, though less consistent and accurate overall, may still offer valuable insights or perform well in specific niches or datasets. This analysis underscores the importance of choosing the right model for specific SER applications, balancing between performance and consistency across diverse emotional speech datasets.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2408.07851/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="461" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Average out-of-domain performance given the training dataset.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Out-of-domain performance given training dataset</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1 In-domain separability ‣ 4 Results and Discussion ‣ SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the average out-of-domain performance for each model, obtained by row-wise reduction of the performance matrix. The Whisper models demonstrate the highest out-of-domain performance, indicating their superior generalization capabilities compared to the SSL models. However, there is high variability in OOD performance across training sets. Training on some datasets like BAUM leads to much better OOD generalization than others like MELD. This warrants further investigation into what properties of datasets lead to more generalizable models. The strong performance of Whisper challenges the common belief that ASR models are suboptimal for SER and highlights the potential of leveraging ASR models for emotion recognition tasks.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2408.07851/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="461" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Average performance on individual datasets when not trained on them.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Average performance on unseen datasets</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Out-of-domain performance given training dataset ‣ 4 Results and Discussion ‣ SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the average performance of the evaluated models on each dataset when the models are not trained on that dataset. The results highlight the varying levels of difficulty across datasets, with some datasets posing greater challenges for out-of-domain generalization. Notably, EMOVO, MELD, and MEAD are the most challenging for models not trained on them, suggesting they have unique characteristics that are harder to learn indirectly. On the other hand, models generalize best to URDU and AESDD, indicating these datasets share more common features with others. Interestingly, the Whisper model consistently achieves strong performance across most datasets, surpassing the SSL models in many cases.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>General outcomes</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.2" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.1 In-domain separability ‣ 4 Results and Discussion ‣ SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides a summary of the key performance metrics for the evaluated models. The second and third columns show the average and standard deviations of the in-domain results, while the next two columns show the out-of-domain performance. The weighted average column is calculated as follows:</p>
<table id="S4.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex1.m1.8" class="ltx_Math" alttext="\scriptsize\begin{split}\text{Weighted Average}&amp;=\frac{\text{Average OOD}+\text{Average ID}}{2}-\lambda_{factor}\\
&amp;\times\frac{\text{Std. Dev. OOD}+\text{Std. Dev. ID}}{2}\end{split}" display="block"><semantics id="S4.Ex1.m1.8a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="S4.Ex1.m1.8.8" xref="S4.Ex1.m1.8.9.1.cmml"><mtr id="S4.Ex1.m1.8.8a" xref="S4.Ex1.m1.8.9.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S4.Ex1.m1.8.8b" xref="S4.Ex1.m1.8.9.1.cmml"><mtext mathsize="70%" id="S4.Ex1.m1.1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.1a.cmml">Weighted Average</mtext></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.Ex1.m1.8.8c" xref="S4.Ex1.m1.8.9.1.cmml"><mrow id="S4.Ex1.m1.6.6.6.6.5" xref="S4.Ex1.m1.8.9.1.cmml"><mi id="S4.Ex1.m1.6.6.6.6.5.6" xref="S4.Ex1.m1.8.9.1.cmml"></mi><mo mathsize="70%" id="S4.Ex1.m1.2.2.2.2.1.1" xref="S4.Ex1.m1.2.2.2.2.1.1.cmml">=</mo><mrow id="S4.Ex1.m1.6.6.6.6.5.7" xref="S4.Ex1.m1.8.9.1.cmml"><mfrac id="S4.Ex1.m1.3.3.3.3.2.2" xref="S4.Ex1.m1.3.3.3.3.2.2.cmml"><mrow id="S4.Ex1.m1.3.3.3.3.2.2.2" xref="S4.Ex1.m1.3.3.3.3.2.2.2.cmml"><mtext mathsize="70%" id="S4.Ex1.m1.3.3.3.3.2.2.2.2" xref="S4.Ex1.m1.3.3.3.3.2.2.2.2a.cmml">Average OOD</mtext><mo mathsize="70%" id="S4.Ex1.m1.3.3.3.3.2.2.2.1" xref="S4.Ex1.m1.3.3.3.3.2.2.2.1.cmml">+</mo><mtext mathsize="70%" id="S4.Ex1.m1.3.3.3.3.2.2.2.3" xref="S4.Ex1.m1.3.3.3.3.2.2.2.3a.cmml">Average ID</mtext></mrow><mn mathsize="70%" id="S4.Ex1.m1.3.3.3.3.2.2.3" xref="S4.Ex1.m1.3.3.3.3.2.2.3.cmml">2</mn></mfrac><mo mathsize="70%" id="S4.Ex1.m1.4.4.4.4.3.3" xref="S4.Ex1.m1.4.4.4.4.3.3.cmml">−</mo><msub id="S4.Ex1.m1.6.6.6.6.5.7.1" xref="S4.Ex1.m1.8.9.1.cmml"><mi mathsize="70%" id="S4.Ex1.m1.5.5.5.5.4.4" xref="S4.Ex1.m1.5.5.5.5.4.4.cmml">λ</mi><mrow id="S4.Ex1.m1.6.6.6.6.5.5.1" xref="S4.Ex1.m1.6.6.6.6.5.5.1.cmml"><mi mathsize="70%" id="S4.Ex1.m1.6.6.6.6.5.5.1.2" xref="S4.Ex1.m1.6.6.6.6.5.5.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.6.6.6.6.5.5.1.1" xref="S4.Ex1.m1.6.6.6.6.5.5.1.1.cmml">​</mo><mi mathsize="70%" id="S4.Ex1.m1.6.6.6.6.5.5.1.3" xref="S4.Ex1.m1.6.6.6.6.5.5.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.6.6.6.6.5.5.1.1a" xref="S4.Ex1.m1.6.6.6.6.5.5.1.1.cmml">​</mo><mi mathsize="70%" id="S4.Ex1.m1.6.6.6.6.5.5.1.4" xref="S4.Ex1.m1.6.6.6.6.5.5.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.6.6.6.6.5.5.1.1b" xref="S4.Ex1.m1.6.6.6.6.5.5.1.1.cmml">​</mo><mi mathsize="70%" id="S4.Ex1.m1.6.6.6.6.5.5.1.5" xref="S4.Ex1.m1.6.6.6.6.5.5.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.6.6.6.6.5.5.1.1c" xref="S4.Ex1.m1.6.6.6.6.5.5.1.1.cmml">​</mo><mi mathsize="70%" id="S4.Ex1.m1.6.6.6.6.5.5.1.6" xref="S4.Ex1.m1.6.6.6.6.5.5.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.Ex1.m1.6.6.6.6.5.5.1.1d" xref="S4.Ex1.m1.6.6.6.6.5.5.1.1.cmml">​</mo><mi mathsize="70%" id="S4.Ex1.m1.6.6.6.6.5.5.1.7" xref="S4.Ex1.m1.6.6.6.6.5.5.1.7.cmml">r</mi></mrow></msub></mrow></mrow></mtd></mtr><mtr id="S4.Ex1.m1.8.8d" xref="S4.Ex1.m1.8.9.1.cmml"><mtd id="S4.Ex1.m1.8.8e" xref="S4.Ex1.m1.8.9.1.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.Ex1.m1.8.8f" xref="S4.Ex1.m1.8.9.1.cmml"><mrow id="S4.Ex1.m1.8.8.8.2.2" xref="S4.Ex1.m1.8.9.1.cmml"><mi id="S4.Ex1.m1.8.8.8.2.2.3" xref="S4.Ex1.m1.8.9.1.cmml"></mi><mo lspace="0.222em" mathsize="70%" rspace="0.222em" id="S4.Ex1.m1.7.7.7.1.1.1" xref="S4.Ex1.m1.7.7.7.1.1.1.cmml">×</mo><mfrac id="S4.Ex1.m1.8.8.8.2.2.2" xref="S4.Ex1.m1.8.8.8.2.2.2.cmml"><mrow id="S4.Ex1.m1.8.8.8.2.2.2.2" xref="S4.Ex1.m1.8.8.8.2.2.2.2.cmml"><mtext mathsize="70%" id="S4.Ex1.m1.8.8.8.2.2.2.2.2" xref="S4.Ex1.m1.8.8.8.2.2.2.2.2a.cmml">Std. Dev. OOD</mtext><mo mathsize="70%" id="S4.Ex1.m1.8.8.8.2.2.2.2.1" xref="S4.Ex1.m1.8.8.8.2.2.2.2.1.cmml">+</mo><mtext mathsize="70%" id="S4.Ex1.m1.8.8.8.2.2.2.2.3" xref="S4.Ex1.m1.8.8.8.2.2.2.2.3a.cmml">Std. Dev. ID</mtext></mrow><mn mathsize="70%" id="S4.Ex1.m1.8.8.8.2.2.2.3" xref="S4.Ex1.m1.8.8.8.2.2.2.3.cmml">2</mn></mfrac></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.8b"><apply id="S4.Ex1.m1.8.9.1.cmml" xref="S4.Ex1.m1.8.8"><eq id="S4.Ex1.m1.2.2.2.2.1.1.cmml" xref="S4.Ex1.m1.2.2.2.2.1.1"></eq><ci id="S4.Ex1.m1.1.1.1.1.1.1a.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1"><mtext mathsize="70%" id="S4.Ex1.m1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1">Weighted Average</mtext></ci><apply id="S4.Ex1.m1.8.9.1.3.cmml" xref="S4.Ex1.m1.8.8"><minus id="S4.Ex1.m1.4.4.4.4.3.3.cmml" xref="S4.Ex1.m1.4.4.4.4.3.3"></minus><apply id="S4.Ex1.m1.3.3.3.3.2.2.cmml" xref="S4.Ex1.m1.3.3.3.3.2.2"><divide id="S4.Ex1.m1.3.3.3.3.2.2.1.cmml" xref="S4.Ex1.m1.3.3.3.3.2.2"></divide><apply id="S4.Ex1.m1.3.3.3.3.2.2.2.cmml" xref="S4.Ex1.m1.3.3.3.3.2.2.2"><plus id="S4.Ex1.m1.3.3.3.3.2.2.2.1.cmml" xref="S4.Ex1.m1.3.3.3.3.2.2.2.1"></plus><ci id="S4.Ex1.m1.3.3.3.3.2.2.2.2a.cmml" xref="S4.Ex1.m1.3.3.3.3.2.2.2.2"><mtext mathsize="70%" id="S4.Ex1.m1.3.3.3.3.2.2.2.2.cmml" xref="S4.Ex1.m1.3.3.3.3.2.2.2.2">Average OOD</mtext></ci><ci id="S4.Ex1.m1.3.3.3.3.2.2.2.3a.cmml" xref="S4.Ex1.m1.3.3.3.3.2.2.2.3"><mtext mathsize="70%" id="S4.Ex1.m1.3.3.3.3.2.2.2.3.cmml" xref="S4.Ex1.m1.3.3.3.3.2.2.2.3">Average ID</mtext></ci></apply><cn type="integer" id="S4.Ex1.m1.3.3.3.3.2.2.3.cmml" xref="S4.Ex1.m1.3.3.3.3.2.2.3">2</cn></apply><apply id="S4.Ex1.m1.8.9.1.3.3.cmml" xref="S4.Ex1.m1.8.8"><times id="S4.Ex1.m1.7.7.7.1.1.1.cmml" xref="S4.Ex1.m1.7.7.7.1.1.1"></times><apply id="S4.Ex1.m1.8.9.1.3.3.2.cmml" xref="S4.Ex1.m1.8.8"><csymbol cd="ambiguous" id="S4.Ex1.m1.8.9.1.3.3.2.1.cmml" xref="S4.Ex1.m1.8.8">subscript</csymbol><ci id="S4.Ex1.m1.5.5.5.5.4.4.cmml" xref="S4.Ex1.m1.5.5.5.5.4.4">𝜆</ci><apply id="S4.Ex1.m1.6.6.6.6.5.5.1.cmml" xref="S4.Ex1.m1.6.6.6.6.5.5.1"><times id="S4.Ex1.m1.6.6.6.6.5.5.1.1.cmml" xref="S4.Ex1.m1.6.6.6.6.5.5.1.1"></times><ci id="S4.Ex1.m1.6.6.6.6.5.5.1.2.cmml" xref="S4.Ex1.m1.6.6.6.6.5.5.1.2">𝑓</ci><ci id="S4.Ex1.m1.6.6.6.6.5.5.1.3.cmml" xref="S4.Ex1.m1.6.6.6.6.5.5.1.3">𝑎</ci><ci id="S4.Ex1.m1.6.6.6.6.5.5.1.4.cmml" xref="S4.Ex1.m1.6.6.6.6.5.5.1.4">𝑐</ci><ci id="S4.Ex1.m1.6.6.6.6.5.5.1.5.cmml" xref="S4.Ex1.m1.6.6.6.6.5.5.1.5">𝑡</ci><ci id="S4.Ex1.m1.6.6.6.6.5.5.1.6.cmml" xref="S4.Ex1.m1.6.6.6.6.5.5.1.6">𝑜</ci><ci id="S4.Ex1.m1.6.6.6.6.5.5.1.7.cmml" xref="S4.Ex1.m1.6.6.6.6.5.5.1.7">𝑟</ci></apply></apply><apply id="S4.Ex1.m1.8.8.8.2.2.2.cmml" xref="S4.Ex1.m1.8.8.8.2.2.2"><divide id="S4.Ex1.m1.8.8.8.2.2.2.1.cmml" xref="S4.Ex1.m1.8.8.8.2.2.2"></divide><apply id="S4.Ex1.m1.8.8.8.2.2.2.2.cmml" xref="S4.Ex1.m1.8.8.8.2.2.2.2"><plus id="S4.Ex1.m1.8.8.8.2.2.2.2.1.cmml" xref="S4.Ex1.m1.8.8.8.2.2.2.2.1"></plus><ci id="S4.Ex1.m1.8.8.8.2.2.2.2.2a.cmml" xref="S4.Ex1.m1.8.8.8.2.2.2.2.2"><mtext mathsize="70%" id="S4.Ex1.m1.8.8.8.2.2.2.2.2.cmml" xref="S4.Ex1.m1.8.8.8.2.2.2.2.2">Std. Dev. OOD</mtext></ci><ci id="S4.Ex1.m1.8.8.8.2.2.2.2.3a.cmml" xref="S4.Ex1.m1.8.8.8.2.2.2.2.3"><mtext mathsize="70%" id="S4.Ex1.m1.8.8.8.2.2.2.2.3.cmml" xref="S4.Ex1.m1.8.8.8.2.2.2.2.3">Std. Dev. ID</mtext></ci></apply><cn type="integer" id="S4.Ex1.m1.8.8.8.2.2.2.3.cmml" xref="S4.Ex1.m1.8.8.8.2.2.2.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.8c">\scriptsize\begin{split}\text{Weighted Average}&amp;=\frac{\text{Average OOD}+\text{Average ID}}{2}-\lambda_{factor}\\
&amp;\times\frac{\text{Std. Dev. OOD}+\text{Std. Dev. ID}}{2}\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS4.p1.1" class="ltx_p">where <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="\lambda_{factor}" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><msub id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mi id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">λ</mi><mrow id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml"><mi id="S4.SS4.p1.1.m1.1.1.3.2" xref="S4.SS4.p1.1.m1.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.1.m1.1.1.3.1" xref="S4.SS4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p1.1.m1.1.1.3.3" xref="S4.SS4.p1.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.1.m1.1.1.3.1a" xref="S4.SS4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p1.1.m1.1.1.3.4" xref="S4.SS4.p1.1.m1.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.1.m1.1.1.3.1b" xref="S4.SS4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p1.1.m1.1.1.3.5" xref="S4.SS4.p1.1.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.1.m1.1.1.3.1c" xref="S4.SS4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p1.1.m1.1.1.3.6" xref="S4.SS4.p1.1.m1.1.1.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.1.m1.1.1.3.1d" xref="S4.SS4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS4.p1.1.m1.1.1.3.7" xref="S4.SS4.p1.1.m1.1.1.3.7.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">𝜆</ci><apply id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3"><times id="S4.SS4.p1.1.m1.1.1.3.1.cmml" xref="S4.SS4.p1.1.m1.1.1.3.1"></times><ci id="S4.SS4.p1.1.m1.1.1.3.2.cmml" xref="S4.SS4.p1.1.m1.1.1.3.2">𝑓</ci><ci id="S4.SS4.p1.1.m1.1.1.3.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3.3">𝑎</ci><ci id="S4.SS4.p1.1.m1.1.1.3.4.cmml" xref="S4.SS4.p1.1.m1.1.1.3.4">𝑐</ci><ci id="S4.SS4.p1.1.m1.1.1.3.5.cmml" xref="S4.SS4.p1.1.m1.1.1.3.5">𝑡</ci><ci id="S4.SS4.p1.1.m1.1.1.3.6.cmml" xref="S4.SS4.p1.1.m1.1.1.3.6">𝑜</ci><ci id="S4.SS4.p1.1.m1.1.1.3.7.cmml" xref="S4.SS4.p1.1.m1.1.1.3.7">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">\lambda_{factor}</annotation></semantics></math> is the discounting factor, which we set to 1.0.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">The Whisper models consistently achieve the highest scores across all metrics, further confirming their effectiveness in cross-lingual SER. However, the high standard deviations indicate that performance is quite variable depending on the specific train/test combination. This suggests that model robustness is still a challenge and there is room for improvement in developing models that perform consistently well across diverse datasets.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">Our benchmark also demonstrates the effectiveness of logit adjustment in addressing the challenges posed by varying class distributions across datasets. By incorporating this technique, we ensure fair comparisons and mitigate the impact of class imbalance on model performance.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we introduced a comprehensive benchmark for evaluating the robustness and generalization of speech emotion recognition models across diverse languages and emotional expressions. Our benchmark focuses on less commonly used datasets to mitigate overfitting and encourage the development of more robust models.
Through extensive experiments with state-of-the-art speech representation models, we found that the Whisper model, primarily designed for automatic speech recognition, outperforms dedicated SSL models in cross-lingual SER. This finding challenges the common belief that ASR models are suboptimal for SER and highlights the potential of leveraging ASR models for emotion recognition tasks.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our benchmark, along with the released code and evaluation protocol, serves as a valuable resource for the research community to assess and advance the state of cross-lingual SER. The insights gained from our work can guide future research efforts in developing more robust and generalizable SER models.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Works</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Future directions include exploring advanced techniques for domain adaptation, few-shot learning, and meta-learning to further improve the generalization capabilities of SER models. Additionally, investigating the specific characteristics of datasets that contribute to better generalization can provide valuable insights for dataset design and selection.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">We hope that our benchmark and findings will inspire researchers to push the boundaries of cross-lingual SER and develop models that can effectively handle the diversity of languages and emotional expressions encountered in real-world applications.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, ``wav2vec 2.0: A framework for self-supervised learning of speech representations,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 33, pp. 12 449–12 460, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, ``Hubert: Self-supervised speech representation learning by masked prediction of hidden units,'' <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 29, pp. 3451–3460, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Wavlm: Large-scale self-supervised pre-training for full stack speech processing,'' <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol. 16, no. 6, pp. 1505–1518, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
L. Pepino, P. Riera, and L. Ferrer, ``Emotion recognition from speech using wav2vec 2.0 embeddings,'' in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M. Osman, T. Nadeem, and G. Khoriba, ``Towards generalizable ser: Soft labeling and data augmentation for modeling temporal emotion shifts in large-scale multilingual speech,'' <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.08607</em>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, ``Robust speech recognition via large-scale weak supervision,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2023, pp. 28 492–28 518.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Baevski, A. Babu, W.-N. Hsu, and M. Auli, ``Efficient self-supervised learning with contextualized target representations for vision, speech and language,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2023, pp. 1416–1429.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, ``Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation,'' in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. K. Menon, S. Jayasumana, A. S. Rawat, H. Jain, A. Veit, and S. Kumar, ``Long-tail learning via logit adjustment,'' <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.07314</em>, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Latif, A. Qayyum, M. Usman, and J. Qadir, ``Cross lingual speech emotion recognition: Urdu vs. western languages,'' in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2018 International conference on frontiers of information technology (FIT)</em>.   IEEE, 2018, pp. 88–93.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, B. Weiss <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``A database of german emotional speech.'' in <em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">Interspeech</em>, vol. 5, 2005, pp. 1517–1520.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
G. Costantini, I. Iaderola, A. Paoloni, and M. Todisco, ``EMOVO corpus: an Italian emotional speech database,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proc. LREC</em>, 2014.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
O. Martin, I. Kotsia, B. Macq, and I. Pitas, ``The enterface'05 audio-visual emotion database,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">22nd international conference on data engineering workshops (ICDEW'06)</em>.   IEEE, 2006, pp. 8–8.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. M. Duville, L. M. Alonso-Valerdi, and D. I. Ibarra-Zarate, ``The mexican emotional speech database (mesd): elaboration and assessment based on machine learning,'' in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</em>.   IEEE, 2021, pp. 1644–1647.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
T. Wu, Y. Yang, Z. Wu, and D. Li, ``Masc: A speech corpus in mandarin for emotion analysis and affective speaker recognition,'' in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">2006 IEEE Odyssey-the speaker and language recognition workshop</em>.   IEEE, 2006, pp. 1–5.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
E. Parada-Cabaleiro, G. Costantini, A. Batliner, M. Schmitt, and B. W. Schuller, ``Demos: An italian emotional speech corpus: Elicitation methods, machine learning, and perception,'' <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Language Resources and Evaluation</em>, vol. 54, no. 2, pp. 341–383, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Tao, F. Liu, M. Zhang, and H. Jia, ``Design of speech corpus for Mandarin text to speech,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">The Blizzard Challenge Workshop</em>, 2008.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
N. Vryzas, R. Kotsakis, A. Liatsou, C. A. Dimoulas, and G. Kalliris, ``Speech emotion recognition for performance interaction,'' <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Journal of the Audio Engineering Society</em>, vol. 66, no. 6, pp. 457–467, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. Zhalehpour, O. Onder, Z. Akhtar, and C. E. Erdem, ``Baum-1: A spontaneous audio-visual face database of affective and mental states,'' <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>, vol. 8, no. 3, pp. 300–313, 2016.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
R. Altrov and H. Pajupuu, ``Estonian emotional speech corpus: culture and age in selecting corpus testers,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Human Language Technologies–The Baltic Perspective</em>.   IOS Press, 2010, pp. 25–32.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
T. Müller and D. Kreutz, ``Thorsten-voice dataset 2021.02,'' Sep. 2021, Please use it to make the world a better place for whole humankind. [Online]. Available: <a target="_blank" href="https://doi.org/10.5281/zenodo.5525342" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.5525342</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
I. Lubenets, N. Davidchuk, and A. Amentes, ``Aniemore.'' [Online]. Available: <a target="_blank" href="https://github.com/aniemore/Aniemore" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/aniemore/Aniemore</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihalcea, ``MELD: A multimodal multi-party dataset for emotion recognition in conversations,'' in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proc. ACL</em>, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
K. Wang, Q. Wu, L. Song, Z. Yang, W. Wu, C. Qian, R. He, Y. Qiao, and C. C. Loy, ``MEAD: A large-scale audio-visual dataset for emotional talking-face generation,'' in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proc. ECCV</em>, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
P. Gournay, O. Lahaie, and R. Lefebvre, ``A Canadian French emotional speech dataset,'' in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proc. ACM Multimedia</em>, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
T. A. Nguyen, W.-N. Hsu, A. d'Avirro, B. Shi, I. Gat, M. Fazel-Zarani, T. Remez, J. Copet, G. Synnaeve, M. Hassid <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Expresso: A benchmark and analysis of discrete expressive speech resynthesis,'' <em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.05725</em>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
O. Mohamad Nezami, P. Jamshid Lou, and M. Karami, ``ShEMO: a large-scale validated database for Persian speech emotion detection,'' in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proc. LREC</em>, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
S. Sultana, M. S. Rahman, M. R. Selim, and M. Z. Iqbal, ``SUST Bangla emotional speech corpus (SUBESCO): An audio-only emotional speech corpus for Bangla,'' in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. PloS One</em>, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S.-w. Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Superb: Speech processing universal performance benchmark,'' <em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.01051</em>, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M. Agarla, S. Bianco, L. Celona, P. Napoletano, A. Petrovsky, F. Piccoli, R. Schettini, and I. Shanin, ``Semi-supervised cross-lingual speech emotion recognition,'' <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, vol. 237, p. 121368, 2024.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, ``IEMOCAP: Interactive emotional dyadic motion capture database,'' in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proc. LREC</em>, 2008.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
R. Lotfian and C. Busso, ``Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings,'' <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>, vol. 10, pp. 471–483, 10 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
H. Wu, H.-C. Chou, K.-W. Chang, L. Goncalves, J. Du, J.-S. R. Jang, C.-C. Lee, and H.-Y. Lee, ``Emo-superb: An in-depth look at speech emotion recognition,'' <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.13018</em>, 2024.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
N. Scheidwasser-Clow, M. Kegler, P. Beckmann, and M. Cernak, ``Serab: A multi-lingual benchmark for speech emotion recognition,'' in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2022, pp. 7697–7701.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu, ``W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,'' in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.   IEEE, 2021, pp. 244–250.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
L. Barrault, Y.-A. Chung, M. C. Meglioli, D. Dale, N. Dong, M. Duppenthaler, P.-A. Duquenne, B. Ellis, H. Elsahar, J. Haaheim <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Seamless: Multilingual expressive and streaming speech translation,'' <em id="bib.bib36.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.05187</em>, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Y. Li, R. Yuan, G. Zhang, Y. Ma, X. Chen, H. Yin, C. Lin, A. Ragni, E. Benetos, N. Gyenge <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Mert: Acoustic music understanding model with large-scale self-supervised training,'' <em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.00107</em>, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
B. Elizalde, S. Deshmukh, M. Al Ismail, and H. Wang, ``Clap learning audio concepts from natural language supervision,'' in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.07850" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.07851" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.07851">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.07851" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.07852" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 16:57:39 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
