<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.19674] Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data</title><meta property="og:description" content="Recent advances in speech recognition and translation rely on hundreds of thousands of hours of Internet speech data.
We argue that state-of-the art accuracy can be reached without relying on web-scale data. Canary - m‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.19674">

<!--Generated on Fri Jul  5 22:50:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.3" class="ltx_p">spacing=nonfrench


<span id="p1.3.1" class="ltx_ERROR undefined">\interspeechcameraready</span>

<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>Krishna C.Puvvada<sup id="p1.3.3" class="ltx_sup">‚àó</sup>
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>Piotr≈ªelasko<sup id="p1.3.5" class="ltx_sup">‚àó</sup>
<span id="p1.3.6" class="ltx_ERROR undefined">\name</span>HeHuang<sup id="p1.3.7" class="ltx_sup">‚àó</sup>
<span id="p1.3.8" class="ltx_ERROR undefined">\name</span>OleksiiHrinchuk
<span id="p1.3.9" class="ltx_ERROR undefined">\name</span>Nithin RaoKoluguri
<span id="p1.3.10" class="ltx_ERROR undefined">\name</span>KunalDhawan
<span id="p1.3.11" class="ltx_ERROR undefined">\name</span>SomshubraMajumdar
<span id="p1.3.12" class="ltx_ERROR undefined">\name</span>ElenaRastorgueva
<span id="p1.3.13" class="ltx_ERROR undefined">\name</span>ZhehuaiChen
<span id="p1.3.14" class="ltx_ERROR undefined">\name</span>VitalyLavrukhin
<span id="p1.3.15" class="ltx_ERROR undefined">\name</span>JagadeeshBalam
<span id="p1.3.16" class="ltx_ERROR undefined">\name</span>BorisGinsburg




</p>
</div>
<h1 class="ltx_title ltx_title_document">Less is More: Accurate Speech Recognition &amp; Translation
<br class="ltx_break">without Web-Scale Data</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Recent advances in speech recognition and translation rely on hundreds of thousands of hours of Internet speech data.
We argue that state-of-the art accuracy can be reached without relying on web-scale data. <em id="id1.id1.1" class="ltx_emph ltx_font_italic">Canary</em> - multilingual ASR and speech translation model,
outperforms current state-of-the-art models ‚Äì Whisper, OWSM, and Seamless-M4T on English, French, Spanish, and German languages, while being trained on an order of magnitude less data than these models. Three key factors enables such data-efficient model: (1) a FastConformer-based attention encoder-decoder architecture (2) training on synthetic data generated with machine translation and (3) advanced training techniques: data-balancing, dynamic data blending, dynamic bucketing and noise-robust fine-tuning. The model, weights, and training code will be open-sourced.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>speech recognition, speech translation, FastConformer, multilingual speech model
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>Equal contribution</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The landscape of automatic speech recognition (ASR) and automatic speech translation (AST) has been disrupted with the introduction of large scale multi-task models.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Whisper¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is a transformer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> attention encoder-decoder (AED) model¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> that has demonstrated impressive ASR and AST capabilities in 96 languages. It was trained initially with 680K hours of data and later extended to 5M hours, out of which 4M were transcribed by an earlier model version.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Seamless¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> is a multimodal streaming translation model supporting around 100 languages. It uses several components pretrained on over 4M unlabeled hours of speech, which are later fine-tuned jointly on 125k hours.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">OWSM¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is the first fully open-source attempt at reproducing Whisper model. It's trained on 180k hours of publicly available data and supports 151 languages. The latest OWSM ver 3.1 adopted E-Branchformer architecture, achieving superior accuracy and speed¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Beyond impressive performance, transformer architecture, and size, what these models share in common are significant resource requirements: the amount of data and time required to train them. OWSM is the only work mentioned here that informs about training time and resources used: 16 days of training on 64 NVIDIA A100 40GB GPUs. It's also trained on the least amount of data amongst models under discussion, hence we expect Whisper and Seamless would require even more resources and/or time. Another observation worth noting is that OWSM and Whisper train with a global batch size of 256 (increased to 1024 for Whisper v2 and v3), where each utterance is padded to be exactly 30s long. It was observed in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> that such long utterances harm the model convergence. We also note that this approach may lead to significant padding in mini-batches, resulting in wasted computation on non-informative frames. We present an alternative approach to sampling and batching that allows us to iterate through data twice as fast, while balancing different languages and data sources better. We further accelerate the training and inference by adopting a FastConformer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> architecture and initializing the encoder from a ASR only pretrained checkpoint.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Besides ASR, all models under discussion are capable of AST, i.e., they can transcribe the recording in any of the supported languages (except for Whisper, which only translates <span id="S1.p6.1.1" class="ltx_text ltx_font_typewriter">X<math id="S1.p6.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S1.p6.1.1.m1.1a"><mo stretchy="false" id="S1.p6.1.1.m1.1.1" xref="S1.p6.1.1.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S1.p6.1.1.m1.1b"><ci id="S1.p6.1.1.m1.1.1.cmml" xref="S1.p6.1.1.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.1.m1.1c">\rightarrow</annotation></semantics></math>en</span>). There exists much less data for AST than for ASR, and creating such datasets usually involves translating the transcript into the target language. We demonstrate it is possible to train a strong AST model without using existing AST data at all: instead, we adopt a text machine translation model to automatically create AST supervisions for training from ASR data.</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">Key contributions of this work:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce Canary, an open-source AED model capable of ASR and AST in English, French, Spanish, and German. Canary outperforms other multi-task AED models of similar size on established benchmarks.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We demonstrate that it is possible to train highly accurate speech translation models using only pseudo-labeled translation data.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We train Canary under 48 hours using 128 NVIDIA A100 80GB GPUs with a total of 225K weight updates by initializing the encoder from pre-trained weights and using dynamic bucketing batching techniques.</p>
</div>
</li>
</ul>
<p id="S1.p7.2" class="ltx_p">Using all techniques mentioned above, we train Canary using ``only" 86K hours of speech. This is an order of magnitude less than amount of data used by Whisper and Seamless models.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Model architecture.</span> Canary uses FastConformer encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and a Transformer decoder. FastConformer is a speech-specific modification of a transformer based on Conformer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> that increases the downsampling factor to 8, achieving 2.8x speedup without loss of modeling capacity¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. We train Canary with a cross-entropy objective.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Multi-task training and prompting.</span> To achieve multi-task support, we condition the model's predictions using prompts. Similarly to Whisper, we adopt the following special prompt tokens: <span id="S2.p2.1.2" class="ltx_text ltx_font_typewriter">&lt;|startoftranscript|&gt;</span>, <span id="S2.p2.1.3" class="ltx_text ltx_font_typewriter">&lt;|transcribe|&gt;</span>, <span id="S2.p2.1.4" class="ltx_text ltx_font_typewriter">&lt;|translate|&gt;</span>, <span id="S2.p2.1.5" class="ltx_text ltx_font_typewriter">&lt;|nospeech|&gt;</span>, <span id="S2.p2.1.6" class="ltx_text ltx_font_typewriter">&lt;|endoftranscript|&gt;</span>, and an additional special token for each supported language. Our prompt is constructed similarly to Whisper's, except we specify both input (audio) and output (text) languages as tokens before and after <span id="S2.p2.1.7" class="ltx_text ltx_font_typewriter">&lt;|translate|&gt;</span>, respectively. We also add new special tokens <span id="S2.p2.1.8" class="ltx_text ltx_font_typewriter">&lt;|pnc|&gt;</span> and <span id="S2.p2.1.9" class="ltx_text ltx_font_typewriter">&lt;|nopnc|&gt;</span> at the end of the prompt control sequence to select whether the model should predict punctuation and capitalization, or not. We adopt SentencePiece¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and concatenated tokenizer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> with a vocabulary size of 1024 for each supported language and a 32-unit sub-tokenizer for the special tokens.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.5" class="ltx_p"><span id="S2.p3.5.1" class="ltx_text ltx_font_bold">Dataset and language blending.</span> Since the training data consists of multiple languages and diverse datasets, we aim to ensure consistent sampling of each throughout the training process. Failing to do so tends to result in training intervals where the model performs better on some domains/languages than others. Given <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">N</annotation></semantics></math> datasets, we define a probability distribution <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="P(d)" display="inline"><semantics id="S2.p3.2.m2.1a"><mrow id="S2.p3.2.m2.1.2" xref="S2.p3.2.m2.1.2.cmml"><mi id="S2.p3.2.m2.1.2.2" xref="S2.p3.2.m2.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.p3.2.m2.1.2.1" xref="S2.p3.2.m2.1.2.1.cmml">‚Äã</mo><mrow id="S2.p3.2.m2.1.2.3.2" xref="S2.p3.2.m2.1.2.cmml"><mo stretchy="false" id="S2.p3.2.m2.1.2.3.2.1" xref="S2.p3.2.m2.1.2.cmml">(</mo><mi id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml">d</mi><mo stretchy="false" id="S2.p3.2.m2.1.2.3.2.2" xref="S2.p3.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><apply id="S2.p3.2.m2.1.2.cmml" xref="S2.p3.2.m2.1.2"><times id="S2.p3.2.m2.1.2.1.cmml" xref="S2.p3.2.m2.1.2.1"></times><ci id="S2.p3.2.m2.1.2.2.cmml" xref="S2.p3.2.m2.1.2.2">ùëÉ</ci><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">ùëë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">P(d)</annotation></semantics></math> of choosing the next example from a specific dataset <math id="S2.p3.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.p3.3.m3.1a"><mi id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">ùëë</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">d</annotation></semantics></math>. To ensure this distribution remains stationary throughout training, we use a stochastic weighted multiplexing mechanism to combine the datasets. When constructing a mini-batch of samples, for each training example we first select the dataset according to <math id="S2.p3.4.m4.1" class="ltx_Math" alttext="P(d)" display="inline"><semantics id="S2.p3.4.m4.1a"><mrow id="S2.p3.4.m4.1.2" xref="S2.p3.4.m4.1.2.cmml"><mi id="S2.p3.4.m4.1.2.2" xref="S2.p3.4.m4.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.p3.4.m4.1.2.1" xref="S2.p3.4.m4.1.2.1.cmml">‚Äã</mo><mrow id="S2.p3.4.m4.1.2.3.2" xref="S2.p3.4.m4.1.2.cmml"><mo stretchy="false" id="S2.p3.4.m4.1.2.3.2.1" xref="S2.p3.4.m4.1.2.cmml">(</mo><mi id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml">d</mi><mo stretchy="false" id="S2.p3.4.m4.1.2.3.2.2" xref="S2.p3.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><apply id="S2.p3.4.m4.1.2.cmml" xref="S2.p3.4.m4.1.2"><times id="S2.p3.4.m4.1.2.1.cmml" xref="S2.p3.4.m4.1.2.1"></times><ci id="S2.p3.4.m4.1.2.2.cmml" xref="S2.p3.4.m4.1.2.2">ùëÉ</ci><ci id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1">ùëë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">P(d)</annotation></semantics></math> and then sample an utterance from this dataset. In the expectation, each mini-batch would have a ratio of data coming from each source according to <math id="S2.p3.5.m5.1" class="ltx_Math" alttext="P(d)" display="inline"><semantics id="S2.p3.5.m5.1a"><mrow id="S2.p3.5.m5.1.2" xref="S2.p3.5.m5.1.2.cmml"><mi id="S2.p3.5.m5.1.2.2" xref="S2.p3.5.m5.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.p3.5.m5.1.2.1" xref="S2.p3.5.m5.1.2.1.cmml">‚Äã</mo><mrow id="S2.p3.5.m5.1.2.3.2" xref="S2.p3.5.m5.1.2.cmml"><mo stretchy="false" id="S2.p3.5.m5.1.2.3.2.1" xref="S2.p3.5.m5.1.2.cmml">(</mo><mi id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml">d</mi><mo stretchy="false" id="S2.p3.5.m5.1.2.3.2.2" xref="S2.p3.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><apply id="S2.p3.5.m5.1.2.cmml" xref="S2.p3.5.m5.1.2"><times id="S2.p3.5.m5.1.2.1.cmml" xref="S2.p3.5.m5.1.2.1"></times><ci id="S2.p3.5.m5.1.2.2.cmml" xref="S2.p3.5.m5.1.2.2">ùëÉ</ci><ci id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1">ùëë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">P(d)</annotation></semantics></math>.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span id="footnote1.1" class="ltx_text ltx_font_typewriter">CutSet.mux</span> method at https://lhotse.readthedocs.io/</span></span></span> We consider weights as ``natural" when they are proportional to the cumulative duration of each dataset. We also experiment with re-weighting strategies based on stratification by language and, within each language, by dataset. Additionally, we explore the use of temperature scaling applied to these dataset weights before sampling.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Note that this approach is compatible with optimized dataloading techniques that rely on sequential I/O (such as webdataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> or Lhotse Shar<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>) when each dataset is stored separately (e.g., as a separate collection of tar file shards).</span></span></span></p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Variable utterance length and dynamic batch sizes.</span> We address the issue of duration distribution variability across utterances, typically ranging between one to forty seconds. We perform stratified sampling based on sample duration with a technique known as bucketing, where there are <math id="S2.p4.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.p4.1.m1.1a"><mi id="S2.p4.1.m1.1.1" xref="S2.p4.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.p4.1.m1.1b"><ci id="S2.p4.1.m1.1.1.cmml" xref="S2.p4.1.m1.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.1.m1.1c">M</annotation></semantics></math> buckets, each containing utterances of similar duration, and any given mini-batch is sampled from just one bucket chosen randomly. Unlike most bucketing implementations that require partitioning data up-front, we leverage a dynamic bucketing technique. We estimate the optimal bucket bins (in the sense of equal bucket utilization given data duration distribution) up-front, but the allocation of utterances into buckets is done dynamically during training with a small utterance buffer in CPU RAM. The mini-batches are sampled to satisfy a maximum total duration, resulting in dynamic batch size.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span id="footnote3.1" class="ltx_text ltx_font_typewriter">DynamicBucketingSampler</span> at https://lhotse.readthedocs.io/</span></span></span> We further account for quadratic sequence length complexity of the encoder by introducing a quadratic duration penalty. We find it helps equalize the GPU utilization across mini-batches from different buckets and improves the throughput. Thanks to this data-driven bucketing calibration, we typically observe only about 3% of padding in mini-batches compared to as much as 50% when using non-stratified sampling.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Improving robustness to hallucinations.</span> In this work, we define hallucinations as producing transcriptions when input audio contains no speech. The severity of hallucinations is influenced by both the model architecture and the loss function employed. The tendency to hallucinate is a known flaw of AED models. We find that adding noise, music, and other non-speech data to the training dataset as a pseudo-language with its own weight significantly reduces hallucinations, though doesn't eliminate them entirely.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental setup</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.3" class="ltx_p"><span id="S3.p1.3.1" class="ltx_text ltx_font_bold">Training data.</span> Canary was trained on a mixture of public and in-house datasets. Table¬†<a href="#S3.T1" title="Table 1 ‚Ä£ 3 Experimental setup ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the composition of training data for the ASR task (81.5K hours in total).
The public portion of English is composed of LibriSpeech, Fisher Corpus, Switchboard-1, WSJ-0 &amp; WSJ-1, National Speech Corpus (Part 1, Part 6), VCTK, VoxPopuli (EN), Europarl-ASR (EN), Multilingual LibriSpeech (MLS)-EN (2k hour subset), Mozilla Common Voice (MCV)-7.0, People's Speech (12K hour subset), MCV-11.0 (1.5k hour subset).
800 hour subset of MCV-12.0, 1.5K hour subset of MLS and 200 hour subset of VoxPopuli were gathered from public sources for German.
For Spanish, 395 hours from MCV-12.0, 780 hours from MLS, 108 hours from VoxPopuli and 141 hours from Fisher were collected from public domain.
Similarly for French, 708 hour subset from MCV-12.0, 926 hours from MLS and 165 hours from VoxPopuli were used from public domain. Table¬†<a href="#S3.T1" title="Table 1 ‚Ä£ 3 Experimental setup ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> also shows number of hours with punctuation and capitalization (PnC) for each language. PnC was obtained from respective dataset metadata when available (e.g. LibriSpeech). Alternatively, PnC was restored using neural models for some datasets. PnC data was further processed to remove all punctuation marks except five (',?.!). Text normalization was applied to ground truth. 300 hours of non-Speech data (AudioSet strongly-labelled subset portion of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>) is used to improve model robustness. Data for AST was solely obtained by generating synthetic labels using Neural Machine Translation models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> without using additional datasets. 43K hours of English ASR data from Table¬†<a href="#S3.T1" title="Table 1 ‚Ä£ 3 Experimental setup ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> was used to generate training data for English<math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.p1.1.m1.1a"><mo stretchy="false" id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\rightarrow</annotation></semantics></math>X (X: German, Spanish, French). All available data from Table¬†<a href="#S3.T1" title="Table 1 ‚Ä£ 3 Experimental setup ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for German, Spanish, French languages was used to prepare X <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.p1.2.m2.1a"><mo stretchy="false" id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\rightarrow</annotation></semantics></math> English direction of translation data. Further, a 4.8K hour English <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.p1.3.m3.1a"><mo stretchy="false" id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\rightarrow</annotation></semantics></math> German translation dataset from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> was also used, which in-itself was also pseudo-labeled, bringing the total size to 86.3K hours.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Test data.</span> Test sets from MCV-16.1, MLS, and VoxPopuli were used to measure ASR performance across all languages. The translation capabilities of the models were examined using FLEURS, mExpresso, and CoVoST-v2. Additionally, standard English test sets such as AMI, Earnings22, Gigaspeech, LibriSpeech (test-clean &amp; test-other), SPGI, and Tedlium were utilized to evaluate model generalization across different domains. The Music and Noise subsets (a total of 48 hours) from MUSAN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> were used to assess robustness to hallucinations.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Training data statistics with breakdown per language and availability of punctuation and capitalization (PnC).</figcaption>
<div id="S3.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:173.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(29.9pt,-12.0pt) scale(1.1600131865139,1.1600131865139) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Language</span></td>
<td id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S3.T1.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1.1.2.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Public</span></td>
</tr>
<tr id="S3.T1.1.1.1.1.2.1.2" class="ltx_tr">
<td id="S3.T1.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">+ In-house [K hours]</span></td>
</tr>
</table>
</td>
<td id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S3.T1.1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1.1.3.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">PnC</span></td>
</tr>
<tr id="S3.T1.1.1.1.1.3.1.2" class="ltx_tr">
<td id="S3.T1.1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">[K hours]</span></td>
</tr>
</table>
</td>
<td id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S3.T1.1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1.1.4.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Dur. [s]</span></td>
</tr>
<tr id="S3.T1.1.1.1.1.4.1.2" class="ltx_tr">
<td id="S3.T1.1.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">[Min, Max]</span></td>
</tr>
</table>
</td>
<td id="S3.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T1.1.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1.1.5.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold"># Utterances</span></td>
</tr>
<tr id="S3.T1.1.1.1.1.5.1.2" class="ltx_tr">
<td id="S3.T1.1.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T1.1.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">[M]</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T1.1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">English</td>
<td id="S3.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">25.5 + 37.9</td>
<td id="S3.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">38.5</td>
<td id="S3.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">[1, 40]</td>
<td id="S3.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_tt">24</td>
</tr>
<tr id="S3.T1.1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">German</td>
<td id="S3.T1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.5 + 3.6</td>
<td id="S3.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.1</td>
<td id="S3.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">[1, 20]</td>
<td id="S3.T1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">2.4</td>
</tr>
<tr id="S3.T1.1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Spanish</td>
<td id="S3.T1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.4 + 5.2</td>
<td id="S3.T1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.4</td>
<td id="S3.T1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">[1, 20]</td>
<td id="S3.T1.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">3.8</td>
</tr>
<tr id="S3.T1.1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">French</td>
<td id="S3.T1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.8 + 3.3</td>
<td id="S3.T1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.8</td>
<td id="S3.T1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">[0.5, 40]</td>
<td id="S3.T1.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">2.5</td>
</tr>
<tr id="S3.T1.1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Non-speech</td>
<td id="S3.T1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.3 + 0.0</td>
<td id="S3.T1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NA</td>
<td id="S3.T1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">[0.47, 10]</td>
<td id="S3.T1.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t">0.1</td>
</tr>
<tr id="S3.T1.1.1.7.7" class="ltx_tr">
<td id="S3.T1.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_tt">Total</td>
<td id="S3.T1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">31.5 + 50</td>
<td id="S3.T1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">47.8</td>
<td id="S3.T1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">NA</td>
<td id="S3.T1.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_tt">32.8</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>WER (%) results on ASR benchmarks. Baseline model predictions are generated using respective public checkpoints. Ground-truth and predictions are normalized using WhisperNormalizer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Canary achieves lowest WER in 10 out of 12 test sets across all languages.</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:74pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-152.6pt,26.0pt) scale(0.586963417761087,0.586963417761087) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T2.1.1.1.1.1" class="ltx_text">Model (WER <math id="S3.T2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.1.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</span></th>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">En</td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">De</td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">Es</td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" colspan="3">Fr</td>
</tr>
<tr id="S3.T2.1.1.2.1" class="ltx_tr">
<td id="S3.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r">MCV-16.1</td>
<td id="S3.T2.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r">MLS</td>
<td id="S3.T2.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r">VoxPopuli</td>
<td id="S3.T2.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r">MCV-16.1</td>
<td id="S3.T2.1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_r">MLS</td>
<td id="S3.T2.1.1.2.1.6" class="ltx_td ltx_align_left ltx_border_r">VoxPopuli</td>
<td id="S3.T2.1.1.2.1.7" class="ltx_td ltx_align_left ltx_border_r">MCV-16.1</td>
<td id="S3.T2.1.1.2.1.8" class="ltx_td ltx_align_left ltx_border_r">MLS</td>
<td id="S3.T2.1.1.2.1.9" class="ltx_td ltx_align_left ltx_border_r">VoxPopuli</td>
<td id="S3.T2.1.1.2.1.10" class="ltx_td ltx_align_left ltx_border_r">MCV-16.1</td>
<td id="S3.T2.1.1.2.1.11" class="ltx_td ltx_align_left ltx_border_r">MLS</td>
<td id="S3.T2.1.1.2.1.12" class="ltx_td ltx_align_left">VoxPopuli</td>
</tr>
<tr id="S3.T2.1.1.3.2" class="ltx_tr">
<th id="S3.T2.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">OWSM-v3.1 (1.02B)</th>
<td id="S3.T2.1.1.3.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">11.87</td>
<td id="S3.T2.1.1.3.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">5.37</td>
<td id="S3.T2.1.1.3.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">7.04</td>
<td id="S3.T2.1.1.3.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">9.24</td>
<td id="S3.T2.1.1.3.2.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">10.49</td>
<td id="S3.T2.1.1.3.2.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">16.25</td>
<td id="S3.T2.1.1.3.2.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">9.59</td>
<td id="S3.T2.1.1.3.2.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">8.84</td>
<td id="S3.T2.1.1.3.2.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">10.17</td>
<td id="S3.T2.1.1.3.2.11" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">13.69</td>
<td id="S3.T2.1.1.3.2.12" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">11.75</td>
<td id="S3.T2.1.1.3.2.13" class="ltx_td ltx_align_right ltx_border_tt">12.61</td>
</tr>
<tr id="S3.T2.1.1.4.3" class="ltx_tr">
<th id="S3.T2.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">SeamlessM4T-medium (1.2B)</th>
<td id="S3.T2.1.1.4.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">10.25</td>
<td id="S3.T2.1.1.4.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">7.05</td>
<td id="S3.T2.1.1.4.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">6.06</td>
<td id="S3.T2.1.1.4.3.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">9.32</td>
<td id="S3.T2.1.1.4.3.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">8.12</td>
<td id="S3.T2.1.1.4.3.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">12.95</td>
<td id="S3.T2.1.1.4.3.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">7.25</td>
<td id="S3.T2.1.1.4.3.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">5.25</td>
<td id="S3.T2.1.1.4.3.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">7.31</td>
<td id="S3.T2.1.1.4.3.11" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">11.07</td>
<td id="S3.T2.1.1.4.3.12" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">7.32</td>
<td id="S3.T2.1.1.4.3.13" class="ltx_td ltx_align_right ltx_border_t">8.77</td>
</tr>
<tr id="S3.T2.1.1.5.4" class="ltx_tr">
<th id="S3.T2.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">SeamlessM4T-large-v2 (2.3B)</th>
<td id="S3.T2.1.1.5.4.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T2.1.1.5.4.2.1" class="ltx_text ltx_font_bold">7.47</span></td>
<td id="S3.T2.1.1.5.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.14</td>
<td id="S3.T2.1.1.5.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.68</td>
<td id="S3.T2.1.1.5.4.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">5.82</td>
<td id="S3.T2.1.1.5.4.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">6.08</td>
<td id="S3.T2.1.1.5.4.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T2.1.1.5.4.7.1" class="ltx_text ltx_font_bold">10.68</span></td>
<td id="S3.T2.1.1.5.4.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.82</td>
<td id="S3.T2.1.1.5.4.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.14</td>
<td id="S3.T2.1.1.5.4.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">6.76</td>
<td id="S3.T2.1.1.5.4.11" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">7.75</td>
<td id="S3.T2.1.1.5.4.12" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">5.38</td>
<td id="S3.T2.1.1.5.4.13" class="ltx_td ltx_align_right ltx_border_t">6.82</td>
</tr>
<tr id="S3.T2.1.1.6.5" class="ltx_tr">
<th id="S3.T2.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Whisper-large-v3 (1.5B)</th>
<td id="S3.T2.1.1.6.5.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">9.92</td>
<td id="S3.T2.1.1.6.5.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">3.53</td>
<td id="S3.T2.1.1.6.5.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">6.23</td>
<td id="S3.T2.1.1.6.5.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">6.17</td>
<td id="S3.T2.1.1.6.5.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">5.83</td>
<td id="S3.T2.1.1.6.5.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">16.50</td>
<td id="S3.T2.1.1.6.5.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.94</td>
<td id="S3.T2.1.1.6.5.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.42</td>
<td id="S3.T2.1.1.6.5.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">8.01</td>
<td id="S3.T2.1.1.6.5.11" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">11.18</td>
<td id="S3.T2.1.1.6.5.12" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">5.38</td>
<td id="S3.T2.1.1.6.5.13" class="ltx_td ltx_align_right ltx_border_t">7.52</td>
</tr>
<tr id="S3.T2.1.1.7.6" class="ltx_tr">
<th id="S3.T2.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_tt">Canary (1B)</th>
<td id="S3.T2.1.1.7.6.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt">7.83</td>
<td id="S3.T2.1.1.7.6.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S3.T2.1.1.7.6.3.1" class="ltx_text ltx_font_bold">3.03</span></td>
<td id="S3.T2.1.1.7.6.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S3.T2.1.1.7.6.4.1" class="ltx_text ltx_font_bold">4.42</span></td>
<td id="S3.T2.1.1.7.6.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S3.T2.1.1.7.6.5.1" class="ltx_text ltx_font_bold">4.49</span></td>
<td id="S3.T2.1.1.7.6.6" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S3.T2.1.1.7.6.6.1" class="ltx_text ltx_font_bold">4.09</span></td>
<td id="S3.T2.1.1.7.6.7" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt">10.70</td>
<td id="S3.T2.1.1.7.6.8" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S3.T2.1.1.7.6.8.1" class="ltx_text ltx_font_bold">3.88</span></td>
<td id="S3.T2.1.1.7.6.9" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S3.T2.1.1.7.6.9.1" class="ltx_text ltx_font_bold">3.12</span></td>
<td id="S3.T2.1.1.7.6.10" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S3.T2.1.1.7.6.10.1" class="ltx_text ltx_font_bold">5.02</span></td>
<td id="S3.T2.1.1.7.6.11" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S3.T2.1.1.7.6.11.1" class="ltx_text ltx_font_bold">6.37</span></td>
<td id="S3.T2.1.1.7.6.12" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S3.T2.1.1.7.6.12.1" class="ltx_text ltx_font_bold">4.06</span></td>
<td id="S3.T2.1.1.7.6.13" class="ltx_td ltx_align_right ltx_border_b ltx_border_tt"><span id="S3.T2.1.1.7.6.13.1" class="ltx_text ltx_font_bold">5.48</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2406.19674/assets/statistical-significance-canary.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Word error rate on 12 test sets for the proposed model and 4 baselines. Vertical bars indicate 95% confidence intervals obtained from boostrap method with <math id="S3.F1.2.m1.1" class="ltx_Math" alttext="10^{4}" display="inline"><semantics id="S3.F1.2.m1.1b"><msup id="S3.F1.2.m1.1.1" xref="S3.F1.2.m1.1.1.cmml"><mn id="S3.F1.2.m1.1.1.2" xref="S3.F1.2.m1.1.1.2.cmml">10</mn><mn id="S3.F1.2.m1.1.1.3" xref="S3.F1.2.m1.1.1.3.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="S3.F1.2.m1.1c"><apply id="S3.F1.2.m1.1.1.cmml" xref="S3.F1.2.m1.1.1"><csymbol cd="ambiguous" id="S3.F1.2.m1.1.1.1.cmml" xref="S3.F1.2.m1.1.1">superscript</csymbol><cn type="integer" id="S3.F1.2.m1.1.1.2.cmml" xref="S3.F1.2.m1.1.1.2">10</cn><cn type="integer" id="S3.F1.2.m1.1.1.3.cmml" xref="S3.F1.2.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.2.m1.1d">10^{4}</annotation></semantics></math> replications¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</figcaption>
</figure>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.7" class="ltx_p"><span id="S3.p3.7.1" class="ltx_text ltx_font_bold">Training settings.</span> Canary uses FastConformer encoder of XL size from ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Along with convolution sub-sampling block, it has 24 conformer layers with model dimension 1024, projection dimension 4096, convolution kernel size 9 and 8 attention heads, with a total parameter count of 0.6B. The decoder is a regular transformer decoder¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, with 24 layers of dimension 1024, projection dimension 4096, and 8 attention heads, with a total parameter count of 0.4B. The decoder uses fixed-positional encoding.
The encoder consumes audio in the form of 128-dim log-mel features extracted every 10 msec over a window of 25 msec (preliminary experiments didn't show significant difference between 80-dim and 128-dim log-mel features for ASR, but 128-dim features showed consistent improvement for AST).
<br class="ltx_break">Lhotse¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> was used for dataloading with a batch duration of 360 sec per GPU, <span id="S3.p3.7.2" class="ltx_text ltx_font_typewriter">quadratic_duration</span> of 20 sec, <span id="S3.p3.7.3" class="ltx_text ltx_font_typewriter">buffer_size</span> of 20000, <span id="S3.p3.7.4" class="ltx_text ltx_font_typewriter">shuffle_buffer_size</span> of 10000 and <span id="S3.p3.7.5" class="ltx_text ltx_font_typewriter">num_buckets</span> as 31. To balance multiple languages and datasets, training examples were sampled based on the probability distribution <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="p_{s}\sim\left(\frac{n_{s}}{N}\right)^{\alpha}" display="inline"><semantics id="S3.p3.1.m1.1a"><mrow id="S3.p3.1.m1.1.2" xref="S3.p3.1.m1.1.2.cmml"><msub id="S3.p3.1.m1.1.2.2" xref="S3.p3.1.m1.1.2.2.cmml"><mi id="S3.p3.1.m1.1.2.2.2" xref="S3.p3.1.m1.1.2.2.2.cmml">p</mi><mi id="S3.p3.1.m1.1.2.2.3" xref="S3.p3.1.m1.1.2.2.3.cmml">s</mi></msub><mo id="S3.p3.1.m1.1.2.1" xref="S3.p3.1.m1.1.2.1.cmml">‚àº</mo><msup id="S3.p3.1.m1.1.2.3" xref="S3.p3.1.m1.1.2.3.cmml"><mrow id="S3.p3.1.m1.1.2.3.2.2" xref="S3.p3.1.m1.1.1.cmml"><mo id="S3.p3.1.m1.1.2.3.2.2.1" xref="S3.p3.1.m1.1.1.cmml">(</mo><mfrac id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><msub id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml"><mi id="S3.p3.1.m1.1.1.2.2" xref="S3.p3.1.m1.1.1.2.2.cmml">n</mi><mi id="S3.p3.1.m1.1.1.2.3" xref="S3.p3.1.m1.1.1.2.3.cmml">s</mi></msub><mi id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">N</mi></mfrac><mo id="S3.p3.1.m1.1.2.3.2.2.2" xref="S3.p3.1.m1.1.1.cmml">)</mo></mrow><mi id="S3.p3.1.m1.1.2.3.3" xref="S3.p3.1.m1.1.2.3.3.cmml">Œ±</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.2.cmml" xref="S3.p3.1.m1.1.2"><csymbol cd="latexml" id="S3.p3.1.m1.1.2.1.cmml" xref="S3.p3.1.m1.1.2.1">similar-to</csymbol><apply id="S3.p3.1.m1.1.2.2.cmml" xref="S3.p3.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.2.2.1.cmml" xref="S3.p3.1.m1.1.2.2">subscript</csymbol><ci id="S3.p3.1.m1.1.2.2.2.cmml" xref="S3.p3.1.m1.1.2.2.2">ùëù</ci><ci id="S3.p3.1.m1.1.2.2.3.cmml" xref="S3.p3.1.m1.1.2.2.3">ùë†</ci></apply><apply id="S3.p3.1.m1.1.2.3.cmml" xref="S3.p3.1.m1.1.2.3"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.2.3.1.cmml" xref="S3.p3.1.m1.1.2.3">superscript</csymbol><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.2.3.2.2"><divide id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.2.3.2.2"></divide><apply id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.2.1.cmml" xref="S3.p3.1.m1.1.1.2">subscript</csymbol><ci id="S3.p3.1.m1.1.1.2.2.cmml" xref="S3.p3.1.m1.1.1.2.2">ùëõ</ci><ci id="S3.p3.1.m1.1.1.2.3.cmml" xref="S3.p3.1.m1.1.1.2.3">ùë†</ci></apply><ci id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3">ùëÅ</ci></apply><ci id="S3.p3.1.m1.1.2.3.3.cmml" xref="S3.p3.1.m1.1.2.3.3">ùõº</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">p_{s}\sim\left(\frac{n_{s}}{N}\right)^{\alpha}</annotation></semantics></math>, where <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">ùë†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">s</annotation></semantics></math> represents a stratum (e.g., a language or a dataset), <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="n_{s}" display="inline"><semantics id="S3.p3.3.m3.1a"><msub id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml"><mi id="S3.p3.3.m3.1.1.2" xref="S3.p3.3.m3.1.1.2.cmml">n</mi><mi id="S3.p3.3.m3.1.1.3" xref="S3.p3.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><apply id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p3.3.m3.1.1.1.cmml" xref="S3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.p3.3.m3.1.1.2.cmml" xref="S3.p3.3.m3.1.1.2">ùëõ</ci><ci id="S3.p3.3.m3.1.1.3.cmml" xref="S3.p3.3.m3.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">n_{s}</annotation></semantics></math> is the number of hours for stratum <math id="S3.p3.4.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">ùë†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">s</annotation></semantics></math>, <math id="S3.p3.5.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.p3.5.m5.1a"><mi id="S3.p3.5.m5.1.1" xref="S3.p3.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><ci id="S3.p3.5.m5.1.1.cmml" xref="S3.p3.5.m5.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">N</annotation></semantics></math> is the total number of hours, and <math id="S3.p3.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.p3.6.m6.1a"><mi id="S3.p3.6.m6.1.1" xref="S3.p3.6.m6.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.1b"><ci id="S3.p3.6.m6.1.1.cmml" xref="S3.p3.6.m6.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.1c">\alpha</annotation></semantics></math> is the up-sampling factor¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. We implemented a two-level hierarchical stratification of the training corpus: initially at the language level and subsequently within each language by dataset. The final weight assigned to a dataset is the product of these two probabilities. In both stratification levels, we set <math id="S3.p3.7.m7.1" class="ltx_Math" alttext="\alpha=0.5" display="inline"><semantics id="S3.p3.7.m7.1a"><mrow id="S3.p3.7.m7.1.1" xref="S3.p3.7.m7.1.1.cmml"><mi id="S3.p3.7.m7.1.1.2" xref="S3.p3.7.m7.1.1.2.cmml">Œ±</mi><mo id="S3.p3.7.m7.1.1.1" xref="S3.p3.7.m7.1.1.1.cmml">=</mo><mn id="S3.p3.7.m7.1.1.3" xref="S3.p3.7.m7.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.7.m7.1b"><apply id="S3.p3.7.m7.1.1.cmml" xref="S3.p3.7.m7.1.1"><eq id="S3.p3.7.m7.1.1.1.cmml" xref="S3.p3.7.m7.1.1.1"></eq><ci id="S3.p3.7.m7.1.1.2.cmml" xref="S3.p3.7.m7.1.1.2">ùõº</ci><cn type="float" id="S3.p3.7.m7.1.1.3.cmml" xref="S3.p3.7.m7.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.7.m7.1c">\alpha=0.5</annotation></semantics></math>. Non-speech audio has been treated as a separate language for the purposes of sampling.
<br class="ltx_break">The model was trained in 2 stages using NVIDIA NeMo¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> framework. In stage-1, the model was trained for 150K updates. We used AdamW with a peak learning rate (LR) of 3e-4 and weight decay of 1e-3. The learning rate was warmed up over 2.5K steps and annealed as per Noam scheduling. The encoder was initialized from an XL version of¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, whose training data was a subset of Table¬†<a href="#S3.T1" title="Table 1 ‚Ä£ 3 Experimental setup ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Encoder initialization helped model converge faster and achieve better metrics overall. The decoder was random initialized. Stage-2 was trained for 75K updates with a peak LR of 2e-5, with remaining hyperparameters being same as stage-1. The main difference between both stages is the inclusion of Non-speech dataset from Table¬†<a href="#S3.T1" title="Table 1 ‚Ä£ 3 Experimental setup ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> in stage-2 (Note that this 2 stage training is merely practical convenience to allow for faster experimentation wrt to robustness and is not a necessity). In both stages, 128 A100 (80GB) GPUs were used.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We evaluate the Canary model on speech recognition (ASR) and speech-to-text translation (AST), and show the results in Table¬†<a href="#S3.T2" title="Table 2 ‚Ä£ 3 Experimental setup ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>
and Table¬†<a href="#S4.T4" title="Table 4 ‚Ä£ 4 Results ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> respectively. We use Whisper, OWSM-v3.1, and SeamlessM4T as baselines by using their official checkpoints and re-running the models on the same test sets. All models use beam search decoding with beam width 5.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparing Canary with SOTA models across different domains for English ASR¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Canary achieves best average WER exhibiting generalizability across domains. </figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:57.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-98.1pt,16.3pt) scale(0.638688854934879,0.638688854934879) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Model (WER <math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</th>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">AMI</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Earnings22</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">GigaSpeech</td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LS Clean</td>
<td id="S4.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LS Other</td>
<td id="S4.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SPGISpeech</td>
<td id="S4.T3.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Tedlium</td>
<td id="S4.T3.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t">Avg. WER</td>
</tr>
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<th id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Whisper-large-v3</th>
<td id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">16.01</td>
<td id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.1.1.2.1.3.1" class="ltx_text ltx_font_bold">11.3</span></td>
<td id="S4.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10.02</td>
<td id="S4.T3.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">2.03</td>
<td id="S4.T3.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">3.91</td>
<td id="S4.T3.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">2.95</td>
<td id="S4.T3.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">3.9</td>
<td id="S4.T3.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_tt">7.16</td>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<th id="S4.T3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Parakeet-RNNT-1.1B</th>
<td id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.1</td>
<td id="S4.T3.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.15</td>
<td id="S4.T3.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.96</td>
<td id="S4.T3.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.46</td>
<td id="S4.T3.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.3.2.6.1" class="ltx_text ltx_font_bold">2.48</span></td>
<td id="S4.T3.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.11</td>
<td id="S4.T3.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.92</td>
<td id="S4.T3.1.1.3.2.9" class="ltx_td ltx_align_center ltx_border_t">7.60</td>
</tr>
<tr id="S4.T3.1.1.4.3" class="ltx_tr">
<th id="S4.T3.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Parakeet-TDT-1.1B</th>
<td id="S4.T3.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.9</td>
<td id="S4.T3.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.65</td>
<td id="S4.T3.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.4.3.4.1" class="ltx_text ltx_font_bold">9.55</span></td>
<td id="S4.T3.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.4.3.5.1" class="ltx_text ltx_font_bold">1.39</span></td>
<td id="S4.T3.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.62</td>
<td id="S4.T3.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.42</td>
<td id="S4.T3.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.56</td>
<td id="S4.T3.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_t">7.30</td>
</tr>
<tr id="S4.T3.1.1.5.4" class="ltx_tr">
<th id="S4.T3.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_tt">Canary (1B)</th>
<td id="S4.T3.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S4.T3.1.1.5.4.2.1" class="ltx_text ltx_font_bold">13.53</span></td>
<td id="S4.T3.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">12.05</td>
<td id="S4.T3.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">10.07</td>
<td id="S4.T3.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">1.47</td>
<td id="S4.T3.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">2.86</td>
<td id="S4.T3.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S4.T3.1.1.5.4.7.1" class="ltx_text ltx_font_bold">2.02</span></td>
<td id="S4.T3.1.1.5.4.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><span id="S4.T3.1.1.5.4.8.1" class="ltx_text ltx_font_bold">3.53</span></td>
<td id="S4.T3.1.1.5.4.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_tt"><span id="S4.T3.1.1.5.4.9.1" class="ltx_text ltx_font_bold">6.50</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>BLEU scores on speech translation (AST) benchmarks. Annotations from the corresponding datasets come with native punctuation and capitalization and are used without further processing/normalization. SeamlessM4T-large-v2 (2.3B) achieves the best overall BLEU scores. Canary (1B) matches or outperforms models of comparable size.</figcaption>
<div id="S4.T4.17" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:77pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-138.1pt,24.5pt) scale(0.610803299152686,0.610803299152686) ;">
<table id="S4.T4.17.17" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.5.5.5" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T4.1.1.1.1.1" class="ltx_text">Model (BLEU <math id="S4.T4.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T4.1.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></th>
<td id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">FLEURS (En <math id="S4.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T4.2.2.2.2.m1.1.1" xref="S4.T4.2.2.2.2.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">\rightarrow</annotation></semantics></math> X)</td>
<td id="S4.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">mExpresso (En <math id="S4.T4.3.3.3.3.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T4.3.3.3.3.m1.1.1" xref="S4.T4.3.3.3.3.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.3.m1.1b"><ci id="S4.T4.3.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.3.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.3.m1.1c">\rightarrow</annotation></semantics></math> X)</td>
<td id="S4.T4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">FLEURS (X <math id="S4.T4.4.4.4.4.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.4.4.4.4.m1.1a"><mo stretchy="false" id="S4.T4.4.4.4.4.m1.1.1" xref="S4.T4.4.4.4.4.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.4.m1.1b"><ci id="S4.T4.4.4.4.4.m1.1.1.cmml" xref="S4.T4.4.4.4.4.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.4.m1.1c">\rightarrow</annotation></semantics></math> En)</td>
<td id="S4.T4.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t" colspan="3">COVOST-v2 (X <math id="S4.T4.5.5.5.5.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.5.5.5.5.m1.1a"><mo stretchy="false" id="S4.T4.5.5.5.5.m1.1.1" xref="S4.T4.5.5.5.5.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.5.m1.1b"><ci id="S4.T4.5.5.5.5.m1.1.1.cmml" xref="S4.T4.5.5.5.5.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.5.m1.1c">\rightarrow</annotation></semantics></math> En)</td>
</tr>
<tr id="S4.T4.17.17.17" class="ltx_tr">
<td id="S4.T4.6.6.6.1" class="ltx_td ltx_align_left ltx_border_r">En <math id="S4.T4.6.6.6.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.6.6.6.1.m1.1a"><mo stretchy="false" id="S4.T4.6.6.6.1.m1.1.1" xref="S4.T4.6.6.6.1.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.1.m1.1b"><ci id="S4.T4.6.6.6.1.m1.1.1.cmml" xref="S4.T4.6.6.6.1.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.1.m1.1c">\rightarrow</annotation></semantics></math> De</td>
<td id="S4.T4.7.7.7.2" class="ltx_td ltx_align_left ltx_border_r">En <math id="S4.T4.7.7.7.2.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.7.7.7.2.m1.1a"><mo stretchy="false" id="S4.T4.7.7.7.2.m1.1.1" xref="S4.T4.7.7.7.2.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.2.m1.1b"><ci id="S4.T4.7.7.7.2.m1.1.1.cmml" xref="S4.T4.7.7.7.2.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.2.m1.1c">\rightarrow</annotation></semantics></math> Es</td>
<td id="S4.T4.8.8.8.3" class="ltx_td ltx_align_left ltx_border_r">En <math id="S4.T4.8.8.8.3.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.8.8.8.3.m1.1a"><mo stretchy="false" id="S4.T4.8.8.8.3.m1.1.1" xref="S4.T4.8.8.8.3.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.8.3.m1.1b"><ci id="S4.T4.8.8.8.3.m1.1.1.cmml" xref="S4.T4.8.8.8.3.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.8.3.m1.1c">\rightarrow</annotation></semantics></math> Fr</td>
<td id="S4.T4.9.9.9.4" class="ltx_td ltx_align_left ltx_border_r">En <math id="S4.T4.9.9.9.4.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.9.9.9.4.m1.1a"><mo stretchy="false" id="S4.T4.9.9.9.4.m1.1.1" xref="S4.T4.9.9.9.4.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.9.4.m1.1b"><ci id="S4.T4.9.9.9.4.m1.1.1.cmml" xref="S4.T4.9.9.9.4.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.9.4.m1.1c">\rightarrow</annotation></semantics></math> De</td>
<td id="S4.T4.10.10.10.5" class="ltx_td ltx_align_left ltx_border_r">En <math id="S4.T4.10.10.10.5.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.10.10.10.5.m1.1a"><mo stretchy="false" id="S4.T4.10.10.10.5.m1.1.1" xref="S4.T4.10.10.10.5.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.10.5.m1.1b"><ci id="S4.T4.10.10.10.5.m1.1.1.cmml" xref="S4.T4.10.10.10.5.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.10.5.m1.1c">\rightarrow</annotation></semantics></math> Es</td>
<td id="S4.T4.11.11.11.6" class="ltx_td ltx_align_left ltx_border_r">En <math id="S4.T4.11.11.11.6.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.11.11.11.6.m1.1a"><mo stretchy="false" id="S4.T4.11.11.11.6.m1.1.1" xref="S4.T4.11.11.11.6.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.11.11.11.6.m1.1b"><ci id="S4.T4.11.11.11.6.m1.1.1.cmml" xref="S4.T4.11.11.11.6.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.11.11.6.m1.1c">\rightarrow</annotation></semantics></math> Fr</td>
<td id="S4.T4.12.12.12.7" class="ltx_td ltx_align_left ltx_border_r">De <math id="S4.T4.12.12.12.7.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.12.12.12.7.m1.1a"><mo stretchy="false" id="S4.T4.12.12.12.7.m1.1.1" xref="S4.T4.12.12.12.7.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.12.7.m1.1b"><ci id="S4.T4.12.12.12.7.m1.1.1.cmml" xref="S4.T4.12.12.12.7.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.12.7.m1.1c">\rightarrow</annotation></semantics></math> En</td>
<td id="S4.T4.13.13.13.8" class="ltx_td ltx_align_left ltx_border_r">Es <math id="S4.T4.13.13.13.8.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.13.13.13.8.m1.1a"><mo stretchy="false" id="S4.T4.13.13.13.8.m1.1.1" xref="S4.T4.13.13.13.8.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.13.13.13.8.m1.1b"><ci id="S4.T4.13.13.13.8.m1.1.1.cmml" xref="S4.T4.13.13.13.8.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.13.13.8.m1.1c">\rightarrow</annotation></semantics></math> En</td>
<td id="S4.T4.14.14.14.9" class="ltx_td ltx_align_left ltx_border_r">Fr <math id="S4.T4.14.14.14.9.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.14.14.14.9.m1.1a"><mo stretchy="false" id="S4.T4.14.14.14.9.m1.1.1" xref="S4.T4.14.14.14.9.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.14.14.14.9.m1.1b"><ci id="S4.T4.14.14.14.9.m1.1.1.cmml" xref="S4.T4.14.14.14.9.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.14.14.14.9.m1.1c">\rightarrow</annotation></semantics></math> En</td>
<td id="S4.T4.15.15.15.10" class="ltx_td ltx_align_left ltx_border_r">De <math id="S4.T4.15.15.15.10.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.15.15.15.10.m1.1a"><mo stretchy="false" id="S4.T4.15.15.15.10.m1.1.1" xref="S4.T4.15.15.15.10.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.15.15.15.10.m1.1b"><ci id="S4.T4.15.15.15.10.m1.1.1.cmml" xref="S4.T4.15.15.15.10.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.15.15.15.10.m1.1c">\rightarrow</annotation></semantics></math> En</td>
<td id="S4.T4.16.16.16.11" class="ltx_td ltx_align_left ltx_border_r">Es <math id="S4.T4.16.16.16.11.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.16.16.16.11.m1.1a"><mo stretchy="false" id="S4.T4.16.16.16.11.m1.1.1" xref="S4.T4.16.16.16.11.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.16.16.16.11.m1.1b"><ci id="S4.T4.16.16.16.11.m1.1.1.cmml" xref="S4.T4.16.16.16.11.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.16.16.16.11.m1.1c">\rightarrow</annotation></semantics></math> En</td>
<td id="S4.T4.17.17.17.12" class="ltx_td ltx_align_left">Fr <math id="S4.T4.17.17.17.12.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.17.17.17.12.m1.1a"><mo stretchy="false" id="S4.T4.17.17.17.12.m1.1.1" xref="S4.T4.17.17.17.12.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.17.17.17.12.m1.1b"><ci id="S4.T4.17.17.17.12.m1.1.1.cmml" xref="S4.T4.17.17.17.12.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.17.17.17.12.m1.1c">\rightarrow</annotation></semantics></math> En</td>
</tr>
<tr id="S4.T4.17.17.18.1" class="ltx_tr">
<th id="S4.T4.17.17.18.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">OWSM-v3.1 (1.02B)</th>
<td id="S4.T4.17.17.18.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">24.37</td>
<td id="S4.T4.17.17.18.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">11.39</td>
<td id="S4.T4.17.17.18.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">16.39</td>
<td id="S4.T4.17.17.18.1.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">19.29</td>
<td id="S4.T4.17.17.18.1.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">10.98</td>
<td id="S4.T4.17.17.18.1.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">8.59</td>
<td id="S4.T4.17.17.18.1.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">13.22</td>
<td id="S4.T4.17.17.18.1.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">9.35</td>
<td id="S4.T4.17.17.18.1.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">12.38</td>
<td id="S4.T4.17.17.18.1.11" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">18.05</td>
<td id="S4.T4.17.17.18.1.12" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">23.90</td>
<td id="S4.T4.17.17.18.1.13" class="ltx_td ltx_align_right ltx_border_tt">24.47</td>
</tr>
<tr id="S4.T4.17.17.19.2" class="ltx_tr">
<th id="S4.T4.17.17.19.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">SeamlessM4T-medium (1.2B)</th>
<td id="S4.T4.17.17.19.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">28.30</td>
<td id="S4.T4.17.17.19.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">21.05</td>
<td id="S4.T4.17.17.19.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">37.36</td>
<td id="S4.T4.17.17.19.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">9.65</td>
<td id="S4.T4.17.17.19.2.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">16.23</td>
<td id="S4.T4.17.17.19.2.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">8.64</td>
<td id="S4.T4.17.17.19.2.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">33.39</td>
<td id="S4.T4.17.17.19.2.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">21.68</td>
<td id="S4.T4.17.17.19.2.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">30.94</td>
<td id="S4.T4.17.17.19.2.11" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">35.60</td>
<td id="S4.T4.17.17.19.2.12" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">39.18</td>
<td id="S4.T4.17.17.19.2.13" class="ltx_td ltx_align_right ltx_border_t">39.27</td>
</tr>
<tr id="S4.T4.17.17.20.3" class="ltx_tr">
<th id="S4.T4.17.17.20.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">SeamlessM4T-large-v2 (2.3B)</th>
<td id="S4.T4.17.17.20.3.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T4.17.17.20.3.2.1" class="ltx_text ltx_font_bold">33.17</span></td>
<td id="S4.T4.17.17.20.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T4.17.17.20.3.3.1" class="ltx_text ltx_font_bold">23.72</span></td>
<td id="S4.T4.17.17.20.3.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T4.17.17.20.3.4.1" class="ltx_text ltx_font_bold">43.05</span></td>
<td id="S4.T4.17.17.20.3.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">21.48</td>
<td id="S4.T4.17.17.20.3.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">34.89</td>
<td id="S4.T4.17.17.20.3.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">26.04</td>
<td id="S4.T4.17.17.20.3.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T4.17.17.20.3.8.1" class="ltx_text ltx_font_bold">37.06</span></td>
<td id="S4.T4.17.17.20.3.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T4.17.17.20.3.9.1" class="ltx_text ltx_font_bold">25.41</span></td>
<td id="S4.T4.17.17.20.3.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T4.17.17.20.3.10.1" class="ltx_text ltx_font_bold">33.70</span></td>
<td id="S4.T4.17.17.20.3.11" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T4.17.17.20.3.11.1" class="ltx_text ltx_font_bold">39.96</span></td>
<td id="S4.T4.17.17.20.3.12" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T4.17.17.20.3.12.1" class="ltx_text ltx_font_bold">42.91</span></td>
<td id="S4.T4.17.17.20.3.13" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T4.17.17.20.3.13.1" class="ltx_text ltx_font_bold">42.12</span></td>
</tr>
<tr id="S4.T4.17.17.21.4" class="ltx_tr">
<th id="S4.T4.17.17.21.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Whisper-large-v3 (1.5B)</th>
<td id="S4.T4.17.17.21.4.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">N/A</td>
<td id="S4.T4.17.17.21.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">N/A</td>
<td id="S4.T4.17.17.21.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">N/A</td>
<td id="S4.T4.17.17.21.4.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">N/A</td>
<td id="S4.T4.17.17.21.4.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">N/A</td>
<td id="S4.T4.17.17.21.4.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">N/A</td>
<td id="S4.T4.17.17.21.4.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">33.40</td>
<td id="S4.T4.17.17.21.4.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">22.70</td>
<td id="S4.T4.17.17.21.4.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">31.02</td>
<td id="S4.T4.17.17.21.4.11" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">34.22</td>
<td id="S4.T4.17.17.21.4.12" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">39.20</td>
<td id="S4.T4.17.17.21.4.13" class="ltx_td ltx_align_right ltx_border_t">35.49</td>
</tr>
<tr id="S4.T4.17.17.22.5" class="ltx_tr">
<th id="S4.T4.17.17.22.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_tt">Canary (1B)</th>
<td id="S4.T4.17.17.22.5.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt">32.13</td>
<td id="S4.T4.17.17.22.5.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt">22.06</td>
<td id="S4.T4.17.17.22.5.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt">39.50</td>
<td id="S4.T4.17.17.22.5.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S4.T4.17.17.22.5.5.1" class="ltx_text ltx_font_bold">24.42</span></td>
<td id="S4.T4.17.17.22.5.6" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S4.T4.17.17.22.5.6.1" class="ltx_text ltx_font_bold">35.76</span></td>
<td id="S4.T4.17.17.22.5.7" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S4.T4.17.17.22.5.7.1" class="ltx_text ltx_font_bold">27.96</span></td>
<td id="S4.T4.17.17.22.5.8" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt">33.70</td>
<td id="S4.T4.17.17.22.5.9" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt">22.06</td>
<td id="S4.T4.17.17.22.5.10" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt">31.57</td>
<td id="S4.T4.17.17.22.5.11" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt">37.92</td>
<td id="S4.T4.17.17.22.5.12" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt">40.79</td>
<td id="S4.T4.17.17.22.5.13" class="ltx_td ltx_align_right ltx_border_b ltx_border_tt">40.58</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Automatic Speech Recognition (ASR)</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We evaluate all models across four languages on MCV-16.1¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, MLS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and VoxPopuli¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> test sets. For the baselines, we input the audios and their corresponding language IDs to the models' inference APIs. For Canary, we additionally include the special token <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">&lt;|pnc|&gt;</span> to ensure all models produce text with punctuation and capitalization.
We then normalize the ground-truth and predictions using the Whisper-Normalizer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> before calculating the word error rate (WER).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Table¬†<a href="#S3.T2" title="Table 2 ‚Ä£ 3 Experimental setup ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that the Canary model achieves the lowest WER in 10 out of 12 test sets across all languages.
On average, Canary model achieves 6.20% WER on English, 6.27% WER on German, 4.09% WER on Spanish and 5.39% WER on French. In comparison, the second best model, SeamlessM4T-large-v2, with twice as many parameters as ours, achieves 5.42% WER on English, 7.53% WER on German, 5.24% WER on Spanish and 6.65% WER on French. Figure¬†<a href="#S3.F1" title="Figure 1 ‚Ä£ 3 Experimental setup ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows that the 95% confidence intervals do not overlap for most test sets and systems, indicating that the WER improvements observed for Canary in Table¬†<a href="#S3.T2" title="Table 2 ‚Ä£ 3 Experimental setup ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> are statistically significant.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">This demonstrates the advantage of the Canary model, achieving state-of-the-art multi-lingual ASR performance with fewer parameters and less training data than contemporary models.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Further, Canary achieves the best average WER of 6.5% across different test sets, highlighting its superior generalization capabilities in English ASR (Table¬†<a href="#S4.T3" title="Table 3 ‚Ä£ 4 Results ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Speech-to-text Translation (AST)</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">To evaluate translating English audios to other languages (En <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mo stretchy="false" id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\rightarrow</annotation></semantics></math> X), we use FLEURS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and mExpresso¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, whereas FLEURS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and CoVoST¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> were used to evaluate translating audio from other languages to English (X <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mo stretchy="false" id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\rightarrow</annotation></semantics></math> En). Annotations from all test sets have punctuation and capitalization and are used without additional processing.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">From Table¬†<a href="#S4.T4" title="Table 4 ‚Ä£ 4 Results ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we notice that SeamlessM4T-large-v2 achieves the highest BLEU scores on all except mExpresso, which is expected given it has the highest number of parameters and the largest size of training data. Meanwhile, Canary model outperforms the SeamlessM4T-medium baseline, which shares similar parameter count but trained on more data, on all test sets. Compared with Whisper-large-v3, Canary achieves better results on CoVoST-2 and comparable performance on FLEURS. In addition, Canary is able to translate English audios into other languages, while Whisper-large-v3 cannot. From these results, we can see that, despite being the smallest model in its class, the Canary model achieves competitive performance on speech-to-text translation.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Long-form ASR Inference</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We investigate the performance of the Canary model on long-form audio by chunking long audios into non-overlapping 30-second segments, performing inference on each segment, and then stitching the transcripts together. We use the FastConformer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> as our baseline, and show the results on Tedlium3¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, Earnings21¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and Earnings22¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> in Table¬†<a href="#S4.T5" title="Table 5 ‚Ä£ 4.3 Long-form ASR Inference ‚Ä£ 4 Results ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. WER's for baselines are copied from the original paper¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. We can see that, although chunking is a naive method, Canary is achieves lowest WER in transcribing long-form audios. Meanwhile, adding streaming capability to Canary remains a direction for future research.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>WER(%) on long-form ASR inference, both ground-truth and predictions were processed by WhisperNormalizer¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. All models use greedy decoding. The FastConformer baseline uses a streaming mechanism, while the Canary model uses simple chunking without overlap. Canary achieves lowest WER.</figcaption>
<div id="S4.T5.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:62.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.3pt,4.9pt) scale(0.863239708761717,0.863239708761717) ;">
<table id="S4.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Model (WER <math id="S4.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T5.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T5.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Tedlium3</th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Earnings21</th>
<th id="S4.T5.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Earnings22</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.2.1" class="ltx_tr">
<th id="S4.T5.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">FastConformer-CTC (FT+LCA+GT)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</th>
<td id="S4.T5.1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">5.53</td>
<td id="S4.T5.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt">15.61</td>
<td id="S4.T5.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_tt">22.37</td>
</tr>
<tr id="S4.T5.1.1.3.2" class="ltx_tr">
<th id="S4.T5.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">FastConformer-RNNT (FT+LCA+GT)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</th>
<td id="S4.T5.1.1.3.2.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.98</td>
<td id="S4.T5.1.1.3.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">13.84</td>
<td id="S4.T5.1.1.3.2.4" class="ltx_td ltx_align_right ltx_border_t">19.49</td>
</tr>
<tr id="S4.T5.1.1.4.3" class="ltx_tr">
<th id="S4.T5.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_tt">Canary (1B)</th>
<td id="S4.T5.1.1.4.3.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S4.T5.1.1.4.3.2.1" class="ltx_text ltx_font_bold">4.68</span></td>
<td id="S4.T5.1.1.4.3.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_tt"><span id="S4.T5.1.1.4.3.3.1" class="ltx_text ltx_font_bold">11.34</span></td>
<td id="S4.T5.1.1.4.3.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_tt"><span id="S4.T5.1.1.4.3.4.1" class="ltx_text ltx_font_bold">14.34</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Hallucination Robustness</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The robustness of ASR models is evaluated on many axes, such as robustness to noise, music, background speech, and multiple speakers talking simultaneously. For AED models trained with the next token prediction objective, a particularly less-studied failure case is the generation of spurious transcripts when an audio sample is provided with long periods of silence (or contains no speech). In such cases, the expected output from the model should be an empty transcript. However, autoregressive AED models often hallucinate an unaligned transcript, especially when trained on web-scale data with insufficient filtering. In this work, we investigate the frequency of such hallucinations in the Canary model. To that end, we transcribe recordings without any speech.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Table¬†<a href="#S4.T6" title="Table 6 ‚Ä£ 4.4 Hallucination Robustness ‚Ä£ 4 Results ‚Ä£ Less is More: Accurate Speech Recognition &amp; Translation without Web-Scale Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> compares the number of hallucinated characters per minute produced by Canary with and without noise-robust training (utilizing the strongly-labeled subset of AudioSet from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>). We also include Whisper-large-v3 as an additional baseline. As shown in the table, Canary generates 16.7% fewer hallucinated characters than Whisper-large-v3, even without noise-robust training. With noise-robust training, Canary further reduces its hallucinated characters by another 26.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Number of hallucinated characters per min, measured using 48-hour non-speech audio subset from MUSAN¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Canary hallucinates the least. (Note that there is some vocals in MUSAN audios, so the actual number of hallucinated characters may be smaller.)</figcaption>
<div id="S4.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:120.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(36.2pt,-12.6pt) scale(1.26386815806653,1.26386815806653) ;">
<table id="S4.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.1.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Model</th>
<th id="S4.T6.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t"># Hallucinated Chars / min (<math id="S4.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T6.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T6.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.m1.1b"><ci id="S4.T6.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.1.1.2.1" class="ltx_tr">
<th id="S4.T6.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Whisper-large-v3</th>
<td id="S4.T6.1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_tt">114.8</td>
</tr>
<tr id="S4.T6.1.1.3.2" class="ltx_tr">
<th id="S4.T6.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<table id="S4.T6.1.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.1.3.2.1.1.1" class="ltx_tr">
<td id="S4.T6.1.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Canary (1B) w/o noise</td>
</tr>
<tr id="S4.T6.1.1.3.2.1.1.2" class="ltx_tr">
<td id="S4.T6.1.1.3.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">robust training</td>
</tr>
</table>
</th>
<td id="S4.T6.1.1.3.2.2" class="ltx_td ltx_align_right ltx_border_t">95.61</td>
</tr>
<tr id="S4.T6.1.1.4.3" class="ltx_tr">
<th id="S4.T6.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_tt">Canary (1B)</th>
<td id="S4.T6.1.1.4.3.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_tt"><span id="S4.T6.1.1.4.3.2.1" class="ltx_text ltx_font_bold">70.75</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we present Canary, a FastConformer-based encoder-decoder ASR and AST model for English, German, Spanish, and French, outperforming similarly sized models on established benchmarks.
We demonstrate that it is possible to match or exceed the performance of contemporary AST models using solely pseudo-labeled translation data.
Canary was trained using 86K hours of data‚Äîan order of magnitude less than contemporary models‚Äîwhile achieving comparable or superior metrics.
We describe effective training techniques, including encoder initialization, data balancing, and dynamic bucketing batching, enabling us to train the model in under two days.
The model and code will be open-sourced through NeMo¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A.¬†Radford, J.¬†W. Kim, T.¬†Xu, G.¬†Brockman, C.¬†McLeavey, and I.¬†Sutskever, ``Robust speech recognition via large-scale weak supervision,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv:2212.04356</em>, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A.¬†Vaswani, N.¬†Shazeer, N.¬†Parmar, J.¬†Uszkoreit, L.¬†Jones, A.¬†N. Gomez, ≈Å.¬†Kaiser, and I.¬†Polosukhin, ``Attention is all you need,'' <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">NeuRIPS</em>, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D.¬†Bahdanau, K.¬†H. Cho, and Y.¬†Bengio, ``Neural machine translation by jointly learning to align and translate,'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
L.¬†Barrault, Y.-A. Chung, M.¬†C. Meglioli, D.¬†Dale, N.¬†Dong, M.¬†Duppenthaler, P.-A. Duquenne, B.¬†Ellis, H.¬†Elsahar, J.¬†Haaheim <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``Seamless: Multilingual expressive and streaming speech translation,'' <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">arXiv:2312.05187</em>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y.¬†Peng, J.¬†Tian, B.¬†Yan, D.¬†Berrebbi, X.¬†Chang, X.¬†Li, J.¬†Shi, S.¬†Arora, W.¬†Chen, R.¬†Sharma <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``Reproducing whisper-style training using an open-source toolkit and publicly available data,'' in <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y.¬†Peng, J.¬†Tian, W.¬†Chen, S.¬†Arora, B.¬†Yan, Y.¬†Sudo, M.¬†Shakeel, K.¬†Choi, J.¬†Shi, X.¬†Chang <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``Owsm v3. 1: Better and faster open whisper-style speech models based on e-branchformer,'' <em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic">arXiv:2401.16658</em>, 2024.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
D.¬†Rekesh, N.¬†R. Koluguri, S.¬†Kriman, S.¬†Majumdar, V.¬†Noroozi, H.¬†Huang, O.¬†Hrinchuk, K.¬†Puvvada, A.¬†Kumar, J.¬†Balam <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``Fast conformer with linearly scalable attention for efficient speech recognition,'' in <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.¬†¬†¬†IEEE, 2023, pp. 1‚Äì8.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A.¬†Gulati, J.¬†Qin, C.-C. Chiu, N.¬†Parmar, Y.¬†Zhang, J.¬†Yu, W.¬†Han, S.¬†Wang, Z.¬†Zhang, Y.¬†Wu <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``Conformer: Convolution-augmented transformer for speech recognition,'' in <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">Interspeech</em>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
T.¬†Kudo and J.¬†Richardson, ``Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">EMNLP: System Demonstrations</em>, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
K.¬†Dhawan, K.¬†Rekesh, and B.¬†Ginsburg, ``Unified model for code-switching speech recognition and language identification based on concatenated tokenizer,'' in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 6th Workshop on Computational Approaches to Linguistic Code-Switching</em>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
``webdataset,'' <a target="_blank" href="https://webdataset.github.io/webdataset/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://webdataset.github.io/webdataset/</a>, accessed: 2024-03-08.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
``Lhotse shar: Storage format optimized for sequential i/o and modularity,'' <a target="_blank" href="https://colab.research.google.com/github/lhotse-speech/lhotse/blob/master/examples/04-lhotse-shar.ipynb" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://colab.research.google.com/github/lhotse-speech/lhotse/blob/master/examples/04-lhotse-shar.ipynb</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C.¬†D. Kim, B.¬†Kim, H.¬†Lee, and G.¬†Kim, ``AudioCaps: Generating captions for audios in the wild,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>.¬†¬†¬†Minneapolis, Minnesota: Association for Computational Linguistics, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
NVIDIA, ``Megatron multilingual model,'' <a target="_blank" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_en_any_500m" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_en_any_500m</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
‚Äî‚Äî, ``Megatron multilingual model,'' <a target="_blank" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_any_en_500m" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/megatronnmt_any_en_500m</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
O.¬†Hrinchuk, V.¬†Bataev, E.¬†Bakhturina, and B.¬†Ginsburg, ``NVIDIA NeMo offline speech translation systems for IWSLT 2023,'' in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IWSLT</em>, E.¬†Salesky, M.¬†Federico, and M.¬†Carpuat, Eds., 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
D.¬†Snyder, G.¬†Chen, and D.¬†Povey, ``MUSAN: A Music, Speech, and Noise Corpus,'' <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv:1510.08484</em>, 2015.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
M.¬†Bisani and H.¬†Ney, ``Bootstrap estimates for confidence intervals in asr performance evaluation,'' <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ICASSP</em>, 2004.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
P.¬†≈ªelasko, D.¬†Povey, J.¬†Trmal, and S.¬†Khudanpur, ``Lhotse: a speech data representation library for the modern deep learning ecosystem,'' in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">NeurIPS Data-Centric AI Workshop</em>, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A.¬†Babu, C.¬†Wang, A.¬†Tjandra, K.¬†Lakhotia, Q.¬†Xu, N.¬†Goyal, K.¬†Singh, P.¬†von Platen, Y.¬†Saraf, J.¬†Pino <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``Xls-r: Self-supervised cross-lingual speech representation learning at scale,'' <em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic">arXiv:2111.09296</em>, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
E.¬†Harper <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``NeMo: a toolkit for Conversational AI and Large Language Models.'' [Online]. Available: <a target="_blank" href="https://github.com/NVIDIA/NeMo" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/NVIDIA/NeMo</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
NVIDIA, ``Stt european fastconformer hybrid transducer-ctc large pnc,'' <a target="_blank" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_multilingual_fastconformer_hybrid_large_pc_blend_eu" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_multilingual_fastconformer_hybrid_large_pc_blend_eu</a>, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
V.¬†Srivastav, S.¬†Majumdar, N.¬†Koluguri, A.¬†Moumen, S.¬†Gandhi <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``Open automatic speech recognition leaderboard,'' <a target="_blank" href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/spaces/hf-audio/open_asr_leaderboard</a>, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
R.¬†Ardila, M.¬†Branson, K.¬†Davis, M.¬†Henretty, M.¬†Kohler, J.¬†Meyer, R.¬†Morais, L.¬†Saunders, F.¬†M. Tyers, and G.¬†Weber, ``Common voice: A massively-multilingual speech corpus,'' in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Conference on Language Resources and Evaluation (LREC )</em>, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
V.¬†Pratap, Q.¬†Xu, A.¬†Sriram, G.¬†Synnaeve, and R.¬†Collobert, ``Mls: A large-scale multilingual dataset for speech research,'' <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv:2012.03411</em>, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
C.¬†Wang, M.¬†Riviere, A.¬†Lee, A.¬†Wu, C.¬†Talnikar, D.¬†Haziza, M.¬†Williamson, J.¬†Pino, and E.¬†Dupoux, ``VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,'' in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, 2021, pp. 993‚Äì1003.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A.¬†Conneau, M.¬†Ma, S.¬†Khanuja, Y.¬†Zhang, V.¬†Axelrod, S.¬†Dalmia, J.¬†Riesa, C.¬†Rivera, and A.¬†Bapna, ``Fleurs: Few-shot learning evaluation of universal representations of speech,'' in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Spoken Language Technology Workshop (SLT)</em>, 2023, pp. 798‚Äì805.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
C.¬†Wang, A.¬†Wu, and J.¬†Pino, ``CoVoST 2: A massively multilingual speech-to-text translation corpus,'' <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv:2007.10310</em>, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
N.¬†R. Koluguri, S.¬†Kriman, G.¬†Zelenfroind, S.¬†Majumdar, D.¬†Rekesh, V.¬†Noroozi, J.¬†Balam, and B.¬†Ginsburg, ``Investigating end-to-end asr architectures for long form audio transcription,'' <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv:2309.09950</em>, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
F.¬†Hernandez, V.¬†Nguyen, S.¬†Ghannay, N.¬†Tomashenko, and Y.¬†Esteve, ``Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation,'' in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Speech and Computer: 20th International Conference, SPECOM</em>, 2018.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M.¬†Del¬†Rio, N.¬†Delworth, R.¬†Westerman, M.¬†Huang, N.¬†Bhandari, J.¬†Palakapilly, Q.¬†McNamara, J.¬†Dong, P.¬†Zelasko, and M.¬†Jett√©, ``Earnings-21: A practical benchmark for asr in the wild,'' <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv:2104.11348</em>, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M.¬†Del¬†Rio, P.¬†Ha, Q.¬†McNamara, C.¬†Miller, and S.¬†Chandra, ``Earnings-22: A practical benchmark for accents in the wild,'' <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv:2203.15591</em>, 2022.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.19672" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.19674" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.19674">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.19674" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.19676" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 22:50:54 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
