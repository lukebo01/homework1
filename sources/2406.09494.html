<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.09494] The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments</title><meta property="og:description" content="The DIarization of SPeaker and LAnguage in Conversational Environments (DISPLACE) 2024 challenge is the second in the series of DISPLACE challenges, which involves tasks of speaker diarization (SD) and language diariza…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.09494">

<!--Generated on Sat Jul  6 00:51:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1]Shareef BabuKalluri
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=1]PrachiSingh
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=2]Pratik Roy Chowdhuri
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=1]ApoorvaKulkarni
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>[affiliation=2]Shikha Baghel
<span id="p1.3.5" class="ltx_ERROR undefined">\name</span>[affiliation=3]Pradyoth Hegde
<span id="p1.3.6" class="ltx_ERROR undefined">\name</span>[affiliation=3]Swapnil Sontakke
<span id="p1.3.7" class="ltx_ERROR undefined">\name</span>[affiliation=3]Deepak K T
<span id="p1.3.8" class="ltx_ERROR undefined">\name</span>[affiliation=4]S. R. Mahadeva Prasanna
<span id="p1.3.9" class="ltx_ERROR undefined">\name</span>[affiliation=2]Deepu Vijayasenan
<span id="p1.3.10" class="ltx_ERROR undefined">\name</span>[affiliation=1]Sriram Ganapathy




</p>
</div>
<h1 class="ltx_title ltx_title_document">The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.3" class="ltx_p">The DIarization of SPeaker and LAnguage in Conversational Environments (DISPLACE) 2024 challenge is the second in the series of DISPLACE challenges, which involves tasks of speaker diarization (SD) and language diarization (LD) on a challenging multilingual conversational speech dataset.
In the DISPLACE 2024 challenge, we also introduced the task of automatic speech recognition (ASR) on this dataset. The dataset containing <math id="id6.1.m1.1" class="ltx_Math" alttext="158" display="inline"><semantics id="id6.1.m1.1a"><mn id="id6.1.m1.1.1" xref="id6.1.m1.1.1.cmml">158</mn><annotation-xml encoding="MathML-Content" id="id6.1.m1.1b"><cn type="integer" id="id6.1.m1.1.1.cmml" xref="id6.1.m1.1.1">158</cn></annotation-xml><annotation encoding="application/x-tex" id="id6.1.m1.1c">158</annotation></semantics></math> hours of speech, consisting of both supervised and unsupervised mono-channel far-field recordings, was released for LD and SD tracks. Further, <math id="id7.2.m2.1" class="ltx_Math" alttext="12" display="inline"><semantics id="id7.2.m2.1a"><mn id="id7.2.m2.1.1" xref="id7.2.m2.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="id7.2.m2.1b"><cn type="integer" id="id7.2.m2.1.1.cmml" xref="id7.2.m2.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="id7.2.m2.1c">12</annotation></semantics></math> hours of close-field mono-channel recordings were provided for the ASR track conducted on <math id="id8.3.m3.1" class="ltx_Math" alttext="5" display="inline"><semantics id="id8.3.m3.1a"><mn id="id8.3.m3.1.1" xref="id8.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="id8.3.m3.1b"><cn type="integer" id="id8.3.m3.1.1.cmml" xref="id8.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="id8.3.m3.1c">5</annotation></semantics></math> Indian languages.
The details of the dataset, baseline systems and the leader board results are highlighted in this paper.
We have also compared our baseline models and the team's performances on evaluation data of DISPLACE-2023 to emphasize the advancements made in this second version of the challenge.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Speaker diarization, language diarization, ASR, code-mixing, conversational speech, DISPLACE challenge.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>This work was funded by the project National Language Translation Mission (NLTM): BHASHINI, the Ministry of Electronics and Information Technology (MeitY), Government of India SP/MITO-22-001 grant, and the British Telecom (BT) grants on conversational speech analytics.</span></span></span>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In multilingual cultures, social interactions frequently comprise code-mixed or code-switched speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.<span id="S1.p1.1.1" class="ltx_text ltx_font_italic"> Code-mixing</span> occurs when morphemes or words from a secondary language are utilized in a primary language phrase
(Eg.: `<span id="S1.p1.1.2" class="ltx_text ltx_font_italic">`Wirst du mitmachen, um mit mir ein IPL-Match anzusehen?</span>'' (Will you join to watch IPL match with me ?)).
In contrast, <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">code-switching</span> involves modifying the conversational language itself at the sentence or phrase level. (Eg.:
`` <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">I'm busy today but, Ich kann beim nächsten Spiel mitmachen</span>''
(I'm busy today but, I can join for next match)).
In multilingual communities like Asia, Europe, America and some parts of the African continent, code-mixing and code-switching are more frequent in social conversations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The code-mixed or code-switched instances pose significant challenges for speech-based systems, such as speaker and language identification or automatic speech recognition (ASR). In multi-speaker and multilingual scenarios, the task of identifying ``<span id="S1.p2.1.1" class="ltx_text ltx_font_italic">who spoke when</span>" and ``<span id="S1.p2.1.2" class="ltx_text ltx_font_italic">which language was spoken when</span>", termed as speaker diarization (SD) and language diarization (LD) respectively, are significantly challenging. We find that the current speech processing systems are ill-equipped to perform these tasks meaningfully <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we detail our efforts in extending the first DISPLACE challenge conducted in <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="2023" display="inline"><semantics id="S1.p3.1.m1.1a"><mn id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">2023</mn><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><cn type="integer" id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">2023</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">2023</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
We created a dataset for the second <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">DISPLACE</span> challenge<span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://displace2024.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://displace2024.github.io/</a></span></span></span> from four different academic institutes preserving the same recording settings for the entire dataset. The data reflects the social interactions in multilingual communities with code-mixed or code-switched speech, natural overlaps, reverberation, and noise.
The key highlights of the DISPLACE-2024 challenge are,</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Introducing the track on speech recognition which attempts to investigate speech transcription in code-mixed multi-speaker settings on <math id="S1.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S1.I1.i1.p1.1.m1.1a"><mn id="S1.I1.i1.p1.1.m1.1.1" xref="S1.I1.i1.p1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i1.p1.1.m1.1b"><cn type="integer" id="S1.I1.i1.p1.1.m1.1.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.p1.1.m1.1c">5</annotation></semantics></math> different languages.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.2" class="ltx_p">Releasing <math id="S1.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="38" display="inline"><semantics id="S1.I1.i2.p1.1.m1.1a"><mn id="S1.I1.i2.p1.1.m1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.cmml">38</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.1.m1.1b"><cn type="integer" id="S1.I1.i2.p1.1.m1.1.1.cmml" xref="S1.I1.i2.p1.1.m1.1.1">38</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.1.m1.1c">38</annotation></semantics></math> hours of annotated data and <math id="S1.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="120" display="inline"><semantics id="S1.I1.i2.p1.2.m2.1a"><mn id="S1.I1.i2.p1.2.m2.1.1" xref="S1.I1.i2.p1.2.m2.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.2.m2.1b"><cn type="integer" id="S1.I1.i2.p1.2.m2.1.1.cmml" xref="S1.I1.i2.p1.2.m2.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.2.m2.1c">120</annotation></semantics></math> hours of unsupervised data.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Updated baseline systems on speaker and language diarization, which improved the benchmark significantly over the DISPLACE-2023 challenge.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">A leader-board platform
<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://codalab.lisn.upsaclay.fr/competitions/17682" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://codalab.lisn.upsaclay.fr/competitions/17682</a></span></span></span>
for all <math id="S1.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S1.I1.i4.p1.1.m1.1a"><mn id="S1.I1.i4.p1.1.m1.1.1" xref="S1.I1.i4.p1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i4.p1.1.m1.1b"><cn type="integer" id="S1.I1.i4.p1.1.m1.1.1.cmml" xref="S1.I1.i4.p1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i4.p1.1.m1.1c">3</annotation></semantics></math> tracks for participants to monitor their progress in system development.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.09494/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="79" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">The DISPLACE 2024 Challenge Timeline.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Speaker Diarization</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Early research in speaker diarization was predominantly influenced by the evaluations conducted by the National Institute of Standards and Technology-Rich Transcription (NIST-RT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> on broadcast news (BN) and informal telephone conversations in English.
The Diarization Error Rate (DER), which continues to be the primary evaluation metric for SD systems, was also proposed during the NIST-RT evaluations.
A series of SD works by Fiscus et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> used in-domain conversations from meetings.
Recently, there have been several evaluation challenges on SD, namely, the DIHARD challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, VoxSRC-20 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> using YouTube videos and Fearless Steps Series <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> in multi-party and multi-stream naturalistic audio.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Language Diarization</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">For the past decade, language diarization has been one of the notable research domains in the multilingual speech-processing research community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
Works carried out for LD tasks mainly used broadcast datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and recorded in closed environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
Recently, MERLIon CCS Challenge on language identification and diarization used code-switched child-directed speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Automatic Speech Recognition</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">ASR systems are majorly developed for monolingual scenarios in clean speech data and are less efficient on low-resourced speech data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. ASR Challenges at Interspeech <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="2018" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mn id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">2018</mn><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><cn type="integer" id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">2018</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">2018</annotation></semantics></math> have targeted read-out and conversational speech data in low-resource Indian languages like Tamil, Telugu, and Gujarati <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and scripted telephone conversations of Assamese, Bengali, Tamil, Urdu, and Hindi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. A series of ASR challenges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, on Tamil, Hindi, and Indian-English targeted lecture data (formal style) and read speech. But code-switching scenarios were explored in limited languages like Bengali-English, Hindi-English <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
There have been efforts in non-Indian languages, such as English and Arabic, to tackle speech recognition in code-switching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and the Oriental Language Recognition challenge for multilingual societies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Despite several recent efforts to provide real labeled conversational datasets through various challenges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, there is still a need for a more generic dataset that can capture multi-speaker, multilingual, code-switched scenarios that are common in day-to-day conversations.
DISPLACE 2024 challenge tries to address this need by providing a natural multilingual conversational dataset without any restrictions on speakers, languages, topics, etc. The challenge evaluates the SD, LD, and ASR performance on the same dataset to make it more meaningful.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>DISPLACE Corpus Details </h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.4" class="ltx_p">DISPLACE corpus has been collected from four different academic institutions. Each institute followed the same recording protocol and instrument specifications but differed in the recording room settings like size, shape, and acoustic properties.
The recorded conversations contained different speakers across different age groups and different regions of India.
The procedure in participant selection ensured their ability to converse fluently with each other in one of the Indian languages (L1) along with Indian-English.
Each recording session lasted about <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S3.p1.1.m1.1a"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><cn type="integer" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">30</annotation></semantics></math>-<math id="S3.p1.2.m2.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S3.p1.2.m2.1a"><mn id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><cn type="integer" id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">60</annotation></semantics></math> minutes, with <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.p1.3.m3.1a"><mn id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><cn type="integer" id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">3</annotation></semantics></math>-<math id="S3.p1.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.p1.4.m4.1a"><mn id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><cn type="integer" id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">5</annotation></semantics></math> participants conversing in their native language, along with natural code-mixing and code-switching instances of Indian English.
The topics for the conversations were mainly about culture, lifestyles, entertainment, and sports, excluding emotionally sensitive or personal topics.
The data is collected using lapel microphones and omnidirectional microphone recorders for close-field and far-field recordings respectively. More details of the recording setup and participant selection are given in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Professional annotators were hired to label the data using close-field recordings for all the targeted speakers in a session. This annotation process involves marking the details of the speaker, language, and transcript of speech region. A single Rich Transcription Time Marked (RTTM) annotation file is generated for each conversation by combining the annotations obtained from all the participants' lapel microphones. Multiple quality checks were performed on the annotations before we released them to the participants. More details on the data annotation and labeling can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Development and Evaluation set </h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In DISPLACE 2024 challenge, we have released development and evaluation data to the participants through Zenodo (a cloud service) with a password-protected link<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://zenodo.org/records/10669296" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://zenodo.org/records/10669296</a></span></span></span>. As was done for the DISPLACE-2023 challenge, no training data was provided to the participants, and they were allowed to use any public data resources and /or proprietary data to train their systems.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.12" class="ltx_p">In this challenge, we have released both supervised (labeled) and unsupervised (unlabeled) data containing <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="666" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">666</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn type="integer" id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">666</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">666</annotation></semantics></math> unique speakers, out of which <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="459" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">459</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><cn type="integer" id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">459</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">459</annotation></semantics></math> are male and <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="207" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mn id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">207</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><cn type="integer" id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">207</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">207</annotation></semantics></math> are female speakers.
All the speakers fall in the age group of <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="17" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mn id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">17</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><cn type="integer" id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">17</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">17</annotation></semantics></math>-<math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="65" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mn id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">65</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><cn type="integer" id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">65</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">65</annotation></semantics></math> years.
There are <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mn id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><cn type="integer" id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">9</annotation></semantics></math> different Indian languages spoken in the <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="237" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mn id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml">237</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><cn type="integer" id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">237</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">237</annotation></semantics></math> sessions, along with Indian English.
For the second DISPLACE challenge, we have combined the previous challenge's annotated data with <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><mn id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><cn type="integer" id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">20</annotation></semantics></math>% of additional annotated data.
In total, we have annotated data of <math id="S3.SS1.p2.9.m9.1" class="ltx_Math" alttext="38" display="inline"><semantics id="S3.SS1.p2.9.m9.1a"><mn id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml">38</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><cn type="integer" id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">38</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">38</annotation></semantics></math> hours of conversational speech from <math id="S3.SS1.p2.10.m10.1" class="ltx_Math" alttext="67" display="inline"><semantics id="S3.SS1.p2.10.m10.1a"><mn id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml">67</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><cn type="integer" id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1">67</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">67</annotation></semantics></math> sessions with <math id="S3.SS1.p2.11.m11.1" class="ltx_Math" alttext="197" display="inline"><semantics id="S3.SS1.p2.11.m11.1a"><mn id="S3.SS1.p2.11.m11.1.1" xref="S3.SS1.p2.11.m11.1.1.cmml">197</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.1b"><cn type="integer" id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1">197</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.1c">197</annotation></semantics></math> speakers.
The data is pre-processed for volume normalization, all the close-field recordings are time-aligned with the far-field recordings, and the audio is resampled to <math id="S3.SS1.p2.12.m12.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S3.SS1.p2.12.m12.1a"><mn id="S3.SS1.p2.12.m12.1.1" xref="S3.SS1.p2.12.m12.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m12.1b"><cn type="integer" id="S3.SS1.p2.12.m12.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m12.1c">16</annotation></semantics></math> kHz and normalized to the [-1,1] range.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.5" class="ltx_p"><span id="S3.SS1.p3.5.1" class="ltx_text ltx_font_bold ltx_font_italic">Development set:</span> We released far-field supervised (labeled) and unsupervised (unlabeled) conversational data of around <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="140" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mn id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">140</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><cn type="integer" id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">140</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">140</annotation></semantics></math> hours as a part of the development (Dev) set to all the registered participants for SD and LD tracks. For the ASR task, we released <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mn id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><cn type="integer" id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">4</annotation></semantics></math> hours of supervised data for both far-field and close-field recordings.
We have released annotated data of <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mn id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><cn type="integer" id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">20</annotation></semantics></math> hours of conversational speech from <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="35" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mn id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">35</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><cn type="integer" id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">35</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">35</annotation></semantics></math> sessions with <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="98" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mn id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">98</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><cn type="integer" id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">98</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">98</annotation></semantics></math> speakers as a part of supervised Dev data for diarization tracks. The sessions chosen for speaker and language diarization tracks are identical, and the labels are provided separately for each track in RTTM format.
We have released supervised data for the ASR track for four Indian languages (i.e., Bengali, Hindi, Kannada, Telugu) and Indian-accented English. Each native language has one hour of labeled transcripts for close and far-field recordings. For the ASR track, we provided the segment files to identify the speech regions along with the transcript labels in native language script.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.3" class="ltx_p"><span id="S3.SS1.p4.3.1" class="ltx_text ltx_font_bold ltx_font_italic">Unsupervised Data:</span> 
We released unsupervised data in the second DISPLACE challenge.
In total, we distributed more than <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="120" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mn id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><cn type="integer" id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">120</annotation></semantics></math> hours of conversational speech data from <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="170" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mn id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">170</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><cn type="integer" id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">170</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">170</annotation></semantics></math> sessions with <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="493" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mn id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">493</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><cn type="integer" id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">493</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">493</annotation></semantics></math> speakers from Indian languages like Hindi, Bengali, Kannada, Telugu, Malayalam, Tamil, Marathi, Assamese, Odiya and Indian English. This unsupervised data was meant to be useful for adapting the models to the environmental conditions of the DISPLACE evaluation data for all three tracks.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_align_left">
<p id="S3.SS1.p5.4" class="ltx_p"><span id="S3.SS1.p5.4.1" class="ltx_text ltx_font_bold ltx_font_italic">Evaluation set:</span> 
As a part of the evaluation (Eval) set, we released the supervised conversational data of <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="18" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mn id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><cn type="integer" id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">18</annotation></semantics></math> hours from <math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="99" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><mn id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">99</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><cn type="integer" id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">99</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">99</annotation></semantics></math> speakers in <math id="S3.SS1.p5.3.m3.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S3.SS1.p5.3.m3.1a"><mn id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><cn type="integer" id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">32</annotation></semantics></math> sessions. We use the same far-field evaluation data for SD and LD tasks. For the ASR task, we have released <math id="S3.SS1.p5.4.m4.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS1.p5.4.m4.1a"><mn id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.1b"><cn type="integer" id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.1c">8</annotation></semantics></math> hours of the close-field recordings.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Challenge Tasks and Organisation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Participants were encouraged to build their own speech activity detection systems. The submissions are evaluated based on speech regions ignoring non-speech regions like background speech, noise, laughing, or clapping.
The second DISPLACE challenge had the following tracks,</p>
</div>
<div id="S4.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Track-1:</span> Speaker Diarization in multilingual scenarios.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Track-2:</span> Language Diarization in multi-speaker settings.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Track-3:</span> ASR on single-speaker code-mixed settings.</p>
</div>
</li>
</ul>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The metric for evaluating the system performance of diarization systems is the diarization error rate (DER) while word error rate (WER) is used for evaluating speech recognition systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
The challenge's timeline is shown in Fig <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span> Baseline Systems</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Speaker Diarization</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.3" class="ltx_p">The speaker diarization system follows the steps outlined in the DISPLACE 2023 challenge baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, which includes speech activity detection, segmenting audio into <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="1.5" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mn id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">1.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><cn type="float" id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">1.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">1.5</annotation></semantics></math>s chunks with a <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="250" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mn id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><cn type="integer" id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">250</annotation></semantics></math>ms shift, followed by extracting x-vectors using a pre-trained <math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="13" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><mn id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><cn type="integer" id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">13</annotation></semantics></math>-layer ETDNN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. These x-vectors are utilized to generate PLDA similarity scores for spectral clustering. The number of speakers is decided based on the threshold set according to the best DER on the dev set. We perform Variational Bayes (VB) re-segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> using the clustering results to generate the final output. Compared to the previous year's baseline, we have integrated Pyannote speech activity detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and overlap detection modules. Based on the posterior probabilities from the VB-hidden Markov model (HMM) module and the output of the overlap detection system, we predict up to two speakers for each audio segment.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Language Diarization</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.6" class="ltx_p">The baseline system follows a two-stage methodology, comprising feature extraction and clustering. We used the Pyannote speech activity detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> for detecting the speech regions.
We then segment the speech regions into short overlapping segments of <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mn id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><cn type="integer" id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">10</annotation></semantics></math>s with a <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mn id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><cn type="integer" id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">200</annotation></semantics></math>ms shift.
Unlike DISPLACE 2023 baseline, which relied on language identification (LID) ECAPA-TDNN model embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we employ a deep multitask model called Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for the feature extraction.
The Whisper model is trained on the Voxlingua107 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, containing data from <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="99" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mn id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">99</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><cn type="integer" id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">99</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">99</annotation></semantics></math> languages and <math id="S5.SS2.p1.4.m4.1" class="ltx_Math" alttext="6628" display="inline"><semantics id="S5.SS2.p1.4.m4.1a"><mn id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml">6628</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><cn type="integer" id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1">6628</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">6628</annotation></semantics></math> hours. Notably, the LID system is trained on <math id="S5.SS2.p1.5.m5.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S5.SS2.p1.5.m5.1a"><mn id="S5.SS2.p1.5.m5.1.1" xref="S5.SS2.p1.5.m5.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><cn type="integer" id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">30</annotation></semantics></math>s speech segments.
Upon feature extraction, the Whisper language detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> furnishes <math id="S5.SS2.p1.6.m6.1" class="ltx_Math" alttext="99" display="inline"><semantics id="S5.SS2.p1.6.m6.1a"><mn id="S5.SS2.p1.6.m6.1.1" xref="S5.SS2.p1.6.m6.1.1.cmml">99</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.6.m6.1b"><cn type="integer" id="S5.SS2.p1.6.m6.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1">99</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.6.m6.1c">99</annotation></semantics></math> language posterior probabilities. These extracted posterior features serve as input for an agglomerative clustering algorithm.
We use a cosine similarity-based distance metric computed using the posteriors from the Whisper model for clustering.
Subsequently, we enhance our results by employing a Variational Bayes x-vector (VBx) model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> for further refinement.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.2" class="ltx_p">We tested our baseline system with DISPLACE-2023 data and report the development (dev) and evaluation (Eval) DER for both the models (DISPLACE-2023 and the DISPLACE-2024 baseline system) in Table <a href="#S5.T1" title="Table 1 ‣ 5.2 Language Diarization ‣ 5 Baseline Systems ‣ The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We observe absolute improvement of <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="5.94" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mn id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">5.94</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><cn type="float" id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">5.94</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">5.94</annotation></semantics></math>% on dev and <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="12.11" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mn id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">12.11</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><cn type="float" id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">12.11</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">12.11</annotation></semantics></math>% on the Eval data, respectively, in terms of DER for the LD task, whereas it is more than 2% absolute improvement in DER for SD task on both dev and Eval sets.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.7.2.1" class="ltx_text" style="font-size:129%;">Table 1</span>: </span><span id="S5.T1.2.1" class="ltx_text" style="font-size:129%;">DER (<math id="S5.T1.2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.T1.2.1.m1.1b"><mo id="S5.T1.2.1.m1.1.1" xref="S5.T1.2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.1.m1.1c"><csymbol cd="latexml" id="S5.T1.2.1.m1.1.1.cmml" xref="S5.T1.2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.1.m1.1d">\%</annotation></semantics></math>) comparison of LD and SD baseline systems using DISPLACE 2023 and DISPLACE 2024 challenge models on DISPLACE 2023 data. </span></figcaption>
<table id="S5.T1.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.8.1.1" class="ltx_tr">
<th id="S5.T1.8.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-top:0.35pt;padding-bottom:0.35pt;"></th>
<th id="S5.T1.8.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:0.35pt;padding-bottom:0.35pt;" colspan="2"><span id="S5.T1.8.1.1.2.1" class="ltx_text" style="font-size:70%;">LD</span></th>
<th id="S5.T1.8.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.35pt;padding-bottom:0.35pt;" colspan="2"><span id="S5.T1.8.1.1.3.1" class="ltx_text" style="font-size:70%;">SD</span></th>
</tr>
<tr id="S5.T1.8.2.2" class="ltx_tr">
<th id="S5.T1.8.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.2.2.1.1" class="ltx_text" style="font-size:70%;">Baseline</span></th>
<th id="S5.T1.8.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.2.2.2.1" class="ltx_text" style="font-size:70%;">Dev</span></th>
<th id="S5.T1.8.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.2.2.3.1" class="ltx_text" style="font-size:70%;">Eval</span></th>
<th id="S5.T1.8.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.2.2.4.1" class="ltx_text" style="font-size:70%;">Dev</span></th>
<th id="S5.T1.8.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.2.2.5.1" class="ltx_text" style="font-size:70%;">Eval</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.8.3.1" class="ltx_tr">
<th id="S5.T1.8.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;">
<span id="S5.T1.8.3.1.1.1" class="ltx_text" style="font-size:70%;">DISPLACE-2023 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T1.8.3.1.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S5.T1.8.3.1.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</th>
<td id="S5.T1.8.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.3.1.2.1" class="ltx_text" style="font-size:70%;">46.95</span></td>
<td id="S5.T1.8.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.3.1.3.1" class="ltx_text" style="font-size:70%;">41.67</span></td>
<td id="S5.T1.8.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.3.1.4.1" class="ltx_text" style="font-size:70%;">27.33</span></td>
<td id="S5.T1.8.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.3.1.5.1" class="ltx_text" style="font-size:70%;">32.18</span></td>
</tr>
<tr id="S5.T1.8.4.2" class="ltx_tr">
<th id="S5.T1.8.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.4.2.1.1" class="ltx_text" style="font-size:70%;">DISPLACE-2024</span></th>
<td id="S5.T1.8.4.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.4.2.2.1" class="ltx_text" style="font-size:70%;">41.01</span></td>
<td id="S5.T1.8.4.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.4.2.3.1" class="ltx_text" style="font-size:70%;">29.56</span></td>
<td id="S5.T1.8.4.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.4.2.4.1" class="ltx_text" style="font-size:70%;">25.45</span></td>
<td id="S5.T1.8.4.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T1.8.4.2.5.1" class="ltx_text" style="font-size:70%;">29.96</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Automatic Speech Recognition</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">As part of the dev and eval phases, we have released Hindi, Bengali, Kannada, and Telugu conversations. We have provided the segment boundaries and ground truth transcripts for the dev data, while only segment boundary labels were released for the eval data.
As a part of the baseline system for ASR track, we have implemented the Google Speech-to-Text cloud services using the close field recordings of development data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
To compute the WER, we use Sclite toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and the WER for dev and eval sets are tabulated in Table <a href="#S5.T2" title="Table 2 ‣ 5.3 Automatic Speech Recognition ‣ 5 Baseline Systems ‣ The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
As the data was significantly challenging, we observe that the baseline WER is similar to those observed in other ASR challenges like CHiME-6 multi-speaker far-field conversational dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Furthermore, the DISPLACE data had code-switched speech, making the ASR task even more demanding.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.4.1.1" class="ltx_text" style="font-size:129%;">Table 2</span>: </span><span id="S5.T2.5.2" class="ltx_text" style="font-size:129%;">Comparison of Track 3- ASR baseline WER with top performing team </span></figcaption>
<table id="S5.T2.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.6.1.1" class="ltx_tr">
<th id="S5.T2.6.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-top:0.7pt;padding-bottom:0.7pt;"></th>
<th id="S5.T2.6.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:0.7pt;padding-bottom:0.7pt;" colspan="2"><span id="S5.T2.6.1.1.2.1" class="ltx_text" style="font-size:70%;">Baseline</span></th>
<th id="S5.T2.6.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.7pt;padding-bottom:0.7pt;" colspan="2"><span id="S5.T2.6.1.1.3.1" class="ltx_text" style="font-size:70%;">T1</span></th>
</tr>
<tr id="S5.T2.6.2.2" class="ltx_tr">
<th id="S5.T2.6.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.2.2.1.1" class="ltx_text" style="font-size:70%;">Language</span></th>
<th id="S5.T2.6.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.2.2.2.1" class="ltx_text" style="font-size:70%;">Dev</span></th>
<th id="S5.T2.6.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.2.2.3.1" class="ltx_text" style="font-size:70%;">Eval</span></th>
<th id="S5.T2.6.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.2.2.4.1" class="ltx_text" style="font-size:70%;">Dev</span></th>
<th id="S5.T2.6.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.2.2.5.1" class="ltx_text" style="font-size:70%;">Eval</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.6.3.1" class="ltx_tr">
<th id="S5.T2.6.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.3.1.1.1" class="ltx_text" style="font-size:70%;">Bengali</span></th>
<td id="S5.T2.6.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.3.1.2.1" class="ltx_text" style="font-size:70%;">63.5</span></td>
<td id="S5.T2.6.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.3.1.3.1" class="ltx_text" style="font-size:70%;">75.23</span></td>
<td id="S5.T2.6.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.3.1.4.1" class="ltx_text" style="font-size:70%;">54.79</span></td>
<td id="S5.T2.6.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.3.1.5.1" class="ltx_text" style="font-size:70%;">63.87</span></td>
</tr>
<tr id="S5.T2.6.4.2" class="ltx_tr">
<th id="S5.T2.6.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.4.2.1.1" class="ltx_text" style="font-size:70%;">Hindi</span></th>
<td id="S5.T2.6.4.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.4.2.2.1" class="ltx_text" style="font-size:70%;">58.5</span></td>
<td id="S5.T2.6.4.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.4.2.3.1" class="ltx_text" style="font-size:70%;">60.43</span></td>
<td id="S5.T2.6.4.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.4.2.4.1" class="ltx_text" style="font-size:70%;">38.90</span></td>
<td id="S5.T2.6.4.2.5" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.4.2.5.1" class="ltx_text" style="font-size:70%;">52.27</span></td>
</tr>
<tr id="S5.T2.6.5.3" class="ltx_tr">
<th id="S5.T2.6.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.5.3.1.1" class="ltx_text" style="font-size:70%;">Kannada</span></th>
<td id="S5.T2.6.5.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.5.3.2.1" class="ltx_text" style="font-size:70%;">80.8</span></td>
<td id="S5.T2.6.5.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.5.3.3.1" class="ltx_text" style="font-size:70%;">81.08</span></td>
<td id="S5.T2.6.5.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.5.3.4.1" class="ltx_text" style="font-size:70%;">65.14</span></td>
<td id="S5.T2.6.5.3.5" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.5.3.5.1" class="ltx_text" style="font-size:70%;">62.88</span></td>
</tr>
<tr id="S5.T2.6.6.4" class="ltx_tr">
<th id="S5.T2.6.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.6.4.1.1" class="ltx_text" style="font-size:70%;">Telugu</span></th>
<td id="S5.T2.6.6.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.6.4.2.1" class="ltx_text" style="font-size:70%;">71.2</span></td>
<td id="S5.T2.6.6.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.6.4.3.1" class="ltx_text" style="font-size:70%;">66.92</span></td>
<td id="S5.T2.6.6.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.6.4.4.1" class="ltx_text" style="font-size:70%;">59.35</span></td>
<td id="S5.T2.6.6.4.5" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.6.4.5.1" class="ltx_text" style="font-size:70%;">57.87</span></td>
</tr>
<tr id="S5.T2.6.7.5" class="ltx_tr">
<th id="S5.T2.6.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.7.5.1.1" class="ltx_text" style="font-size:70%;">Indian-English</span></th>
<td id="S5.T2.6.7.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.7.5.2.1" class="ltx_text" style="font-size:70%;">66.5</span></td>
<td id="S5.T2.6.7.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.7.5.3.1" class="ltx_text" style="font-size:70%;">66.76</span></td>
<td id="S5.T2.6.7.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.7.5.4.1" class="ltx_text" style="font-size:70%;">25.53</span></td>
<td id="S5.T2.6.7.5.5" class="ltx_td ltx_align_center" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.7.5.5.1" class="ltx_text" style="font-size:70%;">34.56</span></td>
</tr>
<tr id="S5.T2.6.8.6" class="ltx_tr">
<th id="S5.T2.6.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.8.6.1.1" class="ltx_text" style="font-size:70%;">Overall</span></th>
<td id="S5.T2.6.8.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.8.6.2.1" class="ltx_text" style="font-size:70%;">66.7</span></td>
<td id="S5.T2.6.8.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.8.6.3.1" class="ltx_text" style="font-size:70%;">67.78</span></td>
<td id="S5.T2.6.8.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.8.6.4.1" class="ltx_text" style="font-size:70%;">37.30</span></td>
<td id="S5.T2.6.8.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:0.7pt;padding-bottom:0.7pt;"><span id="S5.T2.6.8.6.5.1" class="ltx_text" style="font-size:70%;">47.34</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2406.09494/assets/x2.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="207" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.4.2.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S5.F2.2.1" class="ltx_text" style="font-size:90%;">DER (<math id="S5.F2.2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.F2.2.1.m1.1b"><mo id="S5.F2.2.1.m1.1.1" xref="S5.F2.2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.F2.2.1.m1.1c"><csymbol cd="latexml" id="S5.F2.2.1.m1.1.1.cmml" xref="S5.F2.2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.2.1.m1.1d">\%</annotation></semantics></math>) comparison of best of Track 1 Eval Phases of Teams submissions with baseline.</span></figcaption>
</figure>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2406.09494/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="207" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.4.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.2.1" class="ltx_text" style="font-size:90%;">DER (<math id="S5.F3.2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.F3.2.1.m1.1b"><mo id="S5.F3.2.1.m1.1.1" xref="S5.F3.2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.F3.2.1.m1.1c"><csymbol cd="latexml" id="S5.F3.2.1.m1.1.1.cmml" xref="S5.F3.2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.2.1.m1.1d">\%</annotation></semantics></math>) comparison of best of Track 2 Eval Phases of Teams submissions with baseline.</span></figcaption>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.7.2.1" class="ltx_text" style="font-size:129%;">Table 3</span>: </span><span id="S5.T3.2.1" class="ltx_text" style="font-size:129%;">Comparison of top 3 teams model submissions for SD and LD tracks in terms of DER (<math id="S5.T3.2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S5.T3.2.1.m1.1b"><mo id="S5.T3.2.1.m1.1.1" xref="S5.T3.2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.1.m1.1c"><csymbol cd="latexml" id="S5.T3.2.1.m1.1.1.cmml" xref="S5.T3.2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.1.m1.1d">\%</annotation></semantics></math>), DISPLACE-2024 vs DISPLACE 2023 models using DISPLACE-2023 challenge Eval data.</span></figcaption>
<table id="S5.T3.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.8.1.1" class="ltx_tr">
<th id="S5.T3.8.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:0.35pt;padding-bottom:0.35pt;"></th>
<th id="S5.T3.8.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" style="padding-top:0.35pt;padding-bottom:0.35pt;" colspan="2"><span id="S5.T3.8.1.1.2.1" class="ltx_text" style="font-size:70%;">Track-1:SD</span></th>
<th id="S5.T3.8.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.35pt;padding-bottom:0.35pt;" colspan="2"><span id="S5.T3.8.1.1.3.1" class="ltx_text" style="font-size:70%;">Track-2:LD</span></th>
</tr>
<tr id="S5.T3.8.2.2" class="ltx_tr">
<th id="S5.T3.8.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.2.2.1.1" class="ltx_text" style="font-size:70%;">Team No.</span></th>
<th id="S5.T3.8.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.2.2.2.1" class="ltx_text" style="font-size:70%;">2024</span></th>
<th id="S5.T3.8.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.2.2.3.1" class="ltx_text" style="font-size:70%;">2023</span></th>
<th id="S5.T3.8.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.2.2.4.1" class="ltx_text" style="font-size:70%;">2024</span></th>
<th id="S5.T3.8.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.2.2.5.1" class="ltx_text" style="font-size:70%;">2023</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.8.3.1" class="ltx_tr">
<td id="S5.T3.8.3.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.3.1.1.1" class="ltx_text" style="font-size:70%;">T1</span></td>
<td id="S5.T3.8.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.3.1.2.1" class="ltx_text" style="font-size:70%;">21.27</span></td>
<td id="S5.T3.8.3.1.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.3.1.3.1" class="ltx_text" style="font-size:70%;">27.8</span></td>
<td id="S5.T3.8.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.3.1.4.1" class="ltx_text" style="font-size:70%;">25.05</span></td>
<td id="S5.T3.8.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.3.1.5.1" class="ltx_text" style="font-size:70%;">37.6</span></td>
</tr>
<tr id="S5.T3.8.4.2" class="ltx_tr">
<td id="S5.T3.8.4.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.4.2.1.1" class="ltx_text" style="font-size:70%;">T2</span></td>
<td id="S5.T3.8.4.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.4.2.2.1" class="ltx_text" style="font-size:70%;">24.61</span></td>
<td id="S5.T3.8.4.2.3" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.4.2.3.1" class="ltx_text" style="font-size:70%;">28.6</span></td>
<td id="S5.T3.8.4.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.4.2.4.1" class="ltx_text" style="font-size:70%;">28.57</span></td>
<td id="S5.T3.8.4.2.5" class="ltx_td ltx_align_center" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.4.2.5.1" class="ltx_text" style="font-size:70%;">40.2</span></td>
</tr>
<tr id="S5.T3.8.5.3" class="ltx_tr">
<td id="S5.T3.8.5.3.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.5.3.1.1" class="ltx_text" style="font-size:70%;">T3</span></td>
<td id="S5.T3.8.5.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.5.3.2.1" class="ltx_text" style="font-size:70%;">24.43</span></td>
<td id="S5.T3.8.5.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.5.3.3.1" class="ltx_text" style="font-size:70%;">31.5</span></td>
<td id="S5.T3.8.5.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.5.3.4.1" class="ltx_text" style="font-size:70%;">29.83</span></td>
<td id="S5.T3.8.5.3.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.35pt;padding-bottom:0.35pt;"><span id="S5.T3.8.5.3.5.1" class="ltx_text" style="font-size:70%;">41.2</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Summary of Challenge Results</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Track 1: Speaker Diarization</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.4" class="ltx_p">A total of <math id="S6.SS1.p1.1.m1.1" class="ltx_Math" alttext="11" display="inline"><semantics id="S6.SS1.p1.1.m1.1a"><mn id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><cn type="integer" id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">11</annotation></semantics></math> teams participated in Eval-Phase 1 &amp; Eval-Phase 2 and eight teams outperformed the baseline. Figure <a href="#S5.F2" title="Figure 2 ‣ 5.3 Automatic Speech Recognition ‣ 5 Baseline Systems ‣ The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the DER distribution across the teams.
We observe a significant boost in the DER performance in <span id="S6.SS1.p1.4.1" class="ltx_text ltx_font_italic">DISPLACE-2024</span> compared to the best DER obtained in DISPLACE-2023 challenge (absolute improvement of <math id="S6.SS1.p1.2.m2.1" class="ltx_Math" alttext="6.53" display="inline"><semantics id="S6.SS1.p1.2.m2.1a"><mn id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml">6.53</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.1b"><cn type="float" id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1">6.53</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.1c">6.53</annotation></semantics></math>% in DER reported in Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Automatic Speech Recognition ‣ 5 Baseline Systems ‣ The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
<br class="ltx_break"><span id="S6.SS1.p1.4.2" class="ltx_text ltx_font_bold">Top performing systems</span>: The best single system in T1 submission is the Wav-LM-based speaker segmentation model from <span id="S6.SS1.p1.4.3" class="ltx_text ltx_font_italic">pyannote.audio</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> fine-tuned on the <span id="S6.SS1.p1.4.4" class="ltx_text ltx_font_italic">DISPLACE-2024</span> Dev set. The model comprises of end-to-end diarization step followed by global clustering, where the number of speakers is restricted to <math id="S6.SS1.p1.3.m3.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S6.SS1.p1.3.m3.1a"><mn id="S6.SS1.p1.3.m3.1.1" xref="S6.SS1.p1.3.m3.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.3.m3.1b"><cn type="integer" id="S6.SS1.p1.3.m3.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.3.m3.1c">7</annotation></semantics></math>.
The final submission involved an ensemble of Pyannote based models with different configurations, permutation invariant training (PIT) for speaker diarization and mixture invariant training (MixIT) for speech separation (PixIT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> combined using DOVER-Lap.
Team T2 experimented with different pre-trained embedding extractors of the ResNet series <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, and performed spectral clustering and VBx to generate the output. ResNet-152 with spectral clustering achieves the best single system DER.
Finally, the submission contained a fusion of the top <math id="S6.SS1.p1.4.m4.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S6.SS1.p1.4.m4.1a"><mn id="S6.SS1.p1.4.m4.1.1" xref="S6.SS1.p1.4.m4.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.4.m4.1b"><cn type="integer" id="S6.SS1.p1.4.m4.1.1.cmml" xref="S6.SS1.p1.4.m4.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.4.m4.1c">7</annotation></semantics></math> models followed by Pyannote overlap detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Team T3 focused on improving voice activity detection (VAD) and overlap detection components. For VAD, the team explored MarbleNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and Pyannote SAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> models trained using the supervised and unsupervised dev data. The best single system comprises multiscale Titanet-L embedding extractor followed by spectral clustering.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">In Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Automatic Speech Recognition ‣ 5 Baseline Systems ‣ The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we compare the top three teams (T1, T2, and T3) of DISPLACE-2024 on the same Eval data of DISPLACE-2023.
This table highlights significant relative improvements for SD with an average relative improvement of <math id="S6.SS1.p2.1.m1.1" class="ltx_Math" alttext="23.49" display="inline"><semantics id="S6.SS1.p2.1.m1.1a"><mn id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml">23.49</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><cn type="float" id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1">23.49</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">23.49</annotation></semantics></math>% in DER for the top-performing teams.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Track 2: Language Diarization</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.2" class="ltx_p">There were submissions from <math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><mn id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><cn type="integer" id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">6</annotation></semantics></math> teams out of which <math id="S6.SS2.p1.2.m2.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S6.SS2.p1.2.m2.1a"><mn id="S6.SS2.p1.2.m2.1.1" xref="S6.SS2.p1.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.2.m2.1b"><cn type="integer" id="S6.SS2.p1.2.m2.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.2.m2.1c">3</annotation></semantics></math> teams outperformed over the baseline. The DER distribution across all teams is shown in Figure <a href="#S5.F3" title="Figure 3 ‣ 5.3 Automatic Speech Recognition ‣ 5 Baseline Systems ‣ The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para ltx_noindent">
<p id="S6.SS2.p2.1" class="ltx_p"><span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_bold">Top performing systems</span>: T1 system used a wav2vec-BERT model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> to extract language embeddings from 5s segments with 1s shift. T1 trained a probabilistic linear discriminant analysis (PLDA) model using the DISPLACE ASR dev data. The model is fine-tuned on NIST LRE and SRE data and the dev set.
This is followed by PLDA similarity scoring, AHC and VBx re-segmentation to generate the final output.
T2 system used ResNet34 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>and wav2vec 2.0 architectures for extracting the language embeddings, utilized spectral clustering of language embeddings within a sliding window and a heuristic bypass (HBP) method for similarity matrix computation.
T3 system used Pyannote <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> for VAD and features were extracted using the pre-trained Whisper model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Finally, agglomerative hierarchical clustering was used to cluster and assign language cluster labels to the segments.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">In Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Automatic Speech Recognition ‣ 5 Baseline Systems ‣ The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we compare the top three teams (T1, T2, and T3) on the same eval data subset of DISPLACE-2023.
We observe the relative improvement in the model for LD is around <math id="S6.SS2.p3.1.m1.1" class="ltx_Math" alttext="33.37" display="inline"><semantics id="S6.SS2.p3.1.m1.1a"><mn id="S6.SS2.p3.1.m1.1.1" xref="S6.SS2.p3.1.m1.1.1.cmml">33.37</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.1.m1.1b"><cn type="float" id="S6.SS2.p3.1.m1.1.1.cmml" xref="S6.SS2.p3.1.m1.1.1">33.37</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.1.m1.1c">33.37</annotation></semantics></math>% DER for the top-performing team.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Track 3: ASR</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">For Track 3, we have two valid submissions from the participants. The top-performing team (T1) fine-tuned their model using Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for English, Hindi, and Bengali languages. The Bhashini model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> was deployed for Kannada and Telugu inferences. This system showed an absolute improvement of <math id="S6.SS3.p1.1.m1.1" class="ltx_Math" alttext="20.4" display="inline"><semantics id="S6.SS3.p1.1.m1.1a"><mn id="S6.SS3.p1.1.m1.1.1" xref="S6.SS3.p1.1.m1.1.1.cmml">20.4</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.1b"><cn type="float" id="S6.SS3.p1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1">20.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.1c">20.4</annotation></semantics></math>% in WER over the baseline system for close-field recordings. The individual monolingual WER performance along with overall system performance is given Table <a href="#S5.T2" title="Table 2 ‣ 5.3 Automatic Speech Recognition ‣ 5 Baseline Systems ‣ The Second DISPLACE Challenge : DIarization of SPeaker and LAnguage in Conversational Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Summary</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.2" class="ltx_p">This paper provides a comprehensive overview of the second DISPLACE challenge, which aims to encourage research in processing multi-lingual multi-speaker conversational audio. This challenge contained three tracks: 1) speaker diarization, 2) language diarization and 3) speech recognition. Track-1 and Track-2 share the common data, while Track-3 contained <math id="S7.p1.1.m1.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S7.p1.1.m1.1a"><mn id="S7.p1.1.m1.1.1" xref="S7.p1.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S7.p1.1.m1.1b"><cn type="integer" id="S7.p1.1.m1.1.1.cmml" xref="S7.p1.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.m1.1c">12</annotation></semantics></math> hours of audio data in five different Indian languages. As part of the challenge, we released updated baseline systems for the SD and LD tracks which provided improved performance over the first DISPLACE challenge benchmarks. The wide participation for the second DISPLACE challenge across the globe, resulted in significant improvements over the baseline system for both the SD and LD tracks. The ASR track using close-field recordings was observed to be significantly challenging and thus resulted in limited participation. The best-performing system achieved a WER of <math id="S7.p1.2.m2.1" class="ltx_Math" alttext="47" display="inline"><semantics id="S7.p1.2.m2.1a"><mn id="S7.p1.2.m2.1.1" xref="S7.p1.2.m2.1.1.cmml">47</mn><annotation-xml encoding="MathML-Content" id="S7.p1.2.m2.1b"><cn type="integer" id="S7.p1.2.m2.1.1.cmml" xref="S7.p1.2.m2.1.1">47</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.2.m2.1c">47</annotation></semantics></math>% on the evaluation data, highlighting the need for continued efforts on this demanding dataset. We also compared our baseline models and top performing teams with the corresponding results from the DISPLACE-2023 challenge. This comparison led to understand the progress made in processing multilingual, multi-speaker conversational data under the DISPLACE challenge series.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Acknowledgements</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">The authors would like to thank Kaustubh Kumar and Lokesh Kumar from IIT Dharwad for helping with data collection. Also Michael free and Shakti Srivastava from BT for valuable discussions.</p>
</div>
<div id="S8.p2" class="ltx_para">
<span id="S8.p2.1" class="ltx_ERROR undefined">\ninept</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:50%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:50%;">
B. E. Bullock and A. J. E. Toribio, </span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">The Cambridge handbook of linguistic
code-switching.</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:50%;">  Cambridge University
Press, 2009.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:50%;">
E. Yilmaz, M. McLaren, H. van den Heuvel, and D. A. van Leeuwen, ``Language
diarization for semi-supervised bilingual acoustic model training,'' in
</span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Proc. IEEE ASRU</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:50%;">, 2017, pp. 91–96.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:50%;">
P. Auer, </span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Code-switching in conversation: Language, Interaction and
Identity</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:50%;">.  Routledge, 2013.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:50%;">
K. Potowski and J. Rothman, </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Bilingual youth: Spanish in English-speaking
societies</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:50%;">.  John Benjamins Publishing,
2011, vol. 42.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:50%;">
S. Ganji, K. Dhawan, and R. Sinha, ``IITG-Hingcos corpus: A Hinglish
code-switching database for automatic speech recognition,'' </span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Speech
communication</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:50%;">, vol. 110, pp. 76–89, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:50%;">
S. Baghel, S. Ramoji, Sidharth, R. H, P. Singh, S. Jain, P. Roy Chowdhuri,
K. Kulkarni, S. Padhi, D. Vijayasenan, and S. Ganapathy, ``The DISPLACE
Challenge 2023 - DIarization of SPeaker and LAnguage in Conversational
Environments,'' in </span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Proc. INTERSPEECH 2023</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:50%;">, 2023, pp. 3562–3566.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:50%;">
``Rich transcription evaluation,''
</span><a target="_blank" href="https://www.nist.gov/itl/iad/mig/rich-transcription-evaluation" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:50%;">https://www.nist.gov/itl/iad/mig/rich-transcription-evaluation</a><span id="bib.bib7.2.2" class="ltx_text" style="font-size:50%;">,
accessed: 2024-02-29.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:50%;">
J. G. Fiscus, J. Ajot, and J. S. Garofolo, ``The rich transcription 2007
meeting recognition evaluation,'' in </span><em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">International Evaluation Workshop
on Rich Transcription</em><span id="bib.bib8.3.3" class="ltx_text" style="font-size:50%;">.  Springer,
2007, pp. 373–389.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:50%;">
N. Ryant, P. Singh, V. Krishnamohan, R. Varma, K. Church, C. Cieri, J. Du,
S. Ganapathy, and M. Liberman, ``The Third DIHARD Diarization Challenge,''
in </span><em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Proc. Interspeech 2021</em><span id="bib.bib9.3.3" class="ltx_text" style="font-size:50%;">, 2021, pp. 3570–3574.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:50%;">
A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie, M. McLaren, D. A.
Reynolds, and A. Zisserman, ``Voxsrc 2020: The second voxceleb speaker
recognition challenge,'' </span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">arXiv preprint arXiv:2012.06867</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:50%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:50%;">
A. Joglekar, J. H. Hansen, M. C. Shekar, and A. Sangwan, ``FEARLESS STEPS
Challenge (FS-2): Supervised Learning with Massive Naturalistic Apollo
Data,'' in </span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Proc. Interspeech 2020</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:50%;">, 2020, pp. 2617–2621.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:50%;">
H. Liu, H. Xu, L. P. Garcia, A. W. Khong, Y. He, and S. Khudanpur, ``Reducing
language confusion for code-switching speech recognition with token-level
language diarization,'' in </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">2023 IEEE ICASSP</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:50%;">.  IEEE, 2023, pp. 1–5.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:50%;">
S. V, V. Thenkanidiyoor, and D. A. D, ``SVM Based Language Diarization for
Code-Switched Bilingual Indian Speech Using Bottleneck Features,'' in
</span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Proc. SLTU 2018</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:50%;">, 2018, pp. 132–136.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:50%;">
V. Y. H. Chua, H. Liu, L. P. Garcia, F. T. Woon, J. Wong, X. Zhang,
S. Khudanpur, A. W. H. Khong, J. Dauwels, and S. J. Styles, ``MERLIon CCS
Challenge: A English-Mandarin code-switching child-directed speech corpus for
language identification and diarization,'' in </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Proc. INTERSPEECH 2023</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:50%;">,
2023, pp. 4109–4113.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:50%;">
T. Schultz and A. Waibel, ``Language-independent and language-adaptive acoustic
modeling for speech recognition,'' </span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Speech Communication</em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:50%;">, vol. 35, no.
1-2, pp. 31–51, 2001.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:50%;">
B. M. L. Srivastava, S. Sitaram, R. Kumar Mehta, K. Doss Mohan, P. Matani,
S. Satpal, K. Bali, R. Srikanth, and N. Nayak, ``Interspeech 2018 Low
Resource Automatic Speech Recognition Challenge for Indian Languages,'' in
</span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Proc. SLTU 2018</em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:50%;">, 2018, pp. 11–14.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:50%;">
``Interspeech 2018 special session : Speech recognition for Indian
languages,'' </span><a target="_blank" href="https://sites.google.com/view/interspeech2018-ss1" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:50%;">https://sites.google.com/view/interspeech2018-ss1</a><span id="bib.bib17.2.2" class="ltx_text" style="font-size:50%;">,
accessed: 2024-02-29.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:50%;">
``Hindi ASR challenge,'' </span><a target="_blank" href="https://sites.google.com/view/asr-challenge" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:50%;">https://sites.google.com/view/asr-challenge</a><span id="bib.bib18.2.2" class="ltx_text" style="font-size:50%;">,
accessed: 2024-02-29.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:50%;">
``Hindi-Tamil-English ASR challenge,''
</span><a target="_blank" href="https://sites.google.com/view/indian-language-asrchallenge/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:50%;">https://sites.google.com/view/indian-language-asrchallenge/</a><span id="bib.bib19.2.2" class="ltx_text" style="font-size:50%;">, accessed:
2024-02-29.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:50%;">
A. Diwan, R. Vaideeswaran, S. Shah, A. Singh, S. Raghavan, S. Khare, V. Unni,
S. Vyas, A. Rajpuria, C. Yarra </span><em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">et al.</em><span id="bib.bib20.3.3" class="ltx_text" style="font-size:50%;">, ``Multilingual and
code-switching asr challenges for low resource indian languages,''
</span><em id="bib.bib20.4.4" class="ltx_emph ltx_font_italic" style="font-size:50%;">arXiv preprint arXiv:2104.00235</em><span id="bib.bib20.5.5" class="ltx_text" style="font-size:50%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:50%;">
``Multilingual and code-switching speech recognition,''
</span><a target="_blank" href="https://www.clsp.jhu.edu/multilingual-and-code-switching/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:50%;">https://www.clsp.jhu.edu/multilingual-and-code-switching/</a><span id="bib.bib21.2.2" class="ltx_text" style="font-size:50%;">, accessed:
2024-02-29.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:50%;">
J. Li, B. Wang, Y. Zhi, Z. Li, L. Li, Q. Hong, and D. Wang, ``Oriental language
recognition (olr) 2020: Summary and analysis,'' </span><em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">arXiv preprint
arXiv:2107.05365</em><span id="bib.bib22.3.3" class="ltx_text" style="font-size:50%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:50%;">
S. Baghel, S. Ramoji, S. Jain, P. R. Chowdhuri, P. Singh, D. Vijayasenan, and
S. Ganapathy, ``Summary of the displace challenge 2023–diarization of
speaker and language in conversational environments,'' </span><em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">arXiv preprint
arXiv:2311.12564</em><span id="bib.bib23.3.3" class="ltx_text" style="font-size:50%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:50%;">
``SCTK, the NIST scoring toolkit,''
</span><a target="_blank" href="https://github.com/usnistgov/SCTK/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:50%;">https://github.com/usnistgov/SCTK/</a><span id="bib.bib24.2.2" class="ltx_text" style="font-size:50%;">, accessed: 2024-02-29.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:50%;">
D. Snyder, D. Garcia-Romero, G. Sell, A. McCree, D. Povey, and S. Khudanpur,
``Speaker recognition for multi-speaker conversations using x-vectors,'' in
</span><em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">ICASSP 2019-2019 IEEE International conference on acoustics, speech and
signal processing (ICASSP)</em><span id="bib.bib25.3.3" class="ltx_text" style="font-size:50%;">.  IEEE,
2019, pp. 5796–5800.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:50%;">
P. Singh, H. Vardhan, S. Ganapathy, and A. Kanagasundaram, ``LEAP diarization
system for the second DIHARD challenge,'' in </span><em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Proc INTERSPEECH</em><span id="bib.bib26.3.3" class="ltx_text" style="font-size:50%;">, 2019,
pp. 983–987.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:50%;">
H. Bredin, ``pyannote.audio 2.1 speaker diarization pipeline: principle,
benchmark, and recipe,'' in </span><em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Proc. INTERSPEECH 2023</em><span id="bib.bib27.3.3" class="ltx_text" style="font-size:50%;">, 2023, pp.
1983–1987.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:50%;">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever,
``Robust speech recognition via large-scale weak supervision,'' in
</span><em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">International Conference on Machine Learning</em><span id="bib.bib28.3.3" class="ltx_text" style="font-size:50%;">.  PMLR, 2023, pp. 28 492–28 518.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:50%;">
J. Valk and T. Alumäe, ``VoxLingua107: a dataset for spoken language
recognition,'' in </span><em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Proc. IEEE SLT Workshop</em><span id="bib.bib29.3.3" class="ltx_text" style="font-size:50%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:50%;">
B. Vachhani, D. Singh, and R. Lawyer, ``Multi-resolution Approach to
Identification of Spoken Languages and To Improve Overall Language
Diarization System Using Whisper Model,'' in </span><em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Proc. INTERSPEECH 2023</em><span id="bib.bib30.3.3" class="ltx_text" style="font-size:50%;">,
2023, pp. 1993–1997.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:50%;">
F. Landini, J. Profant, M. Diez, and L. Burget, ``Bayesian HMM clustering of
x-vector sequences (vbx) in speaker diarization: theory, implementation and
analysis on standard tasks,'' </span><em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Computer Speech &amp; Language</em><span id="bib.bib31.3.3" class="ltx_text" style="font-size:50%;">, vol. 71, p.
101254, 2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:50%;">
``Turn speech into text using google ai,''
</span><a target="_blank" href="https://cloud.google.com/speech-to-text?hl=en" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:50%;">https://cloud.google.com/speech-to-text?hl=en</a><span id="bib.bib32.2.2" class="ltx_text" style="font-size:50%;">, accessed: 2024-02-29.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:50%;">
S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang,
S. Khudanpur, V. Manohar, D. Povey, D. Raj </span><em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">et al.</em><span id="bib.bib33.3.3" class="ltx_text" style="font-size:50%;">, ``Chime-6
challenge: Tackling multispeaker speech recognition for unsegmented
recordings,'' </span><em id="bib.bib33.4.4" class="ltx_emph ltx_font_italic" style="font-size:50%;">arXiv preprint arXiv:2004.09249</em><span id="bib.bib33.5.5" class="ltx_text" style="font-size:50%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:50%;">
J. Kalda, R. Marxer, T. Alumäe, H. Bredin </span><em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">et al.</em><span id="bib.bib34.3.3" class="ltx_text" style="font-size:50%;">, ``Pixit: Joint
training of speaker diarization and speech separation from real-world
multi-speaker recordings,'' </span><em id="bib.bib34.4.4" class="ltx_emph ltx_font_italic" style="font-size:50%;">arXiv preprint arXiv:2403.02288</em><span id="bib.bib34.5.5" class="ltx_text" style="font-size:50%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:50%;">
H. Wang, C. Liang, S. Wang, Z. Chen, B. Zhang, X. Xiang, Y. Deng, and Y. Qian,
``Wespeaker: A research and production oriented speaker embedding learning
toolkit,'' in </span><em id="bib.bib35.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">ICASSP 2023-2023 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</em><span id="bib.bib35.3.3" class="ltx_text" style="font-size:50%;">.  IEEE, 2023, pp. 1–5.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:50%;">
F. Jia, S. Majumdar, and B. Ginsburg, ``Marblenet: Deep 1d time-channel
separable convolutional neural network for voice activity detection,'' in
</span><em id="bib.bib36.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</em><span id="bib.bib36.3.3" class="ltx_text" style="font-size:50%;">.  IEEE,
2021, pp. 6818–6822.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:50%;">
L. Barrault, Y.-A. Chung, M. C. Meglioli, D. Dale, N. Dong, M. Duppenthaler,
P.-A. Duquenne, B. Ellis, H. Elsahar, J. Haaheim </span><em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">et al.</em><span id="bib.bib37.3.3" class="ltx_text" style="font-size:50%;">, ``Seamless:
Multilingual expressive and streaming speech translation,'' </span><em id="bib.bib37.4.4" class="ltx_emph ltx_font_italic" style="font-size:50%;">arXiv
preprint arXiv:2312.05187</em><span id="bib.bib37.5.5" class="ltx_text" style="font-size:50%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:50%;">
A. Gusev, V. Volokhov, T. Andzhukaev, S. Novoselov, G. Lavrentyeva, M. Volkova,
A. Gazizullina, A. Shulipa, A. Gorlanov, A. Avdeeva, A. Ivanov, A. Kozlov,
T. Pekhovsky, and Y. Matveev, ``Deep Speaker Embeddings for Far-Field
Speaker Recognition on Short Utterances,'' in </span><em id="bib.bib38.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">Proc. The Speaker and
Language Recognition Workshop (Odyssey 2020)</em><span id="bib.bib38.3.3" class="ltx_text" style="font-size:50%;">, 2020, pp. 179–186.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:50%;">
V. S. Lodagala, S. Ghosh, and S. Umesh, ``Ccc-wav2vec 2.0: Clustering aided
cross contrastive self-supervised learning of speech representations,'' in
</span><em id="bib.bib39.2.2" class="ltx_emph ltx_font_italic" style="font-size:50%;">2022 IEEE Spoken Language Technology Workshop (SLT)</em><span id="bib.bib39.3.3" class="ltx_text" style="font-size:50%;">.  IEEE, 2023, pp. 1–8.
</span>
</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.09492" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.09494" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.09494">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.09494" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.09495" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Jul  6 00:51:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
