<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.17852] Scaling A Simple Approach to Zero-Shot Speech Recognition</title><meta property="og:description" content="DDespite rapid progress in increasing the language coverage of automatic speech recognition, the field is still far from covering all languages with a known writing script.
Recent work showed promising results with a z…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scaling A Simple Approach to Zero-Shot Speech Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Scaling A Simple Approach to Zero-Shot Speech Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.17852">

<!--Generated on Mon Aug  5 18:39:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">1]Monash University
2]Meta FAIR
<span id="p1.1.1" class="ltx_ERROR undefined">\contribution</span>[*]Work done during an internship at Meta FAIR.</p>
</div>
<h1 class="ltx_title ltx_title_document">Scaling A Simple Approach to Zero-Shot Speech Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jinming Zhao
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vineel Pratap
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael Auli
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jinming.zhao@monash.edu">jinming.zhao@monash.edu</a>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:vineelkpratap@meta.com">vineelkpratap@meta.com</a>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:michaelauli@meta.com">michaelauli@meta.com</a>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">DDespite rapid progress in increasing the language coverage of automatic speech recognition, the field is still far from covering all languages with a known writing script.
Recent work showed promising results with a zero-shot approach requiring only a small amount of text data, however, accuracy heavily depends on the quality of the used phonemizer which is often weak for unseen languages.
In this paper, we present MMS Zero-shot a conceptually simpler approach based on romanization and an acoustic model trained on data in 1,078 different languages or three orders of magnitude more than prior art.
MMS Zero-shot reduces the average character error rate by a relative 46% over 100 unseen languages compared to the best previous work.
Moreover, the error rate of our approach is only 2.5x higher compared to in-domain supervised baselines, while our approach uses no labeled data for the evaluation languages at all.
Code and models are available at <a target="_blank" href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms/zero_shot" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/fairseq/tree/main/examples/mms/zero_shot</a></p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\correspondence</span>
<p id="p2.2" class="ltx_p">, , 
<span id="p2.2.1" class="ltx_text" lang="en"></span></p>
</div>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">There has been significant work in enabling automatic speech recognition (ASR) for more of the over 7,000 languages spoken around the world <cite class="ltx_cite ltx_citemacro_citep">(Campbell, <a href="#bib.bib4" title="" class="ltx_ref">2008</a>)</cite>.
One approach has been to perform self-supervised learning <cite class="ltx_cite ltx_citemacro_citep">(van den Oord et al., <a href="#bib.bib22" title="" class="ltx_ref">2018</a>; Baevski et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> followed by fine-tuning on labeled data to build models supporting between 100 and 1,000 languages <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib24" title="" class="ltx_ref">2023</a>; Pratap et al., <a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite>.
Another line of work focuses on traditional supervised learning using large amounts of labeled data <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite> which is only available for a small subset of languages.
However, despite rapid progress in language expansion, it appears unlikely that it is possible to obtain a reasonable amount of labeled data for all languages with a writing script.
An alternative is to do away with labeled data altogether via unsupervised ASR methods <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib16" title="" class="ltx_ref">2018</a>; Chen et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>; Baevski et al., <a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite> but a drawback of these methods is the requirement for both unlabeled audio and unlabeled text which may still be difficult to obtain for many new languages.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">More recently, a promising zero-shot approach has been introduced which requires only unlabeled text <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib14" title="" class="ltx_ref">2022a</a>)</cite>.
The idea is to train an acoustic model which outputs language-independent allophones that are then mapped to language-specific phonemes <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>.
Next, a pronunciation model utilizing a zero-shot G2P system <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2022b</a>)</cite> maps phoneme sequences to words and decoding may be further improved via an n-gram language model.
This approach has the downside that the mapping from allophones to phonemes is error-prone <cite class="ltx_cite ltx_citemacro_citep">(Mortensen et al., <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite> and that the G2P phonemizer has poor performance for many unseen languages as we show.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This is likely because the dataset used by the phonemizer covers only 40 out of 110 branches of the Glottolog Phylogenetic tree <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2022b</a>)</cite>.</span></span></span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To sidestep these challenges, we do away with allophones and phonemes and use a single intermediate text representation by romanizing text in different languages which standardizes them to a common Latin-script <cite class="ltx_cite ltx_citemacro_citep">(Hermjakob et al., <a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>.
Our acoustic model is trained on labeled data in 1,078 different languages and outputs romanized text.
For inference, we map model outputs to words by employing a simple lexicon based on a romanized encoding of a modest amount of supplied text in a new language (<cite class="ltx_cite ltx_citemacro_cite">Hermjakob et al. (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>; see Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Method ‣ Scaling A Simple Approach to Zero-Shot Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We do not require language independent phonemizers which we find to result in poor accuracy for many languages and experiments show that this simple approach can lead to large accuracy improvements on 100 unseen MMS-lab and FLEURS languages <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a href="#bib.bib7" title="" class="ltx_ref">2023</a>; Pratap et al., <a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2407.17852/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="253" height="145" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">MMS Zero-shot. We build a universal acoustic model by fine-tuning a pre-trained wav2vec 2.0 model on romanized transcripts (left).
A new language is transcribed by performing beam search decoding with a lexicon mapping words in the new language to romanized text.
If available, then a language model can be used to improve performance (right).
</span></figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Universal Acoustic Model</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">There are a variety of different writing scripts and we standardize different languages using romanization which essentially converts text to a single writing script <cite class="ltx_cite ltx_citemacro_citep">(Wellisch et al., <a href="#bib.bib23" title="" class="ltx_ref">1978</a>)</cite>. We use uroman <cite class="ltx_cite ltx_citemacro_citep">(Hermjakob et al., <a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>, a universal uromanizer, which performs the mapping using a set of heuristics, rather than by relying on language-specific dictionaries. After converting all the transcripts to their romanized version, we finetune a pretrained wav2vec 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Baevski et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> model on many languages for which transcripts are available, so that the model can generalize to unseen languages.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Universal phonemizers <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib14" title="" class="ltx_ref">2022a</a>)</cite> are another way to standardize text, however, they require complex linguistic rules and are less robust compared to uroman, hindering their applicability to many unseen languages.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Universal phonemizers failed on certain languages of this study unlike uroman.</span></span></span></p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Zero-shot decoding</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Zero-shot transcription requires only a list of words in the unseen language.
The first step is to produce a lexicon by applying uroman to each word so as to have <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">&lt;word, uroman_text&gt;</span> pairs.
When decoding with the lexicon, the model is forced to produce words contained in the lexicon.
Open-source databases, such as Panlex <cite class="ltx_cite ltx_citemacro_citep">(Kamholz et al., <a href="#bib.bib12" title="" class="ltx_ref">2014</a>)</cite> provide word lists for over 6k languages, which enables building speech recognizers for all these languages.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">We can additionally integrate a simple n-gram language model during decoding to further improve performance <cite class="ltx_cite ltx_citemacro_citep">(Pratap et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>.
Leveraging databases with word statistics is also an alternative,
e.g., following <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022a</a>)</cite>, we investigate the use of Crúbadán <cite class="ltx_cite ltx_citemacro_citep">(Scannell, <a href="#bib.bib21" title="" class="ltx_ref">2007</a>)</cite> which is a database that includes unigrams, bigrams, and character-level trigrams, together with their statistics, for about 2k languages.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The Crúbadán dataset was received directly from the authors in <cite class="ltx_cite ltx_citemacro_citet">Scannell (<a href="#bib.bib21" title="" class="ltx_ref">2007</a>)</cite>.
Meta did not scrape or collect the data from the original sources by scripts or other means.
</span></span></span></p>
</div>
</section>
</section>
<section id="S3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We use the following datasets: (i) <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">MMS-lab (MMS)</span>, built on the New Testament, has paired labeled data for 1,107 languages <cite class="ltx_cite ltx_citemacro_citep">(Pratap et al., <a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite>,
(ii) <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">FLEURS</span> is a 102-language open-sourced ASR benchmark dataset <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>,
(iii) <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">CommonVoice (CV)</span> is a multilingual ASR dataset, crowdsourced from global volunteers <cite class="ltx_cite ltx_citemacro_citep">(Ardila et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite> comprising 85 languages from version 8.0.
We hold out 90 randomly selected low-resource languages from MMS and 10 from FLEURS of which half are used for development and half for test.
All CV languages are used for training as most of them are high-resource.
The held-out languages are not present in the training data.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Baselines</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We compared our system to results obtained with the public model checkpoint of ASR-2K, a zeroshot ASR model that supports nearly 2k languages <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib14" title="" class="ltx_ref">2022a</a>)</cite>.
Since their method is modular, we use our decoding strategy based on beam search, for a more like for like comparison.
We also compare to supervised models to get a sense of how much the accuracy of zero-shot systems differs from the ideal setting where labeled data is available:
We train three different sets of monolingual models for the 100 dev and test languages based on character outputs (mono-char), uroman outputs (mono-uroman), as well as phoneme outputs (mono-phone) using the universal phonemizer <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2022b</a>)</cite> utilized in ASR-2K.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training and Decoding Setup</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We fine-tune XLS-R <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite><span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We do not use MMS pretrained models as the held-out languages are seen during the pretraining.</span></span></span> on paired speech and uroman text data using CTC <cite class="ltx_cite ltx_citemacro_citep">(Graves et al., <a href="#bib.bib8" title="" class="ltx_ref">2006</a>)</cite> and follow the settings of XLS-R 300M.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/xlsr/README.md" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/xlsr/README.md</a></span></span></span>
We first apply uroman to the text portion of the training data of each language which results in a shared vocabulary of less than 30 characters over all languages that we use for training.
We then fine-tune XLS-R using labeled training data from MMS and CV, comprising 1,078 languages and a total of 39,946 hours of data.
The amount of data per language varies by language, ranging from a few minutes for low-resource languages to a thousand hours for high-resource languages.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Monolingual models are trained on 4 GPUs for 10k updates and multilingual models on 32 GPUs for 200k updates.
The decoding beam size is 2k unless otherwise mentioned.
Models are selcted based on dev set accuracy of the training languages for supervised toplines, and on the dev sets of 50 unseen languages for zero-shot models.
We used flashlight for decoding <cite class="ltx_cite ltx_citemacro_citep">(Kahn et al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite> and report CER on the dev sets and test sets for our unseen dev and test languages, respectively.
We tuned wordscore and language model weights for all models, including ASR-2K, on the dev set of 50 unseen languages, and apply the best aggregated scores to unseen languages at inference time.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>
We found wordscore tuning to be crucial in obtaining good performance and that a wordscore tuned on the dev languages is very competitive to a wordscore tuned on the actual unseen languages.</span></span></span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and Analysis</h2>

<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:424.7pt;height:77.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.5pt,6.8pt) scale(0.85,0.85) ;">
<table id="S4.T1.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.1.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th id="S4.T1.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T1.2.1.1.1.2.1" class="ltx_text">#Train</span></th>
<th id="S4.T1.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">MMS dev</th>
<th id="S4.T1.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">MMS test</th>
<th id="S4.T1.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">FLEURS dev</th>
<th id="S4.T1.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">FLEURS test</th>
<th id="S4.T1.2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">CV test</th>
<th id="S4.T1.2.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2">AVG</th>
</tr>
<tr id="S4.T1.2.1.2.2" class="ltx_tr">
<th id="S4.T1.2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;">Model</th>
<th id="S4.T1.2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;">lang</th>
<th id="S4.T1.2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">lex</th>
<th id="S4.T1.2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1grm</th>
<th id="S4.T1.2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">lex</th>
<th id="S4.T1.2.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1grm</th>
<th id="S4.T1.2.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">lex</th>
<th id="S4.T1.2.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1grm</th>
<th id="S4.T1.2.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">lex</th>
<th id="S4.T1.2.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1grm</th>
<th id="S4.T1.2.1.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">lex</th>
<th id="S4.T1.2.1.2.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1grm</th>
<th id="S4.T1.2.1.2.2.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">lex</th>
<th id="S4.T1.2.1.2.2.14" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1grm</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.1.3.1" class="ltx_tr">
<td id="S4.T1.2.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">ASR-2K</td>
<td id="S4.T1.2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">8</td>
<td id="S4.T1.2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">47.2</td>
<td id="S4.T1.2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">38.2</td>
<td id="S4.T1.2.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">50.4</td>
<td id="S4.T1.2.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">39.5</td>
<td id="S4.T1.2.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">55.5</td>
<td id="S4.T1.2.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">51.4</td>
<td id="S4.T1.2.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">62.2</td>
<td id="S4.T1.2.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">56.1</td>
<td id="S4.T1.2.1.3.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">45.4</td>
<td id="S4.T1.2.1.3.1.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">38.7</td>
<td id="S4.T1.2.1.3.1.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">52.2</td>
<td id="S4.T1.2.1.3.1.14" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">44.8</td>
</tr>
<tr id="S4.T1.2.1.4.2" class="ltx_tr">
<td id="S4.T1.2.1.4.2.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">MMS Zero-shot (CV-only)</td>
<td id="S4.T1.2.1.4.2.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">8</td>
<td id="S4.T1.2.1.4.2.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">32.3</td>
<td id="S4.T1.2.1.4.2.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">29.8</td>
<td id="S4.T1.2.1.4.2.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">36.4</td>
<td id="S4.T1.2.1.4.2.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">33.5</td>
<td id="S4.T1.2.1.4.2.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">48.7</td>
<td id="S4.T1.2.1.4.2.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">46.1</td>
<td id="S4.T1.2.1.4.2.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">50.0</td>
<td id="S4.T1.2.1.4.2.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">46.8</td>
<td id="S4.T1.2.1.4.2.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">36.0</td>
<td id="S4.T1.2.1.4.2.12" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">34.2</td>
<td id="S4.T1.2.1.4.2.13" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">40.7</td>
<td id="S4.T1.2.1.4.2.14" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">38.5</td>
</tr>
<tr id="S4.T1.2.1.5.3" class="ltx_tr">
<td id="S4.T1.2.1.5.3.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">MMS Zero-shot</td>
<td id="S4.T1.2.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">1,078</td>
<td id="S4.T1.2.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">17.5</td>
<td id="S4.T1.2.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">11.6</td>
<td id="S4.T1.2.1.5.3.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">20.0</td>
<td id="S4.T1.2.1.5.3.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">12.6</td>
<td id="S4.T1.2.1.5.3.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">40.4</td>
<td id="S4.T1.2.1.5.3.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">38.6</td>
<td id="S4.T1.2.1.5.3.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">41.4</td>
<td id="S4.T1.2.1.5.3.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">38.2</td>
<td id="S4.T1.2.1.5.3.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">26.8</td>
<td id="S4.T1.2.1.5.3.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">25.2</td>
<td id="S4.T1.2.1.5.3.13" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">29.2</td>
<td id="S4.T1.2.1.5.3.14" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">25.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Comparison to ASR-2K. We report average CER on a total of 107 unseen languages from MMS-lab, FLEURS and CV languages (§<a href="#S3.SS1" title="3.1 Datasets ‣ 3 Experiments ‣ Scaling A Simple Approach to Zero-Shot Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). The average CER is unweighted.
</span></figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:431.1pt;height:76.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.0pt,6.8pt) scale(0.85,0.85) ;">
<table id="S4.T2.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.1.1.1" class="ltx_tr">
<th id="S4.T2.2.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<th id="S4.T2.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="3">MMS dev</th>
<th id="S4.T2.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="3">MMS test</th>
<th id="S4.T2.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="3">FLEURS dev</th>
<th id="S4.T2.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="3">FLEURS test</th>
<th id="S4.T2.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="3">AVG</th>
</tr>
<tr id="S4.T2.2.1.2.2" class="ltx_tr">
<th id="S4.T2.2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding-left:4.0pt;padding-right:4.0pt;">models</th>
<th id="S4.T2.2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">lex</th>
<th id="S4.T2.2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1grm</th>
<th id="S4.T2.2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">3grm</th>
<th id="S4.T2.2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">lex</th>
<th id="S4.T2.2.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1grm</th>
<th id="S4.T2.2.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">3grm</th>
<th id="S4.T2.2.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">lex</th>
<th id="S4.T2.2.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1grm</th>
<th id="S4.T2.2.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">3grm</th>
<th id="S4.T2.2.1.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">lex</th>
<th id="S4.T2.2.1.2.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1grm</th>
<th id="S4.T2.2.1.2.2.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">3grm</th>
<th id="S4.T2.2.1.2.2.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">lex</th>
<th id="S4.T2.2.1.2.2.15" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1grm</th>
<th id="S4.T2.2.1.2.2.16" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">3grm</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.1.3.1" class="ltx_tr">
<td id="S4.T2.2.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">mono-char</td>
<td id="S4.T2.2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1.9</td>
<td id="S4.T2.2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1.9</td>
<td id="S4.T2.2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">1.8</td>
<td id="S4.T2.2.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">2.3</td>
<td id="S4.T2.2.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">2.3</td>
<td id="S4.T2.2.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">2.3</td>
<td id="S4.T2.2.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">16.8</td>
<td id="S4.T2.2.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">17.0</td>
<td id="S4.T2.2.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">16.9</td>
<td id="S4.T2.2.1.3.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">16.7</td>
<td id="S4.T2.2.1.3.1.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">16.8</td>
<td id="S4.T2.2.1.3.1.13" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">16.4</td>
<td id="S4.T2.2.1.3.1.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">9.4</td>
<td id="S4.T2.2.1.3.1.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">9.5</td>
<td id="S4.T2.2.1.3.1.16" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">9.4</td>
</tr>
<tr id="S4.T2.2.1.4.2" class="ltx_tr">
<td id="S4.T2.2.1.4.2.1" class="ltx_td ltx_align_left" style="padding-left:4.0pt;padding-right:4.0pt;">mono-uroman</td>
<td id="S4.T2.2.1.4.2.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">5.4</td>
<td id="S4.T2.2.1.4.2.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">3.1</td>
<td id="S4.T2.2.1.4.2.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">2.7</td>
<td id="S4.T2.2.1.4.2.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">6.0</td>
<td id="S4.T2.2.1.4.2.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">3.2</td>
<td id="S4.T2.2.1.4.2.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">2.9</td>
<td id="S4.T2.2.1.4.2.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">18.5</td>
<td id="S4.T2.2.1.4.2.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">17.7</td>
<td id="S4.T2.2.1.4.2.10" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">17.7</td>
<td id="S4.T2.2.1.4.2.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">25.4</td>
<td id="S4.T2.2.1.4.2.12" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">20.8</td>
<td id="S4.T2.2.1.4.2.13" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">19.6</td>
<td id="S4.T2.2.1.4.2.14" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">13.8</td>
<td id="S4.T2.2.1.4.2.15" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">11.1</td>
<td id="S4.T2.2.1.4.2.16" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">10.7</td>
</tr>
<tr id="S4.T2.2.1.5.3" class="ltx_tr">
<td id="S4.T2.2.1.5.3.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">MMS Zero-shot</td>
<td id="S4.T2.2.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">17.5</td>
<td id="S4.T2.2.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">11.6</td>
<td id="S4.T2.2.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">9.3</td>
<td id="S4.T2.2.1.5.3.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">20.0</td>
<td id="S4.T2.2.1.5.3.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">12.6</td>
<td id="S4.T2.2.1.5.3.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">10.7</td>
<td id="S4.T2.2.1.5.3.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">40.4</td>
<td id="S4.T2.2.1.5.3.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">38.6</td>
<td id="S4.T2.2.1.5.3.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">38.2</td>
<td id="S4.T2.2.1.5.3.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">41.4</td>
<td id="S4.T2.2.1.5.3.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">38.2</td>
<td id="S4.T2.2.1.5.3.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">37.8</td>
<td id="S4.T2.2.1.5.3.14" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">29.8</td>
<td id="S4.T2.2.1.5.3.15" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">25.3</td>
<td id="S4.T2.2.1.5.3.16" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;">24.0</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;">
Comparison of supervised models to MMS Zero-shot on 90 MMS languages and 10 FLEURS languages.
Each supervised monolingual model is trained on the domain it is evaluated on.
The average CER is unweighted.
</span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Comparison to Prior Work</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We first present a comparison to the most relevant prior work, ASR-2K, on three different sets of languages:
MMS-lab (90 languages in total), FLEURS (10 languages), and 7 CommonVoice languages from CV 17 which are unseen for all models since models were trained on CV 6.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>We could not exactly determine the CV version used by <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib14" title="" class="ltx_ref">2022a</a>)</cite> and found CV 6 to be a good match of their data description.</span></span></span>
The average amount of target language text data to build lexicons or LMs is 9k utterances for MMS-lab, 3k for FLEURS and 6.5k for CV.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We compare three models:
first, ASR-2K using flashlight beam search decoding, with a lexicon mapping from words to phonemes.
Second, our uroman zero-shot approach with an acoustic model trained on the same eight languages as ASR-2K using CV 6.0 data to isolate the effect of more training languages compared to ASR-2K (MMS Zero-shot CV-only).
Third, our uroman zero-shot approach with an acoustic model trained on all 1,078 languages of MMS-lab and CV (MMS Zero-shot).
Models are evaluated using only a lexicon or an additional unigram language model estimated on the transcriptions of the training data of a particular language in the respective corpus.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4 Results and Analysis ‣ Scaling A Simple Approach to Zero-Shot Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows that the MMS Zero-Shot (CV-only) model reduces the error rate by between 16-22% relative compared to ASR-2K across all 107 languages, including CV languages, which is in-domain for ASR-2K.<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>We select models based on unseen dev language accuracy which performs better than using seen languages.
We could not determine the ASR-2K model selection strategy, however, even if we select models based on seen languages, CER still decreases by a relative 5-15% on average compared to ASR-2K.</span></span></span>
Therefore, a romanization based encoding can perform very well compared to the allophone-based approach of ASR-2K.
Moreover, our approach has no intermediate phone representations and directly predicts uroman symbols instead of requiring a learned mapping from phones to phonemes.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">When our acoustic model is trained on three orders of magnitude more languages, then performance improves further: MMS Zero-shot reduces the error rate by a relative 46% compared to ASR-2K on average across all languages.
This is partially because the universal phonemizer of ASR-2K leads to poor accuracy on some languages (CER <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mo id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><geq id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">\geq</annotation></semantics></math> 70).
However, even after removing these languages, MMS Zero-shot (CV-only) still outperforms ASR-2K by an average absolute CER of 9.7%/5.1% for lexicon/1-gram decoding.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">Moreover, we can see that unigram language models perform much better than unweighted lexicons which provide no guidance to which words a uroman sequence should be mapped when there is ambiguity.
Our zero-shot models were trained on MMS-lab data which partly explains why improvements are larger on unseen MMS-lab evaluation languages, however, there is still a very sizeable improvement on FLEURS which is out-of-domain for the uroman-based models.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison to Supervised Models</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">How well does zero-shot ASR perform compared to supervised systems?
Zero-shot models are at an inherent disadvantage due to the lack of supervision in the target language but the size of the gap will help us better understand the progress in zero-shot methods.
To get a better sense of this, we compare the MMS Zero-shot model to monolingual supervised systems outputting either uroman or characters (mono-uroman, mono-char).
The monolingual models are trained on MMS-lab and FLEURS data and we evaluate on the same benchmarks.
This is an in-domain setting for the supervised models and presents a very strong baseline.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4 Results and Analysis ‣ Scaling A Simple Approach to Zero-Shot Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows CER with lexicon-based and language model decoding.<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>Lexicons decrease accuracy for mono-char models but we use them for all settings for a like for like comparison.</span></span></span>
Language models substantially improve performance over lexicon-only decoding for uroman models, both supervised and zero-shot, since they can mitigate uroman to character mapping ambiguities.<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>For instance, the letter ‘a’ can be represented with diacritical marks (e.g., ‘á’, ‘â’, ‘å’), which are all mapped to ‘a’ during romanization which in turn leads to ambiguity during decoding.
</span></span></span>
The gap between MMS Zero-shot and the supervised systems varies depending on the benchmark:
on MMS-lab languages, it is relatively large which we suspect is due to the narrow domain of the data that is based on biblical texts, and supervised models perform over-proportionally well due to the relative simplicity of the setting.
However, the gap shrinks for FLEURS whose domain is not as constrained as MMS-lab.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:268.5pt;height:76.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.7pt,6.8pt) scale(0.85,0.85) ;">
<table id="S4.T3.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.1.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T3.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">MMS dev</th>
<th id="S4.T3.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">MMS test</th>
<th id="S4.T3.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">FLEURS dev</th>
<th id="S4.T3.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">FLEURS test</th>
</tr>
<tr id="S4.T3.2.1.2.2" class="ltx_tr">
<th id="S4.T3.2.1.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T3.2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">lex</th>
<th id="S4.T3.2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">1grm</th>
<th id="S4.T3.2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">lex</th>
<th id="S4.T3.2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">1grm</th>
<th id="S4.T3.2.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">lex</th>
<th id="S4.T3.2.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">1grm</th>
<th id="S4.T3.2.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">lex</th>
<th id="S4.T3.2.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">1grm</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.1.3.1" class="ltx_tr">
<th id="S4.T3.2.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">In-domain</th>
<td id="S4.T3.2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">16.9</td>
<td id="S4.T3.2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">10.4</td>
<td id="S4.T3.2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">19.3</td>
<td id="S4.T3.2.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">11.7</td>
<td id="S4.T3.2.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">39.6</td>
<td id="S4.T3.2.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">37.6</td>
<td id="S4.T3.2.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">41.4</td>
<td id="S4.T3.2.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">38.2</td>
</tr>
<tr id="S4.T3.2.1.4.2" class="ltx_tr">
<th id="S4.T3.2.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Crúbadán</th>
<td id="S4.T3.2.1.4.2.2" class="ltx_td ltx_align_center">17.7</td>
<td id="S4.T3.2.1.4.2.3" class="ltx_td ltx_align_center">14.4</td>
<td id="S4.T3.2.1.4.2.4" class="ltx_td ltx_align_center">22.7</td>
<td id="S4.T3.2.1.4.2.5" class="ltx_td ltx_align_center">18.3</td>
<td id="S4.T3.2.1.4.2.6" class="ltx_td ltx_align_center">38.7</td>
<td id="S4.T3.2.1.4.2.7" class="ltx_td ltx_align_center">38.4</td>
<td id="S4.T3.2.1.4.2.8" class="ltx_td ltx_align_center">42.2</td>
<td id="S4.T3.2.1.4.2.9" class="ltx_td ltx_align_center">40.8</td>
</tr>
<tr id="S4.T3.2.1.5.3" class="ltx_tr">
<th id="S4.T3.2.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Panlex</th>
<td id="S4.T3.2.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb">51.4</td>
<td id="S4.T3.2.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S4.T3.2.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb">52.3</td>
<td id="S4.T3.2.1.5.3.5" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S4.T3.2.1.5.3.6" class="ltx_td ltx_align_center ltx_border_bb">57.7</td>
<td id="S4.T3.2.1.5.3.7" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S4.T3.2.1.5.3.8" class="ltx_td ltx_align_center ltx_border_bb">52.8</td>
<td id="S4.T3.2.1.5.3.9" class="ltx_td ltx_align_center ltx_border_bb">-</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.4.2" class="ltx_text" style="font-size:90%;">Comparison of Crúbadán and Panlex text resources to in-domain text data for MMS-lab/FLEURS.
We report CER on 71 (30/32/4/5) MMS-lab dev/test and FLEURS dev/test languages common to both text resources.
</span></figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:334.4pt;height:61.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-29.5pt,5.4pt) scale(0.85,0.85) ;">
<table id="S4.T4.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.1.1.1" class="ltx_tr">
<th id="S4.T4.2.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T4.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">MMS dev</th>
<th id="S4.T4.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">MMS test</th>
<th id="S4.T4.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">FLEURS dev</th>
<th id="S4.T4.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">FLEURS test</th>
<th id="S4.T4.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="2">AVG</th>
</tr>
<tr id="S4.T4.2.1.2.2" class="ltx_tr">
<th id="S4.T4.2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column">Model</th>
<th id="S4.T4.2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">lex</th>
<th id="S4.T4.2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">1grm</th>
<th id="S4.T4.2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">lex</th>
<th id="S4.T4.2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">1grm</th>
<th id="S4.T4.2.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">lex</th>
<th id="S4.T4.2.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">1grm</th>
<th id="S4.T4.2.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">lex</th>
<th id="S4.T4.2.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">1grm</th>
<th id="S4.T4.2.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">lex</th>
<th id="S4.T4.2.1.2.2.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">1grm</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.1.3.1" class="ltx_tr">
<td id="S4.T4.2.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t">mono-phone</td>
<td id="S4.T4.2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">15.3</td>
<td id="S4.T4.2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">7.7</td>
<td id="S4.T4.2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">14.4</td>
<td id="S4.T4.2.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">5.8</td>
<td id="S4.T4.2.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">54.0</td>
<td id="S4.T4.2.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">49.0</td>
<td id="S4.T4.2.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">53.4</td>
<td id="S4.T4.2.1.3.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41.0</td>
<td id="S4.T4.2.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t">34.3</td>
<td id="S4.T4.2.1.3.1.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">27.6</td>
</tr>
<tr id="S4.T4.2.1.4.2" class="ltx_tr">
<td id="S4.T4.2.1.4.2.1" class="ltx_td ltx_align_left ltx_border_bb">mono-uroman</td>
<td id="S4.T4.2.1.4.2.2" class="ltx_td ltx_align_center ltx_border_bb">5.8</td>
<td id="S4.T4.2.1.4.2.3" class="ltx_td ltx_align_center ltx_border_bb">3.1</td>
<td id="S4.T4.2.1.4.2.4" class="ltx_td ltx_align_center ltx_border_bb">6.8</td>
<td id="S4.T4.2.1.4.2.5" class="ltx_td ltx_align_center ltx_border_bb">3.5</td>
<td id="S4.T4.2.1.4.2.6" class="ltx_td ltx_align_center ltx_border_bb">18.5</td>
<td id="S4.T4.2.1.4.2.7" class="ltx_td ltx_align_center ltx_border_bb">17.7</td>
<td id="S4.T4.2.1.4.2.8" class="ltx_td ltx_align_center ltx_border_bb">25.4</td>
<td id="S4.T4.2.1.4.2.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">20.8</td>
<td id="S4.T4.2.1.4.2.10" class="ltx_td ltx_align_center ltx_border_bb">14.1</td>
<td id="S4.T4.2.1.4.2.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">11.9</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.4.2" class="ltx_text" style="font-size:90%;">
Comparison between text representations for acoustic models.
A language-independent phonemizer (phone) performs less well than uroman transliterations.
</span></figcaption>
</figure>
<figure id="S4.T5" class="ltx_table">
<div id="S4.T5.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:170.4pt;height:61.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.0pt,5.4pt) scale(0.85,0.85) ;">
<table id="S4.T5.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.2.1.1.1" class="ltx_tr">
<th id="S4.T5.2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th id="S4.T5.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">lex</th>
<th id="S4.T5.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">1grm</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.2.1.2.1" class="ltx_tr">
<th id="S4.T5.2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">mono-uroman</th>
<td id="S4.T5.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">8.7</td>
<td id="S4.T5.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">4.6</td>
</tr>
<tr id="S4.T5.2.1.3.2" class="ltx_tr">
<th id="S4.T5.2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">mono-phone</th>
<td id="S4.T5.2.1.3.2.2" class="ltx_td ltx_align_center">17.2</td>
<td id="S4.T5.2.1.3.2.3" class="ltx_td ltx_align_center">10.3</td>
</tr>
<tr id="S4.T5.2.1.4.3" class="ltx_tr">
<th id="S4.T5.2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">mono-phone (lang-specific)</th>
<td id="S4.T5.2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">3.2</td>
<td id="S4.T5.2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">2.0</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.3.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.4.2" class="ltx_text" style="font-size:90%;">Comparison between a uroman text encoding, a language-independent phonemizer (phone) and language-specific phonemizers to process text for eight MMS-lab languages.</span></figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">The difference between mono-char and mono-uroman shows that using romanization as text representation is inferior to the actual characters of each language but the drop is relatively small at 1.3% CER absolute when averaged over the four settings.
Overall, the CER of MMS Zero-shot is on average 2.5 times higher compared to character-based monolingual systems, however, the latter represent the ideal setting where in-domain labeled data is available and systems are focused on a single language.
We regard this as a very encouraging result, given the advantage the supervised systems in this comparison enjoy.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Leveraging Existing Low-Resource Text Data</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The zero-shot approach requires only text data in the unseen languages, however, obtaining such data can be challenging for less spoken languages.
To get a better sense of how well our approach performs, we evaluate it using the Crúbadán <cite class="ltx_cite ltx_citemacro_cite">Scannell (<a href="#bib.bib21" title="" class="ltx_ref">2007</a>)</cite> and Panlex <cite class="ltx_cite ltx_citemacro_cite">Kamholz et al. (<a href="#bib.bib12" title="" class="ltx_ref">2014</a>)</cite> low-resource text databases which enable constructing lexicons and even simple unigram language models similar to ASR-2K <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib14" title="" class="ltx_ref">2022a</a>)</cite> for very low resource languages.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Comparison to Supervised Models ‣ 4 Results and Analysis ‣ Scaling A Simple Approach to Zero-Shot Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows results for decoding with lexicons and language models on these databases for languages covered by these resources and compares to in-domain text data as a lower bound.
Crúbadán performs competitively to the in-domain setup with lexicon-only decoding for both MMS-lab and FLEURS.
In-domain unigram language models substantially improve over Crúbadán for MMS-lab but less so for FLEURS.
This may be due to the larger amount of MMS-lab data being available and MMS-lab being easier to transcribe because it is from a relatively narrow domain.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Panlex has much broader language coverage but performs less well due to the much smaller amounts of data:
for the approximately 1,700 languages covered by both databases, the median number of words per language in Panlex is 191 words compared to 4,670 for Crúbadán.
Moreover, Panlex contains very few words for many languages which makes it virtually impossible to use for many low resource languages.
It also contains a large amount of noise, and a handful of languages have incorrect scripts.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Amount of Text Data</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">To get a sense of how much text data to collect for a new language, we measure the accuracy of zero-shot decoding when using increasing amounts of text data in order to build lexicons and unigram language models.
Data is drawn from CommonCrawl (CC; <cite class="ltx_cite ltx_citemacro_citep">(Heafield, <a href="#bib.bib9" title="" class="ltx_ref">2011</a>; Conneau et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>), a web domain corpus, and we measure accuracy on 5 FLEURS dev languages which are low-resource.
We compare this to the ideal setting where we build lexicons and unigram LMs based on approximately 3k utterances of in-domain FLEURS text data (FLEURS-lex/1gram topline).</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Figure <a href="#S4.F2" title="Figure 2 ‣ 4.4 Amount of Text Data ‣ 4 Results and Analysis ‣ Scaling A Simple Approach to Zero-Shot Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that having 5k utterances would allow us to build a lexicon resulting in an average CER of 44.2, versus 40.4 with the in-domain lexicon.
More utterances does not necessarily lead to better results due to the many-to-one correspondence issue with uroman text discussed earlier.
The gap can be further reduced by using out-of-domain language models:
unigram models based on 20k utterances reduce CER to 40.5, compared to 38.6 CER with the FLEURS in-domain language models, however, gains plateau with more data.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><svg id="S4.F2.pic1" class="ltx_picture ltx_centering" height="241.82" overflow="visible" version="1.1" width="340.3"><g transform="translate(0,241.82) matrix(1 0 0 -1 0 0) translate(42.29,0) translate(0,55.02) matrix(1.0 0.0 0.0 1.0 -42.29 -55.02)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(67.11,0) translate(0,55.02)"><g stroke-width="0.4pt" fill="#BFBFBF" stroke="#BFBFBF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#BFBFBF"><path d="M -24.81 0 L 272.92 0 M -24.81 44.43 L 272.92 44.43 M -24.81 88.87 L 272.92 88.87 M -24.81 133.3 L 272.92 133.3 M -24.81 177.73 L 272.92 177.73" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 0 L 0 5.91 M 20.68 0 L 20.68 5.91 M 41.35 0 L 41.35 5.91 M 62.03 0 L 62.03 5.91 M 82.7 0 L 82.7 5.91 M 103.38 0 L 103.38 5.91 M 124.05 0 L 124.05 5.91 M 144.73 0 L 144.73 5.91 M 165.41 0 L 165.41 5.91 M 186.08 0 L 186.08 5.91 M 206.76 0 L 206.76 5.91 M 227.43 0 L 227.43 5.91 M 248.11 0 L 248.11 5.91 M 0 177.73 L 0 171.83 M 20.68 177.73 L 20.68 171.83 M 41.35 177.73 L 41.35 171.83 M 62.03 177.73 L 62.03 171.83 M 82.7 177.73 L 82.7 171.83 M 103.38 177.73 L 103.38 171.83 M 124.05 177.73 L 124.05 171.83 M 144.73 177.73 L 144.73 171.83 M 165.41 177.73 L 165.41 171.83 M 186.08 177.73 L 186.08 171.83 M 206.76 177.73 L 206.76 171.83 M 227.43 177.73 L 227.43 171.83 M 248.11 177.73 L 248.11 171.83" style="fill:none"></path></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M -24.81 0 L -18.91 0 M -24.81 44.43 L -18.91 44.43 M -24.81 88.87 L -18.91 88.87 M -24.81 133.3 L -18.91 133.3 M -24.81 177.73 L -18.91 177.73 M 272.92 0 L 267.02 0 M 272.92 44.43 L 267.02 44.43 M 272.92 88.87 L 267.02 88.87 M 272.92 133.3 L 267.02 133.3 M 272.92 177.73 L 267.02 177.73" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="0.4pt"><path d="M -24.81 0 L -24.81 177.73 L 272.92 177.73 L 272.92 0 L -24.81 0 Z" style="fill:none"></path><g transform="matrix(0.7071 0.7071 -0.7071 0.7071 -17.73 -24.52)" fill="#000000" stroke="#000000"><foreignObject width="24.98" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.6.6.6.6.6.1.1" class="ltx_text">0.1k</span></foreignObject></g><g transform="matrix(0.7071 0.7071 -0.7071 0.7071 2.95 -24.52)" fill="#000000" stroke="#000000"><foreignObject width="24.98" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.7.7.7.7.7.1.1" class="ltx_text">0.5k</span></foreignObject></g><g transform="matrix(0.7071 0.7071 -0.7071 0.7071 31.24 -16.91)" fill="#000000" stroke="#000000"><foreignObject width="14.22" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.8.8.8.8.8.1.1" class="ltx_text">1k</span></foreignObject></g><g transform="matrix(0.7071 0.7071 -0.7071 0.7071 51.91 -16.91)" fill="#000000" stroke="#000000"><foreignObject width="14.22" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.9.9.9.9.9.1.1" class="ltx_text">2k</span></foreignObject></g><g transform="matrix(0.7071 0.7071 -0.7071 0.7071 72.59 -16.91)" fill="#000000" stroke="#000000"><foreignObject width="14.22" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.10.10.10.10.10.1.1" class="ltx_text">3k</span></foreignObject></g><g transform="matrix(0.7071 0.7071 -0.7071 0.7071 93.26 -16.91)" fill="#000000" stroke="#000000"><foreignObject width="14.22" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.11.11.11.11.11.1.1" class="ltx_text">4k</span></foreignObject></g><g transform="matrix(0.7071 0.7071 -0.7071 0.7071 113.94 -16.91)" fill="#000000" stroke="#000000"><foreignObject width="14.22" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.12.12.12.12.12.1.1" class="ltx_text">5k</span></foreignObject></g><g transform="matrix(0.7071 0.7071 -0.7071 0.7071 129.72 -21.8)" fill="#000000" stroke="#000000"><foreignObject width="21.14" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.13.13.13.13.13.1.1" class="ltx_text">10k</span></foreignObject></g><g transform="matrix(0.7071 0.7071 -0.7071 0.7071 150.4 -21.8)" fill="#000000" stroke="#000000"><foreignObject width="21.14" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.14.14.14.14.14.1.1" class="ltx_text">20k</span></foreignObject></g><g transform="matrix(0.7071 0.7071 -0.7071 0.7071 171.07 -21.8)" fill="#000000" stroke="#000000"><foreignObject width="21.14" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.15.15.15.15.15.1.1" class="ltx_text">30k</span></foreignObject></g><g transform="matrix(0.7071 0.7071 -0.7071 0.7071 191.75 -21.8)" fill="#000000" stroke="#000000"><foreignObject width="21.14" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.16.16.16.16.16.1.1" class="ltx_text">40k</span></foreignObject></g><g transform="matrix(0.7071 0.7071 -0.7071 0.7071 212.42 -21.8)" fill="#000000" stroke="#000000"><foreignObject width="21.14" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.17.17.17.17.17.1.1" class="ltx_text">50k</span></foreignObject></g><g transform="matrix(0.7071 0.7071 -0.7071 0.7071 228.21 -26.69)" fill="#000000" stroke="#000000"><foreignObject width="28.06" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.18.18.18.18.18.1.1" class="ltx_text">500k</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -43.54 -4.46)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="35" display="inline"><semantics id="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">35</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">35</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">35</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -43.54 39.97)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">40</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -43.54 84.41)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="45" display="inline"><semantics id="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">45</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">45</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">45</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -43.54 128.84)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">50</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -43.54 173.28)" fill="#000000" stroke="#000000"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="55" display="inline"><semantics id="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">55</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp13"><path d="M -24.81 0 L 272.92 0 L 272.92 177.73 L -24.81 177.73 Z"></path></clipPath><g clip-path="url(#pgfcp13)"><g stroke="#0000FF" fill="#0000FF" stroke-width="0.8pt" color="#0000FF"><path d="M 0 131.52 L 20.68 102.2 L 41.35 94.2 L 62.03 92.42 L 82.7 87.09 L 103.38 81.76 L 124.05 80.87 L 144.73 79.98 L 165.41 79.98 L 186.08 87.09 L 206.76 87.98 L 227.43 91.53 L 248.11 133.3" style="fill:none"></path></g><g></g><g stroke="#FF0000" fill="#FF0000" stroke-width="0.8pt" color="#FF0000"><path d="M 0 130.63 L 20.68 102.2 L 41.35 92.42 L 62.03 79.98 L 82.7 69.32 L 103.38 63.1 L 124.05 58.65 L 144.73 55.99 L 165.41 49.77 L 186.08 47.99 L 206.76 48.88 L 227.43 54.21 L 248.11 54.21" style="fill:none"></path></g><g></g><g stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" fill="#0000FF" stroke="#0000FF" stroke-width="0.8pt" color="#0000FF"><path d="M 0 47.99 L 248.11 47.99" style="fill:none"></path></g><g></g><g stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" fill="#FF0000" stroke="#FF0000" stroke-width="0.8pt" color="#FF0000"><path d="M 0 31.99 L 248.11 31.99" style="fill:none"></path></g><g></g></g><g stroke="#0000FF" fill="#0000FF" stroke-width="0.8pt" color="#0000FF"><path d="M 2.77 131.52 C 2.77 133.05 1.53 134.29 0 134.29 C -1.53 134.29 -2.77 133.05 -2.77 131.52 C -2.77 129.99 -1.53 128.76 0 128.76 C 1.53 128.76 2.77 129.99 2.77 131.52 Z M 0 131.52" style="fill:none"></path><path d="M 23.44 102.2 C 23.44 103.73 22.2 104.96 20.68 104.96 C 19.15 104.96 17.91 103.73 17.91 102.2 C 17.91 100.67 19.15 99.43 20.68 99.43 C 22.2 99.43 23.44 100.67 23.44 102.2 Z M 20.68 102.2" style="fill:none"></path><path d="M 44.12 94.2 C 44.12 95.73 42.88 96.97 41.35 96.97 C 39.82 96.97 38.58 95.73 38.58 94.2 C 38.58 92.67 39.82 91.43 41.35 91.43 C 42.88 91.43 44.12 92.67 44.12 94.2 Z M 41.35 94.2" style="fill:none"></path><path d="M 64.79 92.42 C 64.79 93.95 63.56 95.19 62.03 95.19 C 60.5 95.19 59.26 93.95 59.26 92.42 C 59.26 90.89 60.5 89.65 62.03 89.65 C 63.56 89.65 64.79 90.89 64.79 92.42 Z M 62.03 92.42" style="fill:none"></path><path d="M 85.47 87.09 C 85.47 88.62 84.23 89.86 82.7 89.86 C 81.17 89.86 79.94 88.62 79.94 87.09 C 79.94 85.56 81.17 84.32 82.7 84.32 C 84.23 84.32 85.47 85.56 85.47 87.09 Z M 82.7 87.09" style="fill:none"></path><path d="M 106.15 81.76 C 106.15 83.29 104.91 84.53 103.38 84.53 C 101.85 84.53 100.61 83.29 100.61 81.76 C 100.61 80.23 101.85 78.99 103.38 78.99 C 104.91 78.99 106.15 80.23 106.15 81.76 Z M 103.38 81.76" style="fill:none"></path><path d="M 126.82 80.87 C 126.82 82.4 125.58 83.64 124.05 83.64 C 122.53 83.64 121.29 82.4 121.29 80.87 C 121.29 79.34 122.53 78.1 124.05 78.1 C 125.58 78.1 126.82 79.34 126.82 80.87 Z M 124.05 80.87" style="fill:none"></path><path d="M 147.5 79.98 C 147.5 81.51 146.26 82.75 144.73 82.75 C 143.2 82.75 141.96 81.51 141.96 79.98 C 141.96 78.45 143.2 77.21 144.73 77.21 C 146.26 77.21 147.5 78.45 147.5 79.98 Z M 144.73 79.98" style="fill:none"></path><path d="M 168.17 79.98 C 168.17 81.51 166.93 82.75 165.41 82.75 C 163.88 82.75 162.64 81.51 162.64 79.98 C 162.64 78.45 163.88 77.21 165.41 77.21 C 166.93 77.21 168.17 78.45 168.17 79.98 Z M 165.41 79.98" style="fill:none"></path><path d="M 188.85 87.09 C 188.85 88.62 187.61 89.86 186.08 89.86 C 184.55 89.86 183.31 88.62 183.31 87.09 C 183.31 85.56 184.55 84.32 186.08 84.32 C 187.61 84.32 188.85 85.56 188.85 87.09 Z M 186.08 87.09" style="fill:none"></path><path d="M 209.52 87.98 C 209.52 89.51 208.29 90.75 206.76 90.75 C 205.23 90.75 203.99 89.51 203.99 87.98 C 203.99 86.45 205.23 85.21 206.76 85.21 C 208.29 85.21 209.52 86.45 209.52 87.98 Z M 206.76 87.98" style="fill:none"></path><path d="M 230.2 91.53 C 230.2 93.06 228.96 94.3 227.43 94.3 C 225.9 94.3 224.67 93.06 224.67 91.53 C 224.67 90 225.9 88.77 227.43 88.77 C 228.96 88.77 230.2 90 230.2 91.53 Z M 227.43 91.53" style="fill:none"></path><path d="M 250.88 133.3 C 250.88 134.83 249.64 136.07 248.11 136.07 C 246.58 136.07 245.34 134.83 245.34 133.3 C 245.34 131.77 246.58 130.53 248.11 130.53 C 249.64 130.53 250.88 131.77 250.88 133.3 Z M 248.11 133.3" style="fill:none"></path></g><g stroke="#FF0000" fill="#FF0000" stroke-width="0.8pt" color="#FF0000"><path d="M 2.77 130.63 C 2.77 132.16 1.53 133.4 0 133.4 C -1.53 133.4 -2.77 132.16 -2.77 130.63 C -2.77 129.11 -1.53 127.87 0 127.87 C 1.53 127.87 2.77 129.11 2.77 130.63 Z M 0 130.63" style="fill:none"></path><path d="M 23.44 102.2 C 23.44 103.73 22.2 104.96 20.68 104.96 C 19.15 104.96 17.91 103.73 17.91 102.2 C 17.91 100.67 19.15 99.43 20.68 99.43 C 22.2 99.43 23.44 100.67 23.44 102.2 Z M 20.68 102.2" style="fill:none"></path><path d="M 44.12 92.42 C 44.12 93.95 42.88 95.19 41.35 95.19 C 39.82 95.19 38.58 93.95 38.58 92.42 C 38.58 90.89 39.82 89.65 41.35 89.65 C 42.88 89.65 44.12 90.89 44.12 92.42 Z M 41.35 92.42" style="fill:none"></path><path d="M 64.79 79.98 C 64.79 81.51 63.56 82.75 62.03 82.75 C 60.5 82.75 59.26 81.51 59.26 79.98 C 59.26 78.45 60.5 77.21 62.03 77.21 C 63.56 77.21 64.79 78.45 64.79 79.98 Z M 62.03 79.98" style="fill:none"></path><path d="M 85.47 69.32 C 85.47 70.84 84.23 72.08 82.7 72.08 C 81.17 72.08 79.94 70.84 79.94 69.32 C 79.94 67.79 81.17 66.55 82.7 66.55 C 84.23 66.55 85.47 67.79 85.47 69.32 Z M 82.7 69.32" style="fill:none"></path><path d="M 106.15 63.1 C 106.15 64.62 104.91 65.86 103.38 65.86 C 101.85 65.86 100.61 64.62 100.61 63.1 C 100.61 61.57 101.85 60.33 103.38 60.33 C 104.91 60.33 106.15 61.57 106.15 63.1 Z M 103.38 63.1" style="fill:none"></path><path d="M 126.82 58.65 C 126.82 60.18 125.58 61.42 124.05 61.42 C 122.53 61.42 121.29 60.18 121.29 58.65 C 121.29 57.12 122.53 55.88 124.05 55.88 C 125.58 55.88 126.82 57.12 126.82 58.65 Z M 124.05 58.65" style="fill:none"></path><path d="M 147.5 55.99 C 147.5 57.51 146.26 58.75 144.73 58.75 C 143.2 58.75 141.96 57.51 141.96 55.99 C 141.96 54.46 143.2 53.22 144.73 53.22 C 146.26 53.22 147.5 54.46 147.5 55.99 Z M 144.73 55.99" style="fill:none"></path><path d="M 168.17 49.77 C 168.17 51.29 166.93 52.53 165.41 52.53 C 163.88 52.53 162.64 51.29 162.64 49.77 C 162.64 48.24 163.88 47 165.41 47 C 166.93 47 168.17 48.24 168.17 49.77 Z M 165.41 49.77" style="fill:none"></path><path d="M 188.85 47.99 C 188.85 49.52 187.61 50.76 186.08 50.76 C 184.55 50.76 183.31 49.52 183.31 47.99 C 183.31 46.46 184.55 45.22 186.08 45.22 C 187.61 45.22 188.85 46.46 188.85 47.99 Z M 186.08 47.99" style="fill:none"></path><path d="M 209.52 48.88 C 209.52 50.41 208.29 51.64 206.76 51.64 C 205.23 51.64 203.99 50.41 203.99 48.88 C 203.99 47.35 205.23 46.11 206.76 46.11 C 208.29 46.11 209.52 47.35 209.52 48.88 Z M 206.76 48.88" style="fill:none"></path><path d="M 230.2 54.21 C 230.2 55.74 228.96 56.98 227.43 56.98 C 225.9 56.98 224.67 55.74 224.67 54.21 C 224.67 52.68 225.9 51.44 227.43 51.44 C 228.96 51.44 230.2 52.68 230.2 54.21 Z M 227.43 54.21" style="fill:none"></path><path d="M 250.88 54.21 C 250.88 55.74 249.64 56.98 248.11 56.98 C 246.58 56.98 245.34 55.74 245.34 54.21 C 245.34 52.68 246.58 51.44 248.11 51.44 C 249.64 51.44 250.88 52.68 250.88 54.21 Z M 248.11 54.21" style="fill:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 35.96 -47.71)" fill="#000000" stroke="#000000"><foreignObject width="176.19" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.19.19.19.19.19.1.1" class="ltx_text"># CommonCrawl Utterances</span></foreignObject></g><g transform="matrix(0.0 1.0 -1.0 0.0 -53.04 74.07)" fill="#000000" stroke="#000000"><foreignObject width="29.6" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.20.20.20.20.20.1.1" class="ltx_text">CER</span></foreignObject></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 34.11 104.46)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 56.77)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.13)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.55,0)" fill="#0000FF" stroke="#0000FF" stroke-width="0.8pt" color="#0000FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 14.58 0 C 14.58 1.53 13.34 2.77 11.81 2.77 C 10.28 2.77 9.04 1.53 9.04 0 C 9.04 -1.53 10.28 -2.77 11.81 -2.77 C 13.34 -2.77 14.58 -1.53 14.58 0 Z M 11.81 0" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.29)" fill="#000000" stroke="#000000"><foreignObject width="37.71" height="8.65" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.21.21.21.21.21.1.1.1.1.1" class="ltx_text" style="font-size:90%;">CC-lex</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 24.32)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.55,0)" fill="#FF0000" stroke="#FF0000" stroke-width="0.8pt" color="#FF0000"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 14.58 0 C 14.58 1.53 13.34 2.77 11.81 2.77 C 10.28 2.77 9.04 1.53 9.04 0 C 9.04 -1.53 10.28 -2.77 11.81 -2.77 C 13.34 -2.77 14.58 -1.53 14.58 0 Z M 11.81 0" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.22)" fill="#000000" stroke="#000000"><foreignObject width="56.07" height="10.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.22.22.22.22.22.2.2.1.1.1" class="ltx_text" style="font-size:90%;">CC-1gram</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 40.51)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.55,0)" fill="#0000FF" stroke="#0000FF" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" stroke-width="0.8pt" color="#0000FF"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.29)" fill="#000000" stroke="#000000"><foreignObject width="111.04" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.23.23.23.23.23.3.3.1.1.1" class="ltx_text" style="font-size:90%;">FLEURS-lex topline</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 56.77)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.55,0)" fill="#FF0000" stroke="#FF0000" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" stroke-width="0.8pt" color="#FF0000"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 25.01 0) translate(-0.28,0) matrix(1.0 0.0 0.0 1.0 3.04 -3.29)" fill="#000000" stroke="#000000"><foreignObject width="129.41" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F2.pic1.24.24.24.24.24.4.4.1.1.1" class="ltx_text" style="font-size:90%;">FLEURS-1gram topline</span></foreignObject></g></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">
Accuracy on FLEURS dev languages when increasing the amount of CommonCrawl text data to build lexicons and unigram LMs compared to using in-domain text data (FLEURS-topline; 3k utterances).

</span></figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation of Text Representation</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">A key difference between ASR-2K and our approach is the text representation of the acoustic model.
To provide a controlled like for like comparison between the text representations, we train supervised monolingual models outputting either phonemes (mono-phone) or romanized (mono-uroman) text on eight mid- to high-resource MMS-lab languages.<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>These languages are eng, hin, vie, nya, tgl, ara, spa and khm.</span></span></span>
The phoneme models are trained on transcriptions processed by a multilingual phonemizer <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2022b</a>)</cite> and use lexicons mapping words to phoneme sequences.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">The results (Table <a href="#S4.T4" title="Table 4 ‣ 4.2 Comparison to Supervised Models ‣ 4 Results and Analysis ‣ Scaling A Simple Approach to Zero-Shot Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) show that mono-uroman models perform much better than mono-phone without any language specific representation across all settings, reducing the average CER by a relative 57-59%.
The performance difference is largely due to the phonemizer not performing well on many unseen languages <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2022b</a>)</cite>.
When we switch to language-specific phonemizers<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a target="_blank" href="https://github.com/dmort27/epitran" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/dmort27/epitran</a></span></span></span> then the performance vastly improves (Table <a href="#S4.T5" title="Table 5 ‣ 4.2 Comparison to Supervised Models ‣ 4 Results and Analysis ‣ Scaling A Simple Approach to Zero-Shot Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), however, access to a language-specific phonemizer for unseen languages is highly unlikely.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We present an improved zero-shot approach to automatic speech recognition which is based on uroman transliteration and an acoustic model trained on three orders of magnitude more languages, both of which yield substantial improvements over prior art.
The approach requires only a moderate amount of text data to enable ASR for unseen languages and reduces the character error rate on average by a relative 46% over ASR-2K on 100 languages.
Compared to supervised systems using trigram language models, the zero-shot approach produces error rates which are only about 2.5x higher.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ardila et al. (2020)</span>
<span class="ltx_bibblock">
Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, and Reuben et al. Morais.

</span>
<span class="ltx_bibblock">Common voice: A massively-multilingual speech corpus.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proc. of LREC</em>, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proc. of NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. (2021)</span>
<span class="ltx_bibblock">
Alexei Baevski, Wei-Ning Hsu, Alexis Conneau, and Michael Auli.

</span>
<span class="ltx_bibblock">Unsupervised speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proc. of NeurIPS</em>, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Campbell (2008)</span>
<span class="ltx_bibblock">
Lyle Campbell.

</span>
<span class="ltx_bibblock">Ethnologue: Languages of the world, 2008.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Kuan-Yu Chen, Che-Ping Tsai, Da-Rong Liu, Hung-Yi Lee, and Lin shan Lee.

</span>
<span class="ltx_bibblock">Completely unsupervised speech recognition by a generative adversarial network harmonized with iteratively refined hidden markov models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proc. of Interspeech</em>, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning for speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv</em>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2023)</span>
<span class="ltx_bibblock">
Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna.

</span>
<span class="ltx_bibblock">Fleurs: Few-shot learning evaluation of universal representations of speech.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. of SLT</em>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves et al. (2006)</span>
<span class="ltx_bibblock">
Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proc. of ICML</em>, 2006.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heafield (2011)</span>
<span class="ltx_bibblock">
Kenneth Heafield.

</span>
<span class="ltx_bibblock">Kenlm: Faster and smaller language model queries.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. of WMT</em>, 2011.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hermjakob et al. (2018)</span>
<span class="ltx_bibblock">
Ulf Hermjakob, Jonathan May, and Kevin Knight.

</span>
<span class="ltx_bibblock">Out-of-the-box universal romanization tool uroman.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proc. of ACL</em>, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kahn et al. (2022)</span>
<span class="ltx_bibblock">
Jacob D Kahn, Vineel Pratap, Tatiana Likhomanenko, Qiantong Xu, Awni Hannun, Jeff Cai, et al.

</span>
<span class="ltx_bibblock">Flashlight: Enabling innovation in tools for machine learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proc. of ICML</em>, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamholz et al. (2014)</span>
<span class="ltx_bibblock">
David Kamholz, Jonathan Pool, and Susan M Colowick.

</span>
<span class="ltx_bibblock">Panlex: Building a resource for panlingual lexical translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proc. of LREC</em>, 2014.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Xinjian Li, Siddharth Dalmia, Juncheng Li, Matthew Lee, Patrick Littell, Jiali Yao, et al.

</span>
<span class="ltx_bibblock">Universal phone recognition with a multilingual allophone system.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proc. of ICASSP</em>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022a)</span>
<span class="ltx_bibblock">
Xinjian Li, Florian Metze, David R Mortensen, Alan W Black, and Shinji Watanabe.

</span>
<span class="ltx_bibblock">Asr2k: Speech recognition for around 2000 languages without audio.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv</em>, 2022a.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022b)</span>
<span class="ltx_bibblock">
Xinjian Li, Florian Metze, David R Mortensen, Shinji Watanabe, and Alan W Black.

</span>
<span class="ltx_bibblock">Zero-shot learning for grapheme to phoneme conversion with language ensemble.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proc. of ACL</em>, 2022b.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2018)</span>
<span class="ltx_bibblock">
Da-Rong Liu, Kuan-Yu Chen, Hung-Yi Lee, and Lin shan Lee.

</span>
<span class="ltx_bibblock">Completely unsupervised phoneme recognition by adversarially learning mapping relationships from audio embeddings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proc. of Interspeech</em>, 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mortensen et al. (2020)</span>
<span class="ltx_bibblock">
David R Mortensen, Xinjian Li, Patrick Littell, Alexis Michaud, Shruti Rijhwani, Antonios Anastasopoulos, Alan W Black, Florian Metze, and Graham Neubig.

</span>
<span class="ltx_bibblock">Allovera: A multilingual allophone database.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv</em>, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pratap et al. (2019)</span>
<span class="ltx_bibblock">
Vineel Pratap, Awni Hannun, Qiantong Xu, Jeff Cai, Jacob Kahn, Gabriel Synnaeve, Vitaliy Liptchinsky, and Ronan Collobert.

</span>
<span class="ltx_bibblock">Wav2letter++: A fast open-source speech recognition system.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proc. of ICASSP</em>, 2019.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pratap et al. (2024)</span>
<span class="ltx_bibblock">
Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, et al.

</span>
<span class="ltx_bibblock">Scaling speech technology to 1,000+ languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">JMLR</em>, 2024.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2023)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proc. of ICML</em>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scannell (2007)</span>
<span class="ltx_bibblock">
Kevin P Scannell.

</span>
<span class="ltx_bibblock">The crúbadán project: Corpus building for under-resourced languages.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proc. of WAC</em>, 2007.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van den Oord et al. (2018)</span>
<span class="ltx_bibblock">
Aäron van den Oord, Yazhe Li, and Oriol Vinyals.

</span>
<span class="ltx_bibblock">Representation learning with contrastive predictive coding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proc. of NeurIPS</em>, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wellisch et al. (1978)</span>
<span class="ltx_bibblock">
Hans H Wellisch, Richard Foreman, Lee Breuer, and Robert Wilson.

</span>
<span class="ltx_bibblock">The conversion of scripts, its nature, history, and utilization.

</span>
<span class="ltx_bibblock">1978.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li, and Vera Axelrod et al.

</span>
<span class="ltx_bibblock">Google usm: Scaling automatic speech recognition beyond 100 languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.17851" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.17852" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.17852">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.17852" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.17853" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 18:39:36 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
