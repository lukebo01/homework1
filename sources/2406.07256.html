<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.07256] AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection</title><meta property="og:description" content="The rapid advancements in speech technologies over the past two decades have led to human-level performance in tasks like automatic speech recognition (ASR) for fluent speech. However, the efficacy of these models dimi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.07256">

<!--Generated on Fri Jul  5 22:31:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1*]RongGong
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=2*]HongfeiXue
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=1]LezhiWang
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=3]XinXu
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>[affiliation=4]QishengLi
<span id="p1.3.5" class="ltx_ERROR undefined">\name</span>[affiliation=2]LeiXie
<span id="p1.3.6" class="ltx_ERROR undefined">\name</span>[affiliation=3]HuiBu
<span id="p1.3.7" class="ltx_ERROR undefined">\name</span>[affiliation=4]ShaomeiWu
<span id="p1.3.8" class="ltx_ERROR undefined">\name</span>[affiliation=5]JiamingZhou
<span id="p1.3.9" class="ltx_ERROR undefined">\name</span>[affiliation=5]YongQin
<span id="p1.3.10" class="ltx_ERROR undefined">\name</span>[affiliation=6]BinbinZhang
<span id="p1.3.11" class="ltx_ERROR undefined">\name</span>[affiliation=7]JunDu
<span id="p1.3.12" class="ltx_ERROR undefined">\name</span>[affiliation=1]JiaBin
<span id="p1.3.13" class="ltx_ERROR undefined">\name</span>[affiliation=8]MingLi




</p>
</div>
<h1 class="ltx_title ltx_title_document">AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id18.id1" class="ltx_p">The rapid advancements in speech technologies over the past two decades have led to human-level performance in tasks like automatic speech recognition (ASR) for fluent speech. However, the efficacy of these models diminishes when applied to atypical speech, such as stuttering. This paper introduces AS-70, the first publicly available Mandarin stuttered speech dataset, which stands out as the largest dataset in its category. Encompassing conversational and voice command reading speech, AS-70 includes verbatim manual transcription, rendering it suitable for various speech-related tasks. Furthermore, baseline systems are established, and experimental results are presented for ASR and stuttering event detection (SED) tasks. By incorporating this dataset into the model fine-tuning, significant improvements in the state-of-the-art ASR models, e.g., Whisper and Hubert, are observed, enhancing their inclusivity in addressing stuttered speech.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>mandarin stuttered speech dataset, speech recognition, stuttering event detection
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Stuttering is a speech impediment that affects around 1% of the world's population <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The impact of the stuttering on people who have such a problem could be on both social functioning and mental aspects. Stuttering hinders daily oral communication with speech repetition, prolongation, blocks, and secondary behaviors, such as body movements and facial grimaces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Early children who stutter (CWS) have around 80% of a chance to recover within four years of the onset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Early intervention by speech-language therapists (SLP) is crucial for increasing the recovery rate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Compared with North American or Western European countries where the practice of SLP has been evolving for quite some time, the related practices in Mainland China are still in their infancy. Due to the lack of professionals, many families of children who stutter cannot receive timely diagnosis. While adding more professionals, automatic stuttering diagnosis can also meet the needs of some families.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">When a person's stuttering continues into adulthood, the chance of full recovery becomes very low, and there is a high probability that stuttering will accompany them throughout their lives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. To alleviate their communication barriers or, in a broad sense, eliminate social discrimination, relevant products need to be designed to be inclusive to meet their needs.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">With the rise of smart home devices and chatbot technology, e.g., Alexa and ChatGPT, voice-user interfaces have become indispensable tools in many of our lives. Although current ASR systems can handle fluent speech well <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, they encounter difficulties in recognizing stuttered speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The underlying reasons for this challenge may involve various factors, including insufficient data or a lack of awareness regarding the necessity to develop systems tailored for PWS.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Related works</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Due to the highly sensitive and personal nature of the data, large-scale stuttering speech datasets collected by companies are typically not made openly accessible to the research community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. On the other hand, openly accessible datasets curated by academic researchers, such as FluencyBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and UCLASS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, are often small and primarily intended for use in SLP resources. Larger open datasets, while available, may require additional completeness in terms of annotation and authenticity. For instance, the Sep-28k dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, comprising 28k 3-second podcast audio clips, only provides stuttering labels for the SED task but lacks accompanying text transcription. Meanwhile, the LibriStutter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> dataset includes text transcriptions, but it is artificially generated from recordings of fluent speech. Furthermore, it is noteworthy that all these open stuttering speech datasets are limited to two Western languages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">The primary focus of most ASR research on stuttered speech is directed towards predicting semantic rather than verbatim transcription. Lea et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> observed enhanced performance by modifying the ASR decoder, specifically by increasing the language model weight and imposing a higher penalty for word insertions. Shonibare et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> introduced a frame-level stuttering classifier, termed Detect-and-Pass, designed for the RNNT ASR model. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> demonstrated performance improvement through fine-tuning the ASR model using synthesized stuttering speech. Alharbi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> utilized ASR to generate orthographic transcriptions that include stuttering words.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">SED involves identifying instances of stuttering in speech audio clips. Research in this domain predominantly focused on four openly available datasets: Sep-28k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, FluencyBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, LibriStutter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, and KSoF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Different neural network architectures, including ConvLSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, Stutternet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and SE-Resnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, were developed specifically for this task. Additionally, machine learning techniques like multi-task learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> were employed to enhance the accuracy of stuttering detection.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">The subsequent sections of this paper showcase our primary contribution - the AS-70 dataset, which can be accessed from this download location<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.aishelltech.com/AISHELL_6A" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aishelltech.com/AISHELL_6A</a></span></span></span>. In Section <a href="#S2" title="2 Dataset ‣ AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we elaborate on the data collection, annotation process, and conduct a descriptive analysis that compares AS-70 with other openly available stuttered speech datasets. In Section <a href="#S3" title="3 Experiments ‣ AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we establish baseline systems for the ASR and SED tasks using the AS-70 dataset.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Dataset</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Participants and recording sessions</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The AS-70 dataset was recorded between January 2023 and October 2023 by two of the authors, both adults who stutter (AWS) and native Mandarin speakers. A total of 70 native Mandarin AWS took part in the recording sessions, with 24 of them being female, establishing a male-to-female ratio of 1.9:1. There was no gender selection process as all willing candidates were accepted to participate in the recording sessions.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Each participant engaged in a recording session lasting up to one hour, comprising two parts: conversation and voice command reading. Conversations were conducted through online interviews using platforms like Zoom or Tencent Meet, aiming to capture spontaneous speech on diverse topics. The interviewer, one of the two authors, posed questions based on a prepared list, with the flexibility to introduce impromptu questions as needed.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In the voice command reading part, participants were tasked with reading a set of 200 commands, categorized into car navigation and smart home device interaction. To ensure variety, a new set of 200 commands was introduced for every 25 participants, resulting in a dataset featuring a total of 600 unique commands. Participants were encouraged to employ the Voluntary Stuttering technique, deliberately introducing stuttering.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Annotation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The dataset annotation process involved 15 non-PWS annotators and 5 quality controllers (QCs). Among the 5 QCs, 4 are non-PWS, and 1 is the PWS author (QC0). All annotators and QCs are experienced Mandarin speech annotators, with some having prior experience annotating atypical speech. To ensure annotation quality, QC0 conducted a training session prior to the commencement of the annotation process. In this session, the 19 non-PWS annotators and QCs were initially tasked with annotating a few examples of stuttered speech. Following this, QC0 provided feedback, and the 19 annotators were then asked to annotate new examples, incorporating the lessons learned from the feedback. This iterative process was repeated multiple times until no further feedback was deemed necessary.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Five types of stuttering were specified by the annotation guidelines, including:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">[]</span>: <span id="S2.I1.i1.p1.1.2" class="ltx_text ltx_font_bold">Word/phrase repetition</span>. Designated for marking entire repeated character or phrase.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">/b</span>: <span id="S2.I1.i2.p1.1.2" class="ltx_text ltx_font_bold">block</span>. Gasps for air or stuttered pauses.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">/p</span>: <span id="S2.I1.i3.p1.1.2" class="ltx_text ltx_font_bold">prolongation</span>. Elongated phoneme.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p"><span id="S2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">/r</span>: <span id="S2.I1.i4.p1.1.2" class="ltx_text ltx_font_bold">sound repetition</span>. Repeated phoneme that do not constitute an entire character.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i5.p1" class="ltx_para">
<p id="S2.I1.i5.p1.1" class="ltx_p"><span id="S2.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">/i</span>: <span id="S2.I1.i5.p1.1.2" class="ltx_text ltx_font_bold">interjections</span>. Filler characters due to stuttering e.g., `嗯', `啊', or `呃'. Notably, naturally occurring interjections that don't disrupt the speech flow are excluded.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">The annotation was performed in a verbatim manner, with stuttering labels embedded as markups. An example of the annotated transcription can be ``嗯/i/p，我[我我]的名/b字是小/r明。". The character `嗯' is an interjection and also is prolonged. The character `我' is repeated twice additionally. A block annotation is applied to the character `名'. Furthermore, sound repetition is identified in one of the phonemes of `小'. It is noteworthy that a single character may carry multiple labels. Additional examples of audio and annotations can be accessed through this link<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://stammertalk.github.io/interspeech2024-page" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://stammertalk.github.io/interspeech2024-page</a></span></span></span>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">The conversation and command reading parts of each session were annotated separately by different annotators. Subsequently, one of the 4 non-PWS QCs conducted a cross-check of the annotations. To ensure consistent annotation, QC0 then performed a final cross-check of all session annotations. The annotators noted that this annotation process required approximately three times more time compared to annotating fluent speech.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Personal Identifiable Information (PII) such as name, address, birth date, and workplace has been redacted from the transcription. The audio segments containing PII have been muted. A data collection agreement, adhering to Guidance
for personal information security impact assessment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, was signed between the data collectors and the participant.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Descriptive analysis</h3>

<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Dataset scale and scope as characterized by speech duration (Duration), the number and types of speakers (Speakers), whether it provides speech transcription (Transcription), types of speaking tasks (Tasks), and Language. * Limited to the transcribed portion of the dataset. ** Including two interviewers. Abbreviations: AWS - adults who stutter; CWS - children who stutter; PWS - people who stutter.</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<span id="S2.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.1.1.1" class="ltx_p" style="width:65.0pt;"><span id="S2.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
</span>
</th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<span id="S2.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.2.1.1" class="ltx_p" style="width:34.7pt;"><span id="S2.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Duration</span></span>
</span>
</th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<span id="S2.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.3.1.1" class="ltx_p" style="width:56.4pt;"><span id="S2.T1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Speakers</span></span>
</span>
</th>
<th id="S2.T1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<span id="S2.T1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.4.1.1" class="ltx_p" style="width:56.4pt;"><span id="S2.T1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Transcription</span></span>
</span>
</th>
<th id="S2.T1.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<span id="S2.T1.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.5.1.1" class="ltx_p" style="width:130.1pt;"><span id="S2.T1.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Tasks</span></span>
</span>
</th>
<th id="S2.T1.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S2.T1.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.6.1.1" class="ltx_p" style="width:34.7pt;"><span id="S2.T1.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold">Language</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.1.1.1" class="ltx_p" style="width:65.0pt;">LibriStutter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite></span>
</span>
</td>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.2.1.1" class="ltx_p" style="width:34.7pt;">20 hrs</span>
</span>
</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.3.1.1" class="ltx_p" style="width:56.4pt;">50 non-PWS</span>
</span>
</td>
<td id="S2.T1.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.4.1.1" class="ltx_p" style="width:56.4pt;">Yes</span>
</span>
</td>
<td id="S2.T1.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.5.1.1" class="ltx_p" style="width:130.1pt;">audiobook</span>
</span>
</td>
<td id="S2.T1.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.6.1.1" class="ltx_p" style="width:34.7pt;">English</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.1.1.1" class="ltx_p" style="width:65.0pt;">UCLASS* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite></span>
</span>
</td>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.2.1.1" class="ltx_p" style="width:34.7pt;">53 mins</span>
</span>
</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.3.1.1" class="ltx_p" style="width:56.4pt;">25 CWS</span>
</span>
</td>
<td id="S2.T1.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.4.1.1" class="ltx_p" style="width:56.4pt;">Yes</span>
</span>
</td>
<td id="S2.T1.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.5.1.1" class="ltx_p" style="width:130.1pt;">conversation</span>
</span>
</td>
<td id="S2.T1.1.3.2.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.6.1.1" class="ltx_p" style="width:34.7pt;">English</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.1.1.1" class="ltx_p" style="width:65.0pt;">SEP-28k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite></span>
</span>
</td>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.2.1.1" class="ltx_p" style="width:34.7pt;">23 hrs</span>
</span>
</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.3.1.1" class="ltx_p" style="width:56.4pt;">not reported</span>
</span>
</td>
<td id="S2.T1.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.4.1.1" class="ltx_p" style="width:56.4pt;">No</span>
</span>
</td>
<td id="S2.T1.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.5.1.1" class="ltx_p" style="width:130.1pt;">podcast</span>
</span>
</td>
<td id="S2.T1.1.4.3.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.6.1.1" class="ltx_p" style="width:34.7pt;">English</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.1.1.1" class="ltx_p" style="width:65.0pt;">FluencyBank* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite></span>
</span>
</td>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.2.1.1" class="ltx_p" style="width:34.7pt;">3.5 hrs</span>
</span>
</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.3.1.1" class="ltx_p" style="width:56.4pt;">32 AWS</span>
</span>
</td>
<td id="S2.T1.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.4.1.1" class="ltx_p" style="width:56.4pt;">Yes</span>
</span>
</td>
<td id="S2.T1.1.5.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.5.1.1" class="ltx_p" style="width:130.1pt;">conversation and reading</span>
</span>
</td>
<td id="S2.T1.1.5.4.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.6.1.1" class="ltx_p" style="width:34.7pt;">English</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<td id="S2.T1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.1.1.1" class="ltx_p" style="width:65.0pt;">KSoF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span>
</span>
</td>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.2.1.1" class="ltx_p" style="width:34.7pt;">4.6 hrs</span>
</span>
</td>
<td id="S2.T1.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.3.1.1" class="ltx_p" style="width:56.4pt;">37 PWS</span>
</span>
</td>
<td id="S2.T1.1.6.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.4.1.1" class="ltx_p" style="width:56.4pt;">No</span>
</span>
</td>
<td id="S2.T1.1.6.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.5.1.1" class="ltx_p" style="width:130.1pt;">spontaneous and reading</span>
</span>
</td>
<td id="S2.T1.1.6.5.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.6.1.1" class="ltx_p" style="width:34.7pt;">German</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.7.6" class="ltx_tr">
<td id="S2.T1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r">
<span id="S2.T1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.1.1.1" class="ltx_p" style="width:65.0pt;"><span id="S2.T1.1.7.6.1.1.1.1" class="ltx_text ltx_font_bold">AS-70</span></span>
</span>
</td>
<td id="S2.T1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r">
<span id="S2.T1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.2.1.1" class="ltx_p" style="width:34.7pt;"><span id="S2.T1.1.7.6.2.1.1.1" class="ltx_text ltx_font_bold">48.8 hrs</span></span>
</span>
</td>
<td id="S2.T1.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r">
<span id="S2.T1.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.3.1.1" class="ltx_p" style="width:56.4pt;"><span id="S2.T1.1.7.6.3.1.1.1" class="ltx_text ltx_font_bold">72 AWS**</span></span>
</span>
</td>
<td id="S2.T1.1.7.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r">
<span id="S2.T1.1.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.4.1.1" class="ltx_p" style="width:56.4pt;"><span id="S2.T1.1.7.6.4.1.1.1" class="ltx_text ltx_font_bold">Yes (verbatim)</span></span>
</span>
</td>
<td id="S2.T1.1.7.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r">
<span id="S2.T1.1.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.5.1.1" class="ltx_p" style="width:130.1pt;"><span id="S2.T1.1.7.6.5.1.1.1" class="ltx_text ltx_font_bold">conversation, voice commands</span></span>
</span>
</td>
<td id="S2.T1.1.7.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S2.T1.1.7.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.6.1.1" class="ltx_p" style="width:34.7pt;"><span id="S2.T1.1.7.6.6.1.1.1" class="ltx_text ltx_font_bold">Mandarin</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Distribution of annotated stuttering events. FluencyBank's annotation was done in another work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. <span id="S2.T2.2.1" class="ltx_text ltx_font_bold">AS-70 Conversation</span> includes two interviewers.</figcaption>
<table id="S2.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.3.1.1" class="ltx_tr">
<th id="S2.T2.3.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt"></th>
<th id="S2.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S2.T2.3.1.1.2.1" class="ltx_text ltx_font_bold">Avg. Stuttering Rate</span></th>
<th id="S2.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S2.T2.3.1.1.3.1" class="ltx_text ltx_font_bold">Total</span></th>
<th id="S2.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5"><span id="S2.T2.3.1.1.4.1" class="ltx_text ltx_font_bold">Event Type Distribution %</span></th>
</tr>
<tr id="S2.T2.3.2.2" class="ltx_tr">
<th id="S2.T2.3.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="S2.T2.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S2.T2.3.2.2.2.1" class="ltx_text ltx_font_bold">(per minute)</span></th>
<th id="S2.T2.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S2.T2.3.2.2.3.1" class="ltx_text ltx_font_bold">Stuttering Events</span></th>
<th id="S2.T2.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T2.3.2.2.4.1" class="ltx_text ltx_font_bold">[]</span></th>
<th id="S2.T2.3.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T2.3.2.2.5.1" class="ltx_text ltx_font_bold">/b</span></th>
<th id="S2.T2.3.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T2.3.2.2.6.1" class="ltx_text ltx_font_bold">/p</span></th>
<th id="S2.T2.3.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T2.3.2.2.7.1" class="ltx_text ltx_font_bold">/r</span></th>
<th id="S2.T2.3.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T2.3.2.2.8.1" class="ltx_text ltx_font_bold">/i</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.3.3.1" class="ltx_tr">
<td id="S2.T2.3.3.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">LibriStutter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</td>
<td id="S2.T2.3.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.5</td>
<td id="S2.T2.3.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15,000</td>
<td id="S2.T2.3.3.1.4" class="ltx_td ltx_align_center ltx_border_t">20.0</td>
<td id="S2.T2.3.3.1.5" class="ltx_td ltx_align_center ltx_border_t">20.0</td>
<td id="S2.T2.3.3.1.6" class="ltx_td ltx_align_center ltx_border_t">20.0</td>
<td id="S2.T2.3.3.1.7" class="ltx_td ltx_align_center ltx_border_t">20.0</td>
<td id="S2.T2.3.3.1.8" class="ltx_td ltx_align_center ltx_border_t">20.0</td>
</tr>
<tr id="S2.T2.3.4.2" class="ltx_tr">
<td id="S2.T2.3.4.2.1" class="ltx_td ltx_align_left ltx_border_r">SEP-28k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S2.T2.3.4.2.2" class="ltx_td ltx_align_center ltx_border_r">12.26</td>
<td id="S2.T2.3.4.2.3" class="ltx_td ltx_align_center ltx_border_r">17,267</td>
<td id="S2.T2.3.4.2.4" class="ltx_td ltx_align_center">16.0</td>
<td id="S2.T2.3.4.2.5" class="ltx_td ltx_align_center">19.5</td>
<td id="S2.T2.3.4.2.6" class="ltx_td ltx_align_center">16.3</td>
<td id="S2.T2.3.4.2.7" class="ltx_td ltx_align_center">13.6</td>
<td id="S2.T2.3.4.2.8" class="ltx_td ltx_align_center">34.6</td>
</tr>
<tr id="S2.T2.3.5.3" class="ltx_tr">
<td id="S2.T2.3.5.3.1" class="ltx_td ltx_align_left ltx_border_r">FluencyBank<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S2.T2.3.5.3.2" class="ltx_td ltx_align_center ltx_border_r">13.88</td>
<td id="S2.T2.3.5.3.3" class="ltx_td ltx_align_center ltx_border_r">2,875</td>
<td id="S2.T2.3.5.3.4" class="ltx_td ltx_align_center">15.0</td>
<td id="S2.T2.3.5.3.5" class="ltx_td ltx_align_center">14.9</td>
<td id="S2.T2.3.5.3.6" class="ltx_td ltx_align_center">11.8</td>
<td id="S2.T2.3.5.3.7" class="ltx_td ltx_align_center">19.1</td>
<td id="S2.T2.3.5.3.8" class="ltx_td ltx_align_center">39.3</td>
</tr>
<tr id="S2.T2.3.6.4" class="ltx_tr">
<td id="S2.T2.3.6.4.1" class="ltx_td ltx_align_left ltx_border_r">KSoF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S2.T2.3.6.4.2" class="ltx_td ltx_align_center ltx_border_r">13.05</td>
<td id="S2.T2.3.6.4.3" class="ltx_td ltx_align_center ltx_border_r">3,602</td>
<td id="S2.T2.3.6.4.4" class="ltx_td ltx_align_center">6.0</td>
<td id="S2.T2.3.6.4.5" class="ltx_td ltx_align_center">32.2</td>
<td id="S2.T2.3.6.4.6" class="ltx_td ltx_align_center">18.7</td>
<td id="S2.T2.3.6.4.7" class="ltx_td ltx_align_center">22.9</td>
<td id="S2.T2.3.6.4.8" class="ltx_td ltx_align_center">20.2</td>
</tr>
<tr id="S2.T2.3.7.5" class="ltx_tr">
<td id="S2.T2.3.7.5.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T2.3.7.5.1.1" class="ltx_text ltx_font_bold">AS-70 Conversation</span></td>
<td id="S2.T2.3.7.5.2" class="ltx_td ltx_align_center ltx_border_r">15.58</td>
<td id="S2.T2.3.7.5.3" class="ltx_td ltx_align_center ltx_border_r">29,017</td>
<td id="S2.T2.3.7.5.4" class="ltx_td ltx_align_center">42.7</td>
<td id="S2.T2.3.7.5.5" class="ltx_td ltx_align_center">6.9</td>
<td id="S2.T2.3.7.5.6" class="ltx_td ltx_align_center">19.8</td>
<td id="S2.T2.3.7.5.7" class="ltx_td ltx_align_center">8.7</td>
<td id="S2.T2.3.7.5.8" class="ltx_td ltx_align_center">21.9</td>
</tr>
<tr id="S2.T2.3.8.6" class="ltx_tr">
<td id="S2.T2.3.8.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S2.T2.3.8.6.1.1" class="ltx_text ltx_font_bold">AS-70 Command</span></td>
<td id="S2.T2.3.8.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">8.11</td>
<td id="S2.T2.3.8.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">8,636</td>
<td id="S2.T2.3.8.6.4" class="ltx_td ltx_align_center ltx_border_bb">53.0</td>
<td id="S2.T2.3.8.6.5" class="ltx_td ltx_align_center ltx_border_bb">8.4</td>
<td id="S2.T2.3.8.6.6" class="ltx_td ltx_align_center ltx_border_bb">16.9</td>
<td id="S2.T2.3.8.6.7" class="ltx_td ltx_align_center ltx_border_bb">15.8</td>
<td id="S2.T2.3.8.6.8" class="ltx_td ltx_align_center ltx_border_bb">5.9</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S2.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Stuttering event distribution of SED task datasets. <span id="S2.T3.3.1" class="ltx_text ltx_font_bold">no-dis</span>: no-disfluency. <span id="S2.T3.4.2" class="ltx_text ltx_font_bold">AS-70 Conversation</span> includes two interviewers.</figcaption>
<table id="S2.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T3.5.1.1" class="ltx_tr">
<th id="S2.T3.5.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt"></th>
<th id="S2.T3.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S2.T3.5.1.1.2.1" class="ltx_text ltx_font_bold">Avg. length (s)</span></th>
<th id="S2.T3.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S2.T3.5.1.1.3.1" class="ltx_text ltx_font_bold">Number</span></th>
<th id="S2.T3.5.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="6"><span id="S2.T3.5.1.1.4.1" class="ltx_text ltx_font_bold">Event Type Distribution %</span></th>
</tr>
<tr id="S2.T3.5.2.2" class="ltx_tr">
<th id="S2.T3.5.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="S2.T3.5.2.2.2" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="S2.T3.5.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S2.T3.5.2.2.3.1" class="ltx_text ltx_font_bold">of clips</span></th>
<th id="S2.T3.5.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T3.5.2.2.4.1" class="ltx_text ltx_font_bold">[]</span></th>
<th id="S2.T3.5.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T3.5.2.2.5.1" class="ltx_text ltx_font_bold">/b</span></th>
<th id="S2.T3.5.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T3.5.2.2.6.1" class="ltx_text ltx_font_bold">/p</span></th>
<th id="S2.T3.5.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T3.5.2.2.7.1" class="ltx_text ltx_font_bold">/r</span></th>
<th id="S2.T3.5.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T3.5.2.2.8.1" class="ltx_text ltx_font_bold">/i</span></th>
<th id="S2.T3.5.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S2.T3.5.2.2.9.1" class="ltx_text ltx_font_bold">no-dis</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T3.5.3.1" class="ltx_tr">
<td id="S2.T3.5.3.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">SEP-28k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S2.T3.5.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.00</td>
<td id="S2.T3.5.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28,177</td>
<td id="S2.T3.5.3.1.4" class="ltx_td ltx_align_center ltx_border_t">9.8</td>
<td id="S2.T3.5.3.1.5" class="ltx_td ltx_align_center ltx_border_t">12.0</td>
<td id="S2.T3.5.3.1.6" class="ltx_td ltx_align_center ltx_border_t">10.0</td>
<td id="S2.T3.5.3.1.7" class="ltx_td ltx_align_center ltx_border_t">8.3</td>
<td id="S2.T3.5.3.1.8" class="ltx_td ltx_align_center ltx_border_t">21.2</td>
<td id="S2.T3.5.3.1.9" class="ltx_td ltx_align_center ltx_border_t">56.9</td>
</tr>
<tr id="S2.T3.5.4.2" class="ltx_tr">
<td id="S2.T3.5.4.2.1" class="ltx_td ltx_align_left ltx_border_r">FluencyBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S2.T3.5.4.2.2" class="ltx_td ltx_align_center ltx_border_r">3.00</td>
<td id="S2.T3.5.4.2.3" class="ltx_td ltx_align_center ltx_border_r">4,144</td>
<td id="S2.T3.5.4.2.4" class="ltx_td ltx_align_center">10.38</td>
<td id="S2.T3.5.4.2.5" class="ltx_td ltx_align_center">10.33</td>
<td id="S2.T3.5.4.2.6" class="ltx_td ltx_align_center">8.16</td>
<td id="S2.T3.5.4.2.7" class="ltx_td ltx_align_center">13.25</td>
<td id="S2.T3.5.4.2.8" class="ltx_td ltx_align_center">27.27</td>
<td id="S2.T3.5.4.2.9" class="ltx_td ltx_align_center">54.10</td>
</tr>
<tr id="S2.T3.5.5.3" class="ltx_tr">
<td id="S2.T3.5.5.3.1" class="ltx_td ltx_align_left ltx_border_r">KSoF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S2.T3.5.5.3.2" class="ltx_td ltx_align_center ltx_border_r">3.00</td>
<td id="S2.T3.5.5.3.3" class="ltx_td ltx_align_center ltx_border_r">5,597</td>
<td id="S2.T3.5.5.3.4" class="ltx_td ltx_align_center">3.88</td>
<td id="S2.T3.5.5.3.5" class="ltx_td ltx_align_center">20.74</td>
<td id="S2.T3.5.5.3.6" class="ltx_td ltx_align_center">12.02</td>
<td id="S2.T3.5.5.3.7" class="ltx_td ltx_align_center">14.76</td>
<td id="S2.T3.5.5.3.8" class="ltx_td ltx_align_center">12.97</td>
<td id="S2.T3.5.5.3.9" class="ltx_td ltx_align_center">24.75</td>
</tr>
<tr id="S2.T3.5.6.4" class="ltx_tr">
<td id="S2.T3.5.6.4.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T3.5.6.4.1.1" class="ltx_text ltx_font_bold">AS-70 Conversation</span></td>
<td id="S2.T3.5.6.4.2" class="ltx_td ltx_align_center ltx_border_r">5.69</td>
<td id="S2.T3.5.6.4.3" class="ltx_td ltx_align_center ltx_border_r">19,654</td>
<td id="S2.T3.5.6.4.4" class="ltx_td ltx_align_center">38.67</td>
<td id="S2.T3.5.6.4.5" class="ltx_td ltx_align_center">8.45</td>
<td id="S2.T3.5.6.4.6" class="ltx_td ltx_align_center">22.12</td>
<td id="S2.T3.5.6.4.7" class="ltx_td ltx_align_center">10.77</td>
<td id="S2.T3.5.6.4.8" class="ltx_td ltx_align_center">24.80</td>
<td id="S2.T3.5.6.4.9" class="ltx_td ltx_align_center">36.31</td>
</tr>
<tr id="S2.T3.5.7.5" class="ltx_tr">
<td id="S2.T3.5.7.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S2.T3.5.7.5.1.1" class="ltx_text ltx_font_bold">AS-70 Command</span></td>
<td id="S2.T3.5.7.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">2.87</td>
<td id="S2.T3.5.7.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">22,299</td>
<td id="S2.T3.5.7.5.4" class="ltx_td ltx_align_center ltx_border_bb">14.09</td>
<td id="S2.T3.5.7.5.5" class="ltx_td ltx_align_center ltx_border_bb">2.87</td>
<td id="S2.T3.5.7.5.6" class="ltx_td ltx_align_center ltx_border_bb">5.31</td>
<td id="S2.T3.5.7.5.7" class="ltx_td ltx_align_center ltx_border_bb">5.08</td>
<td id="S2.T3.5.7.5.8" class="ltx_td ltx_align_center ltx_border_bb">1.76</td>
<td id="S2.T3.5.7.5.9" class="ltx_td ltx_align_center ltx_border_bb">79.61</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Table <a href="#S2.T1" title="Table 1 ‣ 2.3 Descriptive analysis ‣ 2 Dataset ‣ AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents a comparison of dataset scales between AS-70 and 4 other open datasets. A total duration of 48.8-hours speech data were included in AS-70 dataset from 70 recording sessions. Excluding two interviewers, each participant contributed on average 457 utterances and 33.0 minutes of total speech, with an average of 141 utterances and 17.8 minutes for conversations and an average of 316 utterances and 15.23 minutes for voice command reading.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Table <a href="#S2.T1" title="Table 1 ‣ 2.3 Descriptive analysis ‣ 2 Dataset ‣ AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> indicates that the AS-70 dataset is the largest among open datasets in terms of duration and participants. When compared to the previously largest open dataset, Sep-28k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which is annotated at a 3-second clip level without transcription, the AS-70 dataset is twice as large, annotated at the character level, and includes verbatim transcription. Moreover, the AS-70 dataset encompasses both spontaneous conversational and voice command reading speech tasks, and stands out as the only non-Western language open dataset.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Table <a href="#S2.T2" title="Table 2 ‣ 2.3 Descriptive analysis ‣ 2 Dataset ‣ AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> depicts the distribution of the stuttering events across four datasets. The <span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_italic">Average Stuttering Rate</span> is calculated by dividing the total count of stuttering events by the duration of the speech. The <span id="S2.SS3.p3.1.2" class="ltx_text ltx_font_italic">Event Type Distribution</span> is calculated by the formula ``number of labels of a certain stuttering event type / overall number of all stuttering event labels". It's important to note that a direct comparison between Sep-28k and AS-70 may not be entirely equitable. This is because AS-70 dataset is annotated on the character level, offering finer granularity compared to the clip level annotation of Sep-28k. This difference may partially account for the higher average stuttering rate observed in the AS-70 conversation part compared to Sep-28k.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Additionally, there is a significant shift towards more word and phrases repetitions and fewer sound repetitions in the AS-70 dataset, signaling potential phonological differences between stuttering in Chinese and in English. Meanwhile we notice that SEP-28k dataset contains almost twice more interjections than in AS-70 conversations, which could be attributed to different definitions of interjections in these two datasets. While SEP-28k includes any filler words, such as ``um,'' ``uh,'' and ``you know'' as stuttering interjections, our annotation excludes natural interjections that blend into the speech flow.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">In line with previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, we split the utterances of the AS-70 dataset into short clips for the SED task. Table <a href="#S2.T3" title="Table 3 ‣ 2.3 Descriptive analysis ‣ 2 Dataset ‣ AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides a comparison between the AS-70 dataset and other three datasets. Notably, AS-70 dataset contains 49% more clips than the Sep-28k dataset.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p">To compute the <span id="S2.SS3.p6.1.1" class="ltx_text ltx_font_italic">Event Type Distribution</span>, we employ the consistent formula as in the work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>: ``number of clips containing a certain stuttering event type / total number of clips". An interesting observation is that the conversation part of the AS-70 exhibits almost 4 times word/phrase repetitions and 2.2 times prolongation compared to Sep-28k. As a relatively easy task for our participants, the command reading task is reflected by a high no-disfluency rate, low interjection, and block rate.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The objective of the work in this section is not to attain state-of-the-art results but to highlight the limitations of specific pre-trained models through an evaluation of the AS-70 dataset. Additionally, we aim to demonstrate the enhancements achievable by integrating our dataset into the model training process.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data partition</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As highlighted in the study of Bayerl et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, the data partition method can heavily impact the reliability of experimental results. A data partition that neglects speaker exclusivity can cause a model to learn speaker-specific traits, resulting in overly optimistic results. We ensure that speakers with varying degrees of stuttering severity are distributed across each partition.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.4" class="ltx_p">The stuttering severity of a speaker is measured using the stuttering rate (<math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="SR" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><times id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></times><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝑆</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">SR</annotation></semantics></math>), calculated by dividing the number of stuttering events by the number of non-stuttering characters in the transcription. E.g., the annotation ``嗯/i/p，我[我我]的名/b字是小/r明。" would lead to a stuttering rate of 71.42% as there are 5 stuttering events and 7 non-stuttering characters. The stuttering severity is classified as mild: <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="SR&lt;=7\%" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mrow id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2.2" xref="S3.SS1.p2.2.m2.1.1.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.1.2.1" xref="S3.SS1.p2.2.m2.1.1.2.1.cmml">​</mo><mi id="S3.SS1.p2.2.m2.1.1.2.3" xref="S3.SS1.p2.2.m2.1.1.2.3.cmml">R</mi></mrow><mo id="S3.SS1.p2.2.m2.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.cmml">&lt;=</mo><mrow id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml"><mn id="S3.SS1.p2.2.m2.1.1.3.2" xref="S3.SS1.p2.2.m2.1.1.3.2.cmml">7</mn><mo id="S3.SS1.p2.2.m2.1.1.3.1" xref="S3.SS1.p2.2.m2.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><leq id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1"></leq><apply id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2"><times id="S3.SS1.p2.2.m2.1.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.1.2.1"></times><ci id="S3.SS1.p2.2.m2.1.1.2.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2.2">𝑆</ci><ci id="S3.SS1.p2.2.m2.1.1.2.3.cmml" xref="S3.SS1.p2.2.m2.1.1.2.3">𝑅</ci></apply><apply id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3"><csymbol cd="latexml" id="S3.SS1.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.p2.2.m2.1.1.3.1">percent</csymbol><cn type="integer" id="S3.SS1.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.p2.2.m2.1.1.3.2">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">SR&lt;=7\%</annotation></semantics></math> for 45 participants, moderate: <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="7\%&lt;SR&lt;=12\%" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mrow id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml"><mn id="S3.SS1.p2.3.m3.1.1.2.2" xref="S3.SS1.p2.3.m3.1.1.2.2.cmml">7</mn><mo id="S3.SS1.p2.3.m3.1.1.2.1" xref="S3.SS1.p2.3.m3.1.1.2.1.cmml">%</mo></mrow><mo id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">&lt;</mo><mrow id="S3.SS1.p2.3.m3.1.1.4" xref="S3.SS1.p2.3.m3.1.1.4.cmml"><mi id="S3.SS1.p2.3.m3.1.1.4.2" xref="S3.SS1.p2.3.m3.1.1.4.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.3.m3.1.1.4.1" xref="S3.SS1.p2.3.m3.1.1.4.1.cmml">​</mo><mi id="S3.SS1.p2.3.m3.1.1.4.3" xref="S3.SS1.p2.3.m3.1.1.4.3.cmml">R</mi></mrow><mo id="S3.SS1.p2.3.m3.1.1.5" xref="S3.SS1.p2.3.m3.1.1.5.cmml">&lt;=</mo><mrow id="S3.SS1.p2.3.m3.1.1.6" xref="S3.SS1.p2.3.m3.1.1.6.cmml"><mn id="S3.SS1.p2.3.m3.1.1.6.2" xref="S3.SS1.p2.3.m3.1.1.6.2.cmml">12</mn><mo id="S3.SS1.p2.3.m3.1.1.6.1" xref="S3.SS1.p2.3.m3.1.1.6.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><and id="S3.SS1.p2.3.m3.1.1a.cmml" xref="S3.SS1.p2.3.m3.1.1"></and><apply id="S3.SS1.p2.3.m3.1.1b.cmml" xref="S3.SS1.p2.3.m3.1.1"><lt id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3"></lt><apply id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2"><csymbol cd="latexml" id="S3.SS1.p2.3.m3.1.1.2.1.cmml" xref="S3.SS1.p2.3.m3.1.1.2.1">percent</csymbol><cn type="integer" id="S3.SS1.p2.3.m3.1.1.2.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2.2">7</cn></apply><apply id="S3.SS1.p2.3.m3.1.1.4.cmml" xref="S3.SS1.p2.3.m3.1.1.4"><times id="S3.SS1.p2.3.m3.1.1.4.1.cmml" xref="S3.SS1.p2.3.m3.1.1.4.1"></times><ci id="S3.SS1.p2.3.m3.1.1.4.2.cmml" xref="S3.SS1.p2.3.m3.1.1.4.2">𝑆</ci><ci id="S3.SS1.p2.3.m3.1.1.4.3.cmml" xref="S3.SS1.p2.3.m3.1.1.4.3">𝑅</ci></apply></apply><apply id="S3.SS1.p2.3.m3.1.1c.cmml" xref="S3.SS1.p2.3.m3.1.1"><leq id="S3.SS1.p2.3.m3.1.1.5.cmml" xref="S3.SS1.p2.3.m3.1.1.5"></leq><share href="#S3.SS1.p2.3.m3.1.1.4.cmml" id="S3.SS1.p2.3.m3.1.1d.cmml" xref="S3.SS1.p2.3.m3.1.1"></share><apply id="S3.SS1.p2.3.m3.1.1.6.cmml" xref="S3.SS1.p2.3.m3.1.1.6"><csymbol cd="latexml" id="S3.SS1.p2.3.m3.1.1.6.1.cmml" xref="S3.SS1.p2.3.m3.1.1.6.1">percent</csymbol><cn type="integer" id="S3.SS1.p2.3.m3.1.1.6.2.cmml" xref="S3.SS1.p2.3.m3.1.1.6.2">12</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">7\%&lt;SR&lt;=12\%</annotation></semantics></math> for 16 participants, and severe: <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="SR&gt;12\%" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mrow id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2.2" xref="S3.SS1.p2.4.m4.1.1.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.1.1.2.1" xref="S3.SS1.p2.4.m4.1.1.2.1.cmml">​</mo><mi id="S3.SS1.p2.4.m4.1.1.2.3" xref="S3.SS1.p2.4.m4.1.1.2.3.cmml">R</mi></mrow><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">&gt;</mo><mrow id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml"><mn id="S3.SS1.p2.4.m4.1.1.3.2" xref="S3.SS1.p2.4.m4.1.1.3.2.cmml">12</mn><mo id="S3.SS1.p2.4.m4.1.1.3.1" xref="S3.SS1.p2.4.m4.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><gt id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1"></gt><apply id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2"><times id="S3.SS1.p2.4.m4.1.1.2.1.cmml" xref="S3.SS1.p2.4.m4.1.1.2.1"></times><ci id="S3.SS1.p2.4.m4.1.1.2.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2.2">𝑆</ci><ci id="S3.SS1.p2.4.m4.1.1.2.3.cmml" xref="S3.SS1.p2.4.m4.1.1.2.3">𝑅</ci></apply><apply id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3"><csymbol cd="latexml" id="S3.SS1.p2.4.m4.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.1.1.3.1">percent</csymbol><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.1.1.3.2">12</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">SR&gt;12\%</annotation></semantics></math> for 9 participants. The speaker numbers of train/development/test partitions can be found in the link<a href="#footnote2" title="footnote 2 ‣ 2.2 Annotation ‣ 2 Dataset ‣ AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The data of two PWS interviewers is included in the train partition.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>ASR</h3>

<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>The CER(%) results of different ASR model.</figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<th id="S3.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Model</th>
<th id="S3.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Pre-trained Dataset</th>
<th id="S3.T4.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Fine-tuned Dataset</th>
<th id="S3.T4.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">mild</th>
<th id="S3.T4.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">moderate</th>
<th id="S3.T4.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">severe</th>
<th id="S3.T4.1.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">conversation</th>
<th id="S3.T4.1.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">command</th>
<th id="S3.T4.1.1.1.9" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt">all</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.1.2.1" class="ltx_tr">
<td id="S3.T4.1.2.1.1" class="ltx_td ltx_border_t"></td>
<td id="S3.T4.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">WenetSpeech</td>
<td id="S3.T4.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">-</td>
<td id="S3.T4.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">11.41</td>
<td id="S3.T4.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">17.81</td>
<td id="S3.T4.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t">33.21</td>
<td id="S3.T4.1.2.1.7" class="ltx_td ltx_align_left ltx_border_t">11.82</td>
<td id="S3.T4.1.2.1.8" class="ltx_td ltx_align_left ltx_border_t">20.40</td>
<td id="S3.T4.1.2.1.9" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">15.00</td>
</tr>
<tr id="S3.T4.1.3.2" class="ltx_tr">
<td id="S3.T4.1.3.2.1" class="ltx_td ltx_align_left"><span id="S3.T4.1.3.2.1.1" class="ltx_text">Conformer</span></td>
<td id="S3.T4.1.3.2.2" class="ltx_td ltx_align_left">WenetSpeech</td>
<td id="S3.T4.1.3.2.3" class="ltx_td ltx_align_left">AS-70</td>
<td id="S3.T4.1.3.2.4" class="ltx_td ltx_align_left">5.20</td>
<td id="S3.T4.1.3.2.5" class="ltx_td ltx_align_left"><span id="S3.T4.1.3.2.5.1" class="ltx_text ltx_font_bold">8.00</span></td>
<td id="S3.T4.1.3.2.6" class="ltx_td ltx_align_left">9.25</td>
<td id="S3.T4.1.3.2.7" class="ltx_td ltx_align_left"><span id="S3.T4.1.3.2.7.1" class="ltx_text ltx_font_bold">7.96</span></td>
<td id="S3.T4.1.3.2.8" class="ltx_td ltx_align_left">3.32</td>
<td id="S3.T4.1.3.2.9" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T4.1.3.2.9.1" class="ltx_text ltx_font_bold">6.24</span></td>
</tr>
<tr id="S3.T4.1.4.3" class="ltx_tr">
<td id="S3.T4.1.4.3.1" class="ltx_td ltx_border_t"></td>
<td id="S3.T4.1.4.3.2" class="ltx_td ltx_align_left ltx_border_t">WenetSpeech (unlabel)</td>
<td id="S3.T4.1.4.3.3" class="ltx_td ltx_align_left ltx_border_t">AISHELL-1</td>
<td id="S3.T4.1.4.3.4" class="ltx_td ltx_align_left ltx_border_t">16.28</td>
<td id="S3.T4.1.4.3.5" class="ltx_td ltx_align_left ltx_border_t">22.86</td>
<td id="S3.T4.1.4.3.6" class="ltx_td ltx_align_left ltx_border_t">26.76</td>
<td id="S3.T4.1.4.3.7" class="ltx_td ltx_align_left ltx_border_t">16.80</td>
<td id="S3.T4.1.4.3.8" class="ltx_td ltx_align_left ltx_border_t">23.79</td>
<td id="S3.T4.1.4.3.9" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">19.06</td>
</tr>
<tr id="S3.T4.1.5.4" class="ltx_tr">
<td id="S3.T4.1.5.4.1" class="ltx_td ltx_align_left"><span id="S3.T4.1.5.4.1.1" class="ltx_text">Hubert-large</span></td>
<td id="S3.T4.1.5.4.2" class="ltx_td ltx_align_left">WenetSpeech (unlabel)</td>
<td id="S3.T4.1.5.4.3" class="ltx_td ltx_align_left">AS-70</td>
<td id="S3.T4.1.5.4.4" class="ltx_td ltx_align_left">6.25</td>
<td id="S3.T4.1.5.4.5" class="ltx_td ltx_align_left">9.67</td>
<td id="S3.T4.1.5.4.6" class="ltx_td ltx_align_left"><span id="S3.T4.1.5.4.6.1" class="ltx_text ltx_font_bold">7.85</span></td>
<td id="S3.T4.1.5.4.7" class="ltx_td ltx_align_left">9.75</td>
<td id="S3.T4.1.5.4.8" class="ltx_td ltx_align_left"><span id="S3.T4.1.5.4.8.1" class="ltx_text ltx_font_bold">2.03</span></td>
<td id="S3.T4.1.5.4.9" class="ltx_td ltx_nopad_r ltx_align_left">7.25</td>
</tr>
<tr id="S3.T4.1.6.5" class="ltx_tr">
<td id="S3.T4.1.6.5.1" class="ltx_td ltx_border_t"></td>
<td id="S3.T4.1.6.5.2" class="ltx_td ltx_align_left ltx_border_t">Whisper</td>
<td id="S3.T4.1.6.5.3" class="ltx_td ltx_align_left ltx_border_t">-</td>
<td id="S3.T4.1.6.5.4" class="ltx_td ltx_align_left ltx_border_t">14.50</td>
<td id="S3.T4.1.6.5.5" class="ltx_td ltx_align_left ltx_border_t">28.50</td>
<td id="S3.T4.1.6.5.6" class="ltx_td ltx_align_left ltx_border_t">95.33</td>
<td id="S3.T4.1.6.5.7" class="ltx_td ltx_align_left ltx_border_t">17.83</td>
<td id="S3.T4.1.6.5.8" class="ltx_td ltx_align_left ltx_border_t">46.85</td>
<td id="S3.T4.1.6.5.9" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">27.20</td>
</tr>
<tr id="S3.T4.1.7.6" class="ltx_tr">
<td id="S3.T4.1.7.6.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T4.1.7.6.1.1" class="ltx_text">Whisper-large-v2</span></td>
<td id="S3.T4.1.7.6.2" class="ltx_td ltx_align_left ltx_border_bb">Whisper</td>
<td id="S3.T4.1.7.6.3" class="ltx_td ltx_align_left ltx_border_bb">AS-70</td>
<td id="S3.T4.1.7.6.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T4.1.7.6.4.1" class="ltx_text ltx_font_bold">5.18</span></td>
<td id="S3.T4.1.7.6.5" class="ltx_td ltx_align_left ltx_border_bb">13.46</td>
<td id="S3.T4.1.7.6.6" class="ltx_td ltx_align_left ltx_border_bb">18.92</td>
<td id="S3.T4.1.7.6.7" class="ltx_td ltx_align_left ltx_border_bb">10.19</td>
<td id="S3.T4.1.7.6.8" class="ltx_td ltx_align_left ltx_border_bb">5.74</td>
<td id="S3.T4.1.7.6.9" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">8.75</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We conduct an in-depth evaluation of ASR performance using the AS-70 dataset, focusing on three distinct models: Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. These models are selected to represent a comprehensive range of approaches to ASR, including supervised end-to-end ASR, self-supervised pretraining, and large-scale semi-supervised methods. To emphasize semantic transcription, the annotations undergo preprocessing to exclude stuttering event labels, stuttering characters, and punctuation.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Conformer.</span>
This system is based on Wenet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. U2++ is a unified two-pass framework with bidirectional attention decoders, which includes the future contextual information by a right-to-left attention decoder to improve the representative ability of the shared encoder and the performance during the rescoring stage.
We show the results of the u2++ conformer model pre-trained using WenetSpeech dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and the results of the fine-tuned model with AS-70 train partition.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Hubert.</span>
HuBERT has demonstrated remarkable efficacy in ASR through self-supervised learning. We employ the Chinese Hubert Large model as our pre-trained experiment system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. We use the results of AISHELL-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> fine-tuning as a baseline to compare with the model fine-tuned by the AS-70 dataset.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Whisper.</span>
Whisper exhibits exceptional proficiency across multiple languages, developed through semi-supervised training on a large-scale dataset. Our baseline employs the Whisper large-v2 model for direct inference. Subsequent efforts are channeled towards fine-tuning the Whisper model, specifically with the AS-70 dataset.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Results and discussion.</span>
Table <a href="#S3.T4" title="Table 4 ‣ 3.2 ASR ‣ 3 Experiments ‣ AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> indicates the above models' Character Error Rate (CER) results against the AS-70 test set. These results communicate a clear message: even though the above models have performed impressively with generic data, their proficiency in dealing with stuttering data seemed to falter. However, after fine-tuning using AS-70 data, all models exhibit commendable improvement. The conformer with AS-70 fine-tuning takes the lead, possibly owing to extensive labeled data in the conformer pretrain stage. On the other hand, Whisper seems to falter on severe data. Upon examination, we notice that Whisper tends to generate an overabundance of repeat labels when decoding utterances from severe-level PWS.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">An interesting finding from these experiments is the relatively low CER registered on command data processed by the AS-70 fine-tuned models, possibly due to the training set containing identical command text as that in the test set, with only variations being the speakers. We plan to address this limitation in the future work by considering the factor of command text in the data partition process. Examples of ASR results can be found in the link<a href="#footnote2" title="footnote 2 ‣ 2.2 Annotation ‣ 2 Dataset ‣ AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>SED</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The evaluation in SED is undertaken by establishing a fundamental benchmark predicated on random guessing, thus providing a comparative baseline for subsequent analyses. We employ some reputed SED methods, such as StutterNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, ConvLSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, Conformer and wav2vec2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, to assess the effectiveness of these methods.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">StutterNet.</span>
StutterNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> uses a time-delay neural network suitable for capturing contextual aspects of the disfluent utterances, which is trained on the MFCC input features. We reproduce the structure in Stutternet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> with 12.2M parameters. We use multi-task learning with two output branches: a fluent/dysfluent prediction and a soft prediction for each of the five event types.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">ConvLSTM.</span>
ConvLSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> input is a set of 40-dimensional mel-filterbank energy features. Feature maps from the convolution layer are combined after batch normalization and fed to three LSTM layers, which result in a 1.6M parameter size. We use the same multi-task learning as in StutterNet.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Conformer.</span>
The same Wenet conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> encoder architecture mentioned in the section <a href="#S3.SS2" title="3.2 ASR ‣ 3 Experiments ‣ AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> is applied. However, due to the small amount of training data, we use 3 conformer blocks, which result in 9.7M parameter size. We use single-task learning, which is to predict the five event types. The multi-label soft margin loss is used for the model training.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Wav2Vec2.0.</span>
Referring to Bayerl el al.'s approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, we fine-tune the wav2vec2.0 base model. The model we used for our initial experiments (Chinese-wav2vec2-base) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> is pretrained in an unsupervised manner on the WenetSpeech corpus. We use the same single-task learning as in Conformer.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>The F1-score(%) of different SED model. We report results for each label individually.</figcaption>
<table id="S3.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T5.1.1.1" class="ltx_tr">
<th id="S3.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Model</th>
<th id="S3.T5.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">/p</th>
<th id="S3.T5.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">/b</th>
<th id="S3.T5.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">/r</th>
<th id="S3.T5.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">[]</th>
<th id="S3.T5.1.1.1.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt">/i</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T5.1.2.1" class="ltx_tr">
<td id="S3.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Random guess</td>
<td id="S3.T5.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">15.65</td>
<td id="S3.T5.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">8.07</td>
<td id="S3.T5.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">15.34</td>
<td id="S3.T5.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">31.7</td>
<td id="S3.T5.1.2.1.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">17.03</td>
</tr>
<tr id="S3.T5.1.3.2" class="ltx_tr">
<td id="S3.T5.1.3.2.1" class="ltx_td ltx_align_left">StutterNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</td>
<td id="S3.T5.1.3.2.2" class="ltx_td ltx_align_left">61.07</td>
<td id="S3.T5.1.3.2.3" class="ltx_td ltx_align_left">33.33</td>
<td id="S3.T5.1.3.2.4" class="ltx_td ltx_align_left">47.81</td>
<td id="S3.T5.1.3.2.5" class="ltx_td ltx_align_left">50.12</td>
<td id="S3.T5.1.3.2.6" class="ltx_td ltx_nopad_r ltx_align_left">58.82</td>
</tr>
<tr id="S3.T5.1.4.3" class="ltx_tr">
<td id="S3.T5.1.4.3.1" class="ltx_td ltx_align_left">ConvLSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S3.T5.1.4.3.2" class="ltx_td ltx_align_left">33.30</td>
<td id="S3.T5.1.4.3.3" class="ltx_td ltx_align_left">18.22</td>
<td id="S3.T5.1.4.3.4" class="ltx_td ltx_align_left">30.19</td>
<td id="S3.T5.1.4.3.5" class="ltx_td ltx_align_left">64.02</td>
<td id="S3.T5.1.4.3.6" class="ltx_td ltx_nopad_r ltx_align_left">46.70</td>
</tr>
<tr id="S3.T5.1.5.4" class="ltx_tr">
<td id="S3.T5.1.5.4.1" class="ltx_td ltx_align_left">Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S3.T5.1.5.4.2" class="ltx_td ltx_align_left">66.77</td>
<td id="S3.T5.1.5.4.3" class="ltx_td ltx_align_left">30.94</td>
<td id="S3.T5.1.5.4.4" class="ltx_td ltx_align_left">46.84</td>
<td id="S3.T5.1.5.4.5" class="ltx_td ltx_align_left">65.49</td>
<td id="S3.T5.1.5.4.6" class="ltx_td ltx_nopad_r ltx_align_left">73.10</td>
</tr>
<tr id="S3.T5.1.6.5" class="ltx_tr">
<td id="S3.T5.1.6.5.1" class="ltx_td ltx_align_left ltx_border_bb">Wav2vec2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S3.T5.1.6.5.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T5.1.6.5.2.1" class="ltx_text ltx_font_bold">70.48</span></td>
<td id="S3.T5.1.6.5.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T5.1.6.5.3.1" class="ltx_text ltx_font_bold">42.51</span></td>
<td id="S3.T5.1.6.5.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T5.1.6.5.4.1" class="ltx_text ltx_font_bold">65.76</span></td>
<td id="S3.T5.1.6.5.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T5.1.6.5.5.1" class="ltx_text ltx_font_bold">78.48</span></td>
<td id="S3.T5.1.6.5.6" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb"><span id="S3.T5.1.6.5.6.1" class="ltx_text ltx_font_bold">83.80</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p"><span id="S3.SS3.p6.1.1" class="ltx_text ltx_font_bold">Results.</span>
Table <a href="#S3.T5" title="Table 5 ‣ 3.3 SED ‣ 3 Experiments ‣ AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> indicates the F1-score of the above models for the five stuttering event types. All models exceed the random guess baseline in each event type. ConvLSTM underperformed in comparison, which is attributed to its limited parameter count of 1.6m. StutterNet and Conformer display intermediate results, whereas wav2vec 2.0 emerged as the most proficient, benefiting significantly from its pre-training on a large dataset. It is worth noting that the block (/b) performance is substantially worse than the other stuttering types, probably attributed to the fact that blocks are often represented solely by silence in the speech. Similar findings are reported in Lea et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and Bayerl et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. For a detailed SED results on stuttering severities and speech tasks, please refer to the link<a href="#footnote2" title="footnote 2 ‣ 2.2 Annotation ‣ 2 Dataset ‣ AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This paper introduces the AS-70 dataset, which is the first publicly available Mandarin stuttered speech dataset and is notable for being the largest in its category. AS-70 consists of conversational and voice command reading speech recordings with verbatim manual transcription, making it suitable for various speech-related tasks. In addition, baseline systems have been established, and experimental results are presented for ASR and SED tasks, demonstrating significant improvements in state-of-the-art ASR models by incorporating this dataset into model fine-tuning processes. We hope that AS-70 can assist in detecting stuttering and aiding people who stutter in developing speech interaction systems.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
E. Yairi, N. Ambrose, and N. Cox, ``Genetics of stuttering,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Journal of Speech, Language, and Hearing Research</em>, vol. 39, pp. 771–784, 1996.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Prasse and G. Kikano, ``Stuttering: An overview,'' <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">American family physician</em>, vol. 77, pp. 1271–6, 2008.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
E. Yairi and N. G. Ambrose, ``Early childhood stuttering i,'' <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Journal of Speech, Language, and Hearing Research</em>, vol. 42, pp. 1097–1112, 1999.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Onslow and S. O'Brian, ``Management of childhood stuttering,'' <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Journal of Paediatrics and Child Health</em>, vol. 49, pp. E112–E115, 2013.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. O. Boyce, V. E. Jackson, O. van Reyk, R. Parker, A. P. Vogel, E. Eising, S. E. Horton, N. A. Gillespie, I. E. Scheffer, D. J. Amor, M. S. Hildebrand, S. E. Fisher, N. G. Martin, S. Reilly, M. Bahlo, and A. T. Morgan, ``Self-reported impact of developmental stuttering across the lifespan,'' <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Developmental Medicine &amp; Child Neurology</em>, vol. 64, pp. 1297–1306, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. Mcleavey, and I. Sutskever, ``Robust speech recognition via large-scale weak supervision,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ICML</em>, vol. 202.   PMLR, 2023, pp. 28 492–28 518.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
C. Lea, Z. Huang, J. Narain, L. Tooley, D. Yee, D. T. Tran, P. Georgiou, J. P. Bigham, and L. Findlater, ``From user perceptions to technical improvement: Enabling people who stutter to better use speech recognition,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>.   Association for Computing Machinery, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
O. Shonibare, X. Tong, and V. Ravichandran, ``Enhancing asr for stuttered speech with limited data using detect and pass,'' <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, vol. abs/2202.05396, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
B. MacDonald, P.-P. Jiang, J. Cattiau, R. Heywood, R. Cave, K. Seaver, M. Ladewig, J. Tobin, M. Brenner, P. Q. Nelson, J. R. Green, and K. Tomanek, ``Disordered speech data collection: Lessons learned at 1 million utterances from project euphonia,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>.   ISCA, 2021, pp. 4833–4837.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
V. Mitra, Z. Huang, C. Lea, L. Tooley, P. Georgiou, S. Kajarekar, and J. Bigham, ``Analysis and tuning of a voice assistant system for dysfluent speech,'' in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>.   ISCA, 2021, pp. 4848–4852.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
X. Zhang, I. Vallés-Pérez, A. Stolcke, C. Yu, J. Droppo, O. Shonibare, R. Barra-Chicote, and V. Ravichandran, ``Stutter-tts: Controlled synthesis and improved recognition of stuttered speech,'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">NeurIPS 2022 Workshop on SyntheticData4ML</em>, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
N. B. Ratner and B. MacWhinney, ``Fluency bank: a new resource for fluency research and practice,'' <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Journal of Fluency Disorders</em>, vol. 56, pp. 69–80, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
P. Howell, S. Davis, and J. Bartrip, ``The university college london archive of stuttered speech (uclass).'' <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Journal of speech, language, and hearing research : JSLHR</em>, vol. 52 2, pp. 556–69, 2009.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C. Lea, V. Mitra, A. Joshi, S. Kajarekar, and J. Bigham, ``Sep-28k: A dataset for stuttering event detection from podcasts with people who stutter,'' in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ICASSP</em>.   IEEE, 2021, pp. 6798–6802.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
T. Kourkounakis, A. Hajavi, and A. Etemad, ``Fluentnet: End-to-end detection of stuttered speech disfluencies with deep learning,'' <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 29, pp. 2986–2999, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. Bayerl, A. Wolff von Gudenberg, F. Hönig, E. Noeth, and K. Riedhammer, ``Ksof: The kassel state of fluency dataset – a therapy centered dataset of stuttering,'' in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Language Resources and Evaluation Conference</em>.   European Language Resources Association, 2022, pp. 1780–1787.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S. Alharbi, A. J. H. Simons, S. Brumfitt, and P. D. Green, ``Automatic recognition of children's read speech for stuttering application,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Workshop on Child, Computer and Interaction</em>, 2017.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S. A. Sheikh, M. Sahidullah, F. Hirsch, and S. Ouni, ``Advancing stuttering detection via data augmentation, class-balanced loss and multi-contextual deep learning,'' <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Biomedical and Health Informatics</em>, vol. 27, pp. 2553–2564, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. P. Bayerl, D. Wagner, I. Baumann, F. Hönig, T. Bocklet, E. Nöth, and K. Riedhammer, ``A stutter seldom comes alone - cross-corpus stuttering detection as a multi-label problem,'' in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>.   ISCA, 2023, pp. 1538–1542.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
``GB/T 39335-2020: Information security technology–Guidance for personal information security impact assessment,'' Standard, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S. P. Bayerl, D. Wagner, E. Nöth, T. Bocklet, and K. Riedhammer, ``The influence of dataset partitioning on dysfluency detection systems,'' in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Text, Speech, and Dialogue</em>.   Springer International Publishing, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A. Gulati, J. Qin, C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, ``Conformer: Convolution-augmented transformer for speech recognition,'' in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>.   ISCA, 2020, pp. 5036–5040.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
W. Hsu, B. Bolte, Y. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, ``Hubert: Self-supervised speech representation learning by masked prediction of hidden units,'' <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE ACM Trans. Audio Speech Lang. Process.</em>, vol. 29, pp. 3451–3460, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Z. Yao, D. Wu, X. Wang, B. Zhang, F. Yu, C. Yang, Z. Peng, X. Chen, L. Xie, and X. Lei, ``Wenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit,'' in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>.   ISCA, 2021, pp. 4054–4058.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
B. Zhang, H. Lv, P. Guo, Q. Shao, C. Yang, L. Xie, X. Xu, H. Bu, X. Chen, C. Zeng <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition,'' in <em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic">ICASSP</em>.   IEEE, 2022, pp. 6182–6186.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
``chinese-hubert-large,'' <a target="_blank" href="https://huggingface.co/TencentGameMate/chinese-hubert-large" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/TencentGameMate/chinese-hubert-large</a>, accessed: 2024-03-07.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, ``Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline,'' in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Oriental COCOSDA</em>, 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
S. P. Bayerl, D. Wagner, E. Nöth, and K. Riedhammer, ``Detecting dysfluencies in stuttering therapy using wav2vec 2.0,'' in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>.   ISCA, 2022, pp. 2868–2872.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
``chinese-wav2vec2-large,'' <a target="_blank" href="https://huggingface.co/TencentGameMate/chinese-wav2vec2-base" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/TencentGameMate/chinese-wav2vec2-base</a>, accessed: 2024-03-07.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.07255" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.07256" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.07256">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.07256" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.07257" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 22:31:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
