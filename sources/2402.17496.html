<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.17496] Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages</title><meta property="og:description" content="Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-th…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.17496">

<!--Generated on Tue Mar  5 13:16:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lucía Gómez-Zaragozá
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">HUMAN-tech Institute, Universitat Politècnica de València, Valencia, 46022, Spain
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rocío del Amor
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">HUMAN-tech Institute, Universitat Politècnica de València, Valencia, 46022, Spain
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elena Parra Vargas
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">HUMAN-tech Institute, Universitat Politècnica de València, Valencia, 46022, Spain
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Valery Naranjo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">HUMAN-tech Institute, Universitat Politècnica de València, Valencia, 46022, Spain
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mariano Alcañiz Raya
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">HUMAN-tech Institute, Universitat Politècnica de València, Valencia, 46022, Spain
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Javier Marín-Morales
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">HUMAN-tech Institute, Universitat Politècnica de València, Valencia, 46022, Spain
</span>
<span class="ltx_contact ltx_role_affiliation">corresponding author(s): Javier Marín-Morales (jamarmo@htech.upv.es)
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension. The experts also provided an extra label corresponding to seven emotion categories. To set a baseline for future investigations using EMOVOME, we implemented emotion recognition models using both speech and audio transcriptions. For speech, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively. For text, we fine-tuned a multilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for valence and arousal respectively.
This database will significantly contribute to research on emotion recognition in the wild, while also providing a unique natural and freely accessible resource for Spanish.</p>
</div>
<section id="Sx1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Background &amp; Summary</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Communication is a fundamental aspect of human existence that allows individuals to articulate complex thoughts, express emotions, and foster connections with others. Among the different human communication methods, speech remains as the most natural and effective way through which individuals interact with their environment. It conveys not only linguistic information, but also the individual’s emotions, which are used to module social interactions. Speech Emotion Recognition (SER) is a field of research that aims to automatically recognise an individual’s emotional state by analysing their voice. SER has potential applications for the study of human-to-human communications, e.g., fear detection in emergency call centres <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> or depression detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Recognising human emotions is also relevant to human-computer interactions, e.g., creating affect-adaptive games <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">SER is still an open-ended problem due to its complexity. A key challenge lies in precisely defining the meaning of emotions. Many different versions have been proposed to define emotions throughout the 20th century <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and there is still no consensus on the matter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The variety of emotional speech databases in the literature, with different methodologies (acted, induced and natural) and emotional models for labelling (dimensional and categorical), is a clear reflection of the lack of agreement. Moreover, they have several limitations for their application in the recognition of real-life emotions. The most popular databases contain acted emotions, which are easier to collect, but they contain stereotypical emotions that differ from real emotions. Natural databases are more suitable for real-life emotion recognition, but are difficult to acquire due to ethical and legal restrictions. In addition, they contain audio recorded in uncontrolled environments (so-called "in the wild" conditions), which may include background noise and overlapping voices, making emotion recognition more difficult.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.2" class="ltx_p">Database selection is crucial when developing a SER system. Existing emotional corpus are mainly in English due to its global prevalence, thus having abundant resources. However, the landscape becomes considerably limited when it comes to languages other than English, with Spanish being one such example. Only twelve Spanish databases have been found in the literature, six of them private <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, one commercially available <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, and five free to research use <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> does not provide the original clips). Most of them are acted databases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> where professional or non-professional actors portray different emotions while reading texts. One of the databases is elicited <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and three are natural <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Within natural databases, two include clips sourced from YouTube videos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. In these cases, speaker awareness during recording may result in controlled or unnatural emotions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and samples might lack the spontaneity seen in natural conversations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The other natural database used dubbed films <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, where acting is inherent and particularly emphasized in the context of dubbed films. In addition, existing databases have a limited number of speakers (<math id="Sx1.p3.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="Sx1.p3.1.m1.1a"><mo id="Sx1.p3.1.m1.1.1" xref="Sx1.p3.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="Sx1.p3.1.m1.1b"><leq id="Sx1.p3.1.m1.1.1.cmml" xref="Sx1.p3.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p3.1.m1.1c">\leq</annotation></semantics></math>15), so samples from the same speaker are used for training and testing, reducing the model’s generalizability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Exceptions are two cases with <math id="Sx1.p3.2.m2.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="Sx1.p3.2.m2.1a"><mo id="Sx1.p3.2.m2.1.1" xref="Sx1.p3.2.m2.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="Sx1.p3.2.m2.1b"><approx id="Sx1.p3.2.m2.1.1.cmml" xref="Sx1.p3.2.m2.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p3.2.m2.1c">\approx</annotation></semantics></math>50 speakers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, one with 105 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and another with 341 speakers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>). However, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, most speakers are female (84/105), while <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> doesn’t specify gender distribution, posing a challenge for creating gender-fair SER models. To the best of our knowledge, no previous work in the literature focuses on creating a public Spanish database with real emotions and a high number of gender-balanced speakers for SER.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">In this work, we present the Emotional Voice Messages (EMOVOME) database, a spontaneous speech corpus that contains 999 audio messages up to one minute collected from real WhatsApp conversations of 100 Spanish speakers, gender-balanced. Voice messages were produced in-the-wild conditions, before participants were recruited, avoiding conscious bias due to the laboratory environment. Samples were labelled by two psychologists and three non-experts regarding valence and arousal and in seven discrete emotions by the experts. To the best of the author’s knowledge, EMOVOME dataset constitutes the first public database with realistic emotions from spontaneous conversations collected in real-world settings. It is also the first of its kind for the Spanish language. Our dataset also offers demographic details for the speakers and annotators that can be used to assess fairness in SER models. In addition, we have included the results of different machine learning and deep learning techniques for SER that may be helpful as a baseline for other researchers conducting new studies on the data provided.</p>
</div>
</section>
<section id="Sx2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Methods</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">This section is divided into the following three parts: 1) Participants, including details of the speakers included in the database; 2) Data collection, where the experimental setup used to obtain the audios is explained; and 3) Data labelling, describing the procedure to assign emotional labels to the collected audios. Figure <a href="#Sx2.F1" title="Figure 1 ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents a diagram illustrating the steps followed to create the EMOVOME database.</p>
</div>
<figure id="Sx2.F1" class="ltx_figure"><img src="/html/2402.17496/assets/overview_emovome.jpg" id="Sx2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="Sx2.F1.3.2" class="ltx_text" style="font-size:90%;">Overview of the methodology.</span></figcaption>
</figure>
<section id="Sx2.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Participants</h3>

<div id="Sx2.SSx1.p1" class="ltx_para">
<p id="Sx2.SSx1.p1.1" class="ltx_p">The database includes 100 Spanish speakers (50% females, 50% males) between the ages of 18 and 55 years old, with no self-reported speech disorder, and used to sending audio messages to their contacts. Details of the demographic information can be found in Table <a href="#Sx2.T1" title="Table 1 ‣ Participants ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, where age ranges are utilised to safeguard individuals’ anonymity. Participants were sourced via a recruitment agency, which identified suitable individuals from their database based on the specified criteria and invited them to participate in the study for a monetary compensation of 25€. All methods and experimental protocols were performed in accordance with the guidelines and regulations of the local ethics committee of the Universitat Politècnica de València (reference number P02_04_06_20).</p>
</div>
<figure id="Sx2.T1" class="ltx_table">
<table id="Sx2.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx2.T1.2.1.1" class="ltx_tr">
<th id="Sx2.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Variables</th>
<th id="Sx2.T1.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Subgroups</th>
<th id="Sx2.T1.2.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Frequency</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx2.T1.2.2.1" class="ltx_tr">
<td id="Sx2.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Gender</td>
<td id="Sx2.T1.2.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Male</td>
<td id="Sx2.T1.2.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">50</td>
</tr>
<tr id="Sx2.T1.2.3.2" class="ltx_tr">
<td id="Sx2.T1.2.3.2.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.3.2.2" class="ltx_td ltx_align_left ltx_border_r">Female</td>
<td id="Sx2.T1.2.3.2.3" class="ltx_td ltx_align_left ltx_border_r">50</td>
</tr>
<tr id="Sx2.T1.2.4.3" class="ltx_tr">
<td id="Sx2.T1.2.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Age</td>
<td id="Sx2.T1.2.4.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">18 - 25</td>
<td id="Sx2.T1.2.4.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">24</td>
</tr>
<tr id="Sx2.T1.2.5.4" class="ltx_tr">
<td id="Sx2.T1.2.5.4.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.5.4.2" class="ltx_td ltx_align_left ltx_border_r">26 - 35</td>
<td id="Sx2.T1.2.5.4.3" class="ltx_td ltx_align_left ltx_border_r">26</td>
</tr>
<tr id="Sx2.T1.2.6.5" class="ltx_tr">
<td id="Sx2.T1.2.6.5.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.6.5.2" class="ltx_td ltx_align_left ltx_border_r">36 - 45</td>
<td id="Sx2.T1.2.6.5.3" class="ltx_td ltx_align_left ltx_border_r">25</td>
</tr>
<tr id="Sx2.T1.2.7.6" class="ltx_tr">
<td id="Sx2.T1.2.7.6.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.7.6.2" class="ltx_td ltx_align_left ltx_border_r">46 - 55</td>
<td id="Sx2.T1.2.7.6.3" class="ltx_td ltx_align_left ltx_border_r">25</td>
</tr>
<tr id="Sx2.T1.2.8.7" class="ltx_tr">
<td id="Sx2.T1.2.8.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Relationship</td>
<td id="Sx2.T1.2.8.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Single</td>
<td id="Sx2.T1.2.8.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">49</td>
</tr>
<tr id="Sx2.T1.2.9.8" class="ltx_tr">
<td id="Sx2.T1.2.9.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">status</td>
<td id="Sx2.T1.2.9.8.2" class="ltx_td ltx_align_left ltx_border_r">Married</td>
<td id="Sx2.T1.2.9.8.3" class="ltx_td ltx_align_left ltx_border_r">46</td>
</tr>
<tr id="Sx2.T1.2.10.9" class="ltx_tr">
<td id="Sx2.T1.2.10.9.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.10.9.2" class="ltx_td ltx_align_left ltx_border_r">Divorced / separated</td>
<td id="Sx2.T1.2.10.9.3" class="ltx_td ltx_align_left ltx_border_r">5</td>
</tr>
<tr id="Sx2.T1.2.11.10" class="ltx_tr">
<td id="Sx2.T1.2.11.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Qualification</td>
<td id="Sx2.T1.2.11.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Primary education</td>
<td id="Sx2.T1.2.11.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">11</td>
</tr>
<tr id="Sx2.T1.2.12.11" class="ltx_tr">
<td id="Sx2.T1.2.12.11.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.12.11.2" class="ltx_td ltx_align_left ltx_border_r">Secondary education</td>
<td id="Sx2.T1.2.12.11.3" class="ltx_td ltx_align_left ltx_border_r">33</td>
</tr>
<tr id="Sx2.T1.2.13.12" class="ltx_tr">
<td id="Sx2.T1.2.13.12.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.13.12.2" class="ltx_td ltx_align_left ltx_border_r">Higher education</td>
<td id="Sx2.T1.2.13.12.3" class="ltx_td ltx_align_left ltx_border_r">56</td>
</tr>
<tr id="Sx2.T1.2.14.13" class="ltx_tr">
<td id="Sx2.T1.2.14.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Economic</td>
<td id="Sx2.T1.2.14.13.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No income</td>
<td id="Sx2.T1.2.14.13.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">19</td>
</tr>
<tr id="Sx2.T1.2.15.14" class="ltx_tr">
<td id="Sx2.T1.2.15.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">situation</td>
<td id="Sx2.T1.2.15.14.2" class="ltx_td ltx_align_left ltx_border_r">&lt;15.000€</td>
<td id="Sx2.T1.2.15.14.3" class="ltx_td ltx_align_left ltx_border_r">31</td>
</tr>
<tr id="Sx2.T1.2.16.15" class="ltx_tr">
<td id="Sx2.T1.2.16.15.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.16.15.2" class="ltx_td ltx_align_left ltx_border_r">15.000€ - 30.000€</td>
<td id="Sx2.T1.2.16.15.3" class="ltx_td ltx_align_left ltx_border_r">38</td>
</tr>
<tr id="Sx2.T1.2.17.16" class="ltx_tr">
<td id="Sx2.T1.2.17.16.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.17.16.2" class="ltx_td ltx_align_left ltx_border_r">30.000€ - 60.000€</td>
<td id="Sx2.T1.2.17.16.3" class="ltx_td ltx_align_left ltx_border_r">8</td>
</tr>
<tr id="Sx2.T1.2.18.17" class="ltx_tr">
<td id="Sx2.T1.2.18.17.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.18.17.2" class="ltx_td ltx_align_left ltx_border_r">No answer</td>
<td id="Sx2.T1.2.18.17.3" class="ltx_td ltx_align_left ltx_border_r">4</td>
</tr>
<tr id="Sx2.T1.2.19.18" class="ltx_tr">
<td id="Sx2.T1.2.19.18.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Occupation</td>
<td id="Sx2.T1.2.19.18.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Student</td>
<td id="Sx2.T1.2.19.18.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">20</td>
</tr>
<tr id="Sx2.T1.2.20.19" class="ltx_tr">
<td id="Sx2.T1.2.20.19.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.20.19.2" class="ltx_td ltx_align_left ltx_border_r">Employed</td>
<td id="Sx2.T1.2.20.19.3" class="ltx_td ltx_align_left ltx_border_r">60</td>
</tr>
<tr id="Sx2.T1.2.21.20" class="ltx_tr">
<td id="Sx2.T1.2.21.20.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.21.20.2" class="ltx_td ltx_align_left ltx_border_r">Self-employed</td>
<td id="Sx2.T1.2.21.20.3" class="ltx_td ltx_align_left ltx_border_r">9</td>
</tr>
<tr id="Sx2.T1.2.22.21" class="ltx_tr">
<td id="Sx2.T1.2.22.21.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.22.21.2" class="ltx_td ltx_align_left ltx_border_r">Unemployed</td>
<td id="Sx2.T1.2.22.21.3" class="ltx_td ltx_align_left ltx_border_r">10</td>
</tr>
<tr id="Sx2.T1.2.23.22" class="ltx_tr">
<td id="Sx2.T1.2.23.22.1" class="ltx_td ltx_border_b ltx_border_l ltx_border_r"></td>
<td id="Sx2.T1.2.23.22.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Retired</td>
<td id="Sx2.T1.2.23.22.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Sx2.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="Sx2.T1.4.2" class="ltx_text" style="font-size:90%;">Participant demographic information.</span></figcaption>
</figure>
</section>
<section id="Sx2.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Data collection</h3>

<div id="Sx2.SSx2.p1" class="ltx_para">
<p id="Sx2.SSx2.p1.1" class="ltx_p">The sample collection was carried out through a web-based application designed for the study. Participants completed the study with their computer, following illustrated step-by-step instructions given on the platform. First, they were required to read the study protocol, ask any questions if needed and provide informed consent. Then, they answered a sociodemographic questionnaire with the information detailed in Table <a href="#Sx2.T1" title="Table 1 ‣ Participants ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. They were required to record an audio reading the short text in Table <a href="#Sx2.T2" title="Table 2 ‣ Data collection ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, indicated in the platform.
They also completed the NEO Five Factor Inventory (NEO-FFI) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, a 60-item questionnaire designed to evaluate the individual’s Big Five personality traits: neuroticism, extraversion, openness to experience, agreeableness, and conscientiousness. Then, they were asked to upload 12 voice messages according to the following criteria: the audios should have been sent to other contacts prior to the study and one-third of them should have positive, neutral and negative valence, respectively. The latter was requested to obtain a balanced sample in terms of valence. The instructions given in the platform provided an explanation of the concepts of positive, neutral and negative valence, with examples of each class.</p>
</div>
<figure id="Sx2.T2" class="ltx_table">
<table id="Sx2.T2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx2.T2.2.1.1" class="ltx_tr">
<td id="Sx2.T2.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="Sx2.T2.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="Sx2.T2.2.1.1.1.1.1" class="ltx_p" style="width:341.4pt;">La Great Barrier Reef australiana —en español, Gran Barrera de Coral, Gran Barrera y Gran Barrera de Arrecifes— es el mayor arrecife de coral del mundo. El arrecife está situado en el mar del Coral, frente a la costa nordeste de Australia. El arrecife, que se extiende a lo largo de unos 2600 kilómetros, puede apreciarse desde el espacio. Resulta difícil delimitar su extensión exacta, aunque se considera que comienza cerca de la latitud 9°S, al sur de Papúa Nueva Guinea, y sigue hacia el sureste hasta la latitud 24°S, la mayor parte como una línea paralela a la costa meridional. Tampoco se trata de una línea continua de arrecife, sino que está formado por más de 2000 arrecifes individuales y casi 1000 islas.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Sx2.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="Sx2.T2.4.2" class="ltx_text" style="font-size:90%;">Short text used for the reading task.</span></figcaption>
</figure>
</section>
<section id="Sx2.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Quality check</h3>

<div id="Sx2.SSx3.p1" class="ltx_para">
<p id="Sx2.SSx3.p1.1" class="ltx_p">Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to the laboratory environment. They were recorded in the participants’ natural environments, which could include a quiet room at home but also the street on the way to work. Therefore, to obtain naturalistic data and good quality, a manual screening of the voice messages was performed on the audios containing emotions. As the audios were uploaded, those recorded in critical background noise conditions (such as microphone failure, background music or TV) were manually identified and rejected. Participants were asked to upload audios with low noise levels, so if an audio was not considered suitable for the study, they were required to send a new audio. As a result, we received a total of 1605 audio recordings, but 574 audios were not included in the EMOVOME database for this reason, resulting in 1031 audios. Finally, 32 audios exceeding 60 seconds were rejected in order to obtain a homogeneous sample concerning duration. Therefore, a total of 999 audios were considered in the final speech database, apart from the 100 audios of the text reading.</p>
</div>
</section>
<section id="Sx2.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Data labelling</h3>

<div id="Sx2.SSx4.p1" class="ltx_para">
<p id="Sx2.SSx4.p1.1" class="ltx_p">The emotional content of the audios that successfully passed the manual evaluation was labelled along two dimensions: valence, i.e., the degree to which an emotion is perceived as positive or negative; and arousal, i.e., how strongly the emotion is felt. For this purpose, the Self-Assessment Manikin (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> procedure was adopted, which consists of a non-verbal scale based on pictures that measures the valence and arousal related to an emotional response to a stimulus (see Figure <a href="#Sx2.F2" title="Figure 2 ‣ Data labelling ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Three non-experts and two expert evaluators were recruited for the task. The latter consisted of two clinical psychologists who rated half of the audios each. The demographic information of the speakers is indicated in Table <a href="#Sx2.T3" title="Table 3 ‣ Data labelling ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The five evaluators used the 5-point SAM scale to rate valence and arousal dimensions on each audio. Additionally, the expert annotators provided an extra label corresponding to 7 categories of emotions: the six basic emotions (happiness, disgust, anger, surprise, fear and sadness) defined by Ekman <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> plus a neutral category.</p>
</div>
<figure id="Sx2.F2" class="ltx_figure"><img src="/html/2402.17496/assets/SAM.jpg" id="Sx2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="210" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="Sx2.F2.3.2" class="ltx_text" style="font-size:90%;">Self-Assessment Manikin (SAM) used for valence and arousal dimensions. Adapted from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</span></figcaption>
</figure>
<figure id="Sx2.T3" class="ltx_table">
<table id="Sx2.T3.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx2.T3.2.1.1" class="ltx_tr">
<td id="Sx2.T3.2.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Type</td>
<td id="Sx2.T3.2.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ID</td>
<td id="Sx2.T3.2.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Gender</td>
<td id="Sx2.T3.2.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Age</td>
<td id="Sx2.T3.2.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Ethnicity</td>
<td id="Sx2.T3.2.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Qualification</td>
<td id="Sx2.T3.2.1.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Occupation</td>
</tr>
<tr id="Sx2.T3.2.2.2" class="ltx_tr">
<td id="Sx2.T3.2.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Experts</td>
<td id="Sx2.T3.2.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1</td>
<td id="Sx2.T3.2.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Female</td>
<td id="Sx2.T3.2.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">64</td>
<td id="Sx2.T3.2.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">White</td>
<td id="Sx2.T3.2.2.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Higher education</td>
<td id="Sx2.T3.2.2.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Self-employed</td>
</tr>
<tr id="Sx2.T3.2.3.3" class="ltx_tr">
<td id="Sx2.T3.2.3.3.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T3.2.3.3.2" class="ltx_td ltx_align_left ltx_border_r">2</td>
<td id="Sx2.T3.2.3.3.3" class="ltx_td ltx_align_left ltx_border_r">Female</td>
<td id="Sx2.T3.2.3.3.4" class="ltx_td ltx_align_left ltx_border_r">29</td>
<td id="Sx2.T3.2.3.3.5" class="ltx_td ltx_align_left ltx_border_r">White</td>
<td id="Sx2.T3.2.3.3.6" class="ltx_td ltx_align_left ltx_border_r">Higher education</td>
<td id="Sx2.T3.2.3.3.7" class="ltx_td ltx_align_left ltx_border_r">Self-employed</td>
</tr>
<tr id="Sx2.T3.2.4.4" class="ltx_tr">
<td id="Sx2.T3.2.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Non-experts</td>
<td id="Sx2.T3.2.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1</td>
<td id="Sx2.T3.2.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Male</td>
<td id="Sx2.T3.2.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">25</td>
<td id="Sx2.T3.2.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">White</td>
<td id="Sx2.T3.2.4.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Higher education</td>
<td id="Sx2.T3.2.4.4.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Employed</td>
</tr>
<tr id="Sx2.T3.2.5.5" class="ltx_tr">
<td id="Sx2.T3.2.5.5.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T3.2.5.5.2" class="ltx_td ltx_align_left ltx_border_r">2</td>
<td id="Sx2.T3.2.5.5.3" class="ltx_td ltx_align_left ltx_border_r">Male</td>
<td id="Sx2.T3.2.5.5.4" class="ltx_td ltx_align_left ltx_border_r">33</td>
<td id="Sx2.T3.2.5.5.5" class="ltx_td ltx_align_left ltx_border_r">White</td>
<td id="Sx2.T3.2.5.5.6" class="ltx_td ltx_align_left ltx_border_r">Higher education</td>
<td id="Sx2.T3.2.5.5.7" class="ltx_td ltx_align_left ltx_border_r">Employed</td>
</tr>
<tr id="Sx2.T3.2.6.6" class="ltx_tr">
<td id="Sx2.T3.2.6.6.1" class="ltx_td ltx_border_b ltx_border_l ltx_border_r"></td>
<td id="Sx2.T3.2.6.6.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">3</td>
<td id="Sx2.T3.2.6.6.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Female</td>
<td id="Sx2.T3.2.6.6.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">24</td>
<td id="Sx2.T3.2.6.6.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">White</td>
<td id="Sx2.T3.2.6.6.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Higher education</td>
<td id="Sx2.T3.2.6.6.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Employed</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Sx2.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="Sx2.T3.4.2" class="ltx_text" style="font-size:90%;">Annotators demographic information.</span></figcaption>
</figure>
<div id="Sx2.SSx4.p2" class="ltx_para">
<p id="Sx2.SSx4.p2.1" class="ltx_p">The ratings of the non-experts and expert evaluators are represented in Figure <a href="#Sx2.F3" title="Figure 3 ‣ Data labelling ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. For valence and arousal dimensions, the ratings present a V-shaped relation, that is, arousal increases with positive or negative valence. To compare the ratings in valence and arousal among evaluators, three categories for valence (positive, neutral and negative) and three labels for arousal (high, neutral and low) were defined by grouping negative and positive scores in the scale [-2, 2] and keeping 0 as neutral. The agreement between evaluators is presented in Table <a href="#Sx2.T4" title="Table 4 ‣ Data labelling ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> using Cohen’s kappa score, for both valence (a) and arousal (b). For valence, the kappa score goes from 0.614 to 0.754, indicating substantial agreement among annotators. In contrast, the agreement decreased considerably for the arousal dimension, with kappa values ranging from 0.075 to a maximum of 0.407. These results suggest that valence recognition was easier than arousal recognition for raters.</p>
</div>
<figure id="Sx2.F3" class="ltx_figure">
<div id="Sx2.F3.2" class="ltx_block">
<figure id="Sx2.F3.sf1" class="ltx_figure ltx_align_center">
<p id="Sx2.F3.sf1.1" class="ltx_p">][c]0.64
<img src="/html/2402.17496/assets/AV_nonExpert.jpg" id="Sx2.F3.sf1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="422" alt="Refer to caption"></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F3.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="Sx2.F3.sf1.4.2" class="ltx_text" style="font-size:90%;">Non-experts</span></figcaption>
</figure>
<figure id="Sx2.F3.sf2" class="ltx_figure ltx_align_center">
<p id="Sx2.F3.sf2.1" class="ltx_p">][c]0.34
<img src="/html/2402.17496/assets/AV_expert.jpg" id="Sx2.F3.sf2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="432" alt="Refer to caption"></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F3.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="Sx2.F3.sf2.4.2" class="ltx_text" style="font-size:90%;">Experts</span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="Sx2.F3.4.2" class="ltx_text" style="font-size:90%;">Valence and arousal ratings.</span></figcaption>
</figure>
<figure id="Sx2.T4" class="ltx_table">
<div id="Sx2.T4.2" class="ltx_block">
<figure id="Sx2.T4.st1" class="ltx_table">
<p id="Sx2.T4.st1.2" class="ltx_p">][c]0.5

<span id="Sx2.T4.st1.2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<span class="ltx_tbody">
<span id="Sx2.T4.st1.2.1.1.1" class="ltx_tr">
<span id="Sx2.T4.st1.2.1.1.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></span>
<span id="Sx2.T4.st1.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NE 1</span>
<span id="Sx2.T4.st1.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NE 2</span>
<span id="Sx2.T4.st1.2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NE 3</span>
<span id="Sx2.T4.st1.2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">E</span></span>
<span id="Sx2.T4.st1.2.1.2.2" class="ltx_tr">
<span id="Sx2.T4.st1.2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">NE 1</span>
<span id="Sx2.T4.st1.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</span>
<span id="Sx2.T4.st1.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.700</span>
<span id="Sx2.T4.st1.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.754</span>
<span id="Sx2.T4.st1.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.614</span></span>
<span id="Sx2.T4.st1.2.1.3.3" class="ltx_tr">
<span id="Sx2.T4.st1.2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">NE 2</span>
<span id="Sx2.T4.st1.2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.700</span>
<span id="Sx2.T4.st1.2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</span>
<span id="Sx2.T4.st1.2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.732</span>
<span id="Sx2.T4.st1.2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.646</span></span>
<span id="Sx2.T4.st1.2.1.4.4" class="ltx_tr">
<span id="Sx2.T4.st1.2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">NE 3</span>
<span id="Sx2.T4.st1.2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.754</span>
<span id="Sx2.T4.st1.2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.732</span>
<span id="Sx2.T4.st1.2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</span>
<span id="Sx2.T4.st1.2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.649</span></span>
<span id="Sx2.T4.st1.2.1.5.5" class="ltx_tr">
<span id="Sx2.T4.st1.2.1.5.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">E</span>
<span id="Sx2.T4.st1.2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.614</span>
<span id="Sx2.T4.st1.2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.646</span>
<span id="Sx2.T4.st1.2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.649</span>
<span id="Sx2.T4.st1.2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</span></span>
</span>
</span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="Sx2.T4.st1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="Sx2.T4.st1.4.2" class="ltx_text" style="font-size:90%;">Valence</span></figcaption>
</figure>
<figure id="Sx2.T4.st2" class="ltx_table">
<p id="Sx2.T4.st2.2" class="ltx_p">][c]0.5

<span id="Sx2.T4.st2.2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<span class="ltx_tbody">
<span id="Sx2.T4.st2.2.1.1.1" class="ltx_tr">
<span id="Sx2.T4.st2.2.1.1.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></span>
<span id="Sx2.T4.st2.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NE 1</span>
<span id="Sx2.T4.st2.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NE 2</span>
<span id="Sx2.T4.st2.2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NE 3</span>
<span id="Sx2.T4.st2.2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">E</span></span>
<span id="Sx2.T4.st2.2.1.2.2" class="ltx_tr">
<span id="Sx2.T4.st2.2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">NE 1</span>
<span id="Sx2.T4.st2.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</span>
<span id="Sx2.T4.st2.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.142</span>
<span id="Sx2.T4.st2.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.407</span>
<span id="Sx2.T4.st2.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.275</span></span>
<span id="Sx2.T4.st2.2.1.3.3" class="ltx_tr">
<span id="Sx2.T4.st2.2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">NE 2</span>
<span id="Sx2.T4.st2.2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.142</span>
<span id="Sx2.T4.st2.2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</span>
<span id="Sx2.T4.st2.2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.235</span>
<span id="Sx2.T4.st2.2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.075</span></span>
<span id="Sx2.T4.st2.2.1.4.4" class="ltx_tr">
<span id="Sx2.T4.st2.2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">NE 3</span>
<span id="Sx2.T4.st2.2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.407</span>
<span id="Sx2.T4.st2.2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.235</span>
<span id="Sx2.T4.st2.2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</span>
<span id="Sx2.T4.st2.2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.241</span></span>
<span id="Sx2.T4.st2.2.1.5.5" class="ltx_tr">
<span id="Sx2.T4.st2.2.1.5.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">E</span>
<span id="Sx2.T4.st2.2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.275</span>
<span id="Sx2.T4.st2.2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.075</span>
<span id="Sx2.T4.st2.2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.214</span>
<span id="Sx2.T4.st2.2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</span></span>
</span>
</span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="Sx2.T4.st2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="Sx2.T4.st2.4.2" class="ltx_text" style="font-size:90%;">Arousal</span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="Sx2.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="Sx2.T4.4.2" class="ltx_text" style="font-size:90%;">Pair-wise Cohen Kappa scores for annotator pairs</span></figcaption>
</figure>
<div id="Sx2.SSx4.p3" class="ltx_para">
<p id="Sx2.SSx4.p3.5" class="ltx_p">The evaluations of the non-experts and the experts were combined to obtain a final audio label in terms of valence and arousal, using the most frequent category among the four assessments (mode). In the case of a two-way tie, we prioritised the expert’s ratings and selected those as the final label for the audio. The distribution of the audio labels is shown in Table <a href="#Sx2.T5" title="Table 5 ‣ Data labelling ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Concerning valence, the resulting distribution is roughly balanced. The majority class is neutral valence (<math id="Sx2.SSx4.p3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="Sx2.SSx4.p3.1.m1.1a"><mo id="Sx2.SSx4.p3.1.m1.1.1" xref="Sx2.SSx4.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p3.1.m1.1b"><csymbol cd="latexml" id="Sx2.SSx4.p3.1.m1.1.1.cmml" xref="Sx2.SSx4.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p3.1.m1.1c">\sim</annotation></semantics></math>37%), while negative valence is the minority class (<math id="Sx2.SSx4.p3.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="Sx2.SSx4.p3.2.m2.1a"><mo id="Sx2.SSx4.p3.2.m2.1.1" xref="Sx2.SSx4.p3.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p3.2.m2.1b"><csymbol cd="latexml" id="Sx2.SSx4.p3.2.m2.1.1.cmml" xref="Sx2.SSx4.p3.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p3.2.m2.1c">\sim</annotation></semantics></math>30%). Regarding arousal, Table <a href="#Sx2.T5" title="Table 5 ‣ Data labelling ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows a clear unbalanced distribution of the labels, with <math id="Sx2.SSx4.p3.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="Sx2.SSx4.p3.3.m3.1a"><mo id="Sx2.SSx4.p3.3.m3.1.1" xref="Sx2.SSx4.p3.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p3.3.m3.1b"><csymbol cd="latexml" id="Sx2.SSx4.p3.3.m3.1.1.cmml" xref="Sx2.SSx4.p3.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p3.3.m3.1c">\sim</annotation></semantics></math>44% of them belonging to high arousal, <math id="Sx2.SSx4.p3.4.m4.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="Sx2.SSx4.p3.4.m4.1a"><mo id="Sx2.SSx4.p3.4.m4.1.1" xref="Sx2.SSx4.p3.4.m4.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p3.4.m4.1b"><csymbol cd="latexml" id="Sx2.SSx4.p3.4.m4.1.1.cmml" xref="Sx2.SSx4.p3.4.m4.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p3.4.m4.1c">\sim</annotation></semantics></math>40% to neutral arousal and <math id="Sx2.SSx4.p3.5.m5.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="Sx2.SSx4.p3.5.m5.1a"><mo id="Sx2.SSx4.p3.5.m5.1.1" xref="Sx2.SSx4.p3.5.m5.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p3.5.m5.1b"><csymbol cd="latexml" id="Sx2.SSx4.p3.5.m5.1.1.cmml" xref="Sx2.SSx4.p3.5.m5.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p3.5.m5.1c">\sim</annotation></semantics></math>16% to low arousal. Considering the combination of valence and arousal labels, the distribution obtained is represented in Figure <a href="#Sx2.F4" title="Figure 4 ‣ Data labelling ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The majority clusters are [positive valence, high arousal], [neutral valence, neutral arousal] and [negative valence, high arousal], with 244, 224 and 152 audios, respectively. The remaining groups have less than 100 audios, with a minimum of 12 audios corresponding to the cluster [positive valence, low arousal]. Although an aggregation method is provided, both non-expert and expert labels are available in the database, offering the flexibility to apply a different method for determining the final label for each audio recording.</p>
</div>
<figure id="Sx2.T5" class="ltx_table">
<table id="Sx2.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx2.T5.2.1.1" class="ltx_tr">
<th id="Sx2.T5.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Label</th>
<th id="Sx2.T5.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Category</th>
<th id="Sx2.T5.2.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">N</th>
<th id="Sx2.T5.2.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">N/total (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx2.T5.2.2.1" class="ltx_tr">
<td id="Sx2.T5.2.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Valence</td>
<td id="Sx2.T5.2.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Positive</td>
<td id="Sx2.T5.2.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">334</td>
<td id="Sx2.T5.2.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">33.43</td>
</tr>
<tr id="Sx2.T5.2.3.2" class="ltx_tr">
<td id="Sx2.T5.2.3.2.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T5.2.3.2.2" class="ltx_td ltx_align_left ltx_border_r">Neutral</td>
<td id="Sx2.T5.2.3.2.3" class="ltx_td ltx_align_left ltx_border_r">367</td>
<td id="Sx2.T5.2.3.2.4" class="ltx_td ltx_align_left ltx_border_r">36.74</td>
</tr>
<tr id="Sx2.T5.2.4.3" class="ltx_tr">
<td id="Sx2.T5.2.4.3.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T5.2.4.3.2" class="ltx_td ltx_align_left ltx_border_r">Negative</td>
<td id="Sx2.T5.2.4.3.3" class="ltx_td ltx_align_left ltx_border_r">298</td>
<td id="Sx2.T5.2.4.3.4" class="ltx_td ltx_align_left ltx_border_r">29.83</td>
</tr>
<tr id="Sx2.T5.2.5.4" class="ltx_tr">
<td id="Sx2.T5.2.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Arousal</td>
<td id="Sx2.T5.2.5.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
<td id="Sx2.T5.2.5.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">440</td>
<td id="Sx2.T5.2.5.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">44.04</td>
</tr>
<tr id="Sx2.T5.2.6.5" class="ltx_tr">
<td id="Sx2.T5.2.6.5.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx2.T5.2.6.5.2" class="ltx_td ltx_align_left ltx_border_r">Neutral</td>
<td id="Sx2.T5.2.6.5.3" class="ltx_td ltx_align_left ltx_border_r">398</td>
<td id="Sx2.T5.2.6.5.4" class="ltx_td ltx_align_left ltx_border_r">39.84</td>
</tr>
<tr id="Sx2.T5.2.7.6" class="ltx_tr">
<td id="Sx2.T5.2.7.6.1" class="ltx_td ltx_border_b ltx_border_l ltx_border_r"></td>
<td id="Sx2.T5.2.7.6.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Low</td>
<td id="Sx2.T5.2.7.6.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">161</td>
<td id="Sx2.T5.2.7.6.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">16.12</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Sx2.T5.3.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="Sx2.T5.4.2" class="ltx_text" style="font-size:90%;">Distribution of the audio labels in terms of valence and arousal, considering the mode among evaluators.</span></figcaption>
</figure>
<figure id="Sx2.F4" class="ltx_figure"><img src="/html/2402.17496/assets/clusters_combined.jpg" id="Sx2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="Sx2.F4.3.2" class="ltx_text" style="font-size:90%;">Distribution of audio samples based on their arousal and valence labels. The circle area is proportional to the number of audio samples in each group.</span></figcaption>
</figure>
<div id="Sx2.SSx4.p4" class="ltx_para">
<p id="Sx2.SSx4.p4.1" class="ltx_p">Additionally, the expert annotators provided an extra label corresponding to 7 categories of emotions. The classes are distributed as follows: happiness (342), disgust (8), anger (199), surprise (118), fear (35), sadness (72) and neutral (225). The distribution is shown in Figure <a href="#Sx2.F5.sf1" title="In Figure 5 ‣ Data labelling ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a>, while Figure <a href="#Sx2.F5.sf2" title="In Figure 5 ‣ Data labelling ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(b)</span></a> shows the relation between categories and the valence and arousal dimensions provided by the researchers.</p>
</div>
<figure id="Sx2.F5" class="ltx_figure">
<div id="Sx2.F5.2" class="ltx_block">
<figure id="Sx2.F5.sf1" class="ltx_figure ltx_align_center">
<p id="Sx2.F5.sf1.1" class="ltx_p">][c]0.42
<img src="/html/2402.17496/assets/categories.jpg" id="Sx2.F5.sf1.1.g1" class="ltx_graphics ltx_img_square" width="598" height="508" alt="Refer to caption"></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F5.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="Sx2.F5.sf1.4.2" class="ltx_text" style="font-size:90%;">Frequency of each category</span></figcaption>
</figure>
<figure id="Sx2.F5.sf2" class="ltx_figure ltx_align_center">
<p id="Sx2.F5.sf2.1" class="ltx_p">][c]0.5
<img src="/html/2402.17496/assets/categories_AV.jpg" id="Sx2.F5.sf2.1.g1" class="ltx_graphics ltx_img_square" width="538" height="438" alt="Refer to caption"></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F5.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="Sx2.F5.sf2.4.2" class="ltx_text" style="font-size:90%;">Valence and arousal ratings for each emotion. The diameter represents the number of samples using a logarithmic scale</span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="Sx2.F5.4.2" class="ltx_text" style="font-size:90%;">Expert categories of emotions.</span></figcaption>
</figure>
</section>
<section id="Sx2.SSx5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Transcription</h3>

<div id="Sx2.SSx5.p1" class="ltx_para">
<p id="Sx2.SSx5.p1.1" class="ltx_p">The EMOVOME database is a comprehensive speech corpus for training emotion recognition models. In addition to the spoken content, audio transcriptions have been included to offer an extra modality for analysis. The voice messages were transcribed using Amazon Transcribe and subsequently manually corrected to ensure accuracy and reliability.</p>
</div>
</section>
</section>
<section id="Sx3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Data Records</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">The EMOVOME database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is available at the following Zenodo repository <a target="_blank" href="https://zenodo.org/records/10694370" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://zenodo.org/records/10694370</a>. It contains four files: 1) “EMOVOME_agreement.pdf“, which is the agreement file detailed in Section Usage Notes; 2) “labels.csv“, containing the ratings of the three non-experts and the expert annotator, independently and combined, as explained in section Data labelling; 3) “participants_ids.csv”, with a table mapping each numerical file ID to its corresponding alphanumeric participant ID; and 4) “transcriptions.csv”, containing the transcriptions of each audio. The repository also includes three folders: “Audios”, “Questionnaires” and “Baseline_emotion_recognition”, with the files detailed below.</p>
</div>
<div id="Sx3.p2" class="ltx_para">
<p id="Sx3.p2.1" class="ltx_p">The “Audios” folder contains the file “features_eGeMAPSv02.csv”, with the standard acoustic feature set used in section Technical validation, and other two folders named “Lecture” and “Emotions”. The “Lecture” folder contains the audios corresponding to the reading of the text in Table <a href="#Sx2.T2" title="Table 2 ‣ Data collection ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, named with the participant ID. The “Emotions” folder contains the voice recordings from the messaging app provided by the user, named with a file ID. Audio files in “Lecture” and “Emotions” are only provided to the users that complete the agreement file. Audio files are in Ogg Vorbis format at 16-bit and 44.1 kHz or 48 kHz. The total size of the “Audios” folder is about 213 MB.</p>
</div>
<div id="Sx3.p3" class="ltx_para">
<p id="Sx3.p3.1" class="ltx_p">The "Questionnaires" folder contains two files: 1) “sociodemographic_spanish.csv” and “sociodemographic_english.csv” are the sociodemographic data of participants in Spanish and English, respectively, including the information indicated in Table <a href="#Sx2.T1" title="Table 1 ‣ Participants ‣ Methods ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>; and 2) NEO-FFI_spanish.csv includes the participants’ answers to the Spanish version of the NEO-FFI questionnaire. The three files include a column indicating the participant ID to link the information.</p>
</div>
<div id="Sx3.p4" class="ltx_para">
<p id="Sx3.p4.1" class="ltx_p">The “Baseline_emotion_recognition” includes three files and two folders. The file “partitions.csv” specifies the data partition used in section Technical validation. Files “baseline_speech.ipynb” and “baseline_text.ipynb” contain the code used to create the baseline emotion recognition models based on speech and text, respectively. The actual trained models for valence and arousal prediction, whose evaluation metrics are indicated in Table <a href="#Sx4.T7" title="Table 7 ‣ Technical Validation ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, are provided in folders “models_speech” and “models_text”.</p>
</div>
</section>
<section id="Sx4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Technical Validation</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">The voice messages included in the EMOVOME database were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to the laboratory environment. As a result, different devices were used for recording, most likely different smartphone microphones, leading to different sampling rates (94% at 48 kHz and 6% at 44.1 kHz). Audios are provided with the original sampling frequencies to let the users choose between applying upsampling at 48kHz to 6% of the audios, applying downsampling at 44.1Hz to 94% of the audios, or eliminating this 6% at 44.1 kHz to have a homogeneous sample in terms of sampling frequency. The audio clips were shared via the WhatsApp application, so they were encoded using the Ogg Vorbis format. Moreover, as indicated in section <span id="Sx4.p1.1.1" class="ltx_text ltx_font_italic">Quality check</span>, a maximum duration of 60 seconds was established to obtain a homogeneous sample with respect to audio length. Audios vary in length from 1 to 59 seconds and contain from 2 to 220 words, as shown in Figure <a href="#Sx4.F6" title="Figure 6 ‣ Technical Validation ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. They have a total length of 293 minutes (<math id="Sx4.p1.1.m1.1" class="ltx_Math" alttext="{\sim}5" display="inline"><semantics id="Sx4.p1.1.m1.1a"><mrow id="Sx4.p1.1.m1.1.1" xref="Sx4.p1.1.m1.1.1.cmml"><mi id="Sx4.p1.1.m1.1.1.2" xref="Sx4.p1.1.m1.1.1.2.cmml"></mi><mo id="Sx4.p1.1.m1.1.1.1" xref="Sx4.p1.1.m1.1.1.1.cmml">∼</mo><mn id="Sx4.p1.1.m1.1.1.3" xref="Sx4.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx4.p1.1.m1.1b"><apply id="Sx4.p1.1.m1.1.1.cmml" xref="Sx4.p1.1.m1.1.1"><csymbol cd="latexml" id="Sx4.p1.1.m1.1.1.1.cmml" xref="Sx4.p1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="Sx4.p1.1.m1.1.1.2.cmml" xref="Sx4.p1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="Sx4.p1.1.m1.1.1.3.cmml" xref="Sx4.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.p1.1.m1.1c">{\sim}5</annotation></semantics></math> hours). Additionally, all audio files were manually checked to remove those recorded in critical background noise conditions.</p>
</div>
<figure id="Sx4.F6" class="ltx_figure">
<div id="Sx4.F6.2" class="ltx_block">
<figure id="Sx4.F6.sf1" class="ltx_figure ltx_align_center">
<p id="Sx4.F6.sf1.1" class="ltx_p">][c]0.49
<img src="/html/2402.17496/assets/histogram_durations.jpg" id="Sx4.F6.sf1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="404" alt="Refer to caption"></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx4.F6.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="Sx4.F6.sf1.4.2" class="ltx_text" style="font-size:90%;">Distribution of the audio duration.</span></figcaption>
</figure>
<figure id="Sx4.F6.sf2" class="ltx_figure ltx_align_center">
<p id="Sx4.F6.sf2.1" class="ltx_p">][c]0.49
<img src="/html/2402.17496/assets/histogram_texts.jpg" id="Sx4.F6.sf2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="404" alt="Refer to caption"></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx4.F6.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="Sx4.F6.sf2.4.2" class="ltx_text" style="font-size:90%;">Distribution of the transcriptions’ length.</span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Sx4.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="Sx4.F6.4.2" class="ltx_text" style="font-size:90%;">EMOVOME data characterization.</span></figcaption>
</figure>
<div id="Sx4.p2" class="ltx_para">
<p id="Sx4.p2.1" class="ltx_p">To validate the database’s usefulness and set a baseline for future investigations on emotion recognition using EMOVOME, we developed emotion recognition models using machine learning for both speech and text modalities. The labels resulting from the combination of experts and non-experts were used. The dataset was divided into 80% and 20% for development and testing, respectively, using a speaker-independent approach, i.e., samples from the same speaker were not included in both development and test. The development set contained 80 participants (40 female, 40 male) containing the following distribution of labels: 241 negative, 305 neutral and 261 positive valence; and 148 low, 328 neutral and 331 high arousal. The test set included 20 participants (10 female, 10 male) with the distribution of labels that follows: 57 negative, 62 neutral and 73 positive valence; and 13 low, 70 neutral and 109 high arousal.</p>
</div>
<div id="Sx4.p3" class="ltx_para">
<p id="Sx4.p3.1" class="ltx_p">For the speech model, all audios were first resampled to 44.1 kHz. Then, we used the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, a standard set developed for computing research purposes that includes features based on their sensitivity to capture changes in the voice produced by affective processes. The use of eGeMAPS facilitates the understanding and reproducibility of the results. The openSMILE toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> was employed to extract the long-term eGeMAPS, obtaining an 88-dimensional feature vector for each audio. Features were normalised by removing the mean and dividing by the standard deviation of the development samples. Different feature selection techniques were applied to avoid overfitting. First, Pearson’s correlation matrix was calculated for the development set and high-correlated features (<math id="Sx4.p3.1.m1.1" class="ltx_Math" alttext="p&gt;0.95" display="inline"><semantics id="Sx4.p3.1.m1.1a"><mrow id="Sx4.p3.1.m1.1.1" xref="Sx4.p3.1.m1.1.1.cmml"><mi id="Sx4.p3.1.m1.1.1.2" xref="Sx4.p3.1.m1.1.1.2.cmml">p</mi><mo id="Sx4.p3.1.m1.1.1.1" xref="Sx4.p3.1.m1.1.1.1.cmml">&gt;</mo><mn id="Sx4.p3.1.m1.1.1.3" xref="Sx4.p3.1.m1.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx4.p3.1.m1.1b"><apply id="Sx4.p3.1.m1.1.1.cmml" xref="Sx4.p3.1.m1.1.1"><gt id="Sx4.p3.1.m1.1.1.1.cmml" xref="Sx4.p3.1.m1.1.1.1"></gt><ci id="Sx4.p3.1.m1.1.1.2.cmml" xref="Sx4.p3.1.m1.1.1.2">𝑝</ci><cn type="float" id="Sx4.p3.1.m1.1.1.3.cmml" xref="Sx4.p3.1.m1.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.p3.1.m1.1c">p&gt;0.95</annotation></semantics></math>) were eliminated. Then, a filter method was implemented, selecting features according to the highest ANOVA F-values. Different values were tested for the percentage of characteristics to be selected, between 25%, 50% and 75% of the total number of features. Finally, two algorithms were selected to create the models: Support Vector Machine (SVM) and K-Nearest Neighbours (KNN). For each, hyperparameter tuning was applied to select the best combination among the options in Table <a href="#Sx4.T6" title="Table 6 ‣ Technical Validation ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We used cross-validation (CV) on the development set, particularly the StratifiedGroupKFold from Scikit-learn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, to create 5 speaker-independent partitions. For the selected combination of features and hyperparameters, a model was trained with the whole development set and evaluated on the test set.</p>
</div>
<figure id="Sx4.T6" class="ltx_table">
<table id="Sx4.T6.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx4.T6.2.1.1" class="ltx_tr">
<th id="Sx4.T6.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Algorithm</th>
<th id="Sx4.T6.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Hyperparameters</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx4.T6.2.2.1" class="ltx_tr">
<th id="Sx4.T6.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">SVM</th>
<td id="Sx4.T6.2.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="Sx4.T6.2.2.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx4.T6.2.2.1.2.1.1" class="ltx_tr">
<td id="Sx4.T6.2.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Kernel: Radial Basis Function (default), Sigmoid</td>
</tr>
<tr id="Sx4.T6.2.2.1.2.1.2" class="ltx_tr">
<td id="Sx4.T6.2.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Gamma: 0.001, 0.01, 0.1, 1, ’auto’, ’scale’ (default)</td>
</tr>
<tr id="Sx4.T6.2.2.1.2.1.3" class="ltx_tr">
<td id="Sx4.T6.2.2.1.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">C: 1 (default), 10, 100, 1000</td>
</tr>
</table>
</td>
</tr>
<tr id="Sx4.T6.2.3.2" class="ltx_tr">
<th id="Sx4.T6.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">KNN</th>
<td id="Sx4.T6.2.3.2.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<table id="Sx4.T6.2.3.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx4.T6.2.3.2.2.1.1" class="ltx_tr">
<td id="Sx4.T6.2.3.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Number of neighbors: 1, 3, 5 (default), 7</td>
</tr>
<tr id="Sx4.T6.2.3.2.2.1.2" class="ltx_tr">
<td id="Sx4.T6.2.3.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Weights: Uniform (default), Distance</td>
</tr>
<tr id="Sx4.T6.2.3.2.2.1.3" class="ltx_tr">
<td id="Sx4.T6.2.3.2.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">Metric: Euclidean, Manhattan, Minkowski (default)</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Sx4.T6.3.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="Sx4.T6.4.2" class="ltx_text" style="font-size:90%;">Hyperparameter sets explored in the baseline approach.</span></figcaption>
</figure>
<div id="Sx4.p4" class="ltx_para">
<p id="Sx4.p4.1" class="ltx_p">For the text model, we selected a multilingual version of the original BERT model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Particularly, we fine-tuned the multilingual BERT Base Cased model (mBERT), which was pre-trained on Wikipedia texts of the 104 languages with the largest content, including Spanish. It is available in TensorFlow Hub. We used the BERT model as the encoder to obtain a pooled output representing each input text. We established a maximum text length of 128 since most of the transcriptions have fewer words (see Figure <a href="#Sx4.F6.sf2" title="In Figure 6 ‣ Technical Validation ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a>). The embeddings were then fed into a dropout layer (ratio = 0.2) and the output dense layer with soft-max activation. We used the AdamW optimizer with a learning rate of 3e-5 and 10% warm-up steps. We fine-tuned the model, varying the number of epochs (3-5) and the batch size (16, 32). The development set was divided into 80% for training (64 speakers, gender-balanced) and 20% for validation (16 speakers, gender-balanced). We used the validation set to select the combination of epochs and batch size, achieving the highest unweighted accuracy, and finally applied the model to the test set.</p>
</div>
<div id="Sx4.p5" class="ltx_para">
<p id="Sx4.p5.1" class="ltx_p">The cross-validation (CV) and test results for the SER models are presented in Table <a href="#Sx4.T7" title="Table 7 ‣ Technical Validation ‣ Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, where different evaluation metrics are reported: weighted accuracy (WA), unweighted accuracy (UA), F1 score (F1) and the confusion matrix (CM). Test results for valence are 49.27% UA when using speech-based models, while the text-based model achieves 61.16% UA. Considering arousal, the speech and text models obtain similar results, with 44.71% and 47.73%, respectively. We would like to note that in a previous study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, we analysed a preliminary version of the EMOVOME database with audios from 24 subjects (62.5% females) and trained binary classification models (high/low arousal, negative/positive valence). We obtained an accuracy of 71.37% and 70.73% for arousal and valence, respectively. However, these results are not comparable as the complete database includes data from 100 individuals and here we trained three-class classification models using pre-trained models. Therefore, we recommend using these results as a baseline for future comparisons.</p>
</div>
<figure id="Sx4.T7" class="ltx_table">
<table id="Sx4.T7.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx4.T7.2.1.1" class="ltx_tr">
<th id="Sx4.T7.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Modality</th>
<th id="Sx4.T7.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Label</th>
<th id="Sx4.T7.2.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Model</th>
<th id="Sx4.T7.2.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">CV WA (%)</th>
<th id="Sx4.T7.2.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">CV UA (%)</th>
<th id="Sx4.T7.2.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">CV F1</th>
<th id="Sx4.T7.2.1.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Test WA (%)</th>
<th id="Sx4.T7.2.1.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Test UA (%)</th>
<th id="Sx4.T7.2.1.1.9" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Test F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx4.T7.2.2.1" class="ltx_tr">
<td id="Sx4.T7.2.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Speech</td>
<td id="Sx4.T7.2.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Valence</td>
<td id="Sx4.T7.2.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">SVC</td>
<td id="Sx4.T7.2.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">45.22 (2.38)</td>
<td id="Sx4.T7.2.2.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">44.39 (2.63)</td>
<td id="Sx4.T7.2.2.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.448 (0.025)</td>
<td id="Sx4.T7.2.2.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">50.00</td>
<td id="Sx4.T7.2.2.1.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">49.27</td>
<td id="Sx4.T7.2.2.1.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.495</td>
</tr>
<tr id="Sx4.T7.2.3.2" class="ltx_tr">
<td id="Sx4.T7.2.3.2.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="Sx4.T7.2.3.2.2" class="ltx_td ltx_align_left ltx_border_r">Arousal</td>
<td id="Sx4.T7.2.3.2.3" class="ltx_td ltx_align_left ltx_border_r">SVC</td>
<td id="Sx4.T7.2.3.2.4" class="ltx_td ltx_align_left ltx_border_r">53.40 (2.17)</td>
<td id="Sx4.T7.2.3.2.5" class="ltx_td ltx_align_left ltx_border_r">49.64 (1.56)</td>
<td id="Sx4.T7.2.3.2.6" class="ltx_td ltx_align_left ltx_border_r">0.534 (0.019)</td>
<td id="Sx4.T7.2.3.2.7" class="ltx_td ltx_align_left ltx_border_r">62.50</td>
<td id="Sx4.T7.2.3.2.8" class="ltx_td ltx_align_left ltx_border_r">44.71</td>
<td id="Sx4.T7.2.3.2.9" class="ltx_td ltx_align_left ltx_border_r">0.621</td>
</tr>
<tr id="Sx4.T7.2.4.3" class="ltx_tr">
<td id="Sx4.T7.2.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Text</td>
<td id="Sx4.T7.2.4.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Valence</td>
<td id="Sx4.T7.2.4.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">mBERT</td>
<td id="Sx4.T7.2.4.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">65.38</td>
<td id="Sx4.T7.2.4.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">65.11</td>
<td id="Sx4.T7.2.4.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.650</td>
<td id="Sx4.T7.2.4.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">60.94</td>
<td id="Sx4.T7.2.4.3.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">61.15</td>
<td id="Sx4.T7.2.4.3.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.612</td>
</tr>
<tr id="Sx4.T7.2.5.4" class="ltx_tr">
<td id="Sx4.T7.2.5.4.1" class="ltx_td ltx_border_b ltx_border_l ltx_border_r"></td>
<td id="Sx4.T7.2.5.4.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">Arousal</td>
<td id="Sx4.T7.2.5.4.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">mBERT</td>
<td id="Sx4.T7.2.5.4.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">51.28</td>
<td id="Sx4.T7.2.5.4.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">40.18</td>
<td id="Sx4.T7.2.5.4.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">0.467</td>
<td id="Sx4.T7.2.5.4.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">62.50</td>
<td id="Sx4.T7.2.5.4.8" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">47.43</td>
<td id="Sx4.T7.2.5.4.9" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">0.603</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Sx4.T7.3.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="Sx4.T7.4.2" class="ltx_text" style="font-size:90%;">Baseline results for emotion recognition models using EMOVOME speech and text.</span></figcaption>
</figure>
</section>
<section id="Sx5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Usage Notes</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">All the data included in the EMOVOME database is publicly available under the Creative Commons Attribution 4.0 International license. The only exception is the original raw audio files, for which an additional step is required as a security measure to safeguard the speakers’ privacy. To request access, interested authors should first complete and sign the agreement file <span id="Sx5.p1.1.1" class="ltx_text ltx_font_italic">EMOVOME_agreement.pdf</span> available in the Zenodo repository <a target="_blank" href="https://zenodo.org/records/10694370" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://zenodo.org/records/10694370</a> and send it to the corresponding author (jamarmo@htech.upv.es). The data included in the EMOVOME database is expected to be used for research purposes only. Therefore, the agreement file states that the authors are not allowed to share the data with profit-making companies or organisations. They are also not expected to distribute the data to other research institutions; instead, they are suggested to kindly refer interested colleagues to the corresponding author of this article. By agreeing to the terms of the agreement, the authors also commit to refraining from publishing the audio content on the media (such as television and radio), in scientific journals (or any other publications), as well as on other platforms on the internet. The agreement must bear the signature of the legally authorised representative of the research institution (e.g., head of laboratory/department). Once the signed agreement is received and validated, the corresponding author will deliver the "Audios" folder containing the audio files through a download procedure. A direct connection between the EMOVOME authors and the applicants guarantees that updates regarding additional materials included in the database can be received by all EMOVOME users.</p>
</div>
<div id="Sx5.p2" class="ltx_para">
<p id="Sx5.p2.1" class="ltx_p">We encourage researchers to use the EMOVOME database in their research to deepen the understanding of emotion recognition in-the-wild conditions. We also encourage researchers to request access to the audio files to apply state-of-the-art techniques requiring the raw audio signal to exploit this database’s potential fully. Furthermore, we provide a partition of the database to facilitate the reproducibility of results, and we have established a baseline for emotion recognition models to serve as a benchmark for future works. We kindly request the researchers using EMOVOME to cite this paper in their publications.</p>
</div>
</section>
<section id="Sx6" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Code availability</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">The code and data partitions for implementing the baseline emotion recognition models are available as part of the EMOVOME database in Zenodo <a target="_blank" href="https://zenodo.org/records/10694370" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://zenodo.org/records/10694370</a>. We developed the code using Python programming language. The speech models rely on the Scikit-learn library, while the text models require the TensorFlow library.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Deschamps-Berger, T., Lamel, L. &amp; Devillers, L.

</span>
<span class="ltx_bibblock">End-to-end speech emotion recognition: Challenges of real-life emergency call centers data recordings.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Affective Computing and Intelligent Interaction (ACII 2021)</em>, <a target="_blank" href="https://doi.org/10.1109/ACII52823.2021.9597419" title="" class="ltx_ref ltx_url">https://doi.org/10.1109/ACII52823.2021.9597419</a> (2021).

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Wu, P. <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Automatic depression recognition by intelligent speech signal processing: A systematic survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.2.1" class="ltx_emph ltx_font_italic"><span id="bib.bib2.2.1.1" class="ltx_ERROR undefined">\JournalTitle</span>CAAI Transactions on Intelligence Technology</em> <span id="bib.bib2.3.2" class="ltx_text ltx_font_bold">8</span>, 701–711, <a target="_blank" href="https://doi.org/10.1049/cit2.12113" title="" class="ltx_ref ltx_url">https://doi.org/10.1049/cit2.12113</a> (2023).

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Croissant, M., Schofield, G. &amp; McCall, C.

</span>
<span class="ltx_bibblock">Theories, methodologies, and effects of affect-adaptive games: A systematic review.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib3.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Entertainment Computing</em> <span id="bib.bib3.2.2" class="ltx_text ltx_font_bold">47</span>, 100591, <a target="_blank" href="https://doi.org/10.1016/j.entcom.2023.100591" title="" class="ltx_ref ltx_url">https://doi.org/10.1016/j.entcom.2023.100591</a> (2023).

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Plutchik, R.

</span>
<span class="ltx_bibblock">The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib4.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>American scientist</em> <span id="bib.bib4.2.2" class="ltx_text ltx_font_bold">89</span>, 344–350 (2001).

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Scherer, K. R.

</span>
<span class="ltx_bibblock">Theory convergence in emotion science is timely and realistic.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib5.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Cognition and Emotion</em> <span id="bib.bib5.2.2" class="ltx_text ltx_font_bold">36</span>, 154–170, <a target="_blank" href="https://doi.org/10.1080/02699931.2021.1973378" title="" class="ltx_ref ltx_url">https://doi.org/10.1080/02699931.2021.1973378</a> (2022).

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Iriondo, I. <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Validation of an acoustical modelling of emotional expression in spanish using speech synthesis techniques.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.2.1" class="ltx_emph ltx_font_italic">ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion</em> (2000).

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Montero, J. M., Gutiérrez-Arriola, J., Colás, J., Enriquez, E. &amp; Pardo, J. M.

</span>
<span class="ltx_bibblock">Analysis and modelling of emotional speech in spanish.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of international conference on phonetic sciences</em>, vol. 2, 957–960 (1999).

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Martínez, C. Á. &amp; Cruz, A. B.

</span>
<span class="ltx_bibblock">Emotion recognition in non-structured utterances for human-robot interaction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.</em>, 19–23 (IEEE, 2005).

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Caballero-Morales, S.-O.

</span>
<span class="ltx_bibblock">Recognition of emotions in mexican spanish speech: An approach based on acoustic modelling of emotion-specific vowels.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib9.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>The Scientific World Journal</em> <span id="bib.bib9.2.2" class="ltx_text ltx_font_bold">2013</span> (2013).

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Barra-Chicote, R. <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Spanish expressive voices: Corpus for emotion research in spanish.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.2.1" class="ltx_emph ltx_font_italic">Proc. of LREC</em> (Citeseer, 2008).

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
López, J. M., Cearreta, I., Fajardo, I. &amp; Garay, N.

</span>
<span class="ltx_bibblock">Validating a multilingual and multimodal affective database.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Conference on Usability and Internationalization</em>, 422–431 (Springer, 2007).

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Emotional speech synthesis database elra-s0329.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://catalog.elra.info/en-us/repository/browse/ELRA-S0329/" title="" class="ltx_ref ltx_url">https://catalog.elra.info/en-us/repository/browse/ELRA-S0329/</a> (2011).

</span>
<span class="ltx_bibblock">Accessed: 2023-12-30.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Garcia-Cuesta, E., Salvador, A. B. &amp; Pãez, D. G.

</span>
<span class="ltx_bibblock">Emomatchspanishdb: study of speech emotion recognition machine learning models in a new spanish elicited database.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib13.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Multimedia Tools and Applications</em> 1–20 (2023).

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Rosas, V. P., Mihalcea, R. &amp; Morency, L.-P.

</span>
<span class="ltx_bibblock">Multimodal sentiment analysis of spanish online videos.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib14.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>IEEE intelligent Systems</em> <span id="bib.bib14.2.2" class="ltx_text ltx_font_bold">28</span>, 38–45 (2013).

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Duville, M. M., Alonso-Valerdi, L. M. &amp; Ibarra-Zarate, D. I.

</span>
<span class="ltx_bibblock">The mexican emotional speech database (mesd): elaboration and assessment based on machine learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</em>, 1644–1647 (IEEE, 2021).

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Zadeh, A. <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Cmu-moseas: A multimodal language dataset for spanish, portuguese, german and french.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.2.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing</em>, vol. 2020, 1801 (NIH Public Access, 2020).

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Parada-Cabaleiro, E., Costantini, G., Batliner, A., Baird, A. &amp; Schuller, B.

</span>
<span class="ltx_bibblock">Categorical vs dimensional perception of italian emotional speech.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2018</em>, 3638–3642, <a target="_blank" href="https://doi.org/10.21437/Interspeech.2018-47" title="" class="ltx_ref ltx_url">https://doi.org/10.21437/Interspeech.2018-47</a> (2018).

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
McIntyre, G. &amp; Göcke, R.

</span>
<span class="ltx_bibblock">The composite sensing of affect.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Affect and Emotion in Human-Computer Interaction: From Theory to Applications</em>, 104–115 (Springer, 2008).

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
El Ayadi, M., Kamel, M. S. &amp; Karray, F.

</span>
<span class="ltx_bibblock">Survey on speech emotion recognition: Features, classification schemes, and databases.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib19.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Pattern Recognition</em> <span id="bib.bib19.2.2" class="ltx_text ltx_font_bold">44</span>, 572–587, <a target="_blank" href="https://doi.org/10.1016/j.patcog.2010.09.020" title="" class="ltx_ref ltx_url">https://doi.org/10.1016/j.patcog.2010.09.020</a> (2011).

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Koolagudi, S. G. &amp; Rao, K. S.

</span>
<span class="ltx_bibblock">Emotion recognition from speech: a review.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib20.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>International journal of speech technology</em> <span id="bib.bib20.2.2" class="ltx_text ltx_font_bold">15</span>, 99–117 (2012).

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Costa, P. &amp; McCrae, R.

</span>
<span class="ltx_bibblock">Neo five-factor inventory (neo-ffi).

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib21.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Odessa, FL: Psychological Assessment Resources</em> <span id="bib.bib21.2.2" class="ltx_text ltx_font_bold">3</span> (1989).

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Bradley, M. M. &amp; Lang, P. J.

</span>
<span class="ltx_bibblock">Measuring emotion: the self-assessment manikin and the semantic differential.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib22.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Journal of behavior therapy and experimental psychiatry</em> <span id="bib.bib22.2.2" class="ltx_text ltx_font_bold">25</span>, 49–59, <a target="_blank" href="https://doi.org/10.1016/0005-7916(94)90063-9" title="" class="ltx_ref ltx_url">https://doi.org/10.1016/0005-7916(94)90063-9</a> (1994).

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Ekman, P.

</span>
<span class="ltx_bibblock">Basic emotions.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib23.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Handbook of cognition and emotion</em> <span id="bib.bib23.2.2" class="ltx_text ltx_font_bold">98</span>, 16 (1999).

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Gómez-Zaragozá, L. <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Emotional voice messages (emovome) database, <a href="10.5281/zenodo.10694370" title="" class="ltx_ref ltx_url">10.5281/zenodo.10694370</a> (2024).

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Eyben, F. <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.2.1" class="ltx_emph ltx_font_italic"><span id="bib.bib25.2.1.1" class="ltx_ERROR undefined">\JournalTitle</span>IEEE transactions on affective computing</em> <span id="bib.bib25.3.2" class="ltx_text ltx_font_bold">7</span>, 190–202 (2015).

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Eyben, F., Wöllmer, M. &amp; Schuller, B.

</span>
<span class="ltx_bibblock">Opensmile: the munich versatile and fast open-source audio feature extractor.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 18th ACM international conference on Multimedia</em>, 1459–1462 (2010).

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Pedregosa, F. <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Scikit-learn: Machine learning in Python.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.2.1" class="ltx_emph ltx_font_italic"><span id="bib.bib27.2.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Journal of Machine Learning Research</em> <span id="bib.bib27.3.2" class="ltx_text ltx_font_bold">12</span>, 2825–2830 (2011).

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib28.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>arXiv preprint arXiv:1810.04805</em> (2018).

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Gómez-Zaragozá, L., Marín-Morales, J., Parra, E., Guixeres, J. &amp; Alcañiz, M.

</span>
<span class="ltx_bibblock">Speech emotion recognition from social media voice messages recorded in the wild.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">HCI International 2020-Posters: 22nd International Conference, HCII 2020, Copenhagen, Denmark, July 19–24, 2020, Proceedings, Part I 22</em>, 330–336 (Springer, 2020).

</span>
</li>
</ul>
</section>
<section id="Sx7" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx7.p1" class="ltx_para">
<p id="Sx7.p1.1" class="ltx_p">This work was supported by the European Union’s Horizon 2020 funded project "HELIOS: A Context-aware Distributed Social Networking Framework” (No 825585), by the Generalitat Valenciana (ACIF/2021/187) and by the Spanish Ministry of Science and Innovation for the DIPSY project (TED2021-131401B-C21).</p>
</div>
</section>
<section id="Sx8" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Author contributions</h2>

<div id="Sx8.p1" class="ltx_para">
<p id="Sx8.p1.1" class="ltx_p">L.G.Z.: study conception and design, data collection, data analysis, writing – original draft, writing – review and editing.
J.M.M.: study conception and design, data collection, data analysis, writing – original draft, writing – review and editing.
E.P.V.: study conception and design, data collection, writing – review and editing.
R.dA.: data collection, data analysis, writing – review and editing.
V.N.: study conception and design, writing – review and editing, funding acquisition.
M.A.R: study conception and design, writing – review and editing, funding acquisition.</p>
</div>
</section>
<section id="Sx9" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Competing interests</h2>

<div id="Sx9.p1" class="ltx_para">
<p id="Sx9.p1.1" class="ltx_p">The authors declare no competing interests.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.17495" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.17496" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.17496">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.17496" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.17497" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 13:16:27 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
