<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.03179] UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization</title><meta property="og:description" content="Video localization tasks aim to temporally locate specific instances in videos, including temporal action localization (TAL), sound event detection (SED) and audio-visual event localization (AVEL).
Existing methods ove…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.03179">

<!--Generated on Sun May  5 22:33:15 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">(eccv)                Package eccv Warning: Package ‘hyperref’ is loaded with option ‘pagebackref’, which is *not* recommended for camera-ready version</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Southern University of Science and Technology, China 
<br class="ltx_break"></span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>University of Birmingham
</span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>The University of Hong Kong 
<br class="ltx_break"></span></span></span><span id="id4" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_note_type">institutetext: </span>William and Mary, US
</span></span></span><span id="id5" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">institutetext: </span>Harbin Institute of Technology (Shenzhen), China
<br class="ltx_break"><span id="id5.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">email: </span>gengtiantian97@gmail.com</span></span></span>,
<span id="id5.2" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">email: </span>tengwang@connect.hku.hk</span></span></span>,
<span id="id5.3" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">email: </span>yzhang105@wm.edu</span></span></span>
<br class="ltx_break"><span id="id5.4" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">email: </span>j.duan@cs.bham.ac.uk</span></span></span>,
<span id="id5.5" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">email: </span>zhengf@sustech.edu.cn</span></span></span>,
<span id="id5.6" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_note_type">email: </span>honeyguan@gmail.com</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tiantian Geng

</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Teng Wang
</span><span class="ltx_author_notes">1133</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yanfu Zhang
</span><span class="ltx_author_notes">44</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jinming Duan
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Weili Guan
</span><span class="ltx_author_notes">55</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Feng Zheng
</span><span class="ltx_author_notes">11</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Video localization tasks aim to temporally locate specific instances in videos, including temporal action localization (TAL), sound event detection (SED) and audio-visual event localization (AVEL).
Existing methods over-specialize on
each task, overlooking the fact that
these instances often occur in the same video to form the complete video content.
In this work, we present UniAV, a Unified Audio-Visual perception network, to achieve joint learning of TAL, SED and AVEL tasks for the first time.
UniAV can leverage <span id="id1.id1.1" class="ltx_text" style="color:#000000;"> diverse data available</span> in task-specific datasets, allowing the model to learn and share mutually beneficial knowledge across tasks and modalities.
To tackle the challenges posed by substantial variations in datasets (size/domain/duration) and distinct task characteristics, we propose to uniformly encode visual and audio modalities of all videos to derive generic representations,
while also designing task-specific experts to capture unique knowledge for each task.
Besides, we develop a unified language-aware classifier by utilizing a pre-trained text encoder, enabling the model to flexibly detect <span id="id1.id1.2" class="ltx_text" style="color:#000000;">various types of</span> instances and previously unseen ones <span id="id1.id1.3" class="ltx_text" style="color:#000000;">by simply changing prompts</span> during inference.
UniAV outperforms its single-task counterparts by a large margin
with fewer parameters, achieving on-par or superior performances compared to state-of-the-art task-specific methods across ActivityNet 1.3, DESED and UnAV-100 benchmarks.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>
<span id="id2.id1" class="ltx_text" style="color:#000000;">Multi-modal video localization Multi-task learning</span>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the explosion of video content due to social networks and digital cameras, video understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> continues to be one of the essential research domains in computer vision.
Videos recorded in natural scenes are always untrimmed and comprise both visual and audio modalities.
They usually cover multiple instances of interest, including visible actions, audible sound events as well as audio-visual events <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> that are both audible and visible at the same time.
For example, as illustrated in <a href="#S1.F1" title="In 1 Introduction ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, we can discern the visual action of “playing ten pins”, the audio-visual event of “striking bowling”, and also the background narration of “man/woman speaking”.
All these events are equally crucial, jointly contributing to the overall understanding of video content.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.03179/assets/figures/fig1_new.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Our unified framework can localize all three kinds of instances in untrimmed videos, including visual actions, sound events and audio-visual events. All these instances contribute to a comprehensive understanding of video content. </span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, current video localization approaches only concentrate on recognizing and detecting one type of these instances, involving the tasks of temporal action localization (TAL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, sound event detection (SED) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> and
audio-visual event localization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (AVEL).
Despite convenience for some specific applications, such separate definitions bring the following drawbacks:
1) Independent designs cause redundant parameters since recent localization models usually adopt similar architectures, <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p2.1.2" class="ltx_text"></span>, transformer backbones in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
2) It hinders models from learning and sharing generic knowledge between different tasks and modalities.
<span id="S1.p2.1.3" class="ltx_text" style="color:#000000;">For example, rich TAL data enables models to identify common instances, which can naturally assist AVEL and SED tasks.
Additionally, AVEL data allows models to learn corresponding audio representations for many visual actions in TAL and learn visual cues for sound events in SED, thereby facilitating improvements in both TAL and SED tasks.</span>
Besides, some recent task-specific methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> have verified that integrating visual and audio modalities is beneficial for TAL and SED tasks.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we aim to develop a unified framework to localize all these types of instances in untrimmed videos, solving three video localization tasks (TAL, AVEL and SED) by a single model.
However, the main obstacles hindering this attempt lie in two aspects.
Firstly, the datasets for these tasks exhibit distinct properties with significant domain and duration gaps.
For example, ActivityNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> for TAL focuses on human activities, while UnAV-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> for AVEL and DESED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> for SED contain events from other domains such as animals, nature and tools.
Besides, the instance duration of different datasets varies greatly, <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p3.1.2" class="ltx_text"></span>, over 50% events in DESED are less than 1s, while the instances in ActivityNet and UnAV-100 are much longer, with the longest ones lasting over 7 minutes.
Secondly, different tasks emphasize different video characteristics and modalities.
TAL pays more attention to capturing temporal relationships of actions on the visual track, while SED is dedicated to
the fine-grained understanding of sound events. In contrast, AVEL assigns equal importance to both auditory and visual cues.
<span id="S1.p3.1.3" class="ltx_text" style="color:#000000;"> Thus, unifying these three tasks is challenging, and just simple joint training may lead to a significant decrease in performance.</span></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To tackle these challenges, we introduce UniAV, a multi-task learning framework for Unified Audio-Visual perception in video localization tasks.
<span id="S1.p4.1.1" class="ltx_text" style="color:#000000;">We unify TAL, SED and AVEL tasks within a single model from three aspects.</span>
1) <span id="S1.p4.1.2" class="ltx_text ltx_font_bold">Unified audio-visual encoding.</span>
In order to unify diversity between the data from different tasks and obtain general input representations, we employ the large pre-trained model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> to uniformly tokenize the visual and audio modalities of all input videos.
Then, the obtained embeddings are fed into an audio-visual pyramid transformer network, enabling the model to detect both very short as well as long instances that span minutes.
2) <span id="S1.p4.1.3" class="ltx_text ltx_font_bold">Task-specific experts.</span> Due to the divergence of different tasks, learning distinct knowledge for each task is critical.
Thus, we design task-specific expert layers in our transformer blocks to learn task-specific features by switching to corresponding experts according to the input data.
3) <span id="S1.p4.1.4" class="ltx_text ltx_font_bold">Unified language-aware classifier.</span> Datasets for different tasks pose their own category sets. Instead of using separate task-specific classification heads, we propose a unified language-aware classifier by tokenizing the class vocabularies with task-specific prompts using the pre-trained text encoder.
<span id="S1.p4.1.5" class="ltx_text" style="color:#000000;">Benefiting from this new formulation, our model gains the flexibility to detect different types of instances by simply changing prompts and expands the capability to recognize previously unseen instances.</span></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.3" class="ltx_p"><span id="S1.p5.3.4" class="ltx_text" style="color:#000000;">With the unified framework, UniAV can learn from diverse task-specific data and handle three video localization tasks with the same model parameters.</span>
<span id="S1.p5.3.3" class="ltx_text" style="color:#000000;">Extensive experiments demonstrate that
UniAV outperforms its single-task counterparts by a large margin with fewer parameters.
Besides, multi-task joint training can be an effective pretraining step for the single-task models, leading to further gains and setting new state-of-the-art results across all three tasks,
<em id="S1.p5.3.3.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p5.3.3.2" class="ltx_text"></span>, ActivityNet 1.3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> (<math id="S1.p5.1.1.m1.1" class="ltx_Math" alttext="36.2\%" display="inline"><semantics id="S1.p5.1.1.m1.1a"><mrow id="S1.p5.1.1.m1.1.1" xref="S1.p5.1.1.m1.1.1.cmml"><mn mathcolor="#000000" id="S1.p5.1.1.m1.1.1.2" xref="S1.p5.1.1.m1.1.1.2.cmml">36.2</mn><mo mathcolor="#000000" id="S1.p5.1.1.m1.1.1.1" xref="S1.p5.1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.1.1.m1.1b"><apply id="S1.p5.1.1.m1.1.1.cmml" xref="S1.p5.1.1.m1.1.1"><csymbol cd="latexml" id="S1.p5.1.1.m1.1.1.1.cmml" xref="S1.p5.1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S1.p5.1.1.m1.1.1.2.cmml" xref="S1.p5.1.1.m1.1.1.2">36.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.1.m1.1c">36.2\%</annotation></semantics></math> average mAP) for TAL, DESED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> (<math id="S1.p5.2.2.m2.1" class="ltx_Math" alttext="61.1\%" display="inline"><semantics id="S1.p5.2.2.m2.1a"><mrow id="S1.p5.2.2.m2.1.1" xref="S1.p5.2.2.m2.1.1.cmml"><mn mathcolor="#000000" id="S1.p5.2.2.m2.1.1.2" xref="S1.p5.2.2.m2.1.1.2.cmml">61.1</mn><mo mathcolor="#000000" id="S1.p5.2.2.m2.1.1.1" xref="S1.p5.2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.2.2.m2.1b"><apply id="S1.p5.2.2.m2.1.1.cmml" xref="S1.p5.2.2.m2.1.1"><csymbol cd="latexml" id="S1.p5.2.2.m2.1.1.1.cmml" xref="S1.p5.2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S1.p5.2.2.m2.1.1.2.cmml" xref="S1.p5.2.2.m2.1.1.2">61.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.2.2.m2.1c">61.1\%</annotation></semantics></math> average mAP) for SED, and UnAV-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (<math id="S1.p5.3.3.m3.1" class="ltx_Math" alttext="51.7\%" display="inline"><semantics id="S1.p5.3.3.m3.1a"><mrow id="S1.p5.3.3.m3.1.1" xref="S1.p5.3.3.m3.1.1.cmml"><mn mathcolor="#000000" id="S1.p5.3.3.m3.1.1.2" xref="S1.p5.3.3.m3.1.1.2.cmml">51.7</mn><mo mathcolor="#000000" id="S1.p5.3.3.m3.1.1.1" xref="S1.p5.3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.3.3.m3.1b"><apply id="S1.p5.3.3.m3.1.1.cmml" xref="S1.p5.3.3.m3.1.1"><csymbol cd="latexml" id="S1.p5.3.3.m3.1.1.1.cmml" xref="S1.p5.3.3.m3.1.1.1">percent</csymbol><cn type="float" id="S1.p5.3.3.m3.1.1.2.cmml" xref="S1.p5.3.3.m3.1.1.2">51.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.3.3.m3.1c">51.7\%</annotation></semantics></math> average mAP) for AVEL. </span></p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">Our contributions can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text" style="color:#000000;">To the best of our knowledge, our UniAV is the first unified framework that solves temporal action localization, sound event detection and audio-visual event localization within a single model,
leading to a holistic understanding of video content in real-world scenarios. </span></p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text" style="color:#000000;">We propose a unified audio-visual encoding pipeline to address data discrepancies across diverse tasks, while also incorporating task-specific experts to capture distinct features for each task.</span></p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text" style="color:#000000;">We design a unified language-aware classifier, allowing the model to flexibly detect various types of instances and previously unseen ones during inference.</span></p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Temporal Localization Tasks</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_bold">Temporal action localization (TAL)</span> aims to localize and classify action instances occurring in an untrimmed video. Supervised learning-based TAL can be categorized into two-stage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and single-stage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> methods.
However, previous works mainly focus on temporal modeling within the visual modality (<em id="S2.SS1.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS1.p1.1.3" class="ltx_text"></span>, RGB and optical flow), ignoring the information in its corresponding audio track.
Recently, some works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> have attempted to utilize the audio modality in videos for TAL, and have found it very helpful to detect the actions with strong audio cues, thus boosting the model performance.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Sound event detection (SED)</span> is a popular task in the audio signal processing community, which involves temporally detecting sound events in a purely acoustic scene. The DCASE Challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> examines sound event detection in domestic environments as one of the challenge tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.
Sound events typically come with their corresponding visual information, <em id="S2.SS1.p2.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS1.p2.1.3" class="ltx_text"></span>, <span id="S2.SS1.p2.1.4" class="ltx_text ltx_font_italic" style="color:#000000;">man speaking<span id="S2.SS1.p2.1.4.1" class="ltx_text" style="color:#000000;">, playing guitar, and dog barking</span></span>.
It has been verified that incorporating visual modality is beneficial for SED in some recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Audio-visual event localization (AVEL)</span> aims to detect events that are simultaneously audible and visible in video content.
Tian <em id="S2.SS1.p3.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.p3.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> introduced the first AVE dataset and proposed an audio-guided visual attention model for the task. Subsequent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> primarily concentrated on event information modeling and cross-modal fusion strategies.
However, these works formulate AVEL as a segment-level classification problem based on trimmed, short video clips. Each video clip only contains a single audio-visual event, which deviates from real-life scenarios involving diverse untrimmed videos.
To solve the problem, Geng <em id="S2.SS1.p3.1.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.p3.1.5" class="ltx_text"></span> built the UnAV-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> dataset for localizing audio-visual events in untrimmed videos and proposed a model to recognize multiple events and regress their temporal boundaries in a single pass.
Furthermore, few approaches for audio-visual video parsing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and multisensory temporal event localization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> strive to identify audio-only, visual-only and audio-visual events in videos. However, they are all segment-level classification methods with fragmented task definitions, and confined to weakly-supervised settings due to
<span id="S2.SS1.p3.1.6" class="ltx_text" style="color:#000000;">the lack of temporal annotations in videos during training.</span>
In comparison, we develop a multi-task supervised framework that learns multi-modal event localization from large-scale individually collected datasets for TAL, SED and AVEL with rich label vocabularies. Moreover, our model is proposal-based, capable of flexibly regressing and recognizing all temporal instances in untrimmed videos.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Multi-Task Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Instead of separate training for each individual task, multi-task learning involves tackling multiple related tasks simultaneously, aiming to share and leverage knowledge across them.
For example, recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>
proposed to jointly learn visual grounding tasks in a collaborative model.
Yan <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> presented Unicorn to solve four object tracking problems.
Lu <em id="S2.SS2.p1.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p1.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> undertook training across 12 vision-language tasks, with each task having its own task-specific prediction head.
For video understanding tasks, UniVTG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> stands out for its emphasis on unifying three temporal grounding tasks: moment retrieval, highlight detection, and video summarization, employing specific query types.
Addressing episodic memory tasks, MINOTAUR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> exhibits proficiency in handling three egocentric vision tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> through a singular model.
In this work, we aim to temporally localize diverse modality-aware instances in untrimmed videos, thereby advancing comprehensive video understanding. This work pioneers the integration of temporal action localization, sound event detection, and audio-visual event localization within a unified framework for the first time.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The UniAV Framework</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our goal is to develop a unified framework to localize visual actions, sound events and audio-visual events in an untrimmed video.
To achieve this, we propose to unite three video localization tasks: TAL, SED and AVEL, leveraging inherent similarities among them.
<a href="#S3.F2" title="In 3 The UniAV Framework ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> shows an overview of the proposed framework.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2404.03179/assets/figures/overview_new.jpg" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="228" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">
The overview of our unified framework. Given a visual and audio pair from an untrimmed video, we first tokenize them by a pair of visual and audio encoders.
Then, the encoded features are fed into an audio-visual pyramid transformer for cross-modal fusion at multiple temporal scales. The task-specific experts in transformer blocks learn distinct knowledge for each task.
And the categories of each task are encoded with prompts to compute similarities with pyramid features, which are used to perform language-aware classification.
Finally, the model recognizes classes and regresses temporal boundaries for all types of instances occurring in the video. </span></figcaption>
</figure>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.12" class="ltx_p"><span id="S3.p2.12.1" class="ltx_text ltx_font_bold">Problem setting.</span>
Given an untrimmed video containing visual and audio tracks, our model aims to output the categories as well as the start/end timestamps of all instances occurring in the video.
We formulate all three tasks (TAL, SED and AVEL) as a joint classification and regression problem.
Formally, we first divide the input video into <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">T</annotation></semantics></math> visual snippets <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="\{V_{t}\}_{t=1}^{T}" display="inline"><semantics id="S3.p2.2.m2.1a"><msubsup id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mrow id="S3.p2.2.m2.1.1.1.1.1" xref="S3.p2.2.m2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.p2.2.m2.1.1.1.1.1.2" xref="S3.p2.2.m2.1.1.1.1.2.cmml">{</mo><msub id="S3.p2.2.m2.1.1.1.1.1.1" xref="S3.p2.2.m2.1.1.1.1.1.1.cmml"><mi id="S3.p2.2.m2.1.1.1.1.1.1.2" xref="S3.p2.2.m2.1.1.1.1.1.1.2.cmml">V</mi><mi id="S3.p2.2.m2.1.1.1.1.1.1.3" xref="S3.p2.2.m2.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S3.p2.2.m2.1.1.1.1.1.3" xref="S3.p2.2.m2.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.p2.2.m2.1.1.1.3" xref="S3.p2.2.m2.1.1.1.3.cmml"><mi id="S3.p2.2.m2.1.1.1.3.2" xref="S3.p2.2.m2.1.1.1.3.2.cmml">t</mi><mo id="S3.p2.2.m2.1.1.1.3.1" xref="S3.p2.2.m2.1.1.1.3.1.cmml">=</mo><mn id="S3.p2.2.m2.1.1.1.3.3" xref="S3.p2.2.m2.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">T</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1">superscript</csymbol><apply id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.2.cmml" xref="S3.p2.2.m2.1.1">subscript</csymbol><set id="S3.p2.2.m2.1.1.1.1.2.cmml" xref="S3.p2.2.m2.1.1.1.1.1"><apply id="S3.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p2.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.p2.2.m2.1.1.1.1.1.1.2">𝑉</ci><ci id="S3.p2.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.p2.2.m2.1.1.1.1.1.1.3">𝑡</ci></apply></set><apply id="S3.p2.2.m2.1.1.1.3.cmml" xref="S3.p2.2.m2.1.1.1.3"><eq id="S3.p2.2.m2.1.1.1.3.1.cmml" xref="S3.p2.2.m2.1.1.1.3.1"></eq><ci id="S3.p2.2.m2.1.1.1.3.2.cmml" xref="S3.p2.2.m2.1.1.1.3.2">𝑡</ci><cn type="integer" id="S3.p2.2.m2.1.1.1.3.3.cmml" xref="S3.p2.2.m2.1.1.1.3.3">1</cn></apply></apply><ci id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\{V_{t}\}_{t=1}^{T}</annotation></semantics></math> and audio snippets <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="\{A_{t}\}_{t=1}^{T}" display="inline"><semantics id="S3.p2.3.m3.1a"><msubsup id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mrow id="S3.p2.3.m3.1.1.1.1.1" xref="S3.p2.3.m3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.p2.3.m3.1.1.1.1.1.2" xref="S3.p2.3.m3.1.1.1.1.2.cmml">{</mo><msub id="S3.p2.3.m3.1.1.1.1.1.1" xref="S3.p2.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.p2.3.m3.1.1.1.1.1.1.2" xref="S3.p2.3.m3.1.1.1.1.1.1.2.cmml">A</mi><mi id="S3.p2.3.m3.1.1.1.1.1.1.3" xref="S3.p2.3.m3.1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S3.p2.3.m3.1.1.1.1.1.3" xref="S3.p2.3.m3.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.p2.3.m3.1.1.1.3" xref="S3.p2.3.m3.1.1.1.3.cmml"><mi id="S3.p2.3.m3.1.1.1.3.2" xref="S3.p2.3.m3.1.1.1.3.2.cmml">t</mi><mo id="S3.p2.3.m3.1.1.1.3.1" xref="S3.p2.3.m3.1.1.1.3.1.cmml">=</mo><mn id="S3.p2.3.m3.1.1.1.3.3" xref="S3.p2.3.m3.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml">T</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1">superscript</csymbol><apply id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.2.cmml" xref="S3.p2.3.m3.1.1">subscript</csymbol><set id="S3.p2.3.m3.1.1.1.1.2.cmml" xref="S3.p2.3.m3.1.1.1.1.1"><apply id="S3.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p2.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1.2">𝐴</ci><ci id="S3.p2.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1.3">𝑡</ci></apply></set><apply id="S3.p2.3.m3.1.1.1.3.cmml" xref="S3.p2.3.m3.1.1.1.3"><eq id="S3.p2.3.m3.1.1.1.3.1.cmml" xref="S3.p2.3.m3.1.1.1.3.1"></eq><ci id="S3.p2.3.m3.1.1.1.3.2.cmml" xref="S3.p2.3.m3.1.1.1.3.2">𝑡</ci><cn type="integer" id="S3.p2.3.m3.1.1.1.3.3.cmml" xref="S3.p2.3.m3.1.1.1.3.3">1</cn></apply></apply><ci id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">\{A_{t}\}_{t=1}^{T}</annotation></semantics></math>, where <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.p2.4.m4.1a"><mi id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><ci id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">T</annotation></semantics></math> varies across videos. Following the setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, the model is expected to predict the event label <math id="S3.p2.5.m5.1" class="ltx_Math" alttext="Y=\{y_{i}\}_{i=1}^{N}" display="inline"><semantics id="S3.p2.5.m5.1a"><mrow id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mi id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml">Y</mi><mo id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml">=</mo><msubsup id="S3.p2.5.m5.1.1.1" xref="S3.p2.5.m5.1.1.1.cmml"><mrow id="S3.p2.5.m5.1.1.1.1.1.1" xref="S3.p2.5.m5.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.p2.5.m5.1.1.1.1.1.1.2" xref="S3.p2.5.m5.1.1.1.1.1.2.cmml">{</mo><msub id="S3.p2.5.m5.1.1.1.1.1.1.1" xref="S3.p2.5.m5.1.1.1.1.1.1.1.cmml"><mi id="S3.p2.5.m5.1.1.1.1.1.1.1.2" xref="S3.p2.5.m5.1.1.1.1.1.1.1.2.cmml">y</mi><mi id="S3.p2.5.m5.1.1.1.1.1.1.1.3" xref="S3.p2.5.m5.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.p2.5.m5.1.1.1.1.1.1.3" xref="S3.p2.5.m5.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.p2.5.m5.1.1.1.1.3" xref="S3.p2.5.m5.1.1.1.1.3.cmml"><mi id="S3.p2.5.m5.1.1.1.1.3.2" xref="S3.p2.5.m5.1.1.1.1.3.2.cmml">i</mi><mo id="S3.p2.5.m5.1.1.1.1.3.1" xref="S3.p2.5.m5.1.1.1.1.3.1.cmml">=</mo><mn id="S3.p2.5.m5.1.1.1.1.3.3" xref="S3.p2.5.m5.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.p2.5.m5.1.1.1.3" xref="S3.p2.5.m5.1.1.1.3.cmml">N</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><eq id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2"></eq><ci id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3">𝑌</ci><apply id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m5.1.1.1.2.cmml" xref="S3.p2.5.m5.1.1.1">superscript</csymbol><apply id="S3.p2.5.m5.1.1.1.1.cmml" xref="S3.p2.5.m5.1.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m5.1.1.1.1.2.cmml" xref="S3.p2.5.m5.1.1.1">subscript</csymbol><set id="S3.p2.5.m5.1.1.1.1.1.2.cmml" xref="S3.p2.5.m5.1.1.1.1.1.1"><apply id="S3.p2.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.p2.5.m5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m5.1.1.1.1.1.1.1.1.cmml" xref="S3.p2.5.m5.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p2.5.m5.1.1.1.1.1.1.1.2.cmml" xref="S3.p2.5.m5.1.1.1.1.1.1.1.2">𝑦</ci><ci id="S3.p2.5.m5.1.1.1.1.1.1.1.3.cmml" xref="S3.p2.5.m5.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S3.p2.5.m5.1.1.1.1.3.cmml" xref="S3.p2.5.m5.1.1.1.1.3"><eq id="S3.p2.5.m5.1.1.1.1.3.1.cmml" xref="S3.p2.5.m5.1.1.1.1.3.1"></eq><ci id="S3.p2.5.m5.1.1.1.1.3.2.cmml" xref="S3.p2.5.m5.1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.p2.5.m5.1.1.1.1.3.3.cmml" xref="S3.p2.5.m5.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.p2.5.m5.1.1.1.3.cmml" xref="S3.p2.5.m5.1.1.1.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">Y=\{y_{i}\}_{i=1}^{N}</annotation></semantics></math>, where <math id="S3.p2.6.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.p2.6.m6.1a"><mi id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><ci id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">N</annotation></semantics></math> is the number of predicted events. Each instance <math id="S3.p2.7.m7.3" class="ltx_Math" alttext="y_{i}=(s_{i},e_{i},c_{i})" display="inline"><semantics id="S3.p2.7.m7.3a"><mrow id="S3.p2.7.m7.3.3" xref="S3.p2.7.m7.3.3.cmml"><msub id="S3.p2.7.m7.3.3.5" xref="S3.p2.7.m7.3.3.5.cmml"><mi id="S3.p2.7.m7.3.3.5.2" xref="S3.p2.7.m7.3.3.5.2.cmml">y</mi><mi id="S3.p2.7.m7.3.3.5.3" xref="S3.p2.7.m7.3.3.5.3.cmml">i</mi></msub><mo id="S3.p2.7.m7.3.3.4" xref="S3.p2.7.m7.3.3.4.cmml">=</mo><mrow id="S3.p2.7.m7.3.3.3.3" xref="S3.p2.7.m7.3.3.3.4.cmml"><mo stretchy="false" id="S3.p2.7.m7.3.3.3.3.4" xref="S3.p2.7.m7.3.3.3.4.cmml">(</mo><msub id="S3.p2.7.m7.1.1.1.1.1" xref="S3.p2.7.m7.1.1.1.1.1.cmml"><mi id="S3.p2.7.m7.1.1.1.1.1.2" xref="S3.p2.7.m7.1.1.1.1.1.2.cmml">s</mi><mi id="S3.p2.7.m7.1.1.1.1.1.3" xref="S3.p2.7.m7.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p2.7.m7.3.3.3.3.5" xref="S3.p2.7.m7.3.3.3.4.cmml">,</mo><msub id="S3.p2.7.m7.2.2.2.2.2" xref="S3.p2.7.m7.2.2.2.2.2.cmml"><mi id="S3.p2.7.m7.2.2.2.2.2.2" xref="S3.p2.7.m7.2.2.2.2.2.2.cmml">e</mi><mi id="S3.p2.7.m7.2.2.2.2.2.3" xref="S3.p2.7.m7.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.p2.7.m7.3.3.3.3.6" xref="S3.p2.7.m7.3.3.3.4.cmml">,</mo><msub id="S3.p2.7.m7.3.3.3.3.3" xref="S3.p2.7.m7.3.3.3.3.3.cmml"><mi id="S3.p2.7.m7.3.3.3.3.3.2" xref="S3.p2.7.m7.3.3.3.3.3.2.cmml">c</mi><mi id="S3.p2.7.m7.3.3.3.3.3.3" xref="S3.p2.7.m7.3.3.3.3.3.3.cmml">i</mi></msub><mo stretchy="false" id="S3.p2.7.m7.3.3.3.3.7" xref="S3.p2.7.m7.3.3.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.3b"><apply id="S3.p2.7.m7.3.3.cmml" xref="S3.p2.7.m7.3.3"><eq id="S3.p2.7.m7.3.3.4.cmml" xref="S3.p2.7.m7.3.3.4"></eq><apply id="S3.p2.7.m7.3.3.5.cmml" xref="S3.p2.7.m7.3.3.5"><csymbol cd="ambiguous" id="S3.p2.7.m7.3.3.5.1.cmml" xref="S3.p2.7.m7.3.3.5">subscript</csymbol><ci id="S3.p2.7.m7.3.3.5.2.cmml" xref="S3.p2.7.m7.3.3.5.2">𝑦</ci><ci id="S3.p2.7.m7.3.3.5.3.cmml" xref="S3.p2.7.m7.3.3.5.3">𝑖</ci></apply><vector id="S3.p2.7.m7.3.3.3.4.cmml" xref="S3.p2.7.m7.3.3.3.3"><apply id="S3.p2.7.m7.1.1.1.1.1.cmml" xref="S3.p2.7.m7.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.7.m7.1.1.1.1.1.1.cmml" xref="S3.p2.7.m7.1.1.1.1.1">subscript</csymbol><ci id="S3.p2.7.m7.1.1.1.1.1.2.cmml" xref="S3.p2.7.m7.1.1.1.1.1.2">𝑠</ci><ci id="S3.p2.7.m7.1.1.1.1.1.3.cmml" xref="S3.p2.7.m7.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.p2.7.m7.2.2.2.2.2.cmml" xref="S3.p2.7.m7.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p2.7.m7.2.2.2.2.2.1.cmml" xref="S3.p2.7.m7.2.2.2.2.2">subscript</csymbol><ci id="S3.p2.7.m7.2.2.2.2.2.2.cmml" xref="S3.p2.7.m7.2.2.2.2.2.2">𝑒</ci><ci id="S3.p2.7.m7.2.2.2.2.2.3.cmml" xref="S3.p2.7.m7.2.2.2.2.2.3">𝑖</ci></apply><apply id="S3.p2.7.m7.3.3.3.3.3.cmml" xref="S3.p2.7.m7.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.p2.7.m7.3.3.3.3.3.1.cmml" xref="S3.p2.7.m7.3.3.3.3.3">subscript</csymbol><ci id="S3.p2.7.m7.3.3.3.3.3.2.cmml" xref="S3.p2.7.m7.3.3.3.3.3.2">𝑐</ci><ci id="S3.p2.7.m7.3.3.3.3.3.3.cmml" xref="S3.p2.7.m7.3.3.3.3.3.3">𝑖</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.3c">y_{i}=(s_{i},e_{i},c_{i})</annotation></semantics></math> is defined by its onset <math id="S3.p2.8.m8.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S3.p2.8.m8.1a"><msub id="S3.p2.8.m8.1.1" xref="S3.p2.8.m8.1.1.cmml"><mi id="S3.p2.8.m8.1.1.2" xref="S3.p2.8.m8.1.1.2.cmml">s</mi><mi id="S3.p2.8.m8.1.1.3" xref="S3.p2.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.8.m8.1b"><apply id="S3.p2.8.m8.1.1.cmml" xref="S3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.1.cmml" xref="S3.p2.8.m8.1.1">subscript</csymbol><ci id="S3.p2.8.m8.1.1.2.cmml" xref="S3.p2.8.m8.1.1.2">𝑠</ci><ci id="S3.p2.8.m8.1.1.3.cmml" xref="S3.p2.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.8.m8.1c">s_{i}</annotation></semantics></math>, offset <math id="S3.p2.9.m9.1" class="ltx_Math" alttext="e_{i}" display="inline"><semantics id="S3.p2.9.m9.1a"><msub id="S3.p2.9.m9.1.1" xref="S3.p2.9.m9.1.1.cmml"><mi id="S3.p2.9.m9.1.1.2" xref="S3.p2.9.m9.1.1.2.cmml">e</mi><mi id="S3.p2.9.m9.1.1.3" xref="S3.p2.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.9.m9.1b"><apply id="S3.p2.9.m9.1.1.cmml" xref="S3.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p2.9.m9.1.1.1.cmml" xref="S3.p2.9.m9.1.1">subscript</csymbol><ci id="S3.p2.9.m9.1.1.2.cmml" xref="S3.p2.9.m9.1.1.2">𝑒</ci><ci id="S3.p2.9.m9.1.1.3.cmml" xref="S3.p2.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.9.m9.1c">e_{i}</annotation></semantics></math> and category <math id="S3.p2.10.m10.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S3.p2.10.m10.1a"><msub id="S3.p2.10.m10.1.1" xref="S3.p2.10.m10.1.1.cmml"><mi id="S3.p2.10.m10.1.1.2" xref="S3.p2.10.m10.1.1.2.cmml">c</mi><mi id="S3.p2.10.m10.1.1.3" xref="S3.p2.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.10.m10.1b"><apply id="S3.p2.10.m10.1.1.cmml" xref="S3.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.p2.10.m10.1.1.1.cmml" xref="S3.p2.10.m10.1.1">subscript</csymbol><ci id="S3.p2.10.m10.1.1.2.cmml" xref="S3.p2.10.m10.1.1.2">𝑐</ci><ci id="S3.p2.10.m10.1.1.3.cmml" xref="S3.p2.10.m10.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.10.m10.1c">c_{i}</annotation></semantics></math>, where <math id="S3.p2.11.m11.4" class="ltx_Math" alttext="s_{i},e_{i}\in[1,T]" display="inline"><semantics id="S3.p2.11.m11.4a"><mrow id="S3.p2.11.m11.4.4" xref="S3.p2.11.m11.4.4.cmml"><mrow id="S3.p2.11.m11.4.4.2.2" xref="S3.p2.11.m11.4.4.2.3.cmml"><msub id="S3.p2.11.m11.3.3.1.1.1" xref="S3.p2.11.m11.3.3.1.1.1.cmml"><mi id="S3.p2.11.m11.3.3.1.1.1.2" xref="S3.p2.11.m11.3.3.1.1.1.2.cmml">s</mi><mi id="S3.p2.11.m11.3.3.1.1.1.3" xref="S3.p2.11.m11.3.3.1.1.1.3.cmml">i</mi></msub><mo id="S3.p2.11.m11.4.4.2.2.3" xref="S3.p2.11.m11.4.4.2.3.cmml">,</mo><msub id="S3.p2.11.m11.4.4.2.2.2" xref="S3.p2.11.m11.4.4.2.2.2.cmml"><mi id="S3.p2.11.m11.4.4.2.2.2.2" xref="S3.p2.11.m11.4.4.2.2.2.2.cmml">e</mi><mi id="S3.p2.11.m11.4.4.2.2.2.3" xref="S3.p2.11.m11.4.4.2.2.2.3.cmml">i</mi></msub></mrow><mo id="S3.p2.11.m11.4.4.3" xref="S3.p2.11.m11.4.4.3.cmml">∈</mo><mrow id="S3.p2.11.m11.4.4.4.2" xref="S3.p2.11.m11.4.4.4.1.cmml"><mo stretchy="false" id="S3.p2.11.m11.4.4.4.2.1" xref="S3.p2.11.m11.4.4.4.1.cmml">[</mo><mn id="S3.p2.11.m11.1.1" xref="S3.p2.11.m11.1.1.cmml">1</mn><mo id="S3.p2.11.m11.4.4.4.2.2" xref="S3.p2.11.m11.4.4.4.1.cmml">,</mo><mi id="S3.p2.11.m11.2.2" xref="S3.p2.11.m11.2.2.cmml">T</mi><mo stretchy="false" id="S3.p2.11.m11.4.4.4.2.3" xref="S3.p2.11.m11.4.4.4.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.11.m11.4b"><apply id="S3.p2.11.m11.4.4.cmml" xref="S3.p2.11.m11.4.4"><in id="S3.p2.11.m11.4.4.3.cmml" xref="S3.p2.11.m11.4.4.3"></in><list id="S3.p2.11.m11.4.4.2.3.cmml" xref="S3.p2.11.m11.4.4.2.2"><apply id="S3.p2.11.m11.3.3.1.1.1.cmml" xref="S3.p2.11.m11.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.p2.11.m11.3.3.1.1.1.1.cmml" xref="S3.p2.11.m11.3.3.1.1.1">subscript</csymbol><ci id="S3.p2.11.m11.3.3.1.1.1.2.cmml" xref="S3.p2.11.m11.3.3.1.1.1.2">𝑠</ci><ci id="S3.p2.11.m11.3.3.1.1.1.3.cmml" xref="S3.p2.11.m11.3.3.1.1.1.3">𝑖</ci></apply><apply id="S3.p2.11.m11.4.4.2.2.2.cmml" xref="S3.p2.11.m11.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.p2.11.m11.4.4.2.2.2.1.cmml" xref="S3.p2.11.m11.4.4.2.2.2">subscript</csymbol><ci id="S3.p2.11.m11.4.4.2.2.2.2.cmml" xref="S3.p2.11.m11.4.4.2.2.2.2">𝑒</ci><ci id="S3.p2.11.m11.4.4.2.2.2.3.cmml" xref="S3.p2.11.m11.4.4.2.2.2.3">𝑖</ci></apply></list><interval closure="closed" id="S3.p2.11.m11.4.4.4.1.cmml" xref="S3.p2.11.m11.4.4.4.2"><cn type="integer" id="S3.p2.11.m11.1.1.cmml" xref="S3.p2.11.m11.1.1">1</cn><ci id="S3.p2.11.m11.2.2.cmml" xref="S3.p2.11.m11.2.2">𝑇</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.11.m11.4c">s_{i},e_{i}\in[1,T]</annotation></semantics></math>, <math id="S3.p2.12.m12.3" class="ltx_Math" alttext="c_{i}\in\{1,\dots,C\}" display="inline"><semantics id="S3.p2.12.m12.3a"><mrow id="S3.p2.12.m12.3.4" xref="S3.p2.12.m12.3.4.cmml"><msub id="S3.p2.12.m12.3.4.2" xref="S3.p2.12.m12.3.4.2.cmml"><mi id="S3.p2.12.m12.3.4.2.2" xref="S3.p2.12.m12.3.4.2.2.cmml">c</mi><mi id="S3.p2.12.m12.3.4.2.3" xref="S3.p2.12.m12.3.4.2.3.cmml">i</mi></msub><mo id="S3.p2.12.m12.3.4.1" xref="S3.p2.12.m12.3.4.1.cmml">∈</mo><mrow id="S3.p2.12.m12.3.4.3.2" xref="S3.p2.12.m12.3.4.3.1.cmml"><mo stretchy="false" id="S3.p2.12.m12.3.4.3.2.1" xref="S3.p2.12.m12.3.4.3.1.cmml">{</mo><mn id="S3.p2.12.m12.1.1" xref="S3.p2.12.m12.1.1.cmml">1</mn><mo id="S3.p2.12.m12.3.4.3.2.2" xref="S3.p2.12.m12.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.p2.12.m12.2.2" xref="S3.p2.12.m12.2.2.cmml">…</mi><mo id="S3.p2.12.m12.3.4.3.2.3" xref="S3.p2.12.m12.3.4.3.1.cmml">,</mo><mi id="S3.p2.12.m12.3.3" xref="S3.p2.12.m12.3.3.cmml">C</mi><mo stretchy="false" id="S3.p2.12.m12.3.4.3.2.4" xref="S3.p2.12.m12.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.12.m12.3b"><apply id="S3.p2.12.m12.3.4.cmml" xref="S3.p2.12.m12.3.4"><in id="S3.p2.12.m12.3.4.1.cmml" xref="S3.p2.12.m12.3.4.1"></in><apply id="S3.p2.12.m12.3.4.2.cmml" xref="S3.p2.12.m12.3.4.2"><csymbol cd="ambiguous" id="S3.p2.12.m12.3.4.2.1.cmml" xref="S3.p2.12.m12.3.4.2">subscript</csymbol><ci id="S3.p2.12.m12.3.4.2.2.cmml" xref="S3.p2.12.m12.3.4.2.2">𝑐</ci><ci id="S3.p2.12.m12.3.4.2.3.cmml" xref="S3.p2.12.m12.3.4.2.3">𝑖</ci></apply><set id="S3.p2.12.m12.3.4.3.1.cmml" xref="S3.p2.12.m12.3.4.3.2"><cn type="integer" id="S3.p2.12.m12.1.1.cmml" xref="S3.p2.12.m12.1.1">1</cn><ci id="S3.p2.12.m12.2.2.cmml" xref="S3.p2.12.m12.2.2">…</ci><ci id="S3.p2.12.m12.3.3.cmml" xref="S3.p2.12.m12.3.3">𝐶</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.12.m12.3c">c_{i}\in\{1,\dots,C\}</annotation></semantics></math> is the pre-defined category.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Unified Audio-Visual Encoding</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.3" class="ltx_p"><span id="S3.SS1.p1.3.1" class="ltx_text ltx_font_bold">Audio and visual representations.</span>
In order to unify representations of different data to minimize data <span id="S3.SS1.p1.3.2" class="ltx_text" style="color:#000000;">discrepancies</span>,
we use a general model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> pre-trained by aligning vision, audio and language modalities to extract video representations.
Specifically, the visual and audio encoders are utilized to tokenize visual and audio snippets of a given input video, respectively.
Since the visual encoder just learned from image data while temporal information in videos is essential for TAL and AVEL tasks, the visual encoder fine-tuned on Kinetics-400 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is also considered.
After tokenization, the visual <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="E_{V}=\{e_{t}^{v}\}_{t=1}^{T}\in\mathbb{R}^{T\times D}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><msub id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">E</mi><mi id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml">V</mi></msub><mo id="S3.SS1.p1.1.m1.1.1.4" xref="S3.SS1.p1.1.m1.1.1.4.cmml">=</mo><msubsup id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.cmml">e</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.3.cmml">t</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml">v</mi></msubsup><mo stretchy="false" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p1.1.m1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p1.1.m1.1.1.1.1.3.1" xref="S3.SS1.p1.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p1.1.m1.1.1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p1.1.m1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.3.cmml">T</mi></msubsup><mo id="S3.SS1.p1.1.m1.1.1.5" xref="S3.SS1.p1.1.m1.1.1.5.cmml">∈</mo><msup id="S3.SS1.p1.1.m1.1.1.6" xref="S3.SS1.p1.1.m1.1.1.6.cmml"><mi id="S3.SS1.p1.1.m1.1.1.6.2" xref="S3.SS1.p1.1.m1.1.1.6.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.1.m1.1.1.6.3" xref="S3.SS1.p1.1.m1.1.1.6.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.6.3.2" xref="S3.SS1.p1.1.m1.1.1.6.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.6.3.1" xref="S3.SS1.p1.1.m1.1.1.6.3.1.cmml">×</mo><mi id="S3.SS1.p1.1.m1.1.1.6.3.3" xref="S3.SS1.p1.1.m1.1.1.6.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><and id="S3.SS1.p1.1.m1.1.1a.cmml" xref="S3.SS1.p1.1.m1.1.1"></and><apply id="S3.SS1.p1.1.m1.1.1b.cmml" xref="S3.SS1.p1.1.m1.1.1"><eq id="S3.SS1.p1.1.m1.1.1.4.cmml" xref="S3.SS1.p1.1.m1.1.1.4"></eq><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">𝐸</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3">𝑉</ci></apply><apply id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1">subscript</csymbol><set id="S3.SS1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1"><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.2">𝑒</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3">𝑣</ci></apply></set><apply id="S3.SS1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3"><eq id="S3.SS1.p1.1.m1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3.1"></eq><ci id="S3.SS1.p1.1.m1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3.2">𝑡</ci><cn type="integer" id="S3.SS1.p1.1.m1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p1.1.m1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.3">𝑇</ci></apply></apply><apply id="S3.SS1.p1.1.m1.1.1c.cmml" xref="S3.SS1.p1.1.m1.1.1"><in id="S3.SS1.p1.1.m1.1.1.5.cmml" xref="S3.SS1.p1.1.m1.1.1.5"></in><share href="#S3.SS1.p1.1.m1.1.1.1.cmml" id="S3.SS1.p1.1.m1.1.1d.cmml" xref="S3.SS1.p1.1.m1.1.1"></share><apply id="S3.SS1.p1.1.m1.1.1.6.cmml" xref="S3.SS1.p1.1.m1.1.1.6"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.6.1.cmml" xref="S3.SS1.p1.1.m1.1.1.6">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.6.2.cmml" xref="S3.SS1.p1.1.m1.1.1.6.2">ℝ</ci><apply id="S3.SS1.p1.1.m1.1.1.6.3.cmml" xref="S3.SS1.p1.1.m1.1.1.6.3"><times id="S3.SS1.p1.1.m1.1.1.6.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.6.3.1"></times><ci id="S3.SS1.p1.1.m1.1.1.6.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.6.3.2">𝑇</ci><ci id="S3.SS1.p1.1.m1.1.1.6.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.6.3.3">𝐷</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">E_{V}=\{e_{t}^{v}\}_{t=1}^{T}\in\mathbb{R}^{T\times D}</annotation></semantics></math> and audio <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="E_{A}=\{e_{t}^{a}\}_{t=1}^{T}\in\mathbb{R}^{T\times D}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><msub id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.1.1.3.2" xref="S3.SS1.p1.2.m2.1.1.3.2.cmml">E</mi><mi id="S3.SS1.p1.2.m2.1.1.3.3" xref="S3.SS1.p1.2.m2.1.1.3.3.cmml">A</mi></msub><mo id="S3.SS1.p1.2.m2.1.1.4" xref="S3.SS1.p1.2.m2.1.1.4.cmml">=</mo><msubsup id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml"><mrow id="S3.SS1.p1.2.m2.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.2.m2.1.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.1.1.1.1.1.2.cmml">{</mo><msubsup id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.2" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.2.cmml">e</mi><mi id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.3" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.3.cmml">t</mi><mi id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3.cmml">a</mi></msubsup><mo stretchy="false" id="S3.SS1.p1.2.m2.1.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p1.2.m2.1.1.1.1.3" xref="S3.SS1.p1.2.m2.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.1.1.1.1.3.2" xref="S3.SS1.p1.2.m2.1.1.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p1.2.m2.1.1.1.1.3.1" xref="S3.SS1.p1.2.m2.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p1.2.m2.1.1.1.1.3.3" xref="S3.SS1.p1.2.m2.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p1.2.m2.1.1.1.3" xref="S3.SS1.p1.2.m2.1.1.1.3.cmml">T</mi></msubsup><mo id="S3.SS1.p1.2.m2.1.1.5" xref="S3.SS1.p1.2.m2.1.1.5.cmml">∈</mo><msup id="S3.SS1.p1.2.m2.1.1.6" xref="S3.SS1.p1.2.m2.1.1.6.cmml"><mi id="S3.SS1.p1.2.m2.1.1.6.2" xref="S3.SS1.p1.2.m2.1.1.6.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.2.m2.1.1.6.3" xref="S3.SS1.p1.2.m2.1.1.6.3.cmml"><mi id="S3.SS1.p1.2.m2.1.1.6.3.2" xref="S3.SS1.p1.2.m2.1.1.6.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.2.m2.1.1.6.3.1" xref="S3.SS1.p1.2.m2.1.1.6.3.1.cmml">×</mo><mi id="S3.SS1.p1.2.m2.1.1.6.3.3" xref="S3.SS1.p1.2.m2.1.1.6.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><and id="S3.SS1.p1.2.m2.1.1a.cmml" xref="S3.SS1.p1.2.m2.1.1"></and><apply id="S3.SS1.p1.2.m2.1.1b.cmml" xref="S3.SS1.p1.2.m2.1.1"><eq id="S3.SS1.p1.2.m2.1.1.4.cmml" xref="S3.SS1.p1.2.m2.1.1.4"></eq><apply id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2">𝐸</ci><ci id="S3.SS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3">𝐴</ci></apply><apply id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.2.m2.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1">subscript</csymbol><set id="S3.SS1.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1"><apply id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.2">𝑒</ci><ci id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><ci id="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1.1.1.3">𝑎</ci></apply></set><apply id="S3.SS1.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.3"><eq id="S3.SS1.p1.2.m2.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.3.1"></eq><ci id="S3.SS1.p1.2.m2.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.3.2">𝑡</ci><cn type="integer" id="S3.SS1.p1.2.m2.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p1.2.m2.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.1.3">𝑇</ci></apply></apply><apply id="S3.SS1.p1.2.m2.1.1c.cmml" xref="S3.SS1.p1.2.m2.1.1"><in id="S3.SS1.p1.2.m2.1.1.5.cmml" xref="S3.SS1.p1.2.m2.1.1.5"></in><share href="#S3.SS1.p1.2.m2.1.1.1.cmml" id="S3.SS1.p1.2.m2.1.1d.cmml" xref="S3.SS1.p1.2.m2.1.1"></share><apply id="S3.SS1.p1.2.m2.1.1.6.cmml" xref="S3.SS1.p1.2.m2.1.1.6"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.6.1.cmml" xref="S3.SS1.p1.2.m2.1.1.6">superscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.6.2.cmml" xref="S3.SS1.p1.2.m2.1.1.6.2">ℝ</ci><apply id="S3.SS1.p1.2.m2.1.1.6.3.cmml" xref="S3.SS1.p1.2.m2.1.1.6.3"><times id="S3.SS1.p1.2.m2.1.1.6.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.6.3.1"></times><ci id="S3.SS1.p1.2.m2.1.1.6.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.6.3.2">𝑇</ci><ci id="S3.SS1.p1.2.m2.1.1.6.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.6.3.3">𝐷</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">E_{A}=\{e_{t}^{a}\}_{t=1}^{T}\in\mathbb{R}^{T\times D}</annotation></semantics></math> feature sequences can be obtained, where <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">D</annotation></semantics></math> is the projected feature dimension.
Note that the parameters of the visual and audio encoders are frozen during this process.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.7" class="ltx_p"><span id="S3.SS1.p2.7.1" class="ltx_text ltx_font_bold">Audio-visual pyramid transformer.</span>
Audio and visual signals are equally crucial for AVEL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, and both serve as important complementary information for TAL and SED to boost the performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
Therefore,
we uniformly feed the video data with both modalities of all tasks into a single backbone, which is implemented by an audio-visual pyramid transformer inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
Specifically, the tokenized visual <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="E_{V}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">E</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝐸</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">E_{V}</annotation></semantics></math> and audio <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="E_{A}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">E</mi><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝐸</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">E_{A}</annotation></semantics></math> sequences added with its position embeddings first pass <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">L</mi><mn id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">𝐿</ci><cn type="integer" id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">L_{1}</annotation></semantics></math> transformer blocks separately for uni-modal long-form temporal modeling.
In untrimmed videos, the occurring instances usually have various lengths. Thus, to detect them at multiple temporal resolutions, the obtained features from two modalities are fed into an audio-visual pyramid fusion module, which consists of <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">L</mi><mn id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝐿</ci><cn type="integer" id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">L_{2}</annotation></semantics></math> transformer blocks.
In each block, 2x downsampling using strided convolutions is first applied, and then audio-visual cross-attention is conducted by assigning the current modality as the key and value vectors while another as the query vector.
Afterward, the enhanced features from different modalities in each pyramid level <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">l</annotation></semantics></math> are concatenated to get an audio-visual feature pyramid <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="Z=\{Z^{l}\}_{l=1}^{L_{2}}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mrow id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">Z</mi><mo id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">=</mo><msubsup id="S3.SS1.p2.6.m6.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.cmml"><mrow id="S3.SS1.p2.6.m6.1.1.1.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.6.m6.1.1.1.1.1.1.2" xref="S3.SS1.p2.6.m6.1.1.1.1.1.2.cmml">{</mo><msup id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.2" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.2.cmml">Z</mi><mi id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.3" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.3.cmml">l</mi></msup><mo stretchy="false" id="S3.SS1.p2.6.m6.1.1.1.1.1.1.3" xref="S3.SS1.p2.6.m6.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p2.6.m6.1.1.1.1.3" xref="S3.SS1.p2.6.m6.1.1.1.1.3.cmml"><mi id="S3.SS1.p2.6.m6.1.1.1.1.3.2" xref="S3.SS1.p2.6.m6.1.1.1.1.3.2.cmml">l</mi><mo id="S3.SS1.p2.6.m6.1.1.1.1.3.1" xref="S3.SS1.p2.6.m6.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p2.6.m6.1.1.1.1.3.3" xref="S3.SS1.p2.6.m6.1.1.1.1.3.3.cmml">1</mn></mrow><msub id="S3.SS1.p2.6.m6.1.1.1.3" xref="S3.SS1.p2.6.m6.1.1.1.3.cmml"><mi id="S3.SS1.p2.6.m6.1.1.1.3.2" xref="S3.SS1.p2.6.m6.1.1.1.3.2.cmml">L</mi><mn id="S3.SS1.p2.6.m6.1.1.1.3.3" xref="S3.SS1.p2.6.m6.1.1.1.3.3.cmml">2</mn></msub></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><eq id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2"></eq><ci id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">𝑍</ci><apply id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.1">superscript</csymbol><apply id="S3.SS1.p2.6.m6.1.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.1">subscript</csymbol><set id="S3.SS1.p2.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1"><apply id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.2">𝑍</ci><ci id="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.1.1.1.3">𝑙</ci></apply></set><apply id="S3.SS1.p2.6.m6.1.1.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.3"><eq id="S3.SS1.p2.6.m6.1.1.1.1.3.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.3.1"></eq><ci id="S3.SS1.p2.6.m6.1.1.1.1.3.2.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.3.2">𝑙</ci><cn type="integer" id="S3.SS1.p2.6.m6.1.1.1.1.3.3.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.SS1.p2.6.m6.1.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.3.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.1.3.2.cmml" xref="S3.SS1.p2.6.m6.1.1.1.3.2">𝐿</ci><cn type="integer" id="S3.SS1.p2.6.m6.1.1.1.3.3.cmml" xref="S3.SS1.p2.6.m6.1.1.1.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">Z=\{Z^{l}\}_{l=1}^{L_{2}}</annotation></semantics></math>, where <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="Z^{l}\in\mathbb{R}^{T_{l}\times 2D}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mrow id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><msup id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2.2" xref="S3.SS1.p2.7.m7.1.1.2.2.cmml">Z</mi><mi id="S3.SS1.p2.7.m7.1.1.2.3" xref="S3.SS1.p2.7.m7.1.1.2.3.cmml">l</mi></msup><mo id="S3.SS1.p2.7.m7.1.1.1" xref="S3.SS1.p2.7.m7.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.2" xref="S3.SS1.p2.7.m7.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.7.m7.1.1.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.cmml"><mrow id="S3.SS1.p2.7.m7.1.1.3.3.2" xref="S3.SS1.p2.7.m7.1.1.3.3.2.cmml"><msub id="S3.SS1.p2.7.m7.1.1.3.3.2.2" xref="S3.SS1.p2.7.m7.1.1.3.3.2.2.cmml"><mi id="S3.SS1.p2.7.m7.1.1.3.3.2.2.2" xref="S3.SS1.p2.7.m7.1.1.3.3.2.2.2.cmml">T</mi><mi id="S3.SS1.p2.7.m7.1.1.3.3.2.2.3" xref="S3.SS1.p2.7.m7.1.1.3.3.2.2.3.cmml">l</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.7.m7.1.1.3.3.2.1" xref="S3.SS1.p2.7.m7.1.1.3.3.2.1.cmml">×</mo><mn id="S3.SS1.p2.7.m7.1.1.3.3.2.3" xref="S3.SS1.p2.7.m7.1.1.3.3.2.3.cmml">2</mn></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p2.7.m7.1.1.3.3.1" xref="S3.SS1.p2.7.m7.1.1.3.3.1.cmml">​</mo><mi id="S3.SS1.p2.7.m7.1.1.3.3.3" xref="S3.SS1.p2.7.m7.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><in id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1.1"></in><apply id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.2.1.cmml" xref="S3.SS1.p2.7.m7.1.1.2">superscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2.2">𝑍</ci><ci id="S3.SS1.p2.7.m7.1.1.2.3.cmml" xref="S3.SS1.p2.7.m7.1.1.2.3">𝑙</ci></apply><apply id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.7.m7.1.1.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3"><times id="S3.SS1.p2.7.m7.1.1.3.3.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.1"></times><apply id="S3.SS1.p2.7.m7.1.1.3.3.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.2"><times id="S3.SS1.p2.7.m7.1.1.3.3.2.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.2.1"></times><apply id="S3.SS1.p2.7.m7.1.1.3.3.2.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.3.3.2.2.1.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.3.3.2.2.2.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.2.2.2">𝑇</ci><ci id="S3.SS1.p2.7.m7.1.1.3.3.2.2.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.2.2.3">𝑙</ci></apply><cn type="integer" id="S3.SS1.p2.7.m7.1.1.3.3.2.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.2.3">2</cn></apply><ci id="S3.SS1.p2.7.m7.1.1.3.3.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">Z^{l}\in\mathbb{R}^{T_{l}\times 2D}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Task-Specific Experts</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Unifying these three tasks under a single framework is inherently challenging as they focus on instances with different characteristics.
Inspired by mixture-of-experts (MoE) networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, we introduce task-specific experts in our transformer blocks.
Unlike previous MoEs that aim to capture modality-specific information (<em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS2.p1.1.2" class="ltx_text"></span>, vision and language), our experts allow the model to learn distinct knowledge for different tasks.
Specifically, as shown in the right of <a href="#S3.F2" title="In 3 The UniAV Framework ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, the output feature <math id="S3.SS2.p1.1.m1.4" class="ltx_Math" alttext="F_{l-1},l\in[1,L_{1}+L_{2}]" display="inline"><semantics id="S3.SS2.p1.1.m1.4a"><mrow id="S3.SS2.p1.1.m1.4.4" xref="S3.SS2.p1.1.m1.4.4.cmml"><mrow id="S3.SS2.p1.1.m1.3.3.1.1" xref="S3.SS2.p1.1.m1.3.3.1.2.cmml"><msub id="S3.SS2.p1.1.m1.3.3.1.1.1" xref="S3.SS2.p1.1.m1.3.3.1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.3.3.1.1.1.2" xref="S3.SS2.p1.1.m1.3.3.1.1.1.2.cmml">F</mi><mrow id="S3.SS2.p1.1.m1.3.3.1.1.1.3" xref="S3.SS2.p1.1.m1.3.3.1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.3.3.1.1.1.3.2" xref="S3.SS2.p1.1.m1.3.3.1.1.1.3.2.cmml">l</mi><mo id="S3.SS2.p1.1.m1.3.3.1.1.1.3.1" xref="S3.SS2.p1.1.m1.3.3.1.1.1.3.1.cmml">−</mo><mn id="S3.SS2.p1.1.m1.3.3.1.1.1.3.3" xref="S3.SS2.p1.1.m1.3.3.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.SS2.p1.1.m1.3.3.1.1.2" xref="S3.SS2.p1.1.m1.3.3.1.2.cmml">,</mo><mi id="S3.SS2.p1.1.m1.2.2" xref="S3.SS2.p1.1.m1.2.2.cmml">l</mi></mrow><mo id="S3.SS2.p1.1.m1.4.4.3" xref="S3.SS2.p1.1.m1.4.4.3.cmml">∈</mo><mrow id="S3.SS2.p1.1.m1.4.4.2.1" xref="S3.SS2.p1.1.m1.4.4.2.2.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.4.4.2.1.2" xref="S3.SS2.p1.1.m1.4.4.2.2.cmml">[</mo><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">1</mn><mo id="S3.SS2.p1.1.m1.4.4.2.1.3" xref="S3.SS2.p1.1.m1.4.4.2.2.cmml">,</mo><mrow id="S3.SS2.p1.1.m1.4.4.2.1.1" xref="S3.SS2.p1.1.m1.4.4.2.1.1.cmml"><msub id="S3.SS2.p1.1.m1.4.4.2.1.1.2" xref="S3.SS2.p1.1.m1.4.4.2.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.4.4.2.1.1.2.2" xref="S3.SS2.p1.1.m1.4.4.2.1.1.2.2.cmml">L</mi><mn id="S3.SS2.p1.1.m1.4.4.2.1.1.2.3" xref="S3.SS2.p1.1.m1.4.4.2.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS2.p1.1.m1.4.4.2.1.1.1" xref="S3.SS2.p1.1.m1.4.4.2.1.1.1.cmml">+</mo><msub id="S3.SS2.p1.1.m1.4.4.2.1.1.3" xref="S3.SS2.p1.1.m1.4.4.2.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.4.4.2.1.1.3.2" xref="S3.SS2.p1.1.m1.4.4.2.1.1.3.2.cmml">L</mi><mn id="S3.SS2.p1.1.m1.4.4.2.1.1.3.3" xref="S3.SS2.p1.1.m1.4.4.2.1.1.3.3.cmml">2</mn></msub></mrow><mo stretchy="false" id="S3.SS2.p1.1.m1.4.4.2.1.4" xref="S3.SS2.p1.1.m1.4.4.2.2.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.4b"><apply id="S3.SS2.p1.1.m1.4.4.cmml" xref="S3.SS2.p1.1.m1.4.4"><in id="S3.SS2.p1.1.m1.4.4.3.cmml" xref="S3.SS2.p1.1.m1.4.4.3"></in><list id="S3.SS2.p1.1.m1.3.3.1.2.cmml" xref="S3.SS2.p1.1.m1.3.3.1.1"><apply id="S3.SS2.p1.1.m1.3.3.1.1.1.cmml" xref="S3.SS2.p1.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.3.3.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.3.3.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.3.3.1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.3.3.1.1.1.2">𝐹</ci><apply id="S3.SS2.p1.1.m1.3.3.1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.3.3.1.1.1.3"><minus id="S3.SS2.p1.1.m1.3.3.1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.3.3.1.1.1.3.1"></minus><ci id="S3.SS2.p1.1.m1.3.3.1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.3.3.1.1.1.3.2">𝑙</ci><cn type="integer" id="S3.SS2.p1.1.m1.3.3.1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.3.3.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.p1.1.m1.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2">𝑙</ci></list><interval closure="closed" id="S3.SS2.p1.1.m1.4.4.2.2.cmml" xref="S3.SS2.p1.1.m1.4.4.2.1"><cn type="integer" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">1</cn><apply id="S3.SS2.p1.1.m1.4.4.2.1.1.cmml" xref="S3.SS2.p1.1.m1.4.4.2.1.1"><plus id="S3.SS2.p1.1.m1.4.4.2.1.1.1.cmml" xref="S3.SS2.p1.1.m1.4.4.2.1.1.1"></plus><apply id="S3.SS2.p1.1.m1.4.4.2.1.1.2.cmml" xref="S3.SS2.p1.1.m1.4.4.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.4.4.2.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.4.4.2.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.4.4.2.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.4.4.2.1.1.2.2">𝐿</ci><cn type="integer" id="S3.SS2.p1.1.m1.4.4.2.1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.4.4.2.1.1.2.3">1</cn></apply><apply id="S3.SS2.p1.1.m1.4.4.2.1.1.3.cmml" xref="S3.SS2.p1.1.m1.4.4.2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.4.4.2.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.4.4.2.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.1.m1.4.4.2.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.4.4.2.1.1.3.2">𝐿</ci><cn type="integer" id="S3.SS2.p1.1.m1.4.4.2.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.4.4.2.1.1.3.3">2</cn></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.4c">F_{l-1},l\in[1,L_{1}+L_{2}]</annotation></semantics></math> from a previous block first passes a shared multi-head attention (MHA) to align information from different tasks.
Then, the task-specific information can be captured by switching to different task experts.
The task experts compose a Multiway feed-forward network (FFN), where each expert is a standard FFN consisting of two linear layers and an activation.</p>
<table id="Pt0.A3.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle F_{l}^{{}^{\prime}}=\mathrm{MHA}(\mathrm{LN}(F_{l-1})+F_{l-1}),\;" display="inline"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msubsup id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml">F</mi><mi id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3.cmml">l</mi><msup id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3a" xref="S3.E1.m1.1.1.1.1.3.3.cmml"></mi><mo id="S3.E1.m1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">′</mo></msup></msubsup><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">MHA</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">LN</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">F</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">l</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">+</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml">F</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">l</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">−</mo><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">𝐹</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3">𝑙</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><ci id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1">′</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">MHA</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2"></plus><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3">LN</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝐹</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></minus><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑙</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2">𝐹</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1"></minus><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2">𝑙</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle F_{l}^{{}^{\prime}}=\mathrm{MHA}(\mathrm{LN}(F_{l-1})+F_{l-1}),\;</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle F_{l}=\mathrm{Multiway\text{-}FFN}(\mathrm{LN}(F_{l}^{{}^{\prime}}))+F_{l}^{{}^{\prime}}," display="inline"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">F</mi><mi id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml">l</mi></msub><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml">Multiway</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">​</mo><mtext id="S3.E2.m1.1.1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.1.1.4a.cmml">-</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2a" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.1.1.5.cmml">FFN</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2b" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml">LN</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">F</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">l</mi><msup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"></mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">′</mo></msup></msubsup><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">+</mo><msubsup id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.1.3.2.2.cmml">F</mi><mi id="S3.E2.m1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.1.3.2.3.cmml">l</mi><msup id="S3.E2.m1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.3.3a" xref="S3.E2.m1.1.1.1.1.1.3.3.cmml"></mi><mo id="S3.E2.m1.1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.1.3.3.1.cmml">′</mo></msup></msubsup></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></eq><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">𝐹</ci><ci id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3">𝑙</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><plus id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></plus><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">Multiway</ci><ci id="S3.E2.m1.1.1.1.1.1.1.4a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.4"><mtext id="S3.E2.m1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.4">-</mtext></ci><ci id="S3.E2.m1.1.1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.5">FFN</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">LN</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝐹</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑙</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3"><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1">′</ci></apply></apply></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2.2">𝐹</ci><ci id="S3.E2.m1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2.3">𝑙</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3"><ci id="S3.E2.m1.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3.1">′</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle F_{l}=\mathrm{Multiway\text{-}FFN}(\mathrm{LN}(F_{l}^{{}^{\prime}}))+F_{l}^{{}^{\prime}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.2" class="ltx_p">where LN is short for layer normalization. <math id="S3.SS2.p1.2.m1.1" class="ltx_Math" alttext="\mathrm{Multiway\text{-}FFN}" display="inline"><semantics id="S3.SS2.p1.2.m1.1a"><mrow id="S3.SS2.p1.2.m1.1.1" xref="S3.SS2.p1.2.m1.1.1.cmml"><mi id="S3.SS2.p1.2.m1.1.1.2" xref="S3.SS2.p1.2.m1.1.1.2.cmml">Multiway</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m1.1.1.1" xref="S3.SS2.p1.2.m1.1.1.1.cmml">​</mo><mtext id="S3.SS2.p1.2.m1.1.1.3" xref="S3.SS2.p1.2.m1.1.1.3a.cmml">-</mtext><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m1.1.1.1a" xref="S3.SS2.p1.2.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.p1.2.m1.1.1.4" xref="S3.SS2.p1.2.m1.1.1.4.cmml">FFN</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m1.1b"><apply id="S3.SS2.p1.2.m1.1.1.cmml" xref="S3.SS2.p1.2.m1.1.1"><times id="S3.SS2.p1.2.m1.1.1.1.cmml" xref="S3.SS2.p1.2.m1.1.1.1"></times><ci id="S3.SS2.p1.2.m1.1.1.2.cmml" xref="S3.SS2.p1.2.m1.1.1.2">Multiway</ci><ci id="S3.SS2.p1.2.m1.1.1.3a.cmml" xref="S3.SS2.p1.2.m1.1.1.3"><mtext id="S3.SS2.p1.2.m1.1.1.3.cmml" xref="S3.SS2.p1.2.m1.1.1.3">-</mtext></ci><ci id="S3.SS2.p1.2.m1.1.1.4.cmml" xref="S3.SS2.p1.2.m1.1.1.4">FFN</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m1.1c">\mathrm{Multiway\text{-}FFN}</annotation></semantics></math> selects the corresponding task experts based on the input data for each task, <em id="S3.SS2.p1.2.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS2.p1.2.2" class="ltx_text"></span>, if the input video is from the TAL task dataset, we use the TAL expert for data processing.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Head Design</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">After the audio-visual encoding in Sec. <a href="#S3.SS1" title="3.1 Unified Audio-Visual Encoding ‣ 3 The UniAV Framework ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, the output features <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\{Z^{l}\}_{l=1}^{L_{2}}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msubsup id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mrow id="S3.SS3.p1.1.m1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p1.1.m1.1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.2.cmml">{</mo><msup id="S3.SS3.p1.1.m1.1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.cmml">Z</mi><mi id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.cmml">l</mi></msup><mo stretchy="false" id="S3.SS3.p1.1.m1.1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS3.p1.1.m1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.3.2" xref="S3.SS3.p1.1.m1.1.1.1.3.2.cmml">l</mi><mo id="S3.SS3.p1.1.m1.1.1.1.3.1" xref="S3.SS3.p1.1.m1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS3.p1.1.m1.1.1.1.3.3" xref="S3.SS3.p1.1.m1.1.1.1.3.3.cmml">1</mn></mrow><msub id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml"><mi id="S3.SS3.p1.1.m1.1.1.3.2" xref="S3.SS3.p1.1.m1.1.1.3.2.cmml">L</mi><mn id="S3.SS3.p1.1.m1.1.1.3.3" xref="S3.SS3.p1.1.m1.1.1.3.3.cmml">2</mn></msub></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1">superscript</csymbol><apply id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><set id="S3.SS3.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1"><apply id="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2">𝑍</ci><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3">𝑙</ci></apply></set><apply id="S3.SS3.p1.1.m1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3"><eq id="S3.SS3.p1.1.m1.1.1.1.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3.1"></eq><ci id="S3.SS3.p1.1.m1.1.1.1.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3.2">𝑙</ci><cn type="integer" id="S3.SS3.p1.1.m1.1.1.1.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS3.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS3.p1.1.m1.1.1.3.2">𝐿</ci><cn type="integer" id="S3.SS3.p1.1.m1.1.1.3.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\{Z^{l}\}_{l=1}^{L_{2}}</annotation></semantics></math> from the audio-visual feature pyramid
will connect to the classification and regression heads to get localization predictions in a single pass.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.6" class="ltx_p"><span id="S3.SS3.p2.6.1" class="ltx_text ltx_font_bold">Language-aware classification head.</span>
Since the datasets for different tasks have different label vocabularies, it is a straightforward way to use task-specific heads for classification.
However, it results in parameter redundancy and lacks flexibility due to fixed categories.
Here, we propose to unify three task-specific heads into one by taking advantage of large pre-trained models.
In detail, we treat the instance categories of each dataset as text information and encode them using <span id="S3.SS3.p2.6.2" class="ltx_text" style="color:#000000;"> a pre-trained text encoder.</span>
To add contextual information, prompts are also customized to help specify labels from different tasks.
The used prompt templates are “<span id="S3.SS3.p2.6.3" class="ltx_text ltx_font_typewriter">A visual event of {label}.</span>”, “<span id="S3.SS3.p2.6.4" class="ltx_text ltx_font_typewriter">An audio visual event of {label}.</span>”, and “<span id="S3.SS3.p2.6.5" class="ltx_text ltx_font_typewriter">A sound event of {label}.</span>” for TAL, AVEL and SED, respectively.
Note that the text encoder is from the same pre-trained model as the visual and audio encoders in Sec. <a href="#S3.SS1" title="3.1 Unified Audio-Visual Encoding ‣ 3 The UniAV Framework ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
which provides a strong prior on measuring the relevancy between modalities.
The texts are encoded as <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{T}=\{\mathcal{T}_{i}\}_{i=1}^{N_{k}}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">𝒯</mi><mo id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">=</mo><msubsup id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml"><mrow id="S3.SS3.p2.1.m1.1.1.1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p2.1.m1.1.1.1.1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS3.p2.1.m1.1.1.1.1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.1.m1.1.1.1.1.1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.1.2.cmml">𝒯</mi><mi id="S3.SS3.p2.1.m1.1.1.1.1.1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS3.p2.1.m1.1.1.1.1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS3.p2.1.m1.1.1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.1.1.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS3.p2.1.m1.1.1.1.1.3.1" xref="S3.SS3.p2.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS3.p2.1.m1.1.1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><msub id="S3.SS3.p2.1.m1.1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.1.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.1.3.2.cmml">N</mi><mi id="S3.SS3.p2.1.m1.1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.1.3.3.cmml">k</mi></msub></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><eq id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2"></eq><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝒯</ci><apply id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1">superscript</csymbol><apply id="S3.SS3.p2.1.m1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1">subscript</csymbol><set id="S3.SS3.p2.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1"><apply id="S3.SS3.p2.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.1.2">𝒯</ci><ci id="S3.SS3.p2.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S3.SS3.p2.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.3"><eq id="S3.SS3.p2.1.m1.1.1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.3.1"></eq><ci id="S3.SS3.p2.1.m1.1.1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS3.p2.1.m1.1.1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.SS3.p2.1.m1.1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1.3">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.1.3.2">𝑁</ci><ci id="S3.SS3.p2.1.m1.1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.1.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\mathcal{T}=\{\mathcal{T}_{i}\}_{i=1}^{N_{k}}</annotation></semantics></math>, where <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="N_{k}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">N</mi><mi id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">𝑁</ci><ci id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">N_{k}</annotation></semantics></math> is the number of classes of the dataset for the <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi mathcolor="#000000" id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">k</annotation></semantics></math><span id="S3.SS3.p2.6.6" class="ltx_text" style="color:#000000;">-th</span> task.
Then, the encoded texts are linearly projected to a shared embedding space <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><msup id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">D</mi><mo id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">superscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2">𝐷</ci><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">D^{\prime}</annotation></semantics></math> with the features from the audio-visual feature pyramid.
We obtain the normalized text vector <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="\hat{\mathcal{T}}=\{\hat{\mathcal{T}_{i}}\}_{i=1}^{N_{k}}\in\mathbb{R}^{N_{k}\times D^{\prime}}" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mrow id="S3.SS3.p2.5.m5.1.2" xref="S3.SS3.p2.5.m5.1.2.cmml"><mover accent="true" id="S3.SS3.p2.5.m5.1.2.2" xref="S3.SS3.p2.5.m5.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.5.m5.1.2.2.2" xref="S3.SS3.p2.5.m5.1.2.2.2.cmml">𝒯</mi><mo id="S3.SS3.p2.5.m5.1.2.2.1" xref="S3.SS3.p2.5.m5.1.2.2.1.cmml">^</mo></mover><mo id="S3.SS3.p2.5.m5.1.2.3" xref="S3.SS3.p2.5.m5.1.2.3.cmml">=</mo><msubsup id="S3.SS3.p2.5.m5.1.2.4" xref="S3.SS3.p2.5.m5.1.2.4.cmml"><mrow id="S3.SS3.p2.5.m5.1.2.4.2.2.2" xref="S3.SS3.p2.5.m5.1.2.4.2.2.1.cmml"><mo stretchy="false" id="S3.SS3.p2.5.m5.1.2.4.2.2.2.1" xref="S3.SS3.p2.5.m5.1.2.4.2.2.1.cmml">{</mo><mover accent="true" id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><msub id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.5.m5.1.1.2.2" xref="S3.SS3.p2.5.m5.1.1.2.2.cmml">𝒯</mi><mi id="S3.SS3.p2.5.m5.1.1.2.3" xref="S3.SS3.p2.5.m5.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS3.p2.5.m5.1.1.1" xref="S3.SS3.p2.5.m5.1.1.1.cmml">^</mo></mover><mo stretchy="false" id="S3.SS3.p2.5.m5.1.2.4.2.2.2.2" xref="S3.SS3.p2.5.m5.1.2.4.2.2.1.cmml">}</mo></mrow><mrow id="S3.SS3.p2.5.m5.1.2.4.2.3" xref="S3.SS3.p2.5.m5.1.2.4.2.3.cmml"><mi id="S3.SS3.p2.5.m5.1.2.4.2.3.2" xref="S3.SS3.p2.5.m5.1.2.4.2.3.2.cmml">i</mi><mo id="S3.SS3.p2.5.m5.1.2.4.2.3.1" xref="S3.SS3.p2.5.m5.1.2.4.2.3.1.cmml">=</mo><mn id="S3.SS3.p2.5.m5.1.2.4.2.3.3" xref="S3.SS3.p2.5.m5.1.2.4.2.3.3.cmml">1</mn></mrow><msub id="S3.SS3.p2.5.m5.1.2.4.3" xref="S3.SS3.p2.5.m5.1.2.4.3.cmml"><mi id="S3.SS3.p2.5.m5.1.2.4.3.2" xref="S3.SS3.p2.5.m5.1.2.4.3.2.cmml">N</mi><mi id="S3.SS3.p2.5.m5.1.2.4.3.3" xref="S3.SS3.p2.5.m5.1.2.4.3.3.cmml">k</mi></msub></msubsup><mo id="S3.SS3.p2.5.m5.1.2.5" xref="S3.SS3.p2.5.m5.1.2.5.cmml">∈</mo><msup id="S3.SS3.p2.5.m5.1.2.6" xref="S3.SS3.p2.5.m5.1.2.6.cmml"><mi id="S3.SS3.p2.5.m5.1.2.6.2" xref="S3.SS3.p2.5.m5.1.2.6.2.cmml">ℝ</mi><mrow id="S3.SS3.p2.5.m5.1.2.6.3" xref="S3.SS3.p2.5.m5.1.2.6.3.cmml"><msub id="S3.SS3.p2.5.m5.1.2.6.3.2" xref="S3.SS3.p2.5.m5.1.2.6.3.2.cmml"><mi id="S3.SS3.p2.5.m5.1.2.6.3.2.2" xref="S3.SS3.p2.5.m5.1.2.6.3.2.2.cmml">N</mi><mi id="S3.SS3.p2.5.m5.1.2.6.3.2.3" xref="S3.SS3.p2.5.m5.1.2.6.3.2.3.cmml">k</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.5.m5.1.2.6.3.1" xref="S3.SS3.p2.5.m5.1.2.6.3.1.cmml">×</mo><msup id="S3.SS3.p2.5.m5.1.2.6.3.3" xref="S3.SS3.p2.5.m5.1.2.6.3.3.cmml"><mi id="S3.SS3.p2.5.m5.1.2.6.3.3.2" xref="S3.SS3.p2.5.m5.1.2.6.3.3.2.cmml">D</mi><mo id="S3.SS3.p2.5.m5.1.2.6.3.3.3" xref="S3.SS3.p2.5.m5.1.2.6.3.3.3.cmml">′</mo></msup></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.2.cmml" xref="S3.SS3.p2.5.m5.1.2"><and id="S3.SS3.p2.5.m5.1.2a.cmml" xref="S3.SS3.p2.5.m5.1.2"></and><apply id="S3.SS3.p2.5.m5.1.2b.cmml" xref="S3.SS3.p2.5.m5.1.2"><eq id="S3.SS3.p2.5.m5.1.2.3.cmml" xref="S3.SS3.p2.5.m5.1.2.3"></eq><apply id="S3.SS3.p2.5.m5.1.2.2.cmml" xref="S3.SS3.p2.5.m5.1.2.2"><ci id="S3.SS3.p2.5.m5.1.2.2.1.cmml" xref="S3.SS3.p2.5.m5.1.2.2.1">^</ci><ci id="S3.SS3.p2.5.m5.1.2.2.2.cmml" xref="S3.SS3.p2.5.m5.1.2.2.2">𝒯</ci></apply><apply id="S3.SS3.p2.5.m5.1.2.4.cmml" xref="S3.SS3.p2.5.m5.1.2.4"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.2.4.1.cmml" xref="S3.SS3.p2.5.m5.1.2.4">superscript</csymbol><apply id="S3.SS3.p2.5.m5.1.2.4.2.cmml" xref="S3.SS3.p2.5.m5.1.2.4"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.2.4.2.1.cmml" xref="S3.SS3.p2.5.m5.1.2.4">subscript</csymbol><set id="S3.SS3.p2.5.m5.1.2.4.2.2.1.cmml" xref="S3.SS3.p2.5.m5.1.2.4.2.2.2"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><ci id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1.1">^</ci><apply id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.1.2.1.cmml" xref="S3.SS3.p2.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.5.m5.1.1.2.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2.2">𝒯</ci><ci id="S3.SS3.p2.5.m5.1.1.2.3.cmml" xref="S3.SS3.p2.5.m5.1.1.2.3">𝑖</ci></apply></apply></set><apply id="S3.SS3.p2.5.m5.1.2.4.2.3.cmml" xref="S3.SS3.p2.5.m5.1.2.4.2.3"><eq id="S3.SS3.p2.5.m5.1.2.4.2.3.1.cmml" xref="S3.SS3.p2.5.m5.1.2.4.2.3.1"></eq><ci id="S3.SS3.p2.5.m5.1.2.4.2.3.2.cmml" xref="S3.SS3.p2.5.m5.1.2.4.2.3.2">𝑖</ci><cn type="integer" id="S3.SS3.p2.5.m5.1.2.4.2.3.3.cmml" xref="S3.SS3.p2.5.m5.1.2.4.2.3.3">1</cn></apply></apply><apply id="S3.SS3.p2.5.m5.1.2.4.3.cmml" xref="S3.SS3.p2.5.m5.1.2.4.3"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.2.4.3.1.cmml" xref="S3.SS3.p2.5.m5.1.2.4.3">subscript</csymbol><ci id="S3.SS3.p2.5.m5.1.2.4.3.2.cmml" xref="S3.SS3.p2.5.m5.1.2.4.3.2">𝑁</ci><ci id="S3.SS3.p2.5.m5.1.2.4.3.3.cmml" xref="S3.SS3.p2.5.m5.1.2.4.3.3">𝑘</ci></apply></apply></apply><apply id="S3.SS3.p2.5.m5.1.2c.cmml" xref="S3.SS3.p2.5.m5.1.2"><in id="S3.SS3.p2.5.m5.1.2.5.cmml" xref="S3.SS3.p2.5.m5.1.2.5"></in><share href="#S3.SS3.p2.5.m5.1.2.4.cmml" id="S3.SS3.p2.5.m5.1.2d.cmml" xref="S3.SS3.p2.5.m5.1.2"></share><apply id="S3.SS3.p2.5.m5.1.2.6.cmml" xref="S3.SS3.p2.5.m5.1.2.6"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.2.6.1.cmml" xref="S3.SS3.p2.5.m5.1.2.6">superscript</csymbol><ci id="S3.SS3.p2.5.m5.1.2.6.2.cmml" xref="S3.SS3.p2.5.m5.1.2.6.2">ℝ</ci><apply id="S3.SS3.p2.5.m5.1.2.6.3.cmml" xref="S3.SS3.p2.5.m5.1.2.6.3"><times id="S3.SS3.p2.5.m5.1.2.6.3.1.cmml" xref="S3.SS3.p2.5.m5.1.2.6.3.1"></times><apply id="S3.SS3.p2.5.m5.1.2.6.3.2.cmml" xref="S3.SS3.p2.5.m5.1.2.6.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.2.6.3.2.1.cmml" xref="S3.SS3.p2.5.m5.1.2.6.3.2">subscript</csymbol><ci id="S3.SS3.p2.5.m5.1.2.6.3.2.2.cmml" xref="S3.SS3.p2.5.m5.1.2.6.3.2.2">𝑁</ci><ci id="S3.SS3.p2.5.m5.1.2.6.3.2.3.cmml" xref="S3.SS3.p2.5.m5.1.2.6.3.2.3">𝑘</ci></apply><apply id="S3.SS3.p2.5.m5.1.2.6.3.3.cmml" xref="S3.SS3.p2.5.m5.1.2.6.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.2.6.3.3.1.cmml" xref="S3.SS3.p2.5.m5.1.2.6.3.3">superscript</csymbol><ci id="S3.SS3.p2.5.m5.1.2.6.3.3.2.cmml" xref="S3.SS3.p2.5.m5.1.2.6.3.3.2">𝐷</ci><ci id="S3.SS3.p2.5.m5.1.2.6.3.3.3.cmml" xref="S3.SS3.p2.5.m5.1.2.6.3.3.3">′</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">\hat{\mathcal{T}}=\{\hat{\mathcal{T}_{i}}\}_{i=1}^{N_{k}}\in\mathbb{R}^{N_{k}\times D^{\prime}}</annotation></semantics></math> and the normalized cross-modal feature vector
<math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="\hat{Z}^{l}\in\mathbb{R}^{T_{l}\times D^{\prime}}" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><mrow id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><msup id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml"><mover accent="true" id="S3.SS3.p2.6.m6.1.1.2.2" xref="S3.SS3.p2.6.m6.1.1.2.2.cmml"><mi id="S3.SS3.p2.6.m6.1.1.2.2.2" xref="S3.SS3.p2.6.m6.1.1.2.2.2.cmml">Z</mi><mo id="S3.SS3.p2.6.m6.1.1.2.2.1" xref="S3.SS3.p2.6.m6.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.SS3.p2.6.m6.1.1.2.3" xref="S3.SS3.p2.6.m6.1.1.2.3.cmml">l</mi></msup><mo id="S3.SS3.p2.6.m6.1.1.1" xref="S3.SS3.p2.6.m6.1.1.1.cmml">∈</mo><msup id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml"><mi id="S3.SS3.p2.6.m6.1.1.3.2" xref="S3.SS3.p2.6.m6.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p2.6.m6.1.1.3.3" xref="S3.SS3.p2.6.m6.1.1.3.3.cmml"><msub id="S3.SS3.p2.6.m6.1.1.3.3.2" xref="S3.SS3.p2.6.m6.1.1.3.3.2.cmml"><mi id="S3.SS3.p2.6.m6.1.1.3.3.2.2" xref="S3.SS3.p2.6.m6.1.1.3.3.2.2.cmml">T</mi><mi id="S3.SS3.p2.6.m6.1.1.3.3.2.3" xref="S3.SS3.p2.6.m6.1.1.3.3.2.3.cmml">l</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.6.m6.1.1.3.3.1" xref="S3.SS3.p2.6.m6.1.1.3.3.1.cmml">×</mo><msup id="S3.SS3.p2.6.m6.1.1.3.3.3" xref="S3.SS3.p2.6.m6.1.1.3.3.3.cmml"><mi id="S3.SS3.p2.6.m6.1.1.3.3.3.2" xref="S3.SS3.p2.6.m6.1.1.3.3.3.2.cmml">D</mi><mo id="S3.SS3.p2.6.m6.1.1.3.3.3.3" xref="S3.SS3.p2.6.m6.1.1.3.3.3.3.cmml">′</mo></msup></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><in id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1.1"></in><apply id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.2.1.cmml" xref="S3.SS3.p2.6.m6.1.1.2">superscript</csymbol><apply id="S3.SS3.p2.6.m6.1.1.2.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2.2"><ci id="S3.SS3.p2.6.m6.1.1.2.2.1.cmml" xref="S3.SS3.p2.6.m6.1.1.2.2.1">^</ci><ci id="S3.SS3.p2.6.m6.1.1.2.2.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2.2.2">𝑍</ci></apply><ci id="S3.SS3.p2.6.m6.1.1.2.3.cmml" xref="S3.SS3.p2.6.m6.1.1.2.3">𝑙</ci></apply><apply id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.3.1.cmml" xref="S3.SS3.p2.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.3.2.cmml" xref="S3.SS3.p2.6.m6.1.1.3.2">ℝ</ci><apply id="S3.SS3.p2.6.m6.1.1.3.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3"><times id="S3.SS3.p2.6.m6.1.1.3.3.1.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.1"></times><apply id="S3.SS3.p2.6.m6.1.1.3.3.2.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.3.3.2.1.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.3.3.2.2.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.2.2">𝑇</ci><ci id="S3.SS3.p2.6.m6.1.1.3.3.2.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.2.3">𝑙</ci></apply><apply id="S3.SS3.p2.6.m6.1.1.3.3.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.3.3.3.1.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.3">superscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.3.3.3.2.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.3.2">𝐷</ci><ci id="S3.SS3.p2.6.m6.1.1.3.3.3.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3.3.3">′</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">\hat{Z}^{l}\in\mathbb{R}^{T_{l}\times D^{\prime}}</annotation></semantics></math> from each pyramid level to compute the similarities between them.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="s^{l}=\sigma(\hat{Z}^{l}\hat{\mathcal{T}}^{\top})," display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msup id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.3.2.cmml">s</mi><mi id="S3.E3.m1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.3.3.cmml">l</mi></msup><mo id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><msup id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">Z</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml">l</mi></msup><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">​</mo><msup id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">𝒯</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml">⊤</mo></msup></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"></eq><apply id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3">superscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2">𝑠</ci><ci id="S3.E3.m1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3">𝑙</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3">𝜎</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2"><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.1">^</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.2">𝑍</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3">𝑙</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2"><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.1">^</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.2">𝒯</ci></apply><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.3">top</csymbol></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">s^{l}=\sigma(\hat{Z}^{l}\hat{\mathcal{T}}^{\top}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p3.6" class="ltx_p">where <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="s^{l}\in\mathbb{R}^{T_{l}\times N_{k}}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><msup id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2.2" xref="S3.SS3.p3.1.m1.1.1.2.2.cmml">s</mi><mi id="S3.SS3.p3.1.m1.1.1.2.3" xref="S3.SS3.p3.1.m1.1.1.2.3.cmml">l</mi></msup><mo id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml"><mi id="S3.SS3.p3.1.m1.1.1.3.2" xref="S3.SS3.p3.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p3.1.m1.1.1.3.3" xref="S3.SS3.p3.1.m1.1.1.3.3.cmml"><msub id="S3.SS3.p3.1.m1.1.1.3.3.2" xref="S3.SS3.p3.1.m1.1.1.3.3.2.cmml"><mi id="S3.SS3.p3.1.m1.1.1.3.3.2.2" xref="S3.SS3.p3.1.m1.1.1.3.3.2.2.cmml">T</mi><mi id="S3.SS3.p3.1.m1.1.1.3.3.2.3" xref="S3.SS3.p3.1.m1.1.1.3.3.2.3.cmml">l</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p3.1.m1.1.1.3.3.1" xref="S3.SS3.p3.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S3.SS3.p3.1.m1.1.1.3.3.3" xref="S3.SS3.p3.1.m1.1.1.3.3.3.cmml"><mi id="S3.SS3.p3.1.m1.1.1.3.3.3.2" xref="S3.SS3.p3.1.m1.1.1.3.3.3.2.cmml">N</mi><mi id="S3.SS3.p3.1.m1.1.1.3.3.3.3" xref="S3.SS3.p3.1.m1.1.1.3.3.3.3.cmml">k</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><in id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></in><apply id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.2.1.cmml" xref="S3.SS3.p3.1.m1.1.1.2">superscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.2.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2.2">𝑠</ci><ci id="S3.SS3.p3.1.m1.1.1.2.3.cmml" xref="S3.SS3.p3.1.m1.1.1.2.3">𝑙</ci></apply><apply id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.3.1.cmml" xref="S3.SS3.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.3.2.cmml" xref="S3.SS3.p3.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS3.p3.1.m1.1.1.3.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3"><times id="S3.SS3.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3.1"></times><apply id="S3.SS3.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.3.3.2.1.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.3.3.2.2.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3.2.2">𝑇</ci><ci id="S3.SS3.p3.1.m1.1.1.3.3.2.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3.2.3">𝑙</ci></apply><apply id="S3.SS3.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3.3.2">𝑁</ci><ci id="S3.SS3.p3.1.m1.1.1.3.3.3.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3.3.3.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">s^{l}\in\mathbb{R}^{T_{l}\times N_{k}}</annotation></semantics></math> indicates the similarities between <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="N_{k}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><msub id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">N</mi><mi id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">𝑁</ci><ci id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">N_{k}</annotation></semantics></math> categories and <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="T_{l}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><msub id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">T</mi><mi id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">𝑇</ci><ci id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">T_{l}</annotation></semantics></math> temporal segments in the pyramid level <math id="S3.SS3.p3.4.m4.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><mi id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><ci id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">l</annotation></semantics></math>, and <math id="S3.SS3.p3.5.m5.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS3.p3.5.m5.1a"><mi id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><ci id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">\sigma</annotation></semantics></math> is a learnable scaling factor used to adaptively adjust the magnitude of the similarities as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.
Then, a sigmoid function is attached to predict the probabilities of <math id="S3.SS3.p3.6.m6.1" class="ltx_Math" alttext="N_{k}" display="inline"><semantics id="S3.SS3.p3.6.m6.1a"><msub id="S3.SS3.p3.6.m6.1.1" xref="S3.SS3.p3.6.m6.1.1.cmml"><mi id="S3.SS3.p3.6.m6.1.1.2" xref="S3.SS3.p3.6.m6.1.1.2.cmml">N</mi><mi id="S3.SS3.p3.6.m6.1.1.3" xref="S3.SS3.p3.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m6.1b"><apply id="S3.SS3.p3.6.m6.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.6.m6.1.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p3.6.m6.1.1.2.cmml" xref="S3.SS3.p3.6.m6.1.1.2">𝑁</ci><ci id="S3.SS3.p3.6.m6.1.1.3.cmml" xref="S3.SS3.p3.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m6.1c">N_{k}</annotation></semantics></math> classes at each moment.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Regression head.</span>
We simply apply a lightweight regression head for each task.
Each regression head as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> consists of 3 layers of 1D convolution attached with layer normalization and ReLU activation.
The parameters of the first two convolutional layers are shared among the three heads. The last layer followed by ReLU outputs the distances to the start/end time of an instance at each moment in the pyramid level <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mi id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">l</annotation></semantics></math> if the instance exists.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Loss function.</span>
Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, we apply two losses to train our model in an end-to-end manner.
The sigmoid focal loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and the generalized IoU loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> are used for classification and regression, respectively. The contributions of these two losses are equal as default.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multi-Task Training</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We jointly train TAL, AVEL and SED tasks in a single, unified architecture by multi-task learning. The advantage is that the tasks can potentially learn mutually beneficial knowledge from each other.
However, the datasets for the tasks vary greatly in size and difficulty, making it very challenging for joint training.
For instance, a single epoch of ActivityNet 1.3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> for TAL task corresponds to around 15 epochs of DESED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> for SED task. Besides, all clips in DESED are 10s while the longest video in ActivityNet 1.3 exceeds 12 minutes.
Hence, following the multi-task training method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we use Round-Robin Batch-Level Sampling strategy to sample batches from tasks in a cyclical manner, where one iteration forwards a batch for each task and updates parameters in sequence.
The Dynamic Stop-and-Go training scheduler is also applied to monitor the validation losses of each task to avoid overfitting.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets and Evaluation Metrics</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Temporal action localization.</span>
ActivityNet 1.3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> is a large-scale benchmark for video action localization. It contains 200 common human activity classes and around 20k untrimmed videos collected from YouTube.
Following the common practice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, we use its training set during training and report the performance on the validation set.
The standard evaluation metric for TAL is mean Average Precision (mAP). We use the mAPs at the tIoU thresholds [0.5:0.05:0.95] and the average mAP is also reported.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Sound event detection.</span>
DESED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> is the benchmark for the DCASE Challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
It consists of 10-second audio clips with 10 sound event classes in domestic environments.
DESED just has sound data and provides strong annotations (<em id="S4.SS1.p2.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.p2.1.3" class="ltx_text"></span>, classes and temporal boundaries) for its recorded validation set (1,168 clips) and public evaluation set (692 clips).
We downloaded their original videos with audio from YouTube.
We use the validation set for training and report the results on the public evaluation set.
Since the traditional evaluation metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> is only suitable for the sound-only clip-level task with extremely fine granularity (<em id="S4.SS1.p2.1.4" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.p2.1.5" class="ltx_text"></span>, 0.05s per clip) on audio spectrograms, we use mAPs@[0.5:0.2:0.9] and report the average mAP of mAPs@[0.1:0.1:0.9] in our experiments.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Audio-visual event localization.</span>
UnAV-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> consists of 10,790 untrimmed videos with around 30k audio-visual events.
It covers 100 event categories spanning a wide range of domains, <em id="S4.SS1.p3.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS1.p3.1.3" class="ltx_text"></span>, human activities, <span id="S4.SS1.p3.1.4" class="ltx_text" style="color:#000000;">animal/natural</span> sounds, <em id="S4.SS1.p3.1.5" class="ltx_emph ltx_font_italic">etc</em>.
Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, we use its train split for training and test split for testing.
For evaluation, we use the mAPs at the tIoU thresholds [0.5:0.1:0.9] and also report the average mAP between 0.1 and 0.9 with the step of 0.1 (<em id="S4.SS1.p3.1.6" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.p3.1.7" class="ltx_text"></span> [0.1:0.1:0.9]).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.3" class="ltx_p">The sampling rates of sounds and video frames are 16 kHz and 16 fps, respectively.
We feed 16 consecutive frames using a sliding window with stride 8 for ActivityNet 1.3, and downsample the features into a fixed length of 256 following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. For UnAV-100 and DESED, we use the stride of 4, and pad or crop the feature sequences to 256 and 64, respectively. For each corresponding 1s audio segment, the same stride duration (0.5/0.25s) is used to temporally align with the visual ones.
The visual, audio and text encoders of ONE-PEACE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> are utilized
<span id="S4.SS2.p1.3.1" class="ltx_text" style="color:#000000;"> to extract semantically aligned three modality embeddings, </span>
where the visual encoder is further fine-tuned on Kinetics-400 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
The extracted feature dimension is 1,536 for all three modalities.
The dimension of the embedding space in the framework is <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="D=D^{{}^{\prime}}=512" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">D</mi><mo id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">=</mo><msup id="S4.SS2.p1.1.m1.1.1.4" xref="S4.SS2.p1.1.m1.1.1.4.cmml"><mi id="S4.SS2.p1.1.m1.1.1.4.2" xref="S4.SS2.p1.1.m1.1.1.4.2.cmml">D</mi><msup id="S4.SS2.p1.1.m1.1.1.4.3" xref="S4.SS2.p1.1.m1.1.1.4.3.cmml"><mi id="S4.SS2.p1.1.m1.1.1.4.3a" xref="S4.SS2.p1.1.m1.1.1.4.3.cmml"></mi><mo id="S4.SS2.p1.1.m1.1.1.4.3.1" xref="S4.SS2.p1.1.m1.1.1.4.3.1.cmml">′</mo></msup></msup><mo id="S4.SS2.p1.1.m1.1.1.5" xref="S4.SS2.p1.1.m1.1.1.5.cmml">=</mo><mn id="S4.SS2.p1.1.m1.1.1.6" xref="S4.SS2.p1.1.m1.1.1.6.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><and id="S4.SS2.p1.1.m1.1.1a.cmml" xref="S4.SS2.p1.1.m1.1.1"></and><apply id="S4.SS2.p1.1.m1.1.1b.cmml" xref="S4.SS2.p1.1.m1.1.1"><eq id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"></eq><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝐷</ci><apply id="S4.SS2.p1.1.m1.1.1.4.cmml" xref="S4.SS2.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.4.1.cmml" xref="S4.SS2.p1.1.m1.1.1.4">superscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.4.2.cmml" xref="S4.SS2.p1.1.m1.1.1.4.2">𝐷</ci><apply id="S4.SS2.p1.1.m1.1.1.4.3.cmml" xref="S4.SS2.p1.1.m1.1.1.4.3"><ci id="S4.SS2.p1.1.m1.1.1.4.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.4.3.1">′</ci></apply></apply></apply><apply id="S4.SS2.p1.1.m1.1.1c.cmml" xref="S4.SS2.p1.1.m1.1.1"><eq id="S4.SS2.p1.1.m1.1.1.5.cmml" xref="S4.SS2.p1.1.m1.1.1.5"></eq><share href="#S4.SS2.p1.1.m1.1.1.4.cmml" id="S4.SS2.p1.1.m1.1.1d.cmml" xref="S4.SS2.p1.1.m1.1.1"></share><cn type="integer" id="S4.SS2.p1.1.m1.1.1.6.cmml" xref="S4.SS2.p1.1.m1.1.1.6">512</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">D=D^{{}^{\prime}}=512</annotation></semantics></math>, and <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="L_{1}=2" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><msub id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2.2" xref="S4.SS2.p1.2.m2.1.1.2.2.cmml">L</mi><mn id="S4.SS2.p1.2.m2.1.1.2.3" xref="S4.SS2.p1.2.m2.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><eq id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></eq><apply id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.2.1.cmml" xref="S4.SS2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.2.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2.2">𝐿</ci><cn type="integer" id="S4.SS2.p1.2.m2.1.1.2.3.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3">1</cn></apply><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">L_{1}=2</annotation></semantics></math>, <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="L_{2}=6" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><msub id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml"><mi id="S4.SS2.p1.3.m3.1.1.2.2" xref="S4.SS2.p1.3.m3.1.1.2.2.cmml">L</mi><mn id="S4.SS2.p1.3.m3.1.1.2.3" xref="S4.SS2.p1.3.m3.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><eq id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></eq><apply id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.1.1.2.1.cmml" xref="S4.SS2.p1.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS2.p1.3.m3.1.1.2.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2.2">𝐿</ci><cn type="integer" id="S4.SS2.p1.3.m3.1.1.2.3.cmml" xref="S4.SS2.p1.3.m3.1.1.2.3">2</cn></apply><cn type="integer" id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">L_{2}=6</annotation></semantics></math>.
During training, we use Adam for optimization and simply set the same hyperparameters for all datasets of three tasks.
Specifically, the mini-batch size is 16, the initial learning rate is 1e-3 and a cosine learning rate decay is used.
Our model is trained only for 5 epochs with a linear warmup of 2 epochs and the weight decay is 1e-4.
During inference, our model outputs the on/offsets and classes with confidence scores for all three types of instances occurring in a given video. The output candidates are then processed by Soft-NMS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> to eliminate highly overlapping ones.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.9.4.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.6.3" class="ltx_text" style="font-size:90%;">Comparison of our multi-task models to single-task performance.
We use both audio and visual modalities for all models. “ST”: single-task, “AT”: all tasks.
“All Tasks Average” is computed by averaging the average mAP results of all three tasks. “<math id="S4.T1.4.1.m1.1" class="ltx_Math" alttext="\cdot\mathrm{M}(\cdot)" display="inline"><semantics id="S4.T1.4.1.m1.1b"><mrow id="S4.T1.4.1.m1.1.2" xref="S4.T1.4.1.m1.1.2.cmml"><mi id="S4.T1.4.1.m1.1.2.2" xref="S4.T1.4.1.m1.1.2.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S4.T1.4.1.m1.1.2.1" xref="S4.T1.4.1.m1.1.2.1.cmml">⋅</mo><mrow id="S4.T1.4.1.m1.1.2.3" xref="S4.T1.4.1.m1.1.2.3.cmml"><mi mathvariant="normal" id="S4.T1.4.1.m1.1.2.3.2" xref="S4.T1.4.1.m1.1.2.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.T1.4.1.m1.1.2.3.1" xref="S4.T1.4.1.m1.1.2.3.1.cmml">​</mo><mrow id="S4.T1.4.1.m1.1.2.3.3.2" xref="S4.T1.4.1.m1.1.2.3.cmml"><mo stretchy="false" id="S4.T1.4.1.m1.1.2.3.3.2.1" xref="S4.T1.4.1.m1.1.2.3.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.T1.4.1.m1.1.1" xref="S4.T1.4.1.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="S4.T1.4.1.m1.1.2.3.3.2.2" xref="S4.T1.4.1.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.4.1.m1.1c"><apply id="S4.T1.4.1.m1.1.2.cmml" xref="S4.T1.4.1.m1.1.2"><ci id="S4.T1.4.1.m1.1.2.1.cmml" xref="S4.T1.4.1.m1.1.2.1">⋅</ci><csymbol cd="latexml" id="S4.T1.4.1.m1.1.2.2.cmml" xref="S4.T1.4.1.m1.1.2.2">absent</csymbol><apply id="S4.T1.4.1.m1.1.2.3.cmml" xref="S4.T1.4.1.m1.1.2.3"><times id="S4.T1.4.1.m1.1.2.3.1.cmml" xref="S4.T1.4.1.m1.1.2.3.1"></times><ci id="S4.T1.4.1.m1.1.2.3.2.cmml" xref="S4.T1.4.1.m1.1.2.3.2">M</ci><ci id="S4.T1.4.1.m1.1.1.cmml" xref="S4.T1.4.1.m1.1.1">⋅</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.1.m1.1d">\cdot\mathrm{M}(\cdot)</annotation></semantics></math>”: total <math id="S4.T1.5.2.m2.1" class="ltx_Math" alttext="\cdot\mathrm{M}" display="inline"><semantics id="S4.T1.5.2.m2.1b"><mrow id="S4.T1.5.2.m2.1.1" xref="S4.T1.5.2.m2.1.1.cmml"><mi id="S4.T1.5.2.m2.1.1.2" xref="S4.T1.5.2.m2.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S4.T1.5.2.m2.1.1.1" xref="S4.T1.5.2.m2.1.1.1.cmml">⋅</mo><mi mathvariant="normal" id="S4.T1.5.2.m2.1.1.3" xref="S4.T1.5.2.m2.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.5.2.m2.1c"><apply id="S4.T1.5.2.m2.1.1.cmml" xref="S4.T1.5.2.m2.1.1"><ci id="S4.T1.5.2.m2.1.1.1.cmml" xref="S4.T1.5.2.m2.1.1.1">⋅</ci><csymbol cd="latexml" id="S4.T1.5.2.m2.1.1.2.cmml" xref="S4.T1.5.2.m2.1.1.2">absent</csymbol><ci id="S4.T1.5.2.m2.1.1.3.cmml" xref="S4.T1.5.2.m2.1.1.3">M</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.2.m2.1d">\cdot\mathrm{M}</annotation></semantics></math> parameters needed for <math id="S4.T1.6.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="S4.T1.6.3.m3.1b"><mo id="S4.T1.6.3.m3.1.1" xref="S4.T1.6.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.3.m3.1c"><ci id="S4.T1.6.3.m3.1.1.cmml" xref="S4.T1.6.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.3.m3.1d">\cdot</annotation></semantics></math> models to conduct all three tasks. </span></figcaption>
<div id="S4.T1.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:101.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-128.8pt,30.2pt) scale(0.627393093971686,0.627393093971686) ;">
<table id="S4.T1.7.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.7.1.2.1" class="ltx_tr">
<td id="S4.T1.7.1.2.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T1.7.1.2.1.2" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T1.7.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">ActivityNet 1.3 (TAL)</td>
<td id="S4.T1.7.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="6">UnAV-100 (AVEL)</td>
<td id="S4.T1.7.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">DESED (SED)</td>
<th id="S4.T1.7.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T1.7.1.2.1.6.1" class="ltx_text"><span id="S4.T1.7.1.2.1.6.1.1" class="ltx_text"></span> <span id="S4.T1.7.1.2.1.6.1.2" class="ltx_text">
<span id="S4.T1.7.1.2.1.6.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.7.1.2.1.6.1.2.1.1" class="ltx_tr">
<span id="S4.T1.7.1.2.1.6.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.7.1.2.1.6.1.2.1.1.1.1" class="ltx_text" style="font-size:80%;">All Tasks</span></span></span>
<span id="S4.T1.7.1.2.1.6.1.2.1.2" class="ltx_tr">
<span id="S4.T1.7.1.2.1.6.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.7.1.2.1.6.1.2.1.2.1.1" class="ltx_text" style="font-size:80%;">Average</span></span></span>
</span></span> <span id="S4.T1.7.1.2.1.6.1.3" class="ltx_text"></span></span></th>
<td id="S4.T1.7.1.2.1.7" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S4.T1.7.1.3.2" class="ltx_tr">
<td id="S4.T1.7.1.3.2.1" class="ltx_td"></td>
<td id="S4.T1.7.1.3.2.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.7.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T1.7.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">0.75</td>
<td id="S4.T1.7.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">0.95</td>
<td id="S4.T1.7.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Avg.</td>
<td id="S4.T1.7.1.3.2.7" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T1.7.1.3.2.8" class="ltx_td ltx_align_center ltx_border_t">0.6</td>
<td id="S4.T1.7.1.3.2.9" class="ltx_td ltx_align_center ltx_border_t">0.7</td>
<td id="S4.T1.7.1.3.2.10" class="ltx_td ltx_align_center ltx_border_t">0.8</td>
<td id="S4.T1.7.1.3.2.11" class="ltx_td ltx_align_center ltx_border_t">0.9</td>
<td id="S4.T1.7.1.3.2.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Avg.</td>
<td id="S4.T1.7.1.3.2.13" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T1.7.1.3.2.14" class="ltx_td ltx_align_center ltx_border_t">0.7</td>
<td id="S4.T1.7.1.3.2.15" class="ltx_td ltx_align_center ltx_border_t">0.9</td>
<td id="S4.T1.7.1.3.2.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Avg.</td>
<td id="S4.T1.7.1.3.2.17" class="ltx_td ltx_align_right"># params</td>
</tr>
<tr id="S4.T1.7.1.4.3" class="ltx_tr">
<td id="S4.T1.7.1.4.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.7.1.4.3.1.1" class="ltx_text" style="color:#808080;">1</span></td>
<td id="S4.T1.7.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Single-Task (ST)</td>
<td id="S4.T1.7.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">56.6</td>
<td id="S4.T1.7.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">35.4</td>
<td id="S4.T1.7.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">5.1</td>
<td id="S4.T1.7.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.3</td>
<td id="S4.T1.7.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t">53.2</td>
<td id="S4.T1.7.1.4.3.8" class="ltx_td ltx_align_center ltx_border_t">46.7</td>
<td id="S4.T1.7.1.4.3.9" class="ltx_td ltx_align_center ltx_border_t">39.9</td>
<td id="S4.T1.7.1.4.3.10" class="ltx_td ltx_align_center ltx_border_t">31.6</td>
<td id="S4.T1.7.1.4.3.11" class="ltx_td ltx_align_center ltx_border_t">19.8</td>
<td id="S4.T1.7.1.4.3.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.6</td>
<td id="S4.T1.7.1.4.3.13" class="ltx_td ltx_align_center ltx_border_t">61.0</td>
<td id="S4.T1.7.1.4.3.14" class="ltx_td ltx_align_center ltx_border_t">45.9</td>
<td id="S4.T1.7.1.4.3.15" class="ltx_td ltx_align_center ltx_border_t">19.8</td>
<td id="S4.T1.7.1.4.3.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">57.7</td>
<td id="S4.T1.7.1.4.3.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.5</td>
<td id="S4.T1.7.1.4.3.18" class="ltx_td ltx_align_right ltx_border_t">186M (3)</td>
</tr>
<tr id="S4.T1.7.1.5.4" class="ltx_tr">
<td id="S4.T1.7.1.5.4.1" class="ltx_td ltx_align_left"><span id="S4.T1.7.1.5.4.1.1" class="ltx_text" style="color:#808080;">2</span></td>
<td id="S4.T1.7.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r">Multi-Task (Base)</td>
<td id="S4.T1.7.1.5.4.3" class="ltx_td ltx_align_center">55.4</td>
<td id="S4.T1.7.1.5.4.4" class="ltx_td ltx_align_center">34.9</td>
<td id="S4.T1.7.1.5.4.5" class="ltx_td ltx_align_center">6.3</td>
<td id="S4.T1.7.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r">34.8</td>
<td id="S4.T1.7.1.5.4.7" class="ltx_td ltx_align_center">53.0</td>
<td id="S4.T1.7.1.5.4.8" class="ltx_td ltx_align_center">47.4</td>
<td id="S4.T1.7.1.5.4.9" class="ltx_td ltx_align_center">41.0</td>
<td id="S4.T1.7.1.5.4.10" class="ltx_td ltx_align_center">33.5</td>
<td id="S4.T1.7.1.5.4.11" class="ltx_td ltx_align_center">21.5</td>
<td id="S4.T1.7.1.5.4.12" class="ltx_td ltx_align_center ltx_border_r">49.8</td>
<td id="S4.T1.7.1.5.4.13" class="ltx_td ltx_align_center">62.7</td>
<td id="S4.T1.7.1.5.4.14" class="ltx_td ltx_align_center">50.5</td>
<td id="S4.T1.7.1.5.4.15" class="ltx_td ltx_align_center">26.2</td>
<td id="S4.T1.7.1.5.4.16" class="ltx_td ltx_align_center ltx_border_r">58.6</td>
<td id="S4.T1.7.1.5.4.17" class="ltx_td ltx_align_center ltx_border_r">47.7</td>
<td id="S4.T1.7.1.5.4.18" class="ltx_td ltx_align_right">62M (1)</td>
</tr>
<tr id="S4.T1.7.1.6.5" class="ltx_tr">
<td id="S4.T1.7.1.6.5.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.7.1.6.5.1.1" class="ltx_text" style="color:#808080;">3</span></td>
<td id="S4.T1.7.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Multi-Task (TAL &amp; AVEL)</td>
<td id="S4.T1.7.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">56.4</td>
<td id="S4.T1.7.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">35.7</td>
<td id="S4.T1.7.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t">5.0</td>
<td id="S4.T1.7.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.7</td>
<td id="S4.T1.7.1.6.5.7" class="ltx_td ltx_align_center ltx_border_t">54.0</td>
<td id="S4.T1.7.1.6.5.8" class="ltx_td ltx_align_center ltx_border_t">48.9</td>
<td id="S4.T1.7.1.6.5.9" class="ltx_td ltx_align_center ltx_border_t">41.9</td>
<td id="S4.T1.7.1.6.5.10" class="ltx_td ltx_align_center ltx_border_t">34.0</td>
<td id="S4.T1.7.1.6.5.11" class="ltx_td ltx_align_center ltx_border_t">21.7</td>
<td id="S4.T1.7.1.6.5.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.8</td>
<td id="S4.T1.7.1.6.5.13" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.7.1.6.5.14" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.7.1.6.5.15" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T1.7.1.6.5.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T1.7.1.6.5.17" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T1.7.1.6.5.18" class="ltx_td ltx_align_right ltx_border_t">97M (1)</td>
</tr>
<tr id="S4.T1.7.1.7.6" class="ltx_tr">
<td id="S4.T1.7.1.7.6.1" class="ltx_td ltx_align_left"><span id="S4.T1.7.1.7.6.1.1" class="ltx_text" style="color:#808080;">4</span></td>
<td id="S4.T1.7.1.7.6.2" class="ltx_td ltx_align_left ltx_border_r">Multi-Task (TAL &amp; SED)</td>
<td id="S4.T1.7.1.7.6.3" class="ltx_td ltx_align_center">56.5</td>
<td id="S4.T1.7.1.7.6.4" class="ltx_td ltx_align_center">36.1</td>
<td id="S4.T1.7.1.7.6.5" class="ltx_td ltx_align_center">4.2</td>
<td id="S4.T1.7.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r">35.5</td>
<td id="S4.T1.7.1.7.6.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.7.1.7.6.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.7.1.7.6.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.7.1.7.6.10" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.7.1.7.6.11" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.7.1.7.6.12" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T1.7.1.7.6.13" class="ltx_td ltx_align_center">61.8</td>
<td id="S4.T1.7.1.7.6.14" class="ltx_td ltx_align_center">49.3</td>
<td id="S4.T1.7.1.7.6.15" class="ltx_td ltx_align_center">26.1</td>
<td id="S4.T1.7.1.7.6.16" class="ltx_td ltx_align_center ltx_border_r">58.2</td>
<td id="S4.T1.7.1.7.6.17" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T1.7.1.7.6.18" class="ltx_td ltx_align_right">97M (1)</td>
</tr>
<tr id="S4.T1.7.1.8.7" class="ltx_tr">
<td id="S4.T1.7.1.8.7.1" class="ltx_td ltx_align_left"><span id="S4.T1.7.1.8.7.1.1" class="ltx_text" style="color:#808080;">5</span></td>
<td id="S4.T1.7.1.8.7.2" class="ltx_td ltx_align_left ltx_border_r">Multi-Task (AVEL &amp; SED)</td>
<td id="S4.T1.7.1.8.7.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.7.1.8.7.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.7.1.8.7.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.7.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T1.7.1.8.7.7" class="ltx_td ltx_align_center">51.8</td>
<td id="S4.T1.7.1.8.7.8" class="ltx_td ltx_align_center">46.8</td>
<td id="S4.T1.7.1.8.7.9" class="ltx_td ltx_align_center">40.1</td>
<td id="S4.T1.7.1.8.7.10" class="ltx_td ltx_align_center">30.0</td>
<td id="S4.T1.7.1.8.7.11" class="ltx_td ltx_align_center">18.2</td>
<td id="S4.T1.7.1.8.7.12" class="ltx_td ltx_align_center ltx_border_r">48.3</td>
<td id="S4.T1.7.1.8.7.13" class="ltx_td ltx_align_center">62.6</td>
<td id="S4.T1.7.1.8.7.14" class="ltx_td ltx_align_center">49.4</td>
<td id="S4.T1.7.1.8.7.15" class="ltx_td ltx_align_center"><span id="S4.T1.7.1.8.7.15.1" class="ltx_text ltx_font_bold">27.2</span></td>
<td id="S4.T1.7.1.8.7.16" class="ltx_td ltx_align_center ltx_border_r">59.4</td>
<td id="S4.T1.7.1.8.7.17" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T1.7.1.8.7.18" class="ltx_td ltx_align_right">97M (1)</td>
</tr>
<tr id="S4.T1.7.1.9.8" class="ltx_tr">
<td id="S4.T1.7.1.9.8.1" class="ltx_td ltx_align_left"><span id="S4.T1.7.1.9.8.1.1" class="ltx_text" style="color:#808080;">6</span></td>
<td id="S4.T1.7.1.9.8.2" class="ltx_td ltx_align_left ltx_border_r">Multi-Task (AT)</td>
<td id="S4.T1.7.1.9.8.3" class="ltx_td ltx_align_center"><span id="S4.T1.7.1.9.8.3.1" class="ltx_text ltx_font_bold">57.1</span></td>
<td id="S4.T1.7.1.9.8.4" class="ltx_td ltx_align_center"><span id="S4.T1.7.1.9.8.4.1" class="ltx_text ltx_font_bold">36.4</span></td>
<td id="S4.T1.7.1.9.8.5" class="ltx_td ltx_align_center">5.5</td>
<td id="S4.T1.7.1.9.8.6" class="ltx_td ltx_align_center ltx_border_r">36.1</td>
<td id="S4.T1.7.1.9.8.7" class="ltx_td ltx_align_center">54.1</td>
<td id="S4.T1.7.1.9.8.8" class="ltx_td ltx_align_center">48.6</td>
<td id="S4.T1.7.1.9.8.9" class="ltx_td ltx_align_center">42.1</td>
<td id="S4.T1.7.1.9.8.10" class="ltx_td ltx_align_center">34.3</td>
<td id="S4.T1.7.1.9.8.11" class="ltx_td ltx_align_center">20.5</td>
<td id="S4.T1.7.1.9.8.12" class="ltx_td ltx_align_center ltx_border_r">50.7</td>
<td id="S4.T1.7.1.9.8.13" class="ltx_td ltx_align_center">64.0</td>
<td id="S4.T1.7.1.9.8.14" class="ltx_td ltx_align_center">49.5</td>
<td id="S4.T1.7.1.9.8.15" class="ltx_td ltx_align_center">27.0</td>
<td id="S4.T1.7.1.9.8.16" class="ltx_td ltx_align_center ltx_border_r">59.8</td>
<td id="S4.T1.7.1.9.8.17" class="ltx_td ltx_align_center ltx_border_r">48.9</td>
<td id="S4.T1.7.1.9.8.18" class="ltx_td ltx_align_right">130M (1)</td>
</tr>
<tr id="S4.T1.7.1.1" class="ltx_tr">
<td id="S4.T1.7.1.1.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S4.T1.7.1.1.2.1" class="ltx_text" style="color:#808080;">7</span></td>
<td id="S4.T1.7.1.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">AT <math id="S4.T1.7.1.1.1.m1.1" class="ltx_Math" alttext="\xrightarrow{\text{finetune}}\mathrm{ST}" display="inline"><semantics id="S4.T1.7.1.1.1.m1.1a"><mrow id="S4.T1.7.1.1.1.m1.1.1" xref="S4.T1.7.1.1.1.m1.1.1.cmml"><mi id="S4.T1.7.1.1.1.m1.1.1.2" xref="S4.T1.7.1.1.1.m1.1.1.2.cmml"></mi><mover accent="true" id="S4.T1.7.1.1.1.m1.1.1.1" xref="S4.T1.7.1.1.1.m1.1.1.1.cmml"><mo stretchy="false" id="S4.T1.7.1.1.1.m1.1.1.1.2" xref="S4.T1.7.1.1.1.m1.1.1.1.2.cmml">→</mo><mtext id="S4.T1.7.1.1.1.m1.1.1.1.1" xref="S4.T1.7.1.1.1.m1.1.1.1.1a.cmml">finetune</mtext></mover><mi id="S4.T1.7.1.1.1.m1.1.1.3" xref="S4.T1.7.1.1.1.m1.1.1.3.cmml">ST</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.7.1.1.1.m1.1b"><apply id="S4.T1.7.1.1.1.m1.1.1.cmml" xref="S4.T1.7.1.1.1.m1.1.1"><apply id="S4.T1.7.1.1.1.m1.1.1.1.cmml" xref="S4.T1.7.1.1.1.m1.1.1.1"><ci id="S4.T1.7.1.1.1.m1.1.1.1.1a.cmml" xref="S4.T1.7.1.1.1.m1.1.1.1.1"><mtext id="S4.T1.7.1.1.1.m1.1.1.1.1.cmml" xref="S4.T1.7.1.1.1.m1.1.1.1.1">finetune</mtext></ci><ci id="S4.T1.7.1.1.1.m1.1.1.1.2.cmml" xref="S4.T1.7.1.1.1.m1.1.1.1.2">→</ci></apply><csymbol cd="latexml" id="S4.T1.7.1.1.1.m1.1.1.2.cmml" xref="S4.T1.7.1.1.1.m1.1.1.2">absent</csymbol><ci id="S4.T1.7.1.1.1.m1.1.1.3.cmml" xref="S4.T1.7.1.1.1.m1.1.1.3">ST</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.1.1.1.m1.1c">\xrightarrow{\text{finetune}}\mathrm{ST}</annotation></semantics></math>
</td>
<td id="S4.T1.7.1.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">56.8</td>
<td id="S4.T1.7.1.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">36.0</td>
<td id="S4.T1.7.1.1.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.7.1.1.5.1" class="ltx_text ltx_font_bold">6.7</span></td>
<td id="S4.T1.7.1.1.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.7.1.1.6.1" class="ltx_text ltx_font_bold">36.2</span></td>
<td id="S4.T1.7.1.1.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.7.1.1.7.1" class="ltx_text ltx_font_bold">54.8</span></td>
<td id="S4.T1.7.1.1.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.7.1.1.8.1" class="ltx_text ltx_font_bold">49.4</span></td>
<td id="S4.T1.7.1.1.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.7.1.1.9.1" class="ltx_text ltx_font_bold">43.2</span></td>
<td id="S4.T1.7.1.1.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.7.1.1.10.1" class="ltx_text ltx_font_bold">35.3</span></td>
<td id="S4.T1.7.1.1.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.7.1.1.11.1" class="ltx_text ltx_font_bold">22.5</span></td>
<td id="S4.T1.7.1.1.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.7.1.1.12.1" class="ltx_text ltx_font_bold">51.7</span></td>
<td id="S4.T1.7.1.1.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.7.1.1.13.1" class="ltx_text ltx_font_bold">65.1</span></td>
<td id="S4.T1.7.1.1.14" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.7.1.1.14.1" class="ltx_text ltx_font_bold">50.9</span></td>
<td id="S4.T1.7.1.1.15" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">26.1</td>
<td id="S4.T1.7.1.1.16" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.7.1.1.16.1" class="ltx_text ltx_font_bold">61.1</span></td>
<td id="S4.T1.7.1.1.17" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.7.1.1.17.1" class="ltx_text ltx_font_bold">49.7</span></td>
<td id="S4.T1.7.1.1.18" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">186M (3)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Multi-Task Performance</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.5" class="ltx_p"><span id="S4.SS3.p1.5.3" class="ltx_text ltx_font_bold">Single-task v.s. multi-task.</span>
To demonstrate the effectiveness of our unified framework, in <a href="#S4.T1" title="In 4.2 Implementation Details ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, we compare our multi-task (AT) model (row 6) with two baseline models, single-task (ST) and multi-task (base) in row 1-2.
Single-task (ST) models were trained individually on the three tasks, <span id="S4.SS3.p1.5.4" class="ltx_text" style="color:#000000;">using standard transformer blocks with one FFN and a 1D-conv classifier similar to the regressor.</span>
The multi-task (base) model has the same architecture as ST models except for using multi-task learning strategies in Sec. <a href="#S3.SS4" title="3.4 Multi-Task Training ‣ 3 The UniAV Framework ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>.
We use the same hyperparameters described in Sec. <a href="#S4.SS2" title="4.2 Implementation Details ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> as the default setting unless specified otherwise.
We observe that simply joint training cannot get decent results on the three tasks (row 2).
The performance of the multi-task (base) model on SED and AVEL saw a slight improvement, due to beneficial knowledge learned from each other. However, the results on TAL declined since there exists a significant gap between ActivityNet 1.3 and other datasets.
In contrast, our multi-task (AT) model (row 6) that applies the task-specific experts and the unified language-aware classifier can achieve performance boosts on all three tasks compared with single-task models, <em id="S4.SS3.p1.5.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS3.p1.5.6" class="ltx_text"></span>, <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="+0.8\%" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mo id="S4.SS3.p1.1.m1.1.1a" xref="S4.SS3.p1.1.m1.1.1.cmml">+</mo><mrow id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2.2" xref="S4.SS3.p1.1.m1.1.1.2.2.cmml">0.8</mn><mo id="S4.SS3.p1.1.m1.1.1.2.1" xref="S4.SS3.p1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><plus id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"></plus><apply id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.SS3.p1.1.m1.1.1.2.1.cmml" xref="S4.SS3.p1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS3.p1.1.m1.1.1.2.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2.2">0.8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">+0.8\%</annotation></semantics></math>, <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="+1.1\%" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mo id="S4.SS3.p1.2.m2.1.1a" xref="S4.SS3.p1.2.m2.1.1.cmml">+</mo><mrow id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2.2" xref="S4.SS3.p1.2.m2.1.1.2.2.cmml">1.1</mn><mo id="S4.SS3.p1.2.m2.1.1.2.1" xref="S4.SS3.p1.2.m2.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><plus id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"></plus><apply id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2"><csymbol cd="latexml" id="S4.SS3.p1.2.m2.1.1.2.1.cmml" xref="S4.SS3.p1.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS3.p1.2.m2.1.1.2.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2.2">1.1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">+1.1\%</annotation></semantics></math> and <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="+2.1\%" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mo id="S4.SS3.p1.3.m3.1.1a" xref="S4.SS3.p1.3.m3.1.1.cmml">+</mo><mrow id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml"><mn id="S4.SS3.p1.3.m3.1.1.2.2" xref="S4.SS3.p1.3.m3.1.1.2.2.cmml">2.1</mn><mo id="S4.SS3.p1.3.m3.1.1.2.1" xref="S4.SS3.p1.3.m3.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><plus id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"></plus><apply id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2"><csymbol cd="latexml" id="S4.SS3.p1.3.m3.1.1.2.1.cmml" xref="S4.SS3.p1.3.m3.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS3.p1.3.m3.1.1.2.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2.2">2.1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">+2.1\%</annotation></semantics></math> at the average mAP on TAL, AVEL and SED, respectively, <span id="S4.SS3.p1.4.1" class="ltx_text" style="color:#000000;">and <math id="S4.SS3.p1.4.1.m1.1" class="ltx_Math" alttext="+1.4\%" display="inline"><semantics id="S4.SS3.p1.4.1.m1.1a"><mrow id="S4.SS3.p1.4.1.m1.1.1" xref="S4.SS3.p1.4.1.m1.1.1.cmml"><mo mathcolor="#000000" id="S4.SS3.p1.4.1.m1.1.1a" xref="S4.SS3.p1.4.1.m1.1.1.cmml">+</mo><mrow id="S4.SS3.p1.4.1.m1.1.1.2" xref="S4.SS3.p1.4.1.m1.1.1.2.cmml"><mn mathcolor="#000000" id="S4.SS3.p1.4.1.m1.1.1.2.2" xref="S4.SS3.p1.4.1.m1.1.1.2.2.cmml">1.4</mn><mo mathcolor="#000000" id="S4.SS3.p1.4.1.m1.1.1.2.1" xref="S4.SS3.p1.4.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.1.m1.1b"><apply id="S4.SS3.p1.4.1.m1.1.1.cmml" xref="S4.SS3.p1.4.1.m1.1.1"><plus id="S4.SS3.p1.4.1.m1.1.1.1.cmml" xref="S4.SS3.p1.4.1.m1.1.1"></plus><apply id="S4.SS3.p1.4.1.m1.1.1.2.cmml" xref="S4.SS3.p1.4.1.m1.1.1.2"><csymbol cd="latexml" id="S4.SS3.p1.4.1.m1.1.1.2.1.cmml" xref="S4.SS3.p1.4.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS3.p1.4.1.m1.1.1.2.2.cmml" xref="S4.SS3.p1.4.1.m1.1.1.2.2">1.4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.1.m1.1c">+1.4\%</annotation></semantics></math> at the average performance of all tasks.</span>
<span id="S4.SS3.p1.5.2" class="ltx_text" style="color:#000000;">Besides, the total number of parameters reduces by a factor of <math id="S4.SS3.p1.5.2.m1.1" class="ltx_math_unparsed" alttext="1.4\times" display="inline"><semantics id="S4.SS3.p1.5.2.m1.1a"><mrow id="S4.SS3.p1.5.2.m1.1b"><mn mathcolor="#000000" id="S4.SS3.p1.5.2.m1.1.1">1.4</mn><mo lspace="0.222em" mathcolor="#000000" id="S4.SS3.p1.5.2.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS3.p1.5.2.m1.1c">1.4\times</annotation></semantics></math> (<em id="S4.SS3.p1.5.2.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS3.p1.5.2.2" class="ltx_text"></span>, 186M to 130M), going from 3 full models to only 1 required for all tasks.
It implies both the effectiveness and efficiency of our multi-task framework.</span></p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.4" class="ltx_p"><span id="S4.SS3.p2.4.4" class="ltx_text ltx_font_bold">Pair-wise task relationships.</span>
We also explore pair-wise task relationships by jointly training two of three tasks, shown in rows 3-5 in <a href="#S4.T1" title="In 4.2 Implementation Details ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.
<span id="S4.SS3.p2.2.2" class="ltx_text" style="color:#000000;">We observe that  when applying our proposed experts and unified classifier, both AVEL and SED (row 3-4) can benefit from the rich common instances in ActivityNet 1.3 when trained with TAL, leading to an obvious improvement compared with the single-task models, <em id="S4.SS3.p2.2.2.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS3.p2.2.2.2" class="ltx_text"></span>, <math id="S4.SS3.p2.1.1.m1.1" class="ltx_Math" alttext="+1.2\%" display="inline"><semantics id="S4.SS3.p2.1.1.m1.1a"><mrow id="S4.SS3.p2.1.1.m1.1.1" xref="S4.SS3.p2.1.1.m1.1.1.cmml"><mo mathcolor="#000000" id="S4.SS3.p2.1.1.m1.1.1a" xref="S4.SS3.p2.1.1.m1.1.1.cmml">+</mo><mrow id="S4.SS3.p2.1.1.m1.1.1.2" xref="S4.SS3.p2.1.1.m1.1.1.2.cmml"><mn mathcolor="#000000" id="S4.SS3.p2.1.1.m1.1.1.2.2" xref="S4.SS3.p2.1.1.m1.1.1.2.2.cmml">1.2</mn><mo mathcolor="#000000" id="S4.SS3.p2.1.1.m1.1.1.2.1" xref="S4.SS3.p2.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.1.m1.1b"><apply id="S4.SS3.p2.1.1.m1.1.1.cmml" xref="S4.SS3.p2.1.1.m1.1.1"><plus id="S4.SS3.p2.1.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.1.m1.1.1"></plus><apply id="S4.SS3.p2.1.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.SS3.p2.1.1.m1.1.1.2.1.cmml" xref="S4.SS3.p2.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS3.p2.1.1.m1.1.1.2.2.cmml" xref="S4.SS3.p2.1.1.m1.1.1.2.2">1.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.1.m1.1c">+1.2\%</annotation></semantics></math> and <math id="S4.SS3.p2.2.2.m2.1" class="ltx_Math" alttext="+0.5\%" display="inline"><semantics id="S4.SS3.p2.2.2.m2.1a"><mrow id="S4.SS3.p2.2.2.m2.1.1" xref="S4.SS3.p2.2.2.m2.1.1.cmml"><mo mathcolor="#000000" id="S4.SS3.p2.2.2.m2.1.1a" xref="S4.SS3.p2.2.2.m2.1.1.cmml">+</mo><mrow id="S4.SS3.p2.2.2.m2.1.1.2" xref="S4.SS3.p2.2.2.m2.1.1.2.cmml"><mn mathcolor="#000000" id="S4.SS3.p2.2.2.m2.1.1.2.2" xref="S4.SS3.p2.2.2.m2.1.1.2.2.cmml">0.5</mn><mo mathcolor="#000000" id="S4.SS3.p2.2.2.m2.1.1.2.1" xref="S4.SS3.p2.2.2.m2.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.2.m2.1b"><apply id="S4.SS3.p2.2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.2.m2.1.1"><plus id="S4.SS3.p2.2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.2.m2.1.1"></plus><apply id="S4.SS3.p2.2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.2.m2.1.1.2"><csymbol cd="latexml" id="S4.SS3.p2.2.2.m2.1.1.2.1.cmml" xref="S4.SS3.p2.2.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS3.p2.2.2.m2.1.1.2.2.cmml" xref="S4.SS3.p2.2.2.m2.1.1.2.2">0.5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.2.m2.1c">+0.5\%</annotation></semantics></math> at the average mAP, respectively.</span>
Besides, SED gains a significant boost when trained with AVEL (row 5), <em id="S4.SS3.p2.4.5" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS3.p2.4.6" class="ltx_text"></span>, <math id="S4.SS3.p2.3.m1.1" class="ltx_Math" alttext="+1.7\%" display="inline"><semantics id="S4.SS3.p2.3.m1.1a"><mrow id="S4.SS3.p2.3.m1.1.1" xref="S4.SS3.p2.3.m1.1.1.cmml"><mo id="S4.SS3.p2.3.m1.1.1a" xref="S4.SS3.p2.3.m1.1.1.cmml">+</mo><mrow id="S4.SS3.p2.3.m1.1.1.2" xref="S4.SS3.p2.3.m1.1.1.2.cmml"><mn id="S4.SS3.p2.3.m1.1.1.2.2" xref="S4.SS3.p2.3.m1.1.1.2.2.cmml">1.7</mn><mo id="S4.SS3.p2.3.m1.1.1.2.1" xref="S4.SS3.p2.3.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m1.1b"><apply id="S4.SS3.p2.3.m1.1.1.cmml" xref="S4.SS3.p2.3.m1.1.1"><plus id="S4.SS3.p2.3.m1.1.1.1.cmml" xref="S4.SS3.p2.3.m1.1.1"></plus><apply id="S4.SS3.p2.3.m1.1.1.2.cmml" xref="S4.SS3.p2.3.m1.1.1.2"><csymbol cd="latexml" id="S4.SS3.p2.3.m1.1.1.2.1.cmml" xref="S4.SS3.p2.3.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS3.p2.3.m1.1.1.2.2.cmml" xref="S4.SS3.p2.3.m1.1.1.2.2">1.7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m1.1c">+1.7\%</annotation></semantics></math> at the average mAP.
<span id="S4.SS3.p2.4.3" class="ltx_text" style="color:#000000;">It could be attributed to category overlap between UnAV-100 and DESED, and training with the large dataset can help prevent overfitting in the small one.
Conversely, due to the large gap in instance duration and dataset scales, SED tends to have a negative effect on the AVEL task, resulting in a decrease of <math id="S4.SS3.p2.4.3.m1.1" class="ltx_Math" alttext="1.3\%" display="inline"><semantics id="S4.SS3.p2.4.3.m1.1a"><mrow id="S4.SS3.p2.4.3.m1.1.1" xref="S4.SS3.p2.4.3.m1.1.1.cmml"><mn mathcolor="#000000" id="S4.SS3.p2.4.3.m1.1.1.2" xref="S4.SS3.p2.4.3.m1.1.1.2.cmml">1.3</mn><mo mathcolor="#000000" id="S4.SS3.p2.4.3.m1.1.1.1" xref="S4.SS3.p2.4.3.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.3.m1.1b"><apply id="S4.SS3.p2.4.3.m1.1.1.cmml" xref="S4.SS3.p2.4.3.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p2.4.3.m1.1.1.1.cmml" xref="S4.SS3.p2.4.3.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p2.4.3.m1.1.1.2.cmml" xref="S4.SS3.p2.4.3.m1.1.1.2">1.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.3.m1.1c">1.3\%</annotation></semantics></math> at the average mAP (row 5). But this effect can be regulated by jointly training all three tasks together using our proposed model (row 6).</span></p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.3" class="ltx_p"><span id="S4.SS3.p3.3.1" class="ltx_text ltx_font_bold">Multi-task learning as pre-training.</span>
Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we finetune each single-task model on our trained AT model to demonstrate that the AT model can allow downstream tasks to take advantage of multi-task training.
The results are shown in row 8 of <a href="#S4.T1" title="In 4.2 Implementation Details ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, <span id="S4.SS3.p3.3.2" class="ltx_text" style="color:#000000;">where we initialize ST models using the trained AT model and finetune them individually using the same training recipe as in row 1.</span>
We can see that the ST models fine-tuned on our AT model outperform the single-task models in row 1 by a large margin, <em id="S4.SS3.p3.3.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS3.p3.3.4" class="ltx_text"></span>, <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="+0.9\%" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mo id="S4.SS3.p3.1.m1.1.1a" xref="S4.SS3.p3.1.m1.1.1.cmml">+</mo><mrow id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml"><mn id="S4.SS3.p3.1.m1.1.1.2.2" xref="S4.SS3.p3.1.m1.1.1.2.2.cmml">0.9</mn><mo id="S4.SS3.p3.1.m1.1.1.2.1" xref="S4.SS3.p3.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><plus id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"></plus><apply id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2"><csymbol cd="latexml" id="S4.SS3.p3.1.m1.1.1.2.1.cmml" xref="S4.SS3.p3.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS3.p3.1.m1.1.1.2.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2.2">0.9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">+0.9\%</annotation></semantics></math>, <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="+2.1\%" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mrow id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mo id="S4.SS3.p3.2.m2.1.1a" xref="S4.SS3.p3.2.m2.1.1.cmml">+</mo><mrow id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml"><mn id="S4.SS3.p3.2.m2.1.1.2.2" xref="S4.SS3.p3.2.m2.1.1.2.2.cmml">2.1</mn><mo id="S4.SS3.p3.2.m2.1.1.2.1" xref="S4.SS3.p3.2.m2.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><plus id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"></plus><apply id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2"><csymbol cd="latexml" id="S4.SS3.p3.2.m2.1.1.2.1.cmml" xref="S4.SS3.p3.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS3.p3.2.m2.1.1.2.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2.2">2.1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">+2.1\%</annotation></semantics></math> and <math id="S4.SS3.p3.3.m3.1" class="ltx_Math" alttext="+3.4\%" display="inline"><semantics id="S4.SS3.p3.3.m3.1a"><mrow id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml"><mo id="S4.SS3.p3.3.m3.1.1a" xref="S4.SS3.p3.3.m3.1.1.cmml">+</mo><mrow id="S4.SS3.p3.3.m3.1.1.2" xref="S4.SS3.p3.3.m3.1.1.2.cmml"><mn id="S4.SS3.p3.3.m3.1.1.2.2" xref="S4.SS3.p3.3.m3.1.1.2.2.cmml">3.4</mn><mo id="S4.SS3.p3.3.m3.1.1.2.1" xref="S4.SS3.p3.3.m3.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><apply id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1"><plus id="S4.SS3.p3.3.m3.1.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1"></plus><apply id="S4.SS3.p3.3.m3.1.1.2.cmml" xref="S4.SS3.p3.3.m3.1.1.2"><csymbol cd="latexml" id="S4.SS3.p3.3.m3.1.1.2.1.cmml" xref="S4.SS3.p3.3.m3.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS3.p3.3.m3.1.1.2.2.cmml" xref="S4.SS3.p3.3.m3.1.1.2.2">3.4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">+3.4\%</annotation></semantics></math> on TAL, AVEL and SED, respectively.
It indicates that joint training can capture knowledge that is mutually beneficial to all these three tasks, being an effective pre-training step for single-task models.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Comparison with Existing Work</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.3" class="ltx_p"><a href="#S4.T2" title="In 4.4 Comparison with Existing Work ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a> presents the comparison results of our model with state-of-the-art works.
For TAL task, we note that many previous TAL methods achieved superior results on ActivityNet 1.3 by combining with the external action classifier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
By contrast, our model has good capabilities in both classification and regression and does not need to rely on any external classification models.
For a fair comparison, we list the results of methods without using external classifiers in <a href="#S4.T2" title="In 4.4 Comparison with Existing Work ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.
<span id="S4.SS4.p1.3.2" class="ltx_text" style="color:#000000;">We simply concatenate audio and visual features as input for those TAL methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> when using both modalities.</span>
<span id="S4.SS4.p1.1.1" class="ltx_text" style="color:#000000;">We can see that, with ONE-PEACE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> features, our single all-task (AT) model reaches an average mAP of <math id="S4.SS4.p1.1.1.m1.1" class="ltx_Math" alttext="36.1\%" display="inline"><semantics id="S4.SS4.p1.1.1.m1.1a"><mrow id="S4.SS4.p1.1.1.m1.1.1" xref="S4.SS4.p1.1.1.m1.1.1.cmml"><mn mathcolor="#000000" id="S4.SS4.p1.1.1.m1.1.1.2" xref="S4.SS4.p1.1.1.m1.1.1.2.cmml">36.1</mn><mo mathcolor="#000000" id="S4.SS4.p1.1.1.m1.1.1.1" xref="S4.SS4.p1.1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.1.m1.1b"><apply id="S4.SS4.p1.1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS4.p1.1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p1.1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.1.m1.1.1.2">36.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.1.m1.1c">36.1\%</annotation></semantics></math>, outperforming state-of-the-art task-specific models.
Here we also trained UnAV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> that is tailored for AVEL task to conduct TAL task, but found the result is much lower than highly specialized TAL models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. It indicates that the task-specific models cannot be generalized effectively to other tasks, while our unified model can achieve superior or on-par performances on all three tasks with good generalizability.
For AVEL task, our AT model gets competitive results to UnAV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> when using the same ONE-PEACE features.</span>
And our single-task fine-tuned model (Ours<sub id="S4.SS4.p1.3.3" class="ltx_sub"><span id="S4.SS4.p1.3.3.1" class="ltx_text ltx_font_italic">AT→ST</span></sub>) further improves the average mAP to <math id="S4.SS4.p1.3.m2.1" class="ltx_Math" alttext="51.7\%" display="inline"><semantics id="S4.SS4.p1.3.m2.1a"><mrow id="S4.SS4.p1.3.m2.1.1" xref="S4.SS4.p1.3.m2.1.1.cmml"><mn id="S4.SS4.p1.3.m2.1.1.2" xref="S4.SS4.p1.3.m2.1.1.2.cmml">51.7</mn><mo id="S4.SS4.p1.3.m2.1.1.1" xref="S4.SS4.p1.3.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m2.1b"><apply id="S4.SS4.p1.3.m2.1.1.cmml" xref="S4.SS4.p1.3.m2.1.1"><csymbol cd="latexml" id="S4.SS4.p1.3.m2.1.1.1.cmml" xref="S4.SS4.p1.3.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p1.3.m2.1.1.2.cmml" xref="S4.SS4.p1.3.m2.1.1.2">51.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m2.1c">51.7\%</annotation></semantics></math>, setting a new state-of-the-art result on the AVEL task.
For SED task, since the traditional SED methods
only support super fine-grained sound spectrograms as input with the evaluation metric not suitable for our unified approach, we implemented Actionformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, TriDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and UnAV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> to conduct SED task. We can see that our AT model outperforms all other methods by a large margin.
<span id="S4.SS4.p1.3.4" class="ltx_text" style="color:#000000;">Overall, we emphasize that our UniAV holds superior or on-par performances on all three tasks using a single unified model, and achieves the best All Tasks Average score compared with other methods, which significantly distinguishes our model from other task-specific models. </span></p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.6.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.7.2" class="ltx_text" style="font-size:90%;">Comparison with existing state-of-the-art methods. We report the mAP at tIoU=0.5 and the average mAP on three tasks. Best results are in <span id="S4.T2.7.2.1" class="ltx_text ltx_font_bold">bold</span> and second best <span id="S4.T2.7.2.2" class="ltx_text ltx_framed ltx_framed_underline">underlined</span>. “OP-V/A” denotes the visual/audio encoder of ONE-PEACE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. “*” denotes that the results
for the AVEL task are from UnAV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
</span></figcaption>
<div id="S4.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:220.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-137.1pt,69.7pt) scale(0.612604752607681,0.612604752607681) ;">
<table id="S4.T2.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.3.1" class="ltx_tr">
<td id="S4.T2.2.2.3.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S4.T2.2.2.3.1.1.1" class="ltx_text">Method</span></td>
<td id="S4.T2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.2.2.3.1.2.1" class="ltx_text">Visual Encoder</span></td>
<td id="S4.T2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.2.2.3.1.3.1" class="ltx_text">Audio Encoder</span></td>
<td id="S4.T2.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">ActivityNet 1.3 (TAL)</td>
<td id="S4.T2.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">UnAV-100 (AVEL)</td>
<td id="S4.T2.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">DESED (SED)</td>
<th id="S4.T2.2.2.3.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T2.2.2.3.1.7.1" class="ltx_text"><span id="S4.T2.2.2.3.1.7.1.1" class="ltx_text"></span> <span id="S4.T2.2.2.3.1.7.1.2" class="ltx_text">
<span id="S4.T2.2.2.3.1.7.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.2.2.3.1.7.1.2.1.1" class="ltx_tr">
<span id="S4.T2.2.2.3.1.7.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.2.2.3.1.7.1.2.1.1.1.1" class="ltx_text" style="font-size:80%;">All Tasks</span></span></span>
<span id="S4.T2.2.2.3.1.7.1.2.1.2" class="ltx_tr">
<span id="S4.T2.2.2.3.1.7.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.2.2.3.1.7.1.2.1.2.1.1" class="ltx_text" style="font-size:80%;">Average</span></span></span>
</span></span> <span id="S4.T2.2.2.3.1.7.1.3" class="ltx_text"></span></span></th>
</tr>
<tr id="S4.T2.2.2.4.2" class="ltx_tr">
<td id="S4.T2.2.2.4.2.1" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="S4.T2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T2.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="S4.T2.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T2.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
</tr>
<tr id="S4.T2.2.2.5.3" class="ltx_tr">
<td id="S4.T2.2.2.5.3.1" class="ltx_td ltx_align_left ltx_border_t">SSN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S4.T2.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_t">I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_t">39.1</td>
<td id="S4.T2.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_t">24.0</td>
<td id="S4.T2.2.2.5.3.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.2.2.5.3.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.2.2.5.3.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.2.2.5.3.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.2.2.5.3.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T2.2.2.6.4" class="ltx_tr">
<td id="S4.T2.2.2.6.4.1" class="ltx_td ltx_align_left">TAL-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S4.T2.2.2.6.4.2" class="ltx_td ltx_align_center">I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.2.2.6.4.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.6.4.4" class="ltx_td ltx_align_center">38.2</td>
<td id="S4.T2.2.2.6.4.5" class="ltx_td ltx_align_center">20.2</td>
<td id="S4.T2.2.2.6.4.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.6.4.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.6.4.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.6.4.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.6.4.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T2.2.2.7.5" class="ltx_tr">
<td id="S4.T2.2.2.7.5.1" class="ltx_td ltx_align_left">P-GCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S4.T2.2.2.7.5.2" class="ltx_td ltx_align_center">I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.2.2.7.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.7.5.4" class="ltx_td ltx_align_center">42.9</td>
<td id="S4.T2.2.2.7.5.5" class="ltx_td ltx_align_center">27.0</td>
<td id="S4.T2.2.2.7.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.7.5.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.7.5.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.7.5.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.7.5.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T2.2.2.8.6" class="ltx_tr">
<td id="S4.T2.2.2.8.6.1" class="ltx_td ltx_align_left">PCG-TAL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S4.T2.2.2.8.6.2" class="ltx_td ltx_align_center">I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.2.2.8.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.8.6.4" class="ltx_td ltx_align_center">42.1</td>
<td id="S4.T2.2.2.8.6.5" class="ltx_td ltx_align_center">27.3</td>
<td id="S4.T2.2.2.8.6.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.8.6.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.8.6.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.8.6.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.8.6.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T2.2.2.9.7" class="ltx_tr">
<td id="S4.T2.2.2.9.7.1" class="ltx_td ltx_align_left">TadTR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>
</td>
<td id="S4.T2.2.2.9.7.2" class="ltx_td ltx_align_center">I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.2.2.9.7.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.9.7.4" class="ltx_td ltx_align_center">43.7</td>
<td id="S4.T2.2.2.9.7.5" class="ltx_td ltx_align_center">29.9</td>
<td id="S4.T2.2.2.9.7.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.9.7.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.9.7.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.9.7.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.9.7.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T2.2.2.10.8" class="ltx_tr">
<td id="S4.T2.2.2.10.8.1" class="ltx_td ltx_align_left">ActionFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
</td>
<td id="S4.T2.2.2.10.8.2" class="ltx_td ltx_align_center">I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.2.2.10.8.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.10.8.4" class="ltx_td ltx_align_center">46.1</td>
<td id="S4.T2.2.2.10.8.5" class="ltx_td ltx_align_center">30.5</td>
<td id="S4.T2.2.2.10.8.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.10.8.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.10.8.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.10.8.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.10.8.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T2.2.2.11.9" class="ltx_tr">
<td id="S4.T2.2.2.11.9.1" class="ltx_td ltx_align_left">TriDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S4.T2.2.2.11.9.2" class="ltx_td ltx_align_center">I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.2.2.11.9.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.11.9.4" class="ltx_td ltx_align_center">48.5</td>
<td id="S4.T2.2.2.11.9.5" class="ltx_td ltx_align_center">31.1</td>
<td id="S4.T2.2.2.11.9.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.11.9.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.11.9.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.11.9.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.11.9.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T2.2.2.12.10" class="ltx_tr">
<td id="S4.T2.2.2.12.10.1" class="ltx_td ltx_align_left">ActionFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
</td>
<td id="S4.T2.2.2.12.10.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.12.10.3" class="ltx_td ltx_align_center">VGGish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S4.T2.2.2.12.10.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.12.10.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.12.10.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.12.10.7" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.12.10.8" class="ltx_td ltx_align_center">39.6</td>
<td id="S4.T2.2.2.12.10.9" class="ltx_td ltx_align_center">37.8</td>
<td id="S4.T2.2.2.12.10.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T2.2.2.13.11" class="ltx_tr">
<td id="S4.T2.2.2.13.11.1" class="ltx_td ltx_align_left ltx_border_t">VSGN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>*</td>
<td id="S4.T2.2.2.13.11.2" class="ltx_td ltx_align_center ltx_border_t">I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.2.2.13.11.3" class="ltx_td ltx_align_center ltx_border_t">VGGish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S4.T2.2.2.13.11.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.2.2.13.11.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.2.2.13.11.6" class="ltx_td ltx_align_center ltx_border_t">24.5</td>
<td id="S4.T2.2.2.13.11.7" class="ltx_td ltx_align_center ltx_border_t">24.1</td>
<td id="S4.T2.2.2.13.11.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.2.2.13.11.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.2.2.13.11.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T2.2.2.14.12" class="ltx_tr">
<td id="S4.T2.2.2.14.12.1" class="ltx_td ltx_align_left">TadTR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>*</td>
<td id="S4.T2.2.2.14.12.2" class="ltx_td ltx_align_center">I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.2.2.14.12.3" class="ltx_td ltx_align_center">VGGish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S4.T2.2.2.14.12.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.14.12.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.14.12.6" class="ltx_td ltx_align_center">30.4</td>
<td id="S4.T2.2.2.14.12.7" class="ltx_td ltx_align_center">29.4</td>
<td id="S4.T2.2.2.14.12.8" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.14.12.9" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.2.2.14.12.10" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T2.2.2.15.13" class="ltx_tr">
<td id="S4.T2.2.2.15.13.1" class="ltx_td ltx_align_left">ActionFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
</td>
<td id="S4.T2.2.2.15.13.2" class="ltx_td ltx_align_center">I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.2.2.15.13.3" class="ltx_td ltx_align_center">VGGish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S4.T2.2.2.15.13.4" class="ltx_td ltx_align_center">47.2</td>
<td id="S4.T2.2.2.15.13.5" class="ltx_td ltx_align_center">31.1</td>
<td id="S4.T2.2.2.15.13.6" class="ltx_td ltx_align_center">43.5</td>
<td id="S4.T2.2.2.15.13.7" class="ltx_td ltx_align_center">42.2</td>
<td id="S4.T2.2.2.15.13.8" class="ltx_td ltx_align_center">42.2</td>
<td id="S4.T2.2.2.15.13.9" class="ltx_td ltx_align_center">39.7</td>
<td id="S4.T2.2.2.15.13.10" class="ltx_td ltx_align_center">37.7</td>
</tr>
<tr id="S4.T2.2.2.16.14" class="ltx_tr">
<td id="S4.T2.2.2.16.14.1" class="ltx_td ltx_align_left">TriDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S4.T2.2.2.16.14.2" class="ltx_td ltx_align_center">I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.2.2.16.14.3" class="ltx_td ltx_align_center">VGGish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S4.T2.2.2.16.14.4" class="ltx_td ltx_align_center">49.3</td>
<td id="S4.T2.2.2.16.14.5" class="ltx_td ltx_align_center">32.1</td>
<td id="S4.T2.2.2.16.14.6" class="ltx_td ltx_align_center">46.2</td>
<td id="S4.T2.2.2.16.14.7" class="ltx_td ltx_align_center">44.4</td>
<td id="S4.T2.2.2.16.14.8" class="ltx_td ltx_align_center">42.0</td>
<td id="S4.T2.2.2.16.14.9" class="ltx_td ltx_align_center">41.2</td>
<td id="S4.T2.2.2.16.14.10" class="ltx_td ltx_align_center">39.2</td>
</tr>
<tr id="S4.T2.2.2.17.15" class="ltx_tr">
<td id="S4.T2.2.2.17.15.1" class="ltx_td ltx_align_left">UnAV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S4.T2.2.2.17.15.2" class="ltx_td ltx_align_center">I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S4.T2.2.2.17.15.3" class="ltx_td ltx_align_center">VGGish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S4.T2.2.2.17.15.4" class="ltx_td ltx_align_center">42.7</td>
<td id="S4.T2.2.2.17.15.5" class="ltx_td ltx_align_center">28.1</td>
<td id="S4.T2.2.2.17.15.6" class="ltx_td ltx_align_center">50.6</td>
<td id="S4.T2.2.2.17.15.7" class="ltx_td ltx_align_center">47.8</td>
<td id="S4.T2.2.2.17.15.8" class="ltx_td ltx_align_center">51.6</td>
<td id="S4.T2.2.2.17.15.9" class="ltx_td ltx_align_center">48.8</td>
<td id="S4.T2.2.2.17.15.10" class="ltx_td ltx_align_center">41.6</td>
</tr>
<tr id="S4.T2.2.2.18.16" class="ltx_tr">
<td id="S4.T2.2.2.18.16.1" class="ltx_td ltx_align_left ltx_border_t">ActionFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
</td>
<td id="S4.T2.2.2.18.16.2" class="ltx_td ltx_align_center ltx_border_t">OP-V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S4.T2.2.2.18.16.3" class="ltx_td ltx_align_center ltx_border_t">OP-A <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S4.T2.2.2.18.16.4" class="ltx_td ltx_align_center ltx_border_t">55.2</td>
<td id="S4.T2.2.2.18.16.5" class="ltx_td ltx_align_center ltx_border_t">35.4</td>
<td id="S4.T2.2.2.18.16.6" class="ltx_td ltx_align_center ltx_border_t">49.2</td>
<td id="S4.T2.2.2.18.16.7" class="ltx_td ltx_align_center ltx_border_t">47.0</td>
<td id="S4.T2.2.2.18.16.8" class="ltx_td ltx_align_center ltx_border_t">48.2</td>
<td id="S4.T2.2.2.18.16.9" class="ltx_td ltx_align_center ltx_border_t">44.6</td>
<td id="S4.T2.2.2.18.16.10" class="ltx_td ltx_align_center ltx_border_t">42.3</td>
</tr>
<tr id="S4.T2.2.2.19.17" class="ltx_tr">
<td id="S4.T2.2.2.19.17.1" class="ltx_td ltx_align_left">TriDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S4.T2.2.2.19.17.2" class="ltx_td ltx_align_center">OP-V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S4.T2.2.2.19.17.3" class="ltx_td ltx_align_center">OP-A <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S4.T2.2.2.19.17.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.19.17.4.1" class="ltx_text ltx_framed ltx_framed_underline">56.9</span></td>
<td id="S4.T2.2.2.19.17.5" class="ltx_td ltx_align_center">35.9</td>
<td id="S4.T2.2.2.19.17.6" class="ltx_td ltx_align_center">49.7</td>
<td id="S4.T2.2.2.19.17.7" class="ltx_td ltx_align_center">47.3</td>
<td id="S4.T2.2.2.19.17.8" class="ltx_td ltx_align_center">48.3</td>
<td id="S4.T2.2.2.19.17.9" class="ltx_td ltx_align_center">46.0</td>
<td id="S4.T2.2.2.19.17.10" class="ltx_td ltx_align_center">43.1</td>
</tr>
<tr id="S4.T2.2.2.20.18" class="ltx_tr">
<td id="S4.T2.2.2.20.18.1" class="ltx_td ltx_align_left">UnAV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S4.T2.2.2.20.18.2" class="ltx_td ltx_align_center">OP-V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S4.T2.2.2.20.18.3" class="ltx_td ltx_align_center">OP-A <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S4.T2.2.2.20.18.4" class="ltx_td ltx_align_center">50.5</td>
<td id="S4.T2.2.2.20.18.5" class="ltx_td ltx_align_center">32.5</td>
<td id="S4.T2.2.2.20.18.6" class="ltx_td ltx_align_center">53.8</td>
<td id="S4.T2.2.2.20.18.7" class="ltx_td ltx_align_center"><span id="S4.T2.2.2.20.18.7.1" class="ltx_text ltx_framed ltx_framed_underline">51.0</span></td>
<td id="S4.T2.2.2.20.18.8" class="ltx_td ltx_align_center">60.9</td>
<td id="S4.T2.2.2.20.18.9" class="ltx_td ltx_align_center">57.8</td>
<td id="S4.T2.2.2.20.18.10" class="ltx_td ltx_align_center">47.1</td>
</tr>
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left">Ours<sub id="S4.T2.1.1.1.1.1" class="ltx_sub"><span id="S4.T2.1.1.1.1.1.1" class="ltx_text ltx_font_italic">AT</span></sub>
</td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center">OP-V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center">OP-A <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">57.1</span></td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.5.1" class="ltx_text ltx_framed ltx_framed_underline">36.1</span></td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.6.1" class="ltx_text ltx_framed ltx_framed_underline">54.1</span></td>
<td id="S4.T2.1.1.1.7" class="ltx_td ltx_align_center">50.7</td>
<td id="S4.T2.1.1.1.8" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.8.1" class="ltx_text ltx_framed ltx_framed_underline">64.0</span></td>
<td id="S4.T2.1.1.1.9" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.9.1" class="ltx_text ltx_framed ltx_framed_underline">59.8</span></td>
<td id="S4.T2.1.1.1.10" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.10.1" class="ltx_text ltx_framed ltx_framed_underline">48.9</span></td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.1" class="ltx_td ltx_align_left ltx_border_bb">Ours<sub id="S4.T2.2.2.2.1.1" class="ltx_sub"><span id="S4.T2.2.2.2.1.1.1" class="ltx_text ltx_font_italic">AT→ST</span></sub>
</td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_bb">OP-V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_bb">OP-A <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_bb">56.8</td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.2.5.1" class="ltx_text ltx_font_bold">36.2</span></td>
<td id="S4.T2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.2.6.1" class="ltx_text ltx_font_bold">54.8</span></td>
<td id="S4.T2.2.2.2.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.2.7.1" class="ltx_text ltx_font_bold">51.7</span></td>
<td id="S4.T2.2.2.2.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.2.8.1" class="ltx_text ltx_font_bold">65.1</span></td>
<td id="S4.T2.2.2.2.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.2.9.1" class="ltx_text ltx_font_bold">61.1</span></td>
<td id="S4.T2.2.2.2.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.2.2.2.10.1" class="ltx_text ltx_font_bold">49.7</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Studies</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.2" class="ltx_p"><span id="S4.SS5.p1.2.1" class="ltx_text ltx_font_bold">Effect of task-specific experts.</span>
To verify our design choices, we explore the effect of applying task-specific experts on different layers of the pyramid transformer.
As shown in <a href="#S4.T3" title="In 4.5 Ablation Studies ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, applying task-specific experts brings a significant performance boost.
Specifically, using experts in the later <math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><msub id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml"><mi id="S4.SS5.p1.1.m1.1.1.2" xref="S4.SS5.p1.1.m1.1.1.2.cmml">L</mi><mn id="S4.SS5.p1.1.m1.1.1.3" xref="S4.SS5.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><apply id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.1.m1.1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS5.p1.1.m1.1.1.2.cmml" xref="S4.SS5.p1.1.m1.1.1.2">𝐿</ci><cn type="integer" id="S4.SS5.p1.1.m1.1.1.3.cmml" xref="S4.SS5.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">L_{2}</annotation></semantics></math> blocks (row 3) has better results than that in the early <math id="S4.SS5.p1.2.m2.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S4.SS5.p1.2.m2.1a"><msub id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml"><mi id="S4.SS5.p1.2.m2.1.1.2" xref="S4.SS5.p1.2.m2.1.1.2.cmml">L</mi><mn id="S4.SS5.p1.2.m2.1.1.3" xref="S4.SS5.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><apply id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS5.p1.2.m2.1.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS5.p1.2.m2.1.1.2.cmml" xref="S4.SS5.p1.2.m2.1.1.2">𝐿</ci><cn type="integer" id="S4.SS5.p1.2.m2.1.1.3.cmml" xref="S4.SS5.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">L_{1}</annotation></semantics></math> blocks (row 2), which indicates that the later stages of the model can capture distinct knowledge that is more beneficial for each task.
Besides, adding experts in all transformer blocks (row 6) achieves the best performances on two of three tasks (<em id="S4.SS5.p1.2.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS5.p1.2.3" class="ltx_text"></span>, TAL and SED).</p>
</div>
<div id="S4.SS5.p2" class="ltx_para ltx_noindent">
<p id="S4.SS5.p2.1" class="ltx_p"><span id="S4.SS5.p2.1.2" class="ltx_text ltx_font_bold">Language-aware classification head.</span>
We also perform ablations on our proposed language-aware classification head (LCH) as shown in <a href="#S4.T3" title="In 4.5 Ablation Studies ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>.
We find that using task-specific classifiers (row 4) leads to quite unstable results on three tasks.
By contrast, notable improvements are observed on TAL and SED tasks when we use LCH with only categories as text tokens (row 5). <span id="S4.SS5.p2.1.1" class="ltx_text" style="color:#000000;">Especially for SED, there is a significant <math id="S4.SS5.p2.1.1.m1.1" class="ltx_Math" alttext="4.5\%" display="inline"><semantics id="S4.SS5.p2.1.1.m1.1a"><mrow id="S4.SS5.p2.1.1.m1.1.1" xref="S4.SS5.p2.1.1.m1.1.1.cmml"><mn mathcolor="#000000" id="S4.SS5.p2.1.1.m1.1.1.2" xref="S4.SS5.p2.1.1.m1.1.1.2.cmml">4.5</mn><mo mathcolor="#000000" id="S4.SS5.p2.1.1.m1.1.1.1" xref="S4.SS5.p2.1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.1.m1.1b"><apply id="S4.SS5.p2.1.1.m1.1.1.cmml" xref="S4.SS5.p2.1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS5.p2.1.1.m1.1.1.1.cmml" xref="S4.SS5.p2.1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS5.p2.1.1.m1.1.1.2.cmml" xref="S4.SS5.p2.1.1.m1.1.1.2">4.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.1.m1.1c">4.5\%</annotation></semantics></math> increase at average mAP. It could be attributed to the unified LCH based on the large language encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, which enhances the model’s generalization, effectively avoiding overfitting on the quite small dataset.</span>
Moreover, using prompts to add context information mentioned in Sec. <a href="#S3.SS3" title="3.3 Head Design ‣ 3 The UniAV Framework ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> can further improve the performance on two of the three tasks.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para ltx_noindent">
<p id="S4.SS5.p3.2" class="ltx_p"><span id="S4.SS5.p3.2.1" class="ltx_text ltx_font_bold">Audio-visual fusion for TAL and SED.</span>
We also verify the effectiveness of audio-visual fusion for TAL and SED tasks in <a href="#S4.T4.st1" title="In Table 4 ‣ 4.5 Ablation Studies ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4(a)</span></a>. We use the single-task model (row 1 in <a href="#S4.T1" title="In 4.2 Implementation Details ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>) for each task to conduct the experiments.
For TAL, we can see the performance increase (<math id="S4.SS5.p3.1.m1.1" class="ltx_Math" alttext="+1.1\%" display="inline"><semantics id="S4.SS5.p3.1.m1.1a"><mrow id="S4.SS5.p3.1.m1.1.1" xref="S4.SS5.p3.1.m1.1.1.cmml"><mo id="S4.SS5.p3.1.m1.1.1a" xref="S4.SS5.p3.1.m1.1.1.cmml">+</mo><mrow id="S4.SS5.p3.1.m1.1.1.2" xref="S4.SS5.p3.1.m1.1.1.2.cmml"><mn id="S4.SS5.p3.1.m1.1.1.2.2" xref="S4.SS5.p3.1.m1.1.1.2.2.cmml">1.1</mn><mo id="S4.SS5.p3.1.m1.1.1.2.1" xref="S4.SS5.p3.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.1.m1.1b"><apply id="S4.SS5.p3.1.m1.1.1.cmml" xref="S4.SS5.p3.1.m1.1.1"><plus id="S4.SS5.p3.1.m1.1.1.1.cmml" xref="S4.SS5.p3.1.m1.1.1"></plus><apply id="S4.SS5.p3.1.m1.1.1.2.cmml" xref="S4.SS5.p3.1.m1.1.1.2"><csymbol cd="latexml" id="S4.SS5.p3.1.m1.1.1.2.1.cmml" xref="S4.SS5.p3.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS5.p3.1.m1.1.1.2.2.cmml" xref="S4.SS5.p3.1.m1.1.1.2.2">1.1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.1.m1.1c">+1.1\%</annotation></semantics></math> at average mAP) as we insert audio signals to apply cross-modal interactions.
For SED, the model obtains a substantial performance boost (<math id="S4.SS5.p3.2.m2.1" class="ltx_Math" alttext="+8.2\%" display="inline"><semantics id="S4.SS5.p3.2.m2.1a"><mrow id="S4.SS5.p3.2.m2.1.1" xref="S4.SS5.p3.2.m2.1.1.cmml"><mo id="S4.SS5.p3.2.m2.1.1a" xref="S4.SS5.p3.2.m2.1.1.cmml">+</mo><mrow id="S4.SS5.p3.2.m2.1.1.2" xref="S4.SS5.p3.2.m2.1.1.2.cmml"><mn id="S4.SS5.p3.2.m2.1.1.2.2" xref="S4.SS5.p3.2.m2.1.1.2.2.cmml">8.2</mn><mo id="S4.SS5.p3.2.m2.1.1.2.1" xref="S4.SS5.p3.2.m2.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.2.m2.1b"><apply id="S4.SS5.p3.2.m2.1.1.cmml" xref="S4.SS5.p3.2.m2.1.1"><plus id="S4.SS5.p3.2.m2.1.1.1.cmml" xref="S4.SS5.p3.2.m2.1.1"></plus><apply id="S4.SS5.p3.2.m2.1.1.2.cmml" xref="S4.SS5.p3.2.m2.1.1.2"><csymbol cd="latexml" id="S4.SS5.p3.2.m2.1.1.2.1.cmml" xref="S4.SS5.p3.2.m2.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS5.p3.2.m2.1.1.2.2.cmml" xref="S4.SS5.p3.2.m2.1.1.2.2">8.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.2.m2.1c">+8.2\%</annotation></semantics></math> at average mAP) when adding visual modalities, indicating the critical role of both modalities for the task.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para ltx_noindent">
<p id="S4.SS5.p4.2" class="ltx_p"><span id="S4.SS5.p4.2.1" class="ltx_text ltx_font_bold">Effect of different visual encoders.</span>
As shown in <a href="#S4.T4.st2" title="In Table 4 ‣ 4.5 Ablation Studies ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4(b)</span></a>, we compare the performances of our models using different visual encoders. For both single-task (ST) and multi-task (AT) models, the performances improve by a large margin when using the visual encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> fine-tuned on Kinetics-400 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In particular, for TAL, the improvements of <math id="S4.SS5.p4.1.m1.1" class="ltx_Math" alttext="3.4\%" display="inline"><semantics id="S4.SS5.p4.1.m1.1a"><mrow id="S4.SS5.p4.1.m1.1.1" xref="S4.SS5.p4.1.m1.1.1.cmml"><mn id="S4.SS5.p4.1.m1.1.1.2" xref="S4.SS5.p4.1.m1.1.1.2.cmml">3.4</mn><mo id="S4.SS5.p4.1.m1.1.1.1" xref="S4.SS5.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.1.m1.1b"><apply id="S4.SS5.p4.1.m1.1.1.cmml" xref="S4.SS5.p4.1.m1.1.1"><csymbol cd="latexml" id="S4.SS5.p4.1.m1.1.1.1.cmml" xref="S4.SS5.p4.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS5.p4.1.m1.1.1.2.cmml" xref="S4.SS5.p4.1.m1.1.1.2">3.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.1.m1.1c">3.4\%</annotation></semantics></math> and <math id="S4.SS5.p4.2.m2.1" class="ltx_Math" alttext="3.5\%" display="inline"><semantics id="S4.SS5.p4.2.m2.1a"><mrow id="S4.SS5.p4.2.m2.1.1" xref="S4.SS5.p4.2.m2.1.1.cmml"><mn id="S4.SS5.p4.2.m2.1.1.2" xref="S4.SS5.p4.2.m2.1.1.2.cmml">3.5</mn><mo id="S4.SS5.p4.2.m2.1.1.1" xref="S4.SS5.p4.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.2.m2.1b"><apply id="S4.SS5.p4.2.m2.1.1.cmml" xref="S4.SS5.p4.2.m2.1.1"><csymbol cd="latexml" id="S4.SS5.p4.2.m2.1.1.1.cmml" xref="S4.SS5.p4.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS5.p4.2.m2.1.1.2.cmml" xref="S4.SS5.p4.2.m2.1.1.2">3.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.2.m2.1c">3.5\%</annotation></semantics></math> at average mAP can be observed for the ST and AT models, respectively. This emphasizes the importance of motion information for video localization tasks, especially for the TAL task.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.12.5.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.8.4" class="ltx_text" style="font-size:90%;">Ablation study on the main proposed components.
“E-<math id="S4.T3.5.1.m1.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S4.T3.5.1.m1.1b"><msub id="S4.T3.5.1.m1.1.1" xref="S4.T3.5.1.m1.1.1.cmml"><mi id="S4.T3.5.1.m1.1.1.2" xref="S4.T3.5.1.m1.1.1.2.cmml">L</mi><mn id="S4.T3.5.1.m1.1.1.3" xref="S4.T3.5.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.5.1.m1.1c"><apply id="S4.T3.5.1.m1.1.1.cmml" xref="S4.T3.5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.5.1.m1.1.1.1.cmml" xref="S4.T3.5.1.m1.1.1">subscript</csymbol><ci id="S4.T3.5.1.m1.1.1.2.cmml" xref="S4.T3.5.1.m1.1.1.2">𝐿</ci><cn type="integer" id="S4.T3.5.1.m1.1.1.3.cmml" xref="S4.T3.5.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.1.m1.1d">L_{1}</annotation></semantics></math>” denotes applying task-specific experts on the early <math id="S4.T3.6.2.m2.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S4.T3.6.2.m2.1b"><msub id="S4.T3.6.2.m2.1.1" xref="S4.T3.6.2.m2.1.1.cmml"><mi id="S4.T3.6.2.m2.1.1.2" xref="S4.T3.6.2.m2.1.1.2.cmml">L</mi><mn id="S4.T3.6.2.m2.1.1.3" xref="S4.T3.6.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.6.2.m2.1c"><apply id="S4.T3.6.2.m2.1.1.cmml" xref="S4.T3.6.2.m2.1.1"><csymbol cd="ambiguous" id="S4.T3.6.2.m2.1.1.1.cmml" xref="S4.T3.6.2.m2.1.1">subscript</csymbol><ci id="S4.T3.6.2.m2.1.1.2.cmml" xref="S4.T3.6.2.m2.1.1.2">𝐿</ci><cn type="integer" id="S4.T3.6.2.m2.1.1.3.cmml" xref="S4.T3.6.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.2.m2.1d">L_{1}</annotation></semantics></math> transformer blocks, and “E-<math id="S4.T3.7.3.m3.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S4.T3.7.3.m3.1b"><msub id="S4.T3.7.3.m3.1.1" xref="S4.T3.7.3.m3.1.1.cmml"><mi id="S4.T3.7.3.m3.1.1.2" xref="S4.T3.7.3.m3.1.1.2.cmml">L</mi><mn id="S4.T3.7.3.m3.1.1.3" xref="S4.T3.7.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.7.3.m3.1c"><apply id="S4.T3.7.3.m3.1.1.cmml" xref="S4.T3.7.3.m3.1.1"><csymbol cd="ambiguous" id="S4.T3.7.3.m3.1.1.1.cmml" xref="S4.T3.7.3.m3.1.1">subscript</csymbol><ci id="S4.T3.7.3.m3.1.1.2.cmml" xref="S4.T3.7.3.m3.1.1.2">𝐿</ci><cn type="integer" id="S4.T3.7.3.m3.1.1.3.cmml" xref="S4.T3.7.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.3.m3.1d">L_{2}</annotation></semantics></math>” denotes applying experts on the later <math id="S4.T3.8.4.m4.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S4.T3.8.4.m4.1b"><msub id="S4.T3.8.4.m4.1.1" xref="S4.T3.8.4.m4.1.1.cmml"><mi id="S4.T3.8.4.m4.1.1.2" xref="S4.T3.8.4.m4.1.1.2.cmml">L</mi><mn id="S4.T3.8.4.m4.1.1.3" xref="S4.T3.8.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.8.4.m4.1c"><apply id="S4.T3.8.4.m4.1.1.cmml" xref="S4.T3.8.4.m4.1.1"><csymbol cd="ambiguous" id="S4.T3.8.4.m4.1.1.1.cmml" xref="S4.T3.8.4.m4.1.1">subscript</csymbol><ci id="S4.T3.8.4.m4.1.1.2.cmml" xref="S4.T3.8.4.m4.1.1.2">𝐿</ci><cn type="integer" id="S4.T3.8.4.m4.1.1.3.cmml" xref="S4.T3.8.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.4.m4.1d">L_{2}</annotation></semantics></math> blocks.
“LCH” is short for language-aware classification head. “Prompt” denotes using prompts when tokenizing instance categories for different tasks. </span></figcaption>
<div id="S4.T3.10" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.5pt;height:108.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-48.9pt,17.6pt) scale(0.756225656238266,0.756225656238266) ;">
<table id="S4.T3.10.2" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.10.2.3" class="ltx_tr">
<td id="S4.T3.10.2.3.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T3.10.2.3.2" class="ltx_td ltx_border_tt"></td>
<td id="S4.T3.10.2.3.3" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T3.10.2.3.4" class="ltx_td ltx_border_tt"></td>
<td id="S4.T3.10.2.3.5" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T3.10.2.3.6" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">TAL</td>
<td id="S4.T3.10.2.3.7" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">AVEL</td>
<td id="S4.T3.10.2.3.8" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">SED</td>
<td id="S4.T3.10.2.3.9" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S4.T3.10.2.2" class="ltx_tr">
<td id="S4.T3.10.2.2.3" class="ltx_td"></td>
<td id="S4.T3.9.1.1.1" class="ltx_td ltx_align_center">E-<math id="S4.T3.9.1.1.1.m1.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S4.T3.9.1.1.1.m1.1a"><msub id="S4.T3.9.1.1.1.m1.1.1" xref="S4.T3.9.1.1.1.m1.1.1.cmml"><mi id="S4.T3.9.1.1.1.m1.1.1.2" xref="S4.T3.9.1.1.1.m1.1.1.2.cmml">L</mi><mn id="S4.T3.9.1.1.1.m1.1.1.3" xref="S4.T3.9.1.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.9.1.1.1.m1.1b"><apply id="S4.T3.9.1.1.1.m1.1.1.cmml" xref="S4.T3.9.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.9.1.1.1.m1.1.1.1.cmml" xref="S4.T3.9.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T3.9.1.1.1.m1.1.1.2.cmml" xref="S4.T3.9.1.1.1.m1.1.1.2">𝐿</ci><cn type="integer" id="S4.T3.9.1.1.1.m1.1.1.3.cmml" xref="S4.T3.9.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.1.1.1.m1.1c">L_{1}</annotation></semantics></math>
</td>
<td id="S4.T3.10.2.2.2" class="ltx_td ltx_align_center ltx_border_r">E-<math id="S4.T3.10.2.2.2.m1.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S4.T3.10.2.2.2.m1.1a"><msub id="S4.T3.10.2.2.2.m1.1.1" xref="S4.T3.10.2.2.2.m1.1.1.cmml"><mi id="S4.T3.10.2.2.2.m1.1.1.2" xref="S4.T3.10.2.2.2.m1.1.1.2.cmml">L</mi><mn id="S4.T3.10.2.2.2.m1.1.1.3" xref="S4.T3.10.2.2.2.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T3.10.2.2.2.m1.1b"><apply id="S4.T3.10.2.2.2.m1.1.1.cmml" xref="S4.T3.10.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.10.2.2.2.m1.1.1.1.cmml" xref="S4.T3.10.2.2.2.m1.1.1">subscript</csymbol><ci id="S4.T3.10.2.2.2.m1.1.1.2.cmml" xref="S4.T3.10.2.2.2.m1.1.1.2">𝐿</ci><cn type="integer" id="S4.T3.10.2.2.2.m1.1.1.3.cmml" xref="S4.T3.10.2.2.2.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.2.2.2.m1.1c">L_{2}</annotation></semantics></math>
</td>
<td id="S4.T3.10.2.2.4" class="ltx_td ltx_align_center">LCH</td>
<td id="S4.T3.10.2.2.5" class="ltx_td ltx_align_center ltx_border_r">Prompt</td>
<td id="S4.T3.10.2.2.6" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T3.10.2.2.7" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="S4.T3.10.2.2.8" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T3.10.2.2.9" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="S4.T3.10.2.2.10" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T3.10.2.2.11" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="S4.T3.10.2.2.12" class="ltx_td ltx_align_center">#params</td>
</tr>
<tr id="S4.T3.10.2.4" class="ltx_tr">
<td id="S4.T3.10.2.4.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.10.2.4.1.1" class="ltx_text" style="color:#808080;">1</span></td>
<td id="S4.T3.10.2.4.2" class="ltx_td ltx_border_t"></td>
<td id="S4.T3.10.2.4.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T3.10.2.4.4" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T3.10.2.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T3.10.2.4.6" class="ltx_td ltx_align_center ltx_border_t">55.8</td>
<td id="S4.T3.10.2.4.7" class="ltx_td ltx_align_center ltx_border_t">35.0</td>
<td id="S4.T3.10.2.4.8" class="ltx_td ltx_align_center ltx_border_t">52.9</td>
<td id="S4.T3.10.2.4.9" class="ltx_td ltx_align_center ltx_border_t">49.9</td>
<td id="S4.T3.10.2.4.10" class="ltx_td ltx_align_center ltx_border_t">63.5</td>
<td id="S4.T3.10.2.4.11" class="ltx_td ltx_align_center ltx_border_t">59.7</td>
<td id="S4.T3.10.2.4.12" class="ltx_td ltx_align_center ltx_border_t">64M</td>
</tr>
<tr id="S4.T3.10.2.5" class="ltx_tr">
<td id="S4.T3.10.2.5.1" class="ltx_td ltx_align_center"><span id="S4.T3.10.2.5.1.1" class="ltx_text" style="color:#808080;">2</span></td>
<td id="S4.T3.10.2.5.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.10.2.5.3" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.10.2.5.4" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.10.2.5.5" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T3.10.2.5.6" class="ltx_td ltx_align_center">56.9</td>
<td id="S4.T3.10.2.5.7" class="ltx_td ltx_align_center">35.9</td>
<td id="S4.T3.10.2.5.8" class="ltx_td ltx_align_center">53.7</td>
<td id="S4.T3.10.2.5.9" class="ltx_td ltx_align_center">50.5</td>
<td id="S4.T3.10.2.5.10" class="ltx_td ltx_align_center">61.4</td>
<td id="S4.T3.10.2.5.11" class="ltx_td ltx_align_center">58.5</td>
<td id="S4.T3.10.2.5.12" class="ltx_td ltx_align_center">80M</td>
</tr>
<tr id="S4.T3.10.2.6" class="ltx_tr">
<td id="S4.T3.10.2.6.1" class="ltx_td ltx_align_center"><span id="S4.T3.10.2.6.1.1" class="ltx_text" style="color:#808080;">3</span></td>
<td id="S4.T3.10.2.6.2" class="ltx_td"></td>
<td id="S4.T3.10.2.6.3" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T3.10.2.6.4" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.10.2.6.5" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T3.10.2.6.6" class="ltx_td ltx_align_center">56.8</td>
<td id="S4.T3.10.2.6.7" class="ltx_td ltx_align_center">35.9</td>
<td id="S4.T3.10.2.6.8" class="ltx_td ltx_align_center">54.5</td>
<td id="S4.T3.10.2.6.9" class="ltx_td ltx_align_center">50.8</td>
<td id="S4.T3.10.2.6.10" class="ltx_td ltx_align_center">62.9</td>
<td id="S4.T3.10.2.6.11" class="ltx_td ltx_align_center">59.3</td>
<td id="S4.T3.10.2.6.12" class="ltx_td ltx_align_center">114M</td>
</tr>
<tr id="S4.T3.10.2.7" class="ltx_tr">
<td id="S4.T3.10.2.7.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.10.2.7.1.1" class="ltx_text" style="color:#808080;">4</span></td>
<td id="S4.T3.10.2.7.2" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T3.10.2.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T3.10.2.7.4" class="ltx_td ltx_border_t"></td>
<td id="S4.T3.10.2.7.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T3.10.2.7.6" class="ltx_td ltx_align_center ltx_border_t">56.2</td>
<td id="S4.T3.10.2.7.7" class="ltx_td ltx_align_center ltx_border_t">35.2</td>
<td id="S4.T3.10.2.7.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.10.2.7.8.1" class="ltx_text ltx_font_bold">54.6</span></td>
<td id="S4.T3.10.2.7.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.10.2.7.9.1" class="ltx_text ltx_font_bold">51.2</span></td>
<td id="S4.T3.10.2.7.10" class="ltx_td ltx_align_center ltx_border_t">57.4</td>
<td id="S4.T3.10.2.7.11" class="ltx_td ltx_align_center ltx_border_t">54.8</td>
<td id="S4.T3.10.2.7.12" class="ltx_td ltx_align_center ltx_border_t">133M</td>
</tr>
<tr id="S4.T3.10.2.8" class="ltx_tr">
<td id="S4.T3.10.2.8.1" class="ltx_td ltx_align_center"><span id="S4.T3.10.2.8.1.1" class="ltx_text" style="color:#808080;">5</span></td>
<td id="S4.T3.10.2.8.2" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.10.2.8.3" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T3.10.2.8.4" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T3.10.2.8.5" class="ltx_td ltx_border_r"></td>
<td id="S4.T3.10.2.8.6" class="ltx_td ltx_align_center">57.0</td>
<td id="S4.T3.10.2.8.7" class="ltx_td ltx_align_center">35.9</td>
<td id="S4.T3.10.2.8.8" class="ltx_td ltx_align_center">54.2</td>
<td id="S4.T3.10.2.8.9" class="ltx_td ltx_align_center">51.0</td>
<td id="S4.T3.10.2.8.10" class="ltx_td ltx_align_center">63.7</td>
<td id="S4.T3.10.2.8.11" class="ltx_td ltx_align_center">59.7</td>
<td id="S4.T3.10.2.8.12" class="ltx_td ltx_align_center">130M</td>
</tr>
<tr id="S4.T3.10.2.9" class="ltx_tr">
<td id="S4.T3.10.2.9.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.10.2.9.1.1" class="ltx_text" style="color:#808080;">6</span></td>
<td id="S4.T3.10.2.9.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">✓</td>
<td id="S4.T3.10.2.9.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">✓</td>
<td id="S4.T3.10.2.9.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">✓</td>
<td id="S4.T3.10.2.9.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">✓</td>
<td id="S4.T3.10.2.9.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.10.2.9.6.1" class="ltx_text ltx_font_bold">57.1</span></td>
<td id="S4.T3.10.2.9.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.10.2.9.7.1" class="ltx_text ltx_font_bold">36.1</span></td>
<td id="S4.T3.10.2.9.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">54.1</td>
<td id="S4.T3.10.2.9.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">50.7</td>
<td id="S4.T3.10.2.9.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.10.2.9.10.1" class="ltx_text ltx_font_bold">64.0</span></td>
<td id="S4.T3.10.2.9.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.10.2.9.11.1" class="ltx_text ltx_font_bold">59.8</span></td>
<td id="S4.T3.10.2.9.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">130M</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S4.T4" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.3.2" class="ltx_text" style="font-size:90%;">Ablation study on the effect of audio-visual fusion for TAL and SED tasks (<a href="#S4.T4.st1" title="In Table 4 ‣ 4.5 Ablation Studies ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4(a)</span></a>) and different visual embeddings for all three tasks (<a href="#S4.T4.st2" title="In Table 4 ‣ 4.5 Ablation Studies ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4(b)</span></a>).</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T4.st1" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.st1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.T4.st1.3.2" class="ltx_text" style="font-size:90%;">
“V” denotes visual-only, “A” denotes audio-only, and “A&amp;V”denotes both audio and visual.</span></figcaption>
<div id="S4.T4.st1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:338.2pt;height:174.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(64.7pt,-33.5pt) scale(1.61979625563739,1.61979625563739) ;">
<table id="S4.T4.st1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.st1.4.1.1" class="ltx_tr">
<td id="S4.T4.st1.4.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="3"><span id="S4.T4.st1.4.1.1.1.1" class="ltx_text">TAL</span></td>
<td id="S4.T4.st1.4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Modality</td>
<td id="S4.T4.st1.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">0.5</td>
<td id="S4.T4.st1.4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">0.75</td>
<td id="S4.T4.st1.4.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">0.95</td>
<td id="S4.T4.st1.4.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Avg.</td>
</tr>
<tr id="S4.T4.st1.4.1.2" class="ltx_tr">
<td id="S4.T4.st1.4.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">V</td>
<td id="S4.T4.st1.4.1.2.2" class="ltx_td ltx_align_center ltx_border_t">55.0</td>
<td id="S4.T4.st1.4.1.2.3" class="ltx_td ltx_align_center ltx_border_t">35.0</td>
<td id="S4.T4.st1.4.1.2.4" class="ltx_td ltx_align_center ltx_border_t">4.8</td>
<td id="S4.T4.st1.4.1.2.5" class="ltx_td ltx_align_center ltx_border_t">34.2</td>
</tr>
<tr id="S4.T4.st1.4.1.3" class="ltx_tr">
<td id="S4.T4.st1.4.1.3.1" class="ltx_td ltx_align_center ltx_border_r">A&amp;V</td>
<td id="S4.T4.st1.4.1.3.2" class="ltx_td ltx_align_center"><span id="S4.T4.st1.4.1.3.2.1" class="ltx_text ltx_font_bold">56.6</span></td>
<td id="S4.T4.st1.4.1.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.st1.4.1.3.3.1" class="ltx_text ltx_font_bold">35.4</span></td>
<td id="S4.T4.st1.4.1.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.st1.4.1.3.4.1" class="ltx_text ltx_font_bold">5.1</span></td>
<td id="S4.T4.st1.4.1.3.5" class="ltx_td ltx_align_center"><span id="S4.T4.st1.4.1.3.5.1" class="ltx_text ltx_font_bold">35.3</span></td>
</tr>
<tr id="S4.T4.st1.4.1.4" class="ltx_tr">
<td id="S4.T4.st1.4.1.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="4"><span id="S4.T4.st1.4.1.4.1.1" class="ltx_text">SED</span></td>
<td id="S4.T4.st1.4.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Modality</td>
<td id="S4.T4.st1.4.1.4.3" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T4.st1.4.1.4.4" class="ltx_td ltx_align_center ltx_border_t">0.7</td>
<td id="S4.T4.st1.4.1.4.5" class="ltx_td ltx_align_center ltx_border_t">0.9</td>
<td id="S4.T4.st1.4.1.4.6" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
</tr>
<tr id="S4.T4.st1.4.1.5" class="ltx_tr">
<td id="S4.T4.st1.4.1.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">A</td>
<td id="S4.T4.st1.4.1.5.2" class="ltx_td ltx_align_center ltx_border_t">51.4</td>
<td id="S4.T4.st1.4.1.5.3" class="ltx_td ltx_align_center ltx_border_t">38.1</td>
<td id="S4.T4.st1.4.1.5.4" class="ltx_td ltx_align_center ltx_border_t">13.9</td>
<td id="S4.T4.st1.4.1.5.5" class="ltx_td ltx_align_center ltx_border_t">49.5</td>
</tr>
<tr id="S4.T4.st1.4.1.6" class="ltx_tr">
<td id="S4.T4.st1.4.1.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">A&amp;V</td>
<td id="S4.T4.st1.4.1.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.st1.4.1.6.2.1" class="ltx_text ltx_font_bold">61.0</span></td>
<td id="S4.T4.st1.4.1.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.st1.4.1.6.3.1" class="ltx_text ltx_font_bold">45.9</span></td>
<td id="S4.T4.st1.4.1.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.st1.4.1.6.4.1" class="ltx_text ltx_font_bold">19.8</span></td>
<td id="S4.T4.st1.4.1.6.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.st1.4.1.6.5.1" class="ltx_text ltx_font_bold">57.7</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T4.st2" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.st2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.T4.st2.3.2" class="ltx_text" style="font-size:90%;">
“FT” denotes using the visual encoder of ONE-PEACE fine-tuned on Kinetics-400.</span></figcaption>
<div id="S4.T4.st2.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:165.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(67.5pt,-28.5pt) scale(1.52857880719356,1.52857880719356) ;">
<table id="S4.T4.st2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.st2.4.1.1" class="ltx_tr">
<td id="S4.T4.st2.4.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T4.st2.4.1.1.2" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T4.st2.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">TAL</td>
<td id="S4.T4.st2.4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">AVEL</td>
<td id="S4.T4.st2.4.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">SED</td>
</tr>
<tr id="S4.T4.st2.4.1.2" class="ltx_tr">
<td id="S4.T4.st2.4.1.2.1" class="ltx_td ltx_align_center">Model</td>
<td id="S4.T4.st2.4.1.2.2" class="ltx_td ltx_align_center ltx_border_r">FT</td>
<td id="S4.T4.st2.4.1.2.3" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T4.st2.4.1.2.4" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="S4.T4.st2.4.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T4.st2.4.1.2.6" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="S4.T4.st2.4.1.2.7" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="S4.T4.st2.4.1.2.8" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
</tr>
<tr id="S4.T4.st2.4.1.3" class="ltx_tr">
<td id="S4.T4.st2.4.1.3.1" class="ltx_td ltx_align_center ltx_border_t">ST</td>
<td id="S4.T4.st2.4.1.3.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T4.st2.4.1.3.3" class="ltx_td ltx_align_center ltx_border_t">50.5</td>
<td id="S4.T4.st2.4.1.3.4" class="ltx_td ltx_align_center ltx_border_t">31.9</td>
<td id="S4.T4.st2.4.1.3.5" class="ltx_td ltx_align_center ltx_border_t">51.4</td>
<td id="S4.T4.st2.4.1.3.6" class="ltx_td ltx_align_center ltx_border_t">48.4</td>
<td id="S4.T4.st2.4.1.3.7" class="ltx_td ltx_align_center ltx_border_t">58.7</td>
<td id="S4.T4.st2.4.1.3.8" class="ltx_td ltx_align_center ltx_border_t">55.9</td>
</tr>
<tr id="S4.T4.st2.4.1.4" class="ltx_tr">
<td id="S4.T4.st2.4.1.4.1" class="ltx_td ltx_align_center">ST</td>
<td id="S4.T4.st2.4.1.4.2" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T4.st2.4.1.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.st2.4.1.4.3.1" class="ltx_text ltx_font_bold">56.6</span></td>
<td id="S4.T4.st2.4.1.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.st2.4.1.4.4.1" class="ltx_text ltx_font_bold">35.3</span></td>
<td id="S4.T4.st2.4.1.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.st2.4.1.4.5.1" class="ltx_text ltx_font_bold">53.2</span></td>
<td id="S4.T4.st2.4.1.4.6" class="ltx_td ltx_align_center"><span id="S4.T4.st2.4.1.4.6.1" class="ltx_text ltx_font_bold">49.6</span></td>
<td id="S4.T4.st2.4.1.4.7" class="ltx_td ltx_align_center"><span id="S4.T4.st2.4.1.4.7.1" class="ltx_text ltx_font_bold">61.0</span></td>
<td id="S4.T4.st2.4.1.4.8" class="ltx_td ltx_align_center"><span id="S4.T4.st2.4.1.4.8.1" class="ltx_text ltx_font_bold">57.7</span></td>
</tr>
<tr id="S4.T4.st2.4.1.5" class="ltx_tr">
<td id="S4.T4.st2.4.1.5.1" class="ltx_td ltx_align_center ltx_border_t">AT</td>
<td id="S4.T4.st2.4.1.5.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T4.st2.4.1.5.3" class="ltx_td ltx_align_center ltx_border_t">51.7</td>
<td id="S4.T4.st2.4.1.5.4" class="ltx_td ltx_align_center ltx_border_t">32.6</td>
<td id="S4.T4.st2.4.1.5.5" class="ltx_td ltx_align_center ltx_border_t">50.7</td>
<td id="S4.T4.st2.4.1.5.6" class="ltx_td ltx_align_center ltx_border_t">48.6</td>
<td id="S4.T4.st2.4.1.5.7" class="ltx_td ltx_align_center ltx_border_t">61.8</td>
<td id="S4.T4.st2.4.1.5.8" class="ltx_td ltx_align_center ltx_border_t">59.3</td>
</tr>
<tr id="S4.T4.st2.4.1.6" class="ltx_tr">
<td id="S4.T4.st2.4.1.6.1" class="ltx_td ltx_align_center ltx_border_bb">AT</td>
<td id="S4.T4.st2.4.1.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S4.T4.st2.4.1.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.st2.4.1.6.3.1" class="ltx_text ltx_font_bold">57.1</span></td>
<td id="S4.T4.st2.4.1.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.st2.4.1.6.4.1" class="ltx_text ltx_font_bold">36.1</span></td>
<td id="S4.T4.st2.4.1.6.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.st2.4.1.6.5.1" class="ltx_text ltx_font_bold">54.1</span></td>
<td id="S4.T4.st2.4.1.6.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.st2.4.1.6.6.1" class="ltx_text ltx_font_bold">50.7</span></td>
<td id="S4.T4.st2.4.1.6.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.st2.4.1.6.7.1" class="ltx_text ltx_font_bold">64.0</span></td>
<td id="S4.T4.st2.4.1.6.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.st2.4.1.6.8.1" class="ltx_text ltx_font_bold">59.8</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2404.03179/assets/figures/visual1_new_new.jpg" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Qualitative results on TAL, AVEL and SED tasks. The examples are from the validation set of ActivityNet 1.3, the test set of UnAV-100, and the public evaluation set of DESED, respectively. “GT” is short for ground truth. “SOTA” denotes the state-of-the-art methods
(TriDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> for TAL, UnAV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> for AVEL,
and the audio-only single-task model for
SED, where all models use the same features). “Ours” is our AT model.
We show the boundaries with the highest overlap with the ground truth.</span></figcaption>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Visualization and Discussion</h3>

<div id="S4.SS6.p1" class="ltx_para ltx_noindent">
<p id="S4.SS6.p1.1" class="ltx_p"><span id="S4.SS6.p1.1.1" class="ltx_text ltx_font_bold">Qualitative results.</span>
In <a href="#S4.F3" title="In 4.5 Ablation Studies ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, we visualize the predictions of our AT model on three tasks. Compared with the other state-of-the-art methods,
our model generates localizations that better overlap with ground truth.
For instance, our model outputs finer boundaries to distinguish repeated occurrences of the same events, <em id="S4.SS6.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS6.p1.1.3" class="ltx_text"></span>, the audio-visual event of “vacuum cleaner cleaning floors” and the sound event of “speech”.
For SED,
visual information helps the model detect the “frying” sound event more accurately compared to that using audio alone.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2404.03179/assets/figures/visual2_new_final.jpg" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="267" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Examples of localizing the visual/sound events that are not present in ActivityNet 1.3/DESED datasets.
The videos are from the test set of UnAV-100. “GT (AVEL)” denotes only AVEL annotations provided during training. “Pred” is the prediction results on each task. “score” is the confidence score of predictions.
</span></figcaption>
</figure>
<div id="S4.SS6.p2" class="ltx_para ltx_noindent">
<p id="S4.SS6.p2.1" class="ltx_p"><span id="S4.SS6.p2.1.1" class="ltx_text ltx_font_bold">Localizing instance categories across tasks.</span>
Benefiting from the language-aware classification head that utilizes the pre-trained text encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, our AT model has a novel capability of localizing the categories from other task datasets when performing the current task.
For example, we select some categories from the UnAV-100 dataset that are not present in ActivityNet 1.3 and DESED, and then tokenize them with the TAL and SED prompts and concat with the original class embeddings to conduct inference on all three tasks.
In <a href="#S4.F4" title="In 4.6 Visualization and Discussion ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we observe that our AT model successfully detects the visual events of “driving motorcycle”, “car passing by”, and “playing cornet”, <em id="S4.SS6.p2.1.2" class="ltx_emph ltx_font_italic">etc</em>.<span id="S4.SS6.p2.1.3" class="ltx_text"></span>, and the sound events of “church bell ringing” and “man speaking", <em id="S4.SS6.p2.1.4" class="ltx_emph ltx_font_italic">etc</em>.<span id="S4.SS6.p2.1.5" class="ltx_text"></span>, even though the model just learned the audio-visual events of these categories during training.
It indicates that our model is capable of detecting all three types of instances (visual, sound and audio-visual) for the categories of all three datasets (total 310 classes).</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2404.03179/assets/figures/visual3_new_final.jpg" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Examples of open-vocabulary localization.
The multi-task model trained on ActivityNet 1.3 for TAL and DESED for SED is utilized to detect unseen instance categories from UnAV-100. The videos are from the test set of UnAV-100.</span></figcaption>
</figure>
<div id="S4.SS6.p3" class="ltx_para ltx_noindent">
<p id="S4.SS6.p3.1" class="ltx_p"><span id="S4.SS6.p3.1.1" class="ltx_text ltx_font_bold">Emergent open-vocabulary localization.</span>
We also explore our model’s ability of open-vocabulary localization.
We use the multi-task model (row 4 in <a href="#S4.T1" title="In 4.2 Implementation Details ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>) trained on ActivityNet 1.3 and DESED and test it on the category set of UnAV-100.
We visualize two examples in <a href="#S4.F5" title="In 4.6 Visualization and Discussion ‣ 4 Experiments ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>.
There is no class “bull bellowing” and “playing drum kit”, even a similar one in ActivityNet 1.3 and DESED, but we surprisingly found that the model can accurately detect the corresponding visual and sound events with high confidence scores.
This demonstrates that our model has strong potential in open-set capabilities.
More ablation studies and qualitative examples can be found in the <span id="S4.SS6.p3.1.2" class="ltx_text ltx_font_italic">Supp. materials</span>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We propose a Unified Audio-Visual perception network (UniAV) for the joint learning of TAL, SED and AVEL tasks for the first time, realizing the localization of visual actions, sound events and audio-visual events in an untrimmed video by a single unified model.
Specifically, we introduce a unified audio-visual encoding pipeline to minimize data discrepancies, while applying task-specific experts to capture distinct knowledge for each task. Moreover, a unified language-aware classifier allows the model to have high flexibility and generalizability during inference.
Extensive experiments demonstrate that UniAV achieves superior and competitive performances on three challenging benchmarks, surpassing its single-task counterparts by a large margin with fewer parameters.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Limitations.</span>
As the first attempt, our model was trained on limited data and exhibits partial generalizability.
In the future, we plan to utilize more available data and fully leverage existing large-scale multi-modal pre-trained models to further explore the model’s capabilities in open-world predictions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Bagchi, A., Mahmood, J., Fernandes, D., Sarvadevabhatla, R.K.: Hear me out: Fusional approaches for audio augmented temporal action localization. arXiv preprint arXiv:2106.14118 (2021)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Bai, Y., Wang, Y., Tong, Y., Yang, Y., Liu, Q., Liu, J.: Boundary content graph neural network for temporal action proposal generation. In: European Conference on Computer Vision. pp. 121–137. Springer (2020)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bao, H., Wang, W., Dong, L., Liu, Q., Mohammed, O.K., Aggarwal, K., Som, S., Piao, S., Wei, F.: Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. Advances in Neural Information Processing Systems <span id="bib.bib3.1.1" class="ltx_text ltx_font_bold">35</span>, 32897–32912 (2022)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Bodla, N., Singh, B., Chellappa, R., Davis, L.S.: Soft-nms–improving object detection with one line of code. In: Proceedings of the IEEE international conference on computer vision. pp. 5561–5569 (2017)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Boes, W., et al.: Audiovisual transfer learning for audio tagging and sound event detection. arXiv preprint arXiv:2106.05408 (2021)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: Activitynet: A large-scale video benchmark for human activity understanding. In: Proceedings of the ieee conference on computer vision and pattern recognition. pp. 961–970 (2015)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the kinetics dataset. In: proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6299–6308 (2017)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Chao, Y.W., Vijayanarasimhan, S., Seybold, B., Ross, D.A., Deng, J., Sukthankar, R.: Rethinking the faster r-cnn architecture for temporal action localization. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1130–1139 (2018)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Damen, D., Doughty, H., Farinella, G.M., Furnari, A., Kazakos, E., Ma, J., Moltisanti, D., Munro, J., Perrett, T., Price, W., et al.: Rescaling egocentric vision. arXiv preprint arXiv:2006.13256 (2020)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Duan, B., Tang, H., Wang, W., Zong, Z., Yang, G., Yan, Y.: Audio-visual event localization via recursive fusion by joint co-attention. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 4013–4022 (2021)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Geng, T., Wang, T., Duan, J., Cong, R., Zheng, F.: Dense-localizing audio-visual events in untrimmed videos: A large-scale benchmark and baseline. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22942–22951 (2023)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Geng, T., Zheng, F., Hou, X., Lu, K., Qi, G.J., Shao, L.: Spatial-temporal pyramid graph reasoning for action recognition. IEEE Transactions on Image Processing <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">31</span>, 5484–5497 (2022)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Goyal, R., Mavroudi, E., Yang, X., Sukhbaatar, S., Sigal, L., Feiszli, M., Torresani, L., Tran, D.: Minotaur: Multi-task video grounding from multimodal queries. arXiv preprint arXiv:2302.08063 (2023)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., et al.: Ego4d: Around the world in 3,000 hours of egocentric video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18995–19012 (2022)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Hebbar, R., Bose, D., Somandepalli, K., Vijai, V., Narayanan, S.: A dataset for audio-visual sound event detection in movies. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 1–5. IEEE (2023)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Hershey, S., Chaudhuri, S., Ellis, D.P., Gemmeke, J.F., Jansen, A., Moore, R.C., Plakal, M., Platt, D., Saurous, R.A., Seybold, B., et al.: Cnn architectures for large-scale audio classification. In: 2017 ieee international conference on acoustics, speech and signal processing (icassp). pp. 131–135. IEEE (2017)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Hou, W., Li, G., Tian, Y., Hu, D.: Towards long form audio-visual video understanding. arXiv preprint arXiv:2306.09431 (2023)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Idrees, H., Zamir, A.R., Jiang, Y.G., Gorban, A., Laptev, I., Sukthankar, R., Shah, M.: The thumos challenge on action recognition for videos “in the wild”. Computer Vision and Image Understanding <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">155</span>, 1–23 (2017)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Khandelwal, T., Das, R.K., Koh, A., Chng, E.S.: Fmsg-ntu submission for dcase 2022 task 4 on sound event detection in domestic environments. Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge (2022)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Kim, C., Yang, S.: Sound event detection system using fix-match for dcase 2022 challenge task 4. Proceedings of the Detection and Classification of Acoustic Scenes and Events (DCASE) Challenge (2022)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Lee, J.T., Jain, M., Park, H., Yun, S.: Cross-attentional audio-visual fusion for weakly-supervised action localization. In: International conference on learning representations (2020)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Lee, J.T., Yun, S., Jain, M.: Leaky gated cross-attention for weakly supervised multi-modal temporal action localization. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 3213–3222 (2022)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Li, M., Sigal, L.: Referring transformer: A one-step approach to multi-task visual grounding. Advances in neural information processing systems <span id="bib.bib23.1.1" class="ltx_text ltx_font_bold">34</span>, 19652–19664 (2021)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Lin, C., Xu, C., Luo, D., Wang, Y., Tai, Y., Wang, C., Li, J., Huang, F., Fu, Y.: Learning salient boundary feature for anchor-free temporal action localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3320–3329 (2021)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Lin, K.Q., Zhang, P., Chen, J., Pramanick, S., Gao, D., Wang, A.J., Yan, R., Shou, M.Z.: Univtg: Towards unified video-language temporal grounding. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2794–2804 (2023)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Lin, T., Liu, X., Li, X., Ding, E., Wen, S.: Bmn: Boundary-matching network for temporal action proposal generation. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 3889–3898 (2019)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Lin, T., Zhao, X., Su, H., Wang, C., Yang, M.: Bsn: Boundary sensitive network for temporal action proposal generation. In: Proceedings of the European conference on computer vision (ECCV). pp. 3–19 (2018)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P.: Focal loss for dense object detection. In: Proceedings of the IEEE international conference on computer vision. pp. 2980–2988 (2017)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Lin, Y.B., Tseng, H.Y., Lee, H.Y., Lin, Y.Y., Yang, M.H.: Exploring cross-video and cross-modality signals for weakly-supervised audio-visual video parsing. Advances in Neural Information Processing Systems <span id="bib.bib29.1.1" class="ltx_text ltx_font_bold">34</span>, 11449–11461 (2021)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Liu, X., Wang, Q., Hu, Y., Tang, X., Zhang, S., Bai, S., Bai, X.: End-to-end temporal action detection with transformer. IEEE Transactions on Image Processing <span id="bib.bib30.1.1" class="ltx_text ltx_font_bold">31</span>, 5427–5441 (2022)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Lu, J., Goswami, V., Rohrbach, M., Parikh, D., Lee, S.: 12-in-1: Multi-task vision and language representation learning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10437–10446 (2020)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Luo, G., Zhou, Y., Sun, X., Cao, L., Wu, C., Deng, C., Ji, R.: Multi-task collaborative network for joint referring expression comprehension and segmentation. In: Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition. pp. 10034–10043 (2020)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Mesaros, A., Heittola, T., Diment, A., Elizalde, B., Shah, A., Vincent, E., Raj, B., Virtanen, T.: Dcase 2017 challenge setup: Tasks, datasets and baseline system. In: DCASE 2017-Workshop on Detection and Classification of Acoustic Scenes and Events (2017)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Mesaros, A., Heittola, T., Virtanen, T.: Metrics for polyphonic sound event detection. Applied Sciences <span id="bib.bib35.1.1" class="ltx_text ltx_font_bold">6</span>(6),  162 (2016)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Mo, S., Tian, Y.: Multi-modal grouping network for weakly-supervised audio-visual video parsing. Advances in Neural Information Processing Systems <span id="bib.bib36.1.1" class="ltx_text ltx_font_bold">35</span>, 34722–34733 (2022)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Mustafa, B., Riquelme, C., Puigcerver, J., Jenatton, R., Houlsby, N.: Multimodal contrastive learning with limoe: the language-image mixture of experts. Advances in Neural Information Processing Systems <span id="bib.bib37.1.1" class="ltx_text ltx_font_bold">35</span>, 9564–9576 (2022)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Rachavarapu, K.K., et al.: Boosting positive segments for weakly-supervised audio-visual video parsing. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10192–10202 (2023)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748–8763. PMLR (2021)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., Savarese, S.: Generalized intersection over union: A metric and a loss for bounding box regression. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 658–666 (2019)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., Dean, J.: Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 (2017)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Shi, D., Zhong, Y., Cao, Q., Ma, L., Li, J., Tao, D.: Tridet: Temporal action detection with relative boundary modeling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18857–18866 (2023)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Su, R., Xu, D., Sheng, L., Ouyang, W.: Pcg-tal: Progressive cross-granularity cooperation for temporal action localization. IEEE Transactions on Image Processing <span id="bib.bib43.1.1" class="ltx_text ltx_font_bold">30</span>, 2103–2113 (2020)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Su, W., Miao, P., Dou, H., Wang, G., Qiao, L., Li, Z., Li, X.: Language adaptive weight generation for multi-task visual grounding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10857–10866 (2023)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Tan, J., Tang, J., Wang, L., Wu, G.: Relaxed transformer decoders for direct action proposal generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13526–13535 (2021)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Tian, Y., Li, D., Xu, C.: Unified multisensory perception: Weakly-supervised audio-visual video parsing. In: European Conference on Computer Vision. pp. 436–454. Springer (2020)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Tian, Y., Shi, J., Li, B., Duan, Z., Xu, C.: Audio-visual event localization in unconstrained videos. In: Proceedings of the European conference on computer vision (ECCV). pp. 247–263 (2018)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Turpault, N., Serizel, R., Parag Shah, A., Salamon, J.: Sound event detection in domestic environments with weakly labeled data and soundscape synthesis. In: Workshop on Detection and Classification of Acoustic Scenes and Events. New York City, United States (Oct 2019), <a target="_blank" href="https://inria.hal.science/hal-02160855" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://inria.hal.science/hal-02160855</a>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Wang, P., Wang, S., Lin, J., Bai, S., Zhou, X., Zhou, J., Wang, X., Zhou, C.: One-peace: Exploring one general representation model toward unlimited modalities. arXiv preprint arXiv:2305.11172 (2023)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Wu, Y., Zhu, L., Yan, Y., Yang, Y.: Dual attention matching for audio-visual event localization. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 6292–6300 (2019)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Wu, Y., Chen, K., Zhang, T., Hui, Y., Berg-Kirkpatrick, T., Dubnov, S.: Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 1–5. IEEE (2023)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Xiao, Y., Khandelwal, T., Das, R.K.: ‘fmsg submission for dcase 2023 challenge task 4 on sound event detection with weak labels and synthetic soundscapes. Proc. DCASE Challenge (2023)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Xiong, Y., Wang, L., Wang, Z., Zhang, B., Song, H., Li, W., Lin, D., Qiao, Y., Van Gool, L., Tang, X.: Cuhk &amp; ethz &amp; siat submission to activitynet challenge 2016. arXiv preprint arXiv:1608.00797 (2016)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Xu, H., Zeng, R., Wu, Q., Tan, M., Gan, C.: Cross-modal relation-aware networks for audio-visual event localization. In: Proceedings of the 28th ACM International Conference on Multimedia. pp. 3893–3901 (2020)

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Xu, M., Zhao, C., Rojas, D.S., Thabet, A., Ghanem, B.: G-tad: Sub-graph localization for temporal action detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10156–10165 (2020)

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Xuan, H., Zhang, Z., Chen, S., Yang, J., Yan, Y.: Cross-modal attention network for temporal inconsistent audio-visual event localization. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 279–286 (2020)

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Yan, B., Jiang, Y., Sun, P., Wang, D., Yuan, Z., Luo, P., Lu, H.: Towards grand unification of object tracking. In: European Conference on Computer Vision. pp. 733–751. Springer (2022)

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Zeng, R., Huang, W., Tan, M., Rong, Y., Zhao, P., Huang, J., Gan, C.: Graph convolutional networks for temporal action localization. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 7094–7103 (2019)

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Zhang, C., Wu, J., Li, Y.: Actionformer: Localizing moments of actions with transformers. In: European Conference on Computer Vision. pp. 492–510. Springer (2022)

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Zhao, C., Thabet, A.K., Ghanem, B.: Video self-stitching graph network for temporal action localization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13658–13667 (2021)

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Zhao, Y., Xiong, Y., Wang, L., Wu, Z., Tang, X., Lin, D.: Temporal action detection with structured segment networks. In: Proceedings of the IEEE international conference on computer vision. pp. 2914–2923 (2017)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Pt0.A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.A </span>More Implementation Details</h2>

<div id="Pt0.A1.p1" class="ltx_para">
<p id="Pt0.A1.p1.1" class="ltx_p">We utilize the visual and audio encoders of ONE-PEACE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> to tokenize visual and audio modalities for all three tasks, respectively.
Specifically, each frame is first resized to ensure that its shortest side is 256 pixels, followed by cropping the center region to obtain 256 <math id="Pt0.A1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="Pt0.A1.p1.1.m1.1a"><mo id="Pt0.A1.p1.1.m1.1.1" xref="Pt0.A1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Pt0.A1.p1.1.m1.1b"><times id="Pt0.A1.p1.1.m1.1.1.cmml" xref="Pt0.A1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.p1.1.m1.1c">\times</annotation></semantics></math> 256 frames. Then, the RGB stacks are passed through the visual encoder, and the average pooling is applied on the temporal axis, producing a 1536-d feature for each stack of 16 frames. Also note that the visual encoder was fine-tuned on the Kinetics-400 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> with inputs of 16 frames.
For the audio modality, we directly input the waveform of 1s audio clips into the audio encoder to obtain a 1536-d feature for each clip.
For our unified framework, in the audio-visual pyramid transformer, the number of attention heads is 4 in both uni-modal and cross-modal blocks. The temporal downsampling operation is realized by using a single depth-wise 1D convolution.
Our models are trained on a Nvidia Tesla V100 GPU, and the code implementation relies on PyTorch framework and will be released upon publication.</p>
</div>
<div id="Pt0.A1.p2" class="ltx_para ltx_noindent">
<p id="Pt0.A1.p2.1" class="ltx_p"><span id="Pt0.A1.p2.1.1" class="ltx_text ltx_font_bold">Dataset selection for multi-task learning.</span>
Facing the fact that the existing datasets for untrimmed video localization tasks are limited, we chose ActivityNet 1.3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, UnAV-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and DESED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> as they are currently the most mainstream datasets with relatively high quality and suitable for our multi-task model training.
For TAL task, there are several other popular datasets, such as THUMOS14 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and EPIC-Kitchens 100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. However, THUMOS14 contains very limited videos (200 videos for training) with only 20 uncommon sport classes, <em id="Pt0.A1.p2.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="Pt0.A1.p2.1.3" class="ltx_text"></span>, “CleanAndJerk”, “PoleVault” and “JavelinThrow”, <em id="Pt0.A1.p2.1.4" class="ltx_emph ltx_font_italic">etc</em>. Besides, EPIC-Kitchens 100 consists of egocentric videos constrained to fine-grained kitchen scenarios. Thus, there exist extremely large domain gaps with the datasets for other tasks. By contrast, ActivityNet 1.3 has 200 rich common human activities including the top-level categories of <span id="Pt0.A1.p2.1.5" class="ltx_text ltx_font_italic">housework, working, sports, eating and drinking, and animal caring</span>, <em id="Pt0.A1.p2.1.6" class="ltx_emph ltx_font_italic">etc</em>.<span id="Pt0.A1.p2.1.7" class="ltx_text"></span>,
which has domain overlaps with other task datasets,
facilitating the model to learn mutually beneficial knowledge across tasks during multi-task training. Furthermore, only UnAV-100 dataset for AVEL task is based on untrimmed videos, and DESED is the only dataset whose audio and visual modalities are both available for SED task to the best of our knowledge.</p>
</div>
</section>
<section id="Pt0.A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.B </span>More Results Analysis</h2>

<div id="Pt0.A2.p1" class="ltx_para ltx_noindent">
<p id="Pt0.A2.p1.2" class="ltx_p"><span id="Pt0.A2.p1.2.1" class="ltx_text ltx_font_bold">Effect of parameter quantity.</span>
From the ablation study on the proposed task-specific experts in Sec. <span id="Pt0.A2.p1.2.2" class="ltx_text" style="color:#FF0000;">4.5</span>, we can see that adding experts on more transformer blocks leads to a significant increase in the number of parameters. In order to explore the effect of parameter quantity on our model, we reduce the dimension <math id="Pt0.A2.p1.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="Pt0.A2.p1.1.m1.1a"><mi id="Pt0.A2.p1.1.m1.1.1" xref="Pt0.A2.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="Pt0.A2.p1.1.m1.1b"><ci id="Pt0.A2.p1.1.m1.1.1.cmml" xref="Pt0.A2.p1.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.p1.1.m1.1c">D</annotation></semantics></math> and <math id="Pt0.A2.p1.2.m2.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="Pt0.A2.p1.2.m2.1a"><msup id="Pt0.A2.p1.2.m2.1.1" xref="Pt0.A2.p1.2.m2.1.1.cmml"><mi id="Pt0.A2.p1.2.m2.1.1.2" xref="Pt0.A2.p1.2.m2.1.1.2.cmml">D</mi><mo id="Pt0.A2.p1.2.m2.1.1.3" xref="Pt0.A2.p1.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="Pt0.A2.p1.2.m2.1b"><apply id="Pt0.A2.p1.2.m2.1.1.cmml" xref="Pt0.A2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="Pt0.A2.p1.2.m2.1.1.1.cmml" xref="Pt0.A2.p1.2.m2.1.1">superscript</csymbol><ci id="Pt0.A2.p1.2.m2.1.1.2.cmml" xref="Pt0.A2.p1.2.m2.1.1.2">𝐷</ci><ci id="Pt0.A2.p1.2.m2.1.1.3.cmml" xref="Pt0.A2.p1.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.p1.2.m2.1c">D^{\prime}</annotation></semantics></math> of the transformer blocks. In <a href="#Pt0.A2.T5" title="In Appendix 0.B More Results Analysis ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a>, we can see that when the dimension drops to 352 with the parameter number halved, our unified all-task (AT) model still keeps superior performances with minor variations. And increasing parameters may not necessarily improve performances, <em id="Pt0.A2.p1.2.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="Pt0.A2.p1.2.4" class="ltx_text"></span>, increasing to 92M causes drops in SED results. Overall, it clearly proves that the performance improvement of our AT model is not due to an increase in parameters but the effectiveness of our proposed task-specific experts.</p>
</div>
<figure id="Pt0.A2.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Pt0.A2.T5.5.2.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="Pt0.A2.T5.2.1" class="ltx_text" style="font-size:90%;">Study on the effect of parameter quantity of our AT model. “Dimension <math id="Pt0.A2.T5.2.1.m1.1" class="ltx_Math" alttext="D/D^{\prime}" display="inline"><semantics id="Pt0.A2.T5.2.1.m1.1b"><mrow id="Pt0.A2.T5.2.1.m1.1.1" xref="Pt0.A2.T5.2.1.m1.1.1.cmml"><mi id="Pt0.A2.T5.2.1.m1.1.1.2" xref="Pt0.A2.T5.2.1.m1.1.1.2.cmml">D</mi><mo id="Pt0.A2.T5.2.1.m1.1.1.1" xref="Pt0.A2.T5.2.1.m1.1.1.1.cmml">/</mo><msup id="Pt0.A2.T5.2.1.m1.1.1.3" xref="Pt0.A2.T5.2.1.m1.1.1.3.cmml"><mi id="Pt0.A2.T5.2.1.m1.1.1.3.2" xref="Pt0.A2.T5.2.1.m1.1.1.3.2.cmml">D</mi><mo id="Pt0.A2.T5.2.1.m1.1.1.3.3" xref="Pt0.A2.T5.2.1.m1.1.1.3.3.cmml">′</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A2.T5.2.1.m1.1c"><apply id="Pt0.A2.T5.2.1.m1.1.1.cmml" xref="Pt0.A2.T5.2.1.m1.1.1"><divide id="Pt0.A2.T5.2.1.m1.1.1.1.cmml" xref="Pt0.A2.T5.2.1.m1.1.1.1"></divide><ci id="Pt0.A2.T5.2.1.m1.1.1.2.cmml" xref="Pt0.A2.T5.2.1.m1.1.1.2">𝐷</ci><apply id="Pt0.A2.T5.2.1.m1.1.1.3.cmml" xref="Pt0.A2.T5.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="Pt0.A2.T5.2.1.m1.1.1.3.1.cmml" xref="Pt0.A2.T5.2.1.m1.1.1.3">superscript</csymbol><ci id="Pt0.A2.T5.2.1.m1.1.1.3.2.cmml" xref="Pt0.A2.T5.2.1.m1.1.1.3.2">𝐷</ci><ci id="Pt0.A2.T5.2.1.m1.1.1.3.3.cmml" xref="Pt0.A2.T5.2.1.m1.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T5.2.1.m1.1d">D/D^{\prime}</annotation></semantics></math>” denotes the dimension of the transformer blocks in the model.</span></figcaption>
<div id="Pt0.A2.T5.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:281.9pt;height:75.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.7pt,7.2pt) scale(0.840817513769263,0.840817513769263) ;">
<table id="Pt0.A2.T5.3.1" class="ltx_tabular ltx_align_middle">
<tr id="Pt0.A2.T5.3.1.1" class="ltx_tr">
<td id="Pt0.A2.T5.3.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" rowspan="2"><span id="Pt0.A2.T5.3.1.1.1.1" class="ltx_text">Dimension <math id="Pt0.A2.T5.3.1.1.1.1.m1.1" class="ltx_Math" alttext="D/D^{\prime}" display="inline"><semantics id="Pt0.A2.T5.3.1.1.1.1.m1.1a"><mrow id="Pt0.A2.T5.3.1.1.1.1.m1.1.1" xref="Pt0.A2.T5.3.1.1.1.1.m1.1.1.cmml"><mi id="Pt0.A2.T5.3.1.1.1.1.m1.1.1.2" xref="Pt0.A2.T5.3.1.1.1.1.m1.1.1.2.cmml">D</mi><mo id="Pt0.A2.T5.3.1.1.1.1.m1.1.1.1" xref="Pt0.A2.T5.3.1.1.1.1.m1.1.1.1.cmml">/</mo><msup id="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3" xref="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3.cmml"><mi id="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3.2" xref="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3.2.cmml">D</mi><mo id="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3.3" xref="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3.3.cmml">′</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A2.T5.3.1.1.1.1.m1.1b"><apply id="Pt0.A2.T5.3.1.1.1.1.m1.1.1.cmml" xref="Pt0.A2.T5.3.1.1.1.1.m1.1.1"><divide id="Pt0.A2.T5.3.1.1.1.1.m1.1.1.1.cmml" xref="Pt0.A2.T5.3.1.1.1.1.m1.1.1.1"></divide><ci id="Pt0.A2.T5.3.1.1.1.1.m1.1.1.2.cmml" xref="Pt0.A2.T5.3.1.1.1.1.m1.1.1.2">𝐷</ci><apply id="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3.cmml" xref="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3.1.cmml" xref="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3">superscript</csymbol><ci id="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3.2.cmml" xref="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3.2">𝐷</ci><ci id="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3.3.cmml" xref="Pt0.A2.T5.3.1.1.1.1.m1.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T5.3.1.1.1.1.m1.1c">D/D^{\prime}</annotation></semantics></math></span></td>
<td id="Pt0.A2.T5.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">TAL</td>
<td id="Pt0.A2.T5.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">AVEL</td>
<td id="Pt0.A2.T5.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">SED</td>
<td id="Pt0.A2.T5.3.1.1.5" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="Pt0.A2.T5.3.1.2" class="ltx_tr">
<td id="Pt0.A2.T5.3.1.2.1" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="Pt0.A2.T5.3.1.2.2" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="Pt0.A2.T5.3.1.2.3" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="Pt0.A2.T5.3.1.2.4" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="Pt0.A2.T5.3.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="Pt0.A2.T5.3.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Avg.</td>
<td id="Pt0.A2.T5.3.1.2.7" class="ltx_td ltx_align_center"># params</td>
</tr>
<tr id="Pt0.A2.T5.3.1.3" class="ltx_tr">
<td id="Pt0.A2.T5.3.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">352</td>
<td id="Pt0.A2.T5.3.1.3.2" class="ltx_td ltx_align_center ltx_border_t">56.7</td>
<td id="Pt0.A2.T5.3.1.3.3" class="ltx_td ltx_align_center ltx_border_t">36.0</td>
<td id="Pt0.A2.T5.3.1.3.4" class="ltx_td ltx_align_center ltx_border_t">53.4</td>
<td id="Pt0.A2.T5.3.1.3.5" class="ltx_td ltx_align_center ltx_border_t">50.3</td>
<td id="Pt0.A2.T5.3.1.3.6" class="ltx_td ltx_align_center ltx_border_t">63.8</td>
<td id="Pt0.A2.T5.3.1.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.2</td>
<td id="Pt0.A2.T5.3.1.3.8" class="ltx_td ltx_align_center ltx_border_t">63M</td>
</tr>
<tr id="Pt0.A2.T5.3.1.4" class="ltx_tr">
<td id="Pt0.A2.T5.3.1.4.1" class="ltx_td ltx_align_center ltx_border_r">424</td>
<td id="Pt0.A2.T5.3.1.4.2" class="ltx_td ltx_align_center">57.3</td>
<td id="Pt0.A2.T5.3.1.4.3" class="ltx_td ltx_align_center">36.1</td>
<td id="Pt0.A2.T5.3.1.4.4" class="ltx_td ltx_align_center">54.6</td>
<td id="Pt0.A2.T5.3.1.4.5" class="ltx_td ltx_align_center">50.8</td>
<td id="Pt0.A2.T5.3.1.4.6" class="ltx_td ltx_align_center">62.4</td>
<td id="Pt0.A2.T5.3.1.4.7" class="ltx_td ltx_align_center ltx_border_r">59.7</td>
<td id="Pt0.A2.T5.3.1.4.8" class="ltx_td ltx_align_center">92M</td>
</tr>
<tr id="Pt0.A2.T5.3.1.5" class="ltx_tr">
<td id="Pt0.A2.T5.3.1.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">512</td>
<td id="Pt0.A2.T5.3.1.5.2" class="ltx_td ltx_align_center ltx_border_bb">57.1</td>
<td id="Pt0.A2.T5.3.1.5.3" class="ltx_td ltx_align_center ltx_border_bb">36.1</td>
<td id="Pt0.A2.T5.3.1.5.4" class="ltx_td ltx_align_center ltx_border_bb">54.1</td>
<td id="Pt0.A2.T5.3.1.5.5" class="ltx_td ltx_align_center ltx_border_bb">50.7</td>
<td id="Pt0.A2.T5.3.1.5.6" class="ltx_td ltx_align_center ltx_border_bb">64.0</td>
<td id="Pt0.A2.T5.3.1.5.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">59.8</td>
<td id="Pt0.A2.T5.3.1.5.8" class="ltx_td ltx_align_center ltx_border_bb">130M</td>
</tr>
</table>
</span></div>
</figure>
<figure id="Pt0.A2.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Pt0.A2.T6.6.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="Pt0.A2.T6.7.2" class="ltx_text" style="font-size:90%;">Comparison with our models using I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and VGGish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> features. “SOTA”: TriDet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> for TAL, UnAV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> for AVEL and SED.</span></figcaption>
<div id="Pt0.A2.T6.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:247.2pt;height:121.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-4.2pt,2.1pt) scale(0.967280470233402,0.967280470233402) ;">
<table id="Pt0.A2.T6.4.4" class="ltx_tabular ltx_align_middle">
<tr id="Pt0.A2.T6.4.4.5" class="ltx_tr">
<td id="Pt0.A2.T6.4.4.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2"><span id="Pt0.A2.T6.4.4.5.1.1" class="ltx_text">Method</span></td>
<td id="Pt0.A2.T6.4.4.5.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">TAL</td>
<td id="Pt0.A2.T6.4.4.5.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">AVEL</td>
<td id="Pt0.A2.T6.4.4.5.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">SED</td>
</tr>
<tr id="Pt0.A2.T6.4.4.6" class="ltx_tr">
<td id="Pt0.A2.T6.4.4.6.1" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="Pt0.A2.T6.4.4.6.2" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="Pt0.A2.T6.4.4.6.3" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="Pt0.A2.T6.4.4.6.4" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="Pt0.A2.T6.4.4.6.5" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="Pt0.A2.T6.4.4.6.6" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
</tr>
<tr id="Pt0.A2.T6.4.4.7" class="ltx_tr">
<td id="Pt0.A2.T6.4.4.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">SOTA</td>
<td id="Pt0.A2.T6.4.4.7.2" class="ltx_td ltx_align_center ltx_border_t"><span id="Pt0.A2.T6.4.4.7.2.1" class="ltx_text ltx_font_bold">49.3</span></td>
<td id="Pt0.A2.T6.4.4.7.3" class="ltx_td ltx_align_center ltx_border_t">32.1</td>
<td id="Pt0.A2.T6.4.4.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Pt0.A2.T6.4.4.7.4.1" class="ltx_text ltx_font_bold">50.6</span></td>
<td id="Pt0.A2.T6.4.4.7.5" class="ltx_td ltx_align_center ltx_border_t">47.8</td>
<td id="Pt0.A2.T6.4.4.7.6" class="ltx_td ltx_align_center ltx_border_t">51.6</td>
<td id="Pt0.A2.T6.4.4.7.7" class="ltx_td ltx_align_center ltx_border_t">48.8</td>
</tr>
<tr id="Pt0.A2.T6.1.1.1" class="ltx_tr">
<td id="Pt0.A2.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r">Ours<sub id="Pt0.A2.T6.1.1.1.1.1" class="ltx_sub"><span id="Pt0.A2.T6.1.1.1.1.1.1" class="ltx_text ltx_font_italic">ST</span></sub>
</td>
<td id="Pt0.A2.T6.1.1.1.2" class="ltx_td ltx_align_center">48.8</td>
<td id="Pt0.A2.T6.1.1.1.3" class="ltx_td ltx_align_center">31.8</td>
<td id="Pt0.A2.T6.1.1.1.4" class="ltx_td ltx_align_center">48.8</td>
<td id="Pt0.A2.T6.1.1.1.5" class="ltx_td ltx_align_center">46.7</td>
<td id="Pt0.A2.T6.1.1.1.6" class="ltx_td ltx_align_center">50.5</td>
<td id="Pt0.A2.T6.1.1.1.7" class="ltx_td ltx_align_center">48.7</td>
</tr>
<tr id="Pt0.A2.T6.2.2.2" class="ltx_tr">
<td id="Pt0.A2.T6.2.2.2.1" class="ltx_td ltx_align_left ltx_border_r">Ours<sub id="Pt0.A2.T6.2.2.2.1.1" class="ltx_sub"><span id="Pt0.A2.T6.2.2.2.1.1.1" class="ltx_text ltx_font_italic">AT</span></sub>
</td>
<td id="Pt0.A2.T6.2.2.2.2" class="ltx_td ltx_align_center">48.4</td>
<td id="Pt0.A2.T6.2.2.2.3" class="ltx_td ltx_align_center">31.9</td>
<td id="Pt0.A2.T6.2.2.2.4" class="ltx_td ltx_align_center">49.3</td>
<td id="Pt0.A2.T6.2.2.2.5" class="ltx_td ltx_align_center">47.0</td>
<td id="Pt0.A2.T6.2.2.2.6" class="ltx_td ltx_align_center">49.8</td>
<td id="Pt0.A2.T6.2.2.2.7" class="ltx_td ltx_align_center">49.2</td>
</tr>
<tr id="Pt0.A2.T6.3.3.3" class="ltx_tr">
<td id="Pt0.A2.T6.3.3.3.1" class="ltx_td ltx_align_left ltx_border_r">Ours<math id="Pt0.A2.T6.3.3.3.1.m1.1" class="ltx_Math" alttext="{}_{AT_{w/oLCH}}" display="inline"><semantics id="Pt0.A2.T6.3.3.3.1.m1.1a"><msub id="Pt0.A2.T6.3.3.3.1.m1.1.1" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.cmml"><mi id="Pt0.A2.T6.3.3.3.1.m1.1.1a" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.cmml"></mi><mrow id="Pt0.A2.T6.3.3.3.1.m1.1.1.1" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.cmml"><mi id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.2" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.1" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.1.cmml">​</mo><msub id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.cmml"><mi id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.2" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.2.cmml">T</mi><mrow id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.cmml"><mrow id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.cmml"><mi id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.2" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.2.cmml">w</mi><mo id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.1" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.1.cmml">/</mo><mi id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.3" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.3.cmml">o</mi></mrow><mo lspace="0em" rspace="0em" id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.1" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.3" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.1a" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.4" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.1b" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.1.cmml">​</mo><mi id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.5" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.5.cmml">H</mi></mrow></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="Pt0.A2.T6.3.3.3.1.m1.1b"><apply id="Pt0.A2.T6.3.3.3.1.m1.1.1.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1"><apply id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1"><times id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.1.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.1"></times><ci id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.2.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.2">𝐴</ci><apply id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3"><csymbol cd="ambiguous" id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.1.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3">subscript</csymbol><ci id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.2.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.2">𝑇</ci><apply id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3"><times id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.1.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.1"></times><apply id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2"><divide id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.1.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.1"></divide><ci id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.2.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.2">𝑤</ci><ci id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.3.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.2.3">𝑜</ci></apply><ci id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.3.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.3">𝐿</ci><ci id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.4.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.4">𝐶</ci><ci id="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.5.cmml" xref="Pt0.A2.T6.3.3.3.1.m1.1.1.1.3.3.5">𝐻</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T6.3.3.3.1.m1.1c">{}_{AT_{w/oLCH}}</annotation></semantics></math>
</td>
<td id="Pt0.A2.T6.3.3.3.2" class="ltx_td ltx_align_center">48.9</td>
<td id="Pt0.A2.T6.3.3.3.3" class="ltx_td ltx_align_center">32.4</td>
<td id="Pt0.A2.T6.3.3.3.4" class="ltx_td ltx_align_center">49.6</td>
<td id="Pt0.A2.T6.3.3.3.5" class="ltx_td ltx_align_center">47.7</td>
<td id="Pt0.A2.T6.3.3.3.6" class="ltx_td ltx_align_center">50.9</td>
<td id="Pt0.A2.T6.3.3.3.7" class="ltx_td ltx_align_center">50.0</td>
</tr>
<tr id="Pt0.A2.T6.4.4.4" class="ltx_tr">
<td id="Pt0.A2.T6.4.4.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">Ours<math id="Pt0.A2.T6.4.4.4.1.m1.1" class="ltx_Math" alttext="{}_{AT_{w/oLCH}\rightarrow ST}" display="inline"><semantics id="Pt0.A2.T6.4.4.4.1.m1.1a"><msub id="Pt0.A2.T6.4.4.4.1.m1.1.1" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.cmml"><mi id="Pt0.A2.T6.4.4.4.1.m1.1.1a" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.cmml"></mi><mrow id="Pt0.A2.T6.4.4.4.1.m1.1.1.1" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.cmml"><mrow id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.cmml"><mi id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.2" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.1" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.1.cmml">​</mo><msub id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.cmml"><mi id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.2" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.2.cmml">T</mi><mrow id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.cmml"><mrow id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.cmml"><mi id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.2" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.2.cmml">w</mi><mo id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.1" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.1.cmml">/</mo><mi id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.3" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.3.cmml">o</mi></mrow><mo lspace="0em" rspace="0em" id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.1" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.1.cmml">​</mo><mi id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.3" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.1a" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.1.cmml">​</mo><mi id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.4" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.1b" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.1.cmml">​</mo><mi id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.5" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.5.cmml">H</mi></mrow></msub></mrow><mo stretchy="false" id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.1" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.1.cmml">→</mo><mrow id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.cmml"><mi id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.2" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.1" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.1.cmml">​</mo><mi id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.3" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.3.cmml">T</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="Pt0.A2.T6.4.4.4.1.m1.1b"><apply id="Pt0.A2.T6.4.4.4.1.m1.1.1.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1"><apply id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1"><ci id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.1.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.1">→</ci><apply id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2"><times id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.1.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.1"></times><ci id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.2.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.2">𝐴</ci><apply id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3"><csymbol cd="ambiguous" id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.1.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3">subscript</csymbol><ci id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.2.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.2">𝑇</ci><apply id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3"><times id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.1.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.1"></times><apply id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2"><divide id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.1.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.1"></divide><ci id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.2.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.2">𝑤</ci><ci id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.3.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.2.3">𝑜</ci></apply><ci id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.3.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.3">𝐿</ci><ci id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.4.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.4">𝐶</ci><ci id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.5.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.2.3.3.5">𝐻</ci></apply></apply></apply><apply id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3"><times id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.1.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.1"></times><ci id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.2.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.2">𝑆</ci><ci id="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.3.cmml" xref="Pt0.A2.T6.4.4.4.1.m1.1.1.1.3.3">𝑇</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A2.T6.4.4.4.1.m1.1c">{}_{AT_{w/oLCH}\rightarrow ST}</annotation></semantics></math>
</td>
<td id="Pt0.A2.T6.4.4.4.2" class="ltx_td ltx_align_center ltx_border_bb">49.0</td>
<td id="Pt0.A2.T6.4.4.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="Pt0.A2.T6.4.4.4.3.1" class="ltx_text ltx_font_bold">32.6</span></td>
<td id="Pt0.A2.T6.4.4.4.4" class="ltx_td ltx_align_center ltx_border_bb">50.1</td>
<td id="Pt0.A2.T6.4.4.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="Pt0.A2.T6.4.4.4.5.1" class="ltx_text ltx_font_bold">48.2</span></td>
<td id="Pt0.A2.T6.4.4.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="Pt0.A2.T6.4.4.4.6.1" class="ltx_text ltx_font_bold">52.4</span></td>
<td id="Pt0.A2.T6.4.4.4.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="Pt0.A2.T6.4.4.4.7.1" class="ltx_text ltx_font_bold">50.6</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="Pt0.A2.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Pt0.A2.T7.2.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="Pt0.A2.T7.3.2" class="ltx_text" style="font-size:90%;">Study on different text encoders for category embedding.</span></figcaption>
<div id="Pt0.A2.T7.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:238.5pt;height:62.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-53.0pt,13.9pt) scale(0.692179321454145,0.692179321454145) ;">
<table id="Pt0.A2.T7.4.1" class="ltx_tabular ltx_align_middle">
<tr id="Pt0.A2.T7.4.1.1" class="ltx_tr">
<td id="Pt0.A2.T7.4.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2"><span id="Pt0.A2.T7.4.1.1.1.1" class="ltx_text">Text Encoder</span></td>
<td id="Pt0.A2.T7.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">TAL</td>
<td id="Pt0.A2.T7.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">AVEL</td>
<td id="Pt0.A2.T7.4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">SED</td>
</tr>
<tr id="Pt0.A2.T7.4.1.2" class="ltx_tr">
<td id="Pt0.A2.T7.4.1.2.1" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="Pt0.A2.T7.4.1.2.2" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="Pt0.A2.T7.4.1.2.3" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="Pt0.A2.T7.4.1.2.4" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
<td id="Pt0.A2.T7.4.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.5</td>
<td id="Pt0.A2.T7.4.1.2.6" class="ltx_td ltx_align_center ltx_border_t">Avg.</td>
</tr>
<tr id="Pt0.A2.T7.4.1.3" class="ltx_tr">
<td id="Pt0.A2.T7.4.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="Pt0.A2.T7.4.1.3.2" class="ltx_td ltx_align_center ltx_border_t">45.5</td>
<td id="Pt0.A2.T7.4.1.3.3" class="ltx_td ltx_align_center ltx_border_t">28.8</td>
<td id="Pt0.A2.T7.4.1.3.4" class="ltx_td ltx_align_center ltx_border_t">43.9</td>
<td id="Pt0.A2.T7.4.1.3.5" class="ltx_td ltx_align_center ltx_border_t">41.5</td>
<td id="Pt0.A2.T7.4.1.3.6" class="ltx_td ltx_align_center ltx_border_t">61.3</td>
<td id="Pt0.A2.T7.4.1.3.7" class="ltx_td ltx_align_center ltx_border_t">57.9</td>
</tr>
<tr id="Pt0.A2.T7.4.1.4" class="ltx_tr">
<td id="Pt0.A2.T7.4.1.4.1" class="ltx_td ltx_align_left ltx_border_r">CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>
</td>
<td id="Pt0.A2.T7.4.1.4.2" class="ltx_td ltx_align_center">55.7</td>
<td id="Pt0.A2.T7.4.1.4.3" class="ltx_td ltx_align_center">34.7</td>
<td id="Pt0.A2.T7.4.1.4.4" class="ltx_td ltx_align_center">53.5</td>
<td id="Pt0.A2.T7.4.1.4.5" class="ltx_td ltx_align_center">49.8</td>
<td id="Pt0.A2.T7.4.1.4.6" class="ltx_td ltx_align_center">62.0</td>
<td id="Pt0.A2.T7.4.1.4.7" class="ltx_td ltx_align_center">58.1</td>
</tr>
<tr id="Pt0.A2.T7.4.1.5" class="ltx_tr">
<td id="Pt0.A2.T7.4.1.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">ONE-PEACE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="Pt0.A2.T7.4.1.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="Pt0.A2.T7.4.1.5.2.1" class="ltx_text ltx_font_bold">57.1</span></td>
<td id="Pt0.A2.T7.4.1.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="Pt0.A2.T7.4.1.5.3.1" class="ltx_text ltx_font_bold">36.1</span></td>
<td id="Pt0.A2.T7.4.1.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="Pt0.A2.T7.4.1.5.4.1" class="ltx_text ltx_font_bold">54.1</span></td>
<td id="Pt0.A2.T7.4.1.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="Pt0.A2.T7.4.1.5.5.1" class="ltx_text ltx_font_bold">50.7</span></td>
<td id="Pt0.A2.T7.4.1.5.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="Pt0.A2.T7.4.1.5.6.1" class="ltx_text ltx_font_bold">64.0</span></td>
<td id="Pt0.A2.T7.4.1.5.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="Pt0.A2.T7.4.1.5.7.1" class="ltx_text ltx_font_bold">59.8</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="Pt0.A2.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Pt0.A2.T8.2.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="Pt0.A2.T8.3.2" class="ltx_text" style="font-size:90%;">Study on different audio and visual encoders.
We show the results on the public evaluation set of DESED for SED task.</span></figcaption>
<div id="Pt0.A2.T8.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:238.5pt;height:67.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-103.1pt,29.2pt) scale(0.536413701244558,0.536413701244558) ;">
<table id="Pt0.A2.T8.4.1" class="ltx_tabular ltx_align_middle">
<tr id="Pt0.A2.T8.4.1.1" class="ltx_tr">
<td id="Pt0.A2.T8.4.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Modality</td>
<td id="Pt0.A2.T8.4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Encoder</td>
<td id="Pt0.A2.T8.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">0.5</td>
<td id="Pt0.A2.T8.4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">0.7</td>
<td id="Pt0.A2.T8.4.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">0.9</td>
<td id="Pt0.A2.T8.4.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Avg.</td>
</tr>
<tr id="Pt0.A2.T8.4.1.2" class="ltx_tr">
<td id="Pt0.A2.T8.4.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="Pt0.A2.T8.4.1.2.1.1" class="ltx_text">A</span></td>
<td id="Pt0.A2.T8.4.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CLAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</td>
<td id="Pt0.A2.T8.4.1.2.3" class="ltx_td ltx_align_center ltx_border_t">27.2</td>
<td id="Pt0.A2.T8.4.1.2.4" class="ltx_td ltx_align_center ltx_border_t">16.5</td>
<td id="Pt0.A2.T8.4.1.2.5" class="ltx_td ltx_align_center ltx_border_t">5.1</td>
<td id="Pt0.A2.T8.4.1.2.6" class="ltx_td ltx_align_center ltx_border_t">26.2</td>
</tr>
<tr id="Pt0.A2.T8.4.1.3" class="ltx_tr">
<td id="Pt0.A2.T8.4.1.3.1" class="ltx_td ltx_align_center ltx_border_r">ONE-PEACE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="Pt0.A2.T8.4.1.3.2" class="ltx_td ltx_align_center">51.4</td>
<td id="Pt0.A2.T8.4.1.3.3" class="ltx_td ltx_align_center">38.1</td>
<td id="Pt0.A2.T8.4.1.3.4" class="ltx_td ltx_align_center">13.9</td>
<td id="Pt0.A2.T8.4.1.3.5" class="ltx_td ltx_align_center">49.5</td>
</tr>
<tr id="Pt0.A2.T8.4.1.4" class="ltx_tr">
<td id="Pt0.A2.T8.4.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="Pt0.A2.T8.4.1.4.1.1" class="ltx_text">V</span></td>
<td id="Pt0.A2.T8.4.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>
</td>
<td id="Pt0.A2.T8.4.1.4.3" class="ltx_td ltx_align_center ltx_border_t">29.2</td>
<td id="Pt0.A2.T8.4.1.4.4" class="ltx_td ltx_align_center ltx_border_t">18.6</td>
<td id="Pt0.A2.T8.4.1.4.5" class="ltx_td ltx_align_center ltx_border_t">8.9</td>
<td id="Pt0.A2.T8.4.1.4.6" class="ltx_td ltx_align_center ltx_border_t">30.6</td>
</tr>
<tr id="Pt0.A2.T8.4.1.5" class="ltx_tr">
<td id="Pt0.A2.T8.4.1.5.1" class="ltx_td ltx_align_center ltx_border_r">ONE-PEACE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="Pt0.A2.T8.4.1.5.2" class="ltx_td ltx_align_center">29.9</td>
<td id="Pt0.A2.T8.4.1.5.3" class="ltx_td ltx_align_center">15.2</td>
<td id="Pt0.A2.T8.4.1.5.4" class="ltx_td ltx_align_center">4.8</td>
<td id="Pt0.A2.T8.4.1.5.5" class="ltx_td ltx_align_center">29.8</td>
</tr>
<tr id="Pt0.A2.T8.4.1.6" class="ltx_tr">
<td id="Pt0.A2.T8.4.1.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="2"><span id="Pt0.A2.T8.4.1.6.1.1" class="ltx_text">A&amp;V</span></td>
<td id="Pt0.A2.T8.4.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CLAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> &amp; CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>
</td>
<td id="Pt0.A2.T8.4.1.6.3" class="ltx_td ltx_align_center ltx_border_t">52.6</td>
<td id="Pt0.A2.T8.4.1.6.4" class="ltx_td ltx_align_center ltx_border_t">39.2</td>
<td id="Pt0.A2.T8.4.1.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="Pt0.A2.T8.4.1.6.5.1" class="ltx_text ltx_font_bold">20.0</span></td>
<td id="Pt0.A2.T8.4.1.6.6" class="ltx_td ltx_align_center ltx_border_t">50.1</td>
</tr>
<tr id="Pt0.A2.T8.4.1.7" class="ltx_tr">
<td id="Pt0.A2.T8.4.1.7.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">ONE-PEACE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="Pt0.A2.T8.4.1.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="Pt0.A2.T8.4.1.7.2.1" class="ltx_text ltx_font_bold">61.0</span></td>
<td id="Pt0.A2.T8.4.1.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="Pt0.A2.T8.4.1.7.3.1" class="ltx_text ltx_font_bold">45.9</span></td>
<td id="Pt0.A2.T8.4.1.7.4" class="ltx_td ltx_align_center ltx_border_bb">19.8</td>
<td id="Pt0.A2.T8.4.1.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="Pt0.A2.T8.4.1.7.5.1" class="ltx_text ltx_font_bold">57.7</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="Pt0.A2.p2" class="ltx_para ltx_noindent">
<p id="Pt0.A2.p2.1" class="ltx_p"><span id="Pt0.A2.p2.1.1" class="ltx_text ltx_font_bold">Experiments using I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and VGGish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> features.</span>
We also evaluate our model using I3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and VGGish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> features that are commonly used on previous models.
The results are shown in Tab. <a href="#Pt0.A2.T6" title="Table 6 ‣ Appendix 0.B More Results Analysis ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
Note that the I3D visual and VGGish audio embeddings are not aligned with the ONE-PEACE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> text embeddings, which severely limits the effectiveness of our unified language-aware classifier (LCH).
Alternatively, our AT model without LCH achieves performance boosts on all three tasks compared to our single-task (ST) models, indicating the great benefit of multi-task learning and the proposed task-specific experts.
Besides, finetuning gains further performance boosts, setting new state-of-the-art results compared with existing methods with the same features.
In conclusion, the lack of modality alignment (audio-visual-language) and limited generalization of traditional I3D and VGGish encoders constrain our model’s capability and flexibility.
It clearly proves the great necessity of using a general model pre-trained by aligning vision, audio and language modalities (<em id="Pt0.A2.p2.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="Pt0.A2.p2.1.3" class="ltx_text"></span>, ONE-PEACE) as our audio/visual encoders.</p>
</div>
<div id="Pt0.A2.p3" class="ltx_para ltx_noindent">
<p id="Pt0.A2.p3.1" class="ltx_p"><span id="Pt0.A2.p3.1.1" class="ltx_text ltx_font_bold">Effect of different text encoders.</span>
In <a href="#Pt0.A2.T7" title="In Appendix 0.B More Results Analysis ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a>, we compare the performances of our multi-task (AT) model using different text encoders for category embedding in the language-aware classifier. RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> is a large natural language processing (NLP) model widely used in various language understanding tasks, and CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> is a visual-language pre-trained model learned from a vast data of image-text pairs.
We can see that using RoBERTa, a purely NLP model, yields poor results across all three tasks, while due to the exceptional text encoding capability of the CLIP, significant performance boosts can be observed.
Furthermore, when utilizing the text encoder of ONE-PEACE, our AT model achieves the best results on all three tasks.
It indicates that our model can benefit from the modality-aligned representations by utilizing the visual, audio and text encoders from the same ONE-PEACE model.</p>
</div>
<div id="Pt0.A2.p4" class="ltx_para ltx_noindent">
<p id="Pt0.A2.p4.1" class="ltx_p"><span id="Pt0.A2.p4.1.1" class="ltx_text ltx_font_bold">Effect of different audio and visual encoders.</span>
In <a href="#Pt0.A2.T8" title="In Appendix 0.B More Results Analysis ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">8</span></a>, we also explore the performances of CLAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> for the encoding of audio and visual modalities, respectively. We show the results of the single-task model for the SED task. CLAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> is a popular language-audio pre-trained model for various downstream tasks, such as text-audio retrieval and audio classification. However, we find that using the audio embeddings extracted from CLAP leads to poor performance on the SED task. It may be attributed to its emphasis on global representations of long audio clips and insufficient fine-grained audio information modeling, making it unsuitable for localization tasks. Besides, we can see that the video features extracted from the CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> visual encoder have comparable performances with those from ONE-PEACE. When using both audio and visual modalities, the ONE-PEACE features can achieve the best results.</p>
</div>
</section>
<section id="Pt0.A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.C </span>More Visualization Examples</h2>

<div id="Pt0.A3.p1" class="ltx_para ltx_noindent">
<p id="Pt0.A3.p1.1" class="ltx_p"><span id="Pt0.A3.p1.1.1" class="ltx_text ltx_font_bold">Qualitative results.</span>
We visualize more prediction results of our all-task (AT) model on three tasks in <a href="#Pt0.A3.F6" title="In Appendix 0.C More Visualization Examples ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, where we compare with the same methods and use the same setting
as in Sec. <span id="Pt0.A3.p1.1.2" class="ltx_text" style="color:#FF0000;">4.6</span>. We can observe that our model obtains relatively better predictions than other state-of-the-art methods.</p>
</div>
<div id="Pt0.A3.p2" class="ltx_para ltx_noindent">
<p id="Pt0.A3.p2.1" class="ltx_p"><span id="Pt0.A3.p2.1.1" class="ltx_text ltx_font_bold">Localizing instance categories across tasks.</span>
More examples are presented in <a href="#Pt0.A3.F7" title="In Appendix 0.C More Visualization Examples ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>, where we use the same setting as in Sec. <span id="Pt0.A3.p2.1.2" class="ltx_text" style="color:#FF0000;">4.6</span>. Our AT model can successfully detect the visual and sound events of “tap dancing”, “woman speaking”, “female singing”, “lions roaring”, “man speaking” and “chainsawing trees”, even though the model just learned the audio-visual events of these categories during training. It further indicates that our AT model has a strong capability of detecting all three types of instances for the categories of all three datasets.</p>
</div>
<div id="Pt0.A3.p3" class="ltx_para ltx_noindent">
<p id="Pt0.A3.p3.1" class="ltx_p"><span id="Pt0.A3.p3.1.1" class="ltx_text ltx_font_bold">Emergent open-vocabulary localization.</span>
We also provide more examples in <a href="#Pt0.A3.F8" title="In Appendix 0.C More Visualization Examples ‣ UniAV: Unified Audio-Visual Perception for Multi-Task Video Localization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>, where we use the same setting as in Sec. <span id="Pt0.A3.p3.1.2" class="ltx_text" style="color:#FF0000;">4.6</span>. There is no class of “playing tabla”, “airplane flyby”, “hammering nails”, and “helicopter”, even a similar one in ActivityNet 1.3 and DESED datasets, but we can see that the model can accurately detect the corresponding visual and sound events with relatively high confidence scores.
It demonstrates our model’s potential capability of open-vocabulary localization.</p>
</div>
<figure id="Pt0.A3.F6" class="ltx_figure"><img src="/html/2404.03179/assets/figures/compare_visual_new.jpg" id="Pt0.A3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="408" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A3.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="Pt0.A3.F6.3.2" class="ltx_text" style="font-size:90%;">More qualitative results on TAL, AVEL and SED tasks. </span></figcaption>
</figure>
<figure id="Pt0.A3.F7" class="ltx_figure"><img src="/html/2404.03179/assets/figures/cross_visual_new.jpg" id="Pt0.A3.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="580" height="533" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A3.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="Pt0.A3.F7.3.2" class="ltx_text" style="font-size:90%;">
More examples of localizing instance categories across tasks.
</span></figcaption>
</figure>
<figure id="Pt0.A3.F8" class="ltx_figure"><img src="/html/2404.03179/assets/figures/open_visual_new.jpg" id="Pt0.A3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="580" height="369" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="Pt0.A3.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="Pt0.A3.F8.3.2" class="ltx_text" style="font-size:90%;">
More examples of open-vocabulary localization.
</span></figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.03178" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.03179" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.03179">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.03179" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.03180" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 22:33:15 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
