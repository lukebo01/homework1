<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.18537] Augmented Conversation with Embedded Speech-Driven On-the-Fly Referencing in AR</title><meta property="og:description" content="This paper introduces the concept of augmented conversation, which aims to support co-located in-person conversations via embedded speech-driven on-the-fly referencing in augmented reality (AR).
Today computing technol…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Augmented Conversation with Embedded Speech-Driven On-the-Fly Referencing in AR">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Augmented Conversation with Embedded Speech-Driven On-the-Fly Referencing in AR">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.18537">

<!--Generated on Wed Jun  5 17:44:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="augmented reality; mixed reality; natural language processing; speech recognition; keyword extraction; embedded visual referencest">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Augmented Conversation with Embedded Speech-Driven On-the-Fly Referencing in AR</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shivesh Jadon
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">University of Calgary</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Calgary</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">Canada</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:shivesh.jadon@ucalgary.ca">shivesh.jadon@ucalgary.ca</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mehrad Faridan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">University of Calgary</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_city">Calgary</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_country">Canada</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:mehrad.faridan1@ucalgary.ca">mehrad.faridan1@ucalgary.ca</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Edward Mah
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">University of Calgary</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_city">Calgary</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_country">Canada</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:edward.mah@ucalgary.ca">edward.mah@ucalgary.ca</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rajan Vaish
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id10.1.id1" class="ltx_text ltx_affiliation_institution">Snap Research, Easel AI</span><span id="id11.2.id2" class="ltx_text ltx_affiliation_city">Los Angeles</span><span id="id12.3.id3" class="ltx_text ltx_affiliation_country">United States</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:vaish.rajan@gmail.com">vaish.rajan@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wesley Willett
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">University of Calgary</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_city">Calgary</span><span id="id15.3.id3" class="ltx_text ltx_affiliation_country">Canada</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:wesley.willett@ucalgary.ca">wesley.willett@ucalgary.ca</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ryo Suzuki
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id16.1.id1" class="ltx_text ltx_affiliation_institution">University of Calgary</span><span id="id17.2.id2" class="ltx_text ltx_affiliation_city">Calgary</span><span id="id18.3.id3" class="ltx_text ltx_affiliation_country">Canada</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:ryo.suzuki@ucalgary.ca">ryo.suzuki@ucalgary.ca</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id19.id1" class="ltx_p">This paper introduces the concept of <span id="id19.id1.1" class="ltx_text ltx_font_bold ltx_font_italic">augmented conversation</span>, which aims to support co-located in-person conversations via embedded <span id="id19.id1.2" class="ltx_text ltx_font_italic">speech-driven on-the-fly referencing</span> in augmented reality (AR).
Today computing technologies like smartphones allow quick access to a variety of references during the conversation. However, these tools often create distractions, reducing eye contact and forcing users to focus their attention on phone screens and manually enter keywords to access relevant information.
In contrast, AR-based on-the-fly referencing provides relevant visual references in real-time, based on keywords extracted automatically from the spoken conversation.
By embedding these visual references in AR around the conversation partner, augmented conversation reduces distraction and friction, allowing users to maintain eye contact and supporting more natural social interactions.
To demonstrate this concept, we developed RealityChat, a Hololens-based interface that leverages real-time speech recognition, natural language processing and gaze-based interactions for on-the-fly embedded visual referencing.
In this paper, we explore the design space of visual referencing for conversations, and describe our our implementation — building on seven design guidelines identified through a user-centered design process.
An initial user study confirms that our system decreases distraction and friction in conversations compared to smartphone searches, while providing highly useful and relevant information.</p>
</div>
<div class="ltx_keywords">augmented reality; mixed reality; natural language processing; speech recognition; keyword extraction; embedded visual referencest
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>; May, 2024; </span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Mixed / augmented reality</span></span></span>
<figure id="S0.F1" class="ltx_figure ltx_teaserfigure"><img src="/html/2405.18537/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="141" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>On-the-fly conversational support through interactive augmented reality application. Our implementation transforms text from real-time speech recognition into visual overlays including interactive keywords and embedded visual references like maps, calendars, weather forecasts, photo galleries, Wikipedia articles, and search results. Individuals can use lightweight gaze and dwell interactions to select keywords and reveal additional details while remaining engaged in the conversation.</figcaption>
</figure>
<figure id="S0.F2" class="ltx_figure">
<div id="S0.F2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:723.8pt;vertical-align:-483.1pt;"><span class="ltx_transformed_inner" style="transform:translate(-172.4pt,95.7pt) scale(0.557019324123365,0.557019324123365) ;"><img src="/html/2405.18537/assets/figures/system-paris.jpg" id="S0.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="1064" height="598" alt="Refer to caption"><img src="/html/2405.18537/assets/figures/system-spain.jpg" id="S0.F2.2.g2" class="ltx_graphics ltx_img_landscape" width="1064" height="598" alt="Refer to caption"><img src="/html/2405.18537/assets/figures/system-picasso.jpg" id="S0.F2.3.g3" class="ltx_graphics ltx_img_landscape" width="1064" height="598" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>On-the-fly conversational support through an interactive augmented reality (AR) application. (Left and Middle) Users interact with an AR interface that transforms real-time speech into visual overlays, displaying information about Paris, including maps, weather forecasts, and landmarks. (Right) A user explores details about Pablo Picasso through an overlay featuring images and textual information. This AR-based system exemplifies the concept of augmented conversation by providing real-time visual references based on spoken conversation keywords.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Today, computing technologies like smartphones have both positive and negative impacts on co-located social conversations.
On one hand, these devices allow quick access to a huge variety of information that can complement ongoing discussions.
For example, we can quickly search for restaurant locations and ratings while making lunch plans, rapidly share photos or videos of recent trips when talking to friends, or quickly examine news or Wikipedia articles when unfamiliar topics or questions arise in conversations.
Most smartphone users rely on these kinds of <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">context-related on-the-fly referencing</span> every day <cite class="ltx_cite ltx_citemacro_citep">(Horrigan and Duggan, <a href="#bib.bib13" title="" class="ltx_ref">2015</a>)</cite>.
However, the use of smartphones also increases friction and distraction <cite class="ltx_cite ltx_citemacro_citep">(Dwyer et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite>, leaving people unable to concentrate fully on conversations.
Focusing on smartphone screens can reduce interpersonal eye contact, which may weaken the feeling of connection <cite class="ltx_cite ltx_citemacro_citep">(Yazıcıoğlu, <a href="#bib.bib52" title="" class="ltx_ref">2017</a>)</cite>.
Moreover, the presence of phones can distract people from their immediate social environment, potentially decreasing the enjoyment of social interactions <cite class="ltx_cite ltx_citemacro_citep">(Przybylski and Weinstein, <a href="#bib.bib38" title="" class="ltx_ref">2013</a>; Srivastava, <a href="#bib.bib45" title="" class="ltx_ref">2005</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address these challenges, we explore the design of technologies that can support everyday conversation without undermining our natural social interactions.
Specifically, we examine how augmented reality (AR) tools can <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">reduce friction</span> by presenting information dynamically during conversations, eliminating the need for multi-step smartphone searches. Presenting information visually and spatially in AR could also <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">reduce distraction</span>, making it possible to examine and share references without disrupting eye contact or interrupting social interactions.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While prior work (like Müller et al.’s CloudBits <cite class="ltx_cite ltx_citemacro_citep">(Müller et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017</a>)</cite>) has examined initial opportunities for presenting information in AR during conversations, we provide a deeper exploration of the design space of AR conversation support systems. We also demonstrate and evaluate an end-to-end system that leverages real-time speech recognition, language processing, and gaze to support fluid visual exploration of relevant references during conversations.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To provide a better understanding of the space of on-the-fly conversation augmentation, we first outline the concept of augmented conversation and present a design space that unpacks choices including: types of associated information, levels of personalization, modes of communication, technology, conversation partners, interaction modalities and levels of interactivity. We then introduce RealityChat, an augmented reality interface that leverages real-time speech recognition, natural language processing, and gaze-based interactive visual reference search in AR (Figure <a href="#S0.F1" title="Figure 1 ‣ Augmented Conversation with Embedded Speech-Driven On-the-Fly Referencing in AR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
RealityChat has the following key features:
<span id="S1.p4.1.1" class="ltx_text ltx_font_bold">1) Real-time speech recognition and transcription</span> lets the system recognize and transcribe the speech in real-time, letting users see the conversation unfold visually.
<span id="S1.p4.1.2" class="ltx_text ltx_font_bold">2) On-the-fly keyword extraction and referencing</span> translates key words in the conversation into <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">interactive</span> elements that provide a fast and fluid alternative to mid-conversation smartphone searches.
<span id="S1.p4.1.4" class="ltx_text ltx_font_bold">3) Visual embedding of references in AR</span> situates interactive keywords, maps, photos, and other visual references in the space immediately around the conversation partner, allowing users to quickly examine references without losing eye contact.
<span id="S1.p4.1.5" class="ltx_text ltx_font_bold">4) Gaze-based interaction</span> allows users to interact with visual elements without the use of gesture or speech commands which might disrupt ongoing conversations.
Our system uses the Microsoft Hololens 2 with modern WebXR tools including A-Frame, Three.js, and React.js. We also use the Web Speech API for real-time speech recognition, a Transformer-based neural network NLP pipeline for real-time keyword extraction, and various web-based APIs (Google Maps, Google Images, Wikipedia, etc.) for context-driven reference searchs.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.4" class="ltx_p">We evaluate the effectiveness of the interface with a usability study with thirteen participants.
Our overall findings indicate that, although participants were split on whether system was distracting or not, most of them agreed that it was intuitive (<math id="S1.p5.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S1.p5.1.m1.1a"><mi id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><ci id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">\mu</annotation></semantics></math>=5, <math id="S1.p5.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S1.p5.2.m2.1a"><mi id="S1.p5.2.m2.1.1" xref="S1.p5.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S1.p5.2.m2.1b"><ci id="S1.p5.2.m2.1.1.cmml" xref="S1.p5.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.2.m2.1c">\sigma</annotation></semantics></math>=1.15), reduced friction and provided better, on-the-fly referencing. Majority of the participants agreed that system features were helpful in reducing the distraction, especially pictures, map, and Wikipedia articles. The participants provided a very positive feedback with all most all the participants agreeing that the referencing features were very helpful and directly relevant in the conversation. However, most of the participants preferred visual features over textual information. In terms of
the design, the participants generally think the interface is well-designed, but needs improvement in visibility, and reference scale and positioning. However, almost all participants found the system interface to be intuitive and as natural as daily conversations (<math id="S1.p5.3.m3.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S1.p5.3.m3.1a"><mi id="S1.p5.3.m3.1.1" xref="S1.p5.3.m3.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S1.p5.3.m3.1b"><ci id="S1.p5.3.m3.1.1.cmml" xref="S1.p5.3.m3.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.3.m3.1c">\mu</annotation></semantics></math>=5, <math id="S1.p5.4.m4.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S1.p5.4.m4.1a"><mi id="S1.p5.4.m4.1.1" xref="S1.p5.4.m4.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S1.p5.4.m4.1b"><ci id="S1.p5.4.m4.1.1.cmml" xref="S1.p5.4.m4.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.4.m4.1c">\sigma</annotation></semantics></math>=1.15).
When asked about the usability of RealityChat in other social settings, participants were highly optimistic about its usefulness in educational (taking lecture notes, attending conferences, group studying, etc.) and professional settings (attending meetings, presenting, brainstorming sessions, etc.). They noted that keywords could help in creating an outline in lectures and conferences (P8), breakdown subject matter in steps for note taking (P9), help naturally search information in conversation (P2), easily take notes while listening to lectures (P4), and provide visual referencing to spoken words in lectures (P11).</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">For future work, we also discuss the realm of improvement and trade-off of the current system (e.g., different use cases, privacy issues, personalized and context-aware information, etc) based on the participants’ feedback.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The paper makes the following contributions:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A design space characterizing <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">augmented conversation</span> for on-the-fly context-driven referencing in augmented reality.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">RealityChat, a proof-of-concept augmented conversation prototype that demonstrates the potential of real-time speech recognition, keyword extraction, visual embedding of references, and gaze-based interaction.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Results and insights from an initial study that highlight the benefits and limitations of these approaches.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background and Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our research builds on previous work exploring the smartphones and user distraction, as well as existing systems for augmenting conversation, and presenting live information in AR.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Smartphone Use and Distraction</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">A growing body of research has shown that the use of smartphones in a social setting can negatively influence face-to-face interactions. While smartphone use can make individuals feel more productive by providing on-the-fly information that may be relevant, it often triggers more negative reactions from others.
Past work has shown that smartphone use can cause distraction from the original topic of the conversation <cite class="ltx_cite ltx_citemacro_citep">(Przybylski and Weinstein, <a href="#bib.bib38" title="" class="ltx_ref">2013</a>)</cite>, isolate users from their surroundings and inhibit engagement in public settings <cite class="ltx_cite ltx_citemacro_citep">(Su and Wang, <a href="#bib.bib46" title="" class="ltx_ref">2015</a>)</cite>.
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This isolation effect has been referred to as <span id="footnote1.1" class="ltx_text ltx_font_italic">phubbing</span> <cite class="ltx_cite ltx_citemacro_citep">(Haigh, <a href="#bib.bib12" title="" class="ltx_ref">2015</a>)</cite>, derived from a portmanteau of phone and snubbing.</span></span></span>
There is also qualitative and quantitative evidence <cite class="ltx_cite ltx_citemacro_citep">(Turkle, <a href="#bib.bib51" title="" class="ltx_ref">2012</a>)</cite> showing that using smartphones during conversations may degrade the shared social experience <cite class="ltx_cite ltx_citemacro_citep">(Shah, <a href="#bib.bib43" title="" class="ltx_ref">2003</a>)</cite>, reduce conversation quality <cite class="ltx_cite ltx_citemacro_citep">(Przybylski and Weinstein, <a href="#bib.bib38" title="" class="ltx_ref">2013</a>)</cite>, and limit social interactions <cite class="ltx_cite ltx_citemacro_citep">(Custers and Aarts, <a href="#bib.bib5" title="" class="ltx_ref">2005</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Context-Aware Conversational Assistance</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">To help address these issues, researchers in the HCI, CSCW, and visualization communities have explored approaches for <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">ambient</span> and <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">seamless</span> conversation assistance and awareness.
For example, researchers have explored approaches for aiding participants during conversations <cite class="ltx_cite ltx_citemacro_citep">(Eddie et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2015</a>)</cite>, supporting information retrieval in conversations through shared displays <cite class="ltx_cite ltx_citemacro_citep">(Lundgren and Torgersson, <a href="#bib.bib25" title="" class="ltx_ref">2013</a>; Moser et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2016</a>)</cite>, providing adaptive notifications <cite class="ltx_cite ltx_citemacro_citep">(Lopez-Tovar et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2015</a>)</cite>, and displaying topics for common interest <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2015</a>)</cite>.
ClearBoard <cite class="ltx_cite ltx_citemacro_citep">(Ishii and Kobayashi, <a href="#bib.bib14" title="" class="ltx_ref">1992</a>)</cite> also demonstrates how augmented communication can facilitate more seamless and natural collaboration by retaining eye-contact in remote collaboration settings.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Meanwhile, visualization tools like Bergstrom and Karahalios’s Conversation Clocks <cite class="ltx_cite ltx_citemacro_citep">(Bergstrom and Karahalios, <a href="#bib.bib3" title="" class="ltx_ref">2007</a>)</cite> and Kim et al.’s Meeting Mediator <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2008</a>)</cite> have sought to use live meeting audio to provide sociometric signals to help participants manage turn-taking and group interaction during meetings.
Other visualization tools (including examples like Conversation Clusters <cite class="ltx_cite ltx_citemacro_citep">(Bergstrom and Karahalios, <a href="#bib.bib4" title="" class="ltx_ref">2009</a>)</cite>, Shi et al.’s MeetingVis <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite>, and Aseniero et al.’s MeetCues <cite class="ltx_cite ltx_citemacro_citep">(Aseniero et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>) provide visual summaries of meetings, but — due to processing and rendering challenges — most work has tended to focus on post-meeting reviews rather than live access.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">In co-located settings, researchers have explored context-aware conversational assistants through <span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_italic">ambient voice search</span> <cite class="ltx_cite ltx_citemacro_citep">(Radeck-Arneth et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2014</a>)</cite> or <span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_italic">zero query search</span>.
For example, Ambient Search <cite class="ltx_cite ltx_citemacro_citep">(Radeck-Arneth et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2014</a>)</cite> is a screen-based conversational assistant that displays and retrieves relevant documents based on the topic of conversation.
Similarly, zero-query search or just-in-time information retrieval <cite class="ltx_cite ltx_citemacro_citep">(Rhodes and Maes, <a href="#bib.bib40" title="" class="ltx_ref">2000</a>)</cite> is a concept that provides information in nonintrusive manner based on the user’s local context.
This is becoming more available in commercial voice assistants, such as Google Now or Microsoft Cortana.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">However, most existing work has still focused on surfacing information on desktop or mobile displays <cite class="ltx_cite ltx_citemacro_citep">(Radeck-Arneth et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2014</a>; Shi et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2018</a>; Bergstrom and Karahalios, <a href="#bib.bib4" title="" class="ltx_ref">2009</a>; Aseniero et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2020</a>; ESTG, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>.
The advent of mainstream augmented reality hardware presents exciting opportunities to embed conversation aids into real world environments <cite class="ltx_cite ltx_citemacro_citep">(Müller et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017</a>; Monteiro et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023</a>; Rivu et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite>.
Initial explorations of this approach, including Müller et al.’s CloudBits <cite class="ltx_cite ltx_citemacro_citep">(Müller et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017</a>)</cite> and Rivu et al.’s StARe <cite class="ltx_cite ltx_citemacro_citep">(Rivu et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite> have used Wizard-of-Oz approaches to highlight the potential of simple conversation aids in AR.
Within this space, CloudBits focuses primarily on visualization and interaction techniques for revealing and arranging relevant snippets of shared information in one-on-one conversation settings. Meanwhile StARe examines the potential for gaze-based interactions with AR content positioned in space around a conversation partner. We expand upon this work, examining a broader design space of conversation augmentations and demonstrating a real-time end-to-end system that leverages speech recognition, natural language processing, and gaze-based interactions to support on-the-fly conversation augmentation in AR.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Real-time Captioning in AR</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">While our work features real-time speech recognition as a key component, AR-based real-time captioning itself is not new <cite class="ltx_cite ltx_citemacro_citep">(Liao et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>.
However, most of the existing applications have focused on transcription from an accessibility perspective, providing automatically generating a closed-captions for deaf and hard-of-hearing (DHH) individuals <cite class="ltx_cite ltx_citemacro_citep">(Mosbah, <a href="#bib.bib29" title="" class="ltx_ref">2006</a>; Peng et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>; Olwal et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Tu et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2020</a>; Miller et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2017</a>; Schipper and Brinkman, <a href="#bib.bib42" title="" class="ltx_ref">2017</a>; Findlater et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>; Jain et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2018a</a>, <a href="#bib.bib16" title="" class="ltx_ref">2015</a>, <a href="#bib.bib17" title="" class="ltx_ref">b</a>)</cite>.
For example, Jain et al. <cite class="ltx_cite ltx_citemacro_citep">(Jain et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2018b</a>)</cite> have developed accessible AR conversation aids to support captioning, environment navigation, and attentional balance between speakers. Similarly, Peng et al.’s SpeechBubbles <cite class="ltx_cite ltx_citemacro_citep">(Peng et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>, Olwal et al.’s Wearable Subtitles <cite class="ltx_cite ltx_citemacro_citep">(Olwal et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>, and Suemitsu et al.’s Caption Support System <cite class="ltx_cite ltx_citemacro_citep">(Suemitsu et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite> surface transcribed speech in viewers’ field of view, helping DHH individuals visually interpret conversations.
On top of captioning, researchers in this space have also investigated related factors such as visibility, usability, glanceability, and placement of AR closed captions <cite class="ltx_cite ltx_citemacro_citep">(Schipper and Brinkman, <a href="#bib.bib42" title="" class="ltx_ref">2017</a>; Jain et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2018a</a>)</cite>.
However, these AR-based speech tools have tended to focus on <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">recognition and captioning</span>, with little additional support for interaction or information retrieval based on them. Our work examines opportunities for making these embedded transcripts <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_italic">interactive</span>, enabling and supporting access to a richer range of information to complement and extend the text. With the recent advent of large language models (LLMs), we believe the integration of AR and AI <cite class="ltx_cite ltx_citemacro_citep">(Suzuki et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite> will become important for interactive conversation support in AR.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Gaze-Based Interaction in AR</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Finally, a variety of recent projects have used head-mounted AR displays for context-aware task support <cite class="ltx_cite ltx_citemacro_citep">(Lindlbauer et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>, providing adaptive information panels <cite class="ltx_cite ltx_citemacro_citep">(Dudley et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite> and showing visual references related to ongoing tasks <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>. Gaze-based interaction is becoming an increasingly popular input modality for hands-free, in-situ, and unobtrusive interaction <cite class="ltx_cite ltx_citemacro_citep">(Pfeuffer et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite> making it a great method of interaction for the these kinds of information displays.
For example, Lu et al.’s Glanceable AR <cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite> uses gaze to access information via unique interactions including head-glance, gaze-summon, and eye-glance. ARtention <cite class="ltx_cite ltx_citemacro_citep">(Pfeuffer et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite> explores a design space of AR gaze interaction across three dimensions: real vs. virtual, layers of content, and consumption vs. selection.
Systems like Shop-i <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2015</a>)</cite>, Looking for Info <cite class="ltx_cite ltx_citemacro_citep">(Piening et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite>, and StARe <cite class="ltx_cite ltx_citemacro_citep">(Rivu et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite> also highlight how a gaze-based interaction can be useful for on-demand information retrieval. Taking inspiration from this work, our system also uses gaze-based interactions to support in-situ and hands-free information access during live conversations.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Augmented Conversation: Concept</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We introduce the concept of <span id="S3.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">augmented conversation</span> to describe new interfaces that allow people to enhance their in-person conversations through <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">on-the-fly context-driven referencing</span>.
The core idea of this approach is a focus on <span id="S3.p1.1.3" class="ltx_text ltx_font_bold ltx_font_italic">speech and attention-driven</span> information retrieval, eschewing explicit keyword searches in favor of ambient presentation of references based on the content of the conversation and the use lightweight attention-based interactions to navigate and explore them. Augmented conversation interfaces like these can display useful contextual information during conversations, while allowing people to keep their focus primarily on the discussion at hand and it’s social participants.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2405.18537/assets/x2.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="187" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>The design space of augmented conversation approaches is large and diverse. Here we identify a variety of possible design options for augmented conversation spanning seven design dimensions. Design possibilities explored in our RealityChat prototype are highlighted in <span id="S3.F3.2.1" class="ltx_text" style="color:#008080;background-color:#CCE6E6;">blue</span>.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Key Features</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As highlighted in these scenarios, our vision of augmented conversation encompasses several key features:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">1. Real-Time Speech Transcription:</span>
Recognizing and transcribing speech in real-time to create a text record of the conversation.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">2. On-the-Fly Referencing:</span>
Dynamically identifying key terms in the conversation and using them to automatically conduct on-the-fly searches for relevant content.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">3. Contextual Information Presentation:</span>
Using contextual information about the scene and references to reduce clutter and highlight relevant information using appropriate representations such as images, maps, article snippets, etc.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold ltx_font_italic">4. Embedded Visual References:</span>
Integrating visual references into the surrounding environment and/or around other speakers to help viewers maintain eye-contact and focus rather than staring at screens or mobile devices.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para ltx_noindent">
<p id="S3.SS1.p6.1" class="ltx_p"><span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_bold ltx_font_italic">5. Natural Interaction:</span>
Relying on implicit and explicit natural interactions to maintain focus on the live conversation, rather than interacting with an interface.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Design Space</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">As illustrated by the scenarios above, augmented conversation tools have the potential to take many different forms — relying on a variety of hardware platforms, interactions, reference types, and visual representations to suit different kinds of conversations and environments. Here, we present an initial design space (Figure <a href="#S3.F3" title="Figure 3 ‣ 3. Augmented Conversation: Concept ‣ Augmented Conversation with Embedded Speech-Driven On-the-Fly Referencing in AR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) that illustrates the diversity of possible augmented conversation approaches, as well as some promising opportunities.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">D1. Types of Associated Information:</span>
Augmented conversation has the potential to enhance conversations with many different types of associated information. These could include a variety of visual, spatial, and text-based content including images, videos, text description, news articles, charts, networks, 3D models, maps, and calendars, as well as other context-aware information like transit schedules and travel times.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">D2. Level of Personalization:</span>
Embedded information can be also categorized according to its degree of privacy and sensitivity, ranging from <span id="S3.SS2.p3.1.2" class="ltx_text ltx_font_italic">public</span> to <span id="S3.SS2.p3.1.3" class="ltx_text ltx_font_italic">personal</span> information.
For example, a user might augment their conversation through publicly available images like a picture of Rocky Mountain or personal photos from the user’s own collection.
Other kinds of personal data, including information about past conversations or private search histories could be more useful, but entail trade-offs related to privacy, visibility, and trust — both in the system and conversation partners. As Müller et al. highlight in their CloudBits work, including more private data may also have implications for the design and visibility of shared information <cite class="ltx_cite ltx_citemacro_citep">(Müller et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">D3. Modes of Communication:</span>
Augmented conversation can support various modes of communication, including <span id="S3.SS2.p4.1.2" class="ltx_text ltx_font_italic">conversation, discussion, listening, presentation, and self-reflection</span>.
The modes of communication can be largely categorized as <span id="S3.SS2.p4.1.3" class="ltx_text ltx_font_italic">active</span> or <span id="S3.SS2.p4.1.4" class="ltx_text ltx_font_italic">passive</span>. In the active communication, the user actively engages with the conversation or reflection, such as in-person conversation, brainstorming discussion with a team, or self-reflection.
On the other hand, augmented communication tools could also enhance the experience of passively listening to a conversation or discussion, including in larger meetings or lecture settings.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold ltx_font_italic">D4. Technology for Augmentation:</span>
There are various ways to display conversation augmentations in the real world, including both <span id="S3.SS2.p5.1.2" class="ltx_text ltx_font_italic">on-body</span> devices and displays in the environment <span id="S3.SS2.p5.1.3" class="ltx_text ltx_font_italic">environment</span>.
These include a variety of immersive technologies, including <span id="S3.SS2.p5.1.4" class="ltx_text ltx_font_italic">head-mounted displays (HMDs), mobile AR on smartphones or tablets, and projection mapping, as well as more traditional displays and tabletops</span>.
Portable, on-body approaches like HMDs and mobile AR could allow users to augment conversations in many different environments, enabling more ubiquitous augmentation.
Personal devices like HMDs and mobile AR also allow more <span id="S3.SS2.p5.1.5" class="ltx_text ltx_font_italic">private and personal</span> context-driven referencing.
On the other hand, public displays and approaches like projection mapping could support <span id="S3.SS2.p5.1.6" class="ltx_text ltx_font_italic">public</span> augmented conversation, revealing consistent information to groups in settings like group brainstorms or in-person lectures.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.1" class="ltx_p"><span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_bold ltx_font_italic">D5. Conversation Partners:</span>
Augmented conversation approaches could apply to a variety of different conversation scenarios, including <span id="S3.SS2.p6.1.2" class="ltx_text ltx_font_italic">one-on-one</span> and <span id="S3.SS2.p6.1.3" class="ltx_text ltx_font_italic">group</span> discussions, as well as conversations with <span id="S3.SS2.p6.1.4" class="ltx_text ltx_font_italic">oneself</span> or with <span id="S3.SS2.p6.1.5" class="ltx_text ltx_font_italic">non-human agents</span>.
While one-on-one and group conversations have received the most discussion in the literature, visual augmentations for individual monologues (in which a person speaks aloud with no other audience) could create new opportunities for personal notetaking and ideation.
Augmented conversation with non-human agents, such as smartphones, smart speakers, IoT devices, and robots could also make it easier for people to interact with them without relying on mobile devices. For example, augmented conversation approaches could allow users to visually see keywords or concepts that emerge in podcasts or in exchanges with digital assistants.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para ltx_noindent">
<p id="S3.SS2.p7.1" class="ltx_p"><span id="S3.SS2.p7.1.1" class="ltx_text ltx_font_bold ltx_font_italic">D6. Interaction Modalities:</span>
There are a large number of possible ways to interact with augmentation conversation interfaces, including <span id="S3.SS2.p7.1.2" class="ltx_text ltx_font_italic">touch, gesture, controllers, voice commands, gaze or head movement, and proximity</span>.
The available interaction modalities are also related to which technology is used. For example, HMDs allow gaze and gestural interaction, while mobile AR does not. Similarly, projection mapping would be more suitable for touch or tangible interaction.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para ltx_noindent">
<p id="S3.SS2.p8.1" class="ltx_p"><span id="S3.SS2.p8.1.1" class="ltx_text ltx_font_bold ltx_font_italic">D7. Level of Interactivity:</span>
Level of interactivity refers to how much active intervention an augmented conversation system requires.
Systems with <span id="S3.SS2.p8.1.2" class="ltx_text ltx_font_italic">low interactivity</span> might show information related to the conversation automatically without requiring (or allowing) the viewer to direct what information is displayed or query further. On the other hand, <span id="S3.SS2.p8.1.3" class="ltx_text ltx_font_italic">high interactivity</span> systems could permit more active modification, organization, and querying.</p>
</div>
<div id="S3.SS2.p9" class="ltx_para ltx_noindent">
<p id="S3.SS2.p9.1" class="ltx_p"><span id="S3.SS2.p9.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Summary of the Design Space:</span>
Figure <a href="#S3.F3" title="Figure 3 ‣ 3. Augmented Conversation: Concept ‣ Augmented Conversation with Embedded Speech-Driven On-the-Fly Referencing in AR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> summarizes all of the dimensions that we discussed in this section, with the set of design options we explored in our RealityChat prototype highlighted in blue.
While our current implementation considers only a subset of the possible designs, the broader design space highlights a numerous opportunities for future work. In fact, given recent advances in natural language processing and context-aware information retrieval, we expect that implementations of most of the possibilities we discuss are already possible.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>The RealityChat System</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To demonstrate the concept of augmented conversation, we developed RealityChat, an augmented reality system for on-the-fly context-driven referencing that instantiates many of the augmented conversation concepts described above.
RealityChat supports <span id="S4.p1.1.1" class="ltx_text ltx_font_bold">real-time speech recognition</span>, <span id="S4.p1.1.2" class="ltx_text ltx_font_bold">on-the-fly keyword extraction and referencing</span>, <span id="S4.p1.1.3" class="ltx_text ltx_font_bold">contextual information retrieval</span>, <span id="S4.p1.1.4" class="ltx_text ltx_font_bold">embedded visual references</span>, and <span id="S4.p1.1.5" class="ltx_text ltx_font_bold">gaze-based interactions</span>. The prototype uses WebXR (A-Frame and React.js) for rendering, Web Speech API for real-time speech recognition, a Transformer-based neural network (for named-entity and keyword extraction), and various web-based APIs (Google Maps and Images, Wikipedia search, etc.) for context-driven keyword reference searches.
The prototype is then deployed on the HoloLens 2.
Figure <a href="#S0.F1" title="Figure 1 ‣ Augmented Conversation with Embedded Speech-Driven On-the-Fly Referencing in AR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the user interface and interactions with RealityChat in an in-person conversation, while Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1. Design Goals ‣ 4. The RealityChat System ‣ Augmented Conversation with Embedded Speech-Driven On-the-Fly Referencing in AR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the overall software structure.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Design Goals</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We developed our prototype using an iterative approach, starting from paper prototypes and low-fidelity experiments with a webcam and web browser. Gradually, we developed a functional tablet-based AR prototype before finally transitioning to a full head-mounted AR device, the HoloLens 2.
At each stage, the authors tested each prototype themselves and informally recruited participants to test successive iterations in both scripted and unscripted conversations. During these design iterations, we formulated a series of design goals that guided the feature set and affordances of our final system.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">G1. Maintain Natural Social Interactions:</span>
The focus of social interactions (including conversations) is always the people, not the supplemental information, thus we should maintain natural conversational gestures such as eye contact.
Thus, it is important to avoid any unnecessary distractions that might hinder the flow of the conversation.
It became clear during the prototyping phase that AR itself created opportunities for distraction and the division of attention. For example, if the embedded information were overlaid on top of the social participant’s face, it would erode the possibility of making eye contact. On the other hand, if the displayed information is very far away, it would force users to tilt their heads at awkward angles and unnecessarily prolong each interaction.
Therefore, it was imperative that we design the user interface such that the possibility of our system itself distracting the user be minimized by identifying bad interface designs and complex interaction techniques.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">G2. Augment Not Automate:</span>
In the prototyping phase, we have tested various different designs and approaches, including auto-generated word clouds and automated references.
We quickly noticed that the unnecessary automation increases the noise of the conversation, forcing the user to pay attention to unimportant information. Instead, a more flexible <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_italic">interactive system</span> that allows users to select which keywords to highlight and display contextual information about, would result in a less error-prone system for augmented conversations.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2405.18537/assets/x3.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Overview of the key features of our RealityChat, each with corresponding design goals. </figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold ltx_font_italic">G3. Encourage Visual Priority:</span>
Through informal testing, we observed that textual representation of the keyword contextual references requires much more time to analyze and understand than visual representations. By the time users have read, analyze and understand the textual information, the conversation topic has changed or a break in the conversation was required to digest the information or users had to give up reading the textual representation at all to keep up with the conversation. Therefore, visual representations should be prioritized when possible.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold ltx_font_italic">G4. Minimize Interaction Cost and Time:</span>
When designing an augmented conversation interface, there are various potential interaction modalities. In AR, gesture, controller, or voice-based commands are the most common interface techniques. While expressive, these interactions often require more time and steps to interact with words and information than would be ideal. For example, gestural interactions, even a minimal air-tap, create an awkward behavior and atmosphere during a conversation. Voice commands are by nature impractical for our system since they’d require differentiating between keywords and voice commands, not to mention how bizarre and awkward it is in the middle of a conversation. Additionally, we observed that conversations are often fast-paced and always in real-time. This meant that the topic of conversation was fluid, often changing (sometimes quite randomly). Thus time is of the essence, and time spent interacting with or analyzing keywords is time not spent on social cues, focusing on the conversation or social participants. Thus, it is critical to minimize the time and effort required for users to interact with our system for keyword selection and lookup. This made gaze or head-movement-based the most attractive option due to its minimal interaction time and cost to maintain the flow of the conversation.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para ltx_noindent">
<p id="S4.SS1.p6.1" class="ltx_p"><span id="S4.SS1.p6.1.1" class="ltx_text ltx_font_bold ltx_font_italic">G5. Avoid Accidental Interaction:</span>
Unexpected behaviors and interactions frustrate users and create the potential for both distraction and friction. Unfortunately, gaze- or head-movement-based interaction often causes accidental interactions or unexpected UI triggers, which undermines the user experience and delays critical reference information.
Thus a balance must be struck between minimal interaction and minimizing accidental interactions.
We alleviate this problem by confirming interactions by waiting for a couple of seconds or by adding visual feedback on the current state. That is, we use gaze-and-dwell as our interaction method.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para ltx_noindent">
<p id="S4.SS1.p7.1" class="ltx_p"><span id="S4.SS1.p7.1.1" class="ltx_text ltx_font_bold ltx_font_italic">G6. Reduce the Cognitive Load for Content Consumption:</span>
While prototyping, we modified the layout of a conversational transcript multiple times. Initially, we displayed the entire transcript, continuously updated as the conversation went on and highlighted identified keywords within the transcript. Not only was this not efficient, but it was time consuming and overwhelming to users. They didn’t need an entire transcript nor was it intuitive to select keywords based on where in the transcript they occurred. Next, we modified the transcript to only display the last sentence. At this point, we already had to display the keywords elsewhere and informal user testing revealed that having a transcript wasn’t really useful. Thus, our final prototype features no transcripts (as our visual priority design would recommend) but instead a list of keywords for users to select from. To reduce the cognitive load further, we colored keywords by category, red for people, blue for locations, purple for organizations and green for dates. Each category also has a corresponding icon. See figure 4 for an example.</p>
</div>
<div id="S4.SS1.p8" class="ltx_para ltx_noindent">
<p id="S4.SS1.p8.1" class="ltx_p"><span id="S4.SS1.p8.1.1" class="ltx_text ltx_font_bold ltx_font_italic">G7. Minimize Latency for Real-time Augmentation:</span>
Finally, latency is the key to a real-time conversation.
Through our testing, it turns out this is a much more crucial factor than we originally anticipated.
In a fast-paced real-world conversation, the topic can quickly change in real-time.
Therefore, the system needs to have a small latency for <span id="S4.SS1.p8.1.2" class="ltx_text ltx_font_italic">every single aspect</span>, including speech recognition, keyword extraction, reference search, and visual rendering.
As we learned, for augmented conversation, latency can directly affect usability, thus minimizing the latency is the key to system design.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Implementation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Based on the design goals, we implemented our final prototype built by WebAR for Microsoft HoloLens.
Each design feature and functionality follows the design goals identified through the iterative design process.
Our system features embedded references with visual priority, head-based interaction to minimize interaction cost, minimal content placement to reduce cognitive load and maintain social interaction, and reduce latency for real-time conversation.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Corresponding Design Space:</span>
As we discussed in Section 3, there are many possible implementation approaches, but in our system, we specifically focused on <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_bold">HMD-based approach (D4)</span> using Microsoft HoloLens 2 for the ubiquity of the augmentation, given the prediction of HMD will be widely available in the next decades.
For other design spaces, we covered <span id="S4.SS2.p2.1.3" class="ltx_text ltx_font_bold">most of the types of associated information (D1)</span> but focused on <span id="S4.SS2.p2.1.4" class="ltx_text ltx_font_bold">non-personalized information (D2)</span> due to the difficulty of implementing personalized information for user testing.
For interaction, we focus on <span id="S4.SS2.p2.1.5" class="ltx_text ltx_font_bold">gaze- or head-based interaction (D6)</span> and <span id="S4.SS2.p2.1.6" class="ltx_text ltx_font_bold">low level of interactivity (D7)</span> informed by the design goals and user-centered design process.
While our system itself can be used for various modes of communication (D3) and use cases (D5), we focus on our use cases mostly centered around <span id="S4.SS2.p2.1.7" class="ltx_text ltx_font_bold">in-person conversation (D3 and D5)</span>, which is our focus in the user evaluation study.
In the following subsections, we describe the implementation details.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Real-time Speech Recognition using the Web Speech API</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">First, the system uses the Web Speech API for real-time speech recognition and transcription.
Web Speech API is a widely available speech recognition engine that can be used in a browser.
The reason why we used Web Speech API is to <span id="S4.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">reduce the latency (G7)</span>.
We originally tested with relatively higher-quality server-based Google or Microsoft speech recognition API, but we noticed that this API-based approach often courses higher latency. Thus, we use Web Speech API.
Microsoft HoloLens 2 has HoloLens Edge browser, but Web Speech API is not yet available in HoloLens Edge browser, thus we place the Android phone next to the user and use Google Chrome-based Web Speech API instead.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">Web Speech API can have several options such as <span id="S4.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_italic">continuous</span> mode, in which we can decide whether or not to use a speech sentence before the speaker finishes talking.
We enable continuous mode and mainly use intermediate results for lower latency.
In a regular conversation with 110 words/min, our Web Speech API implementation recognizes, transcribes, and send the text at 360 times/min for continuous mode and 10 times/min for single mode.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Synchronous Communication through WebSocket</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">In our system architecture, a web browser (Android’s Google Chrome) transcribes the speech to text and sends the detected text to a Node.js web server on MacBook Pro through the WebSocket protocol.
WebSocket protocol allows faster synchronous communication to <span id="S4.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_bold">reduce the latency (G7)</span> than the other protocol like TCP/IP communication.
Once the Node.js server detects the keywords through the above pipeline, it broadcasts the result to all connected browsers (Microsoft HoloLens Edge browser) in JSON format through the WebSocket protocol.
The server-side machine (MacBook Pro), runs a Node.js server and Python script. Whenever the server receives the newly transcribed text, it runs and gets the result through an efficient inter-process communication based on Node PythonShell. The overall latency from sending the transcribed text to receiving the result in a client-side browser is under 10ms.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2405.18537/assets/x4.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="308" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>The RealityChat system as seen from the point of view of a user wearing a HoloLens 2.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2405.18537/assets/x5.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="101" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Closeup views of the visual reference types implemented in RealityChat including keywords, maps, weather, image search results, and Wikipedia snippets. Keywords are extracted from the live transcript (upper left) and labeled by category (organization, location, date, and person). The transcript itself is not shown in the AR view. </figcaption>
</figure>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>Real-time Keyword Extraction through Transformer-Based Natural Language Processing</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">Second, the system extracts keywords and categorizes the types of keywords based on natural language processing.
The reason why we used keyword extraction is to <span id="S4.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_bold">reduce the cognitive load for content consumption (G6)</span>.
We originally let users directly select words from the transcript. However this causes several issues, since the transcript flow was often extremely fast, which caused the amount of content to quickly become overwhelming.
While we tested several keyword extraction engines, we ultimately used Spacy3, a Transformer-based deep neural network pipeline built with Python and Cython.
We use a pre-trained model based on Spacy’s tokenizer and word embeddings model (en_core_web_sm).</p>
</div>
<div id="S4.SS2.SSS3.p2" class="ltx_para">
<p id="S4.SS2.SSS3.p2.1" class="ltx_p">In our NLP pipeline, we first detect noun phrases and combine words (for example, we treat <span id="S4.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_italic">“Human Computer Interaction”</span> as one word, instead of <span id="S4.SS2.SSS3.p2.1.2" class="ltx_text ltx_font_italic">“Human”</span>, <span id="S4.SS2.SSS3.p2.1.3" class="ltx_text ltx_font_italic">“Computer”</span>, and <span id="S4.SS2.SSS3.p2.1.4" class="ltx_text ltx_font_italic">“Interaction”</span>).
Then, we detect the named entity for each word to classify the word to different categories.
For named-entity categories, we have <span id="S4.SS2.SSS3.p2.1.5" class="ltx_text ltx_font_italic">organization</span>, <span id="S4.SS2.SSS3.p2.1.6" class="ltx_text ltx_font_italic">location</span>, <span id="S4.SS2.SSS3.p2.1.7" class="ltx_text ltx_font_italic">person</span>, <span id="S4.SS2.SSS3.p2.1.8" class="ltx_text ltx_font_italic">date</span>, and <span id="S4.SS2.SSS3.p2.1.9" class="ltx_text ltx_font_italic">numerical values</span> (for example detect <span id="S4.SS2.SSS3.p2.1.10" class="ltx_text ltx_font_italic">“Google”</span> as <span id="S4.SS2.SSS3.p2.1.11" class="ltx_text ltx_font_italic">“organization”</span> and <span id="S4.SS2.SSS3.p2.1.12" class="ltx_text ltx_font_italic">“New York”</span> as <span id="S4.SS2.SSS3.p2.1.13" class="ltx_text ltx_font_italic">“location”</span>).
Finally, we extract and rank keywords using a simple TextRank algorithm <cite class="ltx_cite ltx_citemacro_citep">(Mihalcea and Tarau, <a href="#bib.bib26" title="" class="ltx_ref">2004</a>)</cite>, which extracts key phrases based on a graph algorithm that is independent of a specific natural language and does not require domain knowledge.
We use the phrase which has a positive TextRank value as extracted key phrases.
The Transformer-based keyword extraction pipeline runs with Python 3.7 on MacBook Pro 16inch Intel Core i7 CPU, 16GB RAM.
Our keyword extraction pipeline works in real-time with an average of 684 words per second on CPU.
The response time will quickly increase based on the number of words, thus we run this extraction pipeline separately for each intermediate result, rather than all of the accumulated text.</p>
</div>
</section>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4. </span>Query Search through Third-party API</h4>

<div id="S4.SS2.SSS4.p1" class="ltx_para">
<p id="S4.SS2.SSS4.p1.1" class="ltx_p">When the user selects a word, the system also provides a reference related to the information based on <span id="S4.SS2.SSS4.p1.1.1" class="ltx_text ltx_font_bold">visual priority (G2)</span>.
For associated information, we implemented <span id="S4.SS2.SSS4.p1.1.2" class="ltx_text ltx_font_bold ltx_font_italic">image search</span> based on the Google and DuckDuckGo Image APIs, <span id="S4.SS2.SSS4.p1.1.3" class="ltx_text ltx_font_bold ltx_font_italic">maps</span> based on MapBox API, <span id="S4.SS2.SSS4.p1.1.4" class="ltx_text ltx_font_bold ltx_font_italic">search results and news articles</span> based on Google and DuckDuckGo Custom Search APIs, <span id="S4.SS2.SSS4.p1.1.5" class="ltx_text ltx_font_bold ltx_font_italic">weather</span> based on the Open Weather Map API,
and <span id="S4.SS2.SSS4.p1.1.6" class="ltx_text ltx_font_bold ltx_font_italic">text descriptions</span> based on Wikipedia API. All of the API requests are completed either through client-side JavaScript or server-side Node.js code.
For all words, we provide visual references based on <span id="S4.SS2.SSS4.p1.1.7" class="ltx_text ltx_font_italic">images</span> and <span id="S4.SS2.SSS4.p1.1.8" class="ltx_text ltx_font_italic">search results</span>.
To provide richer context-driven references for entities, we use a custom query search based on the named entity category identified through the NLP pipeline.
For example, for location-related keywords we provide <span id="S4.SS2.SSS4.p1.1.9" class="ltx_text ltx_font_italic">maps</span> and <span id="S4.SS2.SSS4.p1.1.10" class="ltx_text ltx_font_italic">weather</span>, for proper nouns we provide <span id="S4.SS2.SSS4.p1.1.11" class="ltx_text ltx_font_italic">Wikipedia</span> information, for date-related information we show a <span id="S4.SS2.SSS4.p1.1.12" class="ltx_text ltx_font_italic">calendar</span>, etc.
To <span id="S4.SS2.SSS4.p1.1.13" class="ltx_text ltx_font_bold">reduce the latency (G7)</span>, we pre-process all of the extracted keywords in the background to obtain the associated information (only images and search results) before the user interaction happens, so that the system can show this associated information immediately when the user selects it.</p>
</div>
</section>
<section id="S4.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.5. </span>Embedded Visual Reference Rendering using A-Frame, Three.js, and HTML Canvas</h4>

<div id="S4.SS2.SSS5.p1" class="ltx_para">
<p id="S4.SS2.SSS5.p1.1" class="ltx_p">All of the processed results including extracted keywords and associated information are shown through embedded graphics in AR.
For rendering, we implemented with A-Frame and Three.js, embedded in the 3D space.
Since most of the information can be shown as 2D objects, we use Three.js Canvas Textures to show them as an interactive HTML element built with React.js and Semantic UI.</p>
</div>
<div id="S4.SS2.SSS5.p2" class="ltx_para">
<p id="S4.SS2.SSS5.p2.1" class="ltx_p">Our user interface design is followed by the design goals.
First, we implemented a JavaScript-based prototype that renders the interface, along with all the associated information panels close to the speaker’s face. This allows the user to maintain the eye-contact while showing the information at a minimal distance from the other user.
This helps the user to
<span id="S4.SS2.SSS5.p2.1.1" class="ltx_text ltx_font_bold">maintain natural social interaction (G1)</span>.
Second, we build our system as an interactive interface so that the user can select the required information by themselves through interaction, based on <span id="S4.SS2.SSS5.p2.1.2" class="ltx_text ltx_font_bold">augmentation rather than automation (G2)</span> principle.
Our interface basically consists of a transcribed text, keyword list, and associated information. The user mainly interacts with the keyword list, compared to the raw transcribed text to reduce the noise and easily let the user identifies the topic.
This can <span id="S4.SS2.SSS5.p2.1.3" class="ltx_text ltx_font_bold">reduce the cognitive load (G6)</span> for the user.
Also, we only have one main associated information panel so that the user can only focus on one thing, while the user can quickly access the other information through the accompanied history view on the side.</p>
</div>
</section>
<section id="S4.SS2.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.6. </span>Head-Gaze Interaction through Hololens 2</h4>

<div id="S4.SS2.SSS6.p1" class="ltx_para">
<p id="S4.SS2.SSS6.p1.1" class="ltx_p">We use Microsoft Hololens 2 for AR rendering devices.
To <span id="S4.SS2.SSS6.p1.1.1" class="ltx_text ltx_font_bold">minimize interaction time (G4)</span>, we use head-gaze as a primary interaction modality.
While Hololens 2 provides both eye-gaze and head-gaze input, in the WebXR framework, currently head-gaze input is only available.
Therefore, we place a cursor in the center of the screen and let the user selects the word by moving her head.
To <span id="S4.SS2.SSS6.p1.1.2" class="ltx_text ltx_font_bold">maintain natural social interaction (G1)</span>, we do not require a common selection technique like air tap.
Instead, whenever the cursor hovers the clickable object, the system recognizes it as a selection input.
However, this native implementation often causes unexpected input.
To <span id="S4.SS2.SSS6.p1.1.3" class="ltx_text ltx_font_bold">avoid accidental interaction (G5)</span>, we also implemented a time-based trigger and visual indicator, so that the system only confirms the selection when the user pauses for at least 250 ms.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Evaluation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We validated our system via an exploratory usability study. The goal of this study was to evaluate the usability of our prototype and to identify limitations or opportunities for future improvements.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Method and Participants</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We recruited 13 participants (9 female / 4 male, ages 18-27) to each take part in a 50 minute in-person session in which they used our prototype during three topic-driven conversations with a member of the study team.
We first introduced the concept of augmented conversation and gave 5 minute live demo of the RealityChat system.
Then, we asked the participants to take part in three 5-minute conversations with one of the experimenters. Each conversation focused on a specific topic (vacation, career, or family). To ensure that all participants had a similar experience, the experimenter used a consistent script to guide their responses, but allowed the participant to direct the conversation and introduce new topics. All participants were fluent English speakers (9 were native English speakers while the rest spoke English as their second language) and conducted all conversations in English.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">During the conversation, both the participant and the experimenter wore a Microsoft HoloLens 2 and participants had access to the RealityChat prototype for the entirety of their conversation.
After the session, we asked the participants to give feedback about the interface and their experience through an online questionnaire. Finally, we conducted a short semi-structured interview to capture qualitative feedback and asked participants to complete a short recall quiz designed to assess their attention to the conversation. The quiz contained 10 questions with answers drawn from the experimenter’s conversation script.
We recorded all participant interactions both through the participant’s HoloLens and via an external video camera.
Each participant was compensated $10 CAD.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Results</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.5" class="ltx_p">Overall, the participants responded very positively to their experience with the prototype.
When we asked 7-point Likert-scale questions about the experience (1=<span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">Strongly Disagree<math id="S5.SS2.p1.1.1.m1.1" class="ltx_Math" alttext="\leftrightarrow" display="inline"><semantics id="S5.SS2.p1.1.1.m1.1a"><mo stretchy="false" id="S5.SS2.p1.1.1.m1.1.1" xref="S5.SS2.p1.1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.1.m1.1b"><ci id="S5.SS2.p1.1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.1.m1.1c">\leftrightarrow</annotation></semantics></math></span> 7=<span id="S5.SS2.p1.5.2" class="ltx_text ltx_font_italic">Strongly Agree</span>), participants reported that they found the overall system helpful and intuitive (<math id="S5.SS2.p1.2.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S5.SS2.p1.2.m1.1a"><mi id="S5.SS2.p1.2.m1.1.1" xref="S5.SS2.p1.2.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m1.1b"><ci id="S5.SS2.p1.2.m1.1.1.cmml" xref="S5.SS2.p1.2.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m1.1c">\mu</annotation></semantics></math>=5, <math id="S5.SS2.p1.3.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S5.SS2.p1.3.m2.1a"><mi id="S5.SS2.p1.3.m2.1.1" xref="S5.SS2.p1.3.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m2.1b"><ci id="S5.SS2.p1.3.m2.1.1.cmml" xref="S5.SS2.p1.3.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m2.1c">\sigma</annotation></semantics></math>=1.15). Participants also agreed that the referencing features were very helpful and directly relevant to the conversation (<math id="S5.SS2.p1.4.m3.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S5.SS2.p1.4.m3.1a"><mi id="S5.SS2.p1.4.m3.1.1" xref="S5.SS2.p1.4.m3.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m3.1b"><ci id="S5.SS2.p1.4.m3.1.1.cmml" xref="S5.SS2.p1.4.m3.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m3.1c">\mu</annotation></semantics></math>=6, <math id="S5.SS2.p1.5.m4.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S5.SS2.p1.5.m4.1a"><mi id="S5.SS2.p1.5.m4.1.1" xref="S5.SS2.p1.5.m4.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m4.1b"><ci id="S5.SS2.p1.5.m4.1.1.cmml" xref="S5.SS2.p1.5.m4.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m4.1c">\sigma</annotation></semantics></math>=0.91).
Moreover, all participants agreed that the approach had great potential and expressed interest in using augmented conversation tools in the future. Broadly, our observations from the study highlighted the relevance of visual references, as well as trade-offs related to distraction and system efficiency.</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1. </span>Content Relevance</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.2" class="ltx_p">Participants found the references very useful, with almost all reporting that the visual references were directly relevant to the conversation (<math id="S5.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S5.SS2.SSS1.p1.1.m1.1a"><mi id="S5.SS2.SSS1.p1.1.m1.1.1" xref="S5.SS2.SSS1.p1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p1.1.m1.1b"><ci id="S5.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p1.1.m1.1c">\mu</annotation></semantics></math>=6, <math id="S5.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S5.SS2.SSS1.p1.2.m2.1a"><mi id="S5.SS2.SSS1.p1.2.m2.1.1" xref="S5.SS2.SSS1.p1.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p1.2.m2.1b"><ci id="S5.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S5.SS2.SSS1.p1.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p1.2.m2.1c">\sigma</annotation></semantics></math>=1.15).
For example, P11 noted that <span id="S5.SS2.SSS1.p1.2.1" class="ltx_text ltx_font_italic">“All the references that did pop up were directly related to the main ideas of what was said.”</span>, while P8 added that <span id="S5.SS2.SSS1.p1.2.2" class="ltx_text ltx_font_italic">“The keywords were really helpful because looking back at them helped me remember the key topics of conversation so far.”</span> (P8).</p>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para">
<p id="S5.SS2.SSS1.p2.8" class="ltx_p">Images were the most popular visual reference (<math id="S5.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S5.SS2.SSS1.p2.1.m1.1a"><mi id="S5.SS2.SSS1.p2.1.m1.1.1" xref="S5.SS2.SSS1.p2.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.1.m1.1b"><ci id="S5.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p2.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.1.m1.1c">\mu</annotation></semantics></math>=5.5, <math id="S5.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S5.SS2.SSS1.p2.2.m2.1a"><mi id="S5.SS2.SSS1.p2.2.m2.1.1" xref="S5.SS2.SSS1.p2.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.2.m2.1b"><ci id="S5.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S5.SS2.SSS1.p2.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.2.m2.1c">\sigma</annotation></semantics></math>=0.77), followed by maps (<math id="S5.SS2.SSS1.p2.3.m3.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S5.SS2.SSS1.p2.3.m3.1a"><mi id="S5.SS2.SSS1.p2.3.m3.1.1" xref="S5.SS2.SSS1.p2.3.m3.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.3.m3.1b"><ci id="S5.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S5.SS2.SSS1.p2.3.m3.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.3.m3.1c">\mu</annotation></semantics></math>=5.1, <math id="S5.SS2.SSS1.p2.4.m4.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S5.SS2.SSS1.p2.4.m4.1a"><mi id="S5.SS2.SSS1.p2.4.m4.1.1" xref="S5.SS2.SSS1.p2.4.m4.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.4.m4.1b"><ci id="S5.SS2.SSS1.p2.4.m4.1.1.cmml" xref="S5.SS2.SSS1.p2.4.m4.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.4.m4.1c">\sigma</annotation></semantics></math>=1.14) and text (<math id="S5.SS2.SSS1.p2.5.m5.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S5.SS2.SSS1.p2.5.m5.1a"><mi id="S5.SS2.SSS1.p2.5.m5.1.1" xref="S5.SS2.SSS1.p2.5.m5.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.5.m5.1b"><ci id="S5.SS2.SSS1.p2.5.m5.1.1.cmml" xref="S5.SS2.SSS1.p2.5.m5.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.5.m5.1c">\mu</annotation></semantics></math>=4.8, <math id="S5.SS2.SSS1.p2.6.m6.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S5.SS2.SSS1.p2.6.m6.1a"><mi id="S5.SS2.SSS1.p2.6.m6.1.1" xref="S5.SS2.SSS1.p2.6.m6.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.6.m6.1b"><ci id="S5.SS2.SSS1.p2.6.m6.1.1.cmml" xref="S5.SS2.SSS1.p2.6.m6.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.6.m6.1c">\sigma</annotation></semantics></math>=1.46). The majority of the people neither liked, nor disliked the weather references (<math id="S5.SS2.SSS1.p2.7.m7.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S5.SS2.SSS1.p2.7.m7.1a"><mi id="S5.SS2.SSS1.p2.7.m7.1.1" xref="S5.SS2.SSS1.p2.7.m7.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.7.m7.1b"><ci id="S5.SS2.SSS1.p2.7.m7.1.1.cmml" xref="S5.SS2.SSS1.p2.7.m7.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.7.m7.1c">\mu</annotation></semantics></math>=4.38, <math id="S5.SS2.SSS1.p2.8.m8.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S5.SS2.SSS1.p2.8.m8.1a"><mi id="S5.SS2.SSS1.p2.8.m8.1.1" xref="S5.SS2.SSS1.p2.8.m8.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.8.m8.1b"><ci id="S5.SS2.SSS1.p2.8.m8.1.1.cmml" xref="S5.SS2.SSS1.p2.8.m8.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.8.m8.1c">\sigma</annotation></semantics></math>=1.04). However, 10 out of 13 participants preferred highly visual features over text — with P11 noting specifically that they “could not read [text] and focus/ participate in the conversation at the same time” but could take in pictures “at a glance”.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2. </span>Visual Augmentation and Distraction</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">Participants also generally agreed that the AR system helped reduce distraction when compared to smartphone search, reporting that the dynamic rendering of references made it easier to stay engaged in the conversation — providing visual anchors and history that helped them stay connected to the conversation rather than diverging from it. As one participant (P8) remarked, <span id="S5.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_italic">“I found that the AR references helped me keep track of the main topics during the discussion. In fact, I found the AR references less distracting than using my phone when trying to maintain a meaningful conversation that goes both ways.”</span> Even for participants who became distracted reported that visual priority (G2) of the references helped them to recall information even after the conversation. P11 emphasized this point, stating that <span id="S5.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_italic">“since I am a visual learner, I was able to recall the main points because I had <span id="S5.SS2.SSS2.p1.1.2.1" class="ltx_text ltx_font_bold">seen</span> the keyword [emphasis added]”</span>. P8 reported a similar experience, noting that “<span id="S5.SS2.SSS2.p1.1.3" class="ltx_text ltx_font_italic">“the AR references were useful subheadings that helped me keep track of the main topics in discussion”</span>.
However, focusing on conversations and references at the same time still sometimes caused divided attention. P11 in particular noted that the <span id="S5.SS2.SSS2.p1.1.4" class="ltx_text ltx_font_italic">“displayed images and keywords were slightly distracting”</span>, but also emphasized that the visual references’ placement in their peripheral vision helped them from being too visually overwhelming.</p>
</div>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3. </span>Interaction and System Efficiency</h4>

<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">In terms of the interaction, the participants had a mixed impression of the system, highlighting how the success of augmented conversation systems may depend heavily on their ability to surface information quickly.
However, despite our focus on minimizing latency, the complexity of the referencing pipeline often meant that keyword gaze interactions took a long time to trigger visual references. For example, P12 noted that <span id="S5.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_italic">“most of the keywords popped up pretty quickly, but some keywords popped up at a later time”</span>.
Similarly, P3 mentioned that <span id="S5.SS2.SSS3.p1.1.2" class="ltx_text ltx_font_italic">“keywords took too long to open. I felt like we switched topics by the time it opened.”</span>.</p>
</div>
<div id="S5.SS2.SSS3.p2" class="ltx_para">
<p id="S5.SS2.SSS3.p2.1" class="ltx_p">However, participants liked the gaze-based interaction, with P3 noting that <span id="S5.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_italic">“It’s like searching things up with your eyes”</span>.
P6 also highlighted how gaze-based interactions helped focus their attention stating <span id="S5.SS2.SSS3.p2.1.2" class="ltx_text ltx_font_italic">“it felt just like a normal conversation, as though your point-of-view became an extension of your phone [in terms of on-the-fly referencing] while avoiding potential distractions like viewing 40 irrelevant notifications”</span>.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Discussion and Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">While our initial exploration focused primarily on one-on-one conversations, augmented conversation tools hold promise for a variety of other settings, including education (taking lecture notes, attending conferences, group studying, etc.) and professional (attending meetings, presenting, brainstorming sessions, etc.) scenarios.
As our transcription pipeline can help dynamically create meeting logs or lecture notes, it could be a highly useful tool not only for real-time referencing but also for documenting events for future reference. As P9 mentioned, <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">“I can see that if I was being taught about the synthesis of aspirin that I could look up the different steps involved and take notes based on the flow of the conversation“</span>.
For educational settings, it could be also interesting to explore different technical approaches and richer interactions. Participants noted that future augmented conversations could help in creating an outline in lectures and conferences (P8), break down subject matter in steps for note taking (P9), help naturally search information in conversation (P2), help them easily take notes while listening to lectures (P4), and provide visual referencing to spoken words in lectures (P11).
Moreover, in the classroom, it might make more sense to use a projection mapping approach, similar to HoloBoard <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite>, or projecting mobile AR screens like RealitySketch <cite class="ltx_cite ltx_citemacro_citep">(Suzuki et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2020</a>)</cite> rather than asking all of the students to wear HMDs.
Going forward, we are especially interested in designing, deploying, and evaluating the system in classroom or meeting settings.
While our work mostly focuses on augmenting everyday conversations, we also see so potential for accessibility assistance both for deaf and hard-of-hearing people as well as people with visual impairments. Applications in this domain might building on the kinds of captioning tools developed by accessibility researchers <cite class="ltx_cite ltx_citemacro_citep">(Mosbah, <a href="#bib.bib29" title="" class="ltx_ref">2006</a>; Peng et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>; Olwal et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Tu et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2020</a>; Miller et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2017</a>; Schipper and Brinkman, <a href="#bib.bib42" title="" class="ltx_ref">2017</a>; Findlater et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>; Jain et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2018a</a>, <a href="#bib.bib16" title="" class="ltx_ref">2015</a>, <a href="#bib.bib17" title="" class="ltx_ref">b</a>)</cite> while adding a range of additional audio-visual referencing.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We present RealityChat, a system that supports co-located in-person conversations via embedded speech-driven on-the-fly referencing in augmented reality. With the help of AR, RealityChat provides relevant visual references in real-time, based on keywords extracted automatically from the spoken conversation. Our study confirms that our system helps in reducing distraction in conversations while providing highly useful and relevant information via simple interactions. Moreover, our experiences augmenting day-to-day conversations highlight broader opportunities for augmented conversation systems. Future tools have the potential to integrate context-based personal information, provide accessibility assistance, and provide value in a variety of educational and professional use cases.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aseniero et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Bon Adriel Aseniero, Marios Constantinides, Sagar Joglekar, Ke Zhou, and Daniele Quercia. 2020.

</span>
<span class="ltx_bibblock">MeetCues: Supporting online meetings experience. In <em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">2020 IEEE Visualization Conference (VIS)</em>. IEEE, 236–240.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bergstrom and Karahalios (2007)</span>
<span class="ltx_bibblock">
Tony Bergstrom and Karrie Karahalios. 2007.

</span>
<span class="ltx_bibblock">Conversation Clock: Visualizing audio patterns in co-located groups. In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">2007 40th Annual Hawaii International Conference on System Sciences (HICSS’07)</em>. IEEE, 78–78.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bergstrom and Karahalios (2009)</span>
<span class="ltx_bibblock">
Tony Bergstrom and Karrie Karahalios. 2009.

</span>
<span class="ltx_bibblock">Conversation clusters: grouping conversation topics through human-computer dialog. In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>. 2349–2352.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Custers and Aarts (2005)</span>
<span class="ltx_bibblock">
Ruud Custers and Henk Aarts. 2005.

</span>
<span class="ltx_bibblock">Positive affect as implicit motivator: on the nonconscious operation of behavioral goals.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Journal of personality and social psychology</em> 89, 2 (2005), 129.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dudley et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
John J Dudley, Jason T Jacques, and Per Ola Kristensson. 2021.

</span>
<span class="ltx_bibblock">Crowdsourcing design guidance for contextual adaptation of text content in augmented reality. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>. 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwyer et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Ryan J Dwyer, Kostadin Kushlev, and Elizabeth W Dunn. 2018.

</span>
<span class="ltx_bibblock">Smartphone use undermines enjoyment of face-to-face social interactions.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Journal of Experimental Social Psychology</em> 78 (2018), 233–239.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eddie et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Thomas Eddie, Juan Ye, and Graeme Stevenson. 2015.

</span>
<span class="ltx_bibblock">Are our mobile phones driving us apart? Divert attention from mobile phones back to physical conversation!. In <em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct</em>. 1082–1087.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ESTG (2019)</span>
<span class="ltx_bibblock">
CIIC ESTG. 2019.

</span>
<span class="ltx_bibblock">Smart Time: a Context-Aware Conversational Agent for Suggesting Free Time Activities.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Findlater et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Leah Findlater, Bonnie Chinh, Dhruv Jain, Jon Froehlich, Raja Kushalnagar, and Angela Carey Lin. 2019.

</span>
<span class="ltx_bibblock">Deaf and hard-of-hearing individuals’ preferences for wearable and mobile sound awareness technologies. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jiangtao Gong, Xiaobai Ji, Siyu Zha, Teng Han, Qicheng Ding, Jiannan Li, Liuxin Zhang, and Qianying Wang. 2021.

</span>
<span class="ltx_bibblock">HoloBoard: an Immersive Teaching Board System. In <em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</em>. 1–4.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haigh (2015)</span>
<span class="ltx_bibblock">
Alex Haigh. 2015.

</span>
<span class="ltx_bibblock">Stop phubbing.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Artikel Online. Tersedia pada http://stopphubbing. com</em> (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horrigan and Duggan (2015)</span>
<span class="ltx_bibblock">
John B Horrigan and Maeve Duggan. 2015.

</span>
<span class="ltx_bibblock">Home broadband 2015.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Pew Research Center</em> 21 (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ishii and Kobayashi (1992)</span>
<span class="ltx_bibblock">
Hiroshi Ishii and Minoru Kobayashi. 1992.

</span>
<span class="ltx_bibblock">Clearboard: A seamless medium for shared drawing and conversation with eye contact. In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the SIGCHI conference on Human factors in computing systems</em>. 525–532.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2018a)</span>
<span class="ltx_bibblock">
Dhruv Jain, Bonnie Chinh, Leah Findlater, Raja Kushalnagar, and Jon Froehlich. 2018a.

</span>
<span class="ltx_bibblock">Exploring augmented reality approaches to real-time captioning: A preliminary autoethnographic study. In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 ACM Conference Companion Publication on Designing Interactive Systems</em>. 7–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Dhruv Jain, Leah Findlater, Jamie Gilkeson, Benjamin Holland, Ramani Duraiswami, Dmitry Zotkin, Christian Vogler, and Jon E Froehlich. 2015.

</span>
<span class="ltx_bibblock">Head-mounted display visualizations to support sound awareness for the deaf and hard of hearing. In <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</em>. 241–250.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2018b)</span>
<span class="ltx_bibblock">
Dhruv Jain, Rachel Franz, Leah Findlater, Jackson Cannon, Raja Kushalnagar, and Jon Froehlich. 2018b.

</span>
<span class="ltx_bibblock">Towards Accessible Conversations in a Mobile Context for People who are Deaf and Hard of Hearing. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility</em>. 81–92.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Mirae Kim, Min Kyung Lee, and Laura Dabbish. 2015.

</span>
<span class="ltx_bibblock">Shop-i: Gaze based interaction in the physical world for in-store social shopping experience. In <em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems</em>. 1253–1258.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Seungwon Kim, Gun Lee, Weidong Huang, Hayun Kim, Woontack Woo, and Mark Billinghurst. 2019.

</span>
<span class="ltx_bibblock">Evaluating the combination of visual communication cues for HMD-based mixed reality remote collaboration. In <em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 CHI conference on human factors in computing systems</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2008)</span>
<span class="ltx_bibblock">
Taemie Kim, Agnes Chang, Lindsey Holland, and Alex Sandy Pentland. 2008.

</span>
<span class="ltx_bibblock">Meeting mediator: enhancing group collaborationusing sociometric feedback. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2008 ACM conference on Computer supported cooperative work</em>. 457–466.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jian Liao, Adnan Karim, Shivesh Singh Jadon, Rubaiat Habib Kazi, and Ryo Suzuki. 2022.

</span>
<span class="ltx_bibblock">RealityTalk: Real-time speech-driven augmented presentation for AR live storytelling. In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lindlbauer et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
David Lindlbauer, Anna Maria Feit, and Otmar Hilliges. 2019.

</span>
<span class="ltx_bibblock">Context-aware online adaptation of mixed reality interfaces. In <em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd annual ACM symposium on user interface software and technology</em>. 147–160.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopez-Tovar et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Hugo Lopez-Tovar, Andreas Charalambous, and John Dowell. 2015.

</span>
<span class="ltx_bibblock">Managing smartphone interruptions through adaptive modes and modulation of notifications. In <em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on Intelligent User Interfaces</em>. 296–299.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Feiyu Lu, Shakiba Davari, Lee Lisle, Yuan Li, and Doug A Bowman. 2020.

</span>
<span class="ltx_bibblock">Glanceable ar: Evaluating information access methods for head-worn augmented reality. In <em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">2020 IEEE conference on virtual reality and 3D user interfaces (VR)</em>. IEEE, 930–939.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lundgren and Torgersson (2013)</span>
<span class="ltx_bibblock">
Sus Lundgren and Olof Torgersson. 2013.

</span>
<span class="ltx_bibblock">Bursting the mobile bubble. In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">First International Workshop on Designing Mobile Face-to-Face Group Interactions, European Conference on Computer Supported Cooperative Work, ECSCW</em>, Vol. 2013.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihalcea and Tarau (2004)</span>
<span class="ltx_bibblock">
Rada Mihalcea and Paul Tarau. 2004.

</span>
<span class="ltx_bibblock">Textrank: Bringing order into text. In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2004 conference on empirical methods in natural language processing</em>. 404–411.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashley Miller, Joan Malasig, Brenda Castro, Vicki L Hanson, Hugo Nicolau, and Alessandra Brandão. 2017.

</span>
<span class="ltx_bibblock">The use of smart glasses for lecture comprehension by deaf and hard of hearing students. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em>. 1909–1915.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Monteiro et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Kyzyl Monteiro, Ritik Vatsal, Neil Chulpongsatorn, Aman Parnami, and Ryo Suzuki. 2023.

</span>
<span class="ltx_bibblock">Teachable reality: Prototyping tangible augmented reality with everyday objects by leveraging interactive machine teaching. In <em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mosbah (2006)</span>
<span class="ltx_bibblock">
B Ben Mosbah. 2006.

</span>
<span class="ltx_bibblock">Speech recognition for disabilities people. In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">2006 2nd International Conference on Information &amp; Communication Technologies</em>, Vol. 1. IEEE, 864–869.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moser et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Carol Moser, Sarita Y Schoenebeck, and Katharina Reinecke. 2016.

</span>
<span class="ltx_bibblock">Technology at the table: Attitudes about mobile phone use at mealtimes. In <em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</em>. 1881–1892.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Müller et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Florian Müller, Sebastian Günther, Azita Hosseini Nejad, Niloofar Dezfuli, Mohammadreza Khalilbeigi, and Max Mühlhäuser. 2017.

</span>
<span class="ltx_bibblock">Cloudbits: supporting conversations through augmented zero-query search visualization. In <em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th Symposium on Spatial User Interaction</em>. 30–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Tien T Nguyen, Duyen T Nguyen, Shamsi T Iqbal, and Eyal Ofek. 2015.

</span>
<span class="ltx_bibblock">The known stranger: Supporting conversations between strangers with personalized topic suggestions. In <em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</em>. 555–564.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olwal et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Alex Olwal, Kevin Balke, Dmitrii Votintcev, Thad Starner, Paula Conn, Bonnie Chinh, and Benoit Corda. 2020.

</span>
<span class="ltx_bibblock">Wearable subtitles: Augmenting spoken communication with lightweight eyewear for all-day captioning. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology</em>. 1108–1120.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al<span id="bib.bib34.3.3.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Yi-Hao Peng, Ming-Wei Hsi, Paul Taele, Ting-Yu Lin, Po-En Lai, Leon Hsu, Tzu-chuan Chen, Te-Yen Wu, Yu-An Chen, Hsien-Hui Tang, et al<span id="bib.bib34.4.1" class="ltx_text">.</span> 2018.

</span>
<span class="ltx_bibblock">Speechbubbles: enhancing captioning experiences for deaf and hard-of-hearing people in group conversations. In <em id="bib.bib34.5.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em>. 1–10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeuffer et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Ken Pfeuffer, Yasmeen Abdrabou, Augusto Esteves, Radiah Rivu, Yomna Abdelrahman, Stefanie Meitner, Amr Saadi, and Florian Alt. 2021.

</span>
<span class="ltx_bibblock">ARtention: A design space for gaze-adaptive user interfaces in augmented reality.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">Computers &amp; Graphics</em> 95 (2021), 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeuffer et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Ken Pfeuffer, Lukas Mecke, Sarah Delgado Rodriguez, Mariam Hassib, Hannah Maier, and Florian Alt. 2020.

</span>
<span class="ltx_bibblock">Empirical evaluation of gaze-enhanced menus in virtual reality. In <em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">26th ACM symposium on virtual reality software and technology</em>. 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piening et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Robin Piening, Ken Pfeuffer, Augusto Esteves, Tim Mittermeier, Sarah Prange, Philippe Schröder, and Florian Alt. 2021.

</span>
<span class="ltx_bibblock">Looking for Info: Evaluation of Gaze Based Information Retrieval in Augmented Reality. In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">IFIP Conference on Human-Computer Interaction</em>. Springer, 544–565.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Przybylski and Weinstein (2013)</span>
<span class="ltx_bibblock">
Andrew K Przybylski and Netta Weinstein. 2013.

</span>
<span class="ltx_bibblock">Can you connect with me now? How the presence of mobile communication technology influences face-to-face conversation quality.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Journal of Social and Personal Relationships</em> 30, 3 (2013), 237–246.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radeck-Arneth et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Stephan Radeck-Arneth, Chris Biemann, and Dirk Schnelle-Walka. 2014.

</span>
<span class="ltx_bibblock">Towards Ambient Search.. In <em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">LWA</em>. 257–259.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rhodes and Maes (2000)</span>
<span class="ltx_bibblock">
Bradley James Rhodes and Pattie Maes. 2000.

</span>
<span class="ltx_bibblock">Just-in-time information retrieval agents.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IBM Systems journal</em> 39, 3.4 (2000), 685–704.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rivu et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Radiah Rivu, Yasmeen Abdrabou, Ken Pfeuffer, Augusto Esteves, Stefanie Meitner, and Florian Alt. 2020.

</span>
<span class="ltx_bibblock">Stare: gaze-assisted face-to-face communication in augmented reality. In <em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">ACM Symposium on Eye Tracking Research and Applications</em>. 1–5.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schipper and Brinkman (2017)</span>
<span class="ltx_bibblock">
Chris Schipper and Bo Brinkman. 2017.

</span>
<span class="ltx_bibblock">Caption placement on an augmented reality head worn device. In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility</em>. 365–366.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah (2003)</span>
<span class="ltx_bibblock">
James Shah. 2003.

</span>
<span class="ltx_bibblock">The motivational looking glass: how significant others implicitly affect goal appraisals.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Journal of personality and social psychology</em> 85, 3 (2003), 424.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Yang Shi, Chris Bryan, Sridatt Bhamidipati, Ying Zhao, Yaoxue Zhang, and Kwan-Liu Ma. 2018.

</span>
<span class="ltx_bibblock">Meetingvis: Visual narratives to assist in recalling meeting context and content.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</em> 24, 6 (2018), 1918–1929.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava (2005)</span>
<span class="ltx_bibblock">
Lara Srivastava. 2005.

</span>
<span class="ltx_bibblock">Mobile phones and the evolution of social behaviour.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Behaviour &amp; information technology</em> 24, 2 (2005), 111–129.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su and Wang (2015)</span>
<span class="ltx_bibblock">
Norman Makoto Su and Lulu Wang. 2015.

</span>
<span class="ltx_bibblock">From third to surveilled place: The mobile in Irish pubs. In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</em>. 1659–1668.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suemitsu et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Kazuki Suemitsu, Keiichi Zempo, Koichi Mizutani, and Naoto Wakatsuki. 2015.

</span>
<span class="ltx_bibblock">Caption support system for complementary dialogical information using see-through head mounted display. In <em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">2015 IEEE 4th Global Conference on Consumer Electronics (GCCE)</em>. IEEE, 368–371.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzuki et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ryo Suzuki, Mar Gonzalez-Franco, Misha Sra, and David Lindlbauer. 2023.

</span>
<span class="ltx_bibblock">XR and AI: AI-Enabled Virtual, Augmented, and Mixed Reality. In <em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</em>. 1–3.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzuki et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Ryo Suzuki, Rubaiat Habib Kazi, Li-Yi Wei, Stephen DiVerdi, Wilmot Li, and Daniel Leithinger. 2020.

</span>
<span class="ltx_bibblock">Realitysketch: Embedding responsive graphics and visualizations in AR through dynamic sketching. In <em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology</em>. 166–181.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tu et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jason Tu, Georgianna Lin, and Thad Starner. 2020.

</span>
<span class="ltx_bibblock">Conversational greeting detection using captioning on head worn displays versus smartphones. In <em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 International Symposium on Wearable Computers</em>. 84–86.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Turkle (2012)</span>
<span class="ltx_bibblock">
Sherry Turkle. 2012.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Alone together: why we expect more form technology and less from each other</em>.

</span>
<span class="ltx_bibblock">Basic Books, a member of the Perseus Books Group.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yazıcıoğlu (2017)</span>
<span class="ltx_bibblock">
Deniz Yazıcıoğlu. 2017.

</span>
<span class="ltx_bibblock">THE SMARTPHONE AFFECT: The Emotional Impact of Smartphone Usage in Public Spaces and it’s Affects on the Subjective Experience of Public Space’s Sociality.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.18536" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.18537" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.18537">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.18537" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.18538" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 17:44:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
