<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.07867] Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation</title><meta property="og:description" content="In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an a…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.07867">

<!--Generated on Fri Jul  5 19:35:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Let’s Go Real Talk: 
<br class="ltx_break">Spoken Dialogue Model for Face-to-Face Conversation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

<span id="id3.3.3" class="ltx_tabular ltx_align_middle">
<span id="id2.2.2.2" class="ltx_tr">
<span id="id2.2.2.2.2" class="ltx_td ltx_align_center">Se Jin Park<sup id="id2.2.2.2.2.1" class="ltx_sup">∗</sup>  Chae Won Kim<sup id="id2.2.2.2.2.2" class="ltx_sup">∗</sup>  Hyeongseop Rha  Minsu Kim</span></span>
<span id="id3.3.3.3" class="ltx_tr">
<span id="id3.3.3.3.1" class="ltx_td ltx_align_center"> Joanna Hong  Jeong Hun Yeo  Yong Man Ro<sup id="id3.3.3.3.1.1" class="ltx_sup">†</sup></span></span>
</span> 
<br class="ltx_break">Integrated Vision and Language Lab, KAIST 
<br class="ltx_break">
<span id="id4.4.id1" class="ltx_tabular ltx_align_middle">
<span id="id4.4.id1.1" class="ltx_tr">
<span id="id4.4.id1.1.1" class="ltx_td ltx_align_center"><span id="id4.4.id1.1.1.1" class="ltx_text ltx_font_typewriter">{jinny960812, chaewonkim, ryool_1832, sedne246, ymro}@kaist.ac.kr</span></span></span>
<span id="id4.4.id1.2" class="ltx_tr">
<span id="id4.4.id1.2.1" class="ltx_td ltx_align_center"><span id="id4.4.id1.2.1.1" class="ltx_text ltx_font_typewriter"> ms.k@ieee.org  joanna2587@gmail.com</span></span></span>
</span><span id="id5.5.id2" class="ltx_text ltx_font_typewriter">
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text. To this end, we newly introduce MultiDialog, the first large-scale multimodal (<em id="id6.id1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="id6.id1.2" class="ltx_text"></span>, audio and visual) spoken dialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat. The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis. Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining. Through extensive experiments, we validate the effectiveness of our model in facilitating a face-to-face conversation. Demo and data are available at <a target="_blank" href="https://multidialog.github.io" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://multidialog.github.io</a> and <a target="_blank" href="https://huggingface.co/datasets/IVLLab/MultiDialog" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/IVLLab/MultiDialog</a>, respectively.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.3" class="ltx_block ltx_align_bottom">
<p id="p1.3.4" class="ltx_p"><span id="p1.3.4.1" class="ltx_text ltx_font_bold">Let’s Go Real Talk: 
<br class="ltx_break">Spoken Dialogue Model for Face-to-Face Conversation</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.3.3" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.3.3.3" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.3.3.3.3" class="ltx_tabular ltx_align_top">
<span id="p1.3.3.3.3.3" class="ltx_tr">
<span id="p1.3.3.3.3.3.3" class="ltx_td ltx_align_center"><span id="p1.3.3.3.3.3.3.3" class="ltx_text ltx_font_bold">

<span id="p1.3.3.3.3.3.3.3.3" class="ltx_tabular ltx_align_middle">
<span id="p1.2.2.2.2.2.2.2.2.2" class="ltx_tr">
<span id="p1.2.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_center">Se Jin Park<sup id="p1.2.2.2.2.2.2.2.2.2.2.1" class="ltx_sup"><span id="p1.2.2.2.2.2.2.2.2.2.2.1.1" class="ltx_text ltx_font_medium">∗</span></sup>  Chae Won Kim<sup id="p1.2.2.2.2.2.2.2.2.2.2.2" class="ltx_sup"><span id="p1.2.2.2.2.2.2.2.2.2.2.2.1" class="ltx_text ltx_font_medium">∗</span></sup>  Hyeongseop Rha  Minsu Kim</span></span>
<span id="p1.3.3.3.3.3.3.3.3.3" class="ltx_tr">
<span id="p1.3.3.3.3.3.3.3.3.3.1" class="ltx_td ltx_align_center"> Joanna Hong  Jeong Hun Yeo  Yong Man Ro<sup id="p1.3.3.3.3.3.3.3.3.3.1.1" class="ltx_sup"><span id="p1.3.3.3.3.3.3.3.3.3.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span></span>
</span></span></span></span>
<span id="p1.3.3.3.3.4" class="ltx_tr">
<span id="p1.3.3.3.3.4.1" class="ltx_td ltx_align_center">Integrated Vision and Language Lab, KAIST</span></span>
<span id="p1.3.3.3.3.5" class="ltx_tr">
<span id="p1.3.3.3.3.5.1" class="ltx_td ltx_align_center">
<span id="p1.3.3.3.3.5.1.1" class="ltx_tabular ltx_align_middle">
<span id="p1.3.3.3.3.5.1.1.1" class="ltx_tr">
<span id="p1.3.3.3.3.5.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.3.3.3.3.5.1.1.1.1.1" class="ltx_text ltx_font_typewriter">{jinny960812, chaewonkim, ryool_1832, sedne246, ymro}@kaist.ac.kr</span></span></span>
<span id="p1.3.3.3.3.5.1.1.2" class="ltx_tr">
<span id="p1.3.3.3.3.5.1.1.2.1" class="ltx_td ltx_align_center"><span id="p1.3.3.3.3.5.1.1.2.1.1" class="ltx_text ltx_font_typewriter"> ms.k@ieee.org  joanna2587@gmail.com</span></span></span>
</span></span></span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><sup id="footnote1.1" class="ltx_sup">∗</sup>Equal contribution. <sup id="footnote1.2" class="ltx_sup">†</sup>Corresponding author. This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2022R1A2C2005529) and Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2022-0-00124, Development of Artificial Intelligence Technology for Self-Improving Competency-Aware Learning Capabilities).</span></span></span>
<figure id="S0.T1" class="ltx_table">
<div id="S0.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:110.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-86.6pt,25.9pt) scale(0.680209150381277,0.680209150381277) ;">
<table id="S0.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S0.T1.1.1.1" class="ltx_tr">
<td id="S0.T1.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
<span id="S0.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span>
</td>
<td id="S0.T1.1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S0.T1.1.1.1.2.1" class="ltx_text ltx_font_bold"># Dialogues</span></td>
<td id="S0.T1.1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S0.T1.1.1.1.3.1" class="ltx_text ltx_font_bold"># Turns</span></td>
<td id="S0.T1.1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S0.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Length (hrs)</span></td>
<td id="S0.T1.1.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S0.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Audio</span></td>
<td id="S0.T1.1.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S0.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">Text</span></td>
<td id="S0.T1.1.1.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S0.T1.1.1.1.7.1" class="ltx_text ltx_font_bold">Video</span></td>
<td id="S0.T1.1.1.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S0.T1.1.1.1.8.1" class="ltx_text ltx_font_bold">Emotion</span></td>
</tr>
<tr id="S0.T1.1.1.2" class="ltx_tr">
<td id="S0.T1.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">IEMOCAP <cite class="ltx_cite ltx_citemacro_cite">Busso et al. (<a href="#bib.bib11" title="" class="ltx_ref">2008</a>)</cite>
</td>
<td id="S0.T1.1.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">151</td>
<td id="S0.T1.1.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">10,039</td>
<td id="S0.T1.1.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">12</td>
<td id="S0.T1.1.1.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">✓</td>
</tr>
<tr id="S0.T1.1.1.3" class="ltx_tr">
<td id="S0.T1.1.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">DSTC2 <cite class="ltx_cite ltx_citemacro_cite">Henderson et al. (<a href="#bib.bib26" title="" class="ltx_ref">2014</a>)</cite>
</td>
<td id="S0.T1.1.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">1,612</td>
<td id="S0.T1.1.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">23,354</td>
<td id="S0.T1.1.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">32</td>
<td id="S0.T1.1.1.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✗</td>
<td id="S0.T1.1.1.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✗</td>
</tr>
<tr id="S0.T1.1.1.4" class="ltx_tr">
<td id="S0.T1.1.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">MELD <cite class="ltx_cite ltx_citemacro_cite">Poria et al. (<a href="#bib.bib52" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S0.T1.1.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">1,433</td>
<td id="S0.T1.1.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">13,000</td>
<td id="S0.T1.1.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">13.7</td>
<td id="S0.T1.1.1.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✗</td>
<td id="S0.T1.1.1.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
</tr>
<tr id="S0.T1.1.1.5" class="ltx_tr">
<td id="S0.T1.1.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">DailyTalk <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S0.T1.1.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">2,514</td>
<td id="S0.T1.1.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">23,774</td>
<td id="S0.T1.1.1.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">21.7</td>
<td id="S0.T1.1.1.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✗</td>
<td id="S0.T1.1.1.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✗</td>
</tr>
<tr id="S0.T1.1.1.6" class="ltx_tr">
<td id="S0.T1.1.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">Expresso <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib47" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S0.T1.1.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">391</td>
<td id="S0.T1.1.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">2,400</td>
<td id="S0.T1.1.1.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">47</td>
<td id="S0.T1.1.1.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✗</td>
<td id="S0.T1.1.1.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
</tr>
<tr id="S0.T1.1.1.7" class="ltx_tr">
<td id="S0.T1.1.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">SpokenWOZ <cite class="ltx_cite ltx_citemacro_cite">Si et al. (<a href="#bib.bib60" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S0.T1.1.1.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">5,700</td>
<td id="S0.T1.1.1.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">203,074</td>
<td id="S0.T1.1.1.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">249</td>
<td id="S0.T1.1.1.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✗</td>
<td id="S0.T1.1.1.7.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✗</td>
</tr>
<tr id="S0.T1.1.1.8" class="ltx_tr">
<td id="S0.T1.1.1.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S0.T1.1.1.8.1.1" class="ltx_text ltx_font_bold">MultiDialog</span></td>
<td id="S0.T1.1.1.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">8,733</td>
<td id="S0.T1.1.1.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">187,859</td>
<td id="S0.T1.1.1.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">340</td>
<td id="S0.T1.1.1.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.8.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.8.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
<td id="S0.T1.1.1.8.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">✓</td>
</tr>
<tr id="S0.T1.1.1.9" class="ltx_tr">
<td id="S0.T1.1.1.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding:1.5pt 0.0pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S0.T1.1.1.9.2" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
<td id="S0.T1.1.1.9.3" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
<td id="S0.T1.1.1.9.4" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
<td id="S0.T1.1.1.9.5" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
<td id="S0.T1.1.1.9.6" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
<td id="S0.T1.1.1.9.7" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
<td id="S0.T1.1.1.9.8" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of MultiDialog dataset with publicly available multimodal dialogue datasets.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Spoken Dialogue System (SDS), often referred to as a conversational agent, engages in natural speech conversations with humans by recognizing speech from user input and providing contextually appropriate and accurate responses with speech. With spoken language as the primary interface, it has numerous applications for human-computer interactions such as customer service and voice assistants.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, when people communicate face-to-face, we utilize not only audio but also visual information of the conversing partner to process spoken words and non-verbal cues (<em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p2.1.2" class="ltx_text"></span>, facial expressions, gestures, and emotions) <cite class="ltx_cite ltx_citemacro_cite">Petridis et al. (<a href="#bib.bib50" title="" class="ltx_ref">2018</a>); Hong et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite>. This multimodal information enhances understanding of the speech content and the speaker’s intent. Furthermore, having a visual counterpart to audio can simulate a real face-to-face conversation experience, making the user feel more connected and engaged.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we explore an audio-visual spoken dialogue system to facilitate direct face-to-face conversation for the first time. Central to the development of dialogue systems is the large amount of high-quality dialogue data. Current dialogue systems are predominantly text-based, driven by the abundance of text dialogue datasets <cite class="ltx_cite ltx_citemacro_cite">Lowe et al. (<a href="#bib.bib44" title="" class="ltx_ref">2015</a>); Li et al. (<a href="#bib.bib43" title="" class="ltx_ref">2017</a>); Zhang et al. (<a href="#bib.bib69" title="" class="ltx_ref">2018</a>); Rashkin et al. (<a href="#bib.bib55" title="" class="ltx_ref">2018</a>); Budzianowski et al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>); Zhou et al. (<a href="#bib.bib73" title="" class="ltx_ref">2018</a>); Reddy et al. (<a href="#bib.bib56" title="" class="ltx_ref">2019</a>); <a href="#bib.bib39" title="" class="ltx_ref">Lambert et al. </a>; Ding et al. (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>); Köpf et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>. Recently, several audio dialogue datasets have been released <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>); Si et al. (<a href="#bib.bib60" title="" class="ltx_ref">2023</a>); Nguyen et al. (<a href="#bib.bib47" title="" class="ltx_ref">2023a</a>)</cite> which augment existing text dialogue data <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib43" title="" class="ltx_ref">2017</a>); Budzianowski et al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> with speech. However, those with visual components remain limited in scale, comprising less than 15 hours in total <cite class="ltx_cite ltx_citemacro_cite">Busso et al. (<a href="#bib.bib11" title="" class="ltx_ref">2008</a>); Poria et al. (<a href="#bib.bib52" title="" class="ltx_ref">2018</a>)</cite>. Addressing this data gap, we introduce MultiDialog, the first large-scale audio-visual spoken dialogue corpus. It consists of 340 hours of audio-visual recordings of approximately 9,000 dialogues, derived from open-domain text dialogue dataset, TopicalChat <cite class="ltx_cite ltx_citemacro_cite">Gopalakrishnan et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite> which is an extensive multi-turn dialogue corpus collected from real conversations covering 9 broad topics. The proposed MultiDialog consists of emotion annotations for each utterance and simultaneous recordings of both the listener and the speaker, presenting opportunities for diverse research; from face-to-face dialogue system to talking face synthesis <cite class="ltx_cite ltx_citemacro_cite">Park et al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>); Zhang et al. (<a href="#bib.bib71" title="" class="ltx_ref">2023b</a>)</cite>, listener’s face synthesis <cite class="ltx_cite ltx_citemacro_cite">Song et al. (<a href="#bib.bib61" title="" class="ltx_ref">2023</a>); Zhou et al. (<a href="#bib.bib74" title="" class="ltx_ref">2023</a>)</cite>, and emotion-conditioned face synthesis <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Based on the MultiDialog dataset, we propose the first audio-visual spoken dialogue model that can directly process audio-visual speech as user input and generate audio-visual speech as the output response. Motivated by the recent success of the direct spoken dialogue model using discretized speech tokens <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib48" title="" class="ltx_ref">2023b</a>); Zhang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2023a</a>)</cite>, we introduce audio-visual (AV) speech tokens extracted by quantizing audio-visual speech features from a self-supervised model <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a href="#bib.bib59" title="" class="ltx_ref">2021</a>)</cite>. Utilizing the AV speech tokens as pseudo texts, we integrate AV speech into a pretrained large-language model (LLM) <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib70" title="" class="ltx_ref">2022</a>)</cite> through joint speech-text pretraining.
The response is also returned in AV speech tokens, which are synthesized into a talking face video as the output for direct interaction with the system.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our contributions are in three folds:
(1) We introduce the first direct Face-to-Face dialogue model which processes multimodal speech from user input and generates multimodal speech as the output response, facilitating a face-to-face conversation system.
(2) To build a face-to-face dialogue system, we propose the first large-scale multimodal (<em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p5.1.2" class="ltx_text"></span>, audio, visual, and text) dialogue corpus, MultiDialog consisting of 340 hours of approximately 9,000 audio-visual conversation streams.
(3) We demonstrate that joint speech-text pretraining leveraging a pre-trained large language model improves upon direct initialization in retaining knowledge of the original large language model.</p>
</div>
<figure id="S1.T2" class="ltx_table">
<div id="S1.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:289.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(4.8pt,-3.7pt) scale(1.02658796589662,1.02658796589662) ;">
<table id="S1.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T2.1.1.1" class="ltx_tr">
<td id="S1.T2.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding:1pt 0.0pt;">
<span id="S1.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.1.1.1.1.1.1" class="ltx_p" style="width:113.8pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></span>
<span id="S1.T2.1.1.1.1.1.2" class="ltx_p"><span id="S1.T2.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">MultiDialog</span></span>
</span>
</td>
<td id="S1.T2.1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;"><span id="S1.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Train</span></td>
<td id="S1.T2.1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;"><span id="S1.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Valid Freq</span></td>
<td id="S1.T2.1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;"><span id="S1.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Valid Rare</span></td>
<td id="S1.T2.1.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;"><span id="S1.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">Test Freq</span></td>
<td id="S1.T2.1.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;"><span id="S1.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">Test Rare</span></td>
<td id="S1.T2.1.1.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;"><span id="S1.T2.1.1.1.7.1" class="ltx_text ltx_font_bold">Total</span></td>
</tr>
<tr id="S1.T2.1.1.2" class="ltx_tr">
<td id="S1.T2.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" style="padding:1pt 0.0pt;">
<span id="S1.T2.1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.1.1.2.1.1.1" class="ltx_p" style="width:113.8pt;"># dialogues</span>
</span>
</td>
<td id="S1.T2.1.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">7,011</td>
<td id="S1.T2.1.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">448</td>
<td id="S1.T2.1.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">443</td>
<td id="S1.T2.1.1.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">450</td>
<td id="S1.T2.1.1.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">381</td>
<td id="S1.T2.1.1.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">8,733</td>
</tr>
<tr id="S1.T2.1.1.3" class="ltx_tr">
<td id="S1.T2.1.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding:1pt 0.0pt;">
<span id="S1.T2.1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.1.1.3.1.1.1" class="ltx_p" style="width:113.8pt;"># utterance</span>
</span>
</td>
<td id="S1.T2.1.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">151,645</td>
<td id="S1.T2.1.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">8,516</td>
<td id="S1.T2.1.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">9,556</td>
<td id="S1.T2.1.1.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">9,811</td>
<td id="S1.T2.1.1.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">8,331</td>
<td id="S1.T2.1.1.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">187,859</td>
</tr>
<tr id="S1.T2.1.1.4" class="ltx_tr">
<td id="S1.T2.1.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding:1pt 0.0pt;">
<span id="S1.T2.1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.1.1.4.1.1.1" class="ltx_p" style="width:113.8pt;">avg # utterance/dialogue</span>
</span>
</td>
<td id="S1.T2.1.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">21.63</td>
<td id="S1.T2.1.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">19.01</td>
<td id="S1.T2.1.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">21.57</td>
<td id="S1.T2.1.1.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">21.80</td>
<td id="S1.T2.1.1.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">21.87</td>
<td id="S1.T2.1.1.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">21.51</td>
</tr>
<tr id="S1.T2.1.1.5" class="ltx_tr">
<td id="S1.T2.1.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding:1pt 0.0pt;">
<span id="S1.T2.1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.1.1.5.1.1.1" class="ltx_p" style="width:113.8pt;">avg length/utterance (s)</span>
</span>
</td>
<td id="S1.T2.1.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">6.50</td>
<td id="S1.T2.1.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">6.23</td>
<td id="S1.T2.1.1.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">6.40</td>
<td id="S1.T2.1.1.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">6.99</td>
<td id="S1.T2.1.1.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">6.49</td>
<td id="S1.T2.1.1.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">6.51</td>
</tr>
<tr id="S1.T2.1.1.6" class="ltx_tr">
<td id="S1.T2.1.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding:1pt 0.0pt;">
<span id="S1.T2.1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.1.1.6.1.1.1" class="ltx_p" style="width:113.8pt;">avg length/dialogue (min)</span>
</span>
</td>
<td id="S1.T2.1.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">2.34</td>
<td id="S1.T2.1.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">1.97</td>
<td id="S1.T2.1.1.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">2.28</td>
<td id="S1.T2.1.1.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">2.54</td>
<td id="S1.T2.1.1.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">2.36</td>
<td id="S1.T2.1.1.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">2.33</td>
</tr>
<tr id="S1.T2.1.1.7" class="ltx_tr">
<td id="S1.T2.1.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top" style="padding:1pt 0.0pt;">
<span id="S1.T2.1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.1.1.7.1.1.1" class="ltx_p" style="width:113.8pt;">total length (hr)</span>
</span>
</td>
<td id="S1.T2.1.1.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">273.93</td>
<td id="S1.T2.1.1.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">14.74</td>
<td id="S1.T2.1.1.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">17.00</td>
<td id="S1.T2.1.1.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">19.04</td>
<td id="S1.T2.1.1.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">15.01</td>
<td id="S1.T2.1.1.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">339.71</td>
</tr>
<tr id="S1.T2.1.1.8" class="ltx_tr">
<td id="S1.T2.1.1.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" style="padding:1pt 0.0pt;">
<span id="S1.T2.1.1.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T2.1.1.8.1.1.1" class="ltx_p" style="width:113.8pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></span>
</span>
</td>
<td id="S1.T2.1.1.8.2" class="ltx_td ltx_border_tt" style="padding:1pt 0.0pt;"></td>
<td id="S1.T2.1.1.8.3" class="ltx_td ltx_border_tt" style="padding:1pt 0.0pt;"></td>
<td id="S1.T2.1.1.8.4" class="ltx_td ltx_border_tt" style="padding:1pt 0.0pt;"></td>
<td id="S1.T2.1.1.8.5" class="ltx_td ltx_border_tt" style="padding:1pt 0.0pt;"></td>
<td id="S1.T2.1.1.8.6" class="ltx_td ltx_border_tt" style="padding:1pt 0.0pt;"></td>
<td id="S1.T2.1.1.8.7" class="ltx_td ltx_border_tt" style="padding:1pt 0.0pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Datailed statistics of MultiDialog</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Spoken Dialogue Dataset</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In recent years, the development of speech dialogue datasets has played a pivotal role in understanding human behavior and building spoken dialogue systems that emulate real-life conversations. Early speech datasets focus on analyzing human behavior such as emotion and intent in speech, establishing the foundation for spoken dialogue systems. IEMOCAP <cite class="ltx_cite ltx_citemacro_cite">Busso et al. (<a href="#bib.bib11" title="" class="ltx_ref">2008</a>)</cite> and MELD <cite class="ltx_cite ltx_citemacro_cite">Poria et al. (<a href="#bib.bib52" title="" class="ltx_ref">2018</a>)</cite>, comprising audio and video recordings of dialogues, are designed to study emotional dynamics in conversations. In addition to understanding emotions, DSTC2 <cite class="ltx_cite ltx_citemacro_cite">Henderson et al. (<a href="#bib.bib26" title="" class="ltx_ref">2014</a>)</cite> presents telephone-based speech dialogues for dialogue state tracking to predict user’s goals. Building upon datasets that study human behavior in speech, recent spoken dialogue datasets were built to model realistic dialogue systems. Expresso <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib47" title="" class="ltx_ref">2023a</a>)</cite> introduces speech dialogues spanning 26 expressive styles for natural speech synthesis. DailyTalk <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> and SpokenWOZ <cite class="ltx_cite ltx_citemacro_cite">Si et al. (<a href="#bib.bib60" title="" class="ltx_ref">2023</a>)</cite> datasets introduce speech-text conversations for spoken dialogues. While existing works have contributed to advancing spoken conversation systems, dialogue datasets are limited in scale and solely consist of audio and text, thereby constraining the development of audio-visual spoken dialogue systems incorporating visual cues. To address these limitations, we expand the spoken dialogue in scale and to the visual modality, and introduce MultiDialog, a large-scale multimodal spoken dialogue dataset. A summary of existing multimodal dialogue datasets and MultiDialog is shown in Table <a href="#S0.T1" title="Table 1 ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Spoken Dialogue Models</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Audio Language Model, driven by transformer-based architecture, has made remarkable strides in speech processing. By treating continuous speech as a discrete set of representations, speech can be effectively modeled as text, allowing the application of Natural Language Processing (NLP) techniques. While it has made notable progress in speech synthesis <cite class="ltx_cite ltx_citemacro_cite">Lakhotia et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>); Borsos et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>); Wang et al. (<a href="#bib.bib63" title="" class="ltx_ref">2023a</a>); Hassid et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>); Nachmani et al. (<a href="#bib.bib46" title="" class="ltx_ref">2023</a>)</cite>, speech translation <cite class="ltx_cite ltx_citemacro_cite">Barrault et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>); Dong et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>); Rubenstein et al. (<a href="#bib.bib57" title="" class="ltx_ref">2023</a>)</cite>, and speech recognition <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib65" title="" class="ltx_ref">2023b</a>)</cite>, spoken dialogue system is a relatively unexplored field of research due to the scarcity of spoken dialogue datasets. Several works made an effort to tackle data issues by leveraging the power of large language models (LLMs). SpeechGPT <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2023a</a>)</cite> first converts speech into discrete speech tokens, and then designs a three-stage training pipeline on paired speech data, speech instruction data, and chain-of-modality instruction data. AudioGPT <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite> instructs LLMs to generate commands for controlling external tools before inputting them into the LLMs. d-GSLM <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib48" title="" class="ltx_ref">2023b</a>)</cite> models two-channel conversations to produce natural turn-taking conversations.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">There are Multimodal Large Language Models (MM-LLM) <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib67" title="" class="ltx_ref">2023</a>); Gong et al. (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite> capable of processing both visual input and output. However, they are visual grounding dialogue systems that use visual information as supplementary for tasks such as image captioning and image editing. In contrast, we aim to build an audio-visual spoken dialogue system (<em id="S2.SS2.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS2.p2.1.2" class="ltx_text"></span>, facial movement related to the speech) to enhance the understanding of speech content and enrich the communication experience, emulating a real face-to-face conversation.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>MultiDialog Dataset</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preparation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To obtain audio-visual recordings of dialogues, we gathered 12 fluent English speakers, with varying gender, age, and nationality. The participants, aged 20 to 30, came from six different countries, with six female and six male actors, as shown in Appendix <a href="#A1.SS2" title="A.2 Participant Information ‣ Appendix A MultiDialog Dataset ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>. We derived dialogue scripts from the open-domain dialogue dataset, TopicalChat <cite class="ltx_cite ltx_citemacro_cite">Gopalakrishnan et al. (<a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite> which is a rich knowledge-grounded dataset collected from real human-human conversations. It spans eight broad topics including fashion, politics, books, sports, general entertainment, music, science &amp; technology, and movies. It is annotated for eight emotions: Disgusted, Angry, Fearful, Happy, Sad, Surprised, Neutral, and Curious to dive deeper. The conversation partners don’t have explicitly defined roles as ‘speaker’ or ‘listener’ so they interact naturally similar to how people engage in real-world conversations. Due to the topical variety, emotion annotation, and representation of natural human conversations, we chose TopicalChat as the foundation for constructing the multimodal dialogue dataset.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Recording</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Data was recorded in a professional recording studio with a green screen and minimal background noise, shown in Appendix <a href="#A1.SS4" title="A.4 Recording Setup ‣ Appendix A MultiDialog Dataset ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a>. During a recording session, two conversation partners sat side-by-side and were recorded with a separate camera and a microphone. The camera position was adjusted according to the individual’s height to capture the upper body, starting from the shoulders. The participants were asked to act according to a given script conveying the desired emotion annotation for each utterance. We specifically provided detailed instructions for visual and audio cues based on the Facial Action Coding System <cite class="ltx_cite ltx_citemacro_cite">Ekman and Friesen (<a href="#bib.bib20" title="" class="ltx_ref">1978</a>)</cite> and tone <cite class="ltx_cite ltx_citemacro_cite">Gangamohan et al. (<a href="#bib.bib21" title="" class="ltx_ref">2016</a>)</cite> for each emotion as follows:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Neutral</span>: normal resting face, emotionless, speak still with natural information.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Happy</span>: lip corner puller, cheek raiser, lips parts, speak cheerfully in a higher tone.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Sad</span>: drooping upper eyelids, slight pulling down of lips corners, speak in a sad, lower tone.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Fearful</span>: eyebrows raised and pulled together, eye pulled open, speak in a soft and low tone.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Surprise</span>: eyebrows raised, eyes wide open, mouth open wider, speak excitedly with high tone.</p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p"><span id="S3.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Disgusted</span>: eyebrows lowered and pulled together, nose wrinkled, cheek raised, upper lip raised, speak in a normal tone with disgusted intonation.</p>
</div>
</li>
<li id="S3.I1.i7" class="ltx_item" style="list-style-type:none;padding-top:-5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i7.p1" class="ltx_para">
<p id="S3.I1.i7.p1.1" class="ltx_p"><span id="S3.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">Anger</span>: eyebrows lowered and pulled together, eyes glare, speak powerfully with high tone.</p>
</div>
</li>
</ul>
<p id="S3.SS2.p1.2" class="ltx_p">For recordings, we combined the emotion labels ‘Neutral’ and ‘Curious to dive deeper’ into a single label ‘Neutral’ due to the lack of visually apparent difference between the two. In addition to the instructions, we displayed sample images on the screen so that the actors could mimic the facial expressions corresponding to the emotion.
Moreover, when the turn passes to another participant, they naturally react while listening. Participants were instructed to press a button to proceed to the next utterance, which recorded the start and end times of each turn for post-processing. The audio streams were recorded in a mono WAV format at 48kHz and the video streams in full HD at 30fps.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Post-Processing</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To refine the data, we had an annotator go through the audio-visual recordings to check if there were any misalignments between the audio and visual streams. We asked the annotator to manually adjust the misalignments by sliding the start time. Additionally, we filtered out recordings that were missing either audio or visual streams. Then, we segmented the recordings into conversations and turns based on the recorded timesteps of each turn. As a result, the post-processed MultiDialog dataset consists of approximately 340 hours of audio-visual videos of 9,000 dialogues between 6 pairs of conversation partners. The final statistics of our dataset are shown in Table <a href="#S1.T2" title="Table 2 ‣ 1 Introduction ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Furthermore, we release a gold emotion dialogue subset selected based on rigorous annotation evaluation. Please refer to the Appendix <a href="#A1.SS3.SSS1" title="A.3.1 Gold Emotion Dialogue Subset ‣ A.3 Annotation Evaluation ‣ Appendix A MultiDialog Dataset ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3.1</span></a> for more details.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<p id="S3.F1.1.1" class="ltx_p ltx_minipage ltx_align_bottom" style="width:433.6pt;"><span id="S3.F1.1.1.1" class="ltx_text"><img src="/html/2406.07867/assets/x1.png" id="S3.F1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="332" height="293" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of the proposed framework for multimodal spoken dialogue language modeling. With the AV speech tokens as the pseudo-texts, it can process audio-visual face video from the user input and generate corresponding response as audio-visual face video.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Audio-Visual Spoken Dialogue System</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Based on the proposed MultiDialog dataset, we introduce an audio-visual spoken dialogue system that directly understands the audio-visual of the user’s face video and generates appropriate responses with audio-visual face video. It consists of three main parts: 1) Encoding audio-visual speech into discrete representations, namely audio-visual (AV) speech tokens. 2) Conducting multimodal spoken dialogue language modeling using the AV speech tokens as pseudo texts. 3) Projecting the output AV speech tokens into the audio and visual space for direct face-to-face dialogue.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Audio-Visual Speech Encoding</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">By integrating both audio and visual modalities, we can improve the dialogue system’s understanding of the speech content. This is because speech not only comprises auditory signals but also visual cues from the movements of the speaker’s mouth. This visual information complements auditory signals, particularly in noisy environments, resulting in more robust performance <cite class="ltx_cite ltx_citemacro_cite">Afouras et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">To this end, we adopt a unified approach to model both the audio and visual of talking face input into audio-visual speech tokens. Inspired by the recent success of utilizing discrete speech tokens extracted from self-supervised speech models <cite class="ltx_cite ltx_citemacro_cite">Schneider et al. (<a href="#bib.bib58" title="" class="ltx_ref">2019</a>); Baevski et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>); Hsu et al. (<a href="#bib.bib29" title="" class="ltx_ref">2021</a>); Chung et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>); Babu et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite> in speech processing <cite class="ltx_cite ltx_citemacro_cite">Lakhotia et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>); Lee et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>); Maiti et al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>); Kim et al. (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>, we tokenize the audio and visual streams into audio-visual speech tokens (<span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">a.k.a.</span> AV speech tokens).
Specifically, we employ one of the multimodal speech models, AV-HuBERT <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a href="#bib.bib59" title="" class="ltx_ref">2021</a>)</cite>, a state-of-the-art self-supervised framework for understanding speech by both seeing and hearing. It is trained on raw audio-visual face videos to predict discrete clusters from speech <cite class="ltx_cite ltx_citemacro_cite">Hassid et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>. The audio-visual speech features are extracted and quantized into discrete tokens as in <cite class="ltx_cite ltx_citemacro_cite">Lakhotia et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>); Popuri et al. (<a href="#bib.bib51" title="" class="ltx_ref">2022</a>); Kim et al. (<a href="#bib.bib35" title="" class="ltx_ref">2024</a>)</cite>. By combining the visual cues and the auditory information, the audio-visual speech tokens extract both linguistic and phonetic information. Then, we treat the AV speech tokens as pseudo text to train our Audio-Visual Spoken Dialogue LM.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Audio-Visual Spoken Dialogue Language Modeling</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">As shown in Fig. <a href="#S3.F1" title="Figure 1 ‣ 3.3 Post-Processing ‣ 3 MultiDialog Dataset ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, our audio-visual spoken dialogue language model is trained with the AV speech tokens on our MultiDialog dataset.
Previous work <cite class="ltx_cite ltx_citemacro_cite">Hassid et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> showed that initializing a speech language model with a textually pretrained language model (LLM) leads to better performance and faster convergence. Accordingly, we use a pretrained LLM, OPT-1.3B <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib70" title="" class="ltx_ref">2022</a>)</cite> to initialize our model and combine the vocabulary of AV speech tokens with the original text vocabulary, as in <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2023a</a>); Nachmani et al. (<a href="#bib.bib46" title="" class="ltx_ref">2023</a>); Maiti et al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite>. This allows us to jointly model the probability of both AV speech tokens and text tokens <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">t</annotation></semantics></math>, where the loss can be represented as,</p>
<table id="A2.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E1.m1.2" class="ltx_Math" alttext="\displaystyle\mathcal{L}=-\sum_{i=1}^{N}\log p(t_{i}\mid t_{1},...,t_{i-1})," display="inline"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.2.1" xref="S4.E1.m1.2.2.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1" xref="S4.E1.m1.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.1.1.3" xref="S4.E1.m1.2.2.1.1.3.cmml">ℒ</mi><mo id="S4.E1.m1.2.2.1.1.2" xref="S4.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.2.2.1.1.1" xref="S4.E1.m1.2.2.1.1.1.cmml"><mo id="S4.E1.m1.2.2.1.1.1a" xref="S4.E1.m1.2.2.1.1.1.cmml">−</mo><mrow id="S4.E1.m1.2.2.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.cmml"><mstyle displaystyle="true" id="S4.E1.m1.2.2.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.2.cmml"><munderover id="S4.E1.m1.2.2.1.1.1.1.2a" xref="S4.E1.m1.2.2.1.1.1.1.2.cmml"><mo movablelimits="false" id="S4.E1.m1.2.2.1.1.1.1.2.2.2" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.2.2.3" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.2.2.3.2" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E1.m1.2.2.1.1.1.1.2.2.3.1" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E1.m1.2.2.1.1.1.1.2.2.3.3" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S4.E1.m1.2.2.1.1.1.1.2.3" xref="S4.E1.m1.2.2.1.1.1.1.2.3.cmml">N</mi></munderover></mstyle><mrow id="S4.E1.m1.2.2.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.3.1" xref="S4.E1.m1.2.2.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S4.E1.m1.2.2.1.1.1.1.1.3a" xref="S4.E1.m1.2.2.1.1.1.1.1.3.cmml">⁡</mo><mi id="S4.E1.m1.2.2.1.1.1.1.1.3.2" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2.cmml">t</mi><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3.cmml">i</mi></msub><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">∣</mo><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml"><msub id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">t</mi><mn id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mi mathvariant="normal" id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">…</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.4" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.cmml">t</mi><mrow id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">i</mi><mo id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">−</mo><mn id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo stretchy="false" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S4.E1.m1.2.2.1.2" xref="S4.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.2.1.1.cmml" xref="S4.E1.m1.2.2.1"><eq id="S4.E1.m1.2.2.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.2"></eq><ci id="S4.E1.m1.2.2.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.3">ℒ</ci><apply id="S4.E1.m1.2.2.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1"><minus id="S4.E1.m1.2.2.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1"></minus><apply id="S4.E1.m1.2.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1"><apply id="S4.E1.m1.2.2.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.2.2.1.1.1.1.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2">subscript</csymbol><sum id="S4.E1.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.2"></sum><apply id="S4.E1.m1.2.2.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3"><eq id="S4.E1.m1.2.2.1.1.1.1.2.2.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.1"></eq><ci id="S4.E1.m1.2.2.1.1.1.1.2.2.3.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S4.E1.m1.2.2.1.1.1.1.2.2.3.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S4.E1.m1.2.2.1.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.2.3">𝑁</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1"><times id="S4.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.2"></times><apply id="S4.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3"><log id="S4.E1.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3.1"></log><ci id="S4.E1.m1.2.2.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.3.2">𝑝</ci></apply><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2">𝑡</ci><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3">𝑖</ci></apply><list id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2"><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3">1</cn></apply><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">…</ci><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2">𝑡</ci><apply id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3"><minus id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1"></minus><ci id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2">𝑖</ci><cn type="integer" id="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S4.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3">1</cn></apply></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">\displaystyle\mathcal{L}=-\sum_{i=1}^{N}\log p(t_{i}\mid t_{1},...,t_{i-1}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p1.2" class="ltx_p">which is the negative log-likelihood of predicting the next token in the sequence of length <math id="S4.SS2.p1.2.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS2.p1.2.m1.1a"><mi id="S4.SS2.p1.2.m1.1.1" xref="S4.SS2.p1.2.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m1.1b"><ci id="S4.SS2.p1.2.m1.1.1.cmml" xref="S4.SS2.p1.2.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m1.1c">N</annotation></semantics></math> tokens.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.3" class="ltx_p">Motivated by the joint speech-text training used in speech processing tasks such as speech translation, audio speech recognition, and text-to-speech synthesis <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>); Maiti et al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>); Dong et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>); Wang et al. (<a href="#bib.bib65" title="" class="ltx_ref">2023b</a>)</cite>, we newly introduce a joint speech-text pre-training scheme tailored for spoken dialogue language modeling. In our setting, each dialogue <math id="S4.SS2.p2.1.m1.7" class="ltx_Math" alttext="D=[T_{1}^{ai},T_{1}^{user},T_{2}^{ai},T_{2}^{user},\ldots,T_{k}^{ai},T_{k}^{user}]" display="inline"><semantics id="S4.SS2.p2.1.m1.7a"><mrow id="S4.SS2.p2.1.m1.7.7" xref="S4.SS2.p2.1.m1.7.7.cmml"><mi id="S4.SS2.p2.1.m1.7.7.8" xref="S4.SS2.p2.1.m1.7.7.8.cmml">D</mi><mo id="S4.SS2.p2.1.m1.7.7.7" xref="S4.SS2.p2.1.m1.7.7.7.cmml">=</mo><mrow id="S4.SS2.p2.1.m1.7.7.6.6" xref="S4.SS2.p2.1.m1.7.7.6.7.cmml"><mo stretchy="false" id="S4.SS2.p2.1.m1.7.7.6.6.7" xref="S4.SS2.p2.1.m1.7.7.6.7.cmml">[</mo><msubsup id="S4.SS2.p2.1.m1.2.2.1.1.1" xref="S4.SS2.p2.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.2.2.1.1.1.2.2" xref="S4.SS2.p2.1.m1.2.2.1.1.1.2.2.cmml">T</mi><mn id="S4.SS2.p2.1.m1.2.2.1.1.1.2.3" xref="S4.SS2.p2.1.m1.2.2.1.1.1.2.3.cmml">1</mn><mrow id="S4.SS2.p2.1.m1.2.2.1.1.1.3" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3.cmml"><mi id="S4.SS2.p2.1.m1.2.2.1.1.1.3.2" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.2.2.1.1.1.3.1" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.2.2.1.1.1.3.3" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3.3.cmml">i</mi></mrow></msubsup><mo id="S4.SS2.p2.1.m1.7.7.6.6.8" xref="S4.SS2.p2.1.m1.7.7.6.7.cmml">,</mo><msubsup id="S4.SS2.p2.1.m1.3.3.2.2.2" xref="S4.SS2.p2.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS2.p2.1.m1.3.3.2.2.2.2.2" xref="S4.SS2.p2.1.m1.3.3.2.2.2.2.2.cmml">T</mi><mn id="S4.SS2.p2.1.m1.3.3.2.2.2.2.3" xref="S4.SS2.p2.1.m1.3.3.2.2.2.2.3.cmml">1</mn><mrow id="S4.SS2.p2.1.m1.3.3.2.2.2.3" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.cmml"><mi id="S4.SS2.p2.1.m1.3.3.2.2.2.3.2" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.3.3.2.2.2.3.1" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.3.3.2.2.2.3.3" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.3.3.2.2.2.3.1a" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.3.3.2.2.2.3.4" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.3.3.2.2.2.3.1b" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.3.3.2.2.2.3.5" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.5.cmml">r</mi></mrow></msubsup><mo id="S4.SS2.p2.1.m1.7.7.6.6.9" xref="S4.SS2.p2.1.m1.7.7.6.7.cmml">,</mo><msubsup id="S4.SS2.p2.1.m1.4.4.3.3.3" xref="S4.SS2.p2.1.m1.4.4.3.3.3.cmml"><mi id="S4.SS2.p2.1.m1.4.4.3.3.3.2.2" xref="S4.SS2.p2.1.m1.4.4.3.3.3.2.2.cmml">T</mi><mn id="S4.SS2.p2.1.m1.4.4.3.3.3.2.3" xref="S4.SS2.p2.1.m1.4.4.3.3.3.2.3.cmml">2</mn><mrow id="S4.SS2.p2.1.m1.4.4.3.3.3.3" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3.cmml"><mi id="S4.SS2.p2.1.m1.4.4.3.3.3.3.2" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.4.4.3.3.3.3.1" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.4.4.3.3.3.3.3" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3.3.cmml">i</mi></mrow></msubsup><mo id="S4.SS2.p2.1.m1.7.7.6.6.10" xref="S4.SS2.p2.1.m1.7.7.6.7.cmml">,</mo><msubsup id="S4.SS2.p2.1.m1.5.5.4.4.4" xref="S4.SS2.p2.1.m1.5.5.4.4.4.cmml"><mi id="S4.SS2.p2.1.m1.5.5.4.4.4.2.2" xref="S4.SS2.p2.1.m1.5.5.4.4.4.2.2.cmml">T</mi><mn id="S4.SS2.p2.1.m1.5.5.4.4.4.2.3" xref="S4.SS2.p2.1.m1.5.5.4.4.4.2.3.cmml">2</mn><mrow id="S4.SS2.p2.1.m1.5.5.4.4.4.3" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3.cmml"><mi id="S4.SS2.p2.1.m1.5.5.4.4.4.3.2" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.5.5.4.4.4.3.1" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.5.5.4.4.4.3.3" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.5.5.4.4.4.3.1a" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.5.5.4.4.4.3.4" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.5.5.4.4.4.3.1b" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.5.5.4.4.4.3.5" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3.5.cmml">r</mi></mrow></msubsup><mo id="S4.SS2.p2.1.m1.7.7.6.6.11" xref="S4.SS2.p2.1.m1.7.7.6.7.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">…</mi><mo id="S4.SS2.p2.1.m1.7.7.6.6.12" xref="S4.SS2.p2.1.m1.7.7.6.7.cmml">,</mo><msubsup id="S4.SS2.p2.1.m1.6.6.5.5.5" xref="S4.SS2.p2.1.m1.6.6.5.5.5.cmml"><mi id="S4.SS2.p2.1.m1.6.6.5.5.5.2.2" xref="S4.SS2.p2.1.m1.6.6.5.5.5.2.2.cmml">T</mi><mi id="S4.SS2.p2.1.m1.6.6.5.5.5.2.3" xref="S4.SS2.p2.1.m1.6.6.5.5.5.2.3.cmml">k</mi><mrow id="S4.SS2.p2.1.m1.6.6.5.5.5.3" xref="S4.SS2.p2.1.m1.6.6.5.5.5.3.cmml"><mi id="S4.SS2.p2.1.m1.6.6.5.5.5.3.2" xref="S4.SS2.p2.1.m1.6.6.5.5.5.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.6.6.5.5.5.3.1" xref="S4.SS2.p2.1.m1.6.6.5.5.5.3.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.6.6.5.5.5.3.3" xref="S4.SS2.p2.1.m1.6.6.5.5.5.3.3.cmml">i</mi></mrow></msubsup><mo id="S4.SS2.p2.1.m1.7.7.6.6.13" xref="S4.SS2.p2.1.m1.7.7.6.7.cmml">,</mo><msubsup id="S4.SS2.p2.1.m1.7.7.6.6.6" xref="S4.SS2.p2.1.m1.7.7.6.6.6.cmml"><mi id="S4.SS2.p2.1.m1.7.7.6.6.6.2.2" xref="S4.SS2.p2.1.m1.7.7.6.6.6.2.2.cmml">T</mi><mi id="S4.SS2.p2.1.m1.7.7.6.6.6.2.3" xref="S4.SS2.p2.1.m1.7.7.6.6.6.2.3.cmml">k</mi><mrow id="S4.SS2.p2.1.m1.7.7.6.6.6.3" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3.cmml"><mi id="S4.SS2.p2.1.m1.7.7.6.6.6.3.2" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.7.7.6.6.6.3.1" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.7.7.6.6.6.3.3" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.7.7.6.6.6.3.1a" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.7.7.6.6.6.3.4" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.7.7.6.6.6.3.1b" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.7.7.6.6.6.3.5" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3.5.cmml">r</mi></mrow></msubsup><mo stretchy="false" id="S4.SS2.p2.1.m1.7.7.6.6.14" xref="S4.SS2.p2.1.m1.7.7.6.7.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.7b"><apply id="S4.SS2.p2.1.m1.7.7.cmml" xref="S4.SS2.p2.1.m1.7.7"><eq id="S4.SS2.p2.1.m1.7.7.7.cmml" xref="S4.SS2.p2.1.m1.7.7.7"></eq><ci id="S4.SS2.p2.1.m1.7.7.8.cmml" xref="S4.SS2.p2.1.m1.7.7.8">𝐷</ci><list id="S4.SS2.p2.1.m1.7.7.6.7.cmml" xref="S4.SS2.p2.1.m1.7.7.6.6"><apply id="S4.SS2.p2.1.m1.2.2.1.1.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1">superscript</csymbol><apply id="S4.SS2.p2.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.2.2.1.1.1.2.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.2.2.1.1.1.2.2.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1.2.2">𝑇</ci><cn type="integer" id="S4.SS2.p2.1.m1.2.2.1.1.1.2.3.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1.2.3">1</cn></apply><apply id="S4.SS2.p2.1.m1.2.2.1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3"><times id="S4.SS2.p2.1.m1.2.2.1.1.1.3.1.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3.1"></times><ci id="S4.SS2.p2.1.m1.2.2.1.1.1.3.2.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3.2">𝑎</ci><ci id="S4.SS2.p2.1.m1.2.2.1.1.1.3.3.cmml" xref="S4.SS2.p2.1.m1.2.2.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S4.SS2.p2.1.m1.3.3.2.2.2.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2">superscript</csymbol><apply id="S4.SS2.p2.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.3.3.2.2.2.2.1.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.p2.1.m1.3.3.2.2.2.2.2.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2.2.2">𝑇</ci><cn type="integer" id="S4.SS2.p2.1.m1.3.3.2.2.2.2.3.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2.2.3">1</cn></apply><apply id="S4.SS2.p2.1.m1.3.3.2.2.2.3.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3"><times id="S4.SS2.p2.1.m1.3.3.2.2.2.3.1.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.1"></times><ci id="S4.SS2.p2.1.m1.3.3.2.2.2.3.2.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.2">𝑢</ci><ci id="S4.SS2.p2.1.m1.3.3.2.2.2.3.3.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.3">𝑠</ci><ci id="S4.SS2.p2.1.m1.3.3.2.2.2.3.4.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.4">𝑒</ci><ci id="S4.SS2.p2.1.m1.3.3.2.2.2.3.5.cmml" xref="S4.SS2.p2.1.m1.3.3.2.2.2.3.5">𝑟</ci></apply></apply><apply id="S4.SS2.p2.1.m1.4.4.3.3.3.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.4.4.3.3.3.1.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3">superscript</csymbol><apply id="S4.SS2.p2.1.m1.4.4.3.3.3.2.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.4.4.3.3.3.2.1.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.p2.1.m1.4.4.3.3.3.2.2.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3.2.2">𝑇</ci><cn type="integer" id="S4.SS2.p2.1.m1.4.4.3.3.3.2.3.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3.2.3">2</cn></apply><apply id="S4.SS2.p2.1.m1.4.4.3.3.3.3.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3"><times id="S4.SS2.p2.1.m1.4.4.3.3.3.3.1.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3.1"></times><ci id="S4.SS2.p2.1.m1.4.4.3.3.3.3.2.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3.2">𝑎</ci><ci id="S4.SS2.p2.1.m1.4.4.3.3.3.3.3.cmml" xref="S4.SS2.p2.1.m1.4.4.3.3.3.3.3">𝑖</ci></apply></apply><apply id="S4.SS2.p2.1.m1.5.5.4.4.4.cmml" xref="S4.SS2.p2.1.m1.5.5.4.4.4"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.5.5.4.4.4.1.cmml" xref="S4.SS2.p2.1.m1.5.5.4.4.4">superscript</csymbol><apply id="S4.SS2.p2.1.m1.5.5.4.4.4.2.cmml" xref="S4.SS2.p2.1.m1.5.5.4.4.4"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.5.5.4.4.4.2.1.cmml" xref="S4.SS2.p2.1.m1.5.5.4.4.4">subscript</csymbol><ci id="S4.SS2.p2.1.m1.5.5.4.4.4.2.2.cmml" xref="S4.SS2.p2.1.m1.5.5.4.4.4.2.2">𝑇</ci><cn type="integer" id="S4.SS2.p2.1.m1.5.5.4.4.4.2.3.cmml" xref="S4.SS2.p2.1.m1.5.5.4.4.4.2.3">2</cn></apply><apply id="S4.SS2.p2.1.m1.5.5.4.4.4.3.cmml" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3"><times id="S4.SS2.p2.1.m1.5.5.4.4.4.3.1.cmml" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3.1"></times><ci id="S4.SS2.p2.1.m1.5.5.4.4.4.3.2.cmml" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3.2">𝑢</ci><ci id="S4.SS2.p2.1.m1.5.5.4.4.4.3.3.cmml" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3.3">𝑠</ci><ci id="S4.SS2.p2.1.m1.5.5.4.4.4.3.4.cmml" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3.4">𝑒</ci><ci id="S4.SS2.p2.1.m1.5.5.4.4.4.3.5.cmml" xref="S4.SS2.p2.1.m1.5.5.4.4.4.3.5">𝑟</ci></apply></apply><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">…</ci><apply id="S4.SS2.p2.1.m1.6.6.5.5.5.cmml" xref="S4.SS2.p2.1.m1.6.6.5.5.5"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.6.6.5.5.5.1.cmml" xref="S4.SS2.p2.1.m1.6.6.5.5.5">superscript</csymbol><apply id="S4.SS2.p2.1.m1.6.6.5.5.5.2.cmml" xref="S4.SS2.p2.1.m1.6.6.5.5.5"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.6.6.5.5.5.2.1.cmml" xref="S4.SS2.p2.1.m1.6.6.5.5.5">subscript</csymbol><ci id="S4.SS2.p2.1.m1.6.6.5.5.5.2.2.cmml" xref="S4.SS2.p2.1.m1.6.6.5.5.5.2.2">𝑇</ci><ci id="S4.SS2.p2.1.m1.6.6.5.5.5.2.3.cmml" xref="S4.SS2.p2.1.m1.6.6.5.5.5.2.3">𝑘</ci></apply><apply id="S4.SS2.p2.1.m1.6.6.5.5.5.3.cmml" xref="S4.SS2.p2.1.m1.6.6.5.5.5.3"><times id="S4.SS2.p2.1.m1.6.6.5.5.5.3.1.cmml" xref="S4.SS2.p2.1.m1.6.6.5.5.5.3.1"></times><ci id="S4.SS2.p2.1.m1.6.6.5.5.5.3.2.cmml" xref="S4.SS2.p2.1.m1.6.6.5.5.5.3.2">𝑎</ci><ci id="S4.SS2.p2.1.m1.6.6.5.5.5.3.3.cmml" xref="S4.SS2.p2.1.m1.6.6.5.5.5.3.3">𝑖</ci></apply></apply><apply id="S4.SS2.p2.1.m1.7.7.6.6.6.cmml" xref="S4.SS2.p2.1.m1.7.7.6.6.6"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.7.7.6.6.6.1.cmml" xref="S4.SS2.p2.1.m1.7.7.6.6.6">superscript</csymbol><apply id="S4.SS2.p2.1.m1.7.7.6.6.6.2.cmml" xref="S4.SS2.p2.1.m1.7.7.6.6.6"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.7.7.6.6.6.2.1.cmml" xref="S4.SS2.p2.1.m1.7.7.6.6.6">subscript</csymbol><ci id="S4.SS2.p2.1.m1.7.7.6.6.6.2.2.cmml" xref="S4.SS2.p2.1.m1.7.7.6.6.6.2.2">𝑇</ci><ci id="S4.SS2.p2.1.m1.7.7.6.6.6.2.3.cmml" xref="S4.SS2.p2.1.m1.7.7.6.6.6.2.3">𝑘</ci></apply><apply id="S4.SS2.p2.1.m1.7.7.6.6.6.3.cmml" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3"><times id="S4.SS2.p2.1.m1.7.7.6.6.6.3.1.cmml" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3.1"></times><ci id="S4.SS2.p2.1.m1.7.7.6.6.6.3.2.cmml" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3.2">𝑢</ci><ci id="S4.SS2.p2.1.m1.7.7.6.6.6.3.3.cmml" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3.3">𝑠</ci><ci id="S4.SS2.p2.1.m1.7.7.6.6.6.3.4.cmml" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3.4">𝑒</ci><ci id="S4.SS2.p2.1.m1.7.7.6.6.6.3.5.cmml" xref="S4.SS2.p2.1.m1.7.7.6.6.6.3.5">𝑟</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.7c">D=[T_{1}^{ai},T_{1}^{user},T_{2}^{ai},T_{2}^{user},\ldots,T_{k}^{ai},T_{k}^{user}]</annotation></semantics></math> consists of <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">k</annotation></semantics></math> rounds of turns <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mi id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><ci id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">T</annotation></semantics></math> between two speakers which we randomly designate as the AI and the User. The goal of this pre-training is to effectively transform the text-based LLM into the AV speech token-based LLM, enabling it to produce relevant AV speech responses from the AI side given a conversation context. It proceeds in the following two stages:</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.5" class="ltx_p"><span id="S4.SS2.p3.5.1" class="ltx_text ltx_font_bold">The first stage</span> is instructing the LLM to interpret and generate AV speech tokens. We segment the dialogue into turns <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">T</annotation></semantics></math> and prepare paired AV speech tokens <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="T_{\text{AV}}" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><msub id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mi id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml">T</mi><mtext id="S4.SS2.p3.2.m2.1.1.3" xref="S4.SS2.p3.2.m2.1.1.3a.cmml">AV</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2">𝑇</ci><ci id="S4.SS2.p3.2.m2.1.1.3a.cmml" xref="S4.SS2.p3.2.m2.1.1.3"><mtext mathsize="70%" id="S4.SS2.p3.2.m2.1.1.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3">AV</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">T_{\text{AV}}</annotation></semantics></math> and text tokens <math id="S4.SS2.p3.3.m3.1" class="ltx_Math" alttext="T_{\text{Text}}" display="inline"><semantics id="S4.SS2.p3.3.m3.1a"><msub id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml"><mi id="S4.SS2.p3.3.m3.1.1.2" xref="S4.SS2.p3.3.m3.1.1.2.cmml">T</mi><mtext id="S4.SS2.p3.3.m3.1.1.3" xref="S4.SS2.p3.3.m3.1.1.3a.cmml">Text</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><apply id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.3.m3.1.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p3.3.m3.1.1.2.cmml" xref="S4.SS2.p3.3.m3.1.1.2">𝑇</ci><ci id="S4.SS2.p3.3.m3.1.1.3a.cmml" xref="S4.SS2.p3.3.m3.1.1.3"><mtext mathsize="70%" id="S4.SS2.p3.3.m3.1.1.3.cmml" xref="S4.SS2.p3.3.m3.1.1.3">Text</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">T_{\text{Text}}</annotation></semantics></math>. We then concatenate the pair with their respective modality prefix tokens, <span id="S4.SS2.p3.5.2" class="ltx_text ltx_markedasmath">&lt;speech&gt;</span> and <span id="S4.SS2.p3.5.3" class="ltx_text ltx_markedasmath">&lt;text&gt;</span>, to indicate the beginning of AV speech and text tokens. Adding the reversed order of concatenation, we construct both audio-visual speech recognition (AVSR) and text-to-speech generation (TTS) training objectives as shown in Fig. <a href="#S4.F2" title="Figure 2 ‣ 4.2 Audio-Visual Spoken Dialogue Language Modeling ‣ 4 Audio-Visual Spoken Dialogue System ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a) and (b), where the loss functions can be respectively represented as:</p>
<table id="A2.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E2.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\text{AVSR}}" display="inline"><semantics id="S4.E2.m1.1a"><msub id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml">ℒ</mi><mtext id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3a.cmml">AVSR</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1">subscript</csymbol><ci id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2">ℒ</ci><ci id="S4.E2.m1.1.1.3a.cmml" xref="S4.E2.m1.1.1.3"><mtext mathsize="70%" id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3">AVSR</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\displaystyle\mathcal{L}_{\text{AVSR}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E2.m2.1" class="ltx_Math" alttext="\displaystyle=\sum_{i=1}^{N}-\log p(\ T_{\text{AV}}^{i}\mid T_{\text{AV}}^{&lt;i},T_{\text{Text}})" display="inline"><semantics id="S4.E2.m2.1a"><mrow id="S4.E2.m2.1.1" xref="S4.E2.m2.1.1.cmml"><mi id="S4.E2.m2.1.1.3" xref="S4.E2.m2.1.1.3.cmml"></mi><mo id="S4.E2.m2.1.1.2" xref="S4.E2.m2.1.1.2.cmml">=</mo><mrow id="S4.E2.m2.1.1.1" xref="S4.E2.m2.1.1.1.cmml"><mstyle displaystyle="true" id="S4.E2.m2.1.1.1.3" xref="S4.E2.m2.1.1.1.3.cmml"><munderover id="S4.E2.m2.1.1.1.3a" xref="S4.E2.m2.1.1.1.3.cmml"><mo movablelimits="false" id="S4.E2.m2.1.1.1.3.2.2" xref="S4.E2.m2.1.1.1.3.2.2.cmml">∑</mo><mrow id="S4.E2.m2.1.1.1.3.2.3" xref="S4.E2.m2.1.1.1.3.2.3.cmml"><mi id="S4.E2.m2.1.1.1.3.2.3.2" xref="S4.E2.m2.1.1.1.3.2.3.2.cmml">i</mi><mo id="S4.E2.m2.1.1.1.3.2.3.1" xref="S4.E2.m2.1.1.1.3.2.3.1.cmml">=</mo><mn id="S4.E2.m2.1.1.1.3.2.3.3" xref="S4.E2.m2.1.1.1.3.2.3.3.cmml">1</mn></mrow><mi id="S4.E2.m2.1.1.1.3.3" xref="S4.E2.m2.1.1.1.3.3.cmml">N</mi></munderover></mstyle><mo id="S4.E2.m2.1.1.1.2" xref="S4.E2.m2.1.1.1.2.cmml">−</mo><mrow id="S4.E2.m2.1.1.1.1" xref="S4.E2.m2.1.1.1.1.cmml"><mrow id="S4.E2.m2.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.3.cmml"><mi id="S4.E2.m2.1.1.1.1.3.1" xref="S4.E2.m2.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S4.E2.m2.1.1.1.1.3a" xref="S4.E2.m2.1.1.1.1.3.cmml">⁡</mo><mi id="S4.E2.m2.1.1.1.1.3.2" xref="S4.E2.m2.1.1.1.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S4.E2.m2.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.2.cmml">​</mo><mrow id="S4.E2.m2.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.1.cmml"><mo rspace="0.500em" stretchy="false" id="S4.E2.m2.1.1.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E2.m2.1.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.1.cmml"><msubsup id="S4.E2.m2.1.1.1.1.1.1.1.4" xref="S4.E2.m2.1.1.1.1.1.1.1.4.cmml"><mi id="S4.E2.m2.1.1.1.1.1.1.1.4.2.2" xref="S4.E2.m2.1.1.1.1.1.1.1.4.2.2.cmml">T</mi><mtext id="S4.E2.m2.1.1.1.1.1.1.1.4.2.3" xref="S4.E2.m2.1.1.1.1.1.1.1.4.2.3a.cmml">AV</mtext><mi id="S4.E2.m2.1.1.1.1.1.1.1.4.3" xref="S4.E2.m2.1.1.1.1.1.1.1.4.3.cmml">i</mi></msubsup><mo id="S4.E2.m2.1.1.1.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.1.1.1.3.cmml">∣</mo><mrow id="S4.E2.m2.1.1.1.1.1.1.1.2.2" xref="S4.E2.m2.1.1.1.1.1.1.1.2.3.cmml"><msubsup id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.2.2.cmml">T</mi><mtext id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.2.3a.cmml">AV</mtext><mrow id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msubsup><mo id="S4.E2.m2.1.1.1.1.1.1.1.2.2.3" xref="S4.E2.m2.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S4.E2.m2.1.1.1.1.1.1.1.2.2.2" xref="S4.E2.m2.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S4.E2.m2.1.1.1.1.1.1.1.2.2.2.2" xref="S4.E2.m2.1.1.1.1.1.1.1.2.2.2.2.cmml">T</mi><mtext id="S4.E2.m2.1.1.1.1.1.1.1.2.2.2.3" xref="S4.E2.m2.1.1.1.1.1.1.1.2.2.2.3a.cmml">Text</mtext></msub></mrow></mrow><mo stretchy="false" id="S4.E2.m2.1.1.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m2.1b"><apply id="S4.E2.m2.1.1.cmml" xref="S4.E2.m2.1.1"><eq id="S4.E2.m2.1.1.2.cmml" xref="S4.E2.m2.1.1.2"></eq><csymbol cd="latexml" id="S4.E2.m2.1.1.3.cmml" xref="S4.E2.m2.1.1.3">absent</csymbol><apply id="S4.E2.m2.1.1.1.cmml" xref="S4.E2.m2.1.1.1"><minus id="S4.E2.m2.1.1.1.2.cmml" xref="S4.E2.m2.1.1.1.2"></minus><apply id="S4.E2.m2.1.1.1.3.cmml" xref="S4.E2.m2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m2.1.1.1.3.1.cmml" xref="S4.E2.m2.1.1.1.3">superscript</csymbol><apply id="S4.E2.m2.1.1.1.3.2.cmml" xref="S4.E2.m2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m2.1.1.1.3.2.1.cmml" xref="S4.E2.m2.1.1.1.3">subscript</csymbol><sum id="S4.E2.m2.1.1.1.3.2.2.cmml" xref="S4.E2.m2.1.1.1.3.2.2"></sum><apply id="S4.E2.m2.1.1.1.3.2.3.cmml" xref="S4.E2.m2.1.1.1.3.2.3"><eq id="S4.E2.m2.1.1.1.3.2.3.1.cmml" xref="S4.E2.m2.1.1.1.3.2.3.1"></eq><ci id="S4.E2.m2.1.1.1.3.2.3.2.cmml" xref="S4.E2.m2.1.1.1.3.2.3.2">𝑖</ci><cn type="integer" id="S4.E2.m2.1.1.1.3.2.3.3.cmml" xref="S4.E2.m2.1.1.1.3.2.3.3">1</cn></apply></apply><ci id="S4.E2.m2.1.1.1.3.3.cmml" xref="S4.E2.m2.1.1.1.3.3">𝑁</ci></apply><apply id="S4.E2.m2.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1"><times id="S4.E2.m2.1.1.1.1.2.cmml" xref="S4.E2.m2.1.1.1.1.2"></times><apply id="S4.E2.m2.1.1.1.1.3.cmml" xref="S4.E2.m2.1.1.1.1.3"><log id="S4.E2.m2.1.1.1.1.3.1.cmml" xref="S4.E2.m2.1.1.1.1.3.1"></log><ci id="S4.E2.m2.1.1.1.1.3.2.cmml" xref="S4.E2.m2.1.1.1.1.3.2">𝑝</ci></apply><apply id="S4.E2.m2.1.1.1.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E2.m2.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S4.E2.m2.1.1.1.1.1.1.1.4.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E2.m2.1.1.1.1.1.1.1.4.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.4">superscript</csymbol><apply id="S4.E2.m2.1.1.1.1.1.1.1.4.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E2.m2.1.1.1.1.1.1.1.4.2.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S4.E2.m2.1.1.1.1.1.1.1.4.2.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.4.2.2">𝑇</ci><ci id="S4.E2.m2.1.1.1.1.1.1.1.4.2.3a.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.4.2.3"><mtext mathsize="70%" id="S4.E2.m2.1.1.1.1.1.1.1.4.2.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.4.2.3">AV</mtext></ci></apply><ci id="S4.E2.m2.1.1.1.1.1.1.1.4.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.4.3">𝑖</ci></apply><list id="S4.E2.m2.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.2.2"><apply id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.2.2">𝑇</ci><ci id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.2.3a.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.2.3">AV</mtext></ci></apply><apply id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3"><lt id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S4.E2.m2.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E2.m2.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S4.E2.m2.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.2.2.2.2">𝑇</ci><ci id="S4.E2.m2.1.1.1.1.1.1.1.2.2.2.3a.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.2.2.2.3"><mtext mathsize="70%" id="S4.E2.m2.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.2.2.2.3">Text</mtext></ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m2.1c">\displaystyle=\sum_{i=1}^{N}-\log p(\ T_{\text{AV}}^{i}\mid T_{\text{AV}}^{&lt;i},T_{\text{Text}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S4.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E3.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\text{TTS}}" display="inline"><semantics id="S4.E3.m1.1a"><msub id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2.cmml">ℒ</mi><mtext id="S4.E3.m1.1.1.3" xref="S4.E3.m1.1.1.3a.cmml">TTS</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1">subscript</csymbol><ci id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2">ℒ</ci><ci id="S4.E3.m1.1.1.3a.cmml" xref="S4.E3.m1.1.1.3"><mtext mathsize="70%" id="S4.E3.m1.1.1.3.cmml" xref="S4.E3.m1.1.1.3">TTS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">\displaystyle\mathcal{L}_{\text{TTS}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E3.m2.1" class="ltx_Math" alttext="\displaystyle=\sum_{i=1}^{N}-\log p(T_{\text{Text}}^{i}\mid T_{\text{Text}}^{&lt;i},T_{\text{AV}})." display="inline"><semantics id="S4.E3.m2.1a"><mrow id="S4.E3.m2.1.1.1" xref="S4.E3.m2.1.1.1.1.cmml"><mrow id="S4.E3.m2.1.1.1.1" xref="S4.E3.m2.1.1.1.1.cmml"><mi id="S4.E3.m2.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.3.cmml"></mi><mo id="S4.E3.m2.1.1.1.1.2" xref="S4.E3.m2.1.1.1.1.2.cmml">=</mo><mrow id="S4.E3.m2.1.1.1.1.1" xref="S4.E3.m2.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S4.E3.m2.1.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.1.3.cmml"><munderover id="S4.E3.m2.1.1.1.1.1.3a" xref="S4.E3.m2.1.1.1.1.1.3.cmml"><mo movablelimits="false" id="S4.E3.m2.1.1.1.1.1.3.2.2" xref="S4.E3.m2.1.1.1.1.1.3.2.2.cmml">∑</mo><mrow id="S4.E3.m2.1.1.1.1.1.3.2.3" xref="S4.E3.m2.1.1.1.1.1.3.2.3.cmml"><mi id="S4.E3.m2.1.1.1.1.1.3.2.3.2" xref="S4.E3.m2.1.1.1.1.1.3.2.3.2.cmml">i</mi><mo id="S4.E3.m2.1.1.1.1.1.3.2.3.1" xref="S4.E3.m2.1.1.1.1.1.3.2.3.1.cmml">=</mo><mn id="S4.E3.m2.1.1.1.1.1.3.2.3.3" xref="S4.E3.m2.1.1.1.1.1.3.2.3.3.cmml">1</mn></mrow><mi id="S4.E3.m2.1.1.1.1.1.3.3" xref="S4.E3.m2.1.1.1.1.1.3.3.cmml">N</mi></munderover></mstyle><mo id="S4.E3.m2.1.1.1.1.1.2" xref="S4.E3.m2.1.1.1.1.1.2.cmml">−</mo><mrow id="S4.E3.m2.1.1.1.1.1.1" xref="S4.E3.m2.1.1.1.1.1.1.cmml"><mrow id="S4.E3.m2.1.1.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m2.1.1.1.1.1.1.3.1" xref="S4.E3.m2.1.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S4.E3.m2.1.1.1.1.1.1.3a" xref="S4.E3.m2.1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S4.E3.m2.1.1.1.1.1.1.3.2" xref="S4.E3.m2.1.1.1.1.1.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S4.E3.m2.1.1.1.1.1.1.2" xref="S4.E3.m2.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E3.m2.1.1.1.1.1.1.1.1" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m2.1.1.1.1.1.1.1.1.2" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m2.1.1.1.1.1.1.1.1.1" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S4.E3.m2.1.1.1.1.1.1.1.1.1.4" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.2.2" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.2.2.cmml">T</mi><mtext id="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.2.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.2.3a.cmml">Text</mtext><mi id="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.3.cmml">i</mi></msubsup><mo id="S4.E3.m2.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.3.cmml">∣</mo><mrow id="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.3.cmml"><msubsup id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">T</mi><mtext id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml">Text</mtext><mrow id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msubsup><mo id="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">T</mi><mtext id="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2.3a.cmml">AV</mtext></msub></mrow></mrow><mo stretchy="false" id="S4.E3.m2.1.1.1.1.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S4.E3.m2.1.1.1.2" xref="S4.E3.m2.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m2.1b"><apply id="S4.E3.m2.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1"><eq id="S4.E3.m2.1.1.1.1.2.cmml" xref="S4.E3.m2.1.1.1.1.2"></eq><csymbol cd="latexml" id="S4.E3.m2.1.1.1.1.3.cmml" xref="S4.E3.m2.1.1.1.1.3">absent</csymbol><apply id="S4.E3.m2.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1"><minus id="S4.E3.m2.1.1.1.1.1.2.cmml" xref="S4.E3.m2.1.1.1.1.1.2"></minus><apply id="S4.E3.m2.1.1.1.1.1.3.cmml" xref="S4.E3.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m2.1.1.1.1.1.3.1.cmml" xref="S4.E3.m2.1.1.1.1.1.3">superscript</csymbol><apply id="S4.E3.m2.1.1.1.1.1.3.2.cmml" xref="S4.E3.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m2.1.1.1.1.1.3.2.1.cmml" xref="S4.E3.m2.1.1.1.1.1.3">subscript</csymbol><sum id="S4.E3.m2.1.1.1.1.1.3.2.2.cmml" xref="S4.E3.m2.1.1.1.1.1.3.2.2"></sum><apply id="S4.E3.m2.1.1.1.1.1.3.2.3.cmml" xref="S4.E3.m2.1.1.1.1.1.3.2.3"><eq id="S4.E3.m2.1.1.1.1.1.3.2.3.1.cmml" xref="S4.E3.m2.1.1.1.1.1.3.2.3.1"></eq><ci id="S4.E3.m2.1.1.1.1.1.3.2.3.2.cmml" xref="S4.E3.m2.1.1.1.1.1.3.2.3.2">𝑖</ci><cn type="integer" id="S4.E3.m2.1.1.1.1.1.3.2.3.3.cmml" xref="S4.E3.m2.1.1.1.1.1.3.2.3.3">1</cn></apply></apply><ci id="S4.E3.m2.1.1.1.1.1.3.3.cmml" xref="S4.E3.m2.1.1.1.1.1.3.3">𝑁</ci></apply><apply id="S4.E3.m2.1.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1"><times id="S4.E3.m2.1.1.1.1.1.1.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.2"></times><apply id="S4.E3.m2.1.1.1.1.1.1.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.3"><log id="S4.E3.m2.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.3.1"></log><ci id="S4.E3.m2.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.3.2">𝑝</ci></apply><apply id="S4.E3.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.4">superscript</csymbol><apply id="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.2.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.2.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.2.2">𝑇</ci><ci id="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.2.3a.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.2.3"><mtext mathsize="70%" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.2.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.2.3">Text</mtext></ci></apply><ci id="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.4.3">𝑖</ci></apply><list id="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2"><apply id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑇</ci><ci id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3">Text</mtext></ci></apply><apply id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3"><lt id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2.2">𝑇</ci><ci id="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2.3a.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2.3"><mtext mathsize="70%" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.2.2.3">AV</mtext></ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m2.1c">\displaystyle=\sum_{i=1}^{N}-\log p(T_{\text{Text}}^{i}\mid T_{\text{Text}}^{&lt;i},T_{\text{AV}}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p3.6" class="ltx_p">We omitted the prefix tokens for conciseness. Only the embedding layer and the projection layer are trained in the first stage, which guides the LLM to understand and generate AV speech tokens while fully retaining the given LLM knowledge needed for dialogue generation.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.4" class="ltx_p"><span id="S4.SS2.p4.4.1" class="ltx_text ltx_font_bold">The second stage</span> is jointly learning the text and AV speech token-based dialogue. We select either one of the speakers as the AI which the model aims to predict and indicate the start of the response with additional speaker prefix tokens, <span id="S4.SS2.p4.4.2" class="ltx_text ltx_markedasmath">&lt;User&gt;</span> and <span id="S4.SS2.p4.4.3" class="ltx_text ltx_markedasmath">&lt;AI&gt;</span>. The speaker prefix token is followed by a modality prefix token, <span id="S4.SS2.p4.4.4" class="ltx_text ltx_markedasmath">&lt;Speech&gt;</span> and <span id="S4.SS2.p4.4.5" class="ltx_text ltx_markedasmath">&lt;Text&gt;</span>, to indicate whether the utterance is in AV speech or text.
The loss function for dialogue language modeling is:</p>
<table id="A2.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E4.m1.5" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\text{dialog}}=\sum_{k=1}^{K}\sum_{n=1}^{N_{k}}-\log p(T_{k}^{ai,n}\mid T_{k}^{ai,&lt;n},T_{&lt;k})," display="inline"><semantics id="S4.E4.m1.5a"><mrow id="S4.E4.m1.5.5.1" xref="S4.E4.m1.5.5.1.1.cmml"><mrow id="S4.E4.m1.5.5.1.1" xref="S4.E4.m1.5.5.1.1.cmml"><msub id="S4.E4.m1.5.5.1.1.3" xref="S4.E4.m1.5.5.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E4.m1.5.5.1.1.3.2" xref="S4.E4.m1.5.5.1.1.3.2.cmml">ℒ</mi><mtext id="S4.E4.m1.5.5.1.1.3.3" xref="S4.E4.m1.5.5.1.1.3.3a.cmml">dialog</mtext></msub><mo id="S4.E4.m1.5.5.1.1.2" xref="S4.E4.m1.5.5.1.1.2.cmml">=</mo><mrow id="S4.E4.m1.5.5.1.1.1" xref="S4.E4.m1.5.5.1.1.1.cmml"><mrow id="S4.E4.m1.5.5.1.1.1.3" xref="S4.E4.m1.5.5.1.1.1.3.cmml"><mstyle displaystyle="true" id="S4.E4.m1.5.5.1.1.1.3.1" xref="S4.E4.m1.5.5.1.1.1.3.1.cmml"><munderover id="S4.E4.m1.5.5.1.1.1.3.1a" xref="S4.E4.m1.5.5.1.1.1.3.1.cmml"><mo movablelimits="false" id="S4.E4.m1.5.5.1.1.1.3.1.2.2" xref="S4.E4.m1.5.5.1.1.1.3.1.2.2.cmml">∑</mo><mrow id="S4.E4.m1.5.5.1.1.1.3.1.2.3" xref="S4.E4.m1.5.5.1.1.1.3.1.2.3.cmml"><mi id="S4.E4.m1.5.5.1.1.1.3.1.2.3.2" xref="S4.E4.m1.5.5.1.1.1.3.1.2.3.2.cmml">k</mi><mo id="S4.E4.m1.5.5.1.1.1.3.1.2.3.1" xref="S4.E4.m1.5.5.1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S4.E4.m1.5.5.1.1.1.3.1.2.3.3" xref="S4.E4.m1.5.5.1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E4.m1.5.5.1.1.1.3.1.3" xref="S4.E4.m1.5.5.1.1.1.3.1.3.cmml">K</mi></munderover></mstyle><mstyle displaystyle="true" id="S4.E4.m1.5.5.1.1.1.3.2" xref="S4.E4.m1.5.5.1.1.1.3.2.cmml"><munderover id="S4.E4.m1.5.5.1.1.1.3.2a" xref="S4.E4.m1.5.5.1.1.1.3.2.cmml"><mo movablelimits="false" id="S4.E4.m1.5.5.1.1.1.3.2.2.2" xref="S4.E4.m1.5.5.1.1.1.3.2.2.2.cmml">∑</mo><mrow id="S4.E4.m1.5.5.1.1.1.3.2.2.3" xref="S4.E4.m1.5.5.1.1.1.3.2.2.3.cmml"><mi id="S4.E4.m1.5.5.1.1.1.3.2.2.3.2" xref="S4.E4.m1.5.5.1.1.1.3.2.2.3.2.cmml">n</mi><mo id="S4.E4.m1.5.5.1.1.1.3.2.2.3.1" xref="S4.E4.m1.5.5.1.1.1.3.2.2.3.1.cmml">=</mo><mn id="S4.E4.m1.5.5.1.1.1.3.2.2.3.3" xref="S4.E4.m1.5.5.1.1.1.3.2.2.3.3.cmml">1</mn></mrow><msub id="S4.E4.m1.5.5.1.1.1.3.2.3" xref="S4.E4.m1.5.5.1.1.1.3.2.3.cmml"><mi id="S4.E4.m1.5.5.1.1.1.3.2.3.2" xref="S4.E4.m1.5.5.1.1.1.3.2.3.2.cmml">N</mi><mi id="S4.E4.m1.5.5.1.1.1.3.2.3.3" xref="S4.E4.m1.5.5.1.1.1.3.2.3.3.cmml">k</mi></msub></munderover></mstyle></mrow><mo id="S4.E4.m1.5.5.1.1.1.2" xref="S4.E4.m1.5.5.1.1.1.2.cmml">−</mo><mrow id="S4.E4.m1.5.5.1.1.1.1" xref="S4.E4.m1.5.5.1.1.1.1.cmml"><mrow id="S4.E4.m1.5.5.1.1.1.1.3" xref="S4.E4.m1.5.5.1.1.1.1.3.cmml"><mi id="S4.E4.m1.5.5.1.1.1.1.3.1" xref="S4.E4.m1.5.5.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S4.E4.m1.5.5.1.1.1.1.3a" xref="S4.E4.m1.5.5.1.1.1.1.3.cmml">⁡</mo><mi id="S4.E4.m1.5.5.1.1.1.1.3.2" xref="S4.E4.m1.5.5.1.1.1.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S4.E4.m1.5.5.1.1.1.1.2" xref="S4.E4.m1.5.5.1.1.1.1.2.cmml">​</mo><mrow id="S4.E4.m1.5.5.1.1.1.1.1.1" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E4.m1.5.5.1.1.1.1.1.1.2" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.5.5.1.1.1.1.1.1.1" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.cmml"><msubsup id="S4.E4.m1.5.5.1.1.1.1.1.1.1.4" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.4.cmml"><mi id="S4.E4.m1.5.5.1.1.1.1.1.1.1.4.2.2" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.4.2.2.cmml">T</mi><mi id="S4.E4.m1.5.5.1.1.1.1.1.1.1.4.2.3" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.4.2.3.cmml">k</mi><mrow id="S4.E4.m1.2.2.2.2" xref="S4.E4.m1.2.2.2.3.cmml"><mrow id="S4.E4.m1.2.2.2.2.1" xref="S4.E4.m1.2.2.2.2.1.cmml"><mi id="S4.E4.m1.2.2.2.2.1.2" xref="S4.E4.m1.2.2.2.2.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.2.2.1.1" xref="S4.E4.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S4.E4.m1.2.2.2.2.1.3" xref="S4.E4.m1.2.2.2.2.1.3.cmml">i</mi></mrow><mo id="S4.E4.m1.2.2.2.2.2" xref="S4.E4.m1.2.2.2.3.cmml">,</mo><mi id="S4.E4.m1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.cmml">n</mi></mrow></msubsup><mo id="S4.E4.m1.5.5.1.1.1.1.1.1.1.3" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.3.cmml">∣</mo><mrow id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.3.cmml"><msubsup id="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.2.2.cmml">T</mi><mi id="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.2.3.cmml">k</mi><mrow id="S4.E4.m1.4.4.2.2" xref="S4.E4.m1.4.4.2.3.cmml"><mrow id="S4.E4.m1.3.3.1.1.1" xref="S4.E4.m1.3.3.1.1.1.cmml"><mi id="S4.E4.m1.3.3.1.1.1.2" xref="S4.E4.m1.3.3.1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.1.1.1.1" xref="S4.E4.m1.3.3.1.1.1.1.cmml">​</mo><mi id="S4.E4.m1.3.3.1.1.1.3" xref="S4.E4.m1.3.3.1.1.1.3.cmml">i</mi></mrow><mo id="S4.E4.m1.4.4.2.2.3" xref="S4.E4.m1.4.4.2.3.cmml">,</mo><mrow id="S4.E4.m1.4.4.2.2.2" xref="S4.E4.m1.4.4.2.2.2.cmml"><mi id="S4.E4.m1.4.4.2.2.2.2" xref="S4.E4.m1.4.4.2.2.2.2.cmml"></mi><mo id="S4.E4.m1.4.4.2.2.2.1" xref="S4.E4.m1.4.4.2.2.2.1.cmml">&lt;</mo><mi id="S4.E4.m1.4.4.2.2.2.3" xref="S4.E4.m1.4.4.2.2.2.3.cmml">n</mi></mrow></mrow></msubsup><mo id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.3" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.2" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.2.cmml">T</mi><mrow id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.2" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.2.cmml"></mi><mo id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.1" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.1.cmml">&lt;</mo><mi id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.3" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.3.cmml">k</mi></mrow></msub></mrow></mrow><mo stretchy="false" id="S4.E4.m1.5.5.1.1.1.1.1.1.3" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E4.m1.5.5.1.2" xref="S4.E4.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.5b"><apply id="S4.E4.m1.5.5.1.1.cmml" xref="S4.E4.m1.5.5.1"><eq id="S4.E4.m1.5.5.1.1.2.cmml" xref="S4.E4.m1.5.5.1.1.2"></eq><apply id="S4.E4.m1.5.5.1.1.3.cmml" xref="S4.E4.m1.5.5.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.1.1.3.1.cmml" xref="S4.E4.m1.5.5.1.1.3">subscript</csymbol><ci id="S4.E4.m1.5.5.1.1.3.2.cmml" xref="S4.E4.m1.5.5.1.1.3.2">ℒ</ci><ci id="S4.E4.m1.5.5.1.1.3.3a.cmml" xref="S4.E4.m1.5.5.1.1.3.3"><mtext mathsize="70%" id="S4.E4.m1.5.5.1.1.3.3.cmml" xref="S4.E4.m1.5.5.1.1.3.3">dialog</mtext></ci></apply><apply id="S4.E4.m1.5.5.1.1.1.cmml" xref="S4.E4.m1.5.5.1.1.1"><minus id="S4.E4.m1.5.5.1.1.1.2.cmml" xref="S4.E4.m1.5.5.1.1.1.2"></minus><apply id="S4.E4.m1.5.5.1.1.1.3.cmml" xref="S4.E4.m1.5.5.1.1.1.3"><apply id="S4.E4.m1.5.5.1.1.1.3.1.cmml" xref="S4.E4.m1.5.5.1.1.1.3.1"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.1.1.1.3.1.1.cmml" xref="S4.E4.m1.5.5.1.1.1.3.1">superscript</csymbol><apply id="S4.E4.m1.5.5.1.1.1.3.1.2.cmml" xref="S4.E4.m1.5.5.1.1.1.3.1"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.1.1.1.3.1.2.1.cmml" xref="S4.E4.m1.5.5.1.1.1.3.1">subscript</csymbol><sum id="S4.E4.m1.5.5.1.1.1.3.1.2.2.cmml" xref="S4.E4.m1.5.5.1.1.1.3.1.2.2"></sum><apply id="S4.E4.m1.5.5.1.1.1.3.1.2.3.cmml" xref="S4.E4.m1.5.5.1.1.1.3.1.2.3"><eq id="S4.E4.m1.5.5.1.1.1.3.1.2.3.1.cmml" xref="S4.E4.m1.5.5.1.1.1.3.1.2.3.1"></eq><ci id="S4.E4.m1.5.5.1.1.1.3.1.2.3.2.cmml" xref="S4.E4.m1.5.5.1.1.1.3.1.2.3.2">𝑘</ci><cn type="integer" id="S4.E4.m1.5.5.1.1.1.3.1.2.3.3.cmml" xref="S4.E4.m1.5.5.1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S4.E4.m1.5.5.1.1.1.3.1.3.cmml" xref="S4.E4.m1.5.5.1.1.1.3.1.3">𝐾</ci></apply><apply id="S4.E4.m1.5.5.1.1.1.3.2.cmml" xref="S4.E4.m1.5.5.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.1.1.1.3.2.1.cmml" xref="S4.E4.m1.5.5.1.1.1.3.2">superscript</csymbol><apply id="S4.E4.m1.5.5.1.1.1.3.2.2.cmml" xref="S4.E4.m1.5.5.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.1.1.1.3.2.2.1.cmml" xref="S4.E4.m1.5.5.1.1.1.3.2">subscript</csymbol><sum id="S4.E4.m1.5.5.1.1.1.3.2.2.2.cmml" xref="S4.E4.m1.5.5.1.1.1.3.2.2.2"></sum><apply id="S4.E4.m1.5.5.1.1.1.3.2.2.3.cmml" xref="S4.E4.m1.5.5.1.1.1.3.2.2.3"><eq id="S4.E4.m1.5.5.1.1.1.3.2.2.3.1.cmml" xref="S4.E4.m1.5.5.1.1.1.3.2.2.3.1"></eq><ci id="S4.E4.m1.5.5.1.1.1.3.2.2.3.2.cmml" xref="S4.E4.m1.5.5.1.1.1.3.2.2.3.2">𝑛</ci><cn type="integer" id="S4.E4.m1.5.5.1.1.1.3.2.2.3.3.cmml" xref="S4.E4.m1.5.5.1.1.1.3.2.2.3.3">1</cn></apply></apply><apply id="S4.E4.m1.5.5.1.1.1.3.2.3.cmml" xref="S4.E4.m1.5.5.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.1.1.1.3.2.3.1.cmml" xref="S4.E4.m1.5.5.1.1.1.3.2.3">subscript</csymbol><ci id="S4.E4.m1.5.5.1.1.1.3.2.3.2.cmml" xref="S4.E4.m1.5.5.1.1.1.3.2.3.2">𝑁</ci><ci id="S4.E4.m1.5.5.1.1.1.3.2.3.3.cmml" xref="S4.E4.m1.5.5.1.1.1.3.2.3.3">𝑘</ci></apply></apply></apply><apply id="S4.E4.m1.5.5.1.1.1.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1"><times id="S4.E4.m1.5.5.1.1.1.1.2.cmml" xref="S4.E4.m1.5.5.1.1.1.1.2"></times><apply id="S4.E4.m1.5.5.1.1.1.1.3.cmml" xref="S4.E4.m1.5.5.1.1.1.1.3"><log id="S4.E4.m1.5.5.1.1.1.1.3.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1.3.1"></log><ci id="S4.E4.m1.5.5.1.1.1.1.3.2.cmml" xref="S4.E4.m1.5.5.1.1.1.1.3.2">𝑝</ci></apply><apply id="S4.E4.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E4.m1.5.5.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S4.E4.m1.5.5.1.1.1.1.1.1.1.4.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.1.1.1.1.1.1.1.4.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.4">superscript</csymbol><apply id="S4.E4.m1.5.5.1.1.1.1.1.1.1.4.2.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.1.1.1.1.1.1.1.4.2.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S4.E4.m1.5.5.1.1.1.1.1.1.1.4.2.2.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.4.2.2">𝑇</ci><ci id="S4.E4.m1.5.5.1.1.1.1.1.1.1.4.2.3.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.4.2.3">𝑘</ci></apply><list id="S4.E4.m1.2.2.2.3.cmml" xref="S4.E4.m1.2.2.2.2"><apply id="S4.E4.m1.2.2.2.2.1.cmml" xref="S4.E4.m1.2.2.2.2.1"><times id="S4.E4.m1.2.2.2.2.1.1.cmml" xref="S4.E4.m1.2.2.2.2.1.1"></times><ci id="S4.E4.m1.2.2.2.2.1.2.cmml" xref="S4.E4.m1.2.2.2.2.1.2">𝑎</ci><ci id="S4.E4.m1.2.2.2.2.1.3.cmml" xref="S4.E4.m1.2.2.2.2.1.3">𝑖</ci></apply><ci id="S4.E4.m1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1">𝑛</ci></list></apply><list id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2"><apply id="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.2.2">𝑇</ci><ci id="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.1.1.1.2.3">𝑘</ci></apply><list id="S4.E4.m1.4.4.2.3.cmml" xref="S4.E4.m1.4.4.2.2"><apply id="S4.E4.m1.3.3.1.1.1.cmml" xref="S4.E4.m1.3.3.1.1.1"><times id="S4.E4.m1.3.3.1.1.1.1.cmml" xref="S4.E4.m1.3.3.1.1.1.1"></times><ci id="S4.E4.m1.3.3.1.1.1.2.cmml" xref="S4.E4.m1.3.3.1.1.1.2">𝑎</ci><ci id="S4.E4.m1.3.3.1.1.1.3.cmml" xref="S4.E4.m1.3.3.1.1.1.3">𝑖</ci></apply><apply id="S4.E4.m1.4.4.2.2.2.cmml" xref="S4.E4.m1.4.4.2.2.2"><lt id="S4.E4.m1.4.4.2.2.2.1.cmml" xref="S4.E4.m1.4.4.2.2.2.1"></lt><csymbol cd="latexml" id="S4.E4.m1.4.4.2.2.2.2.cmml" xref="S4.E4.m1.4.4.2.2.2.2">absent</csymbol><ci id="S4.E4.m1.4.4.2.2.2.3.cmml" xref="S4.E4.m1.4.4.2.2.2.3">𝑛</ci></apply></list></apply><apply id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.2">𝑇</ci><apply id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3"><lt id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.1"></lt><csymbol cd="latexml" id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.2">absent</csymbol><ci id="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S4.E4.m1.5.5.1.1.1.1.1.1.1.2.2.2.3.3">𝑘</ci></apply></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.5c">\displaystyle\mathcal{L}_{\text{dialog}}=\sum_{k=1}^{K}\sum_{n=1}^{N_{k}}-\log p(T_{k}^{ai,n}\mid T_{k}^{ai,&lt;n},T_{&lt;k}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p4.9" class="ltx_p">where <math id="S4.SS2.p4.5.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS2.p4.5.m1.1a"><mi id="S4.SS2.p4.5.m1.1.1" xref="S4.SS2.p4.5.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.5.m1.1b"><ci id="S4.SS2.p4.5.m1.1.1.cmml" xref="S4.SS2.p4.5.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.5.m1.1c">K</annotation></semantics></math> is the total number of rounds, <math id="S4.SS2.p4.6.m2.1" class="ltx_Math" alttext="N_{k}" display="inline"><semantics id="S4.SS2.p4.6.m2.1a"><msub id="S4.SS2.p4.6.m2.1.1" xref="S4.SS2.p4.6.m2.1.1.cmml"><mi id="S4.SS2.p4.6.m2.1.1.2" xref="S4.SS2.p4.6.m2.1.1.2.cmml">N</mi><mi id="S4.SS2.p4.6.m2.1.1.3" xref="S4.SS2.p4.6.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.6.m2.1b"><apply id="S4.SS2.p4.6.m2.1.1.cmml" xref="S4.SS2.p4.6.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.6.m2.1.1.1.cmml" xref="S4.SS2.p4.6.m2.1.1">subscript</csymbol><ci id="S4.SS2.p4.6.m2.1.1.2.cmml" xref="S4.SS2.p4.6.m2.1.1.2">𝑁</ci><ci id="S4.SS2.p4.6.m2.1.1.3.cmml" xref="S4.SS2.p4.6.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.6.m2.1c">N_{k}</annotation></semantics></math> is the number of tokens in the k-th round, <math id="S4.SS2.p4.7.m3.2" class="ltx_Math" alttext="T_{k}^{ai,n}" display="inline"><semantics id="S4.SS2.p4.7.m3.2a"><msubsup id="S4.SS2.p4.7.m3.2.3" xref="S4.SS2.p4.7.m3.2.3.cmml"><mi id="S4.SS2.p4.7.m3.2.3.2.2" xref="S4.SS2.p4.7.m3.2.3.2.2.cmml">T</mi><mi id="S4.SS2.p4.7.m3.2.3.2.3" xref="S4.SS2.p4.7.m3.2.3.2.3.cmml">k</mi><mrow id="S4.SS2.p4.7.m3.2.2.2.2" xref="S4.SS2.p4.7.m3.2.2.2.3.cmml"><mrow id="S4.SS2.p4.7.m3.2.2.2.2.1" xref="S4.SS2.p4.7.m3.2.2.2.2.1.cmml"><mi id="S4.SS2.p4.7.m3.2.2.2.2.1.2" xref="S4.SS2.p4.7.m3.2.2.2.2.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.7.m3.2.2.2.2.1.1" xref="S4.SS2.p4.7.m3.2.2.2.2.1.1.cmml">​</mo><mi id="S4.SS2.p4.7.m3.2.2.2.2.1.3" xref="S4.SS2.p4.7.m3.2.2.2.2.1.3.cmml">i</mi></mrow><mo id="S4.SS2.p4.7.m3.2.2.2.2.2" xref="S4.SS2.p4.7.m3.2.2.2.3.cmml">,</mo><mi id="S4.SS2.p4.7.m3.1.1.1.1" xref="S4.SS2.p4.7.m3.1.1.1.1.cmml">n</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.7.m3.2b"><apply id="S4.SS2.p4.7.m3.2.3.cmml" xref="S4.SS2.p4.7.m3.2.3"><csymbol cd="ambiguous" id="S4.SS2.p4.7.m3.2.3.1.cmml" xref="S4.SS2.p4.7.m3.2.3">superscript</csymbol><apply id="S4.SS2.p4.7.m3.2.3.2.cmml" xref="S4.SS2.p4.7.m3.2.3"><csymbol cd="ambiguous" id="S4.SS2.p4.7.m3.2.3.2.1.cmml" xref="S4.SS2.p4.7.m3.2.3">subscript</csymbol><ci id="S4.SS2.p4.7.m3.2.3.2.2.cmml" xref="S4.SS2.p4.7.m3.2.3.2.2">𝑇</ci><ci id="S4.SS2.p4.7.m3.2.3.2.3.cmml" xref="S4.SS2.p4.7.m3.2.3.2.3">𝑘</ci></apply><list id="S4.SS2.p4.7.m3.2.2.2.3.cmml" xref="S4.SS2.p4.7.m3.2.2.2.2"><apply id="S4.SS2.p4.7.m3.2.2.2.2.1.cmml" xref="S4.SS2.p4.7.m3.2.2.2.2.1"><times id="S4.SS2.p4.7.m3.2.2.2.2.1.1.cmml" xref="S4.SS2.p4.7.m3.2.2.2.2.1.1"></times><ci id="S4.SS2.p4.7.m3.2.2.2.2.1.2.cmml" xref="S4.SS2.p4.7.m3.2.2.2.2.1.2">𝑎</ci><ci id="S4.SS2.p4.7.m3.2.2.2.2.1.3.cmml" xref="S4.SS2.p4.7.m3.2.2.2.2.1.3">𝑖</ci></apply><ci id="S4.SS2.p4.7.m3.1.1.1.1.cmml" xref="S4.SS2.p4.7.m3.1.1.1.1">𝑛</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.7.m3.2c">T_{k}^{ai,n}</annotation></semantics></math> is the n-th token from the AI in the k-th round, <math id="S4.SS2.p4.8.m4.2" class="ltx_Math" alttext="T_{k}^{ai,&lt;n}" display="inline"><semantics id="S4.SS2.p4.8.m4.2a"><msubsup id="S4.SS2.p4.8.m4.2.3" xref="S4.SS2.p4.8.m4.2.3.cmml"><mi id="S4.SS2.p4.8.m4.2.3.2.2" xref="S4.SS2.p4.8.m4.2.3.2.2.cmml">T</mi><mi id="S4.SS2.p4.8.m4.2.3.2.3" xref="S4.SS2.p4.8.m4.2.3.2.3.cmml">k</mi><mrow id="S4.SS2.p4.8.m4.2.2.2.2" xref="S4.SS2.p4.8.m4.2.2.2.3.cmml"><mrow id="S4.SS2.p4.8.m4.1.1.1.1.1" xref="S4.SS2.p4.8.m4.1.1.1.1.1.cmml"><mi id="S4.SS2.p4.8.m4.1.1.1.1.1.2" xref="S4.SS2.p4.8.m4.1.1.1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p4.8.m4.1.1.1.1.1.1" xref="S4.SS2.p4.8.m4.1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS2.p4.8.m4.1.1.1.1.1.3" xref="S4.SS2.p4.8.m4.1.1.1.1.1.3.cmml">i</mi></mrow><mo id="S4.SS2.p4.8.m4.2.2.2.2.3" xref="S4.SS2.p4.8.m4.2.2.2.3.cmml">,</mo><mrow id="S4.SS2.p4.8.m4.2.2.2.2.2" xref="S4.SS2.p4.8.m4.2.2.2.2.2.cmml"><mi id="S4.SS2.p4.8.m4.2.2.2.2.2.2" xref="S4.SS2.p4.8.m4.2.2.2.2.2.2.cmml"></mi><mo id="S4.SS2.p4.8.m4.2.2.2.2.2.1" xref="S4.SS2.p4.8.m4.2.2.2.2.2.1.cmml">&lt;</mo><mi id="S4.SS2.p4.8.m4.2.2.2.2.2.3" xref="S4.SS2.p4.8.m4.2.2.2.2.2.3.cmml">n</mi></mrow></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.8.m4.2b"><apply id="S4.SS2.p4.8.m4.2.3.cmml" xref="S4.SS2.p4.8.m4.2.3"><csymbol cd="ambiguous" id="S4.SS2.p4.8.m4.2.3.1.cmml" xref="S4.SS2.p4.8.m4.2.3">superscript</csymbol><apply id="S4.SS2.p4.8.m4.2.3.2.cmml" xref="S4.SS2.p4.8.m4.2.3"><csymbol cd="ambiguous" id="S4.SS2.p4.8.m4.2.3.2.1.cmml" xref="S4.SS2.p4.8.m4.2.3">subscript</csymbol><ci id="S4.SS2.p4.8.m4.2.3.2.2.cmml" xref="S4.SS2.p4.8.m4.2.3.2.2">𝑇</ci><ci id="S4.SS2.p4.8.m4.2.3.2.3.cmml" xref="S4.SS2.p4.8.m4.2.3.2.3">𝑘</ci></apply><list id="S4.SS2.p4.8.m4.2.2.2.3.cmml" xref="S4.SS2.p4.8.m4.2.2.2.2"><apply id="S4.SS2.p4.8.m4.1.1.1.1.1.cmml" xref="S4.SS2.p4.8.m4.1.1.1.1.1"><times id="S4.SS2.p4.8.m4.1.1.1.1.1.1.cmml" xref="S4.SS2.p4.8.m4.1.1.1.1.1.1"></times><ci id="S4.SS2.p4.8.m4.1.1.1.1.1.2.cmml" xref="S4.SS2.p4.8.m4.1.1.1.1.1.2">𝑎</ci><ci id="S4.SS2.p4.8.m4.1.1.1.1.1.3.cmml" xref="S4.SS2.p4.8.m4.1.1.1.1.1.3">𝑖</ci></apply><apply id="S4.SS2.p4.8.m4.2.2.2.2.2.cmml" xref="S4.SS2.p4.8.m4.2.2.2.2.2"><lt id="S4.SS2.p4.8.m4.2.2.2.2.2.1.cmml" xref="S4.SS2.p4.8.m4.2.2.2.2.2.1"></lt><csymbol cd="latexml" id="S4.SS2.p4.8.m4.2.2.2.2.2.2.cmml" xref="S4.SS2.p4.8.m4.2.2.2.2.2.2">absent</csymbol><ci id="S4.SS2.p4.8.m4.2.2.2.2.2.3.cmml" xref="S4.SS2.p4.8.m4.2.2.2.2.2.3">𝑛</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.8.m4.2c">T_{k}^{ai,&lt;n}</annotation></semantics></math> denotes all previous tokens from the AI within the same round k, and <math id="S4.SS2.p4.9.m5.1" class="ltx_Math" alttext="T_{&lt;k}" display="inline"><semantics id="S4.SS2.p4.9.m5.1a"><msub id="S4.SS2.p4.9.m5.1.1" xref="S4.SS2.p4.9.m5.1.1.cmml"><mi id="S4.SS2.p4.9.m5.1.1.2" xref="S4.SS2.p4.9.m5.1.1.2.cmml">T</mi><mrow id="S4.SS2.p4.9.m5.1.1.3" xref="S4.SS2.p4.9.m5.1.1.3.cmml"><mi id="S4.SS2.p4.9.m5.1.1.3.2" xref="S4.SS2.p4.9.m5.1.1.3.2.cmml"></mi><mo id="S4.SS2.p4.9.m5.1.1.3.1" xref="S4.SS2.p4.9.m5.1.1.3.1.cmml">&lt;</mo><mi id="S4.SS2.p4.9.m5.1.1.3.3" xref="S4.SS2.p4.9.m5.1.1.3.3.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.9.m5.1b"><apply id="S4.SS2.p4.9.m5.1.1.cmml" xref="S4.SS2.p4.9.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.9.m5.1.1.1.cmml" xref="S4.SS2.p4.9.m5.1.1">subscript</csymbol><ci id="S4.SS2.p4.9.m5.1.1.2.cmml" xref="S4.SS2.p4.9.m5.1.1.2">𝑇</ci><apply id="S4.SS2.p4.9.m5.1.1.3.cmml" xref="S4.SS2.p4.9.m5.1.1.3"><lt id="S4.SS2.p4.9.m5.1.1.3.1.cmml" xref="S4.SS2.p4.9.m5.1.1.3.1"></lt><csymbol cd="latexml" id="S4.SS2.p4.9.m5.1.1.3.2.cmml" xref="S4.SS2.p4.9.m5.1.1.3.2">absent</csymbol><ci id="S4.SS2.p4.9.m5.1.1.3.3.cmml" xref="S4.SS2.p4.9.m5.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.9.m5.1c">T_{&lt;k}</annotation></semantics></math> is all prior tokens in previous rounds. Note that we dropped the prefix tokens in the equation for brevity. During the pretraining, we utilize a balanced mix of the AV speech tokens and text which allows the model to utilize both token knowledge to generate dialogue response as in Fig. <a href="#S4.F2" title="Figure 2 ‣ 4.2 Audio-Visual Spoken Dialogue Language Modeling ‣ 4 Audio-Visual Spoken Dialogue System ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(c). Then, we later finetune on pure AV speech token-based dialogue as in Fig. <a href="#S4.F2" title="Figure 2 ‣ 4.2 Audio-Visual Spoken Dialogue Language Modeling ‣ 4 Audio-Visual Spoken Dialogue System ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(d) for real-time face-to-face interaction. This progressive shift helps the model to gradually adapt to AV speech tokens without compromising the quality of dialogue generation of the text-based LLM.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<p id="S4.F2.1.1" class="ltx_p ltx_minipage ltx_align_bottom" style="width:433.6pt;"><span id="S4.F2.1.1.1" class="ltx_text"><img src="/html/2406.07867/assets/x2.png" id="S4.F2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="242" height="218" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Constructed data based on the MultiDialog dataset used for training the audio-visual speech dialogue model. (a-c) are joint pretraining of the audio-visual speech and text tokens and (d) is used to finetune the model.</figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure">
<p id="S4.F3.1.1" class="ltx_p ltx_minipage ltx_align_bottom" style="width:433.6pt;"><span id="S4.F3.1.1.1" class="ltx_text"><img src="/html/2406.07867/assets/x3.png" id="S4.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="242" height="163" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Evaluation prompt of multimodal dialogue language modeling. It is written in text for illustration but the actual prompt is given as audio and visual. </figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Audio-Visual Generation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The generated AV speech tokens are projected to audio and visual to generate the response as a talking face video. As shown in Fig. <a href="#S3.F1" title="Figure 1 ‣ 3.3 Post-Processing ‣ 3 MultiDialog Dataset ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the audio-visual generator consists of a length predictor, a token-based speech decoder, and a token-based face decoder. Since our language model is trained with duplicate reduced AV speech tokens, we train a length predictor to first restore them back to their original length. The token-based speech decoder and token-based face decoder are adapted from an off-the-shelf audio generator <cite class="ltx_cite ltx_citemacro_cite">Kong et al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite> and a talking face generator <cite class="ltx_cite ltx_citemacro_cite">Prajwal et al. (<a href="#bib.bib54" title="" class="ltx_ref">2020</a>)</cite> respectively, where we train them to process AV speech tokens as the input instead of raw audio. Additionally, we incorporate speaker identity information by extracting the speaker embedding <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite> from a target identity sample audio. Also, the target identity’s face and pose prior are utilized as in <cite class="ltx_cite ltx_citemacro_cite">Prajwal et al. (<a href="#bib.bib54" title="" class="ltx_ref">2020</a>)</cite>, to enable the generation of talking face video with desired identity.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluation Metrics</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We evaluate the semantic quality and the generation quality of both audio and video. For the semantic quality, we first generate transcriptions from the synthesized audio-visual output using an off-the-shelf ASR model <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a href="#bib.bib59" title="" class="ltx_ref">2021</a>)</cite>, and employ standard metrics used for text-based dialogue generation: log-perplexity (PPL), BLEU, METEOR, F1, D-1, and D-2. The log-perplexity is calculated using Dialo-GPT model <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib72" title="" class="ltx_ref">2019</a>)</cite> and it is calculated for each utterance and averaged across the test set. To measure the generation quality of video, we adopt metrics used for TFG. This includes Fréchet Inception Distance (FID) <cite class="ltx_cite ltx_citemacro_cite">Heusel et al. (<a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite> to measure visual quality, and LSE-C and LSE-D <cite class="ltx_cite ltx_citemacro_cite">Prajwal et al. (<a href="#bib.bib54" title="" class="ltx_ref">2020</a>)</cite> to measure the audio-visual synchronization. To evaluate the acoustic quality, we compute speaker similarity (SIM) between the given target sample and generated speech using the WavLM-Base model for speaker verification <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>. Please refer to the appendices for a detailed explanation of each metric.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<div id="S5.T3.10" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:429.3pt;height:195.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-81.6pt,37.2pt) scale(0.724549152343851,0.724549152343851) ;">
<table id="S5.T3.10.10" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.10.10.11" class="ltx_tr">
<td id="S5.T3.10.10.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;" rowspan="2">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
<span id="S5.T3.10.10.11.1.1" class="ltx_text"><span id="S5.T3.10.10.11.1.1.1" class="ltx_text"></span> <span id="S5.T3.10.10.11.1.1.2" class="ltx_text">
<span id="S5.T3.10.10.11.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.10.10.11.1.1.2.1.1" class="ltx_tr">
<span id="S5.T3.10.10.11.1.1.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S5.T3.10.10.11.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></span></span>
</span></span> <span id="S5.T3.10.10.11.1.1.3" class="ltx_text"></span></span>
</td>
<td id="S5.T3.10.10.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;" rowspan="2"><span id="S5.T3.10.10.11.2.1" class="ltx_text"><span id="S5.T3.10.10.11.2.1.1" class="ltx_text"></span> <span id="S5.T3.10.10.11.2.1.2" class="ltx_text">
<span id="S5.T3.10.10.11.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.10.10.11.2.1.2.1.1" class="ltx_tr">
<span id="S5.T3.10.10.11.2.1.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S5.T3.10.10.11.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Input</span></span></span>
<span id="S5.T3.10.10.11.2.1.2.1.2" class="ltx_tr">
<span id="S5.T3.10.10.11.2.1.2.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S5.T3.10.10.11.2.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Modality</span></span></span>
</span></span> <span id="S5.T3.10.10.11.2.1.3" class="ltx_text"></span></span></td>
<td id="S5.T3.10.10.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;" rowspan="2"><span id="S5.T3.10.10.11.3.1" class="ltx_text"><span id="S5.T3.10.10.11.3.1.1" class="ltx_text"></span> <span id="S5.T3.10.10.11.3.1.2" class="ltx_text">
<span id="S5.T3.10.10.11.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.10.10.11.3.1.2.1.1" class="ltx_tr">
<span id="S5.T3.10.10.11.3.1.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S5.T3.10.10.11.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Output</span></span></span>
<span id="S5.T3.10.10.11.3.1.2.1.2" class="ltx_tr">
<span id="S5.T3.10.10.11.3.1.2.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S5.T3.10.10.11.3.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Modality</span></span></span>
</span></span> <span id="S5.T3.10.10.11.3.1.3" class="ltx_text"></span></span></td>
<td id="S5.T3.10.10.11.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding:1.5pt 0.0pt;" colspan="6"><span id="S5.T3.10.10.11.4.1" class="ltx_text ltx_font_bold">Semantic Evaluation</span></td>
</tr>
<tr id="S5.T3.6.6.6" class="ltx_tr">
<td id="S5.T3.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">
<span id="S5.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">PPL</span> <math id="S5.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T3.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">
<span id="S5.T3.2.2.2.2.1" class="ltx_text ltx_font_bold">BLEU</span> <math id="S5.T3.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.2.2.2.2.m1.1a"><mo stretchy="false" id="S5.T3.2.2.2.2.m1.1.1" xref="S5.T3.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.2.m1.1b"><ci id="S5.T3.2.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T3.3.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">
<span id="S5.T3.3.3.3.3.1" class="ltx_text ltx_font_bold">METEOR</span> <math id="S5.T3.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.3.3.3.3.m1.1a"><mo stretchy="false" id="S5.T3.3.3.3.3.m1.1.1" xref="S5.T3.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.3.m1.1b"><ci id="S5.T3.3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T3.4.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">
<span id="S5.T3.4.4.4.4.1" class="ltx_text ltx_font_bold">F1</span> <math id="S5.T3.4.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.4.4.4.4.m1.1a"><mo stretchy="false" id="S5.T3.4.4.4.4.m1.1.1" xref="S5.T3.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.4.m1.1b"><ci id="S5.T3.4.4.4.4.m1.1.1.cmml" xref="S5.T3.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T3.5.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">
<span id="S5.T3.5.5.5.5.1" class="ltx_text ltx_font_bold">D-1 </span> <math id="S5.T3.5.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.5.5.5.5.m1.1a"><mo stretchy="false" id="S5.T3.5.5.5.5.m1.1.1" xref="S5.T3.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.5.m1.1b"><ci id="S5.T3.5.5.5.5.m1.1.1.cmml" xref="S5.T3.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T3.6.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">
<span id="S5.T3.6.6.6.6.1" class="ltx_text ltx_font_bold">D-2</span> <math id="S5.T3.6.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.6.6.6.6.m1.1a"><mo stretchy="false" id="S5.T3.6.6.6.6.m1.1.1" xref="S5.T3.6.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.6.6.m1.1b"><ci id="S5.T3.6.6.6.6.m1.1.1.cmml" xref="S5.T3.6.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.6.6.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T3.7.7.7" class="ltx_tr">
<td id="S5.T3.7.7.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1.5pt 0.0pt;" colspan="4">
<math id="S5.T3.7.7.7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T3.7.7.7.1.m1.1a"><mo id="S5.T3.7.7.7.1.m1.1.1" xref="S5.T3.7.7.7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T3.7.7.7.1.m1.1b"><ci id="S5.T3.7.7.7.1.m1.1.1.cmml" xref="S5.T3.7.7.7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.7.7.7.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.T3.7.7.7.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Ground Truth</span>
</td>
<td id="S5.T3.7.7.7.2" class="ltx_td ltx_border_t" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.7.7.7.3" class="ltx_td ltx_border_t" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.7.7.7.4" class="ltx_td ltx_border_t" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.7.7.7.5" class="ltx_td ltx_border_t" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.7.7.7.6" class="ltx_td ltx_border_t" style="padding:1.5pt 0.0pt;"></td>
</tr>
<tr id="S5.T3.10.10.12" class="ltx_tr">
<td id="S5.T3.10.10.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;">GT AV Speech Token</td>
<td id="S5.T3.10.10.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">–</td>
<td id="S5.T3.10.10.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">–</td>
<td id="S5.T3.10.10.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">1054.643</td>
<td id="S5.T3.10.10.12.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">76.326</td>
<td id="S5.T3.10.10.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.565</td>
<td id="S5.T3.10.10.12.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.474</td>
<td id="S5.T3.10.10.12.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.947</td>
<td id="S5.T3.10.10.12.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.996</td>
</tr>
<tr id="S5.T3.8.8.8" class="ltx_tr">
<td id="S5.T3.8.8.8.1" class="ltx_td ltx_align_left" style="padding:1.5pt 0.0pt;" colspan="4">
<math id="S5.T3.8.8.8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T3.8.8.8.1.m1.1a"><mo id="S5.T3.8.8.8.1.m1.1.1" xref="S5.T3.8.8.8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.8.1.m1.1b"><ci id="S5.T3.8.8.8.1.m1.1.1.cmml" xref="S5.T3.8.8.8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.8.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.T3.8.8.8.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Cascaded System</span>
</td>
<td id="S5.T3.8.8.8.2" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.8.8.8.3" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.8.8.8.4" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.8.8.8.5" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.8.8.8.6" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
</tr>
<tr id="S5.T3.10.10.13" class="ltx_tr">
<td id="S5.T3.10.10.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;">AVSR + LM + TTS + TFG</td>
<td id="S5.T3.10.10.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">AV</td>
<td id="S5.T3.10.10.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">AV</td>
<td id="S5.T3.10.10.13.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">1157.586</td>
<td id="S5.T3.10.10.13.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">47.287</td>
<td id="S5.T3.10.10.13.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.075</td>
<td id="S5.T3.10.10.13.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.100</td>
<td id="S5.T3.10.10.13.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.959</td>
<td id="S5.T3.10.10.13.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.977</td>
</tr>
<tr id="S5.T3.9.9.9" class="ltx_tr">
<td id="S5.T3.9.9.9.1" class="ltx_td ltx_align_left" style="padding:1.5pt 0.0pt;" colspan="4">
<math id="S5.T3.9.9.9.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T3.9.9.9.1.m1.1a"><mo id="S5.T3.9.9.9.1.m1.1.1" xref="S5.T3.9.9.9.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T3.9.9.9.1.m1.1b"><ci id="S5.T3.9.9.9.1.m1.1.1.cmml" xref="S5.T3.9.9.9.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.9.9.9.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.T3.9.9.9.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Spoken Dialogue System</span>
</td>
<td id="S5.T3.9.9.9.2" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.9.9.9.3" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.9.9.9.4" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.9.9.9.5" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.9.9.9.6" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
</tr>
<tr id="S5.T3.10.10.14" class="ltx_tr">
<td id="S5.T3.10.10.14.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;">SpeechGPT <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S5.T3.10.10.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">A</td>
<td id="S5.T3.10.10.14.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">A</td>
<td id="S5.T3.10.10.14.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">930.401</td>
<td id="S5.T3.10.10.14.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">20.536</td>
<td id="S5.T3.10.10.14.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.064</td>
<td id="S5.T3.10.10.14.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.054</td>
<td id="S5.T3.10.10.14.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.743</td>
<td id="S5.T3.10.10.14.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.876</td>
</tr>
<tr id="S5.T3.10.10.15" class="ltx_tr">
<td id="S5.T3.10.10.15.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;">d-GSLM <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib48" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S5.T3.10.10.15.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">A</td>
<td id="S5.T3.10.10.15.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">A</td>
<td id="S5.T3.10.10.15.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">1085.265</td>
<td id="S5.T3.10.10.15.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">8.197</td>
<td id="S5.T3.10.10.15.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.065</td>
<td id="S5.T3.10.10.15.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.064</td>
<td id="S5.T3.10.10.15.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.883</td>
<td id="S5.T3.10.10.15.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.876</td>
</tr>
<tr id="S5.T3.10.10.10" class="ltx_tr">
<td id="S5.T3.10.10.10.1" class="ltx_td ltx_nopad_l ltx_align_left" style="padding:1.5pt 0.0pt;" colspan="4">
<span id="S5.T3.10.10.10.1.1" class="ltx_ERROR undefined">\hdashline</span>  <math id="S5.T3.10.10.10.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.T3.10.10.10.1.m1.1a"><mo id="S5.T3.10.10.10.1.m1.1.1" xref="S5.T3.10.10.10.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.T3.10.10.10.1.m1.1b"><ci id="S5.T3.10.10.10.1.m1.1.1.cmml" xref="S5.T3.10.10.10.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.10.10.10.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.T3.10.10.10.1.2" class="ltx_text ltx_font_bold ltx_font_italic">Audio-Visual Spoken Dialogue System</span>
</td>
<td id="S5.T3.10.10.10.2" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.10.10.10.3" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.10.10.10.4" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.10.10.10.5" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.10.10.10.6" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
</tr>
<tr id="S5.T3.10.10.16" class="ltx_tr">
<td id="S5.T3.10.10.16.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;">Scratch</td>
<td id="S5.T3.10.10.16.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">AV</td>
<td id="S5.T3.10.10.16.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">AV</td>
<td id="S5.T3.10.10.16.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">1898.864</td>
<td id="S5.T3.10.10.16.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">13.305</td>
<td id="S5.T3.10.10.16.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.058</td>
<td id="S5.T3.10.10.16.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.064</td>
<td id="S5.T3.10.10.16.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.945</td>
<td id="S5.T3.10.10.16.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.955</td>
</tr>
<tr id="S5.T3.10.10.17" class="ltx_tr">
<td id="S5.T3.10.10.17.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;">+ LLM initialized</td>
<td id="S5.T3.10.10.17.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">AV</td>
<td id="S5.T3.10.10.17.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">AV</td>
<td id="S5.T3.10.10.17.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">1237.757</td>
<td id="S5.T3.10.10.17.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">17.098</td>
<td id="S5.T3.10.10.17.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.059</td>
<td id="S5.T3.10.10.17.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.058</td>
<td id="S5.T3.10.10.17.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.936</td>
<td id="S5.T3.10.10.17.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.963</td>
</tr>
<tr id="S5.T3.10.10.18" class="ltx_tr">
<td id="S5.T3.10.10.18.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;">+ AVSR/TTS Pretraining</td>
<td id="S5.T3.10.10.18.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">AV</td>
<td id="S5.T3.10.10.18.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">AV</td>
<td id="S5.T3.10.10.18.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">1068.904</td>
<td id="S5.T3.10.10.18.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">22.090</td>
<td id="S5.T3.10.10.18.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.062</td>
<td id="S5.T3.10.10.18.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.066</td>
<td id="S5.T3.10.10.18.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.943</td>
<td id="S5.T3.10.10.18.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.965</td>
</tr>
<tr id="S5.T3.10.10.19" class="ltx_tr">
<td id="S5.T3.10.10.19.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;">+ Mixed Text-AV Speech Pretraining</td>
<td id="S5.T3.10.10.19.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">AV</td>
<td id="S5.T3.10.10.19.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">AV</td>
<td id="S5.T3.10.10.19.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">1248.001</td>
<td id="S5.T3.10.10.19.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">24.094</td>
<td id="S5.T3.10.10.19.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.063</td>
<td id="S5.T3.10.10.19.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.065</td>
<td id="S5.T3.10.10.19.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.945</td>
<td id="S5.T3.10.10.19.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.957</td>
</tr>
<tr id="S5.T3.10.10.20" class="ltx_tr">
<td id="S5.T3.10.10.20.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S5.T3.10.10.20.2" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.10.10.20.3" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.10.10.20.4" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.10.10.20.5" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.10.10.20.6" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.10.10.20.7" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.10.10.20.8" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S5.T3.10.10.20.9" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of the semantic quality between state-of-the-art spoken dialogue systems in MultiDialog. Note that our proposed method is the only method that supports both audio and visual at the input and output of the dialogue system without relying on intermediate text. </figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Implementation Details</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">To encode AV speech tokens, we crop the video into the mouth region of size 96<math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mo id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><times id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\times</annotation></semantics></math>96 using a face detector <cite class="ltx_cite ltx_citemacro_cite">Deng et al. (<a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite> and a facial landmark detector <cite class="ltx_cite ltx_citemacro_cite">Bulat and Tzimiropoulos (<a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite>, and resample the audio to 16kHz. We take English-trained AV-HuBERT <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a href="#bib.bib59" title="" class="ltx_ref">2021</a>)</cite> and finetune it to predict corresponding target clusters from HuBERT tokenizer <cite class="ltx_cite ltx_citemacro_cite">Hassid et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> which operates at 25Hz with 500 clusters. We train it for 100k steps on 6 A6000 GPUs with a maximum token length of 2,000.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">We initialize the model with a pre-trained language model, OPT-1.3B <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib70" title="" class="ltx_ref">2022</a>)</cite>. We first pretrain the input embedding layer and the projection layer on AVSR and TTS objectives for 200K steps. Then, we continue training the entire model on a mixture of text and AV speech token dialogue for 5K steps, followed by finetuning for additional 3K steps on AV speech token dialogue only. We use a max token length of 700 on 4 A6000 GPUs.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">The audio-visual generator is trained using ground truth AV speech tokens. The token-based speech decoder and length predictor are jointly trained for 450K steps with a batch size of 32. For training the AV token-based face decoder, we employ the reprogramming strategy in <cite class="ltx_cite ltx_citemacro_cite">Choi et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> and train an adapter layer consisting of two layers of transformer encoder to bridge between the AV speech tokens and the corresponding audio features of the TFG model <cite class="ltx_cite ltx_citemacro_cite">Prajwal et al. (<a href="#bib.bib54" title="" class="ltx_ref">2020</a>)</cite>. This allows to leverage the face generation capabilities of the pretrained TFG model without further finetuning the generator and can be applied to any other TFG models. It is trained for 250K steps with a batch size of 256. We additionally incorporate a face enhancer <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib66" title="" class="ltx_ref">2021b</a>)</cite> to upsample the generated face video into high resolution.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Baselines</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Since there is no previous method that can directly perform audio-visual spoken dialogue synthesis, we compare with the recently proposed spoken dialogue systems, Speech-GPT <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2023a</a>)</cite> and d-GSLM <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib48" title="" class="ltx_ref">2023b</a>)</cite>. They support only audio speech at both input and output. Additionally, we build a cascade system by integrating a series of off-the-shelf pre-trained models: AVSR <cite class="ltx_cite ltx_citemacro_cite">Anwar et al. (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>, LM <cite class="ltx_cite ltx_citemacro_cite">Tang et al. (<a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite>, TTS <cite class="ltx_cite ltx_citemacro_cite">Casanova et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>, and TFG <cite class="ltx_cite ltx_citemacro_cite">Prajwal et al. (<a href="#bib.bib54" title="" class="ltx_ref">2020</a>)</cite>. Please note the objective of the comparisons with the cascaded method is not to achieve state-of-the-art performance, but rather to assess the extent to which the performance of the proposed system can be attained through the direct strategy. For a fair comparison, we finetune SpeechGPT and d-GSLM on our MultiDialog dataset and we use a dialogue language model <cite class="ltx_cite ltx_citemacro_cite">Tang et al. (<a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite> trained on TopicalChat as the LM of the cascade system.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Semantic Evaluation</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">To accurately assess the semantic quality of the generated response, we employ the evaluation strategy used for text-based dialogue language models. We conduct evaluations on the test set of MultiDialog, where the model is prompted to sequentially generate a response for each turn in the conversations. Sample evaluation prompts are illustrated in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Audio-Visual Spoken Dialogue Language Modeling ‣ 4 Audio-Visual Spoken Dialogue System ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The generated response is then transcribed into text and compared against the ground truth response to evaluate its semantic quality. As shown in Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Evaluation Metrics ‣ 5 Experimental Setup ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, compared with the state-of-the-art spoken dialogue systems, SpeechGPT <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2023a</a>)</cite> and d-GSLM <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib48" title="" class="ltx_ref">2023b</a>)</cite>, our proposed method performs the best in BLEU, D-1, and D-2 which demonstrates that our method can generate contextually coherent and diverse response. SpeechGPT has the highest PPL because it is trained on an extensive amount of speech data and PEFT-finetuned <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite> on the MultiDialog, which allows it to generate more fluent speech but fails to match with the reference response as indicated by the lower BLEU score. Also, it requires generating text transcription of the input to generate the response in text first. Notably, our proposed method stands as the first approach to directly recognize and generate response in both audio and visual speech video, without requiring intermediate text generation.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Ablation on the Pretraining Scheme</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">We analyze the pretraining scheme used for our audio-visual spoken dialogue model in the lower section of Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Evaluation Metrics ‣ 5 Experimental Setup ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The results demonstrate that initializing the model with a textually pretrained LLM yields improved semantic quality, which is further enhanced by AVSR/TTS pretraining. Simply training the embedding layer and projection layer to predict corresponding AV speech tokens and text tokens improves the response. When further incorporating mixed text-AV speech token pretraining, we observe an overall enhancement in semantic quality, validating the effectiveness of gradually adapting the AV speech tokens to the LLM. Yet, there is a slight decrease in the PPL score, which we attribute to the model’s increased complexity and adaptability to multimodal inputs.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Audio and Visual Evaluation</h3>

<figure id="S6.T4" class="ltx_table">
<div id="S6.T4.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:429.3pt;height:162.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.6pt,-0.2pt) scale(1.00258024460954,1.00258024460954) ;">
<table id="S6.T4.7.7" class="ltx_tabular ltx_align_middle">
<tr id="S6.T4.4.4.4" class="ltx_tr">
<td id="S6.T4.4.4.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
<span id="S6.T4.4.4.4.5.1" class="ltx_text ltx_font_bold">Method</span>
</td>
<td id="S6.T4.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">FID <math id="S6.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T4.1.1.1.1.m1.1a"><mo stretchy="false" id="S6.T4.1.1.1.1.m1.1.1" xref="S6.T4.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.1.m1.1b"><ci id="S6.T4.1.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S6.T4.2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">LSE-C <math id="S6.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T4.2.2.2.2.m1.1a"><mo stretchy="false" id="S6.T4.2.2.2.2.m1.1.1" xref="S6.T4.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.2.m1.1b"><ci id="S6.T4.2.2.2.2.m1.1.1.cmml" xref="S6.T4.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S6.T4.3.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">LSE-D <math id="S6.T4.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S6.T4.3.3.3.3.m1.1a"><mo stretchy="false" id="S6.T4.3.3.3.3.m1.1.1" xref="S6.T4.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S6.T4.3.3.3.3.m1.1b"><ci id="S6.T4.3.3.3.3.m1.1.1.cmml" xref="S6.T4.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S6.T4.4.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">SIM <math id="S6.T4.4.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T4.4.4.4.4.m1.1a"><mo stretchy="false" id="S6.T4.4.4.4.4.m1.1.1" xref="S6.T4.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T4.4.4.4.4.m1.1b"><ci id="S6.T4.4.4.4.4.m1.1.1.cmml" xref="S6.T4.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.4.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S6.T4.5.5.5" class="ltx_tr">
<td id="S6.T4.5.5.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1.5pt 0.0pt;" colspan="3">
<math id="S6.T4.5.5.5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.T4.5.5.5.1.m1.1a"><mo id="S6.T4.5.5.5.1.m1.1.1" xref="S6.T4.5.5.5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.T4.5.5.5.1.m1.1b"><ci id="S6.T4.5.5.5.1.m1.1.1.cmml" xref="S6.T4.5.5.5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.5.5.5.1.m1.1c">\bullet</annotation></semantics></math> <span id="S6.T4.5.5.5.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Cascade System</span>
</td>
<td id="S6.T4.5.5.5.2" class="ltx_td ltx_border_t" style="padding:1.5pt 0.0pt;"></td>
<td id="S6.T4.5.5.5.3" class="ltx_td ltx_border_t" style="padding:1.5pt 0.0pt;"></td>
</tr>
<tr id="S6.T4.7.7.8" class="ltx_tr">
<td id="S6.T4.7.7.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;">AVSR + LM + TTS + TFG</td>
<td id="S6.T4.7.7.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">30.581</td>
<td id="S6.T4.7.7.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">7.041</td>
<td id="S6.T4.7.7.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">7.640</td>
<td id="S6.T4.7.7.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.433</td>
</tr>
<tr id="S6.T4.6.6.6" class="ltx_tr">
<td id="S6.T4.6.6.6.1" class="ltx_td ltx_nopad_l ltx_align_left" style="padding:1.5pt 0.0pt;" colspan="3">
<span id="S6.T4.6.6.6.1.1" class="ltx_ERROR undefined">\hdashline</span>  <math id="S6.T4.6.6.6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.T4.6.6.6.1.m1.1a"><mo id="S6.T4.6.6.6.1.m1.1.1" xref="S6.T4.6.6.6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.T4.6.6.6.1.m1.1b"><ci id="S6.T4.6.6.6.1.m1.1.1.cmml" xref="S6.T4.6.6.6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.6.6.6.1.m1.1c">\bullet</annotation></semantics></math> <span id="S6.T4.6.6.6.1.2" class="ltx_text ltx_font_bold ltx_font_italic">Spoken Dialogue System</span>
</td>
<td id="S6.T4.6.6.6.2" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S6.T4.6.6.6.3" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
</tr>
<tr id="S6.T4.7.7.9" class="ltx_tr">
<td id="S6.T4.7.7.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;">SpeechGPT <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib68" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S6.T4.7.7.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">-</td>
<td id="S6.T4.7.7.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">-</td>
<td id="S6.T4.7.7.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">-</td>
<td id="S6.T4.7.7.9.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.194</td>
</tr>
<tr id="S6.T4.7.7.10" class="ltx_tr">
<td id="S6.T4.7.7.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;">d-GSLM <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib48" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S6.T4.7.7.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">-</td>
<td id="S6.T4.7.7.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">-</td>
<td id="S6.T4.7.7.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">-</td>
<td id="S6.T4.7.7.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.211</td>
</tr>
<tr id="S6.T4.7.7.7" class="ltx_tr">
<td id="S6.T4.7.7.7.1" class="ltx_td ltx_nopad_l ltx_align_left" style="padding:1.5pt 0.0pt;" colspan="3">
<span id="S6.T4.7.7.7.1.1" class="ltx_ERROR undefined">\hdashline</span>  <math id="S6.T4.7.7.7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S6.T4.7.7.7.1.m1.1a"><mo id="S6.T4.7.7.7.1.m1.1.1" xref="S6.T4.7.7.7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S6.T4.7.7.7.1.m1.1b"><ci id="S6.T4.7.7.7.1.m1.1.1.cmml" xref="S6.T4.7.7.7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.7.7.7.1.m1.1c">\bullet</annotation></semantics></math> <span id="S6.T4.7.7.7.1.2" class="ltx_text ltx_font_bold ltx_font_italic">Audio-Visual Spoken Dialogue System</span>
</td>
<td id="S6.T4.7.7.7.2" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S6.T4.7.7.7.3" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
</tr>
<tr id="S6.T4.7.7.11" class="ltx_tr">
<td id="S6.T4.7.7.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;"><span id="S6.T4.7.7.11.1.1" class="ltx_text ltx_font_bold">Proposed</span></td>
<td id="S6.T4.7.7.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">30.323</td>
<td id="S6.T4.7.7.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">7.298</td>
<td id="S6.T4.7.7.11.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">7.390</td>
<td id="S6.T4.7.7.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.624</td>
</tr>
<tr id="S6.T4.7.7.12" class="ltx_tr">
<td id="S6.T4.7.7.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S6.T4.7.7.12.2" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S6.T4.7.7.12.3" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S6.T4.7.7.12.4" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S6.T4.7.7.12.5" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Evaluation of the audio and visual generation quality. Note that we evaluate the reconstructed audio and visual output of randomly selected 300 videos from the test set of MultiDialog.  </figcaption>
</figure>
<figure id="S6.F4" class="ltx_figure">
<p id="S6.F4.1.1" class="ltx_p ltx_minipage ltx_align_bottom" style="width:433.6pt;"><span id="S6.F4.1.1.1" class="ltx_text"><img src="/html/2406.07867/assets/x4.png" id="S6.F4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="514" height="277" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Audio-visual dialogue generation results of the proposed method, where the last turn is the generated audio-visual response. Note that we have randomly sampled three video frames from each turn for illustration. (a-d) are conversations with four turns and (e-f) are with two turns, The generated responses are in italics and we provide ASR transcriptions below.</figcaption>
</figure>
<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">We evaluate the audio and visual generation quality in Table <a href="#S6.T4" title="Table 4 ‣ 6.3 Audio and Visual Evaluation ‣ 6 Results ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. In terms of speaker voice similarity (SIM), our proposed method not only outperforms the cascaded system but also surpasses spoken dialogue systems. This demonstrates the effectiveness of our AV token-based speech decoder, enriched with speaker embedding, to retain the speaker information from the reference video. When assessing visual quality, we compared it with the cascaded system that uses the same TFG model <cite class="ltx_cite ltx_citemacro_cite">Prajwal et al. (<a href="#bib.bib54" title="" class="ltx_ref">2020</a>)</cite> as ours. While our FID score is comparable, our approach exhibits superior audio-visual synchronization, due to the utilization of discretized audio-visual tokens, which provide clearer alignment between the audio and visual components than raw audio.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">In Figure <a href="#S6.F4" title="Figure 4 ‣ 6.3 Audio and Visual Evaluation ‣ 6 Results ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we show the generated audio-visual response between the two partners along with transcriptions generated with ASR <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a href="#bib.bib59" title="" class="ltx_ref">2021</a>)</cite>. Given a conversation context, our model generates the next response that is contextually coherent and adequate. For example, in Figure <a href="#S6.F4" title="Figure 4 ‣ 6.3 Audio and Visual Evaluation ‣ 6 Results ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (a), it answers the question asked by the user in the previous turn and responds accordingly about the chatting topic, NFL. Also, it successfully synthesizes the speech-relevant movements of the reference face to generate a seamless talking face video. Please refer to the demo for more demonstrations.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Robustness to Acoustic Noise</h3>

<figure id="S6.T5" class="ltx_table">
<div id="S6.T5.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.2pt;height:187.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(112.7pt,-48.8pt) scale(2.08539174652351,2.08539174652351) ;">
<table id="S6.T5.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S6.T5.2.2.3" class="ltx_tr">
<td id="S6.T5.2.2.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;" rowspan="2">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
<span id="S6.T5.2.2.3.1.1" class="ltx_text ltx_font_bold">Method</span>
</td>
<td id="S6.T5.2.2.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;" rowspan="2"><span id="S6.T5.2.2.3.2.1" class="ltx_text"><span id="S6.T5.2.2.3.2.1.1" class="ltx_text"></span> <span id="S6.T5.2.2.3.2.1.2" class="ltx_text">
<span id="S6.T5.2.2.3.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T5.2.2.3.2.1.2.1.1" class="ltx_tr">
<span id="S6.T5.2.2.3.2.1.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S6.T5.2.2.3.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Input</span></span></span>
<span id="S6.T5.2.2.3.2.1.2.1.2" class="ltx_tr">
<span id="S6.T5.2.2.3.2.1.2.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S6.T5.2.2.3.2.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Modality</span></span></span>
</span></span> <span id="S6.T5.2.2.3.2.1.3" class="ltx_text"></span></span></td>
<td id="S6.T5.2.2.3.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding:1.5pt 0.0pt;" colspan="4"><span id="S6.T5.2.2.3.3.1" class="ltx_text ltx_font_bold">SNR (dB)</span></td>
</tr>
<tr id="S6.T5.2.2.4" class="ltx_tr">
<td id="S6.T5.2.2.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">-5</td>
<td id="S6.T5.2.2.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">0</td>
<td id="S6.T5.2.2.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">5</td>
<td id="S6.T5.2.2.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">clean</td>
</tr>
<tr id="S6.T5.1.1.1" class="ltx_tr">
<td id="S6.T5.1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding:1.5pt 0.0pt;" rowspan="2"><span id="S6.T5.1.1.1.2.1" class="ltx_text"><span id="S6.T5.1.1.1.2.1.1" class="ltx_text"></span> <span id="S6.T5.1.1.1.2.1.2" class="ltx_text">
<span id="S6.T5.1.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T5.1.1.1.2.1.2.1.1" class="ltx_tr">
<span id="S6.T5.1.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="S6.T5.1.1.1.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Proposed</span></span></span>
</span></span> <span id="S6.T5.1.1.1.2.1.3" class="ltx_text"></span></span></td>
<td id="S6.T5.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;"><math id="S6.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S6.T5.1.1.1.1.m1.1a"><mi id="S6.T5.1.1.1.1.m1.1.1" xref="S6.T5.1.1.1.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.1.m1.1b"><ci id="S6.T5.1.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.1.m1.1c">A</annotation></semantics></math></td>
<td id="S6.T5.1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">11.340</td>
<td id="S6.T5.1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">14.751</td>
<td id="S6.T5.1.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">21.143</td>
<td id="S6.T5.1.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">23.089</td>
</tr>
<tr id="S6.T5.2.2.2" class="ltx_tr">
<td id="S6.T5.2.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><math id="S6.T5.2.2.2.1.m1.1" class="ltx_Math" alttext="AV" display="inline"><semantics id="S6.T5.2.2.2.1.m1.1a"><mrow id="S6.T5.2.2.2.1.m1.1.1" xref="S6.T5.2.2.2.1.m1.1.1.cmml"><mi id="S6.T5.2.2.2.1.m1.1.1.2" xref="S6.T5.2.2.2.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S6.T5.2.2.2.1.m1.1.1.1" xref="S6.T5.2.2.2.1.m1.1.1.1.cmml">​</mo><mi id="S6.T5.2.2.2.1.m1.1.1.3" xref="S6.T5.2.2.2.1.m1.1.1.3.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T5.2.2.2.1.m1.1b"><apply id="S6.T5.2.2.2.1.m1.1.1.cmml" xref="S6.T5.2.2.2.1.m1.1.1"><times id="S6.T5.2.2.2.1.m1.1.1.1.cmml" xref="S6.T5.2.2.2.1.m1.1.1.1"></times><ci id="S6.T5.2.2.2.1.m1.1.1.2.cmml" xref="S6.T5.2.2.2.1.m1.1.1.2">𝐴</ci><ci id="S6.T5.2.2.2.1.m1.1.1.3.cmml" xref="S6.T5.2.2.2.1.m1.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.2.2.2.1.m1.1c">AV</annotation></semantics></math></td>
<td id="S6.T5.2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">13.853</td>
<td id="S6.T5.2.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">18.144</td>
<td id="S6.T5.2.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">21.186</td>
<td id="S6.T5.2.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">24.094</td>
</tr>
<tr id="S6.T5.2.2.5" class="ltx_tr">
<td id="S6.T5.2.2.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S6.T5.2.2.5.2" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S6.T5.2.2.5.3" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S6.T5.2.2.5.4" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S6.T5.2.2.5.5" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
<td id="S6.T5.2.2.5.6" class="ltx_td" style="padding:1.5pt 0.0pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span> Dialogue response generation performance (BLEU) with different input modalities under acoustic noise corruption with different SNR levels (dB).</figcaption>
</figure>
<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">In Table <a href="#S6.T5" title="Table 5 ‣ 6.4 Robustness to Acoustic Noise ‣ 6 Results ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we analyze the effectiveness of incorporating additional visual modality into the dialogue system. Following <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a href="#bib.bib59" title="" class="ltx_ref">2021</a>)</cite>, we corrupt the input speech with random noise of varying SNR levels (-5, 0, 5, and clean). Compared with audio-only input, audio-visual input enhances the robustness of the system as indicated by less degradation of the performance under noise. This is because the visual modality which is not affected by acoustic noise can complement the missing information in the audio modality to better recognize the speech content and output response. It further demonstrates that our system is applicable for real-world use in unstable speech input scenarios.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Limitation</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We introduce a novel face-to-face spoken dialogue model that directly processes audio-visual speech from the user input and generates audio-visual speech response. This is the first step toward creating a talking face avatar chatbot system, without intermediate text in the generation process. In addition, we release MultiDialog, the largest multimodal dialogue dataset to date with tri-modality (<em id="S7.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S7.p1.1.2" class="ltx_text"></span>, audio, visual, and text) spoken dialogue data. As it is an extensive dataset that captures real human-human conversation covering broad topics, we believe it brings diverse research opportunities for multimodal synthesis, ranging from talking face synthesis to multimodal dialogue language modeling.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">One limitation of our work is that, although the dataset includes emotion labels for each utterance, we have not utilized these labels yet. We plan to address this in future research by integrating emotion recognition from users’ facial expressions to generate more emotion-aware responses, both in speech content and nuances of generation. Also, since our data provides parallel recordings of the speaker and the listener, we can simultaneously model the generation of both faces for more spontaneous and natural conversation.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afouras et al. (2018)</span>
<span class="ltx_bibblock">
Triantafyllos Afouras, Joon Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman. 2018.

</span>
<span class="ltx_bibblock">Deep audio-visual speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</em>, 44(12):8717–8727.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anwar et al. (2023)</span>
<span class="ltx_bibblock">
Mohamed Anwar, Bowen Shi, Vedanuj Goswami, Wei-Ning Hsu, Juan Pino, and Changhan Wang. 2023.

</span>
<span class="ltx_bibblock">Muavic: A multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.00628</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babu et al. (2021)</span>
<span class="ltx_bibblock">
Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, et al. 2021.

</span>
<span class="ltx_bibblock">Xls-r: Self-supervised cross-lingual speech representation learning at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.09296</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:12449–12460.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Lavie (2005)</span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Alon Lavie. 2005.

</span>
<span class="ltx_bibblock">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</em>, pages 65–72.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al. (2023)</span>
<span class="ltx_bibblock">
Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. 2023.

</span>
<span class="ltx_bibblock">Seamlessm4t-massively multilingual &amp; multimodal machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.11596</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et al. (2000)</span>
<span class="ltx_bibblock">
Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. 2000.

</span>
<span class="ltx_bibblock">A neural probabilistic language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 13.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borsos et al. (2023)</span>
<span class="ltx_bibblock">
Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. 2023.

</span>
<span class="ltx_bibblock">Audiolm: a language modeling approach to audio generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Budzianowski et al. (2018)</span>
<span class="ltx_bibblock">
Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašić. 2018.

</span>
<span class="ltx_bibblock">Multiwoz–a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.00278</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulat and Tzimiropoulos (2017)</span>
<span class="ltx_bibblock">
Adrian Bulat and Georgios Tzimiropoulos. 2017.

</span>
<span class="ltx_bibblock">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks).

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">International Conference on Computer Vision</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Busso et al. (2008)</span>
<span class="ltx_bibblock">
Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan. 2008.

</span>
<span class="ltx_bibblock">Iemocap: Interactive emotional dyadic motion capture database.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Language resources and evaluation</em>, 42:335–359.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casanova et al. (2022)</span>
<span class="ltx_bibblock">
Edresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren Gölge, and Moacir A Ponti. 2022.

</span>
<span class="ltx_bibblock">Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 2709–2720. PMLR.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022.

</span>
<span class="ltx_bibblock">Wavlm: Large-scale self-supervised pre-training for full stack speech processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, 16(6):1505–1518.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2023)</span>
<span class="ltx_bibblock">
Yong Cheng, Yu Zhang, Melvin Johnson, Wolfgang Macherey, and Ankur Bapna. 2023.

</span>
<span class="ltx_bibblock">Mu<sup id="bib.bib14.2.1" class="ltx_sup"><span id="bib.bib14.2.1.1" class="ltx_text ltx_font_italic">2</span></sup> slam: Multitask, multilingual speech and language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 5504–5520. PMLR.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al. (2023)</span>
<span class="ltx_bibblock">
Jeongsoo Choi, Minsu Kim, Se Jin Park, and Yong Man Ro. 2023.

</span>
<span class="ltx_bibblock">Reprogramming audio-driven talking face synthesis into text-driven.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.16003</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2021)</span>
<span class="ltx_bibblock">
Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. 2021.

</span>
<span class="ltx_bibblock">W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, pages 244–250. IEEE.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2020)</span>
<span class="ltx_bibblock">
Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. 2020.

</span>
<span class="ltx_bibblock">Retinaface: Single-shot multi-level face localisation in the wild.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 5203–5212.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. (2023)</span>
<span class="ltx_bibblock">
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023.

</span>
<span class="ltx_bibblock">Enhancing chat language models by scaling high-quality instructional conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14233</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2023)</span>
<span class="ltx_bibblock">
Qianqian Dong, Zhiying Huang, Chen Xu, Yunlong Zhao, Kexin Wang, Xuxin Cheng, Tom Ko, Qiao Tian, Tang Li, Fengpeng Yue, et al. 2023.

</span>
<span class="ltx_bibblock">Polyvoice: Language models for speech to speech translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.02982</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ekman and Friesen (1978)</span>
<span class="ltx_bibblock">
Paul Ekman and Wallace V Friesen. 1978.

</span>
<span class="ltx_bibblock">Facial action coding system.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Environmental Psychology &amp; Nonverbal Behavior</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gangamohan et al. (2016)</span>
<span class="ltx_bibblock">
Paidi Gangamohan, Sudarsana Reddy Kadiri, and B Yegnanarayana. 2016.

</span>
<span class="ltx_bibblock">Analysis of emotional speech—a review.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Toward Robotic Socially Believable Behaving Systems-Volume I: Modeling Emotions</em>, pages 205–238.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. (2023)</span>
<span class="ltx_bibblock">
Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. 2023.

</span>
<span class="ltx_bibblock">Multimodal-gpt: A vision and language model for dialogue with humans.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.04790</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gopalakrishnan et al. (2023)</span>
<span class="ltx_bibblock">
Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, and Dilek Hakkani-Tur. 2023.

</span>
<span class="ltx_bibblock">Topical-chat: Towards knowledge-grounded open-domain conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.11995</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2023)</span>
<span class="ltx_bibblock">
Sahil Goyal, Sarthak Bhagat, Shagun Uppal, Hitkul Jangra, Yi Yu, Yifang Yin, and Rajiv Ratn Shah. 2023.

</span>
<span class="ltx_bibblock">Emotionally enhanced talking face generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st International Workshop on Multimedia Content Generation and Evaluation: New Methods and Practice</em>, pages 81–90.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassid et al. (2023)</span>
<span class="ltx_bibblock">
Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, et al. 2023.

</span>
<span class="ltx_bibblock">Textually pretrained speech language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.13009</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Henderson et al. (2014)</span>
<span class="ltx_bibblock">
Matthew Henderson, Blaise Thomson, and Jason D Williams. 2014.

</span>
<span class="ltx_bibblock">The second dialog state tracking challenge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th annual meeting of the special interest group on discourse and dialogue (SIGDIAL)</em>, pages 263–272.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heusel et al. (2017)</span>
<span class="ltx_bibblock">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017.

</span>
<span class="ltx_bibblock">Gans trained by a two time-scale update rule converge to a local nash equilibrium.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et al. (2023)</span>
<span class="ltx_bibblock">
Joanna Hong, Minsu Kim, Jeongsoo Choi, and Yong Man Ro. 2023.

</span>
<span class="ltx_bibblock">Watch or listen: Robust audio-visual speech recognition with visual corruption modeling and reliability scoring.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 18783–18794.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2021)</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021.

</span>
<span class="ltx_bibblock">Hubert: Self-supervised speech representation learning by masked prediction of hidden units.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 29:3451–3460.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.09685</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2018)</span>
<span class="ltx_bibblock">
Guosheng Hu, Li Liu, Yang Yuan, Zehao Yu, Yang Hua, Zhihong Zhang, Fumin Shen, Ling Shao, Timothy Hospedales, Neil Robertson, et al. 2018.

</span>
<span class="ltx_bibblock">Deep multi-task learning to recognise subtle facial expressions of mental states.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer vision (ECCV)</em>, pages 103–119.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023)</span>
<span class="ltx_bibblock">
Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. 2023.

</span>
<span class="ltx_bibblock">Audiogpt: Understanding and generating speech, music, sound, and talking head.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.12995</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. (2018)</span>
<span class="ltx_bibblock">
Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu, et al. 2018.

</span>
<span class="ltx_bibblock">Transfer learning from speaker verification to multispeaker text-to-speech synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 31.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2023)</span>
<span class="ltx_bibblock">
Minsu Kim, Jeongsoo Choi, Dahun Kim, and Yong Man Ro. 2023.

</span>
<span class="ltx_bibblock">Many-to-many spoken language translation via unified speech and text representation learning with unit-to-unit translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.01831</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2024)</span>
<span class="ltx_bibblock">
Minsu Kim, Jeong Hun Yeo, Jeongsoo Choi, Se Jin Park, and Yong Man Ro. 2024.

</span>
<span class="ltx_bibblock">Multilingual visual speech recognition with a single model by learning with discrete visual speech units.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.09802</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong et al. (2020)</span>
<span class="ltx_bibblock">
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020.

</span>
<span class="ltx_bibblock">Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:17022–17033.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köpf et al. (2023)</span>
<span class="ltx_bibblock">
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. 2023.

</span>
<span class="ltx_bibblock">Openassistant conversations–democratizing large language model alignment.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.07327</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lakhotia et al. (2021)</span>
<span class="ltx_bibblock">
Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. 2021.

</span>
<span class="ltx_bibblock">On generative spoken language modeling from raw audio.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 9:1336–1354.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(39)</span>
<span class="ltx_bibblock">
Nathan Lambert, Nazneen Rajani Lewis Tunstall, and Tristan Thrush.

</span>
<span class="ltx_bibblock">Huggingface h4 stack exchange preference dataset. 2023.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">URL https://huggingface. co/datasets/HuggingFaceH4/stack-exchange-preferences</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2021)</span>
<span class="ltx_bibblock">
Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, et al. 2021.

</span>
<span class="ltx_bibblock">Textless speech-to-speech translation on real data.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.08352</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2023)</span>
<span class="ltx_bibblock">
Keon Lee, Kyumin Park, and Daeyoung Kim. 2023.

</span>
<span class="ltx_bibblock">Dailytalk: Spoken dialogue dataset for conversational text-to-speech.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 1–5. IEEE.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2016)</span>
<span class="ltx_bibblock">
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N16-1014" title="" class="ltx_ref ltx_href">A diversity-promoting objective function for neural conversation models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 110–119, San Diego, California. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2017)</span>
<span class="ltx_bibblock">
Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017.

</span>
<span class="ltx_bibblock">Dailydialog: A manually labelled multi-turn dialogue dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.03957</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lowe et al. (2015)</span>
<span class="ltx_bibblock">
Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015.

</span>
<span class="ltx_bibblock">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1506.08909</em>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maiti et al. (2023)</span>
<span class="ltx_bibblock">
Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, and Shinji Watanabe. 2023.

</span>
<span class="ltx_bibblock">Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.07937</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nachmani et al. (2023)</span>
<span class="ltx_bibblock">
Eliya Nachmani, Alon Levkovitch, Julian Salazar, Chulayutsh Asawaroengchai, Soroosh Mariooryad, RJ Skerry-Ryan, and Michelle Tadmor Ramanovich. 2023.

</span>
<span class="ltx_bibblock">Lms with a voice: Spoken language modeling beyond speech tokens.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15255</em>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2023a)</span>
<span class="ltx_bibblock">
Tu Anh Nguyen, Wei-Ning Hsu, Antony d’Avirro, Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Remez, Jade Copet, Gabriel Synnaeve, Michael Hassid, et al. 2023a.

</span>
<span class="ltx_bibblock">Expresso: A benchmark and analysis of discrete expressive speech resynthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.05725</em>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2023b)</span>
<span class="ltx_bibblock">
Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoit Sagot, Abdelrahman Mohamed, et al. 2023b.

</span>
<span class="ltx_bibblock">Generative spoken dialogue language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 11:250–266.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2022)</span>
<span class="ltx_bibblock">
Se Jin Park, Minsu Kim, Joanna Hong, Jeongsoo Choi, and Yong Man Ro. 2022.

</span>
<span class="ltx_bibblock">Synctalkface: Talking face generation with precise lip-syncing via audio-lip memory.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 36, pages 2062–2070.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petridis et al. (2018)</span>
<span class="ltx_bibblock">
Stavros Petridis, Themos Stafylakis, Pingehuan Ma, Feipeng Cai, Georgios Tzimiropoulos, and Maja Pantic. 2018.

</span>
<span class="ltx_bibblock">End-to-end audiovisual speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>, pages 6548–6552. IEEE.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popuri et al. (2022)</span>
<span class="ltx_bibblock">
Sravya Popuri, Peng-Jen Chen, Changhan Wang, Juan Pino, Yossi Adi, Jiatao Gu, Wei-Ning Hsu, and Ann Lee. 2022.

</span>
<span class="ltx_bibblock">Enhanced direct speech-to-speech translation using self-supervised pre-training and data augmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poria et al. (2018)</span>
<span class="ltx_bibblock">
Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. 2018.

</span>
<span class="ltx_bibblock">Meld: A multimodal multi-party dataset for emotion recognition in conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.02508</em>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock">A call for clarity in reporting bleu scores.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1804.08771</em>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prajwal et al. (2020)</span>
<span class="ltx_bibblock">
KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Namboodiri, and CV Jawahar. 2020.

</span>
<span class="ltx_bibblock">A lip sync expert is all you need for speech to lip generation in the wild.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM international conference on multimedia</em>, pages 484–492.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rashkin et al. (2018)</span>
<span class="ltx_bibblock">
Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2018.

</span>
<span class="ltx_bibblock">Towards empathetic open-domain conversation models: A new benchmark and dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.00207</em>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddy et al. (2019)</span>
<span class="ltx_bibblock">
Siva Reddy, Danqi Chen, and Christopher D Manning. 2019.

</span>
<span class="ltx_bibblock">Coqa: A conversational question answering challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 7:249–266.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rubenstein et al. (2023)</span>
<span class="ltx_bibblock">
Paul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. 2023.

</span>
<span class="ltx_bibblock">Audiopalm: A large language model that can speak and listen.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.12925</em>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schneider et al. (2019)</span>
<span class="ltx_bibblock">
Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock">wav2vec: Unsupervised pre-training for speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.05862</em>.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2021)</span>
<span class="ltx_bibblock">
Bowen Shi, Wei-Ning Hsu, Kushal Lakhotia, and Abdelrahman Mohamed. 2021.

</span>
<span class="ltx_bibblock">Learning audio-visual speech representation by masked multimodal cluster prediction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et al. (2023)</span>
<span class="ltx_bibblock">
Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin, Yinpei Dai, Hangyu Li, Rui Yan, Fei Huang, and Yongbin Li. 2023.

</span>
<span class="ltx_bibblock">Spokenwoz: A large-scale speech-text benchmark for spoken task-oriented dialogue agents.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2023)</span>
<span class="ltx_bibblock">
Luchuan Song, Guojun Yin, Zhenchao Jin, Xiaoyi Dong, and Chenliang Xu. 2023.

</span>
<span class="ltx_bibblock">Emotional listener portrait: Realistic listener motion simulation in conversation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, pages 20782–20792. IEEE.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2022)</span>
<span class="ltx_bibblock">
Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2022.

</span>
<span class="ltx_bibblock">Mvp: Multi-task supervised pre-training for natural language generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.12131</em>.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. 2023a.

</span>
<span class="ltx_bibblock">Neural codec language models are zero-shot text to speech synthesizers.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.02111</em>.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021a)</span>
<span class="ltx_bibblock">
Shaocong Wang, Yuan Yuan, Xiangtao Zheng, and Xiaoqiang Lu. 2021a.

</span>
<span class="ltx_bibblock">Local and correlation attention learning for subtle facial expression recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, 453:742–753.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and Furu Wei. 2023b.

</span>
<span class="ltx_bibblock">Viola: Unified codec language models for speech recognition, synthesis, and translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.16107</em>.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021b)</span>
<span class="ltx_bibblock">
Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. 2021b.

</span>
<span class="ltx_bibblock">Towards real-world blind face restoration with generative facial prior.

</span>
<span class="ltx_bibblock">In <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023.

</span>
<span class="ltx_bibblock">Next-gpt: Any-to-any multimodal llm.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.05519</em>.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023a)</span>
<span class="ltx_bibblock">
Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023a.

</span>
<span class="ltx_bibblock">Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.11000</em>.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2018)</span>
<span class="ltx_bibblock">
Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018.

</span>
<span class="ltx_bibblock">Personalizing dialogue agents: I have a dog, do you have pets too?

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1801.07243</em>.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.01068</em>.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023b)</span>
<span class="ltx_bibblock">
Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. 2023b.

</span>
<span class="ltx_bibblock">Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 8652–8661.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2019.

</span>
<span class="ltx_bibblock">Dialogpt: Large-scale generative pre-training for conversational response generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.00536</em>.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2018)</span>
<span class="ltx_bibblock">
Kangyan Zhou, Shrimai Prabhumoye, and Alan W Black. 2018.

</span>
<span class="ltx_bibblock">A dataset for document grounded conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.07358</em>.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, and Tiejun Zhao. 2023.

</span>
<span class="ltx_bibblock">Interactive conversational head generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.02090</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>MultiDialog Dataset</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Dataset Statistics</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">Table <a href="#S1.T2" title="Table 2 ‣ 1 Introduction ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows detailed statistics of MultiDialog. MultiDialog consists of 9,920 human-human conversations, 106,624 turns, 218,248 utterances, totalling to approximately 340 hours of audiovisual dialogue data. A single dialogue contains multiple turns, where each turn includes two utterances. An utterance is an instance of speech by one person followed by silence or another person speaking. In our dataset, a conversation averaged 11.0 turns, 21.9 utterances, 140.2 seconds in length. 12 speakers were paired to record an average of 826.7 dialogues per person.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Participant Information</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">Prior to recording our dataset, we received an IRB approval to collect facial video, speech, and text data to build human multimodal dialogue technology. We recruited students at a university who were fluent in English and could fulfill the designated portion of the dialogues. A recruitment notice included general information about TopicalChat, the dataset to be recorded, wage and responsibilities of the participants, and potential effects and contributions of building a multimodal dialogue dataset. After receiving 25 applications, interviews were conducted on all applicants. During the interview, we notified that we will be collecting audiovisual data of the participant during recording sessions, which will be released to the research field in the future. We also collected participant information such as race, sex, nationality and age, agreement to release audiovisual data, and assessed the English fluency and ability to read and act out a given dialogue script with emotions. Two interviewees in charge of the dataset collection selected actors by ranking each participant on a scale of 1 to 5 on each criterion and considering the diversity of participant demographics. Thus, six female and six male actors from six different countries, and age varying from 20 to 30 were selected. Details on participant information are outlined in Table <a href="#A1.T6" title="Table 6 ‣ A.2 Participant Information ‣ Appendix A MultiDialog Dataset ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="A1.T6" class="ltx_table">
<div id="A1.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:429.3pt;height:578.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(121.2pt,-163.4pt) scale(2.29717916858924,2.29717916858924) ;">
<table id="A1.T6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T6.1.1.1" class="ltx_tr">
<td id="A1.T6.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
<span id="A1.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">Id</span>
</td>
<td id="A1.T6.1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;"><span id="A1.T6.1.1.1.2.1" class="ltx_text ltx_font_bold">Gender</span></td>
<td id="A1.T6.1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;"><span id="A1.T6.1.1.1.3.1" class="ltx_text ltx_font_bold">Age</span></td>
<td id="A1.T6.1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;"><span id="A1.T6.1.1.1.4.1" class="ltx_text ltx_font_bold">Nationality</span></td>
<td id="A1.T6.1.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;"><span id="A1.T6.1.1.1.5.1" class="ltx_text ltx_font_bold"># dialogues</span></td>
<td id="A1.T6.1.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;"><span id="A1.T6.1.1.1.6.1" class="ltx_text ltx_font_bold">Acc.</span></td>
</tr>
<tr id="A1.T6.1.1.2" class="ltx_tr">
<td id="A1.T6.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding:1pt 0.0pt;">a</td>
<td id="A1.T6.1.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">F</td>
<td id="A1.T6.1.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">24</td>
<td id="A1.T6.1.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">Indonesia</td>
<td id="A1.T6.1.1.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">1,453</td>
<td id="A1.T6.1.1.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">69.3</td>
</tr>
<tr id="A1.T6.1.1.3" class="ltx_tr">
<td id="A1.T6.1.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">b</td>
<td id="A1.T6.1.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">F</td>
<td id="A1.T6.1.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">25</td>
<td id="A1.T6.1.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">S. Korea</td>
<td id="A1.T6.1.1.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">1,454</td>
<td id="A1.T6.1.1.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">63.6</td>
</tr>
<tr id="A1.T6.1.1.4" class="ltx_tr">
<td id="A1.T6.1.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">c</td>
<td id="A1.T6.1.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">M</td>
<td id="A1.T6.1.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">23</td>
<td id="A1.T6.1.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">Kazakhstan</td>
<td id="A1.T6.1.1.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">1,772</td>
<td id="A1.T6.1.1.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">59.3</td>
</tr>
<tr id="A1.T6.1.1.5" class="ltx_tr">
<td id="A1.T6.1.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">d</td>
<td id="A1.T6.1.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">M</td>
<td id="A1.T6.1.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">23</td>
<td id="A1.T6.1.1.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">Kazakhstan</td>
<td id="A1.T6.1.1.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">1,108</td>
<td id="A1.T6.1.1.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">33.8</td>
</tr>
<tr id="A1.T6.1.1.6" class="ltx_tr">
<td id="A1.T6.1.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">e</td>
<td id="A1.T6.1.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">F</td>
<td id="A1.T6.1.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">24</td>
<td id="A1.T6.1.1.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">India</td>
<td id="A1.T6.1.1.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">1,718</td>
<td id="A1.T6.1.1.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">41.5</td>
</tr>
<tr id="A1.T6.1.1.7" class="ltx_tr">
<td id="A1.T6.1.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">f</td>
<td id="A1.T6.1.1.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">M</td>
<td id="A1.T6.1.1.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">24</td>
<td id="A1.T6.1.1.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">Pakistan</td>
<td id="A1.T6.1.1.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">1,083</td>
<td id="A1.T6.1.1.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">43.8</td>
</tr>
<tr id="A1.T6.1.1.8" class="ltx_tr">
<td id="A1.T6.1.1.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">g</td>
<td id="A1.T6.1.1.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">F</td>
<td id="A1.T6.1.1.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">20</td>
<td id="A1.T6.1.1.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">Kazakhstan</td>
<td id="A1.T6.1.1.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">1,774</td>
<td id="A1.T6.1.1.8.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">50.0</td>
</tr>
<tr id="A1.T6.1.1.9" class="ltx_tr">
<td id="A1.T6.1.1.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">h</td>
<td id="A1.T6.1.1.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">M</td>
<td id="A1.T6.1.1.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">21</td>
<td id="A1.T6.1.1.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">Pakistan</td>
<td id="A1.T6.1.1.9.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">1,642</td>
<td id="A1.T6.1.1.9.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">37.0</td>
</tr>
<tr id="A1.T6.1.1.10" class="ltx_tr">
<td id="A1.T6.1.1.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">i</td>
<td id="A1.T6.1.1.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">F</td>
<td id="A1.T6.1.1.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">23</td>
<td id="A1.T6.1.1.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">Pakistan</td>
<td id="A1.T6.1.1.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">995</td>
<td id="A1.T6.1.1.10.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">60.0</td>
</tr>
<tr id="A1.T6.1.1.11" class="ltx_tr">
<td id="A1.T6.1.1.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">j</td>
<td id="A1.T6.1.1.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">M</td>
<td id="A1.T6.1.1.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">24</td>
<td id="A1.T6.1.1.11.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">Bangladesh</td>
<td id="A1.T6.1.1.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">1,661</td>
<td id="A1.T6.1.1.11.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">44.7</td>
</tr>
<tr id="A1.T6.1.1.12" class="ltx_tr">
<td id="A1.T6.1.1.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">k</td>
<td id="A1.T6.1.1.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">M</td>
<td id="A1.T6.1.1.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">20</td>
<td id="A1.T6.1.1.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">S. Korea</td>
<td id="A1.T6.1.1.12.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">1,449</td>
<td id="A1.T6.1.1.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">44.0</td>
</tr>
<tr id="A1.T6.1.1.13" class="ltx_tr">
<td id="A1.T6.1.1.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">l</td>
<td id="A1.T6.1.1.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">F</td>
<td id="A1.T6.1.1.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">20</td>
<td id="A1.T6.1.1.13.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">Pakistan</td>
<td id="A1.T6.1.1.13.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">1,357</td>
<td id="A1.T6.1.1.13.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">21.2</td>
</tr>
<tr id="A1.T6.1.1.14" class="ltx_tr">
<td id="A1.T6.1.1.14.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding:1pt 0.0pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="A1.T6.1.1.14.2" class="ltx_td ltx_border_tt" style="padding:1pt 0.0pt;"></td>
<td id="A1.T6.1.1.14.3" class="ltx_td ltx_border_tt" style="padding:1pt 0.0pt;"></td>
<td id="A1.T6.1.1.14.4" class="ltx_td ltx_border_tt" style="padding:1pt 0.0pt;"></td>
<td id="A1.T6.1.1.14.5" class="ltx_td ltx_border_tt" style="padding:1pt 0.0pt;"></td>
<td id="A1.T6.1.1.14.6" class="ltx_td ltx_border_tt" style="padding:1pt 0.0pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Participant information of MultiDialog.</figcaption>
</figure>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p">After all participants were selected, we held an orientation to guide participants on the recording procedure. For a single recording session of three hours, two participants were scheduled to film 50 to 60 conversations in TopicalChat. The number of conversations to film in a session was calculated based on a trial recording session, in which two speakers filmed approximately 60 conversations in a three-hour period, including breaks. Participants learned how to navigate through the dialogue display program to start and end recording conversations, and proceed to the next utterance. The display program showed the conversation script along with the corresponding emotion for each utterance, and the remaining number of conversations to film in the current session. We notified each participant to attach a microphone about 15 to 20 cm from their mouth and adjust the camera to the shoulder level before recording. Lastly, we collected consent forms for providing personal information for compensation and informed consent forms for human subject research participants.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Annotation Evaluation</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">We conducted a comprehensive user study involving 25 participants, where we randomly sampled 70 utterances from the dataset and participants predicted the emotions conveyed within each utterance to verify the quality of emotions.</p>
</div>
<div id="A1.SS3.p2" class="ltx_para">
<p id="A1.SS3.p2.1" class="ltx_p">Table <a href="#A1.T6" title="Table 6 ‣ A.2 Participant Information ‣ Appendix A MultiDialog Dataset ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> includes the accuracy of each actor in conveying the intended emotion in the utterance. Given that real-life conversations often involve subtle and layered emotional expressions, the dataset was designed to mirror this intricacy. Based on previous research <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib31" title="" class="ltx_ref">2018</a>); Wang et al. (<a href="#bib.bib64" title="" class="ltx_ref">2021a</a>)</cite> on subtle emotion recognition, the results from our user study underscore the effectiveness of the actors in portraying these subtle emotions. To enhance the quality of the emotion annotations to be used in future research, we filter out recordings from actors that exhibit low prediction scores and release a subset of MultiDialog.</p>
</div>
<figure id="A1.T7" class="ltx_table">
<div id="A1.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:429.3pt;height:358.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(117.6pt,-98.2pt) scale(2.21249403302812,2.21249403302812) ;">
<table id="A1.T7.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.1.1.1" class="ltx_tr">
<td id="A1.T7.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="A1.T7.1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.1.2.1" class="ltx_text ltx_font_bold">NEU</span></td>
<td id="A1.T7.1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.1.3.1" class="ltx_text ltx_font_bold">HAP</span></td>
<td id="A1.T7.1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.1.4.1" class="ltx_text ltx_font_bold">FEAR</span></td>
<td id="A1.T7.1.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.1.5.1" class="ltx_text ltx_font_bold">ANG</span></td>
<td id="A1.T7.1.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.1.6.1" class="ltx_text ltx_font_bold">DISG</span></td>
<td id="A1.T7.1.1.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.1.7.1" class="ltx_text ltx_font_bold">SUR</span></td>
<td id="A1.T7.1.1.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.1.8.1" class="ltx_text ltx_font_bold">SAD</span></td>
</tr>
<tr id="A1.T7.1.1.2" class="ltx_tr">
<td id="A1.T7.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.2.1.1" class="ltx_text ltx_font_bold">NEU</span></td>
<td id="A1.T7.1.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">0.88</td>
<td id="A1.T7.1.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">0.04</td>
<td id="A1.T7.1.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">0.01</td>
<td id="A1.T7.1.1.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">0.03</td>
<td id="A1.T7.1.1.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">0.00</td>
<td id="A1.T7.1.1.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">0.03</td>
<td id="A1.T7.1.1.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1.5pt 0.0pt;">0.02</td>
</tr>
<tr id="A1.T7.1.1.3" class="ltx_tr">
<td id="A1.T7.1.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.3.1.1" class="ltx_text ltx_font_bold">HAP</span></td>
<td id="A1.T7.1.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.18</td>
<td id="A1.T7.1.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.75</td>
<td id="A1.T7.1.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.00</td>
<td id="A1.T7.1.1.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.01</td>
<td id="A1.T7.1.1.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.01</td>
<td id="A1.T7.1.1.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.05</td>
<td id="A1.T7.1.1.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.001</td>
</tr>
<tr id="A1.T7.1.1.4" class="ltx_tr">
<td id="A1.T7.1.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.4.1.1" class="ltx_text ltx_font_bold">FEAR</span></td>
<td id="A1.T7.1.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.09</td>
<td id="A1.T7.1.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.02</td>
<td id="A1.T7.1.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.39</td>
<td id="A1.T7.1.1.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.03</td>
<td id="A1.T7.1.1.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.13</td>
<td id="A1.T7.1.1.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.22</td>
<td id="A1.T7.1.1.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.13</td>
</tr>
<tr id="A1.T7.1.1.5" class="ltx_tr">
<td id="A1.T7.1.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.5.1.1" class="ltx_text ltx_font_bold">ANG</span></td>
<td id="A1.T7.1.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.07</td>
<td id="A1.T7.1.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.00</td>
<td id="A1.T7.1.1.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.07</td>
<td id="A1.T7.1.1.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.76</td>
<td id="A1.T7.1.1.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.14</td>
<td id="A1.T7.1.1.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.02</td>
<td id="A1.T7.1.1.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.00</td>
</tr>
<tr id="A1.T7.1.1.6" class="ltx_tr">
<td id="A1.T7.1.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.6.1.1" class="ltx_text ltx_font_bold">DISG</span></td>
<td id="A1.T7.1.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.02</td>
<td id="A1.T7.1.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.00</td>
<td id="A1.T7.1.1.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.02</td>
<td id="A1.T7.1.1.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.11</td>
<td id="A1.T7.1.1.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.83</td>
<td id="A1.T7.1.1.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.02</td>
<td id="A1.T7.1.1.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.00</td>
</tr>
<tr id="A1.T7.1.1.7" class="ltx_tr">
<td id="A1.T7.1.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.7.1.1" class="ltx_text ltx_font_bold">SUR</span></td>
<td id="A1.T7.1.1.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.14</td>
<td id="A1.T7.1.1.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.13</td>
<td id="A1.T7.1.1.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.00</td>
<td id="A1.T7.1.1.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.04</td>
<td id="A1.T7.1.1.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.01</td>
<td id="A1.T7.1.1.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.68</td>
<td id="A1.T7.1.1.7.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.00</td>
</tr>
<tr id="A1.T7.1.1.8" class="ltx_tr">
<td id="A1.T7.1.1.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1.5pt 0.0pt;"><span id="A1.T7.1.1.8.1.1" class="ltx_text ltx_font_bold">SAD</span></td>
<td id="A1.T7.1.1.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.12</td>
<td id="A1.T7.1.1.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.00</td>
<td id="A1.T7.1.1.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.14</td>
<td id="A1.T7.1.1.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.04</td>
<td id="A1.T7.1.1.8.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.10</td>
<td id="A1.T7.1.1.8.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.00</td>
<td id="A1.T7.1.1.8.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1.5pt 0.0pt;">0.59</td>
</tr>
<tr id="A1.T7.1.1.9" class="ltx_tr">
<td id="A1.T7.1.1.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" style="padding:1.5pt 0.0pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="A1.T7.1.1.9.2" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
<td id="A1.T7.1.1.9.3" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
<td id="A1.T7.1.1.9.4" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
<td id="A1.T7.1.1.9.5" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
<td id="A1.T7.1.1.9.6" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
<td id="A1.T7.1.1.9.7" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
<td id="A1.T7.1.1.9.8" class="ltx_td ltx_border_tt" style="padding:1.5pt 0.0pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Confusion matrix of emotion categories estimated from the user study.</figcaption>
</figure>
<div id="A1.SS3.p3" class="ltx_para">
<p id="A1.SS3.p3.1" class="ltx_p">Table <a href="#A1.T7" title="Table 7 ‣ A.3 Annotation Evaluation ‣ Appendix A MultiDialog Dataset ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> is the confusion matrix between emotion categories estimated from the user study, focusing on results from actors who achieved above 40% emotion accuracy. The result closely aligns with the human innate ability to recognize emotion from audio-visual <cite class="ltx_cite ltx_citemacro_cite">Busso et al. (<a href="#bib.bib11" title="" class="ltx_ref">2008</a>)</cite>, underlining the effectiveness of MultiDialog in conveying emotion within utterances. Certain emotions, such as fearful and sad, exhibited lower accuracy rates, which we attribute to the inherent complexity and subtlety of these emotions in natural conversations <cite class="ltx_cite ltx_citemacro_cite">Poria et al. (<a href="#bib.bib52" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<section id="A1.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.1 </span>Gold Emotion Dialogue Subset</h4>

<div id="A1.SS3.SSS1.p1" class="ltx_para">
<p id="A1.SS3.SSS1.p1.1" class="ltx_p">We provide a gold emotion dialogue subset in the MultiDialog dataset, a more reliable resource for studying emotional dynamics in conversations. Previous research <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib31" title="" class="ltx_ref">2018</a>); Wang et al. (<a href="#bib.bib64" title="" class="ltx_ref">2021a</a>)</cite> indicates that the accuracy rates for recognizing subtle emotions are slightly under 40%. Thus, we classify dialogues from actors that exhibit emotion accuracy above 40% as gold emotion dialogue. We release the gold emotion annotations of actor IDs along with the dataset in <a target="_blank" href="https://huggingface.co/datasets/IVLLab/MultiDialog" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/IVLLab/MultiDialog</a>.</p>
</div>
</section>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Recording Setup</h3>

<div id="A1.SS4.p1" class="ltx_para">
<p id="A1.SS4.p1.1" class="ltx_p">Fig. <a href="#A1.F5" title="Figure 5 ‣ A.4 Recording Setup ‣ Appendix A MultiDialog Dataset ‣ Let’s Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the studio setup for recording sessions.</p>
</div>
<figure id="A1.F5" class="ltx_figure">
<p id="A1.F5.1.1" class="ltx_p ltx_minipage ltx_align_bottom" style="width:433.6pt;"><span id="A1.F5.1.1.1" class="ltx_text"><img src="/html/2406.07867/assets/x5.png" id="A1.F5.1.1.1.g1" class="ltx_graphics ltx_img_square" width="211" height="190" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Recording studio setup for MultiDialog dataset</figcaption>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Evaluation Metrics</h2>

<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p"><span id="A2.p1.1.1" class="ltx_text ltx_font_bold">BLEU</span> <cite class="ltx_cite ltx_citemacro_cite">Post (<a href="#bib.bib53" title="" class="ltx_ref">2018</a>)</cite> evaluates the fluency and adequacy of generated responses based on n-gram overlap. A higher BLEU score indicates a more natural and engaging dialogue model.</p>
</div>
<div id="A2.p2" class="ltx_para ltx_noindent">
<p id="A2.p2.1" class="ltx_p"><span id="A2.p2.1.1" class="ltx_text ltx_font_bold">PPL</span> <cite class="ltx_cite ltx_citemacro_cite">Bengio et al. (<a href="#bib.bib7" title="" class="ltx_ref">2000</a>)</cite> measures how well a language model predicts the generated response. A lower perplexity indicates that the model is more confident and accurate in predicting the next word, suggesting higher quality in generating coherent and contextually relevant responses.</p>
</div>
<div id="A2.p3" class="ltx_para ltx_noindent">
<p id="A2.p3.1" class="ltx_p"><span id="A2.p3.1.1" class="ltx_text ltx_font_bold">DISTINCT-n</span> <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib42" title="" class="ltx_ref">2016</a>)</cite> evaluates the diversity of generated response by calculating the percentage of unique n-grams in the set of responses. Specifically, D-1 measures the percentage of unique unigrams in the generated text, while D-2 measures the percentage of unique bigrams.</p>
</div>
<div id="A2.p4" class="ltx_para ltx_noindent">
<p id="A2.p4.1" class="ltx_p"><span id="A2.p4.1.1" class="ltx_text ltx_font_bold">METEOR</span> <cite class="ltx_cite ltx_citemacro_cite">Banerjee and Lavie (<a href="#bib.bib5" title="" class="ltx_ref">2005</a>)</cite> (Metric for Evaluation of Translation with Explicit Ordering) evaluates the quality of generated response by computing the alignment-based precision and recall between the generated output and the ground truth, considering synonyms and paraphrases.</p>
</div>
<div id="A2.p5" class="ltx_para ltx_noindent">
<p id="A2.p5.1" class="ltx_p"><span id="A2.p5.1.1" class="ltx_text ltx_font_bold">F1</span> <cite class="ltx_cite ltx_citemacro_cite">Banerjee and Lavie (<a href="#bib.bib5" title="" class="ltx_ref">2005</a>)</cite> combines the accuracy of the generated response (precision) and the coverage of the relevant response (recall). It provides a balanced measure of how well the model performs in generating relevant and accurate responses.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.07866" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.07867" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.07867">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.07867" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.07868" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 19:35:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
