<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.02565] EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS</title><meta property="og:description" content="Continuous speech can be converted into a discrete sequence by deriving discrete units from the hidden features of self-supervised learned (SSL) speech models.
Although SSL models are becoming larger and trained on mor…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.02565">

<!--Generated on Sat Oct  5 22:37:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Continuous speech can be converted into a discrete sequence by deriving discrete units from the hidden features of self-supervised learned (SSL) speech models.
Although SSL models are becoming larger and trained on more data, they are often sensitive to real-life distortions like additive noise or reverberation, which translates to a shift in discrete units.
We propose a parameter-efficient approach to generate noise-robust discrete units from pre-trained SSL models by training a small encoder-decoder model, with or without adapters, to simultaneously denoise and discretise the hidden features of the SSL model.
The model learns to generate a clean discrete sequence for a noisy utterance, conditioned on the SSL features.
The proposed denoiser outperforms several pre-training methods on the tasks of noisy discretisation and noisy speech recognition, and can be finetuned to the target environment with a few recordings of unlabeled target data.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
Discrete units, noise robustness, self-supervised learning, speech recognition</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Self-supervised learning (SSL) has enabled the development of versatile speech models which have advanced the state-of-the-art on a wide array of speech processing tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
Pre-training speech models on large amounts of unlabeled data leads to better generalisation capabilities and an improved robustness against acoustic, speaker and language variations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
Furthermore, within an SSL model, different layers are able to capture various speech attributes without supervision, such as phones, word boundaries and speaker characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
Depending on the application, the hidden states of the most suited layers of the SSL model can be chosen as inputs for a task-specific model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
In automatic speech recognition (ASR), great improvements have been observed by self-supervised pre-training of large multi-purpose speech models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Most SSL methods rely on quantisation or clustering to guide the training towards discovering meaningful and distinct speech units <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Recently, there has been a growing interest in extracting discrete units from self-supervised models, as they have several advantages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
First, the conversion from waveforms or feature vectors to discrete units facilitates a strong compression which allows efficient model training, fast inference and low-cost storage.
Second, temporal clustering of granular features aids the discovery of acoustic units which are strongly correlated with the content of spoken language <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
Third and finally, discretisation allows the integration with natural language processing techniques and models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
The discrete units can be treated as a pseudo-language to pre-train speech decoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and unit language models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> for spoken language processing.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, despite their impressive performance, SSL models still exhibit sensitivity to shifting domains.
This effect has been observed for changing acoustic and linguistic conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, unseen speaker accents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, and noisy environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
Besides a drop in performance, this has implications for discretisation.
For example, HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> was observed to assign clusters given by each noise condition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
As additive noise and reverberation shift the SSL model’s features, the extracted discrete sequence is strongly dependent on the acoustic conditions, which impacts the performance of decoders and unit language models that are trained on clean data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This work focuses on the robustness of discrete SSL units to distorted speech in noisy and reverberant environments, which is relevant for many applications in real-world scenarios.
As SSL foundation models are becoming larger, pre-training noise-robust models from scratch or adapting SSL models to new domains forms a large burden on computing resources.
Therefore, there is a strong need for small and versatile models that can adapt the SSL features to distortions.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We propose a parameter-efficient denoiser to extract robust discrete units in noisy environments.
The denoiser generates a clean cluster sequence from the latent features of a pre-trained SSL model for a distorted speech input, similar to a denoising auto-encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Notice that denoising is not merely mapping one unit to another, but also entails inserting or deleting units.
We investigate an external denoiser and an adapter-based denoiser.
The denoiser approach can be applied to any pre-trained model, and requires a relatively small amount of data to train.
Moreover, it does not require finetuning the SSL model itself on noisy data, and the backbone model (e.g. ASR, voice conversion model, unit language model) can be trained on discrete units extracted from clean data and does not need to be retrained with new clusters.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We evaluate the generated discrete units on a denoised unit prediction task and on a distorted speech recognition task, showing the benefits of the proposed method for several SSL models.
As the model is light-weight, we show that it can be efficiently adapted to new target environments.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.02565/assets/x1.png" id="S1.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="540" height="1080" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S1.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">Offline Clustering</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.02565/assets/x2.png" id="S1.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="540" height="1080" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S1.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">Denoiser</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.02565/assets/x3.png" id="S1.F1.sf3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="540" height="1080" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S1.F1.sf3.3.2" class="ltx_text" style="font-size:90%;">AdaDenoiser</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.02565/assets/x4.png" id="S1.F1.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="538" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S1.F1.sf4.3.2" class="ltx_text" style="font-size:90%;">ASR Model</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Model outline for a) offline target cluster extraction on clean data, b) Denoiser training on augmented data, c) AdaDenoiser training on augmented data, and d) discrete ASR modeling. In every schematic, the lighter blocks are frozen, and the darker blocks are trained.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Several works have investigated noise-robust pre-training of speech SSL models with synthesised noisy mixtures, mainly by constraining the outputs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> or the hidden features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> to match the clean outputs or representations.
For large SSL models, pre-training becomes very costly. This can be circumvented by noise-robust distillation into smaller models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> or by parameter-efficient adaptation through finetuning small adapter blocks, such as Houlsby adapters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, inserted in the SSL model.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> has investigated a technique to improve the augmentation invariance of discrete SSL units in the scope of generative spoken language modeling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
The outputs of an SSL model for an augmented input are mapped to the discrete clusters from a K-means teacher on the clean outputs, by training a new quantiser.
However, their approach has several drawbacks. First, the quantiser is limited to a 3-layer MLP.
Second, the quantiser that has to denoise the clusters is not conditioned on the acoustics (i.e. lower layer features in an SSL model), as it only uses the features of the K-means layer.
Therefore, it has no idea of the signal SNR and how much the clean signal was distorted.
Third, iterative re-training with new clusters is necessary.
Finally, their research focuses on the application of small cluster vocabularies and base SSL models for generative spoken language modeling, instead of the applications for ASR.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Our experiments with discretised speech units and the application to efficient ASR model training and inference follow recent success in this field <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">We aim to extract noise-robust and reverberation-robust discrete speech units from SSL models by training a denoiser model on top of a frozen pre-trained SSL encoder to generate clean discrete units for an augmented input.
First, discrete units are computed for a clean speech dataset using a quantisation mechanism of choice, e.g. K-means, as shown in Figure <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>.
Second, the denoising task is learned on an augmented dataset created by adding noises and reverberation to a small dataset of clean speech.
The denoiser can be external (Figure <a href="#S1.F1.sf2" title="In Figure 1 ‣ 1 Introduction ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>) or contain additional adapter blocks in the SSL encoder (Figure <a href="#S1.F1.sf3" title="In Figure 1 ‣ 1 Introduction ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(c)</span></a>).
Finally, for ASR applications, a noisy speech sample is discretised using the trained denoiser and fed to the discrete ASR model, depicted in Figure <a href="#S1.F1.sf4" title="In Figure 1 ‣ 1 Introduction ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(d)</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Self-Supervised Speech Representation Learning</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">While SSL is a very broad field spanning many research efforts in speech processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, we apply our method to three well-known speech models, namely HuBERT, WavLM and Wav2vec2.
First, HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> learns speech representations by applying masked language modeling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to speech features.
Discrete acoustic units are discovered by offline clustering of MFCC features or the features of a previously pre-trained model.
Then, these audio tokens are predicted with a bidirectional encoder conditioned on masked latent speech features.
In WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> this paradigm is further applied to noisy data and overlapping speech, by predicting clean target tokens for a noisy mixture.
Finally, Wav2vec2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> predicts quantised latent features for masked audio frames with a context network, leveraging contrastive learning techniques.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Discrete Speech Units</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Quantisation of speech can be part of the SSL training scheme, e.g. in wav2vec models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
However, the codebook during training is often much larger than necessary for downstream tasks.
On the other hand, HuBERT-like models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> have shown that offline clustering of hidden layer features leads to informative discrete speech units.
In this work, we apply K-means clustering to the layer of the SSL model that is most informative for word information and performs best on a downstream ASR task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
The clustering model is learned on a small fraction of clean speech and
the chosen cluster vocabulary size depends on the size and output dimension of the SSL model.
We reduce the cluster sequence length by deduplication, i.e. removing repetitions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
Offline clustering is depicted in Figure <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Noise-Aware Model Adaptation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> has addressed the domain shift between pre-training and target domains by continual pre-training of the SSL model on target data.
As HuBERT and Wav2vec2 models are trained on clean speech, adaptation to distorted speech data improves performance in noisy and reverberant environments.
To this end, as a baseline, we adapt pre-trained HuBERT and Wav2vec2 models by continuing the pre-training process on augmented data.
For HuBERT, the K-means quantiser of the pre-trained model is used to generate the target clusters for the augmented dataset.
We denote continual pre-training as <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">COPT</span>.
Moreover, HuBERT can be optimised directly to perform the denoising task, by predicting the clean clusters for an augmented sample (cf. WavLM).
We abbreviate this noise-invariant pre-training as <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">NIPT</span>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">These methods have two main drawbacks.
First, they require the SSL models to be updated entirely, which is computationally expensive.
Second, the adaptation can shift the hidden features, such that the quantiser of a pre-trained ASR model, trained on clean data, might be suboptimal.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Efficient Learning of Robust Discrete Speech Units</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We propose a light-weight external denoiser module that is able to reduce the effects of additive noise and reverberation during discretisation of pre-trained SSL features, without requiring adaptation of the full SSL model.
The module learns to simultaneously denoise the features and generate discrete units.
Given a mixed speech signal, the denoiser encoder processes the hidden SSL features and an autoregressive decoder predicts the discrete cluster sequence of the clean speech.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">In SSL models, the lower layers tend to correlate with acoustic factors (speaker, environment, domain, etc.), while the upper layers are more aligned with phonetic, lexical and semantic features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
Therefore, the hidden features of all layers in the SSL encoder are combined with a learnable weighted sum <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, such that the denoising of the discrete clusters is conditioned on the low-level acoustics as well.
The computed weighted sum is then linearly projected to a smaller dimension for efficient modeling.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">The encoder processes and denoises the features, and the decoder generates a discrete cluster sequence autoregressively by attending to the encoder outputs.
The decoder has to predict <span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_italic">deduplicated</span> cluster units, which makes the model robust against reverberation, compared to an approach with frame-based cluster targets (as in HuBERT adaptation).
Therefore, the sequence lengths of the input and output are not the same, which is why an encoder-decoder approach with CTC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> encoder regularisation is used for the denoiser.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">The outlined method is called <span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_italic">Denoiser</span> and depicted in Figure <a href="#S1.F1.sf2" title="In Figure 1 ‣ 1 Introduction ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>.
It is a completely external module that only requires the hidden features of the SSL model, which can be extracted before training.
Inspired by wide success in efficient model adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, we also investigate a denoiser model that additionally has small residual adapter blocks inserted in the layers of the frozen SSL model.
This method, denoted <span id="S3.SS4.p4.1.2" class="ltx_text ltx_font_italic">AdaDenoiser</span> and depicted in Figure <a href="#S1.F1.sf3" title="In Figure 1 ‣ 1 Introduction ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(c)</span></a>, has the advantages that it can adapt the SSL features directly (i.e. before the weighted sum) and the burden of the denoiser encoder is reduced, but it requires loading the SSL model during training.
We use Houlsby adapters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, consisting of a down-projection to a bottleneck dimension, a non-linearity and an up-projection to the original feature dimension.
The adapters are inserted after the feed-forward layers in the SSL encoder.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We use the <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">train-clean-100</span>, <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">dev-clean</span> and <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">test-clean</span> splits of LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> for the experiments.
The data configuration and training setup closely follow the clustering and ASR modeling setup from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, implemented in ESPnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Offline Clustering</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The K-means model using pre-trained clean SSL features is trained on a randomly selected 30 percent of LibriSpeech <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">train-clean-100</span>.
Based on previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, for HuBERT and WavLM, we learn K=500 cluster centroids from layer 9 features for base models, and K=2000 centroids from layer 21 features for large models.
For Wav2vec2, we use layer 7 for the base model and layer 12 for the large variant.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Data Augmentation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For adaptation to noisy data, we use the same 30 hours data split as used to train the K-means model for a fair comparison.
For every utterance, we make 5 different versions: the original clean utterance, a reverberated utterance, and 3 noisy utterances with different noise types. In total this gives a training set of 150 hours or 42k utterances.
For reverberation, we sample real RIRs from the openSLR28 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> which contains impulse responses from the Aachen IR and RWCP datasets and the REVERB challenge.
For additive noise, for every utterance we sample one noise segment from the DEMAND dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, one from MUSAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and one from CHIME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.
The noises are added to the clean audio with a random SNR between 0 and 20 dB.
For validation, we sample 1000 utterances from <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">dev-clean</span> and create 4 augmentations per sample.
For evaluation, we create <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">test-clean-augmented</span> by sampling 100 utterances from <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">test-clean</span> and generating 13 augmentations per utterance, including 1 reverberated and 12 noisy versions, by mixing with a noise sample from the three noise datasets at SNRs of [5,10,15,20] dB.
Including the clean samples, this gives 1400 test utterances.
The RIRs and noise types for evaluation are unseen during training.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>SSL Baselines</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The HuBERT and Wav2vec2 base models were pre-trained on 960h LibriSpeech and their large variants on 60kh LibriLight. WavLM was trained on augmented data.
The base models have 12 Transformer layers (95M parameters) and the large models have 24 Transformer layers (316M parameters).
In the adaptation experiments, we continue the pre-training of pre-trained HuBERT models in <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">fairseq</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> for 150k steps with 300K tokens per batch and a learning rate of 2e-5 with polynomial decay and 20k warmup steps, on the data from Section <a href="#S4.SS2" title="4.2 Data Augmentation ‣ 4 Experimental Setup ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
In case of training only residual adapters for NIPT adaptation (<span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_italic">AdaNIPT</span>), the learning rate is 1e-3, and the bottleneck dimension of the adapters is set to 64 or 1024 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Denoiser Setup</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The denoiser module is trained on the augmented dataset from Section <a href="#S4.SS2" title="4.2 Data Augmentation ‣ 4 Experimental Setup ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
The inputs are a learned weighted sum of the hidden features of the (frozen) SSL model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, which are linearly mapped to a smaller dimension of 256.
The Denoiser is implemented in ESPnet as a hybrid CTC/Attention encoder-decoder model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, where the encoder is regularised with a CTC objective and the decoder is trained with a cross-entropy loss on target tokens.
The model consists of an encoder with either 2 Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> layers (<span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_italic">Denoiser-S</span>) or 6 Transformer layers (<span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_italic">Denoiser-M</span>) and a 3-layer Transformer decoder.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">For the AdaDenoiser method, the encoders are smaller.
For small SSL models, we use <span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_italic">AdaDenoiser-S</span>, which has no additional encoder, but only retains the linear down-projection layer after the weighted sum as encoder.
For large SSL models, the additional encoder consists of 2 Transformer layers (<span id="S4.SS4.p2.1.2" class="ltx_text ltx_font_italic">AdaDenoiser-M</span>).
The decoder remains the same as in the Denoiser with 3 Transformer layers.
The adapters have a bottleneck dimension of 64 and GELU non-linearity.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">Convolutional subsampling (even with a factor 2) of the input features was found to be disadvantageous for the denoising task.
Temporal input feature masking did not improve the denoiser either.
The denoiser models are trained for 50 epochs with an effective batch size of 256 utterances, and a learning rate of 1e-3 with 5k warmup steps and exponential decay.
The targets for the denoiser models are deduplicated cluster indices.
The predicted clean clusters are decoded with a beam size of 20 and a CTC-weight of 0.3.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>ASR Modeling</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">The ASR model follows <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and consists of a hybrid CTC/Attention encoder-decoder model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, with a 12 layer E-Branchformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> encoder and 6 layer Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> decoder.
The discrete inputs are deduplicated cluster indices.
The target transcriptions are tokenised with 6000 BPE tokens.
We found that especially for noisy ASR, applying BPE modeling to input cluster units does not improve performance.
The discrete inputs are converted into 512-dimensional embeddings, followed by random temporal masking, convolutional subsampling, and then fed to the encoder.
The ASR model is trained on LibriSpeech <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_italic">train-clean-100</span> for 300 epochs with a learning rate of 5e-4 with decay and 5K warmup steps.
The model has 38M parameters and requires 12 GPU hours to train.
Decoding is performed with beam size 20 and CTC weight 0.3, without language model.
The ASR model is trained on discrete units extracted from clean data (<span id="S4.SS5.p1.1.2" class="ltx_text ltx_font_italic">train-clean-100</span>) and is not retrained after adaptation of the discrete units and SSL models (cf. Section <a href="#S5.SS2" title="5.2 Robust ASR ‣ 5 Results ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>).</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Robust Discrete Units</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">Augmentation-invariant discrete speech units should not depend on the presence of noise or reverberation.
In this experiment, we investigate the sensitivity of SSL model discretisation to augmentations by evaluating the <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">Unit Error Rate (UER)</span> between the discrete cluster units extracted from a clean signal with the SSL model and the clusters extracted from distorted versions of the same signal with adapted models.
The UER is computed as the Character Error Rate between the two sequences, treating the discrete units as characters.
We evaluate our approach on HuBERT and Wav2vec2, which were pre-trained on clean data only, and on WavLM, which was pre-trained to perform denoising on augmented data, and compare to the other adaptation strategies.
For all methods, the quantiser is trained on features of the backbone pre-trained SSL model.
Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Robust Discrete Units ‣ 5 Results ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The proposed Denoiser and AdaDenoiser reduce the UER for all models and baselines, with only a fraction of the total model parameters, indicating that the shift between noisy discrete units and clean discrete units is reduced for the same spoken sentence.
Overall, the small Denoiser model is more effective than the larger Denoiser model for UER reduction.
In most cases, for base variants of SSL models the AdaDenoiser outperforms the Denoiser, except for WavLM.
Adapters seem to have less effect for WavLM, which was trained on augmented data.
For WavLM large, the AdaDenoiser approach did not converge to a useful result and probably requires a different architecture to be optimal.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">For HuBERT, the strong improvements on the reverberated test set over the adaptation baselines suggest that the CTC or Attention objective with deduplicated units is more effective than frame-wise denoising for the case of reverberation.
For Wav2vec2, the high UERs in the table indicate that it is the most sensitive to noise-induced feature shifting.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Table 1</span>: </span><span id="S5.T1.4.2" class="ltx_text" style="font-size:90%;">UERs (%) for discrete speech units on <span id="S5.T1.4.2.1" class="ltx_text ltx_font_italic">test-clean-augmented</span>. The SSL models are either used as is or adapted as denoted in Column 2. The third column shows the number of adapted parameters. The UER is calculated per augmentation type: reverberation, additive noise at high SNR (15 or 20 dB, <span id="S5.T1.4.2.2" class="ltx_text ltx_font_italic">Noise-H</span>), noise at low SNR (5 or 10 dB, <span id="S5.T1.4.2.3" class="ltx_text ltx_font_italic">Noise-L</span>), or clean. The standard deviation of the UERs is estimated with a conservative binomial model as 0.1% for clean, 0.05% for noise and 0.2% for reverb. For WavLM large, which is already capable of denoising, we apply <span id="S5.T1.4.2.4" class="ltx_text ltx_font_italic">Denoiser M*</span> which has a smaller 2-layer Conformer encoder and a 4-layer Transformer decoder. The best result is in bold, the second best is underlined.
</span></figcaption>
<div id="S5.T1.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:643.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(34.9pt,-51.7pt) scale(1.19162582966057,1.19162582966057) ;">
<table id="S5.T1.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.5.1.1.1" class="ltx_tr">
<th id="S5.T1.5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt ltx_border_tt"><span id="S5.T1.5.1.1.1.1.1" class="ltx_text ltx_font_bold">SSL</span></th>
<th id="S5.T1.5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt ltx_border_tt"><span id="S5.T1.5.1.1.1.2.1" class="ltx_text ltx_font_bold">Adaptation</span></th>
<th id="S5.T1.5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt ltx_border_tt"><span id="S5.T1.5.1.1.1.3.1" class="ltx_text ltx_font_bold">#Par.</span></th>
<th id="S5.T1.5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_tt"><span id="S5.T1.5.1.1.1.4.1" class="ltx_text ltx_font_bold">Clean</span></th>
<th id="S5.T1.5.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_tt"><span id="S5.T1.5.1.1.1.5.1" class="ltx_text ltx_font_bold">Noise-H</span></th>
<th id="S5.T1.5.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_tt"><span id="S5.T1.5.1.1.1.6.1" class="ltx_text ltx_font_bold">Noise-L</span></th>
<th id="S5.T1.5.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_tt"><span id="S5.T1.5.1.1.1.7.1" class="ltx_text ltx_font_bold">Reverb</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.5.1.2.1" class="ltx_tr">
<th id="S5.T1.5.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S5.T1.5.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">/</th>
<th id="S5.T1.5.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">/</th>
<td id="S5.T1.5.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">0</td>
<td id="S5.T1.5.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">23.8</td>
<td id="S5.T1.5.1.2.1.6" class="ltx_td ltx_align_center ltx_border_tt">38.4</td>
<td id="S5.T1.5.1.2.1.7" class="ltx_td ltx_align_center ltx_border_tt">37.4</td>
</tr>
<tr id="S5.T1.5.1.3.2" class="ltx_tr">
<th id="S5.T1.5.1.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T1.5.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">COPT</th>
<th id="S5.T1.5.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">95M</th>
<td id="S5.T1.5.1.3.2.4" class="ltx_td ltx_align_center">14.4</td>
<td id="S5.T1.5.1.3.2.5" class="ltx_td ltx_align_center">24.9</td>
<td id="S5.T1.5.1.3.2.6" class="ltx_td ltx_align_center">33.7</td>
<td id="S5.T1.5.1.3.2.7" class="ltx_td ltx_align_center">35.4</td>
</tr>
<tr id="S5.T1.5.1.4.3" class="ltx_tr">
<th id="S5.T1.5.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">HuBERT</th>
<th id="S5.T1.5.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">NIPT</th>
<th id="S5.T1.5.1.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">95M</th>
<td id="S5.T1.5.1.4.3.4" class="ltx_td ltx_align_center">15.4</td>
<td id="S5.T1.5.1.4.3.5" class="ltx_td ltx_align_center">24.0</td>
<td id="S5.T1.5.1.4.3.6" class="ltx_td ltx_align_center">31.4</td>
<td id="S5.T1.5.1.4.3.7" class="ltx_td ltx_align_center">34.3</td>
</tr>
<tr id="S5.T1.5.1.5.4" class="ltx_tr">
<th id="S5.T1.5.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">base</th>
<th id="S5.T1.5.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (S)</th>
<th id="S5.T1.5.1.5.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8M</th>
<td id="S5.T1.5.1.5.4.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.5.4.4.1" class="ltx_text ltx_font_bold">13.3</span></td>
<td id="S5.T1.5.1.5.4.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.5.4.5.1" class="ltx_text ltx_font_bold">21.1</span></td>
<td id="S5.T1.5.1.5.4.6" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.5.4.6.1" class="ltx_text ltx_framed ltx_framed_underline">27.3</span></td>
<td id="S5.T1.5.1.5.4.7" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.5.4.7.1" class="ltx_text ltx_framed ltx_framed_underline">25.9</span></td>
</tr>
<tr id="S5.T1.5.1.6.5" class="ltx_tr">
<th id="S5.T1.5.1.6.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T1.5.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (M)</th>
<th id="S5.T1.5.1.6.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10M</th>
<td id="S5.T1.5.1.6.5.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.6.5.4.1" class="ltx_text ltx_framed ltx_framed_underline">14.2</span></td>
<td id="S5.T1.5.1.6.5.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.6.5.5.1" class="ltx_text ltx_framed ltx_framed_underline">21.5</span></td>
<td id="S5.T1.5.1.6.5.6" class="ltx_td ltx_align_center">28.4</td>
<td id="S5.T1.5.1.6.5.7" class="ltx_td ltx_align_center">26.8</td>
</tr>
<tr id="S5.T1.5.1.7.6" class="ltx_tr">
<th id="S5.T1.5.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T1.5.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaDenoiser (S)</th>
<th id="S5.T1.5.1.7.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">7M</th>
<td id="S5.T1.5.1.7.6.4" class="ltx_td ltx_align_center">15.0</td>
<td id="S5.T1.5.1.7.6.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.7.6.5.1" class="ltx_text ltx_font_bold">21.1</span></td>
<td id="S5.T1.5.1.7.6.6" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.7.6.6.1" class="ltx_text ltx_font_bold">26.0</span></td>
<td id="S5.T1.5.1.7.6.7" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.7.6.7.1" class="ltx_text ltx_font_bold">25.5</span></td>
</tr>
<tr id="S5.T1.5.1.8.7" class="ltx_tr">
<th id="S5.T1.5.1.8.7.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S5.T1.5.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">/</th>
<th id="S5.T1.5.1.8.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">/</th>
<td id="S5.T1.5.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S5.T1.5.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t">25.1</td>
<td id="S5.T1.5.1.8.7.6" class="ltx_td ltx_align_center ltx_border_t">39.8</td>
<td id="S5.T1.5.1.8.7.7" class="ltx_td ltx_align_center ltx_border_t">39.3</td>
</tr>
<tr id="S5.T1.5.1.9.8" class="ltx_tr">
<th id="S5.T1.5.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T1.5.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">NIPT</th>
<th id="S5.T1.5.1.9.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">316M</th>
<td id="S5.T1.5.1.9.8.4" class="ltx_td ltx_align_center">13.5</td>
<td id="S5.T1.5.1.9.8.5" class="ltx_td ltx_align_center">24.6</td>
<td id="S5.T1.5.1.9.8.6" class="ltx_td ltx_align_center">33.4</td>
<td id="S5.T1.5.1.9.8.7" class="ltx_td ltx_align_center">38.7</td>
</tr>
<tr id="S5.T1.5.1.10.9" class="ltx_tr">
<th id="S5.T1.5.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">HuBERT</th>
<th id="S5.T1.5.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaNIPT (1024d)</th>
<th id="S5.T1.5.1.10.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">51M</th>
<td id="S5.T1.5.1.10.9.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.10.9.4.1" class="ltx_text ltx_framed ltx_framed_underline">8.2</span></td>
<td id="S5.T1.5.1.10.9.5" class="ltx_td ltx_align_center">22.9</td>
<td id="S5.T1.5.1.10.9.6" class="ltx_td ltx_align_center">35.2</td>
<td id="S5.T1.5.1.10.9.7" class="ltx_td ltx_align_center">38.9</td>
</tr>
<tr id="S5.T1.5.1.11.10" class="ltx_tr">
<th id="S5.T1.5.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">large</th>
<th id="S5.T1.5.1.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaNIPT (64d)</th>
<th id="S5.T1.5.1.11.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4.5M</th>
<td id="S5.T1.5.1.11.10.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.11.10.4.1" class="ltx_text ltx_font_bold">7.4</span></td>
<td id="S5.T1.5.1.11.10.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.11.10.5.1" class="ltx_text ltx_framed ltx_framed_underline">22.7</span></td>
<td id="S5.T1.5.1.11.10.6" class="ltx_td ltx_align_center">35.3</td>
<td id="S5.T1.5.1.11.10.7" class="ltx_td ltx_align_center">38.6</td>
</tr>
<tr id="S5.T1.5.1.12.11" class="ltx_tr">
<th id="S5.T1.5.1.12.11.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T1.5.1.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (S)</th>
<th id="S5.T1.5.1.12.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8M</th>
<td id="S5.T1.5.1.12.11.4" class="ltx_td ltx_align_center">11.9</td>
<td id="S5.T1.5.1.12.11.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.12.11.5.1" class="ltx_text ltx_font_bold">22.0</span></td>
<td id="S5.T1.5.1.12.11.6" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.12.11.6.1" class="ltx_text ltx_framed ltx_framed_underline">27.9</span></td>
<td id="S5.T1.5.1.12.11.7" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.12.11.7.1" class="ltx_text ltx_framed ltx_framed_underline">26.9</span></td>
</tr>
<tr id="S5.T1.5.1.13.12" class="ltx_tr">
<th id="S5.T1.5.1.13.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T1.5.1.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (M)</th>
<th id="S5.T1.5.1.13.12.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10M</th>
<td id="S5.T1.5.1.13.12.4" class="ltx_td ltx_align_center">12.9</td>
<td id="S5.T1.5.1.13.12.5" class="ltx_td ltx_align_center">23.0</td>
<td id="S5.T1.5.1.13.12.6" class="ltx_td ltx_align_center">29.4</td>
<td id="S5.T1.5.1.13.12.7" class="ltx_td ltx_align_center">27.5</td>
</tr>
<tr id="S5.T1.5.1.14.13" class="ltx_tr">
<th id="S5.T1.5.1.14.13.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T1.5.1.14.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaDenoiser (M)</th>
<th id="S5.T1.5.1.14.13.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">11M</th>
<td id="S5.T1.5.1.14.13.4" class="ltx_td ltx_align_center">15.4</td>
<td id="S5.T1.5.1.14.13.5" class="ltx_td ltx_align_center">23.1</td>
<td id="S5.T1.5.1.14.13.6" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.14.13.6.1" class="ltx_text ltx_font_bold">27.5</span></td>
<td id="S5.T1.5.1.14.13.7" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.14.13.7.1" class="ltx_text ltx_font_bold">26.6</span></td>
</tr>
<tr id="S5.T1.5.1.15.14" class="ltx_tr">
<th id="S5.T1.5.1.15.14.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S5.T1.5.1.15.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">/</th>
<th id="S5.T1.5.1.15.14.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">/</th>
<td id="S5.T1.5.1.15.14.4" class="ltx_td ltx_align_center ltx_border_tt">0</td>
<td id="S5.T1.5.1.15.14.5" class="ltx_td ltx_align_center ltx_border_tt">28.1</td>
<td id="S5.T1.5.1.15.14.6" class="ltx_td ltx_align_center ltx_border_tt">39.0</td>
<td id="S5.T1.5.1.15.14.7" class="ltx_td ltx_align_center ltx_border_tt">39.0</td>
</tr>
<tr id="S5.T1.5.1.16.15" class="ltx_tr">
<th id="S5.T1.5.1.16.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">WavLM</th>
<th id="S5.T1.5.1.16.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (S)</th>
<th id="S5.T1.5.1.16.15.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8M</th>
<td id="S5.T1.5.1.16.15.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.16.15.4.1" class="ltx_text ltx_font_bold">15.6</span></td>
<td id="S5.T1.5.1.16.15.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.16.15.5.1" class="ltx_text ltx_font_bold">24.9</span></td>
<td id="S5.T1.5.1.16.15.6" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.16.15.6.1" class="ltx_text ltx_font_bold">29.9</span></td>
<td id="S5.T1.5.1.16.15.7" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.16.15.7.1" class="ltx_text ltx_framed ltx_framed_underline">29.9</span></td>
</tr>
<tr id="S5.T1.5.1.17.16" class="ltx_tr">
<th id="S5.T1.5.1.17.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">base</th>
<th id="S5.T1.5.1.17.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (M)</th>
<th id="S5.T1.5.1.17.16.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10M</th>
<td id="S5.T1.5.1.17.16.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.17.16.4.1" class="ltx_text ltx_framed ltx_framed_underline">16.4</span></td>
<td id="S5.T1.5.1.17.16.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.17.16.5.1" class="ltx_text ltx_framed ltx_framed_underline">25.3</span></td>
<td id="S5.T1.5.1.17.16.6" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.17.16.6.1" class="ltx_text ltx_framed ltx_framed_underline">30.3</span></td>
<td id="S5.T1.5.1.17.16.7" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.17.16.7.1" class="ltx_text ltx_font_bold">29.8</span></td>
</tr>
<tr id="S5.T1.5.1.18.17" class="ltx_tr">
<th id="S5.T1.5.1.18.17.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T1.5.1.18.17.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaDenoiser (S)</th>
<th id="S5.T1.5.1.18.17.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">7M</th>
<td id="S5.T1.5.1.18.17.4" class="ltx_td ltx_align_center">23.2</td>
<td id="S5.T1.5.1.18.17.5" class="ltx_td ltx_align_center">28.3</td>
<td id="S5.T1.5.1.18.17.6" class="ltx_td ltx_align_center">32.4</td>
<td id="S5.T1.5.1.18.17.7" class="ltx_td ltx_align_center">31.6</td>
</tr>
<tr id="S5.T1.5.1.19.18" class="ltx_tr">
<th id="S5.T1.5.1.19.18.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">WavLM</th>
<th id="S5.T1.5.1.19.18.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">/</th>
<th id="S5.T1.5.1.19.18.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">/</th>
<td id="S5.T1.5.1.19.18.4" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S5.T1.5.1.19.18.5" class="ltx_td ltx_align_center ltx_border_t">24.9</td>
<td id="S5.T1.5.1.19.18.6" class="ltx_td ltx_align_center ltx_border_t">35.3</td>
<td id="S5.T1.5.1.19.18.7" class="ltx_td ltx_align_center ltx_border_t">42.2</td>
</tr>
<tr id="S5.T1.5.1.20.19" class="ltx_tr">
<th id="S5.T1.5.1.20.19.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">large</th>
<th id="S5.T1.5.1.20.19.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (S)</th>
<th id="S5.T1.5.1.20.19.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8M</th>
<td id="S5.T1.5.1.20.19.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.20.19.4.1" class="ltx_text ltx_font_bold">11.8</span></td>
<td id="S5.T1.5.1.20.19.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.20.19.5.1" class="ltx_text ltx_font_bold">22.8</span></td>
<td id="S5.T1.5.1.20.19.6" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.20.19.6.1" class="ltx_text ltx_font_bold">27.4</span></td>
<td id="S5.T1.5.1.20.19.7" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.20.19.7.1" class="ltx_text ltx_framed ltx_framed_underline">29.0</span></td>
</tr>
<tr id="S5.T1.5.1.21.20" class="ltx_tr">
<th id="S5.T1.5.1.21.20.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T1.5.1.21.20.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (M*)</th>
<th id="S5.T1.5.1.21.20.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10M</th>
<td id="S5.T1.5.1.21.20.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.21.20.4.1" class="ltx_text ltx_framed ltx_framed_underline">12.1</span></td>
<td id="S5.T1.5.1.21.20.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.21.20.5.1" class="ltx_text ltx_framed ltx_framed_underline">22.9</span></td>
<td id="S5.T1.5.1.21.20.6" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.21.20.6.1" class="ltx_text ltx_framed ltx_framed_underline">27.6</span></td>
<td id="S5.T1.5.1.21.20.7" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.21.20.7.1" class="ltx_text ltx_font_bold">28.9</span></td>
</tr>
<tr id="S5.T1.5.1.22.21" class="ltx_tr">
<th id="S5.T1.5.1.22.21.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S5.T1.5.1.22.21.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">/</th>
<th id="S5.T1.5.1.22.21.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">/</th>
<td id="S5.T1.5.1.22.21.4" class="ltx_td ltx_align_center ltx_border_tt">0</td>
<td id="S5.T1.5.1.22.21.5" class="ltx_td ltx_align_center ltx_border_tt">34.3</td>
<td id="S5.T1.5.1.22.21.6" class="ltx_td ltx_align_center ltx_border_tt">51.0</td>
<td id="S5.T1.5.1.22.21.7" class="ltx_td ltx_align_center ltx_border_tt">47.7</td>
</tr>
<tr id="S5.T1.5.1.23.22" class="ltx_tr">
<th id="S5.T1.5.1.23.22.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Wav2vec2</th>
<th id="S5.T1.5.1.23.22.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">COPT</th>
<th id="S5.T1.5.1.23.22.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">95M</th>
<td id="S5.T1.5.1.23.22.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.23.22.4.1" class="ltx_text ltx_font_bold">18.5</span></td>
<td id="S5.T1.5.1.23.22.5" class="ltx_td ltx_align_center">32.8</td>
<td id="S5.T1.5.1.23.22.6" class="ltx_td ltx_align_center">42.9</td>
<td id="S5.T1.5.1.23.22.7" class="ltx_td ltx_align_center">43.4</td>
</tr>
<tr id="S5.T1.5.1.24.23" class="ltx_tr">
<th id="S5.T1.5.1.24.23.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">base</th>
<th id="S5.T1.5.1.24.23.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (S)</th>
<th id="S5.T1.5.1.24.23.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8M</th>
<td id="S5.T1.5.1.24.23.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.24.23.4.1" class="ltx_text ltx_framed ltx_framed_underline">18.6</span></td>
<td id="S5.T1.5.1.24.23.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.24.23.5.1" class="ltx_text ltx_framed ltx_framed_underline">29.6</span></td>
<td id="S5.T1.5.1.24.23.6" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.24.23.6.1" class="ltx_text ltx_framed ltx_framed_underline">36.3</span></td>
<td id="S5.T1.5.1.24.23.7" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.24.23.7.1" class="ltx_text ltx_framed ltx_framed_underline">34.2</span></td>
</tr>
<tr id="S5.T1.5.1.25.24" class="ltx_tr">
<th id="S5.T1.5.1.25.24.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T1.5.1.25.24.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (M)</th>
<th id="S5.T1.5.1.25.24.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10M</th>
<td id="S5.T1.5.1.25.24.4" class="ltx_td ltx_align_center">20.8</td>
<td id="S5.T1.5.1.25.24.5" class="ltx_td ltx_align_center">30.6</td>
<td id="S5.T1.5.1.25.24.6" class="ltx_td ltx_align_center">37.5</td>
<td id="S5.T1.5.1.25.24.7" class="ltx_td ltx_align_center">34.8</td>
</tr>
<tr id="S5.T1.5.1.26.25" class="ltx_tr">
<th id="S5.T1.5.1.26.25.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T1.5.1.26.25.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaDenoiser (S)</th>
<th id="S5.T1.5.1.26.25.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">7M</th>
<td id="S5.T1.5.1.26.25.4" class="ltx_td ltx_align_center">20.5</td>
<td id="S5.T1.5.1.26.25.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.26.25.5.1" class="ltx_text ltx_font_bold">29.3</span></td>
<td id="S5.T1.5.1.26.25.6" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.26.25.6.1" class="ltx_text ltx_font_bold">34.4</span></td>
<td id="S5.T1.5.1.26.25.7" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.26.25.7.1" class="ltx_text ltx_font_bold">33.8</span></td>
</tr>
<tr id="S5.T1.5.1.27.26" class="ltx_tr">
<th id="S5.T1.5.1.27.26.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S5.T1.5.1.27.26.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">/</th>
<th id="S5.T1.5.1.27.26.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">/</th>
<td id="S5.T1.5.1.27.26.4" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S5.T1.5.1.27.26.5" class="ltx_td ltx_align_center ltx_border_t">38.2</td>
<td id="S5.T1.5.1.27.26.6" class="ltx_td ltx_align_center ltx_border_t">57.1</td>
<td id="S5.T1.5.1.27.26.7" class="ltx_td ltx_align_center ltx_border_t">58.1</td>
</tr>
<tr id="S5.T1.5.1.28.27" class="ltx_tr">
<th id="S5.T1.5.1.28.27.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Wav2vec2</th>
<th id="S5.T1.5.1.28.27.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (S)</th>
<th id="S5.T1.5.1.28.27.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8M</th>
<td id="S5.T1.5.1.28.27.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.28.27.4.1" class="ltx_text ltx_font_bold">24.6</span></td>
<td id="S5.T1.5.1.28.27.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.28.27.5.1" class="ltx_text ltx_framed ltx_framed_underline">36.0</span></td>
<td id="S5.T1.5.1.28.27.6" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.28.27.6.1" class="ltx_text ltx_font_bold">43.1</span></td>
<td id="S5.T1.5.1.28.27.7" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.28.27.7.1" class="ltx_text ltx_font_bold">41.6</span></td>
</tr>
<tr id="S5.T1.5.1.29.28" class="ltx_tr">
<th id="S5.T1.5.1.29.28.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">large</th>
<th id="S5.T1.5.1.29.28.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (M)</th>
<th id="S5.T1.5.1.29.28.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10M</th>
<td id="S5.T1.5.1.29.28.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.29.28.4.1" class="ltx_text ltx_framed ltx_framed_underline">25.6</span></td>
<td id="S5.T1.5.1.29.28.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.29.28.5.1" class="ltx_text ltx_font_bold">35.9</span></td>
<td id="S5.T1.5.1.29.28.6" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.29.28.6.1" class="ltx_text ltx_framed ltx_framed_underline">43.7</span></td>
<td id="S5.T1.5.1.29.28.7" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.29.28.7.1" class="ltx_text ltx_framed ltx_framed_underline">41.7</span></td>
</tr>
<tr id="S5.T1.5.1.30.29" class="ltx_tr">
<th id="S5.T1.5.1.30.29.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_bb ltx_border_r"></th>
<th id="S5.T1.5.1.30.29.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_bb ltx_border_r">AdaDenoiser (M)</th>
<th id="S5.T1.5.1.30.29.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_bb ltx_border_r">11M</th>
<td id="S5.T1.5.1.30.29.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_bb">32.7</td>
<td id="S5.T1.5.1.30.29.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_bb">39.3</td>
<td id="S5.T1.5.1.30.29.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_bb">43.8</td>
<td id="S5.T1.5.1.30.29.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_bb">44.5</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Robust ASR</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">In this section, we analyse the effectiveness and robustness of the discrete clusters for ASR in noisy test environments.
To this end, we train a discrete ASR model on clean data (<span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">train-clean-100</span>) with the clusters from an SSL model, and evaluate on augmented data with the clusters from the noise-adapted models.
We found that retraining the ASR model on adapted units does not have a significant benefit over using ASR models trained on clean SSL units, hence the ASR model can be trained on clean data and the adaptation only uses unlabeled data.
Table <a href="#S5.T2" title="Table 2 ‣ 5.2 Robust ASR ‣ 5 Results ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the Word Error Rates (WER).</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.4.2" class="ltx_text" style="font-size:90%;">WERs (%) with an ASR model on <span id="S5.T2.4.2.1" class="ltx_text ltx_font_italic">test-clean-augmented</span>, using discrete inputs extracted from an SSL or adapted model. The WER is split up per augmentation type: reverberation, noise at high SNR (15 or 20 dB, <span id="S5.T2.4.2.2" class="ltx_text ltx_font_italic">Noise-H</span>), noise at low SNR (5 or 10 dB, <span id="S5.T2.4.2.3" class="ltx_text ltx_font_italic">Noise-L</span>), and clean. The standard deviation of the WER is estimated with a conservative binomial model as 0.4-0.5% for clean, 0.2-0.3% for noise and 0.5-0.7% for reverb. For WavLM large, which is already capable of denoising, we apply <span id="S5.T2.4.2.4" class="ltx_text ltx_font_italic">Denoiser M*</span> which has a smaller 2-layer Conformer encoder and a 4-layer Transformer decoder. The best result is in bold, the second best is underlined.</span></figcaption>
<div id="S5.T2.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:643.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(34.9pt,-51.7pt) scale(1.19162582966057,1.19162582966057) ;">
<table id="S5.T2.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.5.1.1.1" class="ltx_tr">
<th id="S5.T2.5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt ltx_border_tt"><span id="S5.T2.5.1.1.1.1.1" class="ltx_text ltx_font_bold">SSL</span></th>
<th id="S5.T2.5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt ltx_border_tt"><span id="S5.T2.5.1.1.1.2.1" class="ltx_text ltx_font_bold">Adaptation</span></th>
<th id="S5.T2.5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt ltx_border_tt"><span id="S5.T2.5.1.1.1.3.1" class="ltx_text ltx_font_bold">#Par.</span></th>
<th id="S5.T2.5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_tt"><span id="S5.T2.5.1.1.1.4.1" class="ltx_text ltx_font_bold">Clean</span></th>
<th id="S5.T2.5.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_tt"><span id="S5.T2.5.1.1.1.5.1" class="ltx_text ltx_font_bold">Noise-H</span></th>
<th id="S5.T2.5.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_tt"><span id="S5.T2.5.1.1.1.6.1" class="ltx_text ltx_font_bold">Noise-L</span></th>
<th id="S5.T2.5.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_tt"><span id="S5.T2.5.1.1.1.7.1" class="ltx_text ltx_font_bold">Reverb</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.5.1.2.1" class="ltx_tr">
<th id="S5.T2.5.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S5.T2.5.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">/</th>
<th id="S5.T2.5.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">/</th>
<td id="S5.T2.5.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">8.5</td>
<td id="S5.T2.5.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">9.8</td>
<td id="S5.T2.5.1.2.1.6" class="ltx_td ltx_align_center ltx_border_tt">20.0</td>
<td id="S5.T2.5.1.2.1.7" class="ltx_td ltx_align_center ltx_border_tt">16.9</td>
</tr>
<tr id="S5.T2.5.1.3.2" class="ltx_tr">
<th id="S5.T2.5.1.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T2.5.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">COPT</th>
<th id="S5.T2.5.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">95M</th>
<td id="S5.T2.5.1.3.2.4" class="ltx_td ltx_align_center">8.7</td>
<td id="S5.T2.5.1.3.2.5" class="ltx_td ltx_align_center">9.6</td>
<td id="S5.T2.5.1.3.2.6" class="ltx_td ltx_align_center">14.6</td>
<td id="S5.T2.5.1.3.2.7" class="ltx_td ltx_align_center">15.1</td>
</tr>
<tr id="S5.T2.5.1.4.3" class="ltx_tr">
<th id="S5.T2.5.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">HuBERT</th>
<th id="S5.T2.5.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">NIPT</th>
<th id="S5.T2.5.1.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">95M</th>
<td id="S5.T2.5.1.4.3.4" class="ltx_td ltx_align_center">9.2</td>
<td id="S5.T2.5.1.4.3.5" class="ltx_td ltx_align_center">9.2</td>
<td id="S5.T2.5.1.4.3.6" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.4.3.6.1" class="ltx_text ltx_framed ltx_framed_underline">13.6</span></td>
<td id="S5.T2.5.1.4.3.7" class="ltx_td ltx_align_center">14.5</td>
</tr>
<tr id="S5.T2.5.1.5.4" class="ltx_tr">
<th id="S5.T2.5.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">base</th>
<th id="S5.T2.5.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (S)</th>
<th id="S5.T2.5.1.5.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8M</th>
<td id="S5.T2.5.1.5.4.4" class="ltx_td ltx_align_center">9.1</td>
<td id="S5.T2.5.1.5.4.5" class="ltx_td ltx_align_center">9.7</td>
<td id="S5.T2.5.1.5.4.6" class="ltx_td ltx_align_center">15.1</td>
<td id="S5.T2.5.1.5.4.7" class="ltx_td ltx_align_center">13.3</td>
</tr>
<tr id="S5.T2.5.1.6.5" class="ltx_tr">
<th id="S5.T2.5.1.6.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T2.5.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (M)</th>
<th id="S5.T2.5.1.6.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10M</th>
<td id="S5.T2.5.1.6.5.4" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.6.5.4.1" class="ltx_text ltx_framed ltx_framed_underline">7.9</span></td>
<td id="S5.T2.5.1.6.5.5" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.6.5.5.1" class="ltx_text ltx_framed ltx_framed_underline">8.5</span></td>
<td id="S5.T2.5.1.6.5.6" class="ltx_td ltx_align_center">14.9</td>
<td id="S5.T2.5.1.6.5.7" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.6.5.7.1" class="ltx_text ltx_framed ltx_framed_underline">11.6</span></td>
</tr>
<tr id="S5.T2.5.1.7.6" class="ltx_tr">
<th id="S5.T2.5.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T2.5.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaDenoiser (S)</th>
<th id="S5.T2.5.1.7.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">7M</th>
<td id="S5.T2.5.1.7.6.4" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.7.6.4.1" class="ltx_text ltx_font_bold">7.8</span></td>
<td id="S5.T2.5.1.7.6.5" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.7.6.5.1" class="ltx_text ltx_font_bold">8.2</span></td>
<td id="S5.T2.5.1.7.6.6" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.7.6.6.1" class="ltx_text ltx_font_bold">11.4</span></td>
<td id="S5.T2.5.1.7.6.7" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.7.6.7.1" class="ltx_text ltx_font_bold">11.0</span></td>
</tr>
<tr id="S5.T2.5.1.8.7" class="ltx_tr">
<th id="S5.T2.5.1.8.7.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S5.T2.5.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">/</th>
<th id="S5.T2.5.1.8.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">/</th>
<td id="S5.T2.5.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t">5.6</td>
<td id="S5.T2.5.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.5.1.8.7.5.1" class="ltx_text ltx_framed ltx_framed_underline">5.8</span></td>
<td id="S5.T2.5.1.8.7.6" class="ltx_td ltx_align_center ltx_border_t">11.6</td>
<td id="S5.T2.5.1.8.7.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.5.1.8.7.7.1" class="ltx_text ltx_framed ltx_framed_underline">6.5</span></td>
</tr>
<tr id="S5.T2.5.1.9.8" class="ltx_tr">
<th id="S5.T2.5.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T2.5.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">NIPT</th>
<th id="S5.T2.5.1.9.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">316M</th>
<td id="S5.T2.5.1.9.8.4" class="ltx_td ltx_align_center">5.5</td>
<td id="S5.T2.5.1.9.8.5" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.9.8.5.1" class="ltx_text ltx_font_bold">5.7</span></td>
<td id="S5.T2.5.1.9.8.6" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.9.8.6.1" class="ltx_text ltx_framed ltx_framed_underline">7.5</span></td>
<td id="S5.T2.5.1.9.8.7" class="ltx_td ltx_align_center">6.8</td>
</tr>
<tr id="S5.T2.5.1.10.9" class="ltx_tr">
<th id="S5.T2.5.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">HuBERT</th>
<th id="S5.T2.5.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaNIPT (1024d)</th>
<th id="S5.T2.5.1.10.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">51M</th>
<td id="S5.T2.5.1.10.9.4" class="ltx_td ltx_align_center">5.7</td>
<td id="S5.T2.5.1.10.9.5" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.10.9.5.1" class="ltx_text ltx_framed ltx_framed_underline">5.8</span></td>
<td id="S5.T2.5.1.10.9.6" class="ltx_td ltx_align_center">9.5</td>
<td id="S5.T2.5.1.10.9.7" class="ltx_td ltx_align_center">6.8</td>
</tr>
<tr id="S5.T2.5.1.11.10" class="ltx_tr">
<th id="S5.T2.5.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">large</th>
<th id="S5.T2.5.1.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaNIPT (64d)</th>
<th id="S5.T2.5.1.11.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4.5M</th>
<td id="S5.T2.5.1.11.10.4" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.11.10.4.1" class="ltx_text ltx_font_bold">5.3</span></td>
<td id="S5.T2.5.1.11.10.5" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.11.10.5.1" class="ltx_text ltx_font_bold">5.7</span></td>
<td id="S5.T2.5.1.11.10.6" class="ltx_td ltx_align_center">9.8</td>
<td id="S5.T2.5.1.11.10.7" class="ltx_td ltx_align_center">7.0</td>
</tr>
<tr id="S5.T2.5.1.12.11" class="ltx_tr">
<th id="S5.T2.5.1.12.11.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T2.5.1.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (S)</th>
<th id="S5.T2.5.1.12.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8M</th>
<td id="S5.T2.5.1.12.11.4" class="ltx_td ltx_align_center">6.1</td>
<td id="S5.T2.5.1.12.11.5" class="ltx_td ltx_align_center">6.4</td>
<td id="S5.T2.5.1.12.11.6" class="ltx_td ltx_align_center">9.8</td>
<td id="S5.T2.5.1.12.11.7" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.12.11.7.1" class="ltx_text ltx_framed ltx_framed_underline">6.5</span></td>
</tr>
<tr id="S5.T2.5.1.13.12" class="ltx_tr">
<th id="S5.T2.5.1.13.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T2.5.1.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (M)</th>
<th id="S5.T2.5.1.13.12.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10M</th>
<td id="S5.T2.5.1.13.12.4" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.13.12.4.1" class="ltx_text ltx_framed ltx_framed_underline">5.4</span></td>
<td id="S5.T2.5.1.13.12.5" class="ltx_td ltx_align_center">5.9</td>
<td id="S5.T2.5.1.13.12.6" class="ltx_td ltx_align_center">9.6</td>
<td id="S5.T2.5.1.13.12.7" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.13.12.7.1" class="ltx_text ltx_font_bold">6.1</span></td>
</tr>
<tr id="S5.T2.5.1.14.13" class="ltx_tr">
<th id="S5.T2.5.1.14.13.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T2.5.1.14.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaDenoiser (M)</th>
<th id="S5.T2.5.1.14.13.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">11M</th>
<td id="S5.T2.5.1.14.13.4" class="ltx_td ltx_align_center">6.0</td>
<td id="S5.T2.5.1.14.13.5" class="ltx_td ltx_align_center">6.0</td>
<td id="S5.T2.5.1.14.13.6" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.14.13.6.1" class="ltx_text ltx_font_bold">7.0</span></td>
<td id="S5.T2.5.1.14.13.7" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.14.13.7.1" class="ltx_text ltx_framed ltx_framed_underline">6.5</span></td>
</tr>
<tr id="S5.T2.5.1.15.14" class="ltx_tr">
<th id="S5.T2.5.1.15.14.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S5.T2.5.1.15.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">/</th>
<th id="S5.T2.5.1.15.14.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">/</th>
<td id="S5.T2.5.1.15.14.4" class="ltx_td ltx_align_center ltx_border_tt">8.5</td>
<td id="S5.T2.5.1.15.14.5" class="ltx_td ltx_align_center ltx_border_tt">9.4</td>
<td id="S5.T2.5.1.15.14.6" class="ltx_td ltx_align_center ltx_border_tt">15.4</td>
<td id="S5.T2.5.1.15.14.7" class="ltx_td ltx_align_center ltx_border_tt">14.3</td>
</tr>
<tr id="S5.T2.5.1.16.15" class="ltx_tr">
<th id="S5.T2.5.1.16.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">WavLM</th>
<th id="S5.T2.5.1.16.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (S)</th>
<th id="S5.T2.5.1.16.15.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8M</th>
<td id="S5.T2.5.1.16.15.4" class="ltx_td ltx_align_center">8.5</td>
<td id="S5.T2.5.1.16.15.5" class="ltx_td ltx_align_center">8.6</td>
<td id="S5.T2.5.1.16.15.6" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.16.15.6.1" class="ltx_text ltx_font_bold">11.8</span></td>
<td id="S5.T2.5.1.16.15.7" class="ltx_td ltx_align_center">11.5</td>
</tr>
<tr id="S5.T2.5.1.17.16" class="ltx_tr">
<th id="S5.T2.5.1.17.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">base</th>
<th id="S5.T2.5.1.17.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (M)</th>
<th id="S5.T2.5.1.17.16.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10M</th>
<td id="S5.T2.5.1.17.16.4" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.17.16.4.1" class="ltx_text ltx_font_bold">7.7</span></td>
<td id="S5.T2.5.1.17.16.5" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.17.16.5.1" class="ltx_text ltx_font_bold">8.2</span></td>
<td id="S5.T2.5.1.17.16.6" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.17.16.6.1" class="ltx_text ltx_font_bold">11.8</span></td>
<td id="S5.T2.5.1.17.16.7" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.17.16.7.1" class="ltx_text ltx_font_bold">10.9</span></td>
</tr>
<tr id="S5.T2.5.1.18.17" class="ltx_tr">
<th id="S5.T2.5.1.18.17.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T2.5.1.18.17.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaDenoiser (S)</th>
<th id="S5.T2.5.1.18.17.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">7M</th>
<td id="S5.T2.5.1.18.17.4" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.18.17.4.1" class="ltx_text ltx_framed ltx_framed_underline">8.2</span></td>
<td id="S5.T2.5.1.18.17.5" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.18.17.5.1" class="ltx_text ltx_framed ltx_framed_underline">8.3</span></td>
<td id="S5.T2.5.1.18.17.6" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.18.17.6.1" class="ltx_text ltx_font_bold">11.8</span></td>
<td id="S5.T2.5.1.18.17.7" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.18.17.7.1" class="ltx_text ltx_framed ltx_framed_underline">11.4</span></td>
</tr>
<tr id="S5.T2.5.1.19.18" class="ltx_tr">
<th id="S5.T2.5.1.19.18.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">WavLM</th>
<th id="S5.T2.5.1.19.18.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">/</th>
<th id="S5.T2.5.1.19.18.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">/</th>
<td id="S5.T2.5.1.19.18.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.5.1.19.18.4.1" class="ltx_text ltx_font_bold">4.5</span></td>
<td id="S5.T2.5.1.19.18.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.5.1.19.18.5.1" class="ltx_text ltx_font_bold">5.1</span></td>
<td id="S5.T2.5.1.19.18.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.5.1.19.18.6.1" class="ltx_text ltx_font_bold">5.9</span></td>
<td id="S5.T2.5.1.19.18.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.5.1.19.18.7.1" class="ltx_text ltx_font_bold">5.8</span></td>
</tr>
<tr id="S5.T2.5.1.20.19" class="ltx_tr">
<th id="S5.T2.5.1.20.19.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">large</th>
<th id="S5.T2.5.1.20.19.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (S)</th>
<th id="S5.T2.5.1.20.19.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8M</th>
<td id="S5.T2.5.1.20.19.4" class="ltx_td ltx_align_center">5.7</td>
<td id="S5.T2.5.1.20.19.5" class="ltx_td ltx_align_center">6.2</td>
<td id="S5.T2.5.1.20.19.6" class="ltx_td ltx_align_center">7.0</td>
<td id="S5.T2.5.1.20.19.7" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.20.19.7.1" class="ltx_text ltx_framed ltx_framed_underline">6.5</span></td>
</tr>
<tr id="S5.T2.5.1.21.20" class="ltx_tr">
<th id="S5.T2.5.1.21.20.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T2.5.1.21.20.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (M*)</th>
<th id="S5.T2.5.1.21.20.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10M</th>
<td id="S5.T2.5.1.21.20.4" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.21.20.4.1" class="ltx_text ltx_framed ltx_framed_underline">4.9</span></td>
<td id="S5.T2.5.1.21.20.5" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.21.20.5.1" class="ltx_text ltx_framed ltx_framed_underline">5.6</span></td>
<td id="S5.T2.5.1.21.20.6" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.21.20.6.1" class="ltx_text ltx_framed ltx_framed_underline">6.1</span></td>
<td id="S5.T2.5.1.21.20.7" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.21.20.7.1" class="ltx_text ltx_font_bold">5.8</span></td>
</tr>
<tr id="S5.T2.5.1.22.21" class="ltx_tr">
<th id="S5.T2.5.1.22.21.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S5.T2.5.1.22.21.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">/</th>
<th id="S5.T2.5.1.22.21.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">/</th>
<td id="S5.T2.5.1.22.21.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.5.1.22.21.4.1" class="ltx_text ltx_framed ltx_framed_underline">9.1</span></td>
<td id="S5.T2.5.1.22.21.5" class="ltx_td ltx_align_center ltx_border_tt">11.2</td>
<td id="S5.T2.5.1.22.21.6" class="ltx_td ltx_align_center ltx_border_tt">27.7</td>
<td id="S5.T2.5.1.22.21.7" class="ltx_td ltx_align_center ltx_border_tt">22.7</td>
</tr>
<tr id="S5.T2.5.1.23.22" class="ltx_tr">
<th id="S5.T2.5.1.23.22.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Wav2vec2</th>
<th id="S5.T2.5.1.23.22.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">COPT</th>
<th id="S5.T2.5.1.23.22.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">95M</th>
<td id="S5.T2.5.1.23.22.4" class="ltx_td ltx_align_center">9.9</td>
<td id="S5.T2.5.1.23.22.5" class="ltx_td ltx_align_center">10.4</td>
<td id="S5.T2.5.1.23.22.6" class="ltx_td ltx_align_center">16.4</td>
<td id="S5.T2.5.1.23.22.7" class="ltx_td ltx_align_center">16.7</td>
</tr>
<tr id="S5.T2.5.1.24.23" class="ltx_tr">
<th id="S5.T2.5.1.24.23.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">base</th>
<th id="S5.T2.5.1.24.23.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (S)</th>
<th id="S5.T2.5.1.24.23.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8M</th>
<td id="S5.T2.5.1.24.23.4" class="ltx_td ltx_align_center">9.6</td>
<td id="S5.T2.5.1.24.23.5" class="ltx_td ltx_align_center">10.0</td>
<td id="S5.T2.5.1.24.23.6" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.24.23.6.1" class="ltx_text ltx_framed ltx_framed_underline">16.1</span></td>
<td id="S5.T2.5.1.24.23.7" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.24.23.7.1" class="ltx_text ltx_framed ltx_framed_underline">13.3</span></td>
</tr>
<tr id="S5.T2.5.1.25.24" class="ltx_tr">
<th id="S5.T2.5.1.25.24.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T2.5.1.25.24.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (M)</th>
<th id="S5.T2.5.1.25.24.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10M</th>
<td id="S5.T2.5.1.25.24.4" class="ltx_td ltx_align_center">9.2</td>
<td id="S5.T2.5.1.25.24.5" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.25.24.5.1" class="ltx_text ltx_framed ltx_framed_underline">9.9</span></td>
<td id="S5.T2.5.1.25.24.6" class="ltx_td ltx_align_center">17.9</td>
<td id="S5.T2.5.1.25.24.7" class="ltx_td ltx_align_center">14.1</td>
</tr>
<tr id="S5.T2.5.1.26.25" class="ltx_tr">
<th id="S5.T2.5.1.26.25.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S5.T2.5.1.26.25.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AdaDenoiser (S)</th>
<th id="S5.T2.5.1.26.25.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">7M</th>
<td id="S5.T2.5.1.26.25.4" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.26.25.4.1" class="ltx_text ltx_font_bold">8.3</span></td>
<td id="S5.T2.5.1.26.25.5" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.26.25.5.1" class="ltx_text ltx_font_bold">9.0</span></td>
<td id="S5.T2.5.1.26.25.6" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.26.25.6.1" class="ltx_text ltx_font_bold">12.1</span></td>
<td id="S5.T2.5.1.26.25.7" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.26.25.7.1" class="ltx_text ltx_font_bold">11.4</span></td>
</tr>
<tr id="S5.T2.5.1.27.26" class="ltx_tr">
<th id="S5.T2.5.1.27.26.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S5.T2.5.1.27.26.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">/</th>
<th id="S5.T2.5.1.27.26.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">/</th>
<td id="S5.T2.5.1.27.26.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.5.1.27.26.4.1" class="ltx_text ltx_font_bold">6.7</span></td>
<td id="S5.T2.5.1.27.26.5" class="ltx_td ltx_align_center ltx_border_t">8.9</td>
<td id="S5.T2.5.1.27.26.6" class="ltx_td ltx_align_center ltx_border_t">22.6</td>
<td id="S5.T2.5.1.27.26.7" class="ltx_td ltx_align_center ltx_border_t">15.0</td>
</tr>
<tr id="S5.T2.5.1.28.27" class="ltx_tr">
<th id="S5.T2.5.1.28.27.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Wav2vec2</th>
<th id="S5.T2.5.1.28.27.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (S)</th>
<th id="S5.T2.5.1.28.27.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">8M</th>
<td id="S5.T2.5.1.28.27.4" class="ltx_td ltx_align_center">8.4</td>
<td id="S5.T2.5.1.28.27.5" class="ltx_td ltx_align_center">9.4</td>
<td id="S5.T2.5.1.28.27.6" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.28.27.6.1" class="ltx_text ltx_framed ltx_framed_underline">15.7</span></td>
<td id="S5.T2.5.1.28.27.7" class="ltx_td ltx_align_center">10.9</td>
</tr>
<tr id="S5.T2.5.1.29.28" class="ltx_tr">
<th id="S5.T2.5.1.29.28.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">large</th>
<th id="S5.T2.5.1.29.28.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Denoiser (M)</th>
<th id="S5.T2.5.1.29.28.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10M</th>
<td id="S5.T2.5.1.29.28.4" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.29.28.4.1" class="ltx_text ltx_framed ltx_framed_underline">7.6</span></td>
<td id="S5.T2.5.1.29.28.5" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.29.28.5.1" class="ltx_text ltx_font_bold">8.5</span></td>
<td id="S5.T2.5.1.29.28.6" class="ltx_td ltx_align_center">15.9</td>
<td id="S5.T2.5.1.29.28.7" class="ltx_td ltx_align_center"><span id="S5.T2.5.1.29.28.7.1" class="ltx_text ltx_font_bold">9.9</span></td>
</tr>
<tr id="S5.T2.5.1.30.29" class="ltx_tr">
<th id="S5.T2.5.1.30.29.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_bb ltx_border_r"></th>
<th id="S5.T2.5.1.30.29.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_bb ltx_border_r">AdaDenoiser (M)</th>
<th id="S5.T2.5.1.30.29.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_bb ltx_border_r">11M</th>
<td id="S5.T2.5.1.30.29.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_bb">8.1</td>
<td id="S5.T2.5.1.30.29.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_bb"><span id="S5.T2.5.1.30.29.5.1" class="ltx_text ltx_framed ltx_framed_underline">8.7</span></td>
<td id="S5.T2.5.1.30.29.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_bb"><span id="S5.T2.5.1.30.29.6.1" class="ltx_text ltx_font_bold">11.0</span></td>
<td id="S5.T2.5.1.30.29.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_bb"><span id="S5.T2.5.1.30.29.7.1" class="ltx_text ltx_framed ltx_framed_underline">10.6</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">For the base SSL models, we observe strong improvements in almost all settings, both clean and noisy, even for WavLM base, which was already trained to perform denoising.
Of all SSL models, Wav2vec2 benefits most from denoising, as was indicated with the UER results.
For the large models, which were pre-trained on much more data, there are still improvements from the proposed adaptation.
Additionally, the proposed approach requires training much fewer parameters than regular SSL adaptation.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">For most settings, the AdaDenoiser model outperforms the Denoiser model, especially in very low SNR regimes.
In some cases, the WER is reduced by more than 50% compared to the baseline model.
We experienced that an optimal performance for low SNR data requires either adaptation of the convolutional feature extraction layers of the SSL, or inserting layer-wise adapters such as in the AdaDenoiser, which can have a bigger impact on the convolutional outputs compared to the weighted sum approach in the Denoiser.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">For WavLM large, which is already capable of denoising and was pre-trained on augmented data, our method has expectedly less effect besides efficient test-time adaptation.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">We remark that in contrast to discrete unit denoising, for noisy speech recognition a denoiser model (<span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_italic">Denoiser-M</span>) with a more powerful encoder is beneficial.
The relation between UER and WER is non-monotonic, which is reminiscent of the relation between Phone Error Rate and WER.
However, we still observe a general trend of improvements for both Denoiser architectures compared to the baselines.
The WER shows the optimal model for downstream speech recognition models, but depending on the application (e.g. voice conversion, language modeling) the UER might be preferred.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.1" class="ltx_p">Finally, Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Robust ASR ‣ 5 Results ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> details a short ablation study to validate our choice for an encoder-decoder model, which outperforms an encoder-only (with CTC) and a decoder-only variant.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.4.2" class="ltx_text" style="font-size:90%;">Ablation study on HuBERT base for different architectures of the Denoiser model. The encoder is always regularised or trained with a CTC objective. In case there is no decoder, it is pure CTC training. The average UER and WER (in %) on the whole <span id="S5.T3.4.2.1" class="ltx_text ltx_font_italic">test-clean-augmented</span> set are reported.</span></figcaption>
<table id="S5.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.5.1.1" class="ltx_tr">
<th id="S5.T3.5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S5.T3.5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Encoder</span></th>
<th id="S5.T3.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S5.T3.5.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Decoder</span></th>
<td id="S5.T3.5.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.5.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">UER</span></td>
<td id="S5.T3.5.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.5.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">WER</span></td>
</tr>
<tr id="S5.T3.5.2.2" class="ltx_tr">
<th id="S5.T3.5.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T3.5.2.2.1.1" class="ltx_text" style="font-size:80%;">6L Transf.</span></th>
<th id="S5.T3.5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T3.5.2.2.2.1" class="ltx_text" style="font-size:80%;">/</span></th>
<td id="S5.T3.5.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.5.2.2.3.1" class="ltx_text" style="font-size:80%;">32.5</span></td>
<td id="S5.T3.5.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.5.2.2.4.1" class="ltx_text" style="font-size:80%;">16.7</span></td>
</tr>
<tr id="S5.T3.5.3.3" class="ltx_tr">
<th id="S5.T3.5.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T3.5.3.3.1.1" class="ltx_text" style="font-size:80%;">9L Transf.</span></th>
<th id="S5.T3.5.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T3.5.3.3.2.1" class="ltx_text" style="font-size:80%;">/</span></th>
<td id="S5.T3.5.3.3.3" class="ltx_td ltx_align_center"><span id="S5.T3.5.3.3.3.1" class="ltx_text" style="font-size:80%;">32.5</span></td>
<td id="S5.T3.5.3.3.4" class="ltx_td ltx_align_center"><span id="S5.T3.5.3.3.4.1" class="ltx_text" style="font-size:80%;">16.7</span></td>
</tr>
<tr id="S5.T3.5.4.4" class="ltx_tr">
<th id="S5.T3.5.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T3.5.4.4.1.1" class="ltx_text" style="font-size:80%;">/</span></th>
<th id="S5.T3.5.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T3.5.4.4.2.1" class="ltx_text" style="font-size:80%;">3L Transf.</span></th>
<td id="S5.T3.5.4.4.3" class="ltx_td ltx_align_center"><span id="S5.T3.5.4.4.3.1" class="ltx_text" style="font-size:80%;">29.0</span></td>
<td id="S5.T3.5.4.4.4" class="ltx_td ltx_align_center"><span id="S5.T3.5.4.4.4.1" class="ltx_text" style="font-size:80%;">16.2</span></td>
</tr>
<tr id="S5.T3.5.5.5" class="ltx_tr">
<th id="S5.T3.5.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T3.5.5.5.1.1" class="ltx_text" style="font-size:80%;">6L Transf.</span></th>
<th id="S5.T3.5.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T3.5.5.5.2.1" class="ltx_text" style="font-size:80%;">3L Transf.</span></th>
<td id="S5.T3.5.5.5.3" class="ltx_td ltx_align_center"><span id="S5.T3.5.5.5.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">27.7</span></td>
<td id="S5.T3.5.5.5.4" class="ltx_td ltx_align_center"><span id="S5.T3.5.5.5.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">15.8</span></td>
</tr>
<tr id="S5.T3.5.6.6" class="ltx_tr">
<th id="S5.T3.5.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T3.5.6.6.1.1" class="ltx_text" style="font-size:80%;">SSL-Adapters</span></th>
<th id="S5.T3.5.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T3.5.6.6.2.1" class="ltx_text" style="font-size:80%;">/</span></th>
<td id="S5.T3.5.6.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.5.6.6.3.1" class="ltx_text" style="font-size:80%;">35.0</span></td>
<td id="S5.T3.5.6.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.5.6.6.4.1" class="ltx_text" style="font-size:80%;">17.1</span></td>
</tr>
<tr id="S5.T3.5.7.7" class="ltx_tr">
<th id="S5.T3.5.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S5.T3.5.7.7.1.1" class="ltx_text" style="font-size:80%;">SSL-Adapters</span></th>
<th id="S5.T3.5.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S5.T3.5.7.7.2.1" class="ltx_text" style="font-size:80%;">3L Transf.</span></th>
<td id="S5.T3.5.7.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.5.7.7.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">25.1</span></td>
<td id="S5.T3.5.7.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.5.7.7.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">11.9</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Test Time Adaptation</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">Previous sections have shown the capabilities of the proposed denoiser by pre-training on a varied set of noises and evaluating on a test set with unseen noises.
In practice, if one wants to apply the model in a new setting (e.g. a factory with specific noises which were not well represented in the augmented training set), it could be beneficial to adapt the model to the target environment by recording a few samples and then finetune the model.
As the denoiser module is a light-weight extension with few parameters, it can be finetuned on the fly with only a limited amount of unlabeled target data.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">We simulate this setting by choosing a new noise type with several recordings from the NTT Ambient Noise database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.
For every recorded noise sample of 30 seconds, we create 100 utterances by combining the noise with clean speech samples from the training set at different SNR levels between 0 and 20 dB.
Only the encoder of the Denoiser is finetuned, the rest is frozen.
Figure <a href="#S5.F2" title="Figure 2 ‣ 5.3 Test Time Adaptation ‣ 5 Results ‣ EFFICIENT EXTRACTION OF NOISE-ROBUST DISCRETE UNITS FROM SELF-SUPERVISED SPEECH MODELS" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the UER of a finetuned <span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_italic">Denoiser-S</span> model in function of the amount of recorded noise samples, computed on 100 samples from <span id="S5.SS3.p2.1.2" class="ltx_text ltx_font_italic">test-clean</span> augmented with unseen noises of the same noise type.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">We observe that finetuning to a new stationary noise source such as an old car or the inside of a train improves the pre-trained denoiser. For non-stationary noises with babbling like shopping mall recordings, finetuning has less effect.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><svg id="S5.F2.pic1" class="ltx_picture" height="396.03" overflow="visible" version="1.1" width="470.26"><g transform="translate(0,396.03) matrix(1 0 0 -1 0 0) translate(52.19,0) translate(0,36.11)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -52.19 -36.11)" class="ltx_centering"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(87.01,0) translate(0,36.11)" fill="#000000" stroke="#000000" stroke-width="0.5pt"><clipPath id="pgfcp1"><path d="M -34.81 -196.85 L 382.91 -196.85 L 382.91 548.58 L -34.81 548.58"></path></clipPath><g clip-path="url(#pgfcp1)"><g fill="#BFBFBF" stroke="#F9F9F9" stroke-width="0.1pt" color="#BFBFBF"><path d="M -34.81 0 L -34.81 351.73 M 34.81 0 L 34.81 351.73 M 104.43 0 L 104.43 351.73 M 174.05 0 L 174.05 351.73 M 243.67 0 L 243.67 351.73 M 313.29 0 L 313.29 351.73 M 382.91 0 L 382.91 351.73" style="fill:none"></path></g><g></g><g fill="#BFBFBF" stroke="#F9F9F9" stroke-width="0.1pt" color="#BFBFBF"><path d="M 0 0 L 0 351.73 M 69.62 0 L 69.62 351.73 M 139.24 0 L 139.24 351.73 M 208.86 0 L 208.86 351.73 M 278.48 0 L 278.48 351.73 M 348.1 0 L 348.1 351.73" style="fill:none"></path></g><g></g></g><clipPath id="pgfcp2"><path d="M -231.66 0 L -231.66 351.73 L 579.76 351.73 L 579.76 0"></path></clipPath><g clip-path="url(#pgfcp2)"><g fill="#BFBFBF" stroke="#F9F9F9" stroke-width="0.1pt" color="#BFBFBF"><path d="M -34.81 4.69 L 382.91 4.69 M -34.81 14.07 L 382.91 14.07 M -34.81 32.83 L 382.91 32.83 M -34.81 42.21 L 382.91 42.21 M -34.81 51.59 L 382.91 51.59 M -34.81 60.97 L 382.91 60.97 M -34.81 79.73 L 382.91 79.73 M -34.81 89.11 L 382.91 89.11 M -34.81 98.49 L 382.91 98.49 M -34.81 107.87 L 382.91 107.87 M -34.81 126.62 L 382.91 126.62 M -34.81 136 L 382.91 136 M -34.81 145.38 L 382.91 145.38 M -34.81 154.76 L 382.91 154.76 M -34.81 173.52 L 382.91 173.52 M -34.81 182.9 L 382.91 182.9 M -34.81 192.28 L 382.91 192.28 M -34.81 201.66 L 382.91 201.66 M -34.81 220.42 L 382.91 220.42 M -34.81 229.8 L 382.91 229.8 M -34.81 239.18 L 382.91 239.18 M -34.81 248.56 L 382.91 248.56 M -34.81 267.32 L 382.91 267.32 M -34.81 276.7 L 382.91 276.7 M -34.81 286.08 L 382.91 286.08 M -34.81 295.46 L 382.91 295.46 M -34.81 314.22 L 382.91 314.22 M -34.81 323.6 L 382.91 323.6 M -34.81 332.98 L 382.91 332.98 M -34.81 342.35 L 382.91 342.35" style="fill:none"></path></g><g></g><g fill="#BFBFBF" stroke="#F9F9F9" stroke-width="0.1pt" color="#BFBFBF"><path d="M -34.81 23.45 L 382.91 23.45 M -34.81 70.35 L 382.91 70.35 M -34.81 117.24 L 382.91 117.24 M -34.81 164.14 L 382.91 164.14 M -34.81 211.04 L 382.91 211.04 M -34.81 257.94 L 382.91 257.94 M -34.81 304.84 L 382.91 304.84 M -34.81 351.73 L 382.91 351.73" style="fill:none"></path></g><g></g></g><clipPath id="pgfcp3"><path d="M -34.81 -196.85 L 382.91 -196.85 L 382.91 548.58 L -34.81 548.58"></path></clipPath><g clip-path="url(#pgfcp3)"><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M -34.81 0 L -34.81 3.94 M 34.81 0 L 34.81 3.94 M 104.43 0 L 104.43 3.94 M 174.05 0 L 174.05 3.94 M 243.67 0 L 243.67 3.94 M 313.29 0 L 313.29 3.94 M 382.91 0 L 382.91 3.94 M -34.81 351.73 L -34.81 347.8 M 34.81 351.73 L 34.81 347.8 M 104.43 351.73 L 104.43 347.8 M 174.05 351.73 L 174.05 347.8 M 243.67 351.73 L 243.67 347.8 M 313.29 351.73 L 313.29 347.8 M 382.91 351.73 L 382.91 347.8" style="fill:none"></path></g><g></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M 0 0 L 0 5.91 M 69.62 0 L 69.62 5.91 M 139.24 0 L 139.24 5.91 M 208.86 0 L 208.86 5.91 M 278.48 0 L 278.48 5.91 M 348.1 0 L 348.1 5.91 M 0 351.73 L 0 345.83 M 69.62 351.73 L 69.62 345.83 M 139.24 351.73 L 139.24 345.83 M 208.86 351.73 L 208.86 345.83 M 278.48 351.73 L 278.48 345.83 M 348.1 351.73 L 348.1 345.83" style="fill:none"></path></g><g></g></g><clipPath id="pgfcp4"><path d="M -231.66 0 L -231.66 351.73 L 579.76 351.73 L 579.76 0"></path></clipPath><g clip-path="url(#pgfcp4)"><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M -34.81 4.69 L -30.88 4.69 M -34.81 14.07 L -30.88 14.07 M -34.81 32.83 L -30.88 32.83 M -34.81 42.21 L -30.88 42.21 M -34.81 51.59 L -30.88 51.59 M -34.81 60.97 L -30.88 60.97 M -34.81 79.73 L -30.88 79.73 M -34.81 89.11 L -30.88 89.11 M -34.81 98.49 L -30.88 98.49 M -34.81 107.87 L -30.88 107.87 M -34.81 126.62 L -30.88 126.62 M -34.81 136 L -30.88 136 M -34.81 145.38 L -30.88 145.38 M -34.81 154.76 L -30.88 154.76 M -34.81 173.52 L -30.88 173.52 M -34.81 182.9 L -30.88 182.9 M -34.81 192.28 L -30.88 192.28 M -34.81 201.66 L -30.88 201.66 M -34.81 220.42 L -30.88 220.42 M -34.81 229.8 L -30.88 229.8 M -34.81 239.18 L -30.88 239.18 M -34.81 248.56 L -30.88 248.56 M -34.81 267.32 L -30.88 267.32 M -34.81 276.7 L -30.88 276.7 M -34.81 286.08 L -30.88 286.08 M -34.81 295.46 L -30.88 295.46 M -34.81 314.22 L -30.88 314.22 M -34.81 323.6 L -30.88 323.6 M -34.81 332.98 L -30.88 332.98 M -34.81 342.35 L -30.88 342.35 M 382.91 4.69 L 378.97 4.69 M 382.91 14.07 L 378.97 14.07 M 382.91 32.83 L 378.97 32.83 M 382.91 42.21 L 378.97 42.21 M 382.91 51.59 L 378.97 51.59 M 382.91 60.97 L 378.97 60.97 M 382.91 79.73 L 378.97 79.73 M 382.91 89.11 L 378.97 89.11 M 382.91 98.49 L 378.97 98.49 M 382.91 107.87 L 378.97 107.87 M 382.91 126.62 L 378.97 126.62 M 382.91 136 L 378.97 136 M 382.91 145.38 L 378.97 145.38 M 382.91 154.76 L 378.97 154.76 M 382.91 173.52 L 378.97 173.52 M 382.91 182.9 L 378.97 182.9 M 382.91 192.28 L 378.97 192.28 M 382.91 201.66 L 378.97 201.66 M 382.91 220.42 L 378.97 220.42 M 382.91 229.8 L 378.97 229.8 M 382.91 239.18 L 378.97 239.18 M 382.91 248.56 L 378.97 248.56 M 382.91 267.32 L 378.97 267.32 M 382.91 276.7 L 378.97 276.7 M 382.91 286.08 L 378.97 286.08 M 382.91 295.46 L 378.97 295.46 M 382.91 314.22 L 378.97 314.22 M 382.91 323.6 L 378.97 323.6 M 382.91 332.98 L 378.97 332.98 M 382.91 342.35 L 378.97 342.35" style="fill:none"></path></g><g></g><g stroke-width="0.2pt" fill="#808080" stroke="#808080" color="#808080"><path d="M -34.81 23.45 L -28.91 23.45 M -34.81 70.35 L -28.91 70.35 M -34.81 117.24 L -28.91 117.24 M -34.81 164.14 L -28.91 164.14 M -34.81 211.04 L -28.91 211.04 M -34.81 257.94 L -28.91 257.94 M -34.81 304.84 L -28.91 304.84 M -34.81 351.73 L -28.91 351.73 M 382.91 23.45 L 377.01 23.45 M 382.91 70.35 L 377.01 70.35 M 382.91 117.24 L 377.01 117.24 M 382.91 164.14 L 377.01 164.14 M 382.91 211.04 L 377.01 211.04 M 382.91 257.94 L 377.01 257.94 M 382.91 304.84 L 377.01 304.84 M 382.91 351.73 L 377.01 351.73" style="fill:none"></path></g><g></g></g><path d="M -34.81 0 L -34.81 351.73 L 382.91 351.73 L 382.91 0 L -34.81 0 Z" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -2.77 -12.09)" fill="#000000" stroke="#000000"><foreignObject width="5.53" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S5.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S5.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">0</cn></annotation-xml></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 66.85 -12.09)" fill="#000000" stroke="#000000"><foreignObject width="5.53" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S5.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S5.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">1</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 136.47 -12.09)" fill="#000000" stroke="#000000"><foreignObject width="5.53" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S5.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S5.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">2</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 206.09 -12.09)" fill="#000000" stroke="#000000"><foreignObject width="5.53" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S5.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S5.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">3</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 275.71 -12.09)" fill="#000000" stroke="#000000"><foreignObject width="5.53" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S5.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S5.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">4</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 345.33 -12.09)" fill="#000000" stroke="#000000"><foreignObject width="5.53" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S5.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="integer" id="S5.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">5</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -61.76 19.88)" fill="#000000" stroke="#000000"><foreignObject width="21.99" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="16.0" display="inline"><semantics id="S5.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">16.0</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S5.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1">16.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1c">16.0</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -61.76 66.78)" fill="#000000" stroke="#000000"><foreignObject width="21.99" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="18.0" display="inline"><semantics id="S5.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">18.0</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S5.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1">18.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1c">18.0</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -61.76 113.68)" fill="#000000" stroke="#000000"><foreignObject width="21.99" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="20.0" display="inline"><semantics id="S5.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">20.0</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S5.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1">20.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1c">20.0</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -61.76 160.58)" fill="#000000" stroke="#000000"><foreignObject width="21.99" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="22.0" display="inline"><semantics id="S5.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">22.0</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S5.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1">22.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1c">22.0</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -61.76 207.47)" fill="#000000" stroke="#000000"><foreignObject width="21.99" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="24.0" display="inline"><semantics id="S5.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">24.0</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S5.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1">24.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1c">24.0</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -61.76 254.37)" fill="#000000" stroke="#000000"><foreignObject width="21.99" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="26.0" display="inline"><semantics id="S5.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">26.0</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S5.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1">26.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1c">26.0</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -61.76 301.27)" fill="#000000" stroke="#000000"><foreignObject width="21.99" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="28.0" display="inline"><semantics id="S5.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">28.0</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S5.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1">28.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1c">28.0</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -61.76 348.17)" fill="#000000" stroke="#000000"><foreignObject width="21.99" height="7.13" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S5.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="30.0" display="inline"><semantics id="S5.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1a"><mn mathsize="80%" id="S5.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">30.0</mn><annotation-xml encoding="MathML-Content" id="S5.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1b"><cn type="float" id="S5.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1">30.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1c">30.0</annotation></semantics></math></foreignObject></g><clipPath id="pgfcp5"><path d="M -34.81 0 L 382.91 0 L 382.91 351.73 L -34.81 351.73 Z"></path></clipPath><g clip-path="url(#pgfcp5)"><g stroke="#A0A8DC" fill="#A0A8DC" color="#A0A8DC"><path d="M 0 235.19 L 69.62 257.7 L 139.24 237.07 L 208.86 234.96 L 278.48 224.17 L 348.1 225.81" style="fill:none"></path></g><g></g><g stroke="#C0C5E8" fill="#C0C5E8" color="#C0C5E8"><path d="M 0 165.55 L 69.62 167.19 L 139.24 149.6 L 208.86 136.71 L 278.48 127.8 L 348.1 123.58" style="fill:none"></path></g><g></g><g stroke="#818BD0" fill="#818BD0" color="#818BD0"><path d="M 0 83.71 L 69.62 90.28 L 139.24 49.95 L 208.86 40.33 L 278.48 34.24 L 348.1 30.25" style="fill:none"></path></g><g></g><g stroke="#616EC4" fill="#616EC4" color="#616EC4"><path d="M 0 86.06 L 69.62 61.44 L 139.24 34.94 L 208.86 28.37 L 278.48 17.12 L 348.1 11.49" style="fill:none"></path></g><g></g></g><g stroke="#A0A8DC" fill="#A0A8DC" color="#A0A8DC"><path d="M -2.77 232.43 h 5.53 v 5.53 h -5.53 Z"></path><path d="M 66.85 254.94 h 5.53 v 5.53 h -5.53 Z"></path><path d="M 136.47 234.3 h 5.53 v 5.53 h -5.53 Z"></path><path d="M 206.09 232.19 h 5.53 v 5.53 h -5.53 Z"></path><path d="M 275.71 221.4 h 5.53 v 5.53 h -5.53 Z"></path><path d="M 345.33 223.05 h 5.53 v 5.53 h -5.53 Z"></path></g><g stroke="#C0C5E8" fill="#C0C5E8" color="#C0C5E8"><path d="M 2.77 165.55 C 2.77 167.08 1.53 168.32 0 168.32 C -1.53 168.32 -2.77 167.08 -2.77 165.55 C -2.77 164.02 -1.53 162.78 0 162.78 C 1.53 162.78 2.77 164.02 2.77 165.55 Z M 0 165.55"></path><path d="M 72.39 167.19 C 72.39 168.72 71.15 169.96 69.62 169.96 C 68.09 169.96 66.85 168.72 66.85 167.19 C 66.85 165.66 68.09 164.42 69.62 164.42 C 71.15 164.42 72.39 165.66 72.39 167.19 Z M 69.62 167.19"></path><path d="M 142.01 149.6 C 142.01 151.13 140.77 152.37 139.24 152.37 C 137.71 152.37 136.47 151.13 136.47 149.6 C 136.47 148.08 137.71 146.84 139.24 146.84 C 140.77 146.84 142.01 148.08 142.01 149.6 Z M 139.24 149.6"></path><path d="M 211.63 136.71 C 211.63 138.24 210.39 139.47 208.86 139.47 C 207.33 139.47 206.09 138.24 206.09 136.71 C 206.09 135.18 207.33 133.94 208.86 133.94 C 210.39 133.94 211.63 135.18 211.63 136.71 Z M 208.86 136.71"></path><path d="M 281.25 127.8 C 281.25 129.33 280.01 130.56 278.48 130.56 C 276.95 130.56 275.71 129.33 275.71 127.8 C 275.71 126.27 276.95 125.03 278.48 125.03 C 280.01 125.03 281.25 126.27 281.25 127.8 Z M 278.48 127.8"></path><path d="M 350.87 123.58 C 350.87 125.1 349.63 126.34 348.1 126.34 C 346.57 126.34 345.33 125.1 345.33 123.58 C 345.33 122.05 346.57 120.81 348.1 120.81 C 349.63 120.81 350.87 122.05 350.87 123.58 Z M 348.1 123.58"></path></g><g stroke="#818BD0" fill="#818BD0" color="#818BD0"><path d="M 0 86.48 L 2.4 82.33 L -2.4 82.33 Z"></path><path d="M 69.62 93.05 L 72.02 88.89 L 67.22 88.89 Z"></path><path d="M 139.24 52.71 L 141.64 48.56 L 136.84 48.56 Z"></path><path d="M 208.86 43.1 L 211.26 38.95 L 206.46 38.95 Z"></path><path d="M 278.48 37 L 280.88 32.85 L 276.08 32.85 Z"></path><path d="M 348.1 33.02 L 350.5 28.87 L 345.7 28.87 Z"></path></g><g stroke="#616EC4" fill="#616EC4" color="#616EC4"><path d="M 0 88.83 L 2.08 86.06 L 0 83.29 L -2.08 86.06 Z"></path><path d="M 69.62 64.2 L 71.7 61.44 L 69.62 58.67 L 67.54 61.44 Z"></path><path d="M 139.24 37.71 L 141.32 34.94 L 139.24 32.17 L 137.16 34.94 Z"></path><path d="M 208.86 31.14 L 210.94 28.37 L 208.86 25.61 L 206.78 28.37 Z"></path><path d="M 278.48 19.89 L 280.56 17.12 L 278.48 14.35 L 276.4 17.12 Z"></path><path d="M 348.1 14.26 L 350.18 11.49 L 348.1 8.72 L 346.02 11.49 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 115.36 -29.35)" fill="#000000" stroke="#000000"><foreignObject width="117.61" height="9.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F2.pic1.15.15.15.15.15.1.1" class="ltx_text" style="font-size:80%;"># Target Noise Samples</span></foreignObject></g><g transform="matrix(0.0 1.0 -1.0 0.0 -74.09 153.11)" fill="#000000" stroke="#000000"><foreignObject width="45.51" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F2.pic1.16.16.16.16.16.1.1" class="ltx_text" style="font-size:80%;">UER (%)</span></foreignObject></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 182.76 320.06)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 21.455)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 7.17)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.35,0)" fill="#A0A8DC" stroke="#A0A8DC" color="#A0A8DC"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 -2.77 h 5.53 v 5.53 h -5.53 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 24.31 0) translate(33.57,0) matrix(1.0 0.0 0.0 1.0 -30.81 -2.33)" fill="#000000" stroke="#000000"><foreignObject width="61.61" height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F2.pic1.17.17.17.17.17.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Shopping Mall</span></foreignObject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 91.46 0) translate(0.35,0)" fill="#C0C5E8" stroke="#C0C5E8" color="#C0C5E8"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 14.58 0 C 14.58 1.53 13.34 2.77 11.81 2.77 C 10.28 2.77 9.04 1.53 9.04 0 C 9.04 -1.53 10.28 -2.77 11.81 -2.77 C 13.34 -2.77 14.58 -1.53 14.58 0 Z M 11.81 0"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 115.78 0) translate(39.94,0) matrix(1.0 0.0 0.0 1.0 -37.17 -2.27)" fill="#000000" stroke="#000000"><foreignObject width="74.34" height="6.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F2.pic1.18.18.18.18.18.2.2.2.1.1" class="ltx_text" style="font-size:70%;">Construction Site</span></foreignObject></g></g><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 21.45)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0) translate(0.35,0)" fill="#818BD0" stroke="#818BD0" color="#818BD0"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 11.81 2.77 L 14.21 -1.38 L 9.41 -1.38 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 43.85 0) translate(14.04,0) matrix(1.0 0.0 0.0 1.0 -11.28 -2.27)" fill="#000000" stroke="#000000"><foreignObject width="22.9" height="6.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F2.pic1.19.19.19.19.19.3.3.1.1.1" class="ltx_text" style="font-size:70%;">Train</span></foreignObject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 91.46 0) translate(0.35,0)" fill="#616EC4" stroke="#616EC4" color="#616EC4"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 11.81 2.77 L 13.89 0 L 11.81 -2.77 L 9.74 0 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 145.13 0) translate(10.58,0) matrix(1.0 0.0 0.0 1.0 -7.82 -2.27)" fill="#000000" stroke="#000000"><foreignObject width="15.63" height="6.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S5.F2.pic1.20.20.20.20.20.4.4.2.1.1" class="ltx_text" style="font-size:70%;">Car</span></foreignObject></g></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 2</span>: </span><span id="S5.F2.4.2" class="ltx_text" style="font-size:90%;">UERs (%) after finetuning a pre-trained HuBERT base denoiser model on 30s noise samples from a target environment, and evaluating on unseen noise samples recorded in the same environment. The environments are Shopping Mall and Construction Site (evaluated at 10dB), and Car and Train (evaluated at 5dB).</span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper focuses on reducing the sensitivity of SSL model discretisation in noisy and reverberant environments. We have proposed a small encoder-decoder denoiser model and an adapter-based variant that denoise SSL features and predict clean discrete units for noisy inputs. The method is parameter-efficient and able to improve discretisation of SSL models for noisy data, as shown for denoised unit prediction and noisy speech recognition. Future work could apply this same strategy to other fields using speech discretisation, or delve into alternatives for the bottleneck layer, larger datasets and pre-training of the decoder.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Wei-Ning Hsu et al.,

</span>
<span class="ltx_bibblock">“HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,”

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Trans. on Audio, Speech, and Language Processing</span>, vol. 29, pp. 3451–3460, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Sanyuan Chen et al.,

</span>
<span class="ltx_bibblock">“WavLM: Large-scale self-supervised pre-training for full stack speech processing,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</span>, vol. 16, no. 6, pp. 1505–1518, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli,

</span>
<span class="ltx_bibblock">“Wav2vec 2.0: A framework for self-supervised learning of speech representations,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proc. NeurIPS</span>, 2020, pp. 12449–12460.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Abdelrahman Mohamed et al.,

</span>
<span class="ltx_bibblock">“Self-supervised speech representation learning: A review,”

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</span>, vol. 16, no. 6, pp. 1179–1210, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Ankita Pasad, Bowen Shi, and Karen Livescu,

</span>
<span class="ltx_bibblock">“Comparative layer-wise analysis of self-supervised speech models,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Shu wen Yang et al.,

</span>
<span class="ltx_bibblock">“SUPERB: Speech processing universal performance benchmark,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2021, pp. 1194–1198.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Arun Babu et al.,

</span>
<span class="ltx_bibblock">“XLS-R: Self-supervised cross-lingual speech representation learning at scale,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2022, pp. 2278–2282.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Alexei Baevski, Steffen Schneider, and Michael Auli,

</span>
<span class="ltx_bibblock">“vq-wav2vec: Self-supervised learning of discrete speech representations,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proc. ICLR</span>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Yu-An Chung et al.,

</span>
<span class="ltx_bibblock">“w2v-BERT: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proc. ASRU</span>, 2021, pp. 244–250.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Xuankai Chang et al.,

</span>
<span class="ltx_bibblock">“Exploring speech recognition, translation, and understanding with discrete speech units: A comparative study,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2024, pp. 11481–11485.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ali Elkahky et al.,

</span>
<span class="ltx_bibblock">“Do coarser units benefit cluster prediction-based speech pre-training?,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Ann Lee et al.,

</span>
<span class="ltx_bibblock">“Textless speech-to-speech translation on real data,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proc. NAACL</span>, 2022, pp. 860–872.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Kushal Lakhotia et al.,

</span>
<span class="ltx_bibblock">“On generative spoken language modeling from raw audio,”

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Transactions of the Association for Computational Linguistics</span>, vol. 9, pp. 1336–1354, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Alexei Baevski and Abdelrahman Mohamed,

</span>
<span class="ltx_bibblock">“Effectiveness of self-supervised pre-training for ASR,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2020, pp. 7694–7698.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Felix Wu et al.,

</span>
<span class="ltx_bibblock">“Wav2Seq: Pre-training speech-to-text encoder-decoder models using pseudo languages,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Alexei Baevski, Wei-Ning Hsu, Alexis Conneau, and Michael Auli,

</span>
<span class="ltx_bibblock">“Unsupervised speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proc. NeurIPS</span>, 2021, pp. 27826–27839.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Wei-Ning Hsu et al.,

</span>
<span class="ltx_bibblock">“Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2021, pp. 721–725.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Ramon Sanabria, Wei-Ning Hsu, Alexei Baevski, and Michael Auli,

</span>
<span class="ltx_bibblock">“Measuring the impact of domain factors in self-supervised pre-training,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP Workshops</span>, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Anshu Bhatia et al.,

</span>
<span class="ltx_bibblock">“Don’t stop self-supervision: Accent adaptation of speech representations via residual adapters,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2023, pp. 3362–3366.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Jakob Poncelet and Hugo Van hamme,

</span>
<span class="ltx_bibblock">“Unsupervised accent adaptation through masked language model correction of discrete self-supervised speech units,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2024, pp. 10236–10240.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Dianwen Ng et al.,

</span>
<span class="ltx_bibblock">“De’hubert: Disentangling noise in a self-supervised model for robust speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Kuan-Po Huang et al.,

</span>
<span class="ltx_bibblock">“Improving generalizability of distilled self-supervised speech processing models under distorted settings,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proc. SLT</span>, 2023, pp. 1112–1119.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Itai Gat et al.,

</span>
<span class="ltx_bibblock">“Augmentation invariant discrete representation for generative spoken language modeling,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proc. IWSLT</span>, 2023, pp. 465–477.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Mike Lewis et al.,

</span>
<span class="ltx_bibblock">“BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proc. Annual Meeting of ACL</span>, 2020, pp. 7871–7880.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Dongning Yang, Wei Wang, and Yanmin Qian,

</span>
<span class="ltx_bibblock">“FAT-HuBERT: Front-end adaptive training of hidden-unit BERT for distortion-invariant robust speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proc. ASRU</span>, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Qiu-Shi Zhu, Jie Zhang, Zi-Qiang Zhang, and Li-Rong Dai,

</span>
<span class="ltx_bibblock">“A joint speech enhancement and self-supervised representation learning framework for noise-robust speech recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>, vol. 31, pp. 1927–1939, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Wei Wang and Yanmin Qian,

</span>
<span class="ltx_bibblock">“HuBERT-AGG: Aggregated representation distillation of hidden-unit BERT for robust speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Heitor R. Guimarães et al.,

</span>
<span class="ltx_bibblock">“Robustdistiller: Compressing universal speech representations for enhanced environment robustness,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Neil Houlsby et al.,

</span>
<span class="ltx_bibblock">“Parameter-efficient transfer learning for NLP,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proc. ICML</span>, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Xuankai Chang et al.,

</span>
<span class="ltx_bibblock">“Exploration of efficient end-to-end ASR using discretized input from self-supervised learning,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2023, pp. 1399–1403.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,

</span>
<span class="ltx_bibblock">“BERT: Pre-training of deep bidirectional transformers for language understanding,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Proc. NAACL</span>, 2019, pp. 4171–4186.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Ankita Pasad, Chung-Ming Chien, Shane Settle, and Karen Livescu,

</span>
<span class="ltx_bibblock">“What do self-supervised speech models know about words?,”

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Transactions of the Association for Computational Linguistics</span>, vol. 12, pp. 372–391, 2024.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber,

</span>
<span class="ltx_bibblock">“Connectionist Temporal Classification: Labelling unsegmented sequence data with recurrent neural networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proc. ICML</span>, 2006, p. 369–376.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Peter Bell et al.,

</span>
<span class="ltx_bibblock">“Adaptation algorithms for neural network-based speech recognition: An overview,”

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">IEEE Open Journal of Signal Processing</span>, vol. 2, pp. 33–66, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J. Edward Hu et al.,

</span>
<span class="ltx_bibblock">“LoRA: Low-rank adaptation of large language models,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proc. ICLR</span>, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur,

</span>
<span class="ltx_bibblock">“LibriSpeech: An ASR corpus based on public domain audio books,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2015, pp. 5206–5210.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Shinji Watanabe et al.,

</span>
<span class="ltx_bibblock">“ESPnet: End-to-end speech processing toolkit,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2018, pp. 2207–2211.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Tom Ko et al.,

</span>
<span class="ltx_bibblock">“A study on data augmentation of reverberant speech for robust speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2017, pp. 5220–5224.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Joachim Thiemann, Nobutaka Ito, and Emmanuel Vincent,

</span>
<span class="ltx_bibblock">“The diverse environments multi-channel acoustic noise database (DEMAND): A database of multichannel environmental noise recordings,”

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">The Journal of the Acoustical Society of America</span>, vol. 133, pp. 3591, 2013.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
David Snyder, Guoguo Chen, and Daniel Povey,

</span>
<span class="ltx_bibblock">“MUSAN: A music, speech, and noise corpus,” 2015,

</span>
<span class="ltx_bibblock">arXiv:1510.08484v1.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Jon Barker, Ricard Marxer, Emmanuel Vincent, and Shinji Watanabe,

</span>
<span class="ltx_bibblock">“The third ‘CHiME’ speech separation and recognition challenge: Dataset, task and baselines,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Proc. ASRU</span>, 2015, pp. 504–511.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Myle Ott et al.,

</span>
<span class="ltx_bibblock">“fairseq: A fast, extensible toolkit for sequence modeling,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Proc. NAACL (Demonstrations)</span>, 2019, pp. 48–53.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Shinji Watanabe et al.,

</span>
<span class="ltx_bibblock">“Hybrid CTC/Attention architecture for end-to-end speech recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</span>, vol. 11, no. 8, pp. 1240–1253, 2017.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Anmol Gulati et al.,

</span>
<span class="ltx_bibblock">“Conformer: Convolution-augmented transformer for speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2020, pp. 5036–5040.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Kwangyoun Kim et al.,

</span>
<span class="ltx_bibblock">“E-Branchformer: Branchformer with enhanced merging for speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Proc. SLT</span>, 2023, pp. 84–91.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Ashish Vaswani et al.,

</span>
<span class="ltx_bibblock">“Attention is all you need,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Proc. NeurIPS</span>, 2017.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
NTT Advanced Technology (NTT-AT),

</span>
<span class="ltx_bibblock">“Ambient noise database for telephonometry,” 1996.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.02564" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.02565" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.02565">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.02565" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.02566" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 22:37:37 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
