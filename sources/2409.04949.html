<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.04949] Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings</title><meta property="og:description" content="In this research, we present an innovative, parameter-efficient model that utilizes the attention U-Net architecture for the automatic detection and eradication of non-speech vocal sounds, specifically breath sounds, i…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.04949">

<!--Generated on Sun Oct  6 01:48:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="attention u-net architecture,  non-speech vocal sounds,  parameter-efficient model,  audio quality,  deep learning,  sound engineering">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span><span id="id1.1" class="ltx_text" style="font-size:120%;"> Robert Gordon University, Aberdeen, Scotland <span id="id1.1.1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Informatics Institute of Technology, Colombo, Sri Lanka</span></span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document" style="font-size:173%;">Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id2.1.id1" class="ltx_text" style="font-size:120%;">Nidula Elgiriyewithana<sup id="id2.1.id1.1" class="ltx_sup">*</sup>  and N D Kodikara<sup id="id2.1.id1.2" class="ltx_sup">*</sup></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">In this research, we present an innovative, parameter-efficient model that utilizes the attention U-Net architecture for the automatic detection and eradication of non-speech vocal sounds, specifically breath sounds, in vocal recordings. This task is of paramount importance in the field of sound engineering, despite being relatively under-explored. The conventional manual process for detecting and eliminating these sounds requires significant expertise and is extremely time-intensive. Existing automated detection and removal methods often fall short in terms of efficiency and precision.
Our proposed model addresses these limitations by offering a streamlined process and superior accuracy, achieved through the application of advanced deep learning techniques. A unique dataset, derived from Device and Produced Speech (DAPS), was employed for this purpose. The training phase of the model emphasizes a log spectrogram and integrates an early stopping mechanism to prevent overfitting.
Our model not only conserves precious time for sound engineers but also enhances the quality and consistency of audio production. This constitutes a significant breakthrough, as evidenced by its comparative efficiency, necessitating only 1.9M parameters and a training duration of 3.2 hours - markedly less than the top-performing models in this domain. The model is capable of generating identical outputs as previous models with drastically improved precision, making it an optimal choice.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>attention u-net architecture, non-speech vocal sounds, parameter-efficient model, audio quality, deep learning, sound engineering
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Sound engineering encompasses a wide array of activities, including the manipulation and production of audio signals for diverse applications such as music, speech, film, and broadcasting. A prevalent challenge in this field is the occurrence of non-speech vocal sounds in audio recordings, such as breaths and lip smacks. These sounds can be distracting and may degrade the quality and intelligibility of the audio<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, particularly when the recording is intended for professional or academic use.Historically, this issue has been addressed through manual editing of audio waveforms or the use of noise gates, which dynamically reduce signals below a certain threshold by adjusting gain and ratio settings<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, these methods pose their own challenges, including substantial time consumption and the need for domain expert knowledge<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.1" class="ltx_logical-block">
<div id="S1.1.p1" class="ltx_para">
<p id="S1.1.p1.1" class="ltx_p ltx_align_left"><span id="S1.1.p1.1.1" class="ltx_text" style="font-size:80%;">David C. Wyld et al. (Eds): DMML, CSITEC, NLPI, BDBS - 2024 
<br class="ltx_break">pp. 49-58, 2024. CS &amp; IT - CSCP 2024
 <span id="S1.1.p1.1.1.1" class="ltx_text">DOI: 10.5121/csit.2024.140604</span></span></p>
</div>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The manual removal of non-speech vocal sounds is a meticulous and time-consuming task that demands significant expertise and focus from the sound engineer. It involves a detailed examination of the audio waveform, identification of undesired sounds, and their elimination using specialized software tools<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. This method can inadvertently introduce artifacts or distortions into the audio, thereby affecting its authenticity and quality. Furthermore, manual removal is impractical for tasks requiring rapid and efficient audio processing, such as managing lengthy or multiple recordings simultaneously<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
The automatic removal of non-speech vocal sounds presents a more efficient and desirable alternative, potentially enhancing the quality and consistency of audio while conserving the time and effort of sound engineers. Despite its importance, research on the automatic removal of non-speech vocal sounds is surprisingly limited. Most of the existing studies have focused on related but distinct problems, such as noise reduction, speech enhancement, or vocal separation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Only a few studies have explicitly addressed the problem of non-speech vocal sound removal.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While previous methodologies have shown promise, there still exists a need for a more efficient and effective model. In this research, we introduce an innovative, parameter-efficient model that utilizes the attention U-Net architecture for the automatic detection and eradication of non-speech vocal sounds, specifically breath sounds, in vocal recordings. This task is of paramount importance in the field of sound engineering, and our model addresses the limitations of existing methods by offering a streamlined process and superior accuracy, achieved through the application of advanced deep learning techniques.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We begin by providing a comprehensive background on sound engineering and the necessity for effective removal of non-speech vocal sounds in audio recordings  <a href="#S1" title="1 Introduction ‣ Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.Reviewing existing methods, we elaborate on their shortcomings and detail our approach which employs a unique dataset and cutting-edge deep learning methods  <a href="#S2" title="2 Related work ‣ Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>  <a href="#S3" title="3 Methodology ‣ Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We then reveal the results from our innovatory approach, displaying its potential for saving valuable time for sound engineers while boosting the quality and consistency of audio production  <a href="#S4" title="4 Results ‣ Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Further, we explore its real-time application and the possible influence it could wield on sound engineering, underscoring its superior accuracy compared to earlier models  <a href="#S5" title="5 Discussion and Future Work ‣ Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Ultimately, we ponder over the impacts and advancements our research has contributed to the field of sound engineering in the conclusion  <a href="#S6" title="6 Conclusion ‣ Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Our model symbolizes a momentous stride towards the automatic detection and extermination of non-speech vocal sounds, presenting an efficient and robust resolution to a perennial challenge.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In recent years, there has been growing interest in developing algorithms for automatic detection and removal of breath sounds from speech signals, and several studies have explored automated methods for detecting and removing breath sounds from audio recordings.In 2007 Ruinskiy and Lavner presented an algorithm aimed at accurately detecting breaths in speech or song signals using template matching based on mel frequency cepstral coefficients (MFCCs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Their approach achieved a high correct identification rate of 98% with a specificity of 96%.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In 2009, Rapcan and Reilly developed an algorithm for both detecting and removing breath sounds from speech signals, particularly focusing on its impact on cognitive studies of speech and language<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Their approach resulted in a notable increase in discrimination ability, demonstrating the potential of automated breath removal in enhancing classification accuracy.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Magdalena and Zi6lko proposed an algorithm based on wavelet decomposition for automatic detection of breath events in speech signals in 2013<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Their method incorporated temporal features and dynamic time warping to achieve robust breath detection, with applications extending to speech recognition systems.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Dumpala and Alluri 2017 introduced an algorithm for automatic detection of breath sounds in spontaneous speech, emphasizing its significance in speaker recognition systems<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Their rule-based approach outperformed previous methods, highlighting the importance of mitigating the impact of breath sounds on speaker recognition accuracy.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Detecting breathing sounds in realistic Japanese telephone conversations was explored by Takashi et al. in 2018<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. They proposed a method that leverages acoustic information specialized for breathing sounds, leading to a two-step approach that can detect breath events with an accuracy of 97.4%.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Recently, there has been growing interest in using deep learning techniques for speech processing tasks. Marco et al. proposed a differentiable signal processing framework for black-box audio effects in 2021<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Their approach trains a model with a multi-band noise gate Fx layer to automatically remove breath sounds and other non-speech vocalisations from speech signals.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">While significant progress has been made in breath sound detection and removal, there remains a need for more efficient and advanced methods that can handle complex speech signals and provide better accuracy. Our proposed method aims to address this challenge by introducing a novel deep learning architecture that can effectively detect and remove breath sounds from speech signals without damaging the original recording.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our study proposes a comprehensive methodology that includes a meticulously designed model, a robust model training regimen, and a specialized U-Net model. Each of these components is crucial in addressing the complex task of eliminating breath sounds from vocal recordings.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model Architecture </h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The backbone of our study is the specialized U-Net model emphasized with attention mechanism<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Originally envisioned for biomedical image segmentation, the U-Net model has been noted for its capacity to capture both local features and expansive context within an image. In our case, this image is the spectrogram, derived from the Short Time Fourier Transform (STFT) of the audio files.
The Short Time Fourier Transform (STFT) can be represented as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="STFT(x)=\sum_{n=-\infty}^{\infty}x[n]\cdot w[n-m]\cdot e^{-j\omega m}" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><mi id="S3.E1.m1.3.3.3.2" xref="S3.E1.m1.3.3.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.1" xref="S3.E1.m1.3.3.3.1.cmml">​</mo><mi id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.1a" xref="S3.E1.m1.3.3.3.1.cmml">​</mo><mi id="S3.E1.m1.3.3.3.4" xref="S3.E1.m1.3.3.3.4.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.1b" xref="S3.E1.m1.3.3.3.1.cmml">​</mo><mi id="S3.E1.m1.3.3.3.5" xref="S3.E1.m1.3.3.3.5.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.1c" xref="S3.E1.m1.3.3.3.1.cmml">​</mo><mrow id="S3.E1.m1.3.3.3.6.2" xref="S3.E1.m1.3.3.3.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.3.6.2.1" xref="S3.E1.m1.3.3.3.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.E1.m1.3.3.3.6.2.2" xref="S3.E1.m1.3.3.3.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E1.m1.3.3.2" xref="S3.E1.m1.3.3.2.cmml">=</mo><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><munderover id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.2.cmml"><mo movablelimits="false" id="S3.E1.m1.3.3.1.2.2.2" xref="S3.E1.m1.3.3.1.2.2.2.cmml">∑</mo><mrow id="S3.E1.m1.3.3.1.2.2.3" xref="S3.E1.m1.3.3.1.2.2.3.cmml"><mi id="S3.E1.m1.3.3.1.2.2.3.2" xref="S3.E1.m1.3.3.1.2.2.3.2.cmml">n</mi><mo id="S3.E1.m1.3.3.1.2.2.3.1" xref="S3.E1.m1.3.3.1.2.2.3.1.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.2.2.3.3" xref="S3.E1.m1.3.3.1.2.2.3.3.cmml"><mo id="S3.E1.m1.3.3.1.2.2.3.3a" xref="S3.E1.m1.3.3.1.2.2.3.3.cmml">−</mo><mi mathvariant="normal" id="S3.E1.m1.3.3.1.2.2.3.3.2" xref="S3.E1.m1.3.3.1.2.2.3.3.2.cmml">∞</mi></mrow></mrow><mi mathvariant="normal" id="S3.E1.m1.3.3.1.2.3" xref="S3.E1.m1.3.3.1.2.3.cmml">∞</mi></munderover><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.3.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.3.2.cmml"><mi id="S3.E1.m1.3.3.1.1.1.3.2.2" xref="S3.E1.m1.3.3.1.1.1.3.2.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.1.3.2.1" xref="S3.E1.m1.3.3.1.1.1.3.2.1.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.1.3.2.3.2" xref="S3.E1.m1.3.3.1.1.1.3.2.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.3.2.3.2.1" xref="S3.E1.m1.3.3.1.1.1.3.2.3.1.1.cmml">[</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">n</mi><mo rspace="0.055em" stretchy="false" id="S3.E1.m1.3.3.1.1.1.3.2.3.2.2" xref="S3.E1.m1.3.3.1.1.1.3.2.3.1.1.cmml">]</mo></mrow></mrow><mo rspace="0.222em" id="S3.E1.m1.3.3.1.1.1.3.1" xref="S3.E1.m1.3.3.1.1.1.3.1.cmml">⋅</mo><mi id="S3.E1.m1.3.3.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.3.3.cmml">w</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2.cmml">n</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3.cmml">m</mi></mrow><mo rspace="0.055em" stretchy="false" id="S3.E1.m1.3.3.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo rspace="0.222em" id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml">⋅</mo><msup id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.3.2" xref="S3.E1.m1.3.3.1.1.3.2.cmml">e</mi><mrow id="S3.E1.m1.3.3.1.1.3.3" xref="S3.E1.m1.3.3.1.1.3.3.cmml"><mo id="S3.E1.m1.3.3.1.1.3.3a" xref="S3.E1.m1.3.3.1.1.3.3.cmml">−</mo><mrow id="S3.E1.m1.3.3.1.1.3.3.2" xref="S3.E1.m1.3.3.1.1.3.3.2.cmml"><mi id="S3.E1.m1.3.3.1.1.3.3.2.2" xref="S3.E1.m1.3.3.1.1.3.3.2.2.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.3.3.2.1" xref="S3.E1.m1.3.3.1.1.3.3.2.1.cmml">​</mo><mi id="S3.E1.m1.3.3.1.1.3.3.2.3" xref="S3.E1.m1.3.3.1.1.3.3.2.3.cmml">ω</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.3.3.2.1a" xref="S3.E1.m1.3.3.1.1.3.3.2.1.cmml">​</mo><mi id="S3.E1.m1.3.3.1.1.3.3.2.4" xref="S3.E1.m1.3.3.1.1.3.3.2.4.cmml">m</mi></mrow></mrow></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><eq id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.2"></eq><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><times id="S3.E1.m1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.1"></times><ci id="S3.E1.m1.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.2">𝑆</ci><ci id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3">𝑇</ci><ci id="S3.E1.m1.3.3.3.4.cmml" xref="S3.E1.m1.3.3.3.4">𝐹</ci><ci id="S3.E1.m1.3.3.3.5.cmml" xref="S3.E1.m1.3.3.3.5">𝑇</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑥</ci></apply><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><apply id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.2.1.cmml" xref="S3.E1.m1.3.3.1.2">superscript</csymbol><apply id="S3.E1.m1.3.3.1.2.2.cmml" xref="S3.E1.m1.3.3.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.2">subscript</csymbol><sum id="S3.E1.m1.3.3.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.2.2.2"></sum><apply id="S3.E1.m1.3.3.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.2.2.3"><eq id="S3.E1.m1.3.3.1.2.2.3.1.cmml" xref="S3.E1.m1.3.3.1.2.2.3.1"></eq><ci id="S3.E1.m1.3.3.1.2.2.3.2.cmml" xref="S3.E1.m1.3.3.1.2.2.3.2">𝑛</ci><apply id="S3.E1.m1.3.3.1.2.2.3.3.cmml" xref="S3.E1.m1.3.3.1.2.2.3.3"><minus id="S3.E1.m1.3.3.1.2.2.3.3.1.cmml" xref="S3.E1.m1.3.3.1.2.2.3.3"></minus><infinity id="S3.E1.m1.3.3.1.2.2.3.3.2.cmml" xref="S3.E1.m1.3.3.1.2.2.3.3.2"></infinity></apply></apply></apply><infinity id="S3.E1.m1.3.3.1.2.3.cmml" xref="S3.E1.m1.3.3.1.2.3"></infinity></apply><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1"><ci id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2">⋅</ci><apply id="S3.E1.m1.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"><times id="S3.E1.m1.3.3.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2"></times><apply id="S3.E1.m1.3.3.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.3"><ci id="S3.E1.m1.3.3.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.3.1">⋅</ci><apply id="S3.E1.m1.3.3.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.3.2"><times id="S3.E1.m1.3.3.1.1.1.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.3.2.1"></times><ci id="S3.E1.m1.3.3.1.1.1.3.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.3.2.2">𝑥</ci><apply id="S3.E1.m1.3.3.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.3.2.3.2"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.3.2.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.3.2.3.2.1">delimited-[]</csymbol><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑛</ci></apply></apply><ci id="S3.E1.m1.3.3.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.3.3">𝑤</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1"><minus id="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1"></minus><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2">𝑛</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.3">𝑚</ci></apply></apply></apply><apply id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.3">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.3.2">𝑒</ci><apply id="S3.E1.m1.3.3.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.3.3"><minus id="S3.E1.m1.3.3.1.1.3.3.1.cmml" xref="S3.E1.m1.3.3.1.1.3.3"></minus><apply id="S3.E1.m1.3.3.1.1.3.3.2.cmml" xref="S3.E1.m1.3.3.1.1.3.3.2"><times id="S3.E1.m1.3.3.1.1.3.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.3.3.2.1"></times><ci id="S3.E1.m1.3.3.1.1.3.3.2.2.cmml" xref="S3.E1.m1.3.3.1.1.3.3.2.2">𝑗</ci><ci id="S3.E1.m1.3.3.1.1.3.3.2.3.cmml" xref="S3.E1.m1.3.3.1.1.3.3.2.3">𝜔</ci><ci id="S3.E1.m1.3.3.1.1.3.3.2.4.cmml" xref="S3.E1.m1.3.3.1.1.3.3.2.4">𝑚</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">STFT(x)=\sum_{n=-\infty}^{\infty}x[n]\cdot w[n-m]\cdot e^{-j\omega m}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.3" class="ltx_p">where <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="x[n]" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.2" xref="S3.SS1.p2.1.m1.1.2.cmml"><mi id="S3.SS1.p2.1.m1.1.2.2" xref="S3.SS1.p2.1.m1.1.2.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.1.m1.1.2.1" xref="S3.SS1.p2.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS1.p2.1.m1.1.2.3.2" xref="S3.SS1.p2.1.m1.1.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.1.m1.1.2.3.2.1" xref="S3.SS1.p2.1.m1.1.2.3.1.1.cmml">[</mo><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">n</mi><mo stretchy="false" id="S3.SS1.p2.1.m1.1.2.3.2.2" xref="S3.SS1.p2.1.m1.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.2"><times id="S3.SS1.p2.1.m1.1.2.1.cmml" xref="S3.SS1.p2.1.m1.1.2.1"></times><ci id="S3.SS1.p2.1.m1.1.2.2.cmml" xref="S3.SS1.p2.1.m1.1.2.2">𝑥</ci><apply id="S3.SS1.p2.1.m1.1.2.3.1.cmml" xref="S3.SS1.p2.1.m1.1.2.3.2"><csymbol cd="latexml" id="S3.SS1.p2.1.m1.1.2.3.1.1.cmml" xref="S3.SS1.p2.1.m1.1.2.3.2.1">delimited-[]</csymbol><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">x[n]</annotation></semantics></math> is the input signal, <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="w[n-m]" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mrow id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">​</mo><mrow id="S3.SS1.p2.2.m2.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.2.m2.1.1.1.1.2" xref="S3.SS1.p2.2.m2.1.1.1.2.1.cmml">[</mo><mrow id="S3.SS1.p2.2.m2.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.2" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml">n</mi><mo id="S3.SS1.p2.2.m2.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml">−</mo><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.3.cmml">m</mi></mrow><mo stretchy="false" id="S3.SS1.p2.2.m2.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><times id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2"></times><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">𝑤</ci><apply id="S3.SS1.p2.2.m2.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p2.2.m2.1.1.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1"><minus id="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1"></minus><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2">𝑛</ci><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.3">𝑚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">w[n-m]</annotation></semantics></math> is the window function, and <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">m</annotation></semantics></math> is the time index.The STFT renders the audio file as a 2D array, segregating and capturing nuanced patterns along temporal and frequency domains<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.Although the STFT yields a complex signal, our methodology focuses solely on its amplitude. The amplitude undergoes processing through a softmask followed by traversal through our
tailored U-Net algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2409.04949/assets/images/model-architecture.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="417" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Model Architecture</figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The U-Net model possesses an input shape of 3872x2048x1, uses batch normalization, has a singular class, uses 16 filters, includes 4 layers, incorporates a dropout rate of 20 percent, and employs a sigmoid function for output activation. These parameters were meticulously chosen to obtain the desired results.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Loss Function</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The design of the loss function is pivotal in the training of the network model. The loss function essentially quantifies the discrepancy between the predicted output and the actual output. The objective of the training process is to minimize this discrepancy. In this research, we have employed a custom loss function for the signal enhancement task. This loss function is unique in that it not only incorporates a Mean Absolute Error (MAE) term but also an additional term that specifically focuses on preserving speech over reducing noise. The Mean Absolute Error (MAE) is a popular metric in regression problems and is defined as the average of the absolute differences between the predicted and actual values<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. In our context, the MAE term is given by:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.5" class="ltx_Math" alttext="MAE=\frac{1}{N}\sum_{i=1}^{N}|y_{true,i}-y_{pred,i}|" display="block"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml"><mrow id="S3.E2.m1.5.5.3" xref="S3.E2.m1.5.5.3.cmml"><mi id="S3.E2.m1.5.5.3.2" xref="S3.E2.m1.5.5.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.3.1" xref="S3.E2.m1.5.5.3.1.cmml">​</mo><mi id="S3.E2.m1.5.5.3.3" xref="S3.E2.m1.5.5.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.3.1a" xref="S3.E2.m1.5.5.3.1.cmml">​</mo><mi id="S3.E2.m1.5.5.3.4" xref="S3.E2.m1.5.5.3.4.cmml">E</mi></mrow><mo id="S3.E2.m1.5.5.2" xref="S3.E2.m1.5.5.2.cmml">=</mo><mrow id="S3.E2.m1.5.5.1" xref="S3.E2.m1.5.5.1.cmml"><mfrac id="S3.E2.m1.5.5.1.3" xref="S3.E2.m1.5.5.1.3.cmml"><mn id="S3.E2.m1.5.5.1.3.2" xref="S3.E2.m1.5.5.1.3.2.cmml">1</mn><mi id="S3.E2.m1.5.5.1.3.3" xref="S3.E2.m1.5.5.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.2" xref="S3.E2.m1.5.5.1.2.cmml">​</mo><mrow id="S3.E2.m1.5.5.1.1" xref="S3.E2.m1.5.5.1.1.cmml"><munderover id="S3.E2.m1.5.5.1.1.2" xref="S3.E2.m1.5.5.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E2.m1.5.5.1.1.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.5.5.1.1.2.2.3" xref="S3.E2.m1.5.5.1.1.2.2.3.cmml"><mi id="S3.E2.m1.5.5.1.1.2.2.3.2" xref="S3.E2.m1.5.5.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E2.m1.5.5.1.1.2.2.3.1" xref="S3.E2.m1.5.5.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.5.5.1.1.2.2.3.3" xref="S3.E2.m1.5.5.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.5.5.1.1.2.3" xref="S3.E2.m1.5.5.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E2.m1.5.5.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.2.1.cmml">|</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.cmml"><msub id="S3.E2.m1.5.5.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.1.2.2" xref="S3.E2.m1.5.5.1.1.1.1.1.2.2.cmml">y</mi><mrow id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.3.cmml"><mrow id="S3.E2.m1.2.2.2.2.1" xref="S3.E2.m1.2.2.2.2.1.cmml"><mi id="S3.E2.m1.2.2.2.2.1.2" xref="S3.E2.m1.2.2.2.2.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.2.1.3" xref="S3.E2.m1.2.2.2.2.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.1.1a" xref="S3.E2.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.2.1.4" xref="S3.E2.m1.2.2.2.2.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.1.1b" xref="S3.E2.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E2.m1.2.2.2.2.1.5" xref="S3.E2.m1.2.2.2.2.1.5.cmml">e</mi></mrow><mo id="S3.E2.m1.2.2.2.2.2" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">i</mi></mrow></msub><mo id="S3.E2.m1.5.5.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E2.m1.5.5.1.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.1.3.2" xref="S3.E2.m1.5.5.1.1.1.1.1.3.2.cmml">y</mi><mrow id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.3.cmml"><mrow id="S3.E2.m1.4.4.2.2.1" xref="S3.E2.m1.4.4.2.2.1.cmml"><mi id="S3.E2.m1.4.4.2.2.1.2" xref="S3.E2.m1.4.4.2.2.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.2.1.1" xref="S3.E2.m1.4.4.2.2.1.1.cmml">​</mo><mi id="S3.E2.m1.4.4.2.2.1.3" xref="S3.E2.m1.4.4.2.2.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.2.1.1a" xref="S3.E2.m1.4.4.2.2.1.1.cmml">​</mo><mi id="S3.E2.m1.4.4.2.2.1.4" xref="S3.E2.m1.4.4.2.2.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.2.1.1b" xref="S3.E2.m1.4.4.2.2.1.1.cmml">​</mo><mi id="S3.E2.m1.4.4.2.2.1.5" xref="S3.E2.m1.4.4.2.2.1.5.cmml">d</mi></mrow><mo id="S3.E2.m1.4.4.2.2.2" xref="S3.E2.m1.4.4.2.3.cmml">,</mo><mi id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml">i</mi></mrow></msub></mrow><mo stretchy="false" id="S3.E2.m1.5.5.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5"><eq id="S3.E2.m1.5.5.2.cmml" xref="S3.E2.m1.5.5.2"></eq><apply id="S3.E2.m1.5.5.3.cmml" xref="S3.E2.m1.5.5.3"><times id="S3.E2.m1.5.5.3.1.cmml" xref="S3.E2.m1.5.5.3.1"></times><ci id="S3.E2.m1.5.5.3.2.cmml" xref="S3.E2.m1.5.5.3.2">𝑀</ci><ci id="S3.E2.m1.5.5.3.3.cmml" xref="S3.E2.m1.5.5.3.3">𝐴</ci><ci id="S3.E2.m1.5.5.3.4.cmml" xref="S3.E2.m1.5.5.3.4">𝐸</ci></apply><apply id="S3.E2.m1.5.5.1.cmml" xref="S3.E2.m1.5.5.1"><times id="S3.E2.m1.5.5.1.2.cmml" xref="S3.E2.m1.5.5.1.2"></times><apply id="S3.E2.m1.5.5.1.3.cmml" xref="S3.E2.m1.5.5.1.3"><divide id="S3.E2.m1.5.5.1.3.1.cmml" xref="S3.E2.m1.5.5.1.3"></divide><cn type="integer" id="S3.E2.m1.5.5.1.3.2.cmml" xref="S3.E2.m1.5.5.1.3.2">1</cn><ci id="S3.E2.m1.5.5.1.3.3.cmml" xref="S3.E2.m1.5.5.1.3.3">𝑁</ci></apply><apply id="S3.E2.m1.5.5.1.1.cmml" xref="S3.E2.m1.5.5.1.1"><apply id="S3.E2.m1.5.5.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.1.cmml" xref="S3.E2.m1.5.5.1.1.2">superscript</csymbol><apply id="S3.E2.m1.5.5.1.1.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.2">subscript</csymbol><sum id="S3.E2.m1.5.5.1.1.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2"></sum><apply id="S3.E2.m1.5.5.1.1.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3"><eq id="S3.E2.m1.5.5.1.1.2.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.5.5.1.1.2.2.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E2.m1.5.5.1.1.2.2.3.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.5.5.1.1.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.3">𝑁</ci></apply><apply id="S3.E2.m1.5.5.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1"><abs id="S3.E2.m1.5.5.1.1.1.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.2"></abs><apply id="S3.E2.m1.5.5.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1"><minus id="S3.E2.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.2.2">𝑦</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2"><apply id="S3.E2.m1.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.1"><times id="S3.E2.m1.2.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1"></times><ci id="S3.E2.m1.2.2.2.2.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.2">𝑡</ci><ci id="S3.E2.m1.2.2.2.2.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.3">𝑟</ci><ci id="S3.E2.m1.2.2.2.2.1.4.cmml" xref="S3.E2.m1.2.2.2.2.1.4">𝑢</ci><ci id="S3.E2.m1.2.2.2.2.1.5.cmml" xref="S3.E2.m1.2.2.2.2.1.5">𝑒</ci></apply><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑖</ci></list></apply><apply id="S3.E2.m1.5.5.1.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.3.2">𝑦</ci><list id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.2"><apply id="S3.E2.m1.4.4.2.2.1.cmml" xref="S3.E2.m1.4.4.2.2.1"><times id="S3.E2.m1.4.4.2.2.1.1.cmml" xref="S3.E2.m1.4.4.2.2.1.1"></times><ci id="S3.E2.m1.4.4.2.2.1.2.cmml" xref="S3.E2.m1.4.4.2.2.1.2">𝑝</ci><ci id="S3.E2.m1.4.4.2.2.1.3.cmml" xref="S3.E2.m1.4.4.2.2.1.3">𝑟</ci><ci id="S3.E2.m1.4.4.2.2.1.4.cmml" xref="S3.E2.m1.4.4.2.2.1.4">𝑒</ci><ci id="S3.E2.m1.4.4.2.2.1.5.cmml" xref="S3.E2.m1.4.4.2.2.1.5">𝑑</ci></apply><ci id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1">𝑖</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">MAE=\frac{1}{N}\sum_{i=1}^{N}|y_{true,i}-y_{pred,i}|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p">Where <math id="S3.SS2.p3.1.m1.2" class="ltx_Math" alttext="y_{true,i}" display="inline"><semantics id="S3.SS2.p3.1.m1.2a"><msub id="S3.SS2.p3.1.m1.2.3" xref="S3.SS2.p3.1.m1.2.3.cmml"><mi id="S3.SS2.p3.1.m1.2.3.2" xref="S3.SS2.p3.1.m1.2.3.2.cmml">y</mi><mrow id="S3.SS2.p3.1.m1.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.2.3.cmml"><mrow id="S3.SS2.p3.1.m1.2.2.2.2.1" xref="S3.SS2.p3.1.m1.2.2.2.2.1.cmml"><mi id="S3.SS2.p3.1.m1.2.2.2.2.1.2" xref="S3.SS2.p3.1.m1.2.2.2.2.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.2.2.2.2.1.1" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS2.p3.1.m1.2.2.2.2.1.3" xref="S3.SS2.p3.1.m1.2.2.2.2.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.2.2.2.2.1.1a" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS2.p3.1.m1.2.2.2.2.1.4" xref="S3.SS2.p3.1.m1.2.2.2.2.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.1.m1.2.2.2.2.1.1b" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS2.p3.1.m1.2.2.2.2.1.5" xref="S3.SS2.p3.1.m1.2.2.2.2.1.5.cmml">e</mi></mrow><mo id="S3.SS2.p3.1.m1.2.2.2.2.2" xref="S3.SS2.p3.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p3.1.m1.1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.1.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.2b"><apply id="S3.SS2.p3.1.m1.2.3.cmml" xref="S3.SS2.p3.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.3.1.cmml" xref="S3.SS2.p3.1.m1.2.3">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.3.2.cmml" xref="S3.SS2.p3.1.m1.2.3.2">𝑦</ci><list id="S3.SS2.p3.1.m1.2.2.2.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2"><apply id="S3.SS2.p3.1.m1.2.2.2.2.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1"><times id="S3.SS2.p3.1.m1.2.2.2.2.1.1.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.1"></times><ci id="S3.SS2.p3.1.m1.2.2.2.2.1.2.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.2">𝑡</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.1.3.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.3">𝑟</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.1.4.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.4">𝑢</ci><ci id="S3.SS2.p3.1.m1.2.2.2.2.1.5.cmml" xref="S3.SS2.p3.1.m1.2.2.2.2.1.5">𝑒</ci></apply><ci id="S3.SS2.p3.1.m1.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1.1">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.2c">y_{true,i}</annotation></semantics></math> and <math id="S3.SS2.p3.2.m2.2" class="ltx_Math" alttext="y_{pred,i}" display="inline"><semantics id="S3.SS2.p3.2.m2.2a"><msub id="S3.SS2.p3.2.m2.2.3" xref="S3.SS2.p3.2.m2.2.3.cmml"><mi id="S3.SS2.p3.2.m2.2.3.2" xref="S3.SS2.p3.2.m2.2.3.2.cmml">y</mi><mrow id="S3.SS2.p3.2.m2.2.2.2.2" xref="S3.SS2.p3.2.m2.2.2.2.3.cmml"><mrow id="S3.SS2.p3.2.m2.2.2.2.2.1" xref="S3.SS2.p3.2.m2.2.2.2.2.1.cmml"><mi id="S3.SS2.p3.2.m2.2.2.2.2.1.2" xref="S3.SS2.p3.2.m2.2.2.2.2.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.2.2.2.2.1.1" xref="S3.SS2.p3.2.m2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS2.p3.2.m2.2.2.2.2.1.3" xref="S3.SS2.p3.2.m2.2.2.2.2.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.2.2.2.2.1.1a" xref="S3.SS2.p3.2.m2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS2.p3.2.m2.2.2.2.2.1.4" xref="S3.SS2.p3.2.m2.2.2.2.2.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.2.m2.2.2.2.2.1.1b" xref="S3.SS2.p3.2.m2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS2.p3.2.m2.2.2.2.2.1.5" xref="S3.SS2.p3.2.m2.2.2.2.2.1.5.cmml">d</mi></mrow><mo id="S3.SS2.p3.2.m2.2.2.2.2.2" xref="S3.SS2.p3.2.m2.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p3.2.m2.1.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.1.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.2b"><apply id="S3.SS2.p3.2.m2.2.3.cmml" xref="S3.SS2.p3.2.m2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.2.3.1.cmml" xref="S3.SS2.p3.2.m2.2.3">subscript</csymbol><ci id="S3.SS2.p3.2.m2.2.3.2.cmml" xref="S3.SS2.p3.2.m2.2.3.2">𝑦</ci><list id="S3.SS2.p3.2.m2.2.2.2.3.cmml" xref="S3.SS2.p3.2.m2.2.2.2.2"><apply id="S3.SS2.p3.2.m2.2.2.2.2.1.cmml" xref="S3.SS2.p3.2.m2.2.2.2.2.1"><times id="S3.SS2.p3.2.m2.2.2.2.2.1.1.cmml" xref="S3.SS2.p3.2.m2.2.2.2.2.1.1"></times><ci id="S3.SS2.p3.2.m2.2.2.2.2.1.2.cmml" xref="S3.SS2.p3.2.m2.2.2.2.2.1.2">𝑝</ci><ci id="S3.SS2.p3.2.m2.2.2.2.2.1.3.cmml" xref="S3.SS2.p3.2.m2.2.2.2.2.1.3">𝑟</ci><ci id="S3.SS2.p3.2.m2.2.2.2.2.1.4.cmml" xref="S3.SS2.p3.2.m2.2.2.2.2.1.4">𝑒</ci><ci id="S3.SS2.p3.2.m2.2.2.2.2.1.5.cmml" xref="S3.SS2.p3.2.m2.2.2.2.2.1.5">𝑑</ci></apply><ci id="S3.SS2.p3.2.m2.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.2c">y_{pred,i}</annotation></semantics></math> are the true and predicted amplitude values of the speech signal, respectively, and N is the total number of samples.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">The second term in our loss function is designed to prioritize the preservation of speech over noise reduction. This is achieved by penalizing the model more heavily when it fails to correctly predict the amplitude of the speech signal. Mathematically, this is expressed as:</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.7" class="ltx_Math" alttext="SpeechLoss=2*\frac{1}{N}\sum_{i=1}^{N}|(y_{true,i})^{2}-y_{pred,i}*y_{true,i}|" display="block"><semantics id="S3.E3.m1.7a"><mrow id="S3.E3.m1.7.7" xref="S3.E3.m1.7.7.cmml"><mrow id="S3.E3.m1.7.7.3" xref="S3.E3.m1.7.7.3.cmml"><mi id="S3.E3.m1.7.7.3.2" xref="S3.E3.m1.7.7.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.3.1" xref="S3.E3.m1.7.7.3.1.cmml">​</mo><mi id="S3.E3.m1.7.7.3.3" xref="S3.E3.m1.7.7.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.3.1a" xref="S3.E3.m1.7.7.3.1.cmml">​</mo><mi id="S3.E3.m1.7.7.3.4" xref="S3.E3.m1.7.7.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.3.1b" xref="S3.E3.m1.7.7.3.1.cmml">​</mo><mi id="S3.E3.m1.7.7.3.5" xref="S3.E3.m1.7.7.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.3.1c" xref="S3.E3.m1.7.7.3.1.cmml">​</mo><mi id="S3.E3.m1.7.7.3.6" xref="S3.E3.m1.7.7.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.3.1d" xref="S3.E3.m1.7.7.3.1.cmml">​</mo><mi id="S3.E3.m1.7.7.3.7" xref="S3.E3.m1.7.7.3.7.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.3.1e" xref="S3.E3.m1.7.7.3.1.cmml">​</mo><mi id="S3.E3.m1.7.7.3.8" xref="S3.E3.m1.7.7.3.8.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.3.1f" xref="S3.E3.m1.7.7.3.1.cmml">​</mo><mi id="S3.E3.m1.7.7.3.9" xref="S3.E3.m1.7.7.3.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.3.1g" xref="S3.E3.m1.7.7.3.1.cmml">​</mo><mi id="S3.E3.m1.7.7.3.10" xref="S3.E3.m1.7.7.3.10.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.3.1h" xref="S3.E3.m1.7.7.3.1.cmml">​</mo><mi id="S3.E3.m1.7.7.3.11" xref="S3.E3.m1.7.7.3.11.cmml">s</mi></mrow><mo id="S3.E3.m1.7.7.2" xref="S3.E3.m1.7.7.2.cmml">=</mo><mrow id="S3.E3.m1.7.7.1" xref="S3.E3.m1.7.7.1.cmml"><mrow id="S3.E3.m1.7.7.1.3" xref="S3.E3.m1.7.7.1.3.cmml"><mn id="S3.E3.m1.7.7.1.3.2" xref="S3.E3.m1.7.7.1.3.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.7.7.1.3.1" xref="S3.E3.m1.7.7.1.3.1.cmml">∗</mo><mfrac id="S3.E3.m1.7.7.1.3.3" xref="S3.E3.m1.7.7.1.3.3.cmml"><mn id="S3.E3.m1.7.7.1.3.3.2" xref="S3.E3.m1.7.7.1.3.3.2.cmml">1</mn><mi id="S3.E3.m1.7.7.1.3.3.3" xref="S3.E3.m1.7.7.1.3.3.3.cmml">N</mi></mfrac></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.1.2" xref="S3.E3.m1.7.7.1.2.cmml">​</mo><mrow id="S3.E3.m1.7.7.1.1" xref="S3.E3.m1.7.7.1.1.cmml"><munderover id="S3.E3.m1.7.7.1.1.2" xref="S3.E3.m1.7.7.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E3.m1.7.7.1.1.2.2.2" xref="S3.E3.m1.7.7.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E3.m1.7.7.1.1.2.2.3" xref="S3.E3.m1.7.7.1.1.2.2.3.cmml"><mi id="S3.E3.m1.7.7.1.1.2.2.3.2" xref="S3.E3.m1.7.7.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E3.m1.7.7.1.1.2.2.3.1" xref="S3.E3.m1.7.7.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E3.m1.7.7.1.1.2.2.3.3" xref="S3.E3.m1.7.7.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.7.7.1.1.2.3" xref="S3.E3.m1.7.7.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E3.m1.7.7.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.7.7.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.2.1.cmml">|</mo><mrow id="S3.E3.m1.7.7.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.cmml"><msup id="S3.E3.m1.7.7.1.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.3.cmml"><mrow id="S3.E3.m1.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.1.cmml"><mi id="S3.E3.m1.2.2.2.2.1.2" xref="S3.E3.m1.2.2.2.2.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E3.m1.2.2.2.2.1.3" xref="S3.E3.m1.2.2.2.2.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.2.1.1a" xref="S3.E3.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E3.m1.2.2.2.2.1.4" xref="S3.E3.m1.2.2.2.2.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.2.2.1.1b" xref="S3.E3.m1.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E3.m1.2.2.2.2.1.5" xref="S3.E3.m1.2.2.2.2.1.5.cmml">e</mi></mrow><mo id="S3.E3.m1.2.2.2.2.2" xref="S3.E3.m1.2.2.2.3.cmml">,</mo><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">i</mi></mrow></msub><mo stretchy="false" id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E3.m1.7.7.1.1.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3.cmml">2</mn></msup><mo id="S3.E3.m1.7.7.1.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.1.1.2.cmml">−</mo><mrow id="S3.E3.m1.7.7.1.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.1.1.3.cmml"><msub id="S3.E3.m1.7.7.1.1.1.1.1.3.2" xref="S3.E3.m1.7.7.1.1.1.1.1.3.2.cmml"><mi id="S3.E3.m1.7.7.1.1.1.1.1.3.2.2" xref="S3.E3.m1.7.7.1.1.1.1.1.3.2.2.cmml">y</mi><mrow id="S3.E3.m1.4.4.2.2" xref="S3.E3.m1.4.4.2.3.cmml"><mrow id="S3.E3.m1.4.4.2.2.1" xref="S3.E3.m1.4.4.2.2.1.cmml"><mi id="S3.E3.m1.4.4.2.2.1.2" xref="S3.E3.m1.4.4.2.2.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.2.1.1" xref="S3.E3.m1.4.4.2.2.1.1.cmml">​</mo><mi id="S3.E3.m1.4.4.2.2.1.3" xref="S3.E3.m1.4.4.2.2.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.2.1.1a" xref="S3.E3.m1.4.4.2.2.1.1.cmml">​</mo><mi id="S3.E3.m1.4.4.2.2.1.4" xref="S3.E3.m1.4.4.2.2.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.2.1.1b" xref="S3.E3.m1.4.4.2.2.1.1.cmml">​</mo><mi id="S3.E3.m1.4.4.2.2.1.5" xref="S3.E3.m1.4.4.2.2.1.5.cmml">d</mi></mrow><mo id="S3.E3.m1.4.4.2.2.2" xref="S3.E3.m1.4.4.2.3.cmml">,</mo><mi id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml">i</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.7.7.1.1.1.1.1.3.1" xref="S3.E3.m1.7.7.1.1.1.1.1.3.1.cmml">∗</mo><msub id="S3.E3.m1.7.7.1.1.1.1.1.3.3" xref="S3.E3.m1.7.7.1.1.1.1.1.3.3.cmml"><mi id="S3.E3.m1.7.7.1.1.1.1.1.3.3.2" xref="S3.E3.m1.7.7.1.1.1.1.1.3.3.2.cmml">y</mi><mrow id="S3.E3.m1.6.6.2.2" xref="S3.E3.m1.6.6.2.3.cmml"><mrow id="S3.E3.m1.6.6.2.2.1" xref="S3.E3.m1.6.6.2.2.1.cmml"><mi id="S3.E3.m1.6.6.2.2.1.2" xref="S3.E3.m1.6.6.2.2.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.6.6.2.2.1.1" xref="S3.E3.m1.6.6.2.2.1.1.cmml">​</mo><mi id="S3.E3.m1.6.6.2.2.1.3" xref="S3.E3.m1.6.6.2.2.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.6.6.2.2.1.1a" xref="S3.E3.m1.6.6.2.2.1.1.cmml">​</mo><mi id="S3.E3.m1.6.6.2.2.1.4" xref="S3.E3.m1.6.6.2.2.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.6.6.2.2.1.1b" xref="S3.E3.m1.6.6.2.2.1.1.cmml">​</mo><mi id="S3.E3.m1.6.6.2.2.1.5" xref="S3.E3.m1.6.6.2.2.1.5.cmml">e</mi></mrow><mo id="S3.E3.m1.6.6.2.2.2" xref="S3.E3.m1.6.6.2.3.cmml">,</mo><mi id="S3.E3.m1.5.5.1.1" xref="S3.E3.m1.5.5.1.1.cmml">i</mi></mrow></msub></mrow></mrow><mo stretchy="false" id="S3.E3.m1.7.7.1.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.7b"><apply id="S3.E3.m1.7.7.cmml" xref="S3.E3.m1.7.7"><eq id="S3.E3.m1.7.7.2.cmml" xref="S3.E3.m1.7.7.2"></eq><apply id="S3.E3.m1.7.7.3.cmml" xref="S3.E3.m1.7.7.3"><times id="S3.E3.m1.7.7.3.1.cmml" xref="S3.E3.m1.7.7.3.1"></times><ci id="S3.E3.m1.7.7.3.2.cmml" xref="S3.E3.m1.7.7.3.2">𝑆</ci><ci id="S3.E3.m1.7.7.3.3.cmml" xref="S3.E3.m1.7.7.3.3">𝑝</ci><ci id="S3.E3.m1.7.7.3.4.cmml" xref="S3.E3.m1.7.7.3.4">𝑒</ci><ci id="S3.E3.m1.7.7.3.5.cmml" xref="S3.E3.m1.7.7.3.5">𝑒</ci><ci id="S3.E3.m1.7.7.3.6.cmml" xref="S3.E3.m1.7.7.3.6">𝑐</ci><ci id="S3.E3.m1.7.7.3.7.cmml" xref="S3.E3.m1.7.7.3.7">ℎ</ci><ci id="S3.E3.m1.7.7.3.8.cmml" xref="S3.E3.m1.7.7.3.8">𝐿</ci><ci id="S3.E3.m1.7.7.3.9.cmml" xref="S3.E3.m1.7.7.3.9">𝑜</ci><ci id="S3.E3.m1.7.7.3.10.cmml" xref="S3.E3.m1.7.7.3.10">𝑠</ci><ci id="S3.E3.m1.7.7.3.11.cmml" xref="S3.E3.m1.7.7.3.11">𝑠</ci></apply><apply id="S3.E3.m1.7.7.1.cmml" xref="S3.E3.m1.7.7.1"><times id="S3.E3.m1.7.7.1.2.cmml" xref="S3.E3.m1.7.7.1.2"></times><apply id="S3.E3.m1.7.7.1.3.cmml" xref="S3.E3.m1.7.7.1.3"><times id="S3.E3.m1.7.7.1.3.1.cmml" xref="S3.E3.m1.7.7.1.3.1"></times><cn type="integer" id="S3.E3.m1.7.7.1.3.2.cmml" xref="S3.E3.m1.7.7.1.3.2">2</cn><apply id="S3.E3.m1.7.7.1.3.3.cmml" xref="S3.E3.m1.7.7.1.3.3"><divide id="S3.E3.m1.7.7.1.3.3.1.cmml" xref="S3.E3.m1.7.7.1.3.3"></divide><cn type="integer" id="S3.E3.m1.7.7.1.3.3.2.cmml" xref="S3.E3.m1.7.7.1.3.3.2">1</cn><ci id="S3.E3.m1.7.7.1.3.3.3.cmml" xref="S3.E3.m1.7.7.1.3.3.3">𝑁</ci></apply></apply><apply id="S3.E3.m1.7.7.1.1.cmml" xref="S3.E3.m1.7.7.1.1"><apply id="S3.E3.m1.7.7.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.2.1.cmml" xref="S3.E3.m1.7.7.1.1.2">superscript</csymbol><apply id="S3.E3.m1.7.7.1.1.2.2.cmml" xref="S3.E3.m1.7.7.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.2.2.1.cmml" xref="S3.E3.m1.7.7.1.1.2">subscript</csymbol><sum id="S3.E3.m1.7.7.1.1.2.2.2.cmml" xref="S3.E3.m1.7.7.1.1.2.2.2"></sum><apply id="S3.E3.m1.7.7.1.1.2.2.3.cmml" xref="S3.E3.m1.7.7.1.1.2.2.3"><eq id="S3.E3.m1.7.7.1.1.2.2.3.1.cmml" xref="S3.E3.m1.7.7.1.1.2.2.3.1"></eq><ci id="S3.E3.m1.7.7.1.1.2.2.3.2.cmml" xref="S3.E3.m1.7.7.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E3.m1.7.7.1.1.2.2.3.3.cmml" xref="S3.E3.m1.7.7.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.7.7.1.1.2.3.cmml" xref="S3.E3.m1.7.7.1.1.2.3">𝑁</ci></apply><apply id="S3.E3.m1.7.7.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1"><abs id="S3.E3.m1.7.7.1.1.1.2.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.2"></abs><apply id="S3.E3.m1.7.7.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1"><minus id="S3.E3.m1.7.7.1.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.2"></minus><apply id="S3.E3.m1.7.7.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.1.1.1.2">𝑦</ci><list id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2"><apply id="S3.E3.m1.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.1"><times id="S3.E3.m1.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1"></times><ci id="S3.E3.m1.2.2.2.2.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.2">𝑡</ci><ci id="S3.E3.m1.2.2.2.2.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.3">𝑟</ci><ci id="S3.E3.m1.2.2.2.2.1.4.cmml" xref="S3.E3.m1.2.2.2.2.1.4">𝑢</ci><ci id="S3.E3.m1.2.2.2.2.1.5.cmml" xref="S3.E3.m1.2.2.2.2.1.5">𝑒</ci></apply><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">𝑖</ci></list></apply><cn type="integer" id="S3.E3.m1.7.7.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.1.3">2</cn></apply><apply id="S3.E3.m1.7.7.1.1.1.1.1.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.3"><times id="S3.E3.m1.7.7.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.3.1"></times><apply id="S3.E3.m1.7.7.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.7.7.1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.3.2.2">𝑦</ci><list id="S3.E3.m1.4.4.2.3.cmml" xref="S3.E3.m1.4.4.2.2"><apply id="S3.E3.m1.4.4.2.2.1.cmml" xref="S3.E3.m1.4.4.2.2.1"><times id="S3.E3.m1.4.4.2.2.1.1.cmml" xref="S3.E3.m1.4.4.2.2.1.1"></times><ci id="S3.E3.m1.4.4.2.2.1.2.cmml" xref="S3.E3.m1.4.4.2.2.1.2">𝑝</ci><ci id="S3.E3.m1.4.4.2.2.1.3.cmml" xref="S3.E3.m1.4.4.2.2.1.3">𝑟</ci><ci id="S3.E3.m1.4.4.2.2.1.4.cmml" xref="S3.E3.m1.4.4.2.2.1.4">𝑒</ci><ci id="S3.E3.m1.4.4.2.2.1.5.cmml" xref="S3.E3.m1.4.4.2.2.1.5">𝑑</ci></apply><ci id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1.1">𝑖</ci></list></apply><apply id="S3.E3.m1.7.7.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E3.m1.7.7.1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.1.3.3.2">𝑦</ci><list id="S3.E3.m1.6.6.2.3.cmml" xref="S3.E3.m1.6.6.2.2"><apply id="S3.E3.m1.6.6.2.2.1.cmml" xref="S3.E3.m1.6.6.2.2.1"><times id="S3.E3.m1.6.6.2.2.1.1.cmml" xref="S3.E3.m1.6.6.2.2.1.1"></times><ci id="S3.E3.m1.6.6.2.2.1.2.cmml" xref="S3.E3.m1.6.6.2.2.1.2">𝑡</ci><ci id="S3.E3.m1.6.6.2.2.1.3.cmml" xref="S3.E3.m1.6.6.2.2.1.3">𝑟</ci><ci id="S3.E3.m1.6.6.2.2.1.4.cmml" xref="S3.E3.m1.6.6.2.2.1.4">𝑢</ci><ci id="S3.E3.m1.6.6.2.2.1.5.cmml" xref="S3.E3.m1.6.6.2.2.1.5">𝑒</ci></apply><ci id="S3.E3.m1.5.5.1.1.cmml" xref="S3.E3.m1.5.5.1.1">𝑖</ci></list></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.7c">SpeechLoss=2*\frac{1}{N}\sum_{i=1}^{N}|(y_{true,i})^{2}-y_{pred,i}*y_{true,i}|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">The final loss function is a sum of the MAE and the speech loss terms:</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="Loss=MAE+SpeechLoss" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mrow id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.2.2" xref="S3.E4.m1.1.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.2.1" xref="S3.E4.m1.1.1.2.1.cmml">​</mo><mi id="S3.E4.m1.1.1.2.3" xref="S3.E4.m1.1.1.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.2.1a" xref="S3.E4.m1.1.1.2.1.cmml">​</mo><mi id="S3.E4.m1.1.1.2.4" xref="S3.E4.m1.1.1.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.2.1b" xref="S3.E4.m1.1.1.2.1.cmml">​</mo><mi id="S3.E4.m1.1.1.2.5" xref="S3.E4.m1.1.1.2.5.cmml">s</mi></mrow><mo id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mrow id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml"><mi id="S3.E4.m1.1.1.3.2.2" xref="S3.E4.m1.1.1.3.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.1" xref="S3.E4.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.2.3" xref="S3.E4.m1.1.1.3.2.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.2.1a" xref="S3.E4.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.2.4" xref="S3.E4.m1.1.1.3.2.4.cmml">E</mi></mrow><mo id="S3.E4.m1.1.1.3.1" xref="S3.E4.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml"><mi id="S3.E4.m1.1.1.3.3.2" xref="S3.E4.m1.1.1.3.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.1" xref="S3.E4.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.3" xref="S3.E4.m1.1.1.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.1a" xref="S3.E4.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.4" xref="S3.E4.m1.1.1.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.1b" xref="S3.E4.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.5" xref="S3.E4.m1.1.1.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.1c" xref="S3.E4.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.6" xref="S3.E4.m1.1.1.3.3.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.1d" xref="S3.E4.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.7" xref="S3.E4.m1.1.1.3.3.7.cmml">h</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.1e" xref="S3.E4.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.8" xref="S3.E4.m1.1.1.3.3.8.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.1f" xref="S3.E4.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.9" xref="S3.E4.m1.1.1.3.3.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.1g" xref="S3.E4.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.10" xref="S3.E4.m1.1.1.3.3.10.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.3.3.1h" xref="S3.E4.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E4.m1.1.1.3.3.11" xref="S3.E4.m1.1.1.3.3.11.cmml">s</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"></eq><apply id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"><times id="S3.E4.m1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.2.1"></times><ci id="S3.E4.m1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.2.2">𝐿</ci><ci id="S3.E4.m1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.2.3">𝑜</ci><ci id="S3.E4.m1.1.1.2.4.cmml" xref="S3.E4.m1.1.1.2.4">𝑠</ci><ci id="S3.E4.m1.1.1.2.5.cmml" xref="S3.E4.m1.1.1.2.5">𝑠</ci></apply><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><plus id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3.1"></plus><apply id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2"><times id="S3.E4.m1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.3.2.1"></times><ci id="S3.E4.m1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.3.2.2">𝑀</ci><ci id="S3.E4.m1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.3.2.3">𝐴</ci><ci id="S3.E4.m1.1.1.3.2.4.cmml" xref="S3.E4.m1.1.1.3.2.4">𝐸</ci></apply><apply id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3"><times id="S3.E4.m1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.3.3.1"></times><ci id="S3.E4.m1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.3.3.2">𝑆</ci><ci id="S3.E4.m1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.3.3.3">𝑝</ci><ci id="S3.E4.m1.1.1.3.3.4.cmml" xref="S3.E4.m1.1.1.3.3.4">𝑒</ci><ci id="S3.E4.m1.1.1.3.3.5.cmml" xref="S3.E4.m1.1.1.3.3.5">𝑒</ci><ci id="S3.E4.m1.1.1.3.3.6.cmml" xref="S3.E4.m1.1.1.3.3.6">𝑐</ci><ci id="S3.E4.m1.1.1.3.3.7.cmml" xref="S3.E4.m1.1.1.3.3.7">ℎ</ci><ci id="S3.E4.m1.1.1.3.3.8.cmml" xref="S3.E4.m1.1.1.3.3.8">𝐿</ci><ci id="S3.E4.m1.1.1.3.3.9.cmml" xref="S3.E4.m1.1.1.3.3.9">𝑜</ci><ci id="S3.E4.m1.1.1.3.3.10.cmml" xref="S3.E4.m1.1.1.3.3.10">𝑠</ci><ci id="S3.E4.m1.1.1.3.3.11.cmml" xref="S3.E4.m1.1.1.3.3.11">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">Loss=MAE+SpeechLoss</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.1" class="ltx_p">The use of this custom loss function ensures that the model not only minimizes the overall error in amplitude prediction but also prioritizes the preservation of speech components over noise reduction.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Dataset</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The underlying data for our methodology comes from the Device and Produced Speech (DAPS) Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. The DAPS dataset houses 100 authentic and clean speech recordings, scrupulously cleaned of breaths and lip smacks. For effective learning and unbiased evaluation, the data is partitioned into training, validation, and testing repositories. Sizes for these sets are 213.5, 30.2, and 23.8 minutes respectively.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Model Training </h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The model training phase revolves around a log spectrogram which operates on a 22,050Hz sampling rate. Our approach necessitates a frame length of 4096 and a frame step of 512. Additional parameters include a ’hann’ window function and the setting of ’center’ to True for more precise frequency representation. To facilitate a quick and precise learning process, an V100 GPU with its high computation power is exploited. The training procures the use of Adam optimization and incorporates an early stopping mechanism that continuously scrutinizes validation loss to ensure the model’s prevention from overfitting and assures achievement of an optimal state.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this results section, we detail the performance of our model that’s designed to remove non-speech sounds, such as breaths in vocal recordings. We compare it with two of the best models currently available, Inception and MobileNetV2, referenced in the work of Ramirez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Our model’s accuracy is measured by its ability to identify and remove breath sounds from the recordings, which is presented as a percentage. Along with accuracy, we also use the Mel-Frequency Cepstral Coefficients (MFCC) Distance to measure spectral precision, and the Perceptual Evaluation of Speech Quality (PESQ) to assess the clarity of speech. The detailed results are shown in Table <a href="#S4.T1" title="Table 1 ‣ 4 Results ‣ Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, highlighting our model’s effectiveness in improving the quality of vocal recordings by removing unwanted breath sounds.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.1.1" class="ltx_p" style="width:113.8pt;"><span id="S4.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></span>
</span>
</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.2.1.1" class="ltx_p" style="width:113.8pt;"><span id="S4.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">MFCC Distance</span></span>
</span>
</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">PESQ</span></span>
</span>
</th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Accuracy</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.1.1.1" class="ltx_p" style="width:113.8pt;">Proposed Model</span>
</span>
</td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.2.1.1" class="ltx_p" style="width:113.8pt;">0.0371</span>
</span>
</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.3.1.1" class="ltx_p" style="width:56.9pt;">3.8433</span>
</span>
</td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T1.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.4.1.1" class="ltx_p" style="width:56.9pt;">97%</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.1.1.1" class="ltx_p" style="width:113.8pt;">Inception</span>
</span>
</td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.2.1.1" class="ltx_p" style="width:113.8pt;">0.0186</span>
</span>
</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.3.1.1" class="ltx_p" style="width:56.9pt;">3.9452</span>
</span>
</td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T1.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.4.1.1" class="ltx_p" style="width:56.9pt;">98%</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.1.1.1" class="ltx_p" style="width:113.8pt;">MobileNetV2</span>
</span>
</td>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.2.1.1" class="ltx_p" style="width:113.8pt;">0.0231</span>
</span>
</td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.3.1.1" class="ltx_p" style="width:56.9pt;">3.9448</span>
</span>
</td>
<td id="S4.T1.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T1.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.4.1.1" class="ltx_p" style="width:56.9pt;">97%</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance comparison on various metrics</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The MFCC Distance for the proposed model is 0.0371, which is slightly higher than the Inception and MobileNetV2 models, which have an MFCC Distance of 0.0186 and 0.0231 respectively. The MFCC Distance is a measure of the dissimilarity between the original and processed signals, with a lower value indicating a closer match to the original signal. Therefore, while our proposed model does not outperform the Inception and MobileNetV2 models in terms of MFCC Distance, the difference is not substantial, indicating that our model is still highly competitive.The Perceptual Evaluation of Speech Quality (PESQ) scores for our proposed model and the other two models are quite close, with our model having a PESQ score of 3.8433, slightly lower than the Inception’s score of 3.9452 and MobileNetV2’s score of 3.9448. PESQ is a metric that assesses the perceived speech quality after the sound processing, with a higher score indicating better perceived quality. Most importantly, the accuracy of our proposed model in removing breath sounds from vocal recordings is commendable, with a score of 97%. This is on par with the MobileNetV2 model and only slightly less than the Inception model, which has an accuracy of 98%.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Figures <a href="#S4.F2" title="Figure 2 ‣ 4 Results ‣ Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S4.F3" title="Figure 3 ‣ 4 Results ‣ Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrate the original waveform and the waveform post the removal of the breath from the vocal recording. The highlighted box in these figures signifies the existence of breath sound.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2409.04949/assets/images/Org-wv.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Original Waveform</figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2409.04949/assets/images/Gen_wv.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Waveform after Breath Removal</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2409.04949/assets/images/Original_Audio.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Original Spectrogram</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2409.04949/assets/images/Target_Audio.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Target Spectrogram</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2409.04949/assets/images/Generated_Audio.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Model Generated Spectrogram</figcaption>
</figure>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Figures 4, 5, and 6 represent the spectrogram view of the original audio, the
target audio (manually breath sound removed vocal recording), and the model
generated vocal recording, respectively. This demonstrates the model’s ability to
filter out unwanted noise without affecting the overall quality of the recording.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> compares the number of parameters, epochs, and time taken for each model.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.1.1.1" class="ltx_p" style="width:142.3pt;"><span id="S4.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></span>
</span>
</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T2.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Parameters</span></span>
</span>
</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.3.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T2.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Epochs</span></span>
</span>
</th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T2.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T2.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Time (hours)</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T2.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.1.1.1.1" class="ltx_p" style="width:142.3pt;">Proposed Model</span>
</span>
</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.1.2.1.1" class="ltx_p" style="width:56.9pt;">1.9M</span>
</span>
</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.1.3.1.1" class="ltx_p" style="width:56.9pt;">60</span>
</span>
</td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.1.4.1.1" class="ltx_p" style="width:56.9pt;">3.2</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T2.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.2.1.1.1" class="ltx_p" style="width:142.3pt;">Inception</span>
</span>
</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.2.2.1.1" class="ltx_p" style="width:56.9pt;">2.8M</span>
</span>
</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.2.3.1.1" class="ltx_p" style="width:56.9pt;">89</span>
</span>
</td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.2.4.1.1" class="ltx_p" style="width:56.9pt;">7.4</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T2.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.3.1.1.1" class="ltx_p" style="width:142.3pt;">MobileNetV2</span>
</span>
</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T2.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.3.2.1.1" class="ltx_p" style="width:56.9pt;">2.2M</span>
</span>
</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T2.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.3.3.1.1" class="ltx_p" style="width:56.9pt;">60</span>
</span>
</td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T2.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.3.4.1.1" class="ltx_p" style="width:56.9pt;">4.8</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of the number of epochs and time taken</figcaption>
</figure>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">Our proposed model is efficient, with fewer parameters (1.9M) and less time taken (3.2 hours) for training compared to the other models<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
. This underlines the efficiency of our approach, which is able to generate nearly identical output to previous models, but with a more streamlined process.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The study showcases a parameter-efficient model that utilizes deep learning to accurately detect and remove non-speech vocal sounds like breath noises from vocal recordings. The model excels in post-processing, reducing manual work significantly thus, benefitting sound engineering. However, the real potential lies in extending the model to real-time systems such as live broadcasts or interactive voice response systems. In spite of using fewer parameters, the model yielded better results than existing models, as confirmed by audio quality metrics. Future work could focus on devising objective methods to evaluate sound removal, exploring other deep learning approaches, and expanding the dataset to include varied vocal recordings for broader non-speech sound management.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This research has introduced a parameter-efficient model, which utilizes the attention U-Net architecture for the automatic detection and removal of breath sounds in vocal recordings. Demonstrating a significant need for research in this area in the sound engineering field, the paper emphasizes the inefficiencies and inaccuracies of previous automated methods. Our approach, characterized by its innovation and superior performance, leverages advanced deep learning techniques and a carefully constructed methodology for effectively capture local and expansive context within the spectrogram derived from the STFT of the audio file, thereby enabling accurate and efficient detection and removal of breath sounds. Therefore, our research holds significant pertinence in the sound engineering domain and can be used by voice-over artists, singing artists, and other audio recording professionals to improve the quality of their audio files efficiently without the inconvenience of manual editing.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. Owsinski, <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">The mixing engineer’s handbook</span>.

</span>
<span class="ltx_bibblock">Course Technology, Cengage Learning, 2014.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Terrell, J. D. Reiss, and M. Sandler, “Automatic noise gate settings for drum recordings containing bleed from secondary sources,” <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">EURASIP Journal on Advances in Signal Processing</span>, vol. 2010, pp. 1–9, 2011.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
X. Amatriain, <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">DAFX: digital audio effects</span>.

</span>
<span class="ltx_bibblock">Wiley, 2011.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. A. M. Ramírez, O. Wang, P. Smaragdis, and N. J. Bryan, “Differentiable signal processing with black-box audio effects,” in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pp. 66–70, IEEE, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Mehrish, N. Majumder, R. Bharadwaj, R. Mihalcea, and S. Poria, “A review of deep learning techniques for speech processing,” <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Information Fusion</span>, p. 101869, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D. Ruinskiy and Y. Lavner, “An effective algorithm for automatic detection and exact demarcation of breath sounds in speech and song signals,” <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">IEEE transactions on audio, speech, and language processing</span>, vol. 15, no. 3, pp. 838–850, 2007.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
V. Rapcan, S. D’arcy, N. Penard, I. H. Robertson, and R. B. Reilly, “The use of telephone speech recordings for assessment and monitoring of cognitive function in elderly people,” in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Tenth Annual Conference of the International Speech Communication Association</span>, 2009.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. Igras and B. Ziólko, “Wavelet method for breath detection in audio signals,” in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">2013 IEEE International Conference on Multimedia and Expo (ICME)</span>, pp. 1–6, IEEE, 2013.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. H. Dumpala and K. R. Alluri, “An algorithm for detection of breath sounds in spontaneous speech with application to speaker recognition,” in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Speech and Computer: 19th International Conference, SPECOM 2017, Hatfield, UK, September 12-16, 2017, Proceedings 19</span>, pp. 98–108, Springer, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T. Fukuda, O. Ichikawa, and M. Nishimura, “Detecting breathing sounds in realistic japanese telephone conversations and its application to automatic speech recognition,” <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Speech Communication</span>, vol. 98, pp. 95–103, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz, <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Attention u-net: Learning where to look for the pancreas,” <span id="bib.bib11.2.2" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.03999</span>, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
D. Griffin and J. Lim, “Signal estimation from modified short-time fourier transform,” <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on acoustics, speech, and signal processing</span>, vol. 32, no. 2, pp. 236–243, 1984.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. Shimauchi, S. Kudo, Y. Koizumi, and K. Furuya, “On relationships between amplitude and phase of short-time fourier transform,” in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pp. 676–680, IEEE, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C. J. Willmott and K. Matsuura, “Advantages of the mean absolute error (mae) over the root mean square error (rmse) in assessing average model performance,” <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Climate research</span>, vol. 30, no. 1, pp. 79–82, 2005.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
G. J. Mysore, “Can we automatically transform speech recorded on common consumer devices in real-world environments into professional production quality speech?—a dataset, insights, and challenges,” <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Letters</span>, vol. 22, no. 8, pp. 1006–1010, 2015.

</span>
</li>
</ul>
</section>
<div id="id1a" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_left"><span id="id1.p1.1.1" class="ltx_text" style="font-size:80%;">© 2024 By AIRCC Publishing Corporation. This article is published under the Creative Commons Attribution (CC BY) license.</span></p>
</div>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.04948" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.04949" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.04949">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.04949" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.04950" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:48:29 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
