<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.13982] Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance</title><meta property="og:description" content="Automatic speech recognition (ASR) models trained on large amounts of audio data are now widely used to convert speech to written text in a variety of applications from video captioning to automated assistants used in …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.13982">

<!--Generated on Mon Aug  5 18:39:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Changye Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{changyel, cohenta</span>}@uw.edu
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Trevor Cohen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter">{changyel, cohenta</span>}@uw.edu
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Serguei Pakhomov
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter">{pakh0002</span>}@umn.edu
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Automatic speech recognition (ASR) models trained on large amounts of audio data are now widely used to convert speech to written text in a variety of applications from video captioning to automated assistants used in healthcare and other domains. As such, it is important that ASR models and their use is fair and equitable. Prior work examining the performance of commercial ASR systems on the Corpus of Regional African American Language (CORAAL) demonstrated significantly worse ASR performance on African American English (AAE). The current study seeks to understand the factors underlying this disparity by examining the performance of the current state-of-the-art neural network based ASR system (Whisper, OpenAI) on the CORAAL dataset. Two key findings have been identified as a result of the current study. The first confirms prior findings of significant dialectal variation even across neighboring communities, and worse ASR performance on AAE that can be improved to some extent with fine-tuning of ASR models. The second is a novel finding not discussed in prior work on CORAAL: differences in audio recording practices within the dataset have a significant impact on ASR accuracy resulting in a “confounding by provenance” effect in which both language use and recording quality differ by study location. These findings highlight the need for further systematic investigation to disentangle the effects of recording quality and inherent linguistic diversity when examining the fairness and bias present in neural ASR models, as any bias in ASR accuracy may have negative downstream effects on disparities in various domains of life in which ASR technology is used.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_bold">Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;"></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Automatic speech recognition (ASR) models have significantly advanced in recent years due to the introduction of self-supervised learning and pre-training techniques. Pre-training an artificial neural network model to predict the next element in the sequence, such as the next word or the next audio frame, using very large amounts of unlabeled data (e.g. hundreds of thousands of hours of speech) results in a foundational set of neural representations that can then readily adapt to various downstream tasks and applications such as speech-to-text transcription <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib13" title="" class="ltx_ref">2024</a>; Xu et al., <a href="#bib.bib29" title="" class="ltx_ref">2022</a>; Wang et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>; Solinsky et al., <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>. The representations that artificial neural network models learn from the data during pre-training, however, are vulnerable to various biases that may be present in the training data including racial, ethnic and gender biases <cite class="ltx_cite ltx_citemacro_citep">(Koenecke et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Silva et al., <a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite>. The transfer of these biases from data to pre-trained models have significant implications for fair and equitable use of this ASR technology that is becoming ubiquitous. It is critical to recognize these biases and develop methods that can eliminate them or alleviate their negative impact on society.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recent studies, driven by a growing concern for racial disparities in AI <cite class="ltx_cite ltx_citemacro_citep">(Field et al., <a href="#bib.bib4" title="" class="ltx_ref">2021</a>; Blodgett and O’Connor, <a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite>, reported that commercial ASR systems perform significantly worse on African American English (AAE) speech <cite class="ltx_cite ltx_citemacro_citep">(Koenecke et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Martin and Tang, <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> from the Corpus of Regional African American Language (CORAAL) <cite class="ltx_cite ltx_citemacro_citep">(Farrington and Kendall, <a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://oraal.uoregon.edu/coraal/components" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://oraal.uoregon.edu/coraal/components</a></span></span></span>. These studies also reported large variation across CORAAL geographical subsets. For example, ASR had lowest accuracy on speech of participants from Princeville, NC, followed closely by speech from the Washington, DC area participants. However, ASR performance on AAE speech from participants residing in Rochester, NY was very similar to performance on non-AAE speech from participants from Sacramento and Humbolt, CA. Koenecke et al. <cite class="ltx_cite ltx_citemacro_citep">(Koenecke et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> attributed this discrepancy to higher dialect density (proportion linguistic features unique to AAE) in speech samples from Princeville and Washington as compared to other geographical locations/dialect variants in the CORAAL dataset.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">A comprehensive evaluation of the state-of-the-art neural ASR model, OpenAI’s Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>, demonstrated substantially better performance than previous ASR systems on all standard speech benchmarks including the CORAAL dataset. However, in this evaluation the OpenAI team did not specifically focus on examining disparities and only reported overall ASR performance results without stratifying on geographical subsets of the CORAAL data.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">One of the characteristics of the CORAAL dataset is that it represents a growing “living” collection. It relies on a diverse set of data collection practices, as it is continually updated with more audio data from African American participants being collected by multiple investigators using a variety of data collection protocols. For example, some of the earlier subsets of CORAAL were collected using analog equipment (i.e., magnetic tape) and subsequently digitized, and some were collected using professional digital recording equipment. This diversity of data collection practices prompted us to examine more deeply the relationship between the manner in which the audio was collected and the equipment with which it was collected, and the ASR performance results. To the best of our knowledge, none of the previous studies of ASR on the CORAAL dataset examined the impact of the variability in recording quality on ASR accuracy. We hypothesized that variability in audio quality resulting from variable recording protocols and equipment may act as a confounding variable in examining race and dialect-related disparities in ASR performance on CORAAL.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We test this hypothesis on the 2021 snapshot of the CORAAL dataset that is drawn from six geographical locations (ATL - Atlanta, Georgia; DCA, DCB - Washington, District Columbia; LES - Lower east side of Manhattan, New York; PRV - Princeville, North Carolina; ROC - Rochester, New York; VLD - Valdasta, Georgia), with audio collected using digital recorders in four of these locations and analog tape recorders in the other two. One of these latter two locations (i.e., PRV - Princeville, NC) also happened to be the setting for a documentary film <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">This Side of The River - The Story of Princeville</span> in which Princeville residents are interviewed in a nearly identical fashion to the CORAAL PRV protocol. As such, the only major relevant difference between the audio in the PRV subset of CORAAL and the documentary is the quality of the recording equipment. In contrast to the PRV subset, the documentary was recorded using high quality professional digital audio-visual equipment. In this study, we compared ASR accuracy on these two sources of audio and found that the AAE speech in the documentary was transcribed with much higher accuracy by Whisper than the speech in the PRV subset of CORAAL.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The contributions of the work presented in this paper can be summarized as follows: a) we provide preliminary evidence suggesting that audio recording quality may negatively impact ASR performance on CORAAL data and needs to be taken into account as a potential confounding factor in studying racial disparities; b) we provide computational evidence in support of the view that AAE represents a wide variety of distinct variants even across geographically closely-positioned AAE communities; c) our findings suggest that while fine-tuning pre-trained models on domain-specific corpora can improve ASR performance, this approach alone is not sufficient for achieving robust adaptations to the rich linguistic diversity observed within AAE. The improvements we were able to make with fine-tuning are relatively small and do not appear to generalize beyond the CORAAL subset used for fine-tuning<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Our code is available at <a target="_blank" href="https://github.com/LinguisticAnomalies/confounding-ASR" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/LinguisticAnomalies/confounding-ASR</a></span></span></span>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Materials and Methods</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Dataset</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We evaluate our hypothesis using the 2021 release of the CORAAL dataset consisting of 231 interviews, which includes audio recordings from African American residents who live in the following geographical locations: 1) ATL - Atlanta, Georgia; 2) DCA, DCB - Washington, District Columbia (temporal subsets A and B); 3) LES - lower east side of Manhattan, New York City, New York; 4) PRV - Princeville, North Carolina; 5) ROC - Rochester, New York; and 6) Valdasta, Georgia. The DCA and PRV subsets were originally recorded using reel-to-reel and cassette tape recorders. The remaining data subsets were recorded using professional digital devices. However, while LES was recorded digitally, the recording was done without a dedicated interviewee microphone in contrast to the other protocols. Data characteristics are provided in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Dataset ‣ 2 Materials and Methods ‣ Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We used TRESTLE (<span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_bold">T</span>oolkit for <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_bold">R</span>eproducible <span id="S2.SS1.p1.1.3" class="ltx_text ltx_font_bold">E</span>xecution of <span id="S2.SS1.p1.1.4" class="ltx_text ltx_font_bold">S</span>peech <span id="S2.SS1.p1.1.5" class="ltx_text ltx_font_bold">T</span>ext and <span id="S2.SS1.p1.1.6" class="ltx_text ltx_font_bold">L</span>anguage <span id="S2.SS1.p1.1.7" class="ltx_text ltx_font_bold">E</span>xperiments) <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> to perform text and audio preprocessing and split utterances into 50/20/30 training/test/validation subsets. Specifically, we defined a “clean” subset of CORAAL text utterances that contained only speech from the interviewees, and had no overlapping speech or inaudible/unintelligible non-linguistic sounds. We conducted basic preprocessing on the text utterances. Since all numerical values were presented as complete words, we established a simple rule-based method for inverse text normalization to revert them to numerical form. Additionally, we removed any characters in the text utterances that were not letters, digits, whitespace, or single quotes. For the audio recordings, we resampled them at a rate of 16000 (16k) Hz and partitioned them into audio utterances aligned with the text utterances.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_align_left"><span class="ltx_tag ltx_tag_table">Table 1: </span>Characteristics of CORAAL 2021 release. The <span id="S2.T1.4.1" class="ltx_text ltx_font_typewriter">Digital?</span> column represents if the subset is <span id="S2.T1.5.2" class="ltx_text ltx_font_bold">originally</span> recorded with digital format. The <span id="S2.T1.6.3" class="ltx_text ltx_font_typewriter">Urban?</span> column represents if the subset is located in the urban area.</figcaption>
<table id="S2.T1.7" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.7.1.1" class="ltx_tr">
<th id="S2.T1.7.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" rowspan="2"><span id="S2.T1.7.1.1.1.1" class="ltx_text ltx_font_bold">Subset</span></th>
<th id="S2.T1.7.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" rowspan="2"><span id="S2.T1.7.1.1.2.1" class="ltx_text ltx_font_bold">Digital?</span></th>
<th id="S2.T1.7.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" rowspan="2"><span id="S2.T1.7.1.1.3.1" class="ltx_text ltx_font_bold">Urban?</span></th>
<th id="S2.T1.7.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" rowspan="2"><span id="S2.T1.7.1.1.4.1" class="ltx_text ltx_font_bold"># of Speakers</span></th>
<th id="S2.T1.7.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3"><span id="S2.T1.7.1.1.5.1" class="ltx_text ltx_font_bold"># of Utterances</span></th>
</tr>
<tr id="S2.T1.7.2.2" class="ltx_tr">
<th id="S2.T1.7.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Training</th>
<th id="S2.T1.7.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Test</th>
<th id="S2.T1.7.2.2.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">Validation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.7.3.1" class="ltx_tr">
<td id="S2.T1.7.3.1.1" class="ltx_td ltx_align_center ltx_border_t">ATL, GA</td>
<td id="S2.T1.7.3.1.2" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S2.T1.7.3.1.3" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S2.T1.7.3.1.4" class="ltx_td ltx_align_center ltx_border_t">13</td>
<td id="S2.T1.7.3.1.5" class="ltx_td ltx_align_center ltx_border_t">3,083</td>
<td id="S2.T1.7.3.1.6" class="ltx_td ltx_align_center ltx_border_t">1,233</td>
<td id="S2.T1.7.3.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">1,849</td>
</tr>
<tr id="S2.T1.7.4.2" class="ltx_tr">
<td id="S2.T1.7.4.2.1" class="ltx_td ltx_align_center">DCA, DC</td>
<td id="S2.T1.7.4.2.2" class="ltx_td ltx_align_center">✗</td>
<td id="S2.T1.7.4.2.3" class="ltx_td ltx_align_center">✓</td>
<td id="S2.T1.7.4.2.4" class="ltx_td ltx_align_center">68</td>
<td id="S2.T1.7.4.2.5" class="ltx_td ltx_align_center">18,678</td>
<td id="S2.T1.7.4.2.6" class="ltx_td ltx_align_center">7,472</td>
<td id="S2.T1.7.4.2.7" class="ltx_td ltx_nopad_r ltx_align_center">11,207</td>
</tr>
<tr id="S2.T1.7.5.3" class="ltx_tr">
<td id="S2.T1.7.5.3.1" class="ltx_td ltx_align_center">DCB, DC</td>
<td id="S2.T1.7.5.3.2" class="ltx_td ltx_align_center">✓</td>
<td id="S2.T1.7.5.3.3" class="ltx_td ltx_align_center">✓</td>
<td id="S2.T1.7.5.3.4" class="ltx_td ltx_align_center">48</td>
<td id="S2.T1.7.5.3.5" class="ltx_td ltx_align_center">27,195</td>
<td id="S2.T1.7.5.3.6" class="ltx_td ltx_align_center">10,878</td>
<td id="S2.T1.7.5.3.7" class="ltx_td ltx_nopad_r ltx_align_center">16,318</td>
</tr>
<tr id="S2.T1.7.6.4" class="ltx_tr">
<td id="S2.T1.7.6.4.1" class="ltx_td ltx_align_center">LES, NY</td>
<td id="S2.T1.7.6.4.2" class="ltx_td ltx_align_center">✓</td>
<td id="S2.T1.7.6.4.3" class="ltx_td ltx_align_center">✓</td>
<td id="S2.T1.7.6.4.4" class="ltx_td ltx_align_center">10</td>
<td id="S2.T1.7.6.4.5" class="ltx_td ltx_align_center">4,162</td>
<td id="S2.T1.7.6.4.6" class="ltx_td ltx_align_center">1,665</td>
<td id="S2.T1.7.6.4.7" class="ltx_td ltx_nopad_r ltx_align_center">2,496</td>
</tr>
<tr id="S2.T1.7.7.5" class="ltx_tr">
<td id="S2.T1.7.7.5.1" class="ltx_td ltx_align_center">PRV, NC</td>
<td id="S2.T1.7.7.5.2" class="ltx_td ltx_align_center">✗</td>
<td id="S2.T1.7.7.5.3" class="ltx_td ltx_align_center">✗</td>
<td id="S2.T1.7.7.5.4" class="ltx_td ltx_align_center">16</td>
<td id="S2.T1.7.7.5.5" class="ltx_td ltx_align_center">7,053</td>
<td id="S2.T1.7.7.5.6" class="ltx_td ltx_align_center">2,821</td>
<td id="S2.T1.7.7.5.7" class="ltx_td ltx_nopad_r ltx_align_center">4,232</td>
</tr>
<tr id="S2.T1.7.8.6" class="ltx_tr">
<td id="S2.T1.7.8.6.1" class="ltx_td ltx_align_center">ROC, NY</td>
<td id="S2.T1.7.8.6.2" class="ltx_td ltx_align_center">✓</td>
<td id="S2.T1.7.8.6.3" class="ltx_td ltx_align_center">✓</td>
<td id="S2.T1.7.8.6.4" class="ltx_td ltx_align_center">15</td>
<td id="S2.T1.7.8.6.5" class="ltx_td ltx_align_center">7,990</td>
<td id="S2.T1.7.8.6.6" class="ltx_td ltx_align_center">3,196</td>
<td id="S2.T1.7.8.6.7" class="ltx_td ltx_nopad_r ltx_align_center">4,795</td>
</tr>
<tr id="S2.T1.7.9.7" class="ltx_tr">
<td id="S2.T1.7.9.7.1" class="ltx_td ltx_align_center">VLD, GA</td>
<td id="S2.T1.7.9.7.2" class="ltx_td ltx_align_center">✓</td>
<td id="S2.T1.7.9.7.3" class="ltx_td ltx_align_center">✗</td>
<td id="S2.T1.7.9.7.4" class="ltx_td ltx_align_center">12</td>
<td id="S2.T1.7.9.7.5" class="ltx_td ltx_align_center">6,601</td>
<td id="S2.T1.7.9.7.6" class="ltx_td ltx_align_center">2,640</td>
<td id="S2.T1.7.9.7.7" class="ltx_td ltx_nopad_r ltx_align_center">3,961</td>
</tr>
<tr id="S2.T1.7.10.8" class="ltx_tr">
<td id="S2.T1.7.10.8.1" class="ltx_td ltx_align_center ltx_border_bb">Total</td>
<td id="S2.T1.7.10.8.2" class="ltx_td ltx_align_center ltx_border_bb">NA</td>
<td id="S2.T1.7.10.8.3" class="ltx_td ltx_align_center ltx_border_bb">NA</td>
<td id="S2.T1.7.10.8.4" class="ltx_td ltx_align_center ltx_border_bb">182</td>
<td id="S2.T1.7.10.8.5" class="ltx_td ltx_align_center ltx_border_bb">74,762</td>
<td id="S2.T1.7.10.8.6" class="ltx_td ltx_align_center ltx_border_bb">29,905</td>
<td id="S2.T1.7.10.8.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">44,858</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">We also used the publicly available video <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">This Side of The River - The Story of Princeville<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span id="footnote3.1.1.1" class="ltx_text ltx_font_upright">3</span></span><a target="_blank" href="https://www.youtube.com/watch?v=KhRUSZoJ5_Y" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://www.youtube.com/watch?v=KhRUSZoJ5_Y</a></span></span></span></span> to further analyze the confounding factor of recording quality. The video is recorded with professional devices with high audio quality featuring interviews of residents of Princeville on a variety of everyday topics. Specifically, we focused on 316 utterances produced by local Princeville African American residents. We followed the same preprocessing pipeline used for the CORAAL dataset.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>ASR Model</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Whisper is a seq2seq (i.e. encoder-decoder) ASR model pretrained on 680,000 hours of multilingual and multitask supervised data collected from the internet. Whisper resamples and splits audio inputs into 16 kHz 30 seconds chunks. The audio chunks are converted into 80-channel log-
magnitude Mel spectrogram representation using convolution neural networks (CNNs) and fed into the encoder layers to learn acoustic representations. The decoder layers, serving as a (less powerful) language model, decode the acoustic representations to the most likely sequence of text tokens with respect to the audio frame.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In our study, we evaluated both pre-trained and fine-tuned whisper-large-v2 on the test and validation set. We fine-tuned the pre-trained Whisper on the training set with a batch size of 16 over 4 epochs while freezing the encoder layers. Whisper employs an internal text normalization (also commonly referred as inverse text normalization) to convert text from verbalized form into its written form (e.g., “one hundred and twenty-three dollars” to “$123”). Specifically, we disabled Whisper’s internal text normalization during the decoding stage.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>ASR Evaluation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.4" class="ltx_p">ASR performance was evaluated using standard measures of word error rate (WER) formally defined in Equation <a href="#S2.E1" title="In 2.3 ASR Evaluation ‣ 2 Materials and Methods ‣ Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, where <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">S</annotation></semantics></math> is the number of substitutions, <math id="S2.SS3.p1.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.SS3.p1.2.m2.1a"><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">D</annotation></semantics></math> is the number of deletions, <math id="S2.SS3.p1.3.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S2.SS3.p1.3.m3.1a"><mi id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><ci id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">I</annotation></semantics></math> is the number of insertions, and <math id="S2.SS3.p1.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS3.p1.4.m4.1a"><mi id="S2.SS3.p1.4.m4.1.1" xref="S2.SS3.p1.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.4.m4.1b"><ci id="S2.SS3.p1.4.m4.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.4.m4.1c">N</annotation></semantics></math> is the number of words in the reference. A higher WER indicates a greater difference between the hypothesis (ASR-generated transcript) and the reference (manually transcribed verbatim utterance), indicating worse ASR performance.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="WER=\frac{S+D+I}{N}" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mrow id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.2.2" xref="S2.E1.m1.1.1.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.2.1" xref="S2.E1.m1.1.1.2.1.cmml">​</mo><mi id="S2.E1.m1.1.1.2.3" xref="S2.E1.m1.1.1.2.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.2.1a" xref="S2.E1.m1.1.1.2.1.cmml">​</mo><mi id="S2.E1.m1.1.1.2.4" xref="S2.E1.m1.1.1.2.4.cmml">R</mi></mrow><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">=</mo><mfrac id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mrow id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml"><mi id="S2.E1.m1.1.1.3.2.2" xref="S2.E1.m1.1.1.3.2.2.cmml">S</mi><mo id="S2.E1.m1.1.1.3.2.1" xref="S2.E1.m1.1.1.3.2.1.cmml">+</mo><mi id="S2.E1.m1.1.1.3.2.3" xref="S2.E1.m1.1.1.3.2.3.cmml">D</mi><mo id="S2.E1.m1.1.1.3.2.1a" xref="S2.E1.m1.1.1.3.2.1.cmml">+</mo><mi id="S2.E1.m1.1.1.3.2.4" xref="S2.E1.m1.1.1.3.2.4.cmml">I</mi></mrow><mi id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml">N</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><eq id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"></eq><apply id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"><times id="S2.E1.m1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.2.1"></times><ci id="S2.E1.m1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.2.2">𝑊</ci><ci id="S2.E1.m1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.2.3">𝐸</ci><ci id="S2.E1.m1.1.1.2.4.cmml" xref="S2.E1.m1.1.1.2.4">𝑅</ci></apply><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><divide id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3"></divide><apply id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2"><plus id="S2.E1.m1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2.1"></plus><ci id="S2.E1.m1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2">𝑆</ci><ci id="S2.E1.m1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3">𝐷</ci><ci id="S2.E1.m1.1.1.3.2.4.cmml" xref="S2.E1.m1.1.1.3.2.4">𝐼</ci></apply><ci id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">WER=\frac{S+D+I}{N}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Character-level Language Models (LMs)</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.5" class="ltx_p">To better understand the geographical linguistic patterns, We trained a transformer-based character-level autoregressive (i.e., trained to predict the next token sequentially from left to right, or <span id="S2.SS4.p1.5.1" class="ltx_text ltx_font_italic">unidirectionally</span>) language model (LM) on each subset of CORAAL to evaluate its ability to predict characters in the remaining CORAAL subsets. We chose to train character-level LMs because they are likely to be more robust to linguistic variation often observed in AAE, and out-of-vocabulary words (i.e., tokens found in testing data but absent in the training data). A standard performance metric for LMs is the measure of perplexity (PPL), which is a derivative of cross-entropy. PPL is formally defined in Equation <a href="#S2.E2" title="In 2.4 Character-level Language Models (LMs) ‣ 2 Materials and Methods ‣ Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where <math id="S2.SS4.p1.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS4.p1.1.m1.1a"><mi id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><ci id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">S</annotation></semantics></math> denotes a sequence of <math id="S2.SS4.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS4.p1.2.m2.1a"><mi id="S2.SS4.p1.2.m2.1.1" xref="S2.SS4.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.2.m2.1b"><ci id="S2.SS4.p1.2.m2.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.2.m2.1c">n</annotation></semantics></math> characters <math id="S2.SS4.p1.3.m3.3" class="ltx_Math" alttext="(c_{0},\cdots,c_{n})" display="inline"><semantics id="S2.SS4.p1.3.m3.3a"><mrow id="S2.SS4.p1.3.m3.3.3.2" xref="S2.SS4.p1.3.m3.3.3.3.cmml"><mo stretchy="false" id="S2.SS4.p1.3.m3.3.3.2.3" xref="S2.SS4.p1.3.m3.3.3.3.cmml">(</mo><msub id="S2.SS4.p1.3.m3.2.2.1.1" xref="S2.SS4.p1.3.m3.2.2.1.1.cmml"><mi id="S2.SS4.p1.3.m3.2.2.1.1.2" xref="S2.SS4.p1.3.m3.2.2.1.1.2.cmml">c</mi><mn id="S2.SS4.p1.3.m3.2.2.1.1.3" xref="S2.SS4.p1.3.m3.2.2.1.1.3.cmml">0</mn></msub><mo id="S2.SS4.p1.3.m3.3.3.2.4" xref="S2.SS4.p1.3.m3.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S2.SS4.p1.3.m3.1.1" xref="S2.SS4.p1.3.m3.1.1.cmml">⋯</mi><mo id="S2.SS4.p1.3.m3.3.3.2.5" xref="S2.SS4.p1.3.m3.3.3.3.cmml">,</mo><msub id="S2.SS4.p1.3.m3.3.3.2.2" xref="S2.SS4.p1.3.m3.3.3.2.2.cmml"><mi id="S2.SS4.p1.3.m3.3.3.2.2.2" xref="S2.SS4.p1.3.m3.3.3.2.2.2.cmml">c</mi><mi id="S2.SS4.p1.3.m3.3.3.2.2.3" xref="S2.SS4.p1.3.m3.3.3.2.2.3.cmml">n</mi></msub><mo stretchy="false" id="S2.SS4.p1.3.m3.3.3.2.6" xref="S2.SS4.p1.3.m3.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.3.m3.3b"><vector id="S2.SS4.p1.3.m3.3.3.3.cmml" xref="S2.SS4.p1.3.m3.3.3.2"><apply id="S2.SS4.p1.3.m3.2.2.1.1.cmml" xref="S2.SS4.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.3.m3.2.2.1.1.1.cmml" xref="S2.SS4.p1.3.m3.2.2.1.1">subscript</csymbol><ci id="S2.SS4.p1.3.m3.2.2.1.1.2.cmml" xref="S2.SS4.p1.3.m3.2.2.1.1.2">𝑐</ci><cn type="integer" id="S2.SS4.p1.3.m3.2.2.1.1.3.cmml" xref="S2.SS4.p1.3.m3.2.2.1.1.3">0</cn></apply><ci id="S2.SS4.p1.3.m3.1.1.cmml" xref="S2.SS4.p1.3.m3.1.1">⋯</ci><apply id="S2.SS4.p1.3.m3.3.3.2.2.cmml" xref="S2.SS4.p1.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS4.p1.3.m3.3.3.2.2.1.cmml" xref="S2.SS4.p1.3.m3.3.3.2.2">subscript</csymbol><ci id="S2.SS4.p1.3.m3.3.3.2.2.2.cmml" xref="S2.SS4.p1.3.m3.3.3.2.2.2">𝑐</ci><ci id="S2.SS4.p1.3.m3.3.3.2.2.3.cmml" xref="S2.SS4.p1.3.m3.3.3.2.2.3">𝑛</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.3.m3.3c">(c_{0},\cdots,c_{n})</annotation></semantics></math>, and <math id="S2.SS4.p1.4.m4.1" class="ltx_Math" alttext="P(S)" display="inline"><semantics id="S2.SS4.p1.4.m4.1a"><mrow id="S2.SS4.p1.4.m4.1.2" xref="S2.SS4.p1.4.m4.1.2.cmml"><mi id="S2.SS4.p1.4.m4.1.2.2" xref="S2.SS4.p1.4.m4.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.SS4.p1.4.m4.1.2.1" xref="S2.SS4.p1.4.m4.1.2.1.cmml">​</mo><mrow id="S2.SS4.p1.4.m4.1.2.3.2" xref="S2.SS4.p1.4.m4.1.2.cmml"><mo stretchy="false" id="S2.SS4.p1.4.m4.1.2.3.2.1" xref="S2.SS4.p1.4.m4.1.2.cmml">(</mo><mi id="S2.SS4.p1.4.m4.1.1" xref="S2.SS4.p1.4.m4.1.1.cmml">S</mi><mo stretchy="false" id="S2.SS4.p1.4.m4.1.2.3.2.2" xref="S2.SS4.p1.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.4.m4.1b"><apply id="S2.SS4.p1.4.m4.1.2.cmml" xref="S2.SS4.p1.4.m4.1.2"><times id="S2.SS4.p1.4.m4.1.2.1.cmml" xref="S2.SS4.p1.4.m4.1.2.1"></times><ci id="S2.SS4.p1.4.m4.1.2.2.cmml" xref="S2.SS4.p1.4.m4.1.2.2">𝑃</ci><ci id="S2.SS4.p1.4.m4.1.1.cmml" xref="S2.SS4.p1.4.m4.1.1">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.4.m4.1c">P(S)</annotation></semantics></math> denotes the conditional probability assigned to the sequence <math id="S2.SS4.p1.5.m5.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS4.p1.5.m5.1a"><mi id="S2.SS4.p1.5.m5.1.1" xref="S2.SS4.p1.5.m5.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.5.m5.1b"><ci id="S2.SS4.p1.5.m5.1.1.cmml" xref="S2.SS4.p1.5.m5.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.5.m5.1c">S</annotation></semantics></math>.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<table id="S2.E2" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S2.E2X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E2X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle PPL(S)" display="inline"><semantics id="S2.E2X.2.1.1.m1.1a"><mrow id="S2.E2X.2.1.1.m1.1.2" xref="S2.E2X.2.1.1.m1.1.2.cmml"><mi id="S2.E2X.2.1.1.m1.1.2.2" xref="S2.E2X.2.1.1.m1.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E2X.2.1.1.m1.1.2.1" xref="S2.E2X.2.1.1.m1.1.2.1.cmml">​</mo><mi id="S2.E2X.2.1.1.m1.1.2.3" xref="S2.E2X.2.1.1.m1.1.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E2X.2.1.1.m1.1.2.1a" xref="S2.E2X.2.1.1.m1.1.2.1.cmml">​</mo><mi id="S2.E2X.2.1.1.m1.1.2.4" xref="S2.E2X.2.1.1.m1.1.2.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E2X.2.1.1.m1.1.2.1b" xref="S2.E2X.2.1.1.m1.1.2.1.cmml">​</mo><mrow id="S2.E2X.2.1.1.m1.1.2.5.2" xref="S2.E2X.2.1.1.m1.1.2.cmml"><mo stretchy="false" id="S2.E2X.2.1.1.m1.1.2.5.2.1" xref="S2.E2X.2.1.1.m1.1.2.cmml">(</mo><mi id="S2.E2X.2.1.1.m1.1.1" xref="S2.E2X.2.1.1.m1.1.1.cmml">S</mi><mo stretchy="false" id="S2.E2X.2.1.1.m1.1.2.5.2.2" xref="S2.E2X.2.1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2X.2.1.1.m1.1b"><apply id="S2.E2X.2.1.1.m1.1.2.cmml" xref="S2.E2X.2.1.1.m1.1.2"><times id="S2.E2X.2.1.1.m1.1.2.1.cmml" xref="S2.E2X.2.1.1.m1.1.2.1"></times><ci id="S2.E2X.2.1.1.m1.1.2.2.cmml" xref="S2.E2X.2.1.1.m1.1.2.2">𝑃</ci><ci id="S2.E2X.2.1.1.m1.1.2.3.cmml" xref="S2.E2X.2.1.1.m1.1.2.3">𝑃</ci><ci id="S2.E2X.2.1.1.m1.1.2.4.cmml" xref="S2.E2X.2.1.1.m1.1.2.4">𝐿</ci><ci id="S2.E2X.2.1.1.m1.1.1.cmml" xref="S2.E2X.2.1.1.m1.1.1">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2X.2.1.1.m1.1c">\displaystyle PPL(S)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E2X.3.2.2.m1.1" class="ltx_Math" alttext="\displaystyle=P(S)^{-\frac{1}{n}}" display="inline"><semantics id="S2.E2X.3.2.2.m1.1a"><mrow id="S2.E2X.3.2.2.m1.1.2" xref="S2.E2X.3.2.2.m1.1.2.cmml"><mi id="S2.E2X.3.2.2.m1.1.2.2" xref="S2.E2X.3.2.2.m1.1.2.2.cmml"></mi><mo id="S2.E2X.3.2.2.m1.1.2.1" xref="S2.E2X.3.2.2.m1.1.2.1.cmml">=</mo><mrow id="S2.E2X.3.2.2.m1.1.2.3" xref="S2.E2X.3.2.2.m1.1.2.3.cmml"><mi id="S2.E2X.3.2.2.m1.1.2.3.2" xref="S2.E2X.3.2.2.m1.1.2.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E2X.3.2.2.m1.1.2.3.1" xref="S2.E2X.3.2.2.m1.1.2.3.1.cmml">​</mo><msup id="S2.E2X.3.2.2.m1.1.2.3.3" xref="S2.E2X.3.2.2.m1.1.2.3.3.cmml"><mrow id="S2.E2X.3.2.2.m1.1.2.3.3.2.2" xref="S2.E2X.3.2.2.m1.1.2.3.3.cmml"><mo stretchy="false" id="S2.E2X.3.2.2.m1.1.2.3.3.2.2.1" xref="S2.E2X.3.2.2.m1.1.2.3.3.cmml">(</mo><mi id="S2.E2X.3.2.2.m1.1.1" xref="S2.E2X.3.2.2.m1.1.1.cmml">S</mi><mo stretchy="false" id="S2.E2X.3.2.2.m1.1.2.3.3.2.2.2" xref="S2.E2X.3.2.2.m1.1.2.3.3.cmml">)</mo></mrow><mrow id="S2.E2X.3.2.2.m1.1.2.3.3.3" xref="S2.E2X.3.2.2.m1.1.2.3.3.3.cmml"><mo id="S2.E2X.3.2.2.m1.1.2.3.3.3a" xref="S2.E2X.3.2.2.m1.1.2.3.3.3.cmml">−</mo><mfrac id="S2.E2X.3.2.2.m1.1.2.3.3.3.2" xref="S2.E2X.3.2.2.m1.1.2.3.3.3.2.cmml"><mn id="S2.E2X.3.2.2.m1.1.2.3.3.3.2.2" xref="S2.E2X.3.2.2.m1.1.2.3.3.3.2.2.cmml">1</mn><mi id="S2.E2X.3.2.2.m1.1.2.3.3.3.2.3" xref="S2.E2X.3.2.2.m1.1.2.3.3.3.2.3.cmml">n</mi></mfrac></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2X.3.2.2.m1.1b"><apply id="S2.E2X.3.2.2.m1.1.2.cmml" xref="S2.E2X.3.2.2.m1.1.2"><eq id="S2.E2X.3.2.2.m1.1.2.1.cmml" xref="S2.E2X.3.2.2.m1.1.2.1"></eq><csymbol cd="latexml" id="S2.E2X.3.2.2.m1.1.2.2.cmml" xref="S2.E2X.3.2.2.m1.1.2.2">absent</csymbol><apply id="S2.E2X.3.2.2.m1.1.2.3.cmml" xref="S2.E2X.3.2.2.m1.1.2.3"><times id="S2.E2X.3.2.2.m1.1.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.1.2.3.1"></times><ci id="S2.E2X.3.2.2.m1.1.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.1.2.3.2">𝑃</ci><apply id="S2.E2X.3.2.2.m1.1.2.3.3.cmml" xref="S2.E2X.3.2.2.m1.1.2.3.3"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.1.2.3.3.1.cmml" xref="S2.E2X.3.2.2.m1.1.2.3.3">superscript</csymbol><ci id="S2.E2X.3.2.2.m1.1.1.cmml" xref="S2.E2X.3.2.2.m1.1.1">𝑆</ci><apply id="S2.E2X.3.2.2.m1.1.2.3.3.3.cmml" xref="S2.E2X.3.2.2.m1.1.2.3.3.3"><minus id="S2.E2X.3.2.2.m1.1.2.3.3.3.1.cmml" xref="S2.E2X.3.2.2.m1.1.2.3.3.3"></minus><apply id="S2.E2X.3.2.2.m1.1.2.3.3.3.2.cmml" xref="S2.E2X.3.2.2.m1.1.2.3.3.3.2"><divide id="S2.E2X.3.2.2.m1.1.2.3.3.3.2.1.cmml" xref="S2.E2X.3.2.2.m1.1.2.3.3.3.2"></divide><cn type="integer" id="S2.E2X.3.2.2.m1.1.2.3.3.3.2.2.cmml" xref="S2.E2X.3.2.2.m1.1.2.3.3.3.2.2">1</cn><ci id="S2.E2X.3.2.2.m1.1.2.3.3.3.2.3.cmml" xref="S2.E2X.3.2.2.m1.1.2.3.3.3.2.3">𝑛</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2X.3.2.2.m1.1c">\displaystyle=P(S)^{-\frac{1}{n}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="3" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
<tr id="S2.E2Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E2Xa.2.1.1.m1.2" class="ltx_Math" alttext="\displaystyle=P(c_{1}|c_{0})\cdots P(c_{n}|c_{0}c_{1}\cdots c_{n-1})" display="inline"><semantics id="S2.E2Xa.2.1.1.m1.2a"><mrow id="S2.E2Xa.2.1.1.m1.2.2" xref="S2.E2Xa.2.1.1.m1.2.2.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.4" xref="S2.E2Xa.2.1.1.m1.2.2.4.cmml"></mi><mo id="S2.E2Xa.2.1.1.m1.2.2.3" xref="S2.E2Xa.2.1.1.m1.2.2.3.cmml">=</mo><mrow id="S2.E2Xa.2.1.1.m1.2.2.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.2.4" xref="S2.E2Xa.2.1.1.m1.2.2.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E2Xa.2.1.1.m1.2.2.2.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml">​</mo><mrow id="S2.E2Xa.2.1.1.m1.1.1.1.1.1" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.2" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.cmml"><msub id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.2" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.2.cmml">c</mi><mn id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.3" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.3.cmml">1</mn></msub><mo fence="false" id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.1" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3.2" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3.2.cmml">c</mi><mn id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3.3" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo stretchy="false" id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.3" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E2Xa.2.1.1.m1.2.2.2.3a" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml">​</mo><mi mathvariant="normal" id="S2.E2Xa.2.1.1.m1.2.2.2.5" xref="S2.E2Xa.2.1.1.m1.2.2.2.5.cmml">⋯</mi><mo lspace="0em" rspace="0em" id="S2.E2Xa.2.1.1.m1.2.2.2.3b" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml">​</mo><mi id="S2.E2Xa.2.1.1.m1.2.2.2.6" xref="S2.E2Xa.2.1.1.m1.2.2.2.6.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E2Xa.2.1.1.m1.2.2.2.3c" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml">​</mo><mrow id="S2.E2Xa.2.1.1.m1.2.2.2.2.1" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.cmml"><msub id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2.2.cmml">c</mi><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2.3.cmml">n</mi></msub><mo fence="false" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1.cmml">|</mo><mrow id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.cmml"><msub id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2.2.cmml">c</mi><mn id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2.3.cmml">0</mn></msub><mo lspace="0em" rspace="0em" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.1" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.1.cmml">​</mo><msub id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3.2.cmml">c</mi><mn id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.1a" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.4" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.4.cmml">⋯</mi><mo lspace="0em" rspace="0em" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.1b" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.1.cmml">​</mo><msub id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.2.cmml">c</mi><mrow id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.2.cmml">n</mi><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.1" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.1.cmml">−</mo><mn id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo stretchy="false" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2Xa.2.1.1.m1.2b"><apply id="S2.E2Xa.2.1.1.m1.2.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2"><eq id="S2.E2Xa.2.1.1.m1.2.2.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.3"></eq><csymbol cd="latexml" id="S2.E2Xa.2.1.1.m1.2.2.4.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.4">absent</csymbol><apply id="S2.E2Xa.2.1.1.m1.2.2.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2"><times id="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.3"></times><ci id="S2.E2Xa.2.1.1.m1.2.2.2.4.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.4">𝑃</ci><apply id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.2">𝑐</ci><cn type="integer" id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.2.3">1</cn></apply><apply id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3.2">𝑐</ci><cn type="integer" id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.3.3">0</cn></apply></apply><ci id="S2.E2Xa.2.1.1.m1.2.2.2.5.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.5">⋯</ci><ci id="S2.E2Xa.2.1.1.m1.2.2.2.6.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.6">𝑃</ci><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1"><csymbol cd="latexml" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1">conditional</csymbol><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2.2">𝑐</ci><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2.3">𝑛</ci></apply><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3"><times id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.1"></times><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2">subscript</csymbol><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2.2">𝑐</ci><cn type="integer" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.2.3">0</cn></apply><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3">subscript</csymbol><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3.2">𝑐</ci><cn type="integer" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.3.3">1</cn></apply><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.4.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.4">⋯</ci><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5">subscript</csymbol><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.2">𝑐</ci><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3"><minus id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.1"></minus><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.2">𝑛</ci><cn type="integer" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.5.3.3">1</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2Xa.2.1.1.m1.2c">\displaystyle=P(c_{1}|c_{0})\cdots P(c_{n}|c_{0}c_{1}\cdots c_{n-1})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr id="S2.E2Xb" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E2Xb.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle=\prod_{k=1}^{n}p(c_{k}|c_{0}\cdots c_{k-1})" display="inline"><semantics id="S2.E2Xb.2.1.1.m1.1a"><mrow id="S2.E2Xb.2.1.1.m1.1.1" xref="S2.E2Xb.2.1.1.m1.1.1.cmml"><mi id="S2.E2Xb.2.1.1.m1.1.1.3" xref="S2.E2Xb.2.1.1.m1.1.1.3.cmml"></mi><mo id="S2.E2Xb.2.1.1.m1.1.1.2" xref="S2.E2Xb.2.1.1.m1.1.1.2.cmml">=</mo><mrow id="S2.E2Xb.2.1.1.m1.1.1.1" xref="S2.E2Xb.2.1.1.m1.1.1.1.cmml"><mstyle displaystyle="true" id="S2.E2Xb.2.1.1.m1.1.1.1.2" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.cmml"><munderover id="S2.E2Xb.2.1.1.m1.1.1.1.2a" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.cmml"><mo movablelimits="false" id="S2.E2Xb.2.1.1.m1.1.1.1.2.2.2" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.2.2.cmml">∏</mo><mrow id="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.cmml"><mi id="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.2" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.2.cmml">k</mi><mo id="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.1" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.3" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E2Xb.2.1.1.m1.1.1.1.2.3" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.3.cmml">n</mi></munderover></mstyle><mrow id="S2.E2Xb.2.1.1.m1.1.1.1.1" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.cmml"><mi id="S2.E2Xb.2.1.1.m1.1.1.1.1.3" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E2Xb.2.1.1.m1.1.1.1.1.2" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.2" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.cmml"><msub id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2.2" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2.2.cmml">c</mi><mi id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2.3" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2.3.cmml">k</mi></msub><mo fence="false" id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.1" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.1.cmml">|</mo><mrow id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.cmml"><msub id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.2" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.2.cmml">c</mi><mn id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.3" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.3.cmml">0</mn></msub><mo lspace="0em" rspace="0em" id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.3" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.3.cmml">⋯</mi><mo lspace="0em" rspace="0em" id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1a" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml">​</mo><msub id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.cmml"><mi id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.2" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.2.cmml">c</mi><mrow id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.cmml"><mi id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.2" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.2.cmml">k</mi><mo id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.1" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.1.cmml">−</mo><mn id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.3" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo stretchy="false" id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.3" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2Xb.2.1.1.m1.1b"><apply id="S2.E2Xb.2.1.1.m1.1.1.cmml" xref="S2.E2Xb.2.1.1.m1.1.1"><eq id="S2.E2Xb.2.1.1.m1.1.1.2.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.2"></eq><csymbol cd="latexml" id="S2.E2Xb.2.1.1.m1.1.1.3.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.3">absent</csymbol><apply id="S2.E2Xb.2.1.1.m1.1.1.1.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1"><apply id="S2.E2Xb.2.1.1.m1.1.1.1.2.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2Xb.2.1.1.m1.1.1.1.2.1.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.2">superscript</csymbol><apply id="S2.E2Xb.2.1.1.m1.1.1.1.2.2.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2Xb.2.1.1.m1.1.1.1.2.2.1.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.2">subscript</csymbol><csymbol cd="latexml" id="S2.E2Xb.2.1.1.m1.1.1.1.2.2.2.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.2.2">product</csymbol><apply id="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3"><eq id="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.1.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.1"></eq><ci id="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.2.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.2">𝑘</ci><cn type="integer" id="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.3.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E2Xb.2.1.1.m1.1.1.1.2.3.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.2.3">𝑛</ci></apply><apply id="S2.E2Xb.2.1.1.m1.1.1.1.1.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1"><times id="S2.E2Xb.2.1.1.m1.1.1.1.1.2.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.2"></times><ci id="S2.E2Xb.2.1.1.m1.1.1.1.1.3.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.3">𝑝</ci><apply id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2.2">𝑐</ci><ci id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.2.3">𝑘</ci></apply><apply id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3"><times id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1"></times><apply id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.2">𝑐</ci><cn type="integer" id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.3">0</cn></apply><ci id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.3">⋯</ci><apply id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.1.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4">subscript</csymbol><ci id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.2.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.2">𝑐</ci><apply id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3"><minus id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.1.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.1"></minus><ci id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.2.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.2">𝑘</ci><cn type="integer" id="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.3.cmml" xref="S2.E2Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.3.3">1</cn></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2Xb.2.1.1.m1.1c">\displaystyle=\prod_{k=1}^{n}p(c_{k}|c_{0}\cdots c_{k-1})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">Informally, perplexity can be loosely interpreted as a measure of how “easily” an LM can predict individual <span id="S2.SS4.p3.1.1" class="ltx_text ltx_font_italic">characters</span> in a piece of text that was not used to train the LM. Higher PPL means the LM has more choices to consider in trying to predict the next character and indicates worse fit between the LM and the text being evaluated.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">To perform within-subset cross-validation, we split each subset into 90% training and 10% validation splits at the utterance level. We trained 6-layer character-level Transformer LMs using a context length of 512 characters with a batch size of 64 samples. We allowed for a maximum of 50,000 training iterations, evaluating on the validation set every 500 iterations and computing evaluation metrics after 200 iterations.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p id="S2.SS4.p5.1" class="ltx_p">The character-level LMs consisted of 6 layers with 6 attention heads per layer with an embedding size of 384. We employed a dropout rate of 0.2 during training for regularization. The models were optimized using an initial learning rate of 0.0003 with an early stopping criterion of 2 (i.e., the training was halted if no improvement was observed on the validation loss for 2 consecutive evaluations). This training configuration was applied to all CORAAL subsets to ensure fair comparison during cross-validation.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Measuring Semantic Similarity Between ASR Output and Manual Reference</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">To address the limitation of WER as a measure of how well an ASR model captures the meaning of the spoken utterance (vs. the form of the utterance), we computed the cosine between reference and hypothesis utterance embedding vectors extracted from pre-trained encoder-based (i.e., trained to learn the representation <span id="S2.SS5.p1.1.1" class="ltx_text ltx_font_italic">bidirectionally</span>) LMs. We focused on utterances pairs having WER between 0% and 100%. While WER can exceed 100% in cases where the ASR model generates more words than were spoken and transcribed manually in the reference transcript (e.g., due to “hallucinations”). For the purposes of the present analysis of the utility of cosine similarity, we discarded utterances with WER greater than 100%, as these utterances typically contain a large number of insertion errors (i.e., extra tokens “inserted” by the ASR model into the hypothesis half of the alignment graph between the reference and the hypothesis transcripts that do not correspond to any tokens in the reference half of the graph). Therefore, these inserted tokens do not represent the kind of errors for which the cosine similarity approach is meant to compensate (i.e., where the ASR model produces a semantically equivalent but orthographically different token from the corresponding token in the reference transcript resulting in a higher WER).</p>
</div>
<div id="S2.SS5.p2" class="ltx_para">
<p id="S2.SS5.p2.1" class="ltx_p">Cosine similarity is a widely used metric to measure semantic similarity between texts by calculating the inner product between their vector representations <cite class="ltx_cite ltx_citemacro_citep">(Rahutomo et al., <a href="#bib.bib19" title="" class="ltx_ref">2012</a>)</cite>. It is based on the basic intuition that vector representations utterances in multidimensional space tend to point in a similar direction for utterances that are similar in meaning. We leveraged Sentence-BERT <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite> to compute the semantic representations and subsequently cosine similarity between the reference and hypothesis transcripts obtained from the CORAAL validation set. Sentence-BERT uses a siamese and triplet network structure to extract <span id="S2.SS5.p2.1.1" class="ltx_text ltx_font_italic">sentence</span>-level embeddings from pre-trained transformer-based LMs for computing cosines. Specifically, we used a 6-layer MiniLM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib28" title="" class="ltx_ref">2020</a>)</cite> to extract the sentence-level embeddings for both references and hypotheses. MiniLM is a distilled LM with BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> as the base encoder. MiniLM is further fine-tuned on over 1 billion sentence pairs to generate better sentence representations tailored for semantic similarity tasks. We then computed the cosine between each reference-hypothesis utterance pair as an approximation of semantic similarity between them.</p>
</div>
</section>
<section id="S2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Comparing Pairwise Utterance-level WER and Cosine Similarity</h3>

<div id="S2.SS6.p1" class="ltx_para">
<p id="S2.SS6.p1.1" class="ltx_p">Informally, WER measures the ratio of the incorrectly recognized words in the hypothesis (i.e, ASR-generated transcripts) compared to the reference (i.e., manually created verbatim transcripts). However, WER provides no details on the nature of the errors. For example, the WER for the following pair of reference and hypothesis texts - “on the on the rise” and “on the rise” - is 0.67. This WER is quite high indicating poor ASR performance due to the disfluency “on the” repeated in the reference transcript. However, from a semantic perspective, the ASR transcript has correctly captured the intended meaning by ignoring the repetition.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>The Impact of Audio Quality on ASR Performance</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">When evaluating CORAAL’s recordings in their long-form format (i.e., including all possible utterances from both interviewer and interviewee), the pre-trained version of Whisper achieved WERs of 21.05%, 22.12%, 19.78%, 26.55%, 33.16%, 13.67%, and 19.09% on ATL, DCA, DCB, PRV, ROC, LES, and VLD subsets, respectively (the average WER across all of these subsets is 22.2% (SD: 6.17%). Notably, the pre-trained Whisper model performed worst on DCA (WER of 22.12%) and PRV (WER of 33.16%), which were recorded in analogue format on magnetic tape, and LES (WER of 26.55%), which was recorded digitally but without a dedicated microphone for interviewees. OpenAI reported a similar mean WER of 19.6% on all subsets of the same version of CORAAL as the one used in our study. <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To investigate the differences in WERs by geographical location further we compared the pre-trained Whisper performance to that of Whisper fine-tuned on the various geographical subsets of CORAAL. Fine-tuning requires splitting each CORAAL subset into a training and a held-out test sets. On average across train-test splits of all CORAAL subsets, the pre-trained and fine-tuned Whisper achieved WERs of 34.35% and 27.15%, respectively, where both pre-trained and fine-tuned Whisper models were evaluated on the test portion of the train-test splits. As illustrated in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 The Impact of Audio Quality on ASR Performance ‣ 3 Results ‣ Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we observed slightly better performance with fine-tuned Whisper models than with their exclusively pre-trained counterparts. We also found that both pre-trained and fine-tuned Whisper models performed considerably worse on the DCA, LES, and PRV subsets.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Another important observation is that the 2021 CORAAL release contains two subsets from the state of Georgia (ATL and VLD), which geographically and historically is very close to North Carolina (PRV), yet the pre-trained whisper WERs on the former are between 25% and 27% and the WER on the latter is double that at 56%. To investigate this observation further, we extracted and analyzed 316 utterances from Princeville, NC residents from a publicly available documentary film, <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">This Side of The River - The Story of Princeville</span>, which was recorded with professional equipment yielding high audio quality. In contrast to the WER of 56% on the PVR subset of CORAAL, the pre-trained Whisper reached WER as low as 29.31% on these 316 utterances; however, contrary to our expectations, the fine-tuned Whisper reached a WER of 82.07%, which is far worse than the reported metrics in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 The Impact of Audio Quality on ASR Performance ‣ 3 Results ‣ Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2407.13982/assets/whisper_large_wer_plot.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="445" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The WER performance of pre-trained and fine-tuned Whisper on CORAAL utterance-level test sets separated by geographical location.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Relationship Between Linguistic Variation and ASR Performance</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To elucidate the relationship between dialectal differences in language use and the differences in WER by geographical location, we trained several character-level autoregressive LMs on each subset of CORAAL and validated on the remaining subsets using LM perplexity (PPL) as a metric. Since PPL is a model-intrinsic evaluation metric indicative of how well a LM predicts sequences in a new piece of text (in our case, a single character), we used this metric to determine the degree of similarity (at the level of predictability of individual characters) between various subsets of CORAAL by training in a round-robin fashion an LM on one of the subsets and calculating the mean PPL of that LM on the remaining subsets. As shown in Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Relationship Between Linguistic Variation and ASR Performance ‣ 3 Results ‣ Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we observed that with the exception of PRV, the rest of the CORAAL subsets are very similar to each other in terms of the PPLs. As expected, CORAAL subsets originating in the same state (i.e., LES and ROC from NY, DCA and DCB from DC, ATL and VLD from GA) exhibit PPLs more similar to each other than to other subsets.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of mean LM perplexities (PPLs) estimated by character-level autoregressive LMs trained on various CORAAL subsets. Note: Higher PPL means lower similarity in this context.</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" rowspan="2"><span id="S3.T2.1.1.1.1.1" class="ltx_text">Validated on</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="7">Trained on</th>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<th id="S3.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ATL</th>
<th id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">DCA</th>
<th id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">DCB</th>
<th id="S3.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">LES</th>
<th id="S3.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">PRV</th>
<th id="S3.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">ROC</th>
<th id="S3.T2.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">VLD</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.3.1" class="ltx_tr">
<td id="S3.T2.1.3.1.1" class="ltx_td ltx_align_center ltx_border_t">ATL</td>
<td id="S3.T2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S3.T2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">8.72</td>
<td id="S3.T2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">8.00</td>
<td id="S3.T2.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">9.07</td>
<td id="S3.T2.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">8.71</td>
<td id="S3.T2.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">8.38</td>
<td id="S3.T2.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">8.28</td>
</tr>
<tr id="S3.T2.1.4.2" class="ltx_tr">
<td id="S3.T2.1.4.2.1" class="ltx_td ltx_align_center">DCA</td>
<td id="S3.T2.1.4.2.2" class="ltx_td ltx_align_center">8.42</td>
<td id="S3.T2.1.4.2.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T2.1.4.2.4" class="ltx_td ltx_align_center">8.06</td>
<td id="S3.T2.1.4.2.5" class="ltx_td ltx_align_center">8.46</td>
<td id="S3.T2.1.4.2.6" class="ltx_td ltx_align_center">7.39</td>
<td id="S3.T2.1.4.2.7" class="ltx_td ltx_align_center">8.52</td>
<td id="S3.T2.1.4.2.8" class="ltx_td ltx_align_center">7.65</td>
</tr>
<tr id="S3.T2.1.5.3" class="ltx_tr">
<td id="S3.T2.1.5.3.1" class="ltx_td ltx_align_center">DCB</td>
<td id="S3.T2.1.5.3.2" class="ltx_td ltx_align_center">8.44</td>
<td id="S3.T2.1.5.3.3" class="ltx_td ltx_align_center">8.37</td>
<td id="S3.T2.1.5.3.4" class="ltx_td ltx_align_center">–</td>
<td id="S3.T2.1.5.3.5" class="ltx_td ltx_align_center">8.19</td>
<td id="S3.T2.1.5.3.6" class="ltx_td ltx_align_center">7.89</td>
<td id="S3.T2.1.5.3.7" class="ltx_td ltx_align_center">8.44</td>
<td id="S3.T2.1.5.3.8" class="ltx_td ltx_align_center">7.84</td>
</tr>
<tr id="S3.T2.1.6.4" class="ltx_tr">
<td id="S3.T2.1.6.4.1" class="ltx_td ltx_align_center">LES</td>
<td id="S3.T2.1.6.4.2" class="ltx_td ltx_align_center">7.83</td>
<td id="S3.T2.1.6.4.3" class="ltx_td ltx_align_center">6.72</td>
<td id="S3.T2.1.6.4.4" class="ltx_td ltx_align_center">6.85</td>
<td id="S3.T2.1.6.4.5" class="ltx_td ltx_align_center">–</td>
<td id="S3.T2.1.6.4.6" class="ltx_td ltx_align_center">6.75</td>
<td id="S3.T2.1.6.4.7" class="ltx_td ltx_align_center">6.87</td>
<td id="S3.T2.1.6.4.8" class="ltx_td ltx_align_center">6.90</td>
</tr>
<tr id="S3.T2.1.7.5" class="ltx_tr">
<td id="S3.T2.1.7.5.1" class="ltx_td ltx_align_center">PRV</td>
<td id="S3.T2.1.7.5.2" class="ltx_td ltx_align_center">10.11</td>
<td id="S3.T2.1.7.5.3" class="ltx_td ltx_align_center">10.59</td>
<td id="S3.T2.1.7.5.4" class="ltx_td ltx_align_center">13.78</td>
<td id="S3.T2.1.7.5.5" class="ltx_td ltx_align_center">9.65</td>
<td id="S3.T2.1.7.5.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T2.1.7.5.7" class="ltx_td ltx_align_center">11.22</td>
<td id="S3.T2.1.7.5.8" class="ltx_td ltx_align_center">9.01</td>
</tr>
<tr id="S3.T2.1.8.6" class="ltx_tr">
<td id="S3.T2.1.8.6.1" class="ltx_td ltx_align_center">ROC</td>
<td id="S3.T2.1.8.6.2" class="ltx_td ltx_align_center">7.86</td>
<td id="S3.T2.1.8.6.3" class="ltx_td ltx_align_center">7.34</td>
<td id="S3.T2.1.8.6.4" class="ltx_td ltx_align_center">7.91</td>
<td id="S3.T2.1.8.6.5" class="ltx_td ltx_align_center">7.42</td>
<td id="S3.T2.1.8.6.6" class="ltx_td ltx_align_center">7.26</td>
<td id="S3.T2.1.8.6.7" class="ltx_td ltx_align_center">–</td>
<td id="S3.T2.1.8.6.8" class="ltx_td ltx_align_center">7.02</td>
</tr>
<tr id="S3.T2.1.9.7" class="ltx_tr">
<td id="S3.T2.1.9.7.1" class="ltx_td ltx_align_center ltx_border_bb">VLD</td>
<td id="S3.T2.1.9.7.2" class="ltx_td ltx_align_center ltx_border_bb">8.90</td>
<td id="S3.T2.1.9.7.3" class="ltx_td ltx_align_center ltx_border_bb">8.91</td>
<td id="S3.T2.1.9.7.4" class="ltx_td ltx_align_center ltx_border_bb">10.72</td>
<td id="S3.T2.1.9.7.5" class="ltx_td ltx_align_center ltx_border_bb">8.83</td>
<td id="S3.T2.1.9.7.6" class="ltx_td ltx_align_center ltx_border_bb">8.14</td>
<td id="S3.T2.1.9.7.7" class="ltx_td ltx_align_center ltx_border_bb">9.35</td>
<td id="S3.T2.1.9.7.8" class="ltx_td ltx_align_center ltx_border_bb">–</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Based on the results presented in Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Relationship Between Linguistic Variation and ASR Performance ‣ 3 Results ‣ Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we can observe that character-level LMs trained on CORAAL subsets collected in urban areas (e.g., LES and ROC from NY, and DCA and DCB from DC) generally produced lower PPLs relative to subsets obtained from rural areas. PRV and VLD subsets yielded higher PPL measures relative to other subsets which indicates that these subsets are more different in terms of their language patterns as compared to other subsets. While the ATL subset is also in the same state of Georgia as VLD, we did not find a similarly high PPL on this subset to VLD potentially due to the fact that the ATL subset was collected in an urban area, whereas the VLD subset was collected in a rural area.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">As shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Relationship Between Linguistic Variation and ASR Performance ‣ 3 Results ‣ Reexamining Racial Disparities in Automatic Speech Recognition Performance: The Role of Confounding by Provenance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we can also observe that most LMs produced substantially higher PPLs when used to predict utterances from the PRV subset. Conversely, the LM trained on the PRV subset produced significantly lower PPLs on most of the other subsets, highlighting the extent to which the unique nature of the language patterns in this subset differ from the training data that all of these models have in common.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2407.13982/assets/char_gpt.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="487" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Means and 95% t-distribution confidence intervals of PPLs estimated from character-level LMs trained on CORAAL subsets.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Comparison Between Utterance-level WER and Cosine Similarity</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Informal manual examination of the errors produced by Whisper on CORAAL highlighted several possible reasons for ASR errors: a) the presence of disfluencies (um’s and ah’s, repetitions, word fragments and repairs); b) misspelled and/or low-frequency words in the reference-hypothesis pairs; and c) grammatical errors. These features are common in spontaneous connected speech <cite class="ltx_cite ltx_citemacro_citep">(Shriberg, <a href="#bib.bib21" title="" class="ltx_ref">2001</a>)</cite> and do not significantly affect how well one can understand the content of speech <cite class="ltx_cite ltx_citemacro_citep">(Fraundorf and Watson, <a href="#bib.bib5" title="" class="ltx_ref">2011</a>)</cite>; however, they can heavily impact WER calculations. Due to this observation (albeit informal), we wanted to investigate if a commonly used computational linguistic measure of semantic similarity and relatedness <cite class="ltx_cite ltx_citemacro_citep">(Pedersen et al., <a href="#bib.bib17" title="" class="ltx_ref">2007</a>)</cite> could yield useful insights when evaluating ASR performance. We measured semantic similarity by comparing vector representations of the automatically transcribed utterances (<span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">hypotheses</span>) and their manually transcribed (<span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">reference</span>) counterparts as detailed in the Methods section.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">We observed a strong negative correlation (Spearman’s <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\rho</annotation></semantics></math> = -0.88) between WERs and cosine similarity scores across all reference-hypothesis transcript pairs in the CORAAL validation set, indicating that utterances with lower WER tended to exhibit higher semantic similarity (that is, more accurate transcriptions exhibit higher reference-hypothesis semantic similarity). Building upon the previous findings suggesting that WER up to 30% may be acceptable for certain use cases such as archiving <cite class="ltx_cite ltx_citemacro_citep">(Gaur et al., <a href="#bib.bib6" title="" class="ltx_ref">2016</a>; Munteanu et al., <a href="#bib.bib16" title="" class="ltx_ref">2006</a>)</cite>, we further expanded our error analysis to utterance pairs with WER exceeding this 30% threshold. We found that the median cosine similarity score reached 0.69, whereas the mean cosine similarity score reached 0.68 for those utterances exceeding the 30% threshold. We also found that the majority (77%) of utterance pairs present a moderate level of semantic similarity with cosine similarity scores exceeding 0.5. Interestingly, we found that out of the 766 utterances with high WER values greater than 80%, 247 of them still attained at least a moderate level of semantic similarity, with scores exceeding 0.5. Furthermore, 53 out of 766 utterance pairs have cosine similarity score greater than 0.8, indicating a high degree of semantic similarity between the references and hypotheses despite relatively poor WER values.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Our findings highlight two key factors affecting the performance of state-of-the-art ASR models on AAE speech data from CORAAL. One of these factors, dialectal variation, has been previously identified as a weakness in modern ASR systems <cite class="ltx_cite ltx_citemacro_citep">(Koenecke et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Martin and Tang, <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> potentially contributing to the problem of racial disparity. Our findings confirm that even though the state-of-the-art ASR technology based on the neural transformer architecture is substantially more accurate on the task of transcribing speech of African-Americans in the CORAAL dataset than its predecessors, it still falls short of the performance on other datasets even those containing accented English speech <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>. The other factor, audio recording quality, has not been previously addressed. While Whisper performs relatively well on the CORAAL data overall, we observed some degree of confounding by provenance. Although the precise definition of confounding has been vigorously debated <cite class="ltx_cite ltx_citemacro_citep">(VanderWeele and Shpitser, <a href="#bib.bib25" title="" class="ltx_ref">2013</a>)</cite>, confounders have been defined in the epidemiology literature as variables that a) relate to the outcome independently (i.e., audio recording quality affects WER); b) are associated with the exposure of interest (i.e., audio recording quality differs in sites in which different dialects predominate); and c) are extraneous to the relationship of interest (i.e., between the dialect and WER). In the context of our study, audio recording quality - which differs with the provenance of the recording - fits this definition of a confounder and should be taken into account when investigating and reporting ASR performance results, especially in heterogeneously collected datasets such as CORAAL.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Confounding by provenance notwithstanding, the existing gap in ASR performance on AAE speech will likely make it harder for African Americans to benefit from state-of-the-art ASR technology applications. This problem can present a particularly significant barrier for historically marginalized populations in healthcare, as voice-based virtual assistants are being rapidly adopted in this domain <cite class="ltx_cite ltx_citemacro_citep">(Mengesha et al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite> for various applications including support for patients seeking health-related information <cite class="ltx_cite ltx_citemacro_citep">(Harrington et al., <a href="#bib.bib7" title="" class="ltx_ref">2022</a>)</cite>. While our current study demonstrates that ASR performance can be marginally improved by fine-tuning with dialect-specific corpora, our results also indicate that fine-tuned ASR models may not generalize well outside of the data used for fine-tuning. Therefore, adaptations of ASR to African American speech may need to include targeted adjustments in acoustic and language models in addition to using dialect-specific datasets.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Our results also indicate that the standard ASR evaluation metric of WER derived from the Levenshtein distance <cite class="ltx_cite ltx_citemacro_citep">(Levenshtein et al., <a href="#bib.bib11" title="" class="ltx_ref">1966</a>)</cite> may systematically underestimate ASR accuracy with respect to the ASR system’s ability to capture the content of the speech vs. its ability to transcribe verbatim. For some downstream applications such as voice assistants, for example, the former is more important than the latter. Extensive research on human speech production has demonstrated that a conceptual message conveyed via a spoken utterance undergoes a complex process of grammatical, phonological and articulatory planning and encoding before it is produced by the speaker <cite class="ltx_cite ltx_citemacro_citep">(Levelt, <a href="#bib.bib10" title="" class="ltx_ref">1989</a>)</cite>. The speech emerging as a result of this process of encoding a conceptual message is hardly perfectly fluent and error-free in truly spontaneous connected discourse as evidenced by various corpora of speech errors and disfleuncies used to study these phenomena <cite class="ltx_cite ltx_citemacro_citep">(Shriberg, <a href="#bib.bib21" title="" class="ltx_ref">2001</a>; Stemberger, <a href="#bib.bib24" title="" class="ltx_ref">1982</a>)</cite>. ASR systems that employ a trained language model either as a separate module (as with older HMM-based ASR systems) or part of the end-to-end neural network (as with more recent ASR systems such as Whisper) are able to compensate for some of the speech errors and disfluencies by overriding the acoustic evidence in the speech signal with the predictions of the language model; however, such “corrections” may actually result in higher WER if the reference transcript reflects the speech verbatim and does not correct the speech errors and disfluencies in the same manner. Furthermore, a language model may offer semantically close or equivalent but orthographically different words that are more likely to occur in the context of a given utterance than those chosen by the speaker. While detrimental to ASR accuracy measured with WER, disfluencies, speech errors, and spelling errors and variants found in spontaneous speech transcription appear to have less impact on the overall semantic similarity captured by the cosine similarity measure. Similarly, misspelled words or spelling variants, which increase WER, may not necessarily corrupt the overall meaning of the transcribed text and could be compensated by downstream processes robust to noise in the input such as question-answering systems driven by large language models <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib26" title="" class="ltx_ref">2024</a>)</cite>. As such, over-reliance on WER alone may present an overly pessimistic perspective on ASR capabilities, especially when used to convert spontaneous speech to text in chat-based applications. Our findings highlight the need for developing a multi-faceted and more robust evaluation metric for ASR, which incorporates both WER and semantic similarity.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We present two key factors underlying the poor performance of the state-of-the-art ASR models on AAE speech in CORAAL: dialectal variation in neighboring communities and audio recording quality. Our study provides preliminary and experimental evidence suggesting that audio recording quality needs to be taken into account as a potential confounding factor in studying disparities in ASR performance on heterogeneous datasets such as CORAAL.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The results presented in this paper should be interpreted in light of several limitations. First, while CORAAL is a unique resource, it is relatively small and does not capture all of the diversity in AAE variants; however, various investigators continue to contribute data to CORAAL on an ongoing basis. Future work should continue to examine ASR performance also on an ongoing basis, as this and other collections of AAE speech. Second, while the WER differences between the PRV subset in CORAAL and <span id="Sx1.p1.1.1" class="ltx_text ltx_font_italic">This Side of The River</span> suggest a strong influence of recording quality, further experiments should investigate the nature and extent of this relationship and develop methods for adjusting results for this potential confounder. Future studies should also investigate the potential influence of speaking style and other contextual factors <cite class="ltx_cite ltx_citemacro_citep">(Kendall and Wolfram, <a href="#bib.bib8" title="" class="ltx_ref">2009</a>)</cite>, in addition to recording quality, to provide a more holistic understanding of the factors that contribute to ASR performance disparities. Moreover, the use of PPL as a metric to quantify linguistic variation is limited in its ability to directly capture the nuanced features of language. PPL serves as a proxy measure, but does not necessarily reflect the full complexity of the language as a construct.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">This research was supported by grants from USA National Library of Medicine R01LM014056-02S1, USA National Institute on Aging R21AG069792-01.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blodgett and O’Connor (2017)</span>
<span class="ltx_bibblock">
Su Lin Blodgett and Brendan O’Connor. 2017.

</span>
<span class="ltx_bibblock">Racial disparity in natural language processing: A case study of
social media african-american english.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1707.00061</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Farrington and Kendall (2021)</span>
<span class="ltx_bibblock">
Charlie Farrington and Tyler Kendall. 2021.

</span>
<span class="ltx_bibblock">The corpus of regional african american language.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Field et al. (2021)</span>
<span class="ltx_bibblock">
Anjalie Field, Su Lin Blodgett, Zeerak Waseem, and Yulia Tsvetkov. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.149" title="" class="ltx_ref ltx_href">A survey of
race, racism, and anti-racism in NLP</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 1905–1925,
Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fraundorf and Watson (2011)</span>
<span class="ltx_bibblock">
Scott H. Fraundorf and Duane G. Watson. 2011.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1016/j.jml.2011.03.004" title="" class="ltx_ref ltx_href">The disfluent
discourse: Effects of filled pauses on recall</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Journal of Memory and Language</em>, 65(2):161–175.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gaur et al. (2016)</span>
<span class="ltx_bibblock">
Yashesh Gaur, Walter S Lasecki, Florian Metze, and Jeffrey P Bigham. 2016.

</span>
<span class="ltx_bibblock">The effects of automatic speech recognition quality on human
transcription latency.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th International Web for All
Conference</em>, pages 1–8.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harrington et al. (2022)</span>
<span class="ltx_bibblock">
Christina N Harrington, Radhika Garg, Amanda Woodward, and Dimitri Williams.
2022.

</span>
<span class="ltx_bibblock">“it’s kind of like code-switching”: Black older adults’
experiences with a voice assistant for health information seeking.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 CHI Conference on Human Factors in
Computing Systems</em>, pages 1–15.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kendall and Wolfram (2009)</span>
<span class="ltx_bibblock">
Tyler Kendall and Walt Wolfram. 2009.

</span>
<span class="ltx_bibblock">Local and external language standards in african american english.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Journal of English Linguistics</em>, 37(4):305–330.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koenecke et al. (2020)</span>
<span class="ltx_bibblock">
Allison Koenecke, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion
Mengesha, Connor Toups, John R. Rickford, Dan Jurafsky, and Sharad Goel.
2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1073/pnas.1915768117" title="" class="ltx_ref ltx_href">Racial disparities
in automated speech recognition</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em>,
117(14):7684–7689.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levelt (1989)</span>
<span class="ltx_bibblock">
Willem J. M. Levelt. 1989.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.7551/mitpress/6393.001.0001" title="" class="ltx_ref ltx_href"><em id="bib.bib10.1.1.1" class="ltx_emph ltx_font_italic">Speaking: From Intention to Articulation</em></a>.

</span>
<span class="ltx_bibblock">The MIT Press.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levenshtein et al. (1966)</span>
<span class="ltx_bibblock">
Vladimir I Levenshtein et al. 1966.

</span>
<span class="ltx_bibblock">Binary codes capable of correcting deletions, insertions, and
reversals.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Soviet physics doklady</em>, volume 10, pages 707–710. Soviet
Union.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Changye Li, Weizhe Xu, Trevor Cohen, Martin Michalowski, and Serguei Pakhomov.
2023.

</span>
<span class="ltx_bibblock">Trestle: Toolkit for reproducible execution of speech, text and
language experiments.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">AMIA Joint Summits on Translational Science proceedings. AMIA
Joint Summits on Translational Science</em>, 2023:360–369.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Changye Li, Weizhe Xu, Trevor Cohen, and Serguei Pakhomov. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1016/j.jbi.2024.104598" title="" class="ltx_ref ltx_href">Useful blunders:
Can automated speech recognition errors improve downstream dementia
classification?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Journal of Biomedical Informatics</em>, page 104598.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martin and Tang (2020)</span>
<span class="ltx_bibblock">
Joshua L Martin and Kevin Tang. 2020.

</span>
<span class="ltx_bibblock">Understanding racial disparities in automatic speech recognition: The
case of habitual" be".

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>, pages 626–630.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mengesha et al. (2021)</span>
<span class="ltx_bibblock">
Zion Mengesha, Courtney Heldreth, Michal Lahav, Juliana Sublewski, and Elyse
Tuennerman. 2021.

</span>
<span class="ltx_bibblock">“i don’t think these devices are very culturally sensitive.”
impact of automated speech recognition errors on african americans.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Frontiers in Artificial Intelligence</em>, 4:169.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Munteanu et al. (2006)</span>
<span class="ltx_bibblock">
Cosmin Munteanu, Gerald Penn, Ron Baecker, Elaine Toms, and David James. 2006.

</span>
<span class="ltx_bibblock">Measuring the acceptable word error rate of machine-generated webcast
transcripts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Ninth International Conference on Spoken Language
Processing</em>. Citeseer.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pedersen et al. (2007)</span>
<span class="ltx_bibblock">
Ted Pedersen, Serguei V.S. Pakhomov, Siddharth Patwardhan, and Christopher G.
Chute. 2007.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1016/j.jbi.2006.06.004" title="" class="ltx_ref ltx_href">Measures of
semantic similarity and relatedness in the biomedical domain</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Journal of Biomedical Informatics</em>, 40(3):288–299.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2023)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and
Ilya Sutskever. 2023.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine
Learning</em>, volume 202 of <em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>,
pages 28492–28518. PMLR.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rahutomo et al. (2012)</span>
<span class="ltx_bibblock">
Faisal Rahutomo, Teruaki Kitasuka, Masayoshi Aritsugi, et al. 2012.

</span>
<span class="ltx_bibblock">Semantic cosine similarity.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">The 7th international student conference on advanced science
and technology ICAST</em>, volume 4, page 1. University of Seoul South Korea.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/1908.10084" title="" class="ltx_ref ltx_href">Sentence-bert: Sentence
embeddings using siamese bert-networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing</em>. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shriberg (2001)</span>
<span class="ltx_bibblock">
Elizabeth Shriberg. 2001.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1017/S0025100301001128" title="" class="ltx_ref ltx_href">To ‘errrr’ is
human: ecology and acoustics of speech disfluencies</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Journal of the International Phonetic Association</em>,
31(1):153–169.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silva et al. (2021)</span>
<span class="ltx_bibblock">
Andrew Silva, Pradyumna Tambwekar, and Matthew Gombolay. 2021.

</span>
<span class="ltx_bibblock">Towards a comprehensive understanding and accurate evaluation of
societal biases in pre-trained transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 2383–2389.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Solinsky et al. (2023)</span>
<span class="ltx_bibblock">
Jacob C Solinsky, Raymond L Finzel, Martin Michalowski, and Serguei Pakhomov.
2023.

</span>
<span class="ltx_bibblock">Automated neural nursing assistant (anna): An over-the-phone system
for cognitive monitoring.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference of the International
Speech Communication Association, INTERSPEECH</em>, volume 2023, pages 684–685.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stemberger (1982)</span>
<span class="ltx_bibblock">
Joseph Paul Stemberger. 1982.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1016/0024-3841(82)90012-2" title="" class="ltx_ref ltx_href">The nature of
segments in the lexicon: Evidence from speech errors</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Lingua</em>, 56(3):235–259.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VanderWeele and Shpitser (2013)</span>
<span class="ltx_bibblock">
Tyler J VanderWeele and Ilya Shpitser. 2013.

</span>
<span class="ltx_bibblock">On the definition of a confounder.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Annals of statistics</em>, 41(1):196.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Bin Wang, Chengwei Wei, Zhengyuan Liu, Geyu Lin, and Nancy F. Chen. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2404.09754" title="" class="ltx_ref ltx_href">Resilience of large
language models for noisy instructions</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2404.09754.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Weichen Wang, Weizhe Xu, Ayesha Chander, Subigya Nepal, Benjamin Buck, Serguei
Pakhomov, Trevor Cohen, Dror Ben-Zeev, and Andrew Campbell. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3610890" title="" class="ltx_ref ltx_href">The power of speech in the
wild: Discriminative power of daily voice diaries in understanding auditory
verbal hallucinations using deep learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em>, 7(3).

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="" class="ltx_ref ltx_href">Minilm: Deep self-attention distillation for task-agnostic compression of
pre-trained transformers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 33, pages 5776–5788. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2022)</span>
<span class="ltx_bibblock">
Weizhe Xu, Weichen Wang, Jake Portanova, Ayesha Chander, Andrew Campbell,
Serguei Pakhomov, Dror Ben-Zeev, and Trevor Cohen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1016/j.jbi.2022.103998" title="" class="ltx_ref ltx_href">Fully automated
detection of formal thought disorder with time-series augmented
representations for detection of incoherent speech (tardis)</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Journal of Biomedical Informatics</em>, 126:103998.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.13981" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.13982" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.13982">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.13982" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.13983" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 18:39:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
