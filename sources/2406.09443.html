<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.09443] Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness</title><meta property="og:description" content="Voice activity detection (VAD) is a critical component in various applications such as speech recognition, speech enhancement, and hands-free communication systems. With the increasing demand for personalized and conte…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.09443">

<!--Generated on Fri Jul  5 21:55:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1,*,†]SatyamKumar
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=2,*]Sai SrujanaBuddi
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=2]Utkarsh OggySarawgi
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=2]VineetGarg
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>[affiliation=2]ShiveshRanjan
<span id="p1.3.5" class="ltx_ERROR undefined">\name</span>[affiliation=2]Ognjen (Oggi)Rudovic
<span id="p1.3.6" class="ltx_ERROR undefined">\name</span>[affiliation=2]AhmedHussen Abdelaziz
<span id="p1.3.7" class="ltx_ERROR undefined">\name</span>[affiliation=2]SaurabhAdya




</p>
</div>
<h1 class="ltx_title ltx_title_document">Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Voice activity detection (VAD) is a critical component in various applications such as speech recognition, speech enhancement, and hands-free communication systems. With the increasing demand for personalized and context-aware technologies, the need for effective personalized VAD systems has become paramount. In this paper, we present a comparative analysis of Personalized Voice Activity Detection (PVAD) systems to assess their real-world effectiveness. We introduce a comprehensive approach to assess PVAD systems, incorporating various performance metrics such as frame-level and utterance-level error rates, detection latency and accuracy, alongside user-level analysis. Through extensive experimentation and evaluation, we provide a thorough understanding of the strengths and limitations of various PVAD variants. This paper advances the understanding of PVAD technology by offering insights into its efficacy and viability in practical applications using a comprehensive set of metrics.
</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>voice activity detection, personalization, personalized voice activity detection, human-computer interaction
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>These authors contributed equally to this work</span></span></span><span id="footnotex2" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotetext: </span>Work done at Apple</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">VAD systems are crucial for modern speech recognition pipelines, allowing for the identification of speech and non-speech segments on a frame-by-frame basis and initiating downstream tasks such as speech recognition, enhancement, and endpointing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. However, in real-world scenarios involving multiple users, traditional VAD systems often trigger erroneously with disruptive false positives, necessitating Personalized Voice Activity Detection (PVAD) systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Traditional cascaded VAD and speaker verification models can address the PVAD task but fall short of meeting comprehensive requirements for seamless integration. PVAD systems, commonly used alongside speech recognition and other models, demand lightweight solutions for widespread real-time deployment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. While standalone speaker verification systems are reliable, they come with high resource demands, operational costs, and detection latencies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. With the imperative for real-time processing and triggering downstream tasks, low detection latency becomes crucial for PVAD systems. Therefore, PVAD systems must effectively balance accuracy, efficiency, and responsiveness to cater to various applications.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The introduction of PVAD by Ding et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> represented a significant advancement, offering a lightweight solution for robust target speaker speech detection. Unlike conventional methods relying on heavy speaker verification models generating dynamic speaker information in real-time, this approach extracts speaker information from enrollment utterances beforehand and integrates this static speaker information into the VAD architecture. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, authors extended the PVAD architecture and assessed its effectiveness in downstream Automatic Speech Recognition (ASR) tasks. Addressing the challenge of missing user-specific enrollments, Makishima et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> proposed an approach for enrollment less training of PVAD. Cheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> explored target speaker detection in the context of diarization. Medennikov et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> proposed techniques rooted in dinner party scenarios involving multi-speaker diarization. Other recent works have proposed competitive methods for target speaker voice activity detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">While these studies have introduced different forms of personalization in the voice activity detection pipeline, they often lack systematic evaluations across a range of crucial real-world metrics of accuracy, efficiency, and responsiveness. Notably, none of the studies have compared or benchmarked responsiveness, a critical factor in real-time systems. Furthermore, in personalized machine learning, it is essential to systematically assess any approach to ensure consistent performance improvements across users <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, lacking in previous PVAD works.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Finally, for real-world deployment, PVAD systems must adapt to a wide range of devices and environments. While high-end devices support intricate PVAD models, resource-constrained devices like smartwatches require lightweight and efficient solutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. To this end, our paper presents a detailed analysis of various fusion strategies for PVAD systems. By examining these strategies across different performance metrics, we aim to provide insights into their efficacy across diverse devices and usage scenarios, ultimately contributing to the development of versatile and optimized PVAD systems rooted in real-world viability. We first establish the components of PVAD systems and a variety of its comparative fusion strategies (Section <a href="#S2" title="2 System and Components ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), then introduce the evaluation metrics and experimental setup (Sections <a href="#S3" title="3 Evaluation Metrics ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S4" title="4 Experimental Setup ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), and finally discuss the results and conclusion (Sections <a href="#S5" title="5 Results and Discussion ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S6" title="6 Conclusion ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.09443/assets/pvads3.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> Pictorial depiction of different fusion strategies for personalization in Voice Activity Detection Systems.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>System and Components</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.8" class="ltx_p">A typical PVAD system includes VAD for speech detection, a speaker verification model, and fusion to integrate information from both components to detect the presence of the target speaker. For speaker verification, we utilized a pre-trained d-vector model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> to generate voice characteristic embeddings referred to as ``d-vectors". These d-vectors, computed and stored during a one-time enrollment process from user recordings, facilitate speaker verification using cosine similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and are denoted as <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="y_{speaker\_cosine\_similarity}" display="inline"><semantics id="S2.p1.1.m1.1a"><msub id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">y</mi><mrow id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml"><mi id="S2.p1.1.m1.1.1.3.2" xref="S2.p1.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.3" xref="S2.p1.1.m1.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1a" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.4" xref="S2.p1.1.m1.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1b" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.5" xref="S2.p1.1.m1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1c" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.6" xref="S2.p1.1.m1.1.1.3.6.cmml">k</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1d" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.7" xref="S2.p1.1.m1.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1e" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.8" xref="S2.p1.1.m1.1.1.3.8.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1f" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S2.p1.1.m1.1.1.3.9" xref="S2.p1.1.m1.1.1.3.9.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1g" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.10" xref="S2.p1.1.m1.1.1.3.10.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1h" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.11" xref="S2.p1.1.m1.1.1.3.11.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1i" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.12" xref="S2.p1.1.m1.1.1.3.12.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1j" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.13" xref="S2.p1.1.m1.1.1.3.13.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1k" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.14" xref="S2.p1.1.m1.1.1.3.14.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1l" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.15" xref="S2.p1.1.m1.1.1.3.15.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1m" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S2.p1.1.m1.1.1.3.16" xref="S2.p1.1.m1.1.1.3.16.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1n" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.17" xref="S2.p1.1.m1.1.1.3.17.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1o" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.18" xref="S2.p1.1.m1.1.1.3.18.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1p" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.19" xref="S2.p1.1.m1.1.1.3.19.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1q" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.20" xref="S2.p1.1.m1.1.1.3.20.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1r" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.21" xref="S2.p1.1.m1.1.1.3.21.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1s" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.22" xref="S2.p1.1.m1.1.1.3.22.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1t" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.23" xref="S2.p1.1.m1.1.1.3.23.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1u" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.24" xref="S2.p1.1.m1.1.1.3.24.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1v" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.25" xref="S2.p1.1.m1.1.1.3.25.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.1w" xref="S2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.p1.1.m1.1.1.3.26" xref="S2.p1.1.m1.1.1.3.26.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">𝑦</ci><apply id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3"><times id="S2.p1.1.m1.1.1.3.1.cmml" xref="S2.p1.1.m1.1.1.3.1"></times><ci id="S2.p1.1.m1.1.1.3.2.cmml" xref="S2.p1.1.m1.1.1.3.2">𝑠</ci><ci id="S2.p1.1.m1.1.1.3.3.cmml" xref="S2.p1.1.m1.1.1.3.3">𝑝</ci><ci id="S2.p1.1.m1.1.1.3.4.cmml" xref="S2.p1.1.m1.1.1.3.4">𝑒</ci><ci id="S2.p1.1.m1.1.1.3.5.cmml" xref="S2.p1.1.m1.1.1.3.5">𝑎</ci><ci id="S2.p1.1.m1.1.1.3.6.cmml" xref="S2.p1.1.m1.1.1.3.6">𝑘</ci><ci id="S2.p1.1.m1.1.1.3.7.cmml" xref="S2.p1.1.m1.1.1.3.7">𝑒</ci><ci id="S2.p1.1.m1.1.1.3.8.cmml" xref="S2.p1.1.m1.1.1.3.8">𝑟</ci><ci id="S2.p1.1.m1.1.1.3.9.cmml" xref="S2.p1.1.m1.1.1.3.9">_</ci><ci id="S2.p1.1.m1.1.1.3.10.cmml" xref="S2.p1.1.m1.1.1.3.10">𝑐</ci><ci id="S2.p1.1.m1.1.1.3.11.cmml" xref="S2.p1.1.m1.1.1.3.11">𝑜</ci><ci id="S2.p1.1.m1.1.1.3.12.cmml" xref="S2.p1.1.m1.1.1.3.12">𝑠</ci><ci id="S2.p1.1.m1.1.1.3.13.cmml" xref="S2.p1.1.m1.1.1.3.13">𝑖</ci><ci id="S2.p1.1.m1.1.1.3.14.cmml" xref="S2.p1.1.m1.1.1.3.14">𝑛</ci><ci id="S2.p1.1.m1.1.1.3.15.cmml" xref="S2.p1.1.m1.1.1.3.15">𝑒</ci><ci id="S2.p1.1.m1.1.1.3.16.cmml" xref="S2.p1.1.m1.1.1.3.16">_</ci><ci id="S2.p1.1.m1.1.1.3.17.cmml" xref="S2.p1.1.m1.1.1.3.17">𝑠</ci><ci id="S2.p1.1.m1.1.1.3.18.cmml" xref="S2.p1.1.m1.1.1.3.18">𝑖</ci><ci id="S2.p1.1.m1.1.1.3.19.cmml" xref="S2.p1.1.m1.1.1.3.19">𝑚</ci><ci id="S2.p1.1.m1.1.1.3.20.cmml" xref="S2.p1.1.m1.1.1.3.20">𝑖</ci><ci id="S2.p1.1.m1.1.1.3.21.cmml" xref="S2.p1.1.m1.1.1.3.21">𝑙</ci><ci id="S2.p1.1.m1.1.1.3.22.cmml" xref="S2.p1.1.m1.1.1.3.22">𝑎</ci><ci id="S2.p1.1.m1.1.1.3.23.cmml" xref="S2.p1.1.m1.1.1.3.23">𝑟</ci><ci id="S2.p1.1.m1.1.1.3.24.cmml" xref="S2.p1.1.m1.1.1.3.24">𝑖</ci><ci id="S2.p1.1.m1.1.1.3.25.cmml" xref="S2.p1.1.m1.1.1.3.25">𝑡</ci><ci id="S2.p1.1.m1.1.1.3.26.cmml" xref="S2.p1.1.m1.1.1.3.26">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">y_{speaker\_cosine\_similarity}</annotation></semantics></math>. Conventional VAD systems with mel filter bank features and LSTM networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> were employed to develop an end-to-end LSTM-based network for frame-level binary classification of `speech' (<math id="S2.p1.2.m2.1" class="ltx_Math" alttext="p_{s}" display="inline"><semantics id="S2.p1.2.m2.1a"><msub id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">p</mi><mi id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">𝑝</ci><ci id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">p_{s}</annotation></semantics></math>) and `no speech' (<math id="S2.p1.3.m3.1" class="ltx_Math" alttext="p_{ns}" display="inline"><semantics id="S2.p1.3.m3.1a"><msub id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml"><mi id="S2.p1.3.m3.1.1.2" xref="S2.p1.3.m3.1.1.2.cmml">p</mi><mrow id="S2.p1.3.m3.1.1.3" xref="S2.p1.3.m3.1.1.3.cmml"><mi id="S2.p1.3.m3.1.1.3.2" xref="S2.p1.3.m3.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m3.1.1.3.1" xref="S2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.p1.3.m3.1.1.3.3" xref="S2.p1.3.m3.1.1.3.3.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><apply id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p1.3.m3.1.1.1.cmml" xref="S2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.p1.3.m3.1.1.2.cmml" xref="S2.p1.3.m3.1.1.2">𝑝</ci><apply id="S2.p1.3.m3.1.1.3.cmml" xref="S2.p1.3.m3.1.1.3"><times id="S2.p1.3.m3.1.1.3.1.cmml" xref="S2.p1.3.m3.1.1.3.1"></times><ci id="S2.p1.3.m3.1.1.3.2.cmml" xref="S2.p1.3.m3.1.1.3.2">𝑛</ci><ci id="S2.p1.3.m3.1.1.3.3.cmml" xref="S2.p1.3.m3.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">p_{ns}</annotation></semantics></math>) at each frame (<math id="S2.p1.4.m4.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S2.p1.4.m4.1a"><msub id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml"><mi id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml">y</mi><mi id="S2.p1.4.m4.1.1.3" xref="S2.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2">𝑦</ci><ci id="S2.p1.4.m4.1.1.3.cmml" xref="S2.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">y_{i}</annotation></semantics></math>). The fusion component processes VAD and speaker verification component outputs to generate <math id="S2.p1.5.m5.3" class="ltx_Math" alttext="y\in{p_{ts},p_{nts},p_{ns}}" display="inline"><semantics id="S2.p1.5.m5.3a"><mrow id="S2.p1.5.m5.3.3" xref="S2.p1.5.m5.3.3.cmml"><mi id="S2.p1.5.m5.3.3.5" xref="S2.p1.5.m5.3.3.5.cmml">y</mi><mo id="S2.p1.5.m5.3.3.4" xref="S2.p1.5.m5.3.3.4.cmml">∈</mo><mrow id="S2.p1.5.m5.3.3.3.3" xref="S2.p1.5.m5.3.3.3.4.cmml"><msub id="S2.p1.5.m5.1.1.1.1.1" xref="S2.p1.5.m5.1.1.1.1.1.cmml"><mi id="S2.p1.5.m5.1.1.1.1.1.2" xref="S2.p1.5.m5.1.1.1.1.1.2.cmml">p</mi><mrow id="S2.p1.5.m5.1.1.1.1.1.3" xref="S2.p1.5.m5.1.1.1.1.1.3.cmml"><mi id="S2.p1.5.m5.1.1.1.1.1.3.2" xref="S2.p1.5.m5.1.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p1.5.m5.1.1.1.1.1.3.1" xref="S2.p1.5.m5.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.p1.5.m5.1.1.1.1.1.3.3" xref="S2.p1.5.m5.1.1.1.1.1.3.3.cmml">s</mi></mrow></msub><mo id="S2.p1.5.m5.3.3.3.3.4" xref="S2.p1.5.m5.3.3.3.4.cmml">,</mo><msub id="S2.p1.5.m5.2.2.2.2.2" xref="S2.p1.5.m5.2.2.2.2.2.cmml"><mi id="S2.p1.5.m5.2.2.2.2.2.2" xref="S2.p1.5.m5.2.2.2.2.2.2.cmml">p</mi><mrow id="S2.p1.5.m5.2.2.2.2.2.3" xref="S2.p1.5.m5.2.2.2.2.2.3.cmml"><mi id="S2.p1.5.m5.2.2.2.2.2.3.2" xref="S2.p1.5.m5.2.2.2.2.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.p1.5.m5.2.2.2.2.2.3.1" xref="S2.p1.5.m5.2.2.2.2.2.3.1.cmml">​</mo><mi id="S2.p1.5.m5.2.2.2.2.2.3.3" xref="S2.p1.5.m5.2.2.2.2.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p1.5.m5.2.2.2.2.2.3.1a" xref="S2.p1.5.m5.2.2.2.2.2.3.1.cmml">​</mo><mi id="S2.p1.5.m5.2.2.2.2.2.3.4" xref="S2.p1.5.m5.2.2.2.2.2.3.4.cmml">s</mi></mrow></msub><mo id="S2.p1.5.m5.3.3.3.3.5" xref="S2.p1.5.m5.3.3.3.4.cmml">,</mo><msub id="S2.p1.5.m5.3.3.3.3.3" xref="S2.p1.5.m5.3.3.3.3.3.cmml"><mi id="S2.p1.5.m5.3.3.3.3.3.2" xref="S2.p1.5.m5.3.3.3.3.3.2.cmml">p</mi><mrow id="S2.p1.5.m5.3.3.3.3.3.3" xref="S2.p1.5.m5.3.3.3.3.3.3.cmml"><mi id="S2.p1.5.m5.3.3.3.3.3.3.2" xref="S2.p1.5.m5.3.3.3.3.3.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.p1.5.m5.3.3.3.3.3.3.1" xref="S2.p1.5.m5.3.3.3.3.3.3.1.cmml">​</mo><mi id="S2.p1.5.m5.3.3.3.3.3.3.3" xref="S2.p1.5.m5.3.3.3.3.3.3.3.cmml">s</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.3b"><apply id="S2.p1.5.m5.3.3.cmml" xref="S2.p1.5.m5.3.3"><in id="S2.p1.5.m5.3.3.4.cmml" xref="S2.p1.5.m5.3.3.4"></in><ci id="S2.p1.5.m5.3.3.5.cmml" xref="S2.p1.5.m5.3.3.5">𝑦</ci><list id="S2.p1.5.m5.3.3.3.4.cmml" xref="S2.p1.5.m5.3.3.3.3"><apply id="S2.p1.5.m5.1.1.1.1.1.cmml" xref="S2.p1.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.5.m5.1.1.1.1.1.1.cmml" xref="S2.p1.5.m5.1.1.1.1.1">subscript</csymbol><ci id="S2.p1.5.m5.1.1.1.1.1.2.cmml" xref="S2.p1.5.m5.1.1.1.1.1.2">𝑝</ci><apply id="S2.p1.5.m5.1.1.1.1.1.3.cmml" xref="S2.p1.5.m5.1.1.1.1.1.3"><times id="S2.p1.5.m5.1.1.1.1.1.3.1.cmml" xref="S2.p1.5.m5.1.1.1.1.1.3.1"></times><ci id="S2.p1.5.m5.1.1.1.1.1.3.2.cmml" xref="S2.p1.5.m5.1.1.1.1.1.3.2">𝑡</ci><ci id="S2.p1.5.m5.1.1.1.1.1.3.3.cmml" xref="S2.p1.5.m5.1.1.1.1.1.3.3">𝑠</ci></apply></apply><apply id="S2.p1.5.m5.2.2.2.2.2.cmml" xref="S2.p1.5.m5.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.p1.5.m5.2.2.2.2.2.1.cmml" xref="S2.p1.5.m5.2.2.2.2.2">subscript</csymbol><ci id="S2.p1.5.m5.2.2.2.2.2.2.cmml" xref="S2.p1.5.m5.2.2.2.2.2.2">𝑝</ci><apply id="S2.p1.5.m5.2.2.2.2.2.3.cmml" xref="S2.p1.5.m5.2.2.2.2.2.3"><times id="S2.p1.5.m5.2.2.2.2.2.3.1.cmml" xref="S2.p1.5.m5.2.2.2.2.2.3.1"></times><ci id="S2.p1.5.m5.2.2.2.2.2.3.2.cmml" xref="S2.p1.5.m5.2.2.2.2.2.3.2">𝑛</ci><ci id="S2.p1.5.m5.2.2.2.2.2.3.3.cmml" xref="S2.p1.5.m5.2.2.2.2.2.3.3">𝑡</ci><ci id="S2.p1.5.m5.2.2.2.2.2.3.4.cmml" xref="S2.p1.5.m5.2.2.2.2.2.3.4">𝑠</ci></apply></apply><apply id="S2.p1.5.m5.3.3.3.3.3.cmml" xref="S2.p1.5.m5.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.p1.5.m5.3.3.3.3.3.1.cmml" xref="S2.p1.5.m5.3.3.3.3.3">subscript</csymbol><ci id="S2.p1.5.m5.3.3.3.3.3.2.cmml" xref="S2.p1.5.m5.3.3.3.3.3.2">𝑝</ci><apply id="S2.p1.5.m5.3.3.3.3.3.3.cmml" xref="S2.p1.5.m5.3.3.3.3.3.3"><times id="S2.p1.5.m5.3.3.3.3.3.3.1.cmml" xref="S2.p1.5.m5.3.3.3.3.3.3.1"></times><ci id="S2.p1.5.m5.3.3.3.3.3.3.2.cmml" xref="S2.p1.5.m5.3.3.3.3.3.3.2">𝑛</ci><ci id="S2.p1.5.m5.3.3.3.3.3.3.3.cmml" xref="S2.p1.5.m5.3.3.3.3.3.3.3">𝑠</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.3c">y\in{p_{ts},p_{nts},p_{ns}}</annotation></semantics></math> per frame, where <math id="S2.p1.6.m6.1" class="ltx_Math" alttext="p_{ts}" display="inline"><semantics id="S2.p1.6.m6.1a"><msub id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml"><mi id="S2.p1.6.m6.1.1.2" xref="S2.p1.6.m6.1.1.2.cmml">p</mi><mrow id="S2.p1.6.m6.1.1.3" xref="S2.p1.6.m6.1.1.3.cmml"><mi id="S2.p1.6.m6.1.1.3.2" xref="S2.p1.6.m6.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p1.6.m6.1.1.3.1" xref="S2.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.p1.6.m6.1.1.3.3" xref="S2.p1.6.m6.1.1.3.3.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><apply id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.p1.6.m6.1.1.1.cmml" xref="S2.p1.6.m6.1.1">subscript</csymbol><ci id="S2.p1.6.m6.1.1.2.cmml" xref="S2.p1.6.m6.1.1.2">𝑝</ci><apply id="S2.p1.6.m6.1.1.3.cmml" xref="S2.p1.6.m6.1.1.3"><times id="S2.p1.6.m6.1.1.3.1.cmml" xref="S2.p1.6.m6.1.1.3.1"></times><ci id="S2.p1.6.m6.1.1.3.2.cmml" xref="S2.p1.6.m6.1.1.3.2">𝑡</ci><ci id="S2.p1.6.m6.1.1.3.3.cmml" xref="S2.p1.6.m6.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">p_{ts}</annotation></semantics></math> denotes target speaker speech, <math id="S2.p1.7.m7.1" class="ltx_Math" alttext="p_{nts}" display="inline"><semantics id="S2.p1.7.m7.1a"><msub id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml"><mi id="S2.p1.7.m7.1.1.2" xref="S2.p1.7.m7.1.1.2.cmml">p</mi><mrow id="S2.p1.7.m7.1.1.3" xref="S2.p1.7.m7.1.1.3.cmml"><mi id="S2.p1.7.m7.1.1.3.2" xref="S2.p1.7.m7.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.1.1.3.1" xref="S2.p1.7.m7.1.1.3.1.cmml">​</mo><mi id="S2.p1.7.m7.1.1.3.3" xref="S2.p1.7.m7.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.1.1.3.1a" xref="S2.p1.7.m7.1.1.3.1.cmml">​</mo><mi id="S2.p1.7.m7.1.1.3.4" xref="S2.p1.7.m7.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.1b"><apply id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.p1.7.m7.1.1.1.cmml" xref="S2.p1.7.m7.1.1">subscript</csymbol><ci id="S2.p1.7.m7.1.1.2.cmml" xref="S2.p1.7.m7.1.1.2">𝑝</ci><apply id="S2.p1.7.m7.1.1.3.cmml" xref="S2.p1.7.m7.1.1.3"><times id="S2.p1.7.m7.1.1.3.1.cmml" xref="S2.p1.7.m7.1.1.3.1"></times><ci id="S2.p1.7.m7.1.1.3.2.cmml" xref="S2.p1.7.m7.1.1.3.2">𝑛</ci><ci id="S2.p1.7.m7.1.1.3.3.cmml" xref="S2.p1.7.m7.1.1.3.3">𝑡</ci><ci id="S2.p1.7.m7.1.1.3.4.cmml" xref="S2.p1.7.m7.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.1c">p_{nts}</annotation></semantics></math> denotes non-target speaker speech, and <math id="S2.p1.8.m8.1" class="ltx_Math" alttext="p_{ns}" display="inline"><semantics id="S2.p1.8.m8.1a"><msub id="S2.p1.8.m8.1.1" xref="S2.p1.8.m8.1.1.cmml"><mi id="S2.p1.8.m8.1.1.2" xref="S2.p1.8.m8.1.1.2.cmml">p</mi><mrow id="S2.p1.8.m8.1.1.3" xref="S2.p1.8.m8.1.1.3.cmml"><mi id="S2.p1.8.m8.1.1.3.2" xref="S2.p1.8.m8.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.p1.8.m8.1.1.3.1" xref="S2.p1.8.m8.1.1.3.1.cmml">​</mo><mi id="S2.p1.8.m8.1.1.3.3" xref="S2.p1.8.m8.1.1.3.3.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p1.8.m8.1b"><apply id="S2.p1.8.m8.1.1.cmml" xref="S2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.p1.8.m8.1.1.1.cmml" xref="S2.p1.8.m8.1.1">subscript</csymbol><ci id="S2.p1.8.m8.1.1.2.cmml" xref="S2.p1.8.m8.1.1.2">𝑝</ci><apply id="S2.p1.8.m8.1.1.3.cmml" xref="S2.p1.8.m8.1.1.3"><times id="S2.p1.8.m8.1.1.3.1.cmml" xref="S2.p1.8.m8.1.1.3.1"></times><ci id="S2.p1.8.m8.1.1.3.2.cmml" xref="S2.p1.8.m8.1.1.3.2">𝑛</ci><ci id="S2.p1.8.m8.1.1.3.3.cmml" xref="S2.p1.8.m8.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m8.1c">p_{ns}</annotation></semantics></math> denotes no speech. Following sections discuss approaches to achieve this fusion.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Fusion Strategies for PVAD</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In this section, we explore various PVAD variants that integrate Speaker Verification and VAD systems using multimodal fusion techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. These variants, including early, latent, and score fusions, utilize both static and dynamic speaker embeddings. While multiple combinations are feasible, we concentrate on established architectures shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Unless noted otherwise, all methods rely on static speaker information.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Dynamic Score Combination (DSC)</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">In this approach, acoustic features undergo processing by VAD and Speaker verification models as discussed earlier, resulting in speech detection and speaker verification scores per frame. These scores are then integrated to produce PVAD outputs per frame (<math id="S2.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="f^{i}" display="inline"><semantics id="S2.SS1.SSS1.p1.1.m1.1a"><msup id="S2.SS1.SSS1.p1.1.m1.1.1" xref="S2.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.SSS1.p1.1.m1.1.1.2" xref="S2.SS1.SSS1.p1.1.m1.1.1.2.cmml">f</mi><mi id="S2.SS1.SSS1.p1.1.m1.1.1.3" xref="S2.SS1.SSS1.p1.1.m1.1.1.3.cmml">i</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.1.m1.1b"><apply id="S2.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1">superscript</csymbol><ci id="S2.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1.2">𝑓</ci><ci id="S2.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.1.m1.1c">f^{i}</annotation></semantics></math>). However, operating at the frame level for both VAD and speaker verification models, this method is resource-intensive.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Early Fusion (EF)</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.3" class="ltx_p">Drawing inspiration from the approach described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, we combine the acoustic features <math id="S2.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="x_{features}" display="inline"><semantics id="S2.SS1.SSS2.p1.1.m1.1a"><msub id="S2.SS1.SSS2.p1.1.m1.1.1" xref="S2.SS1.SSS2.p1.1.m1.1.1.cmml"><mi id="S2.SS1.SSS2.p1.1.m1.1.1.2" xref="S2.SS1.SSS2.p1.1.m1.1.1.2.cmml">x</mi><mrow id="S2.SS1.SSS2.p1.1.m1.1.1.3" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.cmml"><mi id="S2.SS1.SSS2.p1.1.m1.1.1.3.2" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.1.m1.1.1.3.1" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.1.m1.1.1.3.3" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.1.m1.1.1.3.1a" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.1.m1.1.1.3.4" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.1.m1.1.1.3.1b" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.1.m1.1.1.3.5" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.1.m1.1.1.3.1c" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.1.m1.1.1.3.6" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.1.m1.1.1.3.1d" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.1.m1.1.1.3.7" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.1.m1.1.1.3.1e" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.1.m1.1.1.3.8" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.1.m1.1.1.3.1f" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.1.m1.1.1.3.9" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.9.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.1.m1.1b"><apply id="S2.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.2">𝑥</ci><apply id="S2.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.3"><times id="S2.SS1.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.1"></times><ci id="S2.SS1.SSS2.p1.1.m1.1.1.3.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.2">𝑓</ci><ci id="S2.SS1.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.3">𝑒</ci><ci id="S2.SS1.SSS2.p1.1.m1.1.1.3.4.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.4">𝑎</ci><ci id="S2.SS1.SSS2.p1.1.m1.1.1.3.5.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.5">𝑡</ci><ci id="S2.SS1.SSS2.p1.1.m1.1.1.3.6.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.6">𝑢</ci><ci id="S2.SS1.SSS2.p1.1.m1.1.1.3.7.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.7">𝑟</ci><ci id="S2.SS1.SSS2.p1.1.m1.1.1.3.8.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.8">𝑒</ci><ci id="S2.SS1.SSS2.p1.1.m1.1.1.3.9.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.9">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.1.m1.1c">x_{features}</annotation></semantics></math> with the enrollment speaker embeddings <math id="S2.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="x_{enroll\_speaker\_embedding}" display="inline"><semantics id="S2.SS1.SSS2.p1.2.m2.1a"><msub id="S2.SS1.SSS2.p1.2.m2.1.1" xref="S2.SS1.SSS2.p1.2.m2.1.1.cmml"><mi id="S2.SS1.SSS2.p1.2.m2.1.1.2" xref="S2.SS1.SSS2.p1.2.m2.1.1.2.cmml">x</mi><mrow id="S2.SS1.SSS2.p1.2.m2.1.1.3" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.cmml"><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.2" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.3" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1a" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.4" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1b" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.5" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1c" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.6" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1d" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.7" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.7.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1e" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S2.SS1.SSS2.p1.2.m2.1.1.3.8" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.8.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1f" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.9" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.9.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1g" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.10" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.10.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1h" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.11" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.11.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1i" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.12" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.12.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1j" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.13" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.13.cmml">k</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1k" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.14" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.14.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1l" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.15" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.15.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1m" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S2.SS1.SSS2.p1.2.m2.1.1.3.16" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.16.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1n" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.17" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.17.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1o" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.18" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.18.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1p" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.19" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.19.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1q" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.20" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.20.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1r" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.21" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.21.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1s" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.22" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.22.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1t" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.23" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.23.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1u" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.24" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.24.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.2.m2.1.1.3.1v" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS2.p1.2.m2.1.1.3.25" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.25.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.2.m2.1b"><apply id="S2.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.2">𝑥</ci><apply id="S2.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3"><times id="S2.SS1.SSS2.p1.2.m2.1.1.3.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.1"></times><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.2.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.2">𝑒</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.3.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.3">𝑛</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.4.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.4">𝑟</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.5.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.5">𝑜</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.6.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.6">𝑙</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.7.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.7">𝑙</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.8.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.8">_</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.9.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.9">𝑠</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.10.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.10">𝑝</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.11.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.11">𝑒</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.12.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.12">𝑎</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.13.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.13">𝑘</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.14.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.14">𝑒</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.15.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.15">𝑟</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.16.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.16">_</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.17.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.17">𝑒</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.18.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.18">𝑚</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.19.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.19">𝑏</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.20.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.20">𝑒</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.21.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.21">𝑑</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.22.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.22">𝑑</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.23.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.23">𝑖</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.24.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.24">𝑛</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3.25.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3.25">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.2.m2.1c">x_{enroll\_speaker\_embedding}</annotation></semantics></math> for every frame <math id="S2.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="f_{i}" display="inline"><semantics id="S2.SS1.SSS2.p1.3.m3.1a"><msub id="S2.SS1.SSS2.p1.3.m3.1.1" xref="S2.SS1.SSS2.p1.3.m3.1.1.cmml"><mi id="S2.SS1.SSS2.p1.3.m3.1.1.2" xref="S2.SS1.SSS2.p1.3.m3.1.1.2.cmml">f</mi><mi id="S2.SS1.SSS2.p1.3.m3.1.1.3" xref="S2.SS1.SSS2.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.3.m3.1b"><apply id="S2.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.3.m3.1.1.1.cmml" xref="S2.SS1.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.3.m3.1.1.2.cmml" xref="S2.SS1.SSS2.p1.3.m3.1.1.2">𝑓</ci><ci id="S2.SS1.SSS2.p1.3.m3.1.1.3.cmml" xref="S2.SS1.SSS2.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.3.m3.1c">f_{i}</annotation></semantics></math> across all utterances in the dataset. This combined feature set is utilized to train a PVAD model end-to-end, which predicts the likelihood of the target speaker's speech presence for each frame. As this approach relies on static enrollment speaker information, it eliminates the necessity of running speaker verification models, thus enhancing resource efficiency.</p>
</div>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Latent Fusion (LF)</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.3" class="ltx_p">The EF technique discussed in Section <a href="#S2.SS1.SSS2" title="2.1.2 Early Fusion (EF) ‣ 2.1 Fusion Strategies for PVAD ‣ 2 System and Components ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1.2</span></a> yields high-dimensional feature spaces, leading to a parameter-intensive model. To address this, we employ a two-step process. Initially, a VAD-like network extracts speech embeddings, <math id="S2.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="x_{encoder}" display="inline"><semantics id="S2.SS1.SSS3.p1.1.m1.1a"><msub id="S2.SS1.SSS3.p1.1.m1.1.1" xref="S2.SS1.SSS3.p1.1.m1.1.1.cmml"><mi id="S2.SS1.SSS3.p1.1.m1.1.1.2" xref="S2.SS1.SSS3.p1.1.m1.1.1.2.cmml">x</mi><mrow id="S2.SS1.SSS3.p1.1.m1.1.1.3" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.cmml"><mi id="S2.SS1.SSS3.p1.1.m1.1.1.3.2" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.1.m1.1.1.3.1" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.1.m1.1.1.3.3" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.1.m1.1.1.3.1a" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.1.m1.1.1.3.4" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.1.m1.1.1.3.1b" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.1.m1.1.1.3.5" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.1.m1.1.1.3.1c" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.1.m1.1.1.3.6" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.6.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.1.m1.1.1.3.1d" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.1.m1.1.1.3.7" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.1.m1.1.1.3.1e" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.1.m1.1.1.3.8" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.8.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.1.m1.1b"><apply id="S2.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS3.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.2">𝑥</ci><apply id="S2.SS1.SSS3.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3"><times id="S2.SS1.SSS3.p1.1.m1.1.1.3.1.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.1"></times><ci id="S2.SS1.SSS3.p1.1.m1.1.1.3.2.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.2">𝑒</ci><ci id="S2.SS1.SSS3.p1.1.m1.1.1.3.3.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.3">𝑛</ci><ci id="S2.SS1.SSS3.p1.1.m1.1.1.3.4.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.4">𝑐</ci><ci id="S2.SS1.SSS3.p1.1.m1.1.1.3.5.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.5">𝑜</ci><ci id="S2.SS1.SSS3.p1.1.m1.1.1.3.6.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.6">𝑑</ci><ci id="S2.SS1.SSS3.p1.1.m1.1.1.3.7.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.7">𝑒</ci><ci id="S2.SS1.SSS3.p1.1.m1.1.1.3.8.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.8">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.1.m1.1c">x_{encoder}</annotation></semantics></math>, from acoustic features, <math id="S2.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="x_{features}" display="inline"><semantics id="S2.SS1.SSS3.p1.2.m2.1a"><msub id="S2.SS1.SSS3.p1.2.m2.1.1" xref="S2.SS1.SSS3.p1.2.m2.1.1.cmml"><mi id="S2.SS1.SSS3.p1.2.m2.1.1.2" xref="S2.SS1.SSS3.p1.2.m2.1.1.2.cmml">x</mi><mrow id="S2.SS1.SSS3.p1.2.m2.1.1.3" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.cmml"><mi id="S2.SS1.SSS3.p1.2.m2.1.1.3.2" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.2.m2.1.1.3.1" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.2.m2.1.1.3.3" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.2.m2.1.1.3.1a" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.2.m2.1.1.3.4" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.2.m2.1.1.3.1b" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.2.m2.1.1.3.5" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.2.m2.1.1.3.1c" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.2.m2.1.1.3.6" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.2.m2.1.1.3.1d" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.2.m2.1.1.3.7" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.2.m2.1.1.3.1e" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.2.m2.1.1.3.8" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.2.m2.1.1.3.1f" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.2.m2.1.1.3.9" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.9.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.2.m2.1b"><apply id="S2.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS3.p1.2.m2.1.1.1.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.SSS3.p1.2.m2.1.1.2.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.2">𝑥</ci><apply id="S2.SS1.SSS3.p1.2.m2.1.1.3.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.3"><times id="S2.SS1.SSS3.p1.2.m2.1.1.3.1.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.1"></times><ci id="S2.SS1.SSS3.p1.2.m2.1.1.3.2.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.2">𝑓</ci><ci id="S2.SS1.SSS3.p1.2.m2.1.1.3.3.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.3">𝑒</ci><ci id="S2.SS1.SSS3.p1.2.m2.1.1.3.4.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.4">𝑎</ci><ci id="S2.SS1.SSS3.p1.2.m2.1.1.3.5.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.5">𝑡</ci><ci id="S2.SS1.SSS3.p1.2.m2.1.1.3.6.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.6">𝑢</ci><ci id="S2.SS1.SSS3.p1.2.m2.1.1.3.7.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.7">𝑟</ci><ci id="S2.SS1.SSS3.p1.2.m2.1.1.3.8.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.8">𝑒</ci><ci id="S2.SS1.SSS3.p1.2.m2.1.1.3.9.cmml" xref="S2.SS1.SSS3.p1.2.m2.1.1.3.9">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.2.m2.1c">x_{features}</annotation></semantics></math>, capturing crucial information about speech presence. Subsequently, we augment these speech embeddings with user-specific embeddings, <math id="S2.SS1.SSS3.p1.3.m3.1" class="ltx_Math" alttext="x_{enroll\_speaker\_embedding}" display="inline"><semantics id="S2.SS1.SSS3.p1.3.m3.1a"><msub id="S2.SS1.SSS3.p1.3.m3.1.1" xref="S2.SS1.SSS3.p1.3.m3.1.1.cmml"><mi id="S2.SS1.SSS3.p1.3.m3.1.1.2" xref="S2.SS1.SSS3.p1.3.m3.1.1.2.cmml">x</mi><mrow id="S2.SS1.SSS3.p1.3.m3.1.1.3" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.cmml"><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.2" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.3" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1a" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.4" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1b" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.5" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1c" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.6" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1d" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.7" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.7.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1e" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S2.SS1.SSS3.p1.3.m3.1.1.3.8" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.8.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1f" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.9" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.9.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1g" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.10" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.10.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1h" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.11" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.11.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1i" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.12" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.12.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1j" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.13" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.13.cmml">k</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1k" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.14" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.14.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1l" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.15" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.15.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1m" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S2.SS1.SSS3.p1.3.m3.1.1.3.16" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.16.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1n" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.17" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.17.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1o" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.18" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.18.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1p" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.19" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.19.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1q" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.20" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.20.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1r" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.21" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.21.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1s" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.22" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.22.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1t" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.23" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.23.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1u" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.24" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.24.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.3.m3.1.1.3.1v" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.3.m3.1.1.3.25" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.25.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.3.m3.1b"><apply id="S2.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS3.p1.3.m3.1.1.1.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.SSS3.p1.3.m3.1.1.2.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.2">𝑥</ci><apply id="S2.SS1.SSS3.p1.3.m3.1.1.3.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3"><times id="S2.SS1.SSS3.p1.3.m3.1.1.3.1.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.1"></times><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.2.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.2">𝑒</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.3.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.3">𝑛</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.4.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.4">𝑟</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.5.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.5">𝑜</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.6.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.6">𝑙</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.7.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.7">𝑙</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.8.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.8">_</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.9.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.9">𝑠</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.10.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.10">𝑝</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.11.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.11">𝑒</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.12.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.12">𝑎</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.13.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.13">𝑘</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.14.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.14">𝑒</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.15.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.15">𝑟</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.16.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.16">_</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.17.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.17">𝑒</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.18.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.18">𝑚</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.19.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.19">𝑏</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.20.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.20">𝑒</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.21.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.21">𝑑</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.22.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.22">𝑑</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.23.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.23">𝑖</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.24.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.24">𝑛</ci><ci id="S2.SS1.SSS3.p1.3.m3.1.1.3.25.cmml" xref="S2.SS1.SSS3.p1.3.m3.1.1.3.25">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.3.m3.1c">x_{enroll\_speaker\_embedding}</annotation></semantics></math>, creating a unified representation fed to Fully Connected Network (FCN) of PVAD. This approach effectively manages input feature dimensionality while preserving the advantages of static early fusion as discussed earlier.</p>
</div>
</section>
<section id="S2.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>Conditioned Latent Fusion (CLF)</h4>

<div id="S2.SS1.SSS4.p1" class="ltx_para">
<p id="S2.SS1.SSS4.p1.1" class="ltx_p">Combining speaker embeddings and acoustic embeddings, as discussed in <a href="#S2.SS1.SSS3" title="2.1.3 Latent Fusion (LF) ‣ 2.1 Fusion Strategies for PVAD ‣ 2 System and Components ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1.3</span></a>, is suboptimal because they capture different information modalities, which restricts the model's learning capabilities without suitable inductive biases. To remedy this, we utilize Feature-wise Linear Modulation (FiLM) modulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, wherein speaker embeddings modulate the acoustic embeddings of each frame, thereby enhancing the fusion of multimodal information.</p>
</div>
</section>
<section id="S2.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.5 </span>Dynamic Conditioned Latent Fusion (DCLF)</h4>

<div id="S2.SS1.SSS5.p1" class="ltx_para">
<p id="S2.SS1.SSS5.p1.1" class="ltx_p">In this strategy, we improve CLF by incorporating dynamic speaker information through speaker embedding extraction using a lightweight speaker embedding encoder network along with static enrollment embeddings. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, we jointly train this network with the conditioned fusion architecture, integrating the enrollment speaker embedding and cosine similarity between enrollment and frame-level dynamic speaker embeddings from the lightweight extractor (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation Metrics</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Considering the diverse applications of PVAD in triggering downstream tasks such as speech recognition, diarization, and endpointing, a well-evaluated PVAD model has the potential to be versatile across multiple use cases. To align with different usage scenarios and requirements, we have employed the following metrics to evaluate PVAD systems:</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Equal Error Rate</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Equal Error Rate (EER) represents the point on the Detection Error Tradeoff (DET) curve <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> where the False Positive Rate (FPR) equals the False Negative Rate (FNR). A lower EER is preferable. We employed the following EER metrics to determine system accuracy owing to real-world usage:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Frame-level EER (fEER)</p>
<ul id="S3.I1.i1.I1" class="ltx_itemize">
<li id="S3.I1.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S3.I1.i1.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S3.I1.i1.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.I1.i1.p1.1" class="ltx_p">In the PVAD setting, fEER PVAD measures the system's accuracy in distinguishing between targeted user speech, non-targeted user speech, and no speech at the frame-level.</p>
</div>
</li>
<li id="S3.I1.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S3.I1.i1.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S3.I1.i1.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i1.I1.i2.p1.1" class="ltx_p">In VAD setting, fEER VAD measures the system's accuracy in distinguishing between speech and no speech at frame-level where enrollment speaker embeddings are missing<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Utterance-level EER (uEER)</p>
<ul id="S3.I1.i2.I1" class="ltx_itemize">
<li id="S3.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S3.I1.i2.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S3.I1.i2.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i2.I1.i1.p1.1" class="ltx_p">In PVAD setting. uEER PVAD aggregates granular frame-level understanding to the utterance-level, providing a better representation of the system's effectiveness in triggering downstream tasks.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Detection Latency</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">While EER is crucial for model assessment and comparison, detection latency in targeted speech detection is pivotal for real-time responsiveness, particularly in PVAD systems triggering downstream tasks real-time. In this study, we quantify detection latency at the utterance-level, defined as the time from targeted speech onset to its detection at the designated uEER operating point.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Detection Accuracy</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To evaluate improvements in target speaker detection per user, we determine the accuracy of detecting the target speaker in their test utterances at the uEER operating point. Detection accuracy is calculated as the percentage of utterances where the target user's speech is accurately detected out of the total utterances spoken by the target user.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.5" class="ltx_p">We experimented with the LibriSpeech dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, containing approximately <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="960" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">960</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn type="integer" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">960</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">960</annotation></semantics></math> hours of speech from <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="2338" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mn id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">2338</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><cn type="integer" id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">2338</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">2338</annotation></semantics></math> users. The dataset includes meticulously segmented and aligned speech segments from audiobooks, with a training subset of <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="460" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mn id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">460</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><cn type="integer" id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">460</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">460</annotation></semantics></math> hours of clean speech and <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="500" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mn id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><cn type="integer" id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">500</annotation></semantics></math> hours of noisy speech. Test and validation sets each consists of <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="11" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mn id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><cn type="integer" id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">11</annotation></semantics></math> hours of clean and noisy data.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.2" class="ltx_p">To simulate natural conversational dynamics, we created concatenated audio files by uniformly sampling segments from <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="1-3" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">1</mn><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">−</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><minus id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></minus><cn type="integer" id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">1</cn><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">1-3</annotation></semantics></math> users and augmented them with noise from the MUSAN corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, varying the SNR from <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="0-30" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mn id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">0</mn><mo id="S4.SS1.p2.2.m2.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.cmml">−</mo><mn id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><minus id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1"></minus><cn type="integer" id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">0</cn><cn type="integer" id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">0-30</annotation></semantics></math> dB for increased robustness. Each resulting audio file represented an ``utterance", with one user randomly assigned as the target user.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.3" class="ltx_p"><math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="3-5" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mn id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">3</mn><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">−</mo><mn id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><minus id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></minus><cn type="integer" id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">3</cn><cn type="integer" id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">3-5</annotation></semantics></math> speech segments are randomly selected from each user as ``enrollment segments" to acquire their ``enrollment speaker embeddings". These embeddings are derived from a speaker verification model and averaged to generate a robust d-vector embedding estimate. To accommodate scenarios where users opt not to enroll their voices, we randomly excluded enrollment speaker embeddings for <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mrow id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mn id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2.cmml">20</mn><mo id="S4.SS1.p3.2.m2.1.1.1" xref="S4.SS1.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">20\%</annotation></semantics></math> of concatenated utterances. In such cases, target enrollment embeddings were replaced with <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="256-" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mrow id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mn id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml">256</mn><mo id="S4.SS1.p3.3.m3.1.1.3" xref="S4.SS1.p3.3.m3.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><csymbol cd="latexml" id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">limit-from</csymbol><cn type="integer" id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2">256</cn><minus id="S4.SS1.p3.3.m3.1.1.3.cmml" xref="S4.SS1.p3.3.m3.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">256-</annotation></semantics></math>dimensional zero embeddings, treating both target and non-target speakers as the target speaker.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.3" class="ltx_p">In each utterance, segments corresponding to the target speaker were labeled as <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="ts" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><times id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1"></times><ci id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">𝑡</ci><ci id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">ts</annotation></semantics></math>, segments from other speakers as <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="nts" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mrow id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mi id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.2.m2.1.1.1" xref="S4.SS1.p4.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.2.m2.1.1.1a" xref="S4.SS1.p4.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS1.p4.2.m2.1.1.4" xref="S4.SS1.p4.2.m2.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><times id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1.1"></times><ci id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">𝑛</ci><ci id="S4.SS1.p4.2.m2.1.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3">𝑡</ci><ci id="S4.SS1.p4.2.m2.1.1.4.cmml" xref="S4.SS1.p4.2.m2.1.1.4">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">nts</annotation></semantics></math>, and non-speech segments as <math id="S4.SS1.p4.3.m3.1" class="ltx_Math" alttext="ns" display="inline"><semantics id="S4.SS1.p4.3.m3.1a"><mrow id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml"><mi id="S4.SS1.p4.3.m3.1.1.2" xref="S4.SS1.p4.3.m3.1.1.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.3.m3.1.1.1" xref="S4.SS1.p4.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS1.p4.3.m3.1.1.3" xref="S4.SS1.p4.3.m3.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><apply id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1"><times id="S4.SS1.p4.3.m3.1.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1.1"></times><ci id="S4.SS1.p4.3.m3.1.1.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2">𝑛</ci><ci id="S4.SS1.p4.3.m3.1.1.3.cmml" xref="S4.SS1.p4.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">ns</annotation></semantics></math>, facilitating differentiation of speech segments based on speaker identity and content for PVAD model training and evaluation.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Model Architecture</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.10" class="ltx_p">The featurizer (shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) extracts, <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="x_{features}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">x</mi><mrow id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml"><mi id="S4.SS2.p1.1.m1.1.1.3.2" xref="S4.SS2.p1.1.m1.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.3.1" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3.3" xref="S4.SS2.p1.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.3.1a" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3.4" xref="S4.SS2.p1.1.m1.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.3.1b" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3.5" xref="S4.SS2.p1.1.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.3.1c" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3.6" xref="S4.SS2.p1.1.m1.1.1.3.6.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.3.1d" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3.7" xref="S4.SS2.p1.1.m1.1.1.3.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.3.1e" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3.8" xref="S4.SS2.p1.1.m1.1.1.3.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.3.1f" xref="S4.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3.9" xref="S4.SS2.p1.1.m1.1.1.3.9.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝑥</ci><apply id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><times id="S4.SS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3.1"></times><ci id="S4.SS2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.3.2">𝑓</ci><ci id="S4.SS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3">𝑒</ci><ci id="S4.SS2.p1.1.m1.1.1.3.4.cmml" xref="S4.SS2.p1.1.m1.1.1.3.4">𝑎</ci><ci id="S4.SS2.p1.1.m1.1.1.3.5.cmml" xref="S4.SS2.p1.1.m1.1.1.3.5">𝑡</ci><ci id="S4.SS2.p1.1.m1.1.1.3.6.cmml" xref="S4.SS2.p1.1.m1.1.1.3.6">𝑢</ci><ci id="S4.SS2.p1.1.m1.1.1.3.7.cmml" xref="S4.SS2.p1.1.m1.1.1.3.7">𝑟</ci><ci id="S4.SS2.p1.1.m1.1.1.3.8.cmml" xref="S4.SS2.p1.1.m1.1.1.3.8">𝑒</ci><ci id="S4.SS2.p1.1.m1.1.1.3.9.cmml" xref="S4.SS2.p1.1.m1.1.1.3.9">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">x_{features}</annotation></semantics></math>, a <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn type="integer" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">40</annotation></semantics></math>-dimensional log-mel filter bank features per frame, employing a frame length of <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mn id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><cn type="integer" id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">25</annotation></semantics></math>ms and a step size of <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mn id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><cn type="integer" id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">10</annotation></semantics></math>ms, resulting in features at <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mn id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><cn type="integer" id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">100</annotation></semantics></math>Hz for <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="x_{audio}" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><msub id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mi id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml">x</mi><mrow id="S4.SS2.p1.6.m6.1.1.3" xref="S4.SS2.p1.6.m6.1.1.3.cmml"><mi id="S4.SS2.p1.6.m6.1.1.3.2" xref="S4.SS2.p1.6.m6.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.3.1" xref="S4.SS2.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.6.m6.1.1.3.3" xref="S4.SS2.p1.6.m6.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.3.1a" xref="S4.SS2.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.6.m6.1.1.3.4" xref="S4.SS2.p1.6.m6.1.1.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.3.1b" xref="S4.SS2.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.6.m6.1.1.3.5" xref="S4.SS2.p1.6.m6.1.1.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.3.1c" xref="S4.SS2.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.6.m6.1.1.3.6" xref="S4.SS2.p1.6.m6.1.1.3.6.cmml">o</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2">𝑥</ci><apply id="S4.SS2.p1.6.m6.1.1.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3"><times id="S4.SS2.p1.6.m6.1.1.3.1.cmml" xref="S4.SS2.p1.6.m6.1.1.3.1"></times><ci id="S4.SS2.p1.6.m6.1.1.3.2.cmml" xref="S4.SS2.p1.6.m6.1.1.3.2">𝑎</ci><ci id="S4.SS2.p1.6.m6.1.1.3.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3.3">𝑢</ci><ci id="S4.SS2.p1.6.m6.1.1.3.4.cmml" xref="S4.SS2.p1.6.m6.1.1.3.4">𝑑</ci><ci id="S4.SS2.p1.6.m6.1.1.3.5.cmml" xref="S4.SS2.p1.6.m6.1.1.3.5">𝑖</ci><ci id="S4.SS2.p1.6.m6.1.1.3.6.cmml" xref="S4.SS2.p1.6.m6.1.1.3.6">𝑜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">x_{audio}</annotation></semantics></math> in the dataset. The feature encoder, depicted in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, consists of a <math id="S4.SS2.p1.7.m7.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS2.p1.7.m7.1a"><mn id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><cn type="integer" id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">2</annotation></semantics></math>-layer LSTM network with a hidden size of <math id="S4.SS2.p1.8.m8.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S4.SS2.p1.8.m8.1a"><mn id="S4.SS2.p1.8.m8.1.1" xref="S4.SS2.p1.8.m8.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.8.m8.1b"><cn type="integer" id="S4.SS2.p1.8.m8.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.8.m8.1c">64</annotation></semantics></math>, capturing temporal dependencies within utterances. This is followed by two FCN layers with tanh activation functions to enable nonlinear transformations of the extracted embeddings. In the CLF strategy, we employed a FiLM layer to modulate the speech embeddings produced by the feature encoder using the speaker embedding, before passing it to FCN layers. As for DCLF, we utilized a single-layer LSTM (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>: Speaker embedding generator) to generate a dynamic <math id="S4.SS2.p1.9.m9.1" class="ltx_Math" alttext="256-" display="inline"><semantics id="S4.SS2.p1.9.m9.1a"><mrow id="S4.SS2.p1.9.m9.1.1" xref="S4.SS2.p1.9.m9.1.1.cmml"><mn id="S4.SS2.p1.9.m9.1.1.2" xref="S4.SS2.p1.9.m9.1.1.2.cmml">256</mn><mo id="S4.SS2.p1.9.m9.1.1.3" xref="S4.SS2.p1.9.m9.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.9.m9.1b"><apply id="S4.SS2.p1.9.m9.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1"><csymbol cd="latexml" id="S4.SS2.p1.9.m9.1.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1">limit-from</csymbol><cn type="integer" id="S4.SS2.p1.9.m9.1.1.2.cmml" xref="S4.SS2.p1.9.m9.1.1.2">256</cn><minus id="S4.SS2.p1.9.m9.1.1.3.cmml" xref="S4.SS2.p1.9.m9.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.9.m9.1c">256-</annotation></semantics></math>dimensional speaker embedding, which is then used to extract <math id="S4.SS2.p1.10.m10.1" class="ltx_Math" alttext="y_{speaker\_cosine\_similarity}" display="inline"><semantics id="S4.SS2.p1.10.m10.1a"><msub id="S4.SS2.p1.10.m10.1.1" xref="S4.SS2.p1.10.m10.1.1.cmml"><mi id="S4.SS2.p1.10.m10.1.1.2" xref="S4.SS2.p1.10.m10.1.1.2.cmml">y</mi><mrow id="S4.SS2.p1.10.m10.1.1.3" xref="S4.SS2.p1.10.m10.1.1.3.cmml"><mi id="S4.SS2.p1.10.m10.1.1.3.2" xref="S4.SS2.p1.10.m10.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.3" xref="S4.SS2.p1.10.m10.1.1.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1a" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.4" xref="S4.SS2.p1.10.m10.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1b" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.5" xref="S4.SS2.p1.10.m10.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1c" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.6" xref="S4.SS2.p1.10.m10.1.1.3.6.cmml">k</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1d" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.7" xref="S4.SS2.p1.10.m10.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1e" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.8" xref="S4.SS2.p1.10.m10.1.1.3.8.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1f" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S4.SS2.p1.10.m10.1.1.3.9" xref="S4.SS2.p1.10.m10.1.1.3.9.cmml">_</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1g" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.10" xref="S4.SS2.p1.10.m10.1.1.3.10.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1h" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.11" xref="S4.SS2.p1.10.m10.1.1.3.11.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1i" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.12" xref="S4.SS2.p1.10.m10.1.1.3.12.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1j" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.13" xref="S4.SS2.p1.10.m10.1.1.3.13.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1k" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.14" xref="S4.SS2.p1.10.m10.1.1.3.14.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1l" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.15" xref="S4.SS2.p1.10.m10.1.1.3.15.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1m" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="S4.SS2.p1.10.m10.1.1.3.16" xref="S4.SS2.p1.10.m10.1.1.3.16.cmml">_</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1n" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.17" xref="S4.SS2.p1.10.m10.1.1.3.17.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1o" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.18" xref="S4.SS2.p1.10.m10.1.1.3.18.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1p" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.19" xref="S4.SS2.p1.10.m10.1.1.3.19.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1q" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.20" xref="S4.SS2.p1.10.m10.1.1.3.20.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1r" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.21" xref="S4.SS2.p1.10.m10.1.1.3.21.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1s" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.22" xref="S4.SS2.p1.10.m10.1.1.3.22.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1t" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.23" xref="S4.SS2.p1.10.m10.1.1.3.23.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1u" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.24" xref="S4.SS2.p1.10.m10.1.1.3.24.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1v" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.25" xref="S4.SS2.p1.10.m10.1.1.3.25.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.3.1w" xref="S4.SS2.p1.10.m10.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3.26" xref="S4.SS2.p1.10.m10.1.1.3.26.cmml">y</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.10.m10.1b"><apply id="S4.SS2.p1.10.m10.1.1.cmml" xref="S4.SS2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.10.m10.1.1.1.cmml" xref="S4.SS2.p1.10.m10.1.1">subscript</csymbol><ci id="S4.SS2.p1.10.m10.1.1.2.cmml" xref="S4.SS2.p1.10.m10.1.1.2">𝑦</ci><apply id="S4.SS2.p1.10.m10.1.1.3.cmml" xref="S4.SS2.p1.10.m10.1.1.3"><times id="S4.SS2.p1.10.m10.1.1.3.1.cmml" xref="S4.SS2.p1.10.m10.1.1.3.1"></times><ci id="S4.SS2.p1.10.m10.1.1.3.2.cmml" xref="S4.SS2.p1.10.m10.1.1.3.2">𝑠</ci><ci id="S4.SS2.p1.10.m10.1.1.3.3.cmml" xref="S4.SS2.p1.10.m10.1.1.3.3">𝑝</ci><ci id="S4.SS2.p1.10.m10.1.1.3.4.cmml" xref="S4.SS2.p1.10.m10.1.1.3.4">𝑒</ci><ci id="S4.SS2.p1.10.m10.1.1.3.5.cmml" xref="S4.SS2.p1.10.m10.1.1.3.5">𝑎</ci><ci id="S4.SS2.p1.10.m10.1.1.3.6.cmml" xref="S4.SS2.p1.10.m10.1.1.3.6">𝑘</ci><ci id="S4.SS2.p1.10.m10.1.1.3.7.cmml" xref="S4.SS2.p1.10.m10.1.1.3.7">𝑒</ci><ci id="S4.SS2.p1.10.m10.1.1.3.8.cmml" xref="S4.SS2.p1.10.m10.1.1.3.8">𝑟</ci><ci id="S4.SS2.p1.10.m10.1.1.3.9.cmml" xref="S4.SS2.p1.10.m10.1.1.3.9">_</ci><ci id="S4.SS2.p1.10.m10.1.1.3.10.cmml" xref="S4.SS2.p1.10.m10.1.1.3.10">𝑐</ci><ci id="S4.SS2.p1.10.m10.1.1.3.11.cmml" xref="S4.SS2.p1.10.m10.1.1.3.11">𝑜</ci><ci id="S4.SS2.p1.10.m10.1.1.3.12.cmml" xref="S4.SS2.p1.10.m10.1.1.3.12">𝑠</ci><ci id="S4.SS2.p1.10.m10.1.1.3.13.cmml" xref="S4.SS2.p1.10.m10.1.1.3.13">𝑖</ci><ci id="S4.SS2.p1.10.m10.1.1.3.14.cmml" xref="S4.SS2.p1.10.m10.1.1.3.14">𝑛</ci><ci id="S4.SS2.p1.10.m10.1.1.3.15.cmml" xref="S4.SS2.p1.10.m10.1.1.3.15">𝑒</ci><ci id="S4.SS2.p1.10.m10.1.1.3.16.cmml" xref="S4.SS2.p1.10.m10.1.1.3.16">_</ci><ci id="S4.SS2.p1.10.m10.1.1.3.17.cmml" xref="S4.SS2.p1.10.m10.1.1.3.17">𝑠</ci><ci id="S4.SS2.p1.10.m10.1.1.3.18.cmml" xref="S4.SS2.p1.10.m10.1.1.3.18">𝑖</ci><ci id="S4.SS2.p1.10.m10.1.1.3.19.cmml" xref="S4.SS2.p1.10.m10.1.1.3.19">𝑚</ci><ci id="S4.SS2.p1.10.m10.1.1.3.20.cmml" xref="S4.SS2.p1.10.m10.1.1.3.20">𝑖</ci><ci id="S4.SS2.p1.10.m10.1.1.3.21.cmml" xref="S4.SS2.p1.10.m10.1.1.3.21">𝑙</ci><ci id="S4.SS2.p1.10.m10.1.1.3.22.cmml" xref="S4.SS2.p1.10.m10.1.1.3.22">𝑎</ci><ci id="S4.SS2.p1.10.m10.1.1.3.23.cmml" xref="S4.SS2.p1.10.m10.1.1.3.23">𝑟</ci><ci id="S4.SS2.p1.10.m10.1.1.3.24.cmml" xref="S4.SS2.p1.10.m10.1.1.3.24">𝑖</ci><ci id="S4.SS2.p1.10.m10.1.1.3.25.cmml" xref="S4.SS2.p1.10.m10.1.1.3.25">𝑡</ci><ci id="S4.SS2.p1.10.m10.1.1.3.26.cmml" xref="S4.SS2.p1.10.m10.1.1.3.26">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.10.m10.1c">y_{speaker\_cosine\_similarity}</annotation></semantics></math> for conditioned fusion.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Training Setup</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We trained and evaluated five models: one for score combination in the VAD setting as discussed in Section <a href="#S2.SS1.SSS1" title="2.1.1 Dynamic Score Combination (DSC) ‣ 2.1 Fusion Strategies for PVAD ‣ 2 System and Components ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>, and one for each end-to-end PVAD variant described in <a href="#S2.SS1" title="2.1 Fusion Strategies for PVAD ‣ 2 System and Components ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>. All models used cross-entropy loss, binary for VAD and categorical for PVAD, with training employing the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> at a learning rate of <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="1e^{-3}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">​</mo><msup id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml"><mi id="S4.SS3.p1.1.m1.1.1.3.2" xref="S4.SS3.p1.1.m1.1.1.3.2.cmml">e</mi><mrow id="S4.SS3.p1.1.m1.1.1.3.3" xref="S4.SS3.p1.1.m1.1.1.3.3.cmml"><mo id="S4.SS3.p1.1.m1.1.1.3.3a" xref="S4.SS3.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS3.p1.1.m1.1.1.3.3.2" xref="S4.SS3.p1.1.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">1</cn><apply id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.3.2.cmml" xref="S4.SS3.p1.1.m1.1.1.3.2">𝑒</ci><apply id="S4.SS3.p1.1.m1.1.1.3.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3.3"><minus id="S4.SS3.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS3.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS3.p1.1.m1.1.1.3.3.2.cmml" xref="S4.SS3.p1.1.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">1e^{-3}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We evaluated five models trained as described in Section <a href="#S4.SS3" title="4.3 Training Setup ‣ 4 Experimental Setup ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>, using the evaluation metrics outlined in Section <a href="#S3" title="3 Evaluation Metrics ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The standard DSC model, detailed in Section <a href="#S2.SS1.SSS1" title="2.1.1 Dynamic Score Combination (DSC) ‣ 2.1 Fusion Strategies for PVAD ‣ 2 System and Components ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>, serves as our baseline for comparison against the end-to-end trained PVAD models.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Accuracy: Equal Error Rate</h3>

<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Frame level Analysis</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">Table <a href="#S5.T1" title="Table 1 ‣ 5.1.1 Frame level Analysis ‣ 5.1 Accuracy: Equal Error Rate ‣ 5 Results and Discussion ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the frame-level Equal Error Rate (fEER) comparison among all PVAD variants in both PVAD and VAD settings, with and without enrollment speaker embeddings. Across the PVAD setting, the four end-to-end PVAD variants outperform the DSC baseline. Notably, the CLF variant marginally outperforms DCLF in terms of fEER PVAD, indicating that DCLF, resembling a standalone speaker verification model, requires more frames for precise prediction.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">DSC</span></th>
<th id="S5.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">EF</span></th>
<th id="S5.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">LF</span></th>
<th id="S5.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">CLF</span></th>
<th id="S5.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">DCLF</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<th id="S5.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T1.1.2.1.1.1" class="ltx_text ltx_font_bold">fEER PVAD</span></th>
<td id="S5.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">26.2</td>
<td id="S5.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">12.2</td>
<td id="S5.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">10.6</td>
<td id="S5.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.1.2.1.5.1" class="ltx_text ltx_font_bold ltx_font_italic">10.2</span></td>
<td id="S5.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">10.4</td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<th id="S5.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T1.1.3.2.1.1" class="ltx_text ltx_font_bold">fEER VAD</span></th>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_center"><span id="S5.T1.1.3.2.2.1" class="ltx_text ltx_font_bold ltx_font_italic">5.9</span></td>
<td id="S5.T1.1.3.2.3" class="ltx_td ltx_align_center">7.4</td>
<td id="S5.T1.1.3.2.4" class="ltx_td ltx_align_center">6.6</td>
<td id="S5.T1.1.3.2.5" class="ltx_td ltx_align_center">6.6</td>
<td id="S5.T1.1.3.2.6" class="ltx_td ltx_align_center">6.6</td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<th id="S5.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T1.1.4.3.1.1" class="ltx_text ltx_font_bold">uEER PVAD</span></th>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">11.6</td>
<td id="S5.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">11.2</td>
<td id="S5.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">11.3</td>
<td id="S5.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb">10.6</td>
<td id="S5.T1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.1.4.3.6.1" class="ltx_text ltx_font_bold ltx_font_italic">9.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>EER comparison of PVAD variants.</figcaption>
</figure>
<div id="S5.SS1.SSS1.p2" class="ltx_para">
<p id="S5.SS1.SSS1.p2.1" class="ltx_p">Ideally, PVAD systems should achieve comparable performance to VAD systems in detecting speech, in scenarios with missing speaker enrollment information. Table <a href="#S5.T1" title="Table 1 ‣ 5.1.1 Frame level Analysis ‣ 5.1 Accuracy: Equal Error Rate ‣ 5 Results and Discussion ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> displays the fEER VAD of all PVAD systems evaluated as VAD systems. While the DSC variant shows marginally superior fEER compared to other fusion strategies, likely due to the task-specific training of DSC's VAD component, the rest of the end-to-end PVAD models introduce a new dimension for inferring the presence of the target speaker through lightweight fusion.</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Utterance level Analysis</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">We smoothed the frame-level PVAD output scores to extract an utterance-level score by calculating a moving average over a window frame of 5 and selecting the highest score. Table <a href="#S5.T1" title="Table 1 ‣ 5.1.1 Frame level Analysis ‣ 5.1 Accuracy: Equal Error Rate ‣ 5 Results and Discussion ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> compares the utterance-level EER of PVAD variants for the PVAD task. Our findings reveal that all fusion strategies outperform the baseline DSC variant. Notably, CLF outperforms basic concatenation-based fusions like LF and EF due to the FiLM-based fusion strategy. This strategy enhances information integration by leveraging speaker embeddings and acoustic features, resulting in improved CF performance over EF and LF. Additionally, DCLF outperforms other fusion approaches by integrating dynamic speaker cosine similarity scores extracted at the frame-level with the static enrollment speaker embedding. Due to the absence of speech-free utterances in the LibriSpeech dataset, uEER computation for the VAD task was omitted.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Detection Latency and Detection Accuracy</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Studies indicates that speaker verification models achieve higher accuracy with increased audio context <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Figure <a href="#S5.F2" title="Figure 2 ‣ 5.2 Detection Latency and Detection Accuracy ‣ 5 Results and Discussion ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> explores the impact of audio duration on PVAD model accuracy at the uEER operating point. End-to-end PVAD variants notably outperform DSC accuracies with shorter audio context, highlighting their high responsiveness and reliability, making them ideal for real-time streaming applications. As audio context increases, their performance converges.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2406.09443/assets/newFigures/audioLevelPlots.jpg" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Impact of audio duration on detection accuracy</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Table <a href="#S5.T2" title="Table 2 ‣ 5.2 Detection Latency and Detection Accuracy ‣ 5 Results and Discussion ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the median detection latency and accuracy of all users in the test set. It shows that all PVAD variants surpass the baseline DSC in both metrics, achieving a minimum 40% improvement in latency alongside enhanced accuracy.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">DSC</span></th>
<th id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">EF</span></th>
<th id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">LF</span></th>
<th id="S5.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">CLF</span></th>
<th id="S5.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">DCLF</span></th>
</tr>
<tr id="S5.T2.1.2.2" class="ltx_tr">
<td id="S5.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T2.1.2.2.1.1" class="ltx_text ltx_font_bold">Latency</span></td>
<td id="S5.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">432.5</td>
<td id="S5.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">240</td>
<td id="S5.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">202.5</td>
<td id="S5.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.1.2.2.5.1" class="ltx_text ltx_font_bold">185</span></td>
<td id="S5.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">240</td>
</tr>
<tr id="S5.T2.1.3.3" class="ltx_tr">
<td id="S5.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T2.1.3.3.1.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S5.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_bb">93.2</td>
<td id="S5.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_bb">96.7</td>
<td id="S5.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_bb">96.6</td>
<td id="S5.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_bb">95.9</td>
<td id="S5.T2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.1.3.3.6.1" class="ltx_text ltx_font_bold">96.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Median latency (ms) and accuracy results of PVAD models.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>User-level evaluation</h3>

<figure id="S5.F3" class="ltx_figure"><img src="/html/2406.09443/assets/newFigures/subjectLevelStackedBarPlots.jpg" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="340" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> User-level analysis of a) detection latency, b) accuracy of target speaker presence.</figcaption>
</figure>
<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.3" class="ltx_p">Ensuring consistent performance improvements across users is crucial for personalization<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. To explore this aspect, we analyze the results discussed in Section <a href="#S5.SS2" title="5.2 Detection Latency and Detection Accuracy ‣ 5 Results and Discussion ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> at a user level across 73 users in the test set, focusing on the best-performing PVAD variants, CLF and DCLF, and comparing them against the baseline DSC. Figure <a href="#S5.F3" title="Figure 3 ‣ 5.3 User-level evaluation ‣ 5 Results and Discussion ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates a user-level comparison of latency among DSC, CLF, and DCLF. In terms of detection latency, both CLF and DCLF significantly outperform DSC (p-value <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="&lt;0.01" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mrow id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mi id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml"></mi><mo id="S5.SS3.p1.1.m1.1.1.1" xref="S5.SS3.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><lt id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">absent</csymbol><cn type="float" id="S5.SS3.p1.1.m1.1.1.3.cmml" xref="S5.SS3.p1.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">&lt;0.01</annotation></semantics></math> using Wilcoxon signed-rank test <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>). Precisely, CLF and DCLF improve latency over DSC in 67 and 59 out of 73 users, respectively. Finally, both DCLF and CLF results in increase or retention of detection accuracy at EER operating point among majority of users compared to DSC (CLF <math id="S5.SS3.p1.2.m2.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S5.SS3.p1.2.m2.1a"><mo id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><geq id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">\geq</annotation></semantics></math> DSC: 42/73, DCLF <math id="S5.SS3.p1.3.m3.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S5.SS3.p1.3.m3.1a"><mo id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><geq id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">\geq</annotation></semantics></math> DSC: 52/73), although no significant difference was observed in any pairwise comparisons of detection accuracy.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Model Efficiency</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">The end-to-end PVAD models surpass the baseline across all metrics despite being significantly smaller in size. Specifically, CLF and DCLF consist of only 5% and 26%, respectively, of the parameters compared to the baseline DSC, as shown in Table <a href="#S5.T3" title="Table 3 ‣ 5.4 Model Efficiency ‣ 5 Results and Discussion ‣ Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. This trait, along with their superior performance, renders them ideal for on-device deployments.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">DSC</span></td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">EF</span></td>
<td id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">LF</span></td>
<td id="S5.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">CLF</span></td>
<td id="S5.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.1.6.1" class="ltx_text ltx_font_bold">DCLF</span></td>
</tr>
<tr id="S5.T3.1.2.2" class="ltx_tr">
<th id="S5.T3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S5.T3.1.2.2.1.1" class="ltx_text ltx_font_bold">Parameters</span></th>
<td id="S5.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1.50</td>
<td id="S5.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.13</td>
<td id="S5.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.08</td>
<td id="S5.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.10</td>
<td id="S5.T3.1.2.2.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.40</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Parameter count of PVAD models (in millions).</figcaption>
</figure>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">Our analysis reveals that relying solely on one performance metric is inadequate for evaluating and deploying PVAD systems. While CLF shows superior latency and frame-level EER, DCLF performs better in accurately detecting the target speaker at the utterance level. Different fusion-based models suit specific use cases: CLF or EF for devices with minimal latency and limited on-device requirements, and DCLF for devices prioritizing accuracy.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This study investigates various PVAD systems, revealing that lightweight end-to-end models, incorporating static speaker information and significantly smaller than parameter-intensive speaker verification-based models, outperform baseline systems. Our analysis indicates these models enhance the speed of detecting target speaker speech by at least 40%, crucial for real-world deployment. Moreover, dynamically estimating speaker characteristics improves accuracy over static fusion methods. Additionally, these PVAD models demonstrate comparable performance in VAD tasks compared to models trained solely for VAD. These findings highlight the potential of PVAD systems in real-world applications, offering reliable detection of the target speaker while improving speed and accuracy in speech recognition. PVAD systems represent a promising advancement, demonstrating effectiveness and suitability for practical deployment.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Sohn, N. S. Kim, and W. Sung, ``A statistical model-based voice activity detection,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE signal processing letters</em>, vol. 6, no. 1, pp. 1–3, 1999.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Ramırez, J. C. Segura, C. Benıtez, A. De La Torre, and A. Rubio, ``Efficient voice activity detection algorithms using long-term speech information,'' <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Speech communication</em>, vol. 42, no. 3-4, pp. 271–287, 2004.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Ramirez, J. M. Górriz, and J. C. Segura, ``Voice activity detection. fundamentals and speech recognition system robustness,'' <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Robust speech recognition and understanding</em>, vol. 6, no. 9, pp. 1–22, 2007.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
U. Oggy Sarawgi, J. Berkowitz, V. Garg, A. Kundu, M. Cho, S. Srujana Buddi, S. Adya, and A. Tewfik, ``Streaming anchor loss: Augmenting supervision with temporal significance,'' <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, pp. arXiv–2310, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S. S. Buddi, U. O. Sarawgi, T. Heeramun, K. Sawnhey, E. Yanosik, S. Rathinam, and S. Adya, ``Efficient multimodal neural networks for trigger-less voice assistants,'' <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.12063</em>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S.-Y. Chang, R. Prabhavalkar, Y. He, T. N. Sainath, and G. Simko, ``Joint endpointing and decoding with end-to-end models,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2019, pp. 5626–5630.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Chen, S. Wang, Y. Qian, and K. Yu, ``End-to-end speaker-dependent voice activity detection,'' <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.09906</em>, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
E. Variani, X. Lei, E. McDermott, I. L. Moreno, and J. Gonzalez-Dominguez, ``Deep neural networks for small footprint text-dependent speaker verification,'' in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2014 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.   IEEE, 2014, pp. 4052–4056.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. Ding, Q. Wang, S.-y. Chang, L. Wan, and I. L. Moreno, ``Personal VAD: Speaker-conditioned voice activity detection,'' <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.04284</em>, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Ding, R. Rikhye, Q. Liang, Y. He, Q. Wang, A. Narayanan, T. O'Malley, and I. McGraw, ``Personal VAD 2.0: Optimizing personal voice activity detection for on-device speech recognition,'' <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.03793</em>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
N. Makishima, M. Ihori, T. Tanaka, A. Takashima, S. Orihashi, and R. Masumura, ``Enrollment-less training for personalized voice activity detection,'' <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.12132</em>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Cheng, W. Wang, Y. Zhang, X. Qin, and M. Li, ``Target-speaker voice activity detection via sequence-to-sequence prediction,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Korenevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko, I. Podluzhny <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Target-speaker voice activity detection: a novel approach for multi-speaker diarization in a dinner party scenario,'' <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.07272</em>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. He, D. Raj, Z. Huang, J. Du, Z. Chen, and S. Watanabe, ``Target-speaker voice activity detection with improved i-vector estimation for unknown number of speaker,'' <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.03342</em>, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
W. Wang, X. Qin, and M. Li, ``Cross-channel attention-based target speaker voice activity detection: Experimental results for the m2met challenge,'' in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2022, pp. 9171–9175.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Z. Kang, J. Wang, J. Peng, and J. Xiao, ``SVVAD: Personal voice activity detection for speaker verification,'' <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.19581</em>, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
D. Wang, X. Xiao, N. Kanda, T. Yoshioka, and J. Wu, ``Target speaker voice activity detection with transformers and its integration with end-to-end neural diarization,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Z.-H. Tan, N. Dehak <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``rvad: An unsupervised segment-based robust voice activity detection method,'' <em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic">Computer speech &amp; language</em>, vol. 59, pp. 1–21, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
O. Rudovic, J. Lee, M. Dai, B. Schuller, and R. W. Picard, ``Personalized machine learning for robot perception of affect and engagement in autism therapy,'' <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Science Robotics</em>, vol. 3, no. 19, p. eaao6760, 2018.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
L. Wan, Q. Wang, A. Papir, and I. L. Moreno, ``Generalized end-to-end loss for speaker verification,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2018, pp. 4879–4883.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
K. K. George, C. S. Kumar, and A. Panda, ``Cosine distance features for robust speaker verification,'' in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Sixteenth annual conference of the international speech communication association</em>, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S. Tong, H. Gu, and K. Yu, ``A comparative study of robustness of deep learning approaches for VAD,'' in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2016, pp. 5695–5699.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
H. Sak, A. Senior, and F. Beaufays, ``Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition,'' 2014.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
T. Baltrušaitis, C. Ahuja, and L.-P. Morency, ``Multimodal machine learning: A survey and taxonomy,'' <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</em>, vol. 41, no. 2, pp. 423–443, 2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville, ``Film: Visual reasoning with a general conditioning layer,'' in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial intelligence</em>, vol. 32, no. 1, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. F. Martin, G. R. Doddington, T. Kamm, M. Ordowski, and M. A. Przybocki, ``The det curve in assessment of detection task performance.'' in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Eurospeech</em>, vol. 4, 1997, pp. 1895–1898.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S. Tong, N. Chen, Y. Qian, and K. Yu, ``Evaluating vad for automatic speech recognition,'' in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">2014 12th International Conference on Signal Processing (ICSP)</em>, 2014, pp. 2308–2314.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, ``Librispeech: an asr corpus based on public domain audio books,'' in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.   IEEE, 2015, pp. 5206–5210.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
D. Snyder, G. Chen, and D. Povey, ``Musan: A music, speech, and noise corpus,'' <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1510.08484</em>, 2015.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
D. P. Kingma and J. Ba, ``Adam: A method for stochastic optimization,'' <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>, 2014.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. J. Eronen, V. T. Peltonen, J. T. Tuomi, A. P. Klapuri, S. Fagerlund, T. Sorsa, G. Lorho, and J. Huopaniemi, ``Audio-based context recognition,'' <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Audio, Speech, and Language Processing</em>, vol. 14, no. 1, pp. 321–329, 2005.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
H. Fan and M. S. Poole, ``What is personalization? perspectives on the design and implementation of personalization in information systems,'' <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Journal of Organizational Computing and Electronic Commerce</em>, vol. 16, no. 3-4, pp. 179–202, 2006.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
R. F. Woolson, ``Wilcoxon signed-rank test,'' <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Wiley encyclopedia of clinical trials</em>, pp. 1–3, 2007.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.09441" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.09443" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.09443">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.09443" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.09444" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 21:55:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
