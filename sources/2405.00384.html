<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.00384] Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol</title><meta property="og:description" content="This paper presents a baseline approach and an experimental protocol for a specific content verification problem: detecting discrepancies between the audio and video modalities in multimedia content. We first design an…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.00384">

<!--Generated on Wed Jun  5 14:09:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Audio-visual forensics,  Audio-visual scene classification Content verification,  Content verification,  Self-attention">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Konstantinos Apostolidis
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:kapost@iti.gr">kapost@iti.gr</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-9470-6332" title="ORCID identifier" class="ltx_ref">0000-0002-9470-6332</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">Information Technologies Institute, CERTH</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Thessaloniki</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">Greece</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jakob Abeßer
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jakob.abesser@idmt.fraunhofer.de">jakob.abesser@idmt.fraunhofer.de</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-4689-7944" title="ORCID identifier" class="ltx_ref">0000-0003-4689-7944</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">Fraunhofer Institute for Digital Media Technology</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_city">Ilmenau</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_country">Germany</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luca Cuccovillo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:luca.cuccovillo@idmt.fraunhofer.de">luca.cuccovillo@idmt.fraunhofer.de</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-5559-6508" title="ORCID identifier" class="ltx_ref">0000-0001-5559-6508</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">Fraunhofer Institute for Digital Media Technology</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_city">Ilmenau</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_country">Germany</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vasileios Mezaris
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:bmezaris@iti.gr">bmezaris@iti.gr</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-0121-4364" title="ORCID identifier" class="ltx_ref">0000-0002-0121-4364</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id10.1.id1" class="ltx_text ltx_affiliation_institution">Information Technologies Institute, CERTH</span><span id="id11.2.id2" class="ltx_text ltx_affiliation_city">Thessaloniki</span><span id="id12.3.id3" class="ltx_text ltx_affiliation_country">Greece</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id13.id1" class="ltx_p">This paper presents a baseline approach and an experimental protocol for a specific content verification problem: detecting discrepancies between the audio and video modalities in multimedia content. We first design and optimize an audio-visual scene classifier, to compare with existing classification baselines that use both modalities. Then, by applying this classifier separately to the audio and the visual modality, we can detect scene-class inconsistencies between them. To facilitate further research and provide a common evaluation platform, we introduce an experimental protocol and a benchmark dataset simulating such inconsistencies. Our approach achieves state-of-the-art results in scene classification and promising outcomes in audio-visual discrepancies detection, highlighting its potential in content verification applications.</p>
</div>
<div class="ltx_keywords">Audio-visual forensics, Audio-visual scene classification Content verification, Content verification, Self-attention
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>M3rd ACM International Workshop on Multimedia AI against Disinformation organized with the ACM International Conference on Multimedia Retrieval (ICMR ’24); June 10–13, 2024; Phuket, Thailand</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>3rd ACM International
Workshop on Multimedia AI against Disinformation; June 10, 2024; Phuket,
Thailand</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>3rd ACM International Workshop on Multimedia AI against
Disinformation (MAD ’24), June 10, 2024, Phuket, Thailand</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3643491.3660287</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0552-6/24/06</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Multimedia information systems</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Machine learning</span></span></span><span id="id10" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Computer vision</span></span></span><span id="id11" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Hardware Digital signal processing</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Digital disinformation, the intentional dissemination of false or misleading information through digital media, encompasses various deceptive tactics, including fabricated news, tampered images, manipulated videos, and misleading narratives. Professionals across fields, such as journalists, security experts, and emergency management officers, are increasingly concerned about discerning genuine content from manipulated material. In response, AI-based content verification tools and integrated toolboxes like <cite class="ltx_cite ltx_citemacro_citep">(Teyssou et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> have emerged to assess the authenticity and integrity of digital content. While manual verification is labor-intensive, such support tools driven by machine learning offer scalable solutions, allowing rapid multimedia analysis and aiding in prioritizing human effort.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In audio editing, incorporating the soundscape of an acoustic scene has become standard practice. This soundscape addition serves two main purposes: first, it helps to mask editing points, seamlessly blending different audio segments; second, it enhances immersion, providing viewers with a more engaging auditory experience. For instance, in the context of news broadcasts or documentary filmmaking, audio soundscapes are often captured at the event being depicted in the video.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, when malicious users attempt to manipulate multimedia content, they are unlikely to have access to authentic audio soundscapes from the actual events. Instead, they may resort to using pre-existing ambient sounds, which may not align with the visual content they are manipulating. As a result, the manipulated content produced by malicious users could contain inconsistencies between the audio and video modalities. These inconsistencies could range from subtle discrepancies in background noise to more obvious mismatches in environmental cues, leading to a loss of the video’s credibility.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">While detecting AI-generated fabrications has garnered attention, identifying subtle but crucial disparities in audio-visual streams remains unexplored. Content verification often overlooks inconsistencies between different modalities, such as between the audio and visual components of video. Such disparities, whether it is an audio track incongruous with the visual scene, or the presence of conflicting environmental cues, may signal that an incoming video has been fabricated. While certain instances of audio-visual mismatches identified by our algorithm may be easily perceptible to human observers, our proposed method proves particularly valuable in automatically processing extensive raw video corpora, providing discerning insights into potential tampering occurrences.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our proposed baseline method adapts visual- and audio-scene classification techniques to detect such discrepancies, as depicted in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Another key contribution of this paper is the introduction of an experimental protocol and a benchmark dataset tailored to this task. We freely provide this dataset, as well as the source code of the proposed baseline method to serve as a valuable resource for the scientific community, promoting further advancements in the field of content verification.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2405.00384/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="657" height="395" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>The overall procedure employed in this paper. The red blocks represent the ensemble of visual embeddings (three blank rectangles inside). The blue blocks represent the ensemble of audio embeddings (three blank rectangles inside).</figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The rest of this paper is organized as follows: Section <a href="#S2" title="2. Related Work ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of relevant work in audio and visual scene classification and methods related to visual-audio discrepancies detection. Section <a href="#S3" title="3. Dataset and Experimental Protocol ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the process of generating our benchmark dataset and its key attributes. Section <a href="#S4" title="4. Proposed Method ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> describes our methodology for detecting discrepancies using audio and visual scene classifiers and the proposed aggregation scheme. Section <a href="#S5" title="5. Experiments ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> details our experiments on scene classification, visual-audio discrepancies detection, and a short ablation study for our proposed classifier, followed by our conclusions in Section <a href="#S6" title="6. Conclusions ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Acoustic Scene Classification</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Acoustic Scene Classification (ASC) is an audio tagging task in which an audio recording is classified into a semantic scene category based on the location of the recording. While acoustic scenes are often characterized by some unique sound events (such as car and truck sounds in the acoustic scene “street traffic”), some groups of scene classes share many types of sounds (such as the scene classes “pedestrian street” and “public square”), making their classification often challenging and ambiguous. The annual Detection and Classification of Acoustic Scenes and Events (DCASE) challenges and workshops have strongly stimulated research in the field of ASC over the last 10 years.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Early ASC methods combined traditional feature representations such as Mel-Frequency Cepstral Coefficients (MFCC) and classification approaches such as Gaussian Mixture Models (GMM) or Support Vector Machines (SVM). The current state-of-the-art models are instead based on deep neural network architectures such as convolutional neural networks or ResNets, and process audio data represented either as raw audio samples or as time-frequency representations such as Mel-spectrograms <cite class="ltx_cite ltx_citemacro_citep">(Virtanen et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>. In contrast to the sound event detection (SED) task, where sounds need to be localized in time, ASC models usually have a final pooling layer to obtain a time-aggregated classification result. Two of the most current research trends in the field of ASC are the development of domain adaptation methods to cope with microphone mismatch scenarios <cite class="ltx_cite ltx_citemacro_citep">(Drossos et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2019</a>; Mezza et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2021</a>; Johnson and Grollmisch, <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite> and the design of resource-efficient deep neural network architectures, which enable the deployment of ASC algorithms on mobile devices and hearables <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>. An extensive literature survey on ASC was published in <cite class="ltx_cite ltx_citemacro_citep">(Abeßer, <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Visual Scene Classification</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Visual Scene Classification (VSC) categorizes images or videos into predefined scene classes based on visual content, often relying on image classification techniques. Deep learning has revolutionized image classification by automatically learning complex features from data. Influential architectures like Residual Network (ResNet) <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite>, DenseNet <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>, and EfficientNet <cite class="ltx_cite ltx_citemacro_citep">(Tan and Le, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite> optimize model depth, width, and resolution. More recently, vision transformers <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite> have built upon the idea of adapting the transformer architecture, originally designed for natural language processing tasks, to the field of image classification. The image is broken into fixed-size patches and treated as tokens, enabling the application of self-attention mechanisms. It has gained significant attention for its competitive performance on various benchmark datasets. Finally, contrastive language-image models aim to learn joint representations of textual and visual data. These models align image and text representations in a shared latent space, bringing similar image-text pairs closer while pushing dissimilar ones apart. One notable example is CLIP (Contrastive Language-Image Pre-training) <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>, which has been successfully used for zero-shot and few-shot image classification, without explicit task-specific training as in <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Most of the discussed architectures have readily available pre-trained models on large image classification datasets like ImageNet and Places365. Places365 is particularly noteworthy, as it is extensively utilized in scene classification research. This dataset provides a rich and diverse assortment of images depicting various indoor and outdoor scenes from across the world. Each image is labeled with a specific scene category, such as natural landscapes, urban environments, interiors, and outdoor spaces. In total, the dataset includes 365 scene categories and comprises 1.8 million images.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Visual-audio discrepancies detection</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">To our knowledge, the sole existing work addressing similar concerns is the <cite class="ltx_cite ltx_citemacro_citep">(Bolles et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite> study, which dedicates a section to detecting scene discrepancies between audio and visual streams. However, their approach involves the independent design of audio and visual scene detectors, lacking evaluation on a standardized benchmark dataset. Furthermore, the evaluation relies on synthetic media constructed from video items of a publicly available dataset, forming a very limited testing set size (100 videos) and an undisclosed evaluation protocol. Additionally, the study focuses on a binary classification of audio-visual scenes, categorizing them as either “outdoors” or “indoors”. A closely related work to this domain is Task 1B of the DCASE 2021’s challenge which involves categorizing videos based on their audio and visual content. Participants in this challenge are required to develop and evaluate systems that can jointly analyze audio and visual information to determine the type of scene depicted in multimedia recordings. The performance of participating systems is evaluated based on classification accuracy on the provided TAU Urban Acoustic Scenes 2020 Mobile challenge <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2021b</a>)</cite> dataset - from now on we will refer to this dataset simply as TAU.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">The top-performing method of DCASE 2021 Task 1B, as discussed in <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2021a</a>)</cite>, leverages transfer learning and a hybrid fusion strategy. Its authors employ pre-trained deep neural networks for extracting both audio and visual features from videos. These features are combined using a hybrid fusion strategy, encompassing both early fusion (combining features before classification) and late fusion (combining classification results from separate audio and visual models).</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Dataset and Experimental Protocol</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Original scene classification dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The TAU dataset focuses on Audio-Visual Scene Classification (AVSC), featuring audio and video recordings from various urban locations. It includes 10 urban acoustic scene classes: “bus”, “metro”, “street pedestrian”, “public square”, “street traffic”, “tram”, “park”, “airport”, “shopping mall”, and “metro station”. Each class has audio and video recordings. Audio is in mono, 48 kHz, 24-bit WAV format, while video is synchronized MP4 clips at 25 fps, with resolutions ranging from 480p to 1080p.
The dataset is divided into <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">Development</span> and <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">Evaluation</span> sets. The <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">Development</span> set consists of 12,291 one-shot videos (i.e., a single continuous video shot, without any cuts, edits, or interruptions), each 10 seconds long, divided into training and testing portions. The <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_italic">Evaluation</span> set contains 72,000 videos, each lasting 2 seconds. Annotations for the <span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_italic">Evaluation</span> set are withheld and were managed by DCASE2021 organizers solely for challenge participants.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Proposed visual-audio discrepancies experimental protocol</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We introduce the Visual-Audio Discrepancy Detection (VADD) experimental protocol, curated to facilitate research in detecting discrepancies between visual and audio streams in videos. The dataset includes a subset of videos in which the visual content portrays one class (e.g., an outdoor scenery), while the accompanying audio track is sourced from a different class (e.g., the sound of an indoor environment). Aiming to leverage the wealth of visual and auditory data already available in the already existing TAU dataset, additionally expediting the data collection process, our VADD experimental protocol and dataset is created by re-purposing data and providing annotations for the TAU dataset. Specifically, we swap the audio and video streams for half of the videos in the TAU dataset, to create “manipulated” samples while keeping the rest of the videos unchanged to have “unmodified” samples. We ensured balanced sets of unmodifiedand manipulated samples through the following process (see Table <a href="#S3.T1" title="Table 1 ‣ 3.2. Proposed visual-audio discrepancies experimental protocol ‣ 3. Dataset and Experimental Protocol ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for the resulting distribution of samples across classes):</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Randomly select half of each class’s samples for inclusion in the “unmodified” set, and put the remaining half in a “bucket”;</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Randomly select from the bucket two items belonging to different classes, mutually swap their audio streams so that two new audio-visual samples are generated, and add them to the “manipulated” set;</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Repeat step 2 until all items left in the bucket belong to the same class;</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Finally, add these remaining items to the “unmodified” set.</p>
</div>
</li>
</ol>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Class-wise Distribution of Unmodifiedand Manipulated Samples in the VADD dataset</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Class</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Total</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Unmodified(%)</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Manipulated (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">airport</th>
<th id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">281</th>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">141 (50.18%)</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">140 (49.82%)</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">bus</th>
<th id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">327</th>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center">164 (50.15%)</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center">163 (49.85%)</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">metro</th>
<th id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">360</th>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center">180 (50.00%)</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center">180 (50.00%)</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">metro station</th>
<th id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">386</th>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center">193 (50.00%)</td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center">193 (50.00%)</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">park</th>
<th id="S3.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">386</th>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_center">193 (50.00%)</td>
<td id="S3.T1.1.6.5.4" class="ltx_td ltx_align_center">193 (50.00%)</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<th id="S3.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">public square</th>
<th id="S3.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">387</th>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_center">194 (50.13%)</td>
<td id="S3.T1.1.7.6.4" class="ltx_td ltx_align_center">193 (49.87%)</td>
</tr>
<tr id="S3.T1.1.8.7" class="ltx_tr">
<th id="S3.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">shopping mall</th>
<th id="S3.T1.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">387</th>
<td id="S3.T1.1.8.7.3" class="ltx_td ltx_align_center">194 (50.13%)</td>
<td id="S3.T1.1.8.7.4" class="ltx_td ltx_align_center">193 (49.87%)</td>
</tr>
<tr id="S3.T1.1.9.8" class="ltx_tr">
<th id="S3.T1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">street pedestrian</th>
<th id="S3.T1.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">421</th>
<td id="S3.T1.1.9.8.3" class="ltx_td ltx_align_center">211 (50.12%)</td>
<td id="S3.T1.1.9.8.4" class="ltx_td ltx_align_center">210 (49.88%)</td>
</tr>
<tr id="S3.T1.1.10.9" class="ltx_tr">
<th id="S3.T1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">street traffic</th>
<th id="S3.T1.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">402</th>
<td id="S3.T1.1.10.9.3" class="ltx_td ltx_align_center">201 (50.00%)</td>
<td id="S3.T1.1.10.9.4" class="ltx_td ltx_align_center">201 (50.00%)</td>
</tr>
<tr id="S3.T1.1.11.10" class="ltx_tr">
<th id="S3.T1.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">tram</th>
<th id="S3.T1.1.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">308</th>
<td id="S3.T1.1.11.10.3" class="ltx_td ltx_align_center">154 (50.00%)</td>
<td id="S3.T1.1.11.10.4" class="ltx_td ltx_align_center">154 (50.00%)</td>
</tr>
<tr id="S3.T1.1.12.11" class="ltx_tr">
<th id="S3.T1.1.12.11.1" class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_t"></th>
<th id="S3.T1.1.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t">3645</th>
<td id="S3.T1.1.12.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">1825 (50.07%)</td>
<td id="S3.T1.1.12.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">1820 (49.93%)</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We introduce a 3-class version of our dataset, derived from the original 10-class dataset, motivated by the need to provide two levels of difficulty. This 3-class taxonomy aligns with the “Low-Complexity Acoustic Scene Classification” sub-task of the “Acoustic scene classification” challenge, a simplification of the acoustic scene classification task, where the 10 acoustic scene classes are mapped to three classes: indoor, outdoor, and vehicle. In this 3-class variant of VADD, videos are categorized into higher-level scene classes, simplifying the task by condensing the number of potential discrepancies. Conversely, the 10-class variant offers a more intricate challenge, reflecting the diversity of discrepancies in more realistic scenarios.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">The goal of our dataset is to enable the evaluation of methods that can detect discrepancies between the visual and audio streams. To assess the effectiveness of such methods in identifying manipulated samples from the VADD dataset, we propose using the F1-score of Precision and Recall. We provide this dataset in our GitHub repository<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/IDT-ITI/Visual-Audio-Discrepancy-Detection</span></span></span>, i.e. the list of TAU’s videos that belong to the unmodifiedsamples set of the VADD dataset, and the tuples of TAU’s videos for which the audio should be swapped to create the set of manipulated videos.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Proposed Method</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We have developed a methodology for detecting discrepancies between audio and visual elements in video content. Initially, we train a robust joint audio-visual classifier specifically designed for scene classification tasks, utilizing both audio and visual modalities. This training utilizes the training portion of the development set from the TAU dataset. Subsequently, the performance of the joint classifier is evaluated on the test portion of the development set from the TAU dataset to ensure its efficacy in scene classification tasks. Following the selection of the architecture and the evaluation of the scene classifier’s effectiveness, we retrain separate classifiers for each modality. These classifiers are intended for use individually on the audio and visual streams of videos, enabling them to identify discrepancies within the audio-visual content. Finally, we assess the performance of these separate classifiers by applying them to the VADD dataset to measure their effectiveness in detecting inconsistencies between audio and visual elements.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Visual scene representations</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We follow a transfer learning approach for the analysis of the visual content of videos. Recognizing the quality of models pre-trained on large-scale datasets, we leverage their power to extract features that capture semantically meaningful information from input images. Initially, we experimented with several pre-trained models, including wide ResNet and DenseNet trained on the Places365 dataset, as well as an EfficientNetV2 trained on the ImageNet dataset. Aiming for a rich representation of visual information, we selected three diverse models (i.e., different network architectures pre-trained on different datasets), which are detailed in the following.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter">ViT embeddings</span>: We reviewed available pre-trained models and opted for the ViT_H_14 vision transformer architecture from PyTorch’s Torchvision, featuring a 14x14 input patch size. We utilized the “IMAGENET1K_SWAG_E2E_V1” weights, achieving a top-1 classification accuracy of 98.694%. We extracted activations from the penultimate layer, resulting in a 1000-dimensional embedding vector.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter">CLIP embeddings</span>: We utilized OpenAI’s CLIP architecture, which can comprehend both images and text simultaneously. Specifically, we employed the “ViT-H/14” architecture with “laion2b_s32b_b79k” model weights, achieving an accuracy of 78.0% on LAION-2B. The resulting image encoding is a 1024-dimensional vector.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter">ResNet embeddings</span>: Among the various models pre-trained on the Places365 dataset available<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/CSAILVision/places365" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/CSAILVision/places365</a></span></span></span>, we selected the ResNet50 with a top-1 error rate of 44.82%. Once again, we utilized activations from the penultimate layer, resulting in a 2048-dimensional embedding vector.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Audio scene representations</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Similar to the visual analysis, we employ a transfer learning approach for the analysis of the audio stream. Following the methodology outlined in <cite class="ltx_cite ltx_citemacro_citep">(Abeßer et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>, we explore different deep audio embeddings (DAEs) as audio feature representations, derived from DNN models pre-trained on large-scale datasets like AudioSet <cite class="ltx_cite ltx_citemacro_citep">(Gemmeke et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite>, which covers over 2 million weakly labeled audio files across 527 sound classes.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.p1.1.1" class="ltx_text ltx_font_typewriter">OpenL3 embeddings</span> <cite class="ltx_cite ltx_citemacro_citep">(Cramer et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>: Trained in a self-supervised fashion using an audio-visual correlation criterion during training, OpenL3 embeddings utilize separate audio and video networks, combined via fusion layers. The audio network, utilized for computing OpenL3 embeddings, consists of four convolutional layers with intermediate max-pooling operations. In this work, we use the ”music” configuration, converting a Mel spectrogram with 256 Mel bands into 512-dimensional embedding vectors with a feature rate of 42 Hz.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.p1.1.1" class="ltx_text ltx_font_typewriter">PANN</span>: The Pre-trained Audio Neural Network (PANN) <cite class="ltx_cite ltx_citemacro_citep">(Kong et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite> embeddings are based on a CNN architecture with 12 convolutional layers and two dense layers. As input, the “Wavegram-Logmel-CNN” model processes both raw audio samples and a Mel spectrogram with 64 bands. The model incorporates temporal aggregation using maximum and average pooling, resulting in 512-dimensional embedding vectors at a feature rate of 1 Hz.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p"><span id="S4.I2.i3.p1.1.1" class="ltx_text ltx_font_typewriter">IOV</span>: As a third audio representation, we obtain embeddings from a ResNet model, which has been trained for the task of indoor-outdoor-vehicle classification. We adopt the “CNN420” model from <cite class="ltx_cite ltx_citemacro_citep">(Grollmisch and Cano, <a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite>, which combines a convolutional block with four residual blocks. Intermediate dropout with a rate of <math id="S4.I2.i3.p1.1.m1.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S4.I2.i3.p1.1.m1.1a"><mn id="S4.I2.i3.p1.1.m1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.1.m1.1b"><cn type="float" id="S4.I2.i3.p1.1.m1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.1.m1.1c">0.1</annotation></semantics></math> and batch normalization is used in all blocks. After the residual blocks, average pooling is applied to aggregate feature maps, with the output of the pooling layer serving as embeddings.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Combining modalities</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">For aggregating the three visual embeddings and three audio embeddings to classify the scene of an input video, we utilize a neural network architecture. Our approach involves concatenating these embeddings and applying a self-attention mechanism, followed by two Fully Connected (FC) layers. To prevent overfitting and enhance generalization, we include a dropout layer between the two FC layers. This straightforward architecture is illustrated in Fig. <a href="#S4.F2" title="Figure 2 ‣ 4.3. Combining modalities ‣ 4. Proposed Method ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The architecture of the employed audio-visual scene classifier.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Attention <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>, in the context of deep learning, is a mechanism most commonly associated with transformer architectures; attention-based architectures have proven to be a powerful tool for both vision <cite class="ltx_cite ltx_citemacro_citep">(Sun and Yang, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite> and audio <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> tasks. Unlike standard attention techniques, self-attention operates within a single sequence, capturing dependencies between elements within the same sequence. It allows a model to weigh the importance of different elements within a sequence while processing that sequence. In the context of our scene classification classifier, self-attention mechanisms are leveraged to emphasize the importance of certain parts of the different input embeddings, whether they are visual features from images or audio features from sound clips.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">For the visual component, we sample the middle frame of each second in a video. Given that all videos in our training dataset are 10 seconds long, this results in 10 embeddings per video. For training purposes, we use all 10 instances of embeddings derived from the same video, all of which carry the same label. In the case of audio embeddings, we employ an averaging technique to create a single feature vector for each embedding for each second, ensuring alignment between visual and audio feature vectors. During the evaluation, we forward all 10 instances of the same video through our classifier and employ a voting scheme to infer a final classification for the video.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">To enrich the training dataset and reduce training time, we employ data augmentation techniques while separating feature extraction and classification stages. We pre-compute embeddings from pre-trained models as input features for our scene classifier, significantly reducing training time. Data augmentation involves creating a duplicate training set and applying synthetic transformations only to the second half. These transformations include horizontal flips, random brightness and contrast adjustments, and rotation.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Regarding the training procedure, we use a batch size of 32, as an effective balance between computational efficiency and convergence speed. We used the PyTorch machine learning library (version 1.13) and the Torchvision package utilized for accessing pre-trained models. We employ a standard stochastic gradient descent (SGD) optimizer for optimization. Learning rate scheduling is implemented, starting with a higher rate of 0.001 in the first epoch to swiftly capture prominent features. Subsequently, the rate is linearly reduced to 0.00001 by the 19th epoch to encourage meticulous fine-tuning. Training occurs over 20 epochs, as experiments showed that the loss plateaus before the 20th epoch. We selected the model snapshot from the 20th epoch and evaluated its accuracy on the test partition of the TAU dataset’s developmental portion. We select the model snapshot from the 20th epoch and evaluate its accuracy on the test partition of the TAU dataset’s developmental portion. All experiments are performed on a PC equipped with an Nvidia GeForce GTX 1080 Ti GPU.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Comparison with SoA on scene classification</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We first evaluate our (joint visual-audio) scene classifier on the test portion of the development set of the TAU dataset to compare it with <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2021a</a>)</cite>, which achieved the highest performance in the DCASE 2021 audio-visual scene classification task. Our method achieves 97.24% accuracy compared to the best score of 95.1% reported in <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2021a</a>)</cite>, therefore it is clear that our method is superior, which is due to the use of modern features and, as shown in the ablation study sub-section, below, the well-chosen placing of self-attention mechanisms in our classifier architecture.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Visual-audio discrepancies detection</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">After confirming the effectiveness of our multimodal model in scene classification using the TAU dataset, we proceed to apply separate visual and audio classifiers to detect discrepancies in the VADD dataset. We train distinct classifiers for each modality and use them to detect the manipulated videos of the VADD dataset. The evaluation of the separate classifiers on the VADD dataset, both for the 3-class and 10-class variants, is included in Table <a href="#S5.T2" title="Table 2 ‣ 5.2. Visual-audio discrepancies detection ‣ 5. Experiments ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We observe that the audio scene and visual scene classifiers achieve near-perfect performance in the 3-class variant of VADD dataset. This high accuracy suggests that our classifiers can effectively discern the intended scenes within the multimedia content. Consequently, when our model detects a discrepancy between the audio and visual scenes, it indicates a high likelihood of actual inconsistencies in the analyzed media item, minimizing the risk of false positives.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Evaluation of the proposed visual-audio (VASC), visual (VSC), and audio (ASC) scene classifiers on the TAU dataset, for the 3-class and 10-class variants.</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:50.0pt;">
<span id="S5.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.1.1.1.1" class="ltx_p">Approach</span>
</span>
</td>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:70.0pt;">
<span id="S5.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.1.2.1.1" class="ltx_p">Accuracy (%) on TAU using the 3-class variant</span>
</span>
</td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:70.0pt;">
<span id="S5.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.1.3.1.1" class="ltx_p">Accuracy (%) on TAU using the 10-class variant</span>
</span>
</td>
</tr>
<tr id="S5.T2.1.2.2" class="ltx_tr">
<td id="S5.T2.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:50.0pt;">
<span id="S5.T2.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.2.2.1.1.1" class="ltx_p">VASC</span>
</span>
</td>
<td id="S5.T2.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:70.0pt;">
<span id="S5.T2.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.2.2.2.1.1" class="ltx_p">99.95</span>
</span>
</td>
<td id="S5.T2.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:70.0pt;">
<span id="S5.T2.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.2.2.3.1.1" class="ltx_p">97.24</span>
</span>
</td>
</tr>
<tr id="S5.T2.1.3.3" class="ltx_tr">
<td id="S5.T2.1.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:50.0pt;">
<span id="S5.T2.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.3.3.1.1.1" class="ltx_p">ASC</span>
</span>
</td>
<td id="S5.T2.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:70.0pt;">
<span id="S5.T2.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.3.3.2.1.1" class="ltx_p">99.84</span>
</span>
</td>
<td id="S5.T2.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:70.0pt;">
<span id="S5.T2.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.3.3.3.1.1" class="ltx_p">78.84</span>
</span>
</td>
</tr>
<tr id="S5.T2.1.4.4" class="ltx_tr">
<td id="S5.T2.1.4.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:50.0pt;">
<span id="S5.T2.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.4.4.1.1.1" class="ltx_p">VSC</span>
</span>
</td>
<td id="S5.T2.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:70.0pt;">
<span id="S5.T2.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.4.4.2.1.1" class="ltx_p">99.93</span>
</span>
</td>
<td id="S5.T2.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:70.0pt;">
<span id="S5.T2.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.4.4.3.1.1" class="ltx_p">94.32</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2405.00384/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="503" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Confusion Matrix for our visual-audio scene classifier on the 10-class variant of the VADD dataset.</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">We evaluate the effectiveness of the proposed baseline method for detecting visual-audio discrepancies on the VADD dataset, with the findings being reported in Table <a href="#S5.T3" title="Table 3 ‣ 5.2. Visual-audio discrepancies detection ‣ 5. Experiments ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Our assessment reveals notable differences between the 3-class and 10-class variants. The 3-class variant demonstrates higher accuracy, achieving an F1-score of 95.54%. This outcome is expected due to the reduced class complexity. However, it’s crucial to include the 10-class variant for a more realistic and challenging evaluation, reflecting the complexities of real-world multimedia content. When applied to the 10-class variant, the baseline method achieves a lower F1-score of 79.16%. Figure <a href="#S5.F3" title="Figure 3 ‣ 5.2. Visual-audio discrepancies detection ‣ 5. Experiments ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> displays the confusion matrix for our visual-audio scene classifier on the 10-class variant of the VADD dataset, where the x-axis represents the predicted labels and the y-axis represents the true labels. A brief analysis of the results using confusion matrices showed that our classifier exhibited confusion between “tram” and “bus”, “public square” and “street pedestrian”, as well as “airports” and “metro station” class tuples, contributing to the lower performance of our method in the 10-class variant of the problem. Researchers can leverage both variants to evaluate their detection methods across different complexity levels.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Comparison of the proposed baseline method on the VADD dataset.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:80.0pt;">
<span id="S5.T3.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.1.1.1.1.1" class="ltx_p">VADD dataset variant used</span>
</span>
</th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:60.0pt;">
<span id="S5.T3.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.1.1.2.1.1" class="ltx_p">F1-score (%) of the proposed method</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<td id="S5.T3.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:80.0pt;">
<span id="S5.T3.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.2.1.1.1.1" class="ltx_p">3-class VADD</span>
</span>
</td>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:60.0pt;">
<span id="S5.T3.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.2.1.2.1.1" class="ltx_p">95.54</span>
</span>
</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<td id="S5.T3.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:80.0pt;">
<span id="S5.T3.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.3.2.1.1.1" class="ltx_p">10-class VADD</span>
</span>
</td>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:60.0pt;">
<span id="S5.T3.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.1.3.2.2.1.1" class="ltx_p">79.16</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Ablation study</h3>

<figure id="S5.F4" class="ltx_figure"><img src="" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>The architecture of the early self-attention (ES) variant.</figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure"><img src="" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>The architecture of the per-modality self-attention (MS) variant.</figcaption>
</figure>
<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In this section, we conduct a brief ablation study to provide insights into the effectiveness of the core elements of our proposed model for scene classification. Our initial model combines six embeddings (three visual and three audio) to predict scene categories. We explore two primary scenarios to assess the impact of model configurations and design choices:</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">Scenario #1 examines different variants of Self-Attention layer placement, including Late Self-Attention (LS), where self-attention is applied after concatenating all input embeddings (Fig. <a href="#S4.F2" title="Figure 2 ‣ 4.3. Combining modalities ‣ 4. Proposed Method ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>); Early Self-Attention (ES), which applies self-attention directly to individual visual and audio embeddings before concatenation (Fig. <a href="#S5.F4" title="Figure 4 ‣ 5.3. Ablation study ‣ 5. Experiments ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>); Per-modality Self-Attention (MS), which applies self-attention to concatenated visual and audio embeddings (Fig. <a href="#S5.F5" title="Figure 5 ‣ 5.3. Ablation study ‣ 5. Experiments ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>); Combined Self-Attention, which encompasses various combinations of ES, MS, and LS approaches; and finally, not using self-attention at all (NS).</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">Scenario #2 investigates Data Augmentation (DA) techniques and their impact on scene classification performance through experiments both with and without augmentation.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">Scenario #3 - Single vs. Double FC Layers: we aim to determine whether the additional FC layer contributes significantly to the model’s predictive power.</p>
</div>
</li>
</ol>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">While we present only the ablation study results using the 3-class variant of the VADD dataset, it’s important to note that the findings remain applicable to the 10-class variant as well.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Comparison of different design options for the proposed classifier</figcaption>
<table id="S5.T4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:50.0pt;"></th>
<th id="S5.T4.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:80.0pt;">
<span id="S5.T4.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.2.1.1" class="ltx_p">Approach</span>
</span>
</th>
<th id="S5.T4.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:70.0pt;">
<span id="S5.T4.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.3.1.1" class="ltx_p">Accuracy (%) on TAU dataset</span>
</span>
</th>
</tr>
<tr id="S5.T4.1.2.2" class="ltx_tr">
<th id="S5.T4.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:50.0pt;"></th>
<th id="S5.T4.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:80.0pt;">
<span id="S5.T4.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.2.2.2.1.1" class="ltx_p">Proposed (LS + DA + + Double FC)</span>
</span>
</th>
<th id="S5.T4.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:70.0pt;">
<span id="S5.T4.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.2.2.3.1.1" class="ltx_p"><span id="S5.T4.1.2.2.3.1.1.1" class="ltx_text ltx_font_bold">97.24</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.3.1" class="ltx_tr">
<td id="S5.T4.1.3.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:50.0pt;">
<span id="S5.T4.1.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.3.1.1.1.1" class="ltx_p">
<span id="S5.T4.1.3.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T4.1.3.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T4.1.3.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Scenario #1:</span></span>
<span id="S5.T4.1.3.1.1.1.1.1.2" class="ltx_tr">
<span id="S5.T4.1.3.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Self-attention</span></span>
<span id="S5.T4.1.3.1.1.1.1.1.3" class="ltx_tr">
<span id="S5.T4.1.3.1.1.1.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">variants</span></span>
<span id="S5.T4.1.3.1.1.1.1.1.4" class="ltx_tr">
<span id="S5.T4.1.3.1.1.1.1.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">(all using DA)</span></span>
</span></span>
</span>
</td>
<td id="S5.T4.1.3.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:80.0pt;">
<span id="S5.T4.1.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.3.1.2.1.1" class="ltx_p">ES</span>
</span>
</td>
<td id="S5.T4.1.3.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:70.0pt;">
<span id="S5.T4.1.3.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.3.1.3.1.1" class="ltx_p">91.73</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.4.2" class="ltx_tr">
<td id="S5.T4.1.4.2.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:50.0pt;"></td>
<td id="S5.T4.1.4.2.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S5.T4.1.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.4.2.2.1.1" class="ltx_p">MS</span>
</span>
</td>
<td id="S5.T4.1.4.2.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:70.0pt;">
<span id="S5.T4.1.4.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.4.2.3.1.1" class="ltx_p">96.98</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.5.3" class="ltx_tr">
<td id="S5.T4.1.5.3.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:50.0pt;"></td>
<td id="S5.T4.1.5.3.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S5.T4.1.5.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.5.3.2.1.1" class="ltx_p">NS</span>
</span>
</td>
<td id="S5.T4.1.5.3.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:70.0pt;">
<span id="S5.T4.1.5.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.5.3.3.1.1" class="ltx_p">94.16</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.6.4" class="ltx_tr">
<td id="S5.T4.1.6.4.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:50.0pt;"></td>
<td id="S5.T4.1.6.4.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S5.T4.1.6.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.6.4.2.1.1" class="ltx_p">ES + LS</span>
</span>
</td>
<td id="S5.T4.1.6.4.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:70.0pt;">
<span id="S5.T4.1.6.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.6.4.3.1.1" class="ltx_p">93.75</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.7.5" class="ltx_tr">
<td id="S5.T4.1.7.5.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:50.0pt;"></td>
<td id="S5.T4.1.7.5.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S5.T4.1.7.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.7.5.2.1.1" class="ltx_p">MS + LS</span>
</span>
</td>
<td id="S5.T4.1.7.5.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:70.0pt;">
<span id="S5.T4.1.7.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.7.5.3.1.1" class="ltx_p">97.02</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.8.6" class="ltx_tr">
<td id="S5.T4.1.8.6.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:50.0pt;"></td>
<td id="S5.T4.1.8.6.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S5.T4.1.8.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.8.6.2.1.1" class="ltx_p">ES + MS</span>
</span>
</td>
<td id="S5.T4.1.8.6.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:70.0pt;">
<span id="S5.T4.1.8.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.8.6.3.1.1" class="ltx_p">92.65</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.9.7" class="ltx_tr">
<td id="S5.T4.1.9.7.1" class="ltx_td ltx_align_justify ltx_align_middle" style="width:50.0pt;"></td>
<td id="S5.T4.1.9.7.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:80.0pt;">
<span id="S5.T4.1.9.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.9.7.2.1.1" class="ltx_p">ES + MS + LS</span>
</span>
</td>
<td id="S5.T4.1.9.7.3" class="ltx_td ltx_align_justify ltx_align_middle" style="width:70.0pt;">
<span id="S5.T4.1.9.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.9.7.3.1.1" class="ltx_p">94.05</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.10.8" class="ltx_tr">
<td id="S5.T4.1.10.8.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:50.0pt;">
<span id="S5.T4.1.10.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.10.8.1.1.1" class="ltx_p">Scenario #2</span>
</span>
</td>
<td id="S5.T4.1.10.8.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:80.0pt;">
<span id="S5.T4.1.10.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.10.8.2.1.1" class="ltx_p">Proposed without DA</span>
</span>
</td>
<td id="S5.T4.1.10.8.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:70.0pt;">
<span id="S5.T4.1.10.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.10.8.3.1.1" class="ltx_p">96.98</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.11.9" class="ltx_tr">
<td id="S5.T4.1.11.9.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" style="width:50.0pt;">
<span id="S5.T4.1.11.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.11.9.1.1.1" class="ltx_p">Scenario #3</span>
</span>
</td>
<td id="S5.T4.1.11.9.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" style="width:80.0pt;">
<span id="S5.T4.1.11.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.11.9.2.1.1" class="ltx_p">Single FC layer</span>
</span>
</td>
<td id="S5.T4.1.11.9.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" style="width:70.0pt;">
<span id="S5.T4.1.11.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.11.9.3.1.1" class="ltx_p">97.18</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.2" class="ltx_p">The outcome of the ablation study is reported in Table <a href="#S5.T4" title="Table 4 ‣ 5.3. Ablation study ‣ 5. Experiments ‣ Visual and audio scene classification for detecting discrepancies in video: a baseline method and experimental protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Regarding the position of a self-attention mechanism in the model, its placement can significantly influence the model’s ability to capture relevant information and dependencies within the input data. Placing self-attention early in the model allows the network to attend to fine-grained features at lower levels of abstraction, potentially enhancing feature learning. On the other hand, positioning self-attention later in the model enables the network to integrate contextual information and global dependencies, facilitating better aggregation of information from multiple input sources. In our case, positioning the self-attention late in the network works best for the embedding aggregation scheme (LS <math id="S5.SS3.p4.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.SS3.p4.1.m1.1a"><mo id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><gt id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">&gt;</annotation></semantics></math> [ES, MS]). Furthermore, overloading the network with more than one self-attention layer seems to “short-circuit” the model, leading to significantly reduced accuracy (LS, ES, MS <math id="S5.SS3.p4.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.SS3.p4.2.m2.1a"><mo id="S5.SS3.p4.2.m2.1.1" xref="S5.SS3.p4.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m2.1b"><gt id="S5.SS3.p4.2.m2.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m2.1c">&gt;</annotation></semantics></math> [ES+LS, MS+LS, ES+MS, ES+MS+LS]). Data augmentation schemes benefit training by increasing the diversity and richness of the training data, thereby improving the model’s ability to generalize to unseen examples. Employing a data augmentation scheme increases accuracy on the specific task. Finally, the performance improvement observed when using two fully connected (FC) layers instead of one can be attributed to the increased capacity and expressiveness of the deeper network architecture. Our experiments across all scenarios consistently demonstrate that the chosen model architecture, data augmentation strategies, and the number of FC layers used, are well-suited for the task at hand.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we introduce a baseline method that utilizes multi-modal scene classification techniques for content verification, specifically focusing on identifying inconsistencies between audio and visual elements in videos. Our approach involves developing and evaluating a robust joint audio-visual classifier using the existing TAU dataset, demonstrating its effectiveness in scene classification. We extend this classifier to the context of content verification, providing a valuable tool for assessing media integrity. Additionally, we introduce a benchmark dataset to facilitate further research in this area. The evaluation of our separate classifiers on the newly introduced dataset reveals promising results in the 3-class variant, while also highlighting current limitations in the 10-class variant of the proposed experimental protocol. In conclusion, our research aims to advance the detection of audio-visual discrepancies, offering valuable resources and insights for future studies. We intend to look into different feature fusion strategies and contrastive learning approaches as well as incorporating temporal information so that we capture discrepancies over time, as our next steps.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work was supported by the EU Horizon Europe programme under grant agreement 101070093 vera.ai.

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abeßer (2020)</span>
<span class="ltx_bibblock">
Jakob Abeßer. 2020.

</span>
<span class="ltx_bibblock">A Review of Deep Learning Based Methods for Acoustic Scene Classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em> 10, 6 (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abeßer et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jakob Abeßer, Sascha Grollmisch, and Meinard Müller. 2023.

</span>
<span class="ltx_bibblock">How Robust are Audio Embeddings for Polyphonic Sound Event Tagging?

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> 31 (2023), 2658–2667.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bolles et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Robert C Bolles, J Brian Burns, Martin Graciarena, Andreas Kathol, Aaron Lawson, Mitchell McLaren, Thomas Mensink, et al<span id="bib.bib4.3.1" class="ltx_text">.</span> 2017.

</span>
<span class="ltx_bibblock">Spotting Audio-Visual Inconsistencies (SAVI) in Manipulated Video.. In <em id="bib.bib4.4.1" class="ltx_emph ltx_font_italic">CVPR Workshops</em>. 1907–1914.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cramer et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jason Cramer, Ho-Hsiang Wu, Justin Salamon, and Juan Pablo Bello. 2019.

</span>
<span class="ltx_bibblock">Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. Brighton, UK, 3852–3856.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.

</span>
<span class="ltx_bibblock">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning Representations (ICLR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Drossos et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Konstantinos Drossos, Paul Magron, and Tuomas Virtanen. 2019.

</span>
<span class="ltx_bibblock">Unsupervised Adversarial Domain Adaptation Based on the Wasserstein Distance for Acoustic Scene Classification. In <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</em>. IEEE, 259–263.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/WASPAA.2019.8937231" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/WASPAA.2019.8937231</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemmeke et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. 2017.

</span>
<span class="ltx_bibblock">Audio Set: An Ontology and Human-Labeled Dataset for Audio Events. In <em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. New Orleans, LA, USA, 776–780.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grollmisch and Cano (2021)</span>
<span class="ltx_bibblock">
Sascha Grollmisch and Estefanía Cano. 2021.

</span>
<span class="ltx_bibblock">Improving Semi-Supervised Learning for Audio Classification with FixMatch.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Electronics</em> 10, 15 (2021).

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep Residual Learning for Image Recognition. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ying Hu, Shijing Hou, Huamin Yang, Hao Huang, and Liang He. 2023.

</span>
<span class="ltx_bibblock">A Joint Network Based on Interactive Attention for Speech Emotion Recognition. In <em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">2023 IEEE International Conference on Multimedia and Expo (ICME)</em>. IEEE, 1715–1720.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. 2017.

</span>
<span class="ltx_bibblock">Densely Connected Convolutional Networks. In <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson and Grollmisch (2021)</span>
<span class="ltx_bibblock">
David Johnson and Sascha Grollmisch. 2021.

</span>
<span class="ltx_bibblock">Techniques Improving the Robustness of Deep Learning Models for Industrial Sound Analysis. In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th European Signal Processing Conference (EUSIPCO)</em>. Online, 81–85.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.23919/Eusipco47968.2020.9287327" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.23919/Eusipco47968.2020.9287327</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Byeonggeun Kim, Seunghan Yang, Jangho Kim, and Simyung Chang. 2021.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">QTI Submission to DCASE 2021: Residual Normalization for Device-Imbalanced Acoustic Scene Classification with Efficient Design</em>.

</span>
<span class="ltx_bibblock">Technical Report. DCASE2020 Challenge.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kong et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D. Plumbley. 2021.

</span>
<span class="ltx_bibblock">PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> 28 (2021), 2880–2894.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mezza et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Alessandro Ilic Mezza, Emanuel A. P. Habets, Meinard Müller, and Augusto Sarti. 2021.

</span>
<span class="ltx_bibblock">Unsupervised Domain Adaptation for Acoustic Scene Classification Using Band-Wise Statistics Matching. In <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Procedings of the 28th European Signal Processing Conference (EUSIPCO)</em>. Amsterdam, The Netherlands, 11–15.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.23919/Eusipco47968.2020.9287533" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.23919/Eusipco47968.2020.9287533</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al<span id="bib.bib17.3.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">Learning Transferable Visual Models From Natural Language Supervision. In <em id="bib.bib17.4.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>. PMLR, 8748–8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun and Yang (2023)</span>
<span class="ltx_bibblock">
Guoying Sun and Meng Yang. 2023.

</span>
<span class="ltx_bibblock">Self-Attention Prediction Correction with Channel Suppression for Weakly-Supervised Semantic Segmentation. In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2023 IEEE International Conference on Multimedia and Expo (ICME)</em>. IEEE, 846–851.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Le (2019)</span>
<span class="ltx_bibblock">
Mingxing Tan and Quoc Le. 2019.

</span>
<span class="ltx_bibblock">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>. PMLR, 6105–6114.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teyssou et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Denis Teyssou, Jean-Michel Leung, Evlampios Apostolidis, Konstantinos Apostolidis, Symeon Papadopoulos, Markos Zampoglou, Olga Papadopoulou, and Vasileios Mezaris. 2017.

</span>
<span class="ltx_bibblock">The InVID Plug-in: Web Video Verification on the Browser. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st International Workshop on Multimedia Verification</em>. 23–30.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention Is All You Need.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Virtanen et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Tuomas Virtanen, Mark D. Plumbley, and Dan (Eds.) Ellis. 2018.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Computational Analysis of Sound Scenes and Events</em> (1st ed.).

</span>
<span class="ltx_bibblock">Springer International Publishing.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. 2023.

</span>
<span class="ltx_bibblock">Exploring clip for assessing the look and feel of images. In <em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 37. 2555–2563.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Meng Wang, Chengxin Chen, Yuan Xie, Hangting Chen, Yuzhuo Liu, and Pengyuan Zhang. 2021a.

</span>
<span class="ltx_bibblock">Audio-Visual Scene Classification Using Transfer Learning and Hybrid Fusion Strategy.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">DCASE2021 Challenge, Tech. Rep, Tech. Rep.</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Shanshan Wang, Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen. 2021b.

</span>
<span class="ltx_bibblock">A Curated Dataset of Urban Scenes for Audio-Visual Scene Analysis. In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 626–630.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.00383" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.00384" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.00384">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.00384" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.00385" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 14:09:35 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
