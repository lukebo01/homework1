<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.11252] WER We Stand: Benchmarking Urdu ASR Models</title><meta property="og:description" content="This paper presents a comprehensive evaluation of Urdu Automatic Speech Recognition (ASR) models. We analyze the performance of three ASR model families: Whisper, MMS, and Seamless-M4T using Word Error Rate (WER), alon…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="WER We Stand: Benchmarking Urdu ASR Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="WER We Stand: Benchmarking Urdu ASR Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.11252">

<!--Generated on Sat Oct  5 19:19:06 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">WER We Stand: Benchmarking Urdu ASR Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id1.1.id1" class="ltx_text ltx_font_bold">Samee Arif<sup id="id1.1.id1.1" class="ltx_sup">1</sup></span>,
<span id="id2.2.id2" class="ltx_text ltx_font_bold">Aamina Jamal Khan<sup id="id2.2.id2.1" class="ltx_sup">1</sup><sup id="id2.2.id2.2" class="ltx_sup">*</sup></span>,
<span id="id3.3.id3" class="ltx_text ltx_font_bold">Mustafa Abbas<sup id="id3.3.id3.1" class="ltx_sup">1</sup><sup id="id3.3.id3.2" class="ltx_sup">*</sup></span>,

<br class="ltx_break"><span id="id4.4.id4" class="ltx_text ltx_font_bold">Agha Ali Raza<sup id="id4.4.id4.1" class="ltx_sup">1</sup></span>,
<span id="id5.5.id5" class="ltx_text ltx_font_bold">Awais Athar<sup id="id5.5.id5.1" class="ltx_sup">2</sup></span>,

<br class="ltx_break">{samee.arif, 25100162, 25100079, agha.ali.raza}@lums.edu.pk,
awais@ebi.ac.uk

<br class="ltx_break"><sup id="id6.6.id6" class="ltx_sup">1</sup>Lahore University of Management Sciences,

<br class="ltx_break"><sup id="id7.7.id7" class="ltx_sup">2</sup>EMBL European Bioinformatics Institute
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">This paper presents a comprehensive evaluation of Urdu Automatic Speech Recognition (ASR) models. We analyze the performance of three ASR model families: <span id="id8.id1.1" class="ltx_text ltx_font_typewriter">Whisper</span>, <span id="id8.id1.2" class="ltx_text ltx_font_typewriter">MMS</span>, and <span id="id8.id1.3" class="ltx_text ltx_font_typewriter">Seamless-M4T</span> using Word Error Rate (WER), along with a detailed examination of the most frequent wrong words and error types including insertions, deletions, and substitutions. Our analysis is conducted using two types of datasets, read speech and conversational speech. Notably, we present the first conversational speech dataset designed for benchmarking Urdu ASR models. We find that <span id="id8.id1.4" class="ltx_text ltx_font_typewriter">seamless-large</span> outperforms other ASR models on the read speech dataset, while <span id="id8.id1.5" class="ltx_text ltx_font_typewriter">whisper-large</span> performs best on the conversational speech dataset. Furthermore, this evaluation highlights the complexities of assessing ASR models for low-resource languages like Urdu using quantitative metrics alone and emphasizes the need for a robust Urdu text normalization system. Our findings contribute valuable insights for developing robust ASR systems for low-resource languages like Urdu.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_bold">WER We Stand: Benchmarking Urdu ASR Models</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.2.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.2.1.1.1.1" class="ltx_tr">
<span id="p1.1.2.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">
Samee Arif<sup id="p1.1.2.1.1.1.1.1.1.1" class="ltx_sup">1</sup>,
Aamina Jamal Khan<sup id="p1.1.2.1.1.1.1.1.1.2" class="ltx_sup">1</sup><sup id="p1.1.2.1.1.1.1.1.1.3" class="ltx_sup">*</sup>,
Mustafa Abbas<sup id="p1.1.2.1.1.1.1.1.1.4" class="ltx_sup">1</sup><sup id="p1.1.2.1.1.1.1.1.1.5" class="ltx_sup">*</sup>,</span></span></span>
<span id="p1.1.2.1.1.2.2" class="ltx_tr">
<span id="p1.1.2.1.1.2.2.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Agha Ali Raza<sup id="p1.1.2.1.1.2.2.1.1.1" class="ltx_sup">1</sup></span>,
<span id="p1.1.2.1.1.2.2.1.2" class="ltx_text ltx_font_bold">Awais Athar<sup id="p1.1.2.1.1.2.2.1.2.1" class="ltx_sup">2</sup></span>,</span></span>
<span id="p1.1.2.1.1.3.3" class="ltx_tr">
<span id="p1.1.2.1.1.3.3.1" class="ltx_td ltx_align_center">{samee.arif, 25100162, 25100079, agha.ali.raza}@lums.edu.pk,
awais@ebi.ac.uk</span></span>
<span id="p1.1.2.1.1.4.4" class="ltx_tr">
<span id="p1.1.2.1.1.4.4.1" class="ltx_td ltx_align_center"><sup id="p1.1.2.1.1.4.4.1.1" class="ltx_sup">1</sup>Lahore University of Management Sciences,</span></span>
<span id="p1.1.2.1.1.5.5" class="ltx_tr">
<span id="p1.1.2.1.1.5.5.1" class="ltx_td ltx_align_center"><sup id="p1.1.2.1.1.5.5.1.1" class="ltx_sup">2</sup>EMBL European Bioinformatics Institute</span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>These authors contributed equally to this work.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Automatic Speech Recognition (ASR) systems have become integral to modern technology, enabling numerous applications. Use cases include virtual assistants <cite class="ltx_cite ltx_citemacro_citep">(Adline Freeda et al., <a href="#bib.bib2" title="" class="ltx_ref">2024</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Subhash et al., <a href="#bib.bib23" title="" class="ltx_ref">2020</a>)</cite>, smart homes <cite class="ltx_cite ltx_citemacro_citep">(Chen and Zhang, <a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Caranica et al., <a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>, medical assistance <cite class="ltx_cite ltx_citemacro_citep">(Maier et al., <a href="#bib.bib16" title="" class="ltx_ref">2010</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a href="#bib.bib13" title="" class="ltx_ref">2014</a>)</cite>, telecommunications <cite class="ltx_cite ltx_citemacro_citep">(Rabiner, <a href="#bib.bib20" title="" class="ltx_ref">1997</a>)</cite>, and more. The ability to convert spoken language into text has revolutionized human-computer interaction, making technology more accessible and user-friendly.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">ASR systems have made substantial progress in recent years, driven by advances in deep learning and the availability of large-scale datasets. However, the majority of this progress has been concentrated on resource-rich languages, leaving low-resource languages like Urdu with significant gaps in accuracy and reliability. Urdu, with over 70 million native speakers, is characterized by its rich phonetic diversity, complex morphological structure, and variety of regional dialects, all of which pose additional challenges for ASR systems. These challenges are further amplified in conversational contexts by informal speech patterns, code-switching <cite class="ltx_cite ltx_citemacro_citep">(Khan et al., <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>, and spontaneous speech disfluencies. The availability of annotated datasets is also limited compared to high-resource languages, which hinders the training and evaluation of ASR models.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Addressing these challenges is crucial for making ASR technology accessible and effective for Urdu speakers, particularly in scenarios involving both read and conversational speech. This paper focuses on evaluating and improving the performance of state-of-the-art ASR models and post-processing techniques for Urdu. Specifically, we examine three prominent ASR model families—<span id="S1.p3.1.1" class="ltx_text ltx_font_typewriter">Whisper</span> <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>, <span id="S1.p3.1.2" class="ltx_text ltx_font_typewriter">MMS</span> <cite class="ltx_cite ltx_citemacro_citep">(Pratap et al., <a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>, and <span id="S1.p3.1.3" class="ltx_text ltx_font_typewriter">Seamless-M4T</span> <cite class="ltx_cite ltx_citemacro_citep">(Communication, <a href="#bib.bib9" title="" class="ltx_ref">2023a</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Communication, <a href="#bib.bib10" title="" class="ltx_ref">2023b</a>)</cite>—each of which has demonstrated strong performance across various languages.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we introduce the first conversational speech dataset specifically designed for evaluating Urdu ASR models. We release all our fine-tuned models, datasets, evaluation scripts, and outputs to the community to foster further research in Urdu ASR tasks. By making these resources publicly available, we aim to bridge the gap in ASR technology for low-resource languages like Urdu, thereby contributing to more inclusive and effective speech recognition solutions.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our contributions can be summarized as follows:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We present the first conversational speech dataset for benchmarking Urdu ASR models.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We fine-tune ASR models from the <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter">Whisper</span>, <span id="S1.I1.i2.p1.1.2" class="ltx_text ltx_font_typewriter">MMS</span>, and <span id="S1.I1.i2.p1.1.3" class="ltx_text ltx_font_typewriter">Seamless-M4T</span> families on Urdu, using both read speech and conversational speech datasets.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We conduct a comprehensive quantitative and qualitative analysis of non-fine-tuned and fine-tuned ASR models. We also highlight the complexity of evaluating Urdu ASR models using quantitative metrics alone, underscore the need for a robust text normalization system to address variations in word forms and improve overall model accuracy.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The code, and model outputs are publicly available on GitHub<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/ulrs0/Urdu-ASR-Today" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ulrs0/Urdu-ASR-Today</a></span></span></span>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Modern ASR Models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Modern ASR models, include <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">Wav2Vec2</span> <cite class="ltx_cite ltx_citemacro_citep">(Baevski et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>, <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">Whisper</span>, <span id="S2.SS1.p1.1.3" class="ltx_text ltx_font_typewriter">MMS</span> and <span id="S2.SS1.p1.1.4" class="ltx_text ltx_font_typewriter">Seamless-M4T</span>. <span id="S2.SS1.p1.1.5" class="ltx_text ltx_font_typewriter">Wav2Vec2</span>, introduced by Facebook AI, revolutionizes ASR by learning powerful speech representations from raw audio, enabling it to perform exceptionally well even with limited labeled data. <span id="S2.SS1.p1.1.6" class="ltx_text ltx_font_typewriter">Whisper</span> by OpenAI, leverages large-scale datasets and transformers to achieve state-of-the-art accuracy across multiple languages. Similarly, Meta’s <span id="S2.SS1.p1.1.7" class="ltx_text ltx_font_typewriter">MMS</span> and <span id="S2.SS1.p1.1.8" class="ltx_text ltx_font_typewriter">Seamless-M4T</span> models push the boundaries of multilingual and cross-modal ASR by incorporating vast amounts of diverse linguistic data. These advancements have significantly improved the ability of ASR systems to handle continuous, natural speech across a wide variety of languages and contexts.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Low-Resource ASR</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The ASRoIL survey <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite> provides valuable insights for Indian languages into the challenges of variability in speech signals and the availability of corpora, detailing feature extraction techniques like MFCC, LPCC, and PLP, as well as various modeling and classification techniques. <cite class="ltx_cite ltx_citemacro_citep">Unnibhavi and Jangamshetti, <a href="#bib.bib26" title="" class="ltx_ref">2016</a></cite>’s work offers an overview of ASR systems for South Indian languages, explaining methodologies and feature extraction methods by giving digests of seven papers. <cite class="ltx_cite ltx_citemacro_citep">Dhouib et al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a></cite>’s work systematically reviewed Arabic ASR studies from 2011 to 2021, covering feature extraction and classification techniques along with various aspects, such as the types of Arabic language supported, performance metrics, and existing research gaps. <cite class="ltx_cite ltx_citemacro_citep">Abdelhamid et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a></cite> focus on end-to-end deep learning frameworks, which integrate and simultaneously train the language model, pronunciation, and acoustic components, thus simplifying the pipeline. <cite class="ltx_cite ltx_citemacro_citep">Kadyan et al., <a href="#bib.bib14" title="" class="ltx_ref">2019</a></cite> presents a comparative study of Deep Neural Network (DNN) based Punjabi-ASR systems demonstrating that DNN-HMM hybrid models outperform traditional GMM-HMM architectures.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Low-resource languages have demonstrated significant advancements in multilingual settings, where models benefit from unexpected but advantageous learning capabilities. <cite class="ltx_cite ltx_citemacro_citep">Toshniwal et al., <a href="#bib.bib24" title="" class="ltx_ref">2018</a></cite> found that multilingual models exhibit cross-lingual transfer learning, which enhances the accuracy of low-resource languages by leveraging similarities and commonalities from high-resource languages. <cite class="ltx_cite ltx_citemacro_citep">Tüske et al., <a href="#bib.bib25" title="" class="ltx_ref">2013</a></cite> further analyzed cross-lingual transfer effects, finding that multilingual models fine-tuned on low-resource languages outperformed monolingual models, demonstrating the effectiveness of cross-lingual transfer. Overall, their work shows that low-resource languages like Urdu are more sensitive to model architecture due to data scarcity, and multilingual models can be trained effectively for both low-resource multilingual and monolingual tasks.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Urdu ASR</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">To the best of our knowledge, there is no up-to-date survey on the state of ASR in Urdu. <cite class="ltx_cite ltx_citemacro_citep">Ashraf et al., <a href="#bib.bib5" title="" class="ltx_ref">2010</a></cite> developed a small-sized GMM using Word List Grammar by processing individual words and combining it with a Knowledge Base storing audio representations, pronunciations, and probabilities, achieving a Word Error Rate (WER) of 5.33 on seen speakers and 10.66 on unseen speakers. <cite class="ltx_cite ltx_citemacro_citep">Asadullah et al., <a href="#bib.bib4" title="" class="ltx_ref">2016</a></cite> explored the impact of vocabulary size on HMM performance for Urdu ASR, finding an insignificant WER increase of 0.5% between 100 and 250-word vocabularies. <cite class="ltx_cite ltx_citemacro_citep">Naeem et al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a></cite> used a GMM combined with a graphene-to-phone LSTM CMUsphinx and KENLM to calculate and store n-gram probabilities, achieving a 9.64% WER, significantly improving accuracy. <cite class="ltx_cite ltx_citemacro_citep">Khan et al., <a href="#bib.bib15" title="" class="ltx_ref">2023</a></cite> combined HMM and CNN-TDNN with LF-MMI, achieving a 13% WER.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Models and Datasets</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>ASR and Post-Processing Models</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this paper we evaluate 9 ASR models given in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 ASR and Post-Processing Models ‣ 3 Models and Datasets ‣ WER We Stand: Benchmarking Urdu ASR Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Family</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Whisper</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">Tiny, Base, Small, Medium,</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td" style="padding-bottom:5.0pt;"></td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_left" style="padding-bottom:5.0pt;">Large-v3</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left" style="padding-bottom:5.0pt;">MMS</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_left" style="padding-bottom:5.0pt;">300 Million, 1 Billion</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_bb">Seamless-M4T</td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_bb">Medium, v2-Large</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>ASR model evaluated in this paper.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>ASR Datasets</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We evaluate the ASR models on two types of datasets: (1) Read Speech, and (2) Conversational Speech. We developed and present the conversational speech dataset in this paper.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Read Speech</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">We use the ARL Urdu Speech Database<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://catalog.ldc.upenn.edu/LDC2007S03" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ldc.upenn.edu/LDC2007S03</a></span></span></span> which has 159,996 audios. The distribution of speaker dialects in the corpus is given in Table <a href="#S3.T2" title="Table 2 ‣ 3.2.1 Read Speech ‣ 3.2 ASR Datasets ‣ 3 Models and Datasets ‣ WER We Stand: Benchmarking Urdu ASR Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Accent</span>
</th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Number of Speakers</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<th id="S3.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>South Sindh</th>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<th id="S3.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>North Sindh</th>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<th id="S3.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>South Punjab</th>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27</td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<th id="S3.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>North Punjab</th>
<td id="S3.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29</td>
</tr>
<tr id="S3.T2.1.6.5" class="ltx_tr">
<th id="S3.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>Capital Area</th>
<td id="S3.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29</td>
</tr>
<tr id="S3.T2.1.7.6" class="ltx_tr">
<th id="S3.T2.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>North West Regions</th>
<td id="S3.T2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30</td>
</tr>
<tr id="S3.T2.1.8.7" class="ltx_tr">
<th id="S3.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>Baluchistan</th>
<td id="S3.T2.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">26</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Distribution of Speakers Across Different Accents in the Dataset</figcaption>
</figure>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">For the dataset creation each speaker is presented with 400 prompts to read: sentences, place names, and person names. Two microphones set at different distances to the speaker are used for the recordings. Punctuation are omitted and numbers were written out in full.</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">We also evaluate the models using the bona fide audio files from the CSaLT Deepfake dataset <cite class="ltx_cite ltx_citemacro_citep">(Munir et al., <a href="#bib.bib17" title="" class="ltx_ref">2024</a>)</cite>, which includes 6 female and 11 male speakers, with a total of 3,398 audio files amounting to 42.9 hours of recordings.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Conversational Speech</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">We present the first Urdu conversational speech dataset, which consists of 471 audio recordings (1.3 hours). These audios were recorded through calls over the internet. This is to mirror actual conversational environments and meetings, making the data more relevant and practical for real-world applications.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">The dataset features 4 female and 6 male speakers who used the microphones they had on hand to replicate real-life audio quality. To ensure smooth and natural conversations, the participants, all native Urdu speakers and computer science students, were asked to form groups and pairs with people they felt comfortable with. The recordings were done in various group sizes. Four sessions involved groups of 2, One session involved a group of 3, and another session involved a group of 4.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">Participants were also encouraged to choose topics they were comfortable discussing, which resulted in a diverse range of discussions. The final topics included Pakistan Independence Day, Group Projects, Ramadan and Eid, Neighbors Discussing Load-shedding and Prices, Health Issues, and Gossips.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p">For transcription, the process was carried out in three passes to ensure accuracy and consistency. The first pass transcription was completed by the original recorders themselves. This was followed by a second pass, where two research interns, who were not involved in the recording, refined the transcriptions and split the audio into smaller chunks. The final pass was done by two of the authors of this paper, ensuring the highest level of accuracy in the transcriptions.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Design</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We fine-tune the ASR models in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 ASR and Post-Processing Models ‣ 3 Models and Datasets ‣ WER We Stand: Benchmarking Urdu ASR Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> using a combined dataset of Mozilla’s Common Voice <cite class="ltx_cite ltx_citemacro_citep">(Ardila et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> and
Google’s Fleurs <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>, which together provide 16,156 audio samples. Given the limited amount of training data available, we opted for a 90-10 split, with 90% of the data used for training and the remaining 10% for validation. We evaluate both the non-fine-tuned and the fine-tuned models on ARL and CSaLT dataset for read speech and on our dataset for conversational speech. <span id="S4.p1.1.1" class="ltx_text ltx_font_typewriter">mms-300m</span> base model is not fine-tuned for downstream task of speech recognition so we only evaluate our fine-tuned version. Before conducting the evaluations, we normalize both the ground truth and the model predictions. This involves removing punctuation and disfluencies. This preprocessing step ensures that the evaluations focus on meaningful transcription accuracy rather than being skewed by minor formatting inconsistencies or speech disfluencies and punctuation marks. We noticed that Whisper and MMS models have built-in disfluency filtering and perform this task effectively. However, Seamless models output disfluencies marked with a # (e.g., #um), which we removed using regex during preprocessing.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Table <a href="#S5.T3" title="Table 3 ‣ 5 Results and Discussion ‣ WER We Stand: Benchmarking Urdu ASR Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the WER of the ASR models, fine-tuned and non-fine-tuned, on read speech datasets—ARL and CSaLT—and our conversational speech dataset.</p>
</div>
<figure id="S5.T3" class="ltx_table ltx_align_center">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Read Speech (ARL)</span></td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Read Speech (CSaLT)</span></td>
<td id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">Conversational Speech</span></td>
</tr>
<tr id="S5.T3.1.2.2" class="ltx_tr">
<th id="S5.T3.1.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S5.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.2.2.2.1" class="ltx_text ltx_font_bold">Base Model</span></td>
<td id="S5.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.2.2.3.1" class="ltx_text ltx_font_bold">Fine-tuned</span></td>
<td id="S5.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.2.2.4.1" class="ltx_text ltx_font_bold">Base Model</span></td>
<td id="S5.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.2.2.5.1" class="ltx_text ltx_font_bold">Fine-tuned</span></td>
<td id="S5.T3.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.2.2.6.1" class="ltx_text ltx_font_bold">Base Model</span></td>
<td id="S5.T3.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.2.2.7.1" class="ltx_text ltx_font_bold">Fine-tuned</span></td>
</tr>
<tr id="S5.T3.1.3.3" class="ltx_tr">
<th id="S5.T3.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.1.3.3.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S5.T3.1.3.3.2" class="ltx_td"></td>
<td id="S5.T3.1.3.3.3" class="ltx_td"></td>
<td id="S5.T3.1.3.3.4" class="ltx_td"></td>
<td id="S5.T3.1.3.3.5" class="ltx_td"></td>
<td id="S5.T3.1.3.3.6" class="ltx_td"></td>
<td id="S5.T3.1.3.3.7" class="ltx_td"></td>
</tr>
<tr id="S5.T3.1.4.4" class="ltx_tr">
<th id="S5.T3.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T3.1.4.4.1.1" class="ltx_text ltx_font_typewriter">whisper-tiny</span></th>
<td id="S5.T3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">116.92</td>
<td id="S5.T3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">45.59</td>
<td id="S5.T3.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">96.57</td>
<td id="S5.T3.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">42.12</td>
<td id="S5.T3.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">163.18</td>
<td id="S5.T3.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">59.99</td>
</tr>
<tr id="S5.T3.1.5.5" class="ltx_tr">
<th id="S5.T3.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.1.5.5.1.1" class="ltx_text ltx_font_typewriter">whisper-base</span></th>
<td id="S5.T3.1.5.5.2" class="ltx_td ltx_align_center">71.53</td>
<td id="S5.T3.1.5.5.3" class="ltx_td ltx_align_center">39.84</td>
<td id="S5.T3.1.5.5.4" class="ltx_td ltx_align_center">57.77</td>
<td id="S5.T3.1.5.5.5" class="ltx_td ltx_align_center">38.86</td>
<td id="S5.T3.1.5.5.6" class="ltx_td ltx_align_center">163.52</td>
<td id="S5.T3.1.5.5.7" class="ltx_td ltx_align_center">48.61</td>
</tr>
<tr id="S5.T3.1.6.6" class="ltx_tr">
<th id="S5.T3.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.1.6.6.1.1" class="ltx_text ltx_font_typewriter">whisper-small</span></th>
<td id="S5.T3.1.6.6.2" class="ltx_td ltx_align_center">48.70</td>
<td id="S5.T3.1.6.6.3" class="ltx_td ltx_align_center">28.60</td>
<td id="S5.T3.1.6.6.4" class="ltx_td ltx_align_center">41.10</td>
<td id="S5.T3.1.6.6.5" class="ltx_td ltx_align_center">27.39</td>
<td id="S5.T3.1.6.6.6" class="ltx_td ltx_align_center">55.67</td>
<td id="S5.T3.1.6.6.7" class="ltx_td ltx_align_center">32.92</td>
</tr>
<tr id="S5.T3.1.7.7" class="ltx_tr">
<th id="S5.T3.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.1.7.7.1.1" class="ltx_text ltx_font_typewriter">whisper-medium</span></th>
<td id="S5.T3.1.7.7.2" class="ltx_td ltx_align_center">37.04</td>
<td id="S5.T3.1.7.7.3" class="ltx_td ltx_align_center">25.38</td>
<td id="S5.T3.1.7.7.4" class="ltx_td ltx_align_center">33.39</td>
<td id="S5.T3.1.7.7.5" class="ltx_td ltx_align_center">24.15</td>
<td id="S5.T3.1.7.7.6" class="ltx_td ltx_align_center">40.22</td>
<td id="S5.T3.1.7.7.7" class="ltx_td ltx_align_center">28.87</td>
</tr>
<tr id="S5.T3.1.8.8" class="ltx_tr">
<th id="S5.T3.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.1.8.8.1.1" class="ltx_text ltx_font_typewriter">whisper-large</span></th>
<td id="S5.T3.1.8.8.2" class="ltx_td ltx_align_center">26.25</td>
<td id="S5.T3.1.8.8.3" class="ltx_td ltx_align_center">23.79</td>
<td id="S5.T3.1.8.8.4" class="ltx_td ltx_align_center">24.44</td>
<td id="S5.T3.1.8.8.5" class="ltx_td ltx_align_center">22.35</td>
<td id="S5.T3.1.8.8.6" class="ltx_td ltx_align_center"><span id="S5.T3.1.8.8.6.1" class="ltx_text ltx_font_bold">18.30</span></td>
<td id="S5.T3.1.8.8.7" class="ltx_td ltx_align_center"><span id="S5.T3.1.8.8.7.1" class="ltx_text ltx_font_bold">17.86</span></td>
</tr>
<tr id="S5.T3.1.9.9" class="ltx_tr">
<th id="S5.T3.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.1.9.9.1.1" class="ltx_text ltx_font_typewriter">mms-300m</span></th>
<td id="S5.T3.1.9.9.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.9.9.3" class="ltx_td ltx_align_center">51.48</td>
<td id="S5.T3.1.9.9.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.9.9.5" class="ltx_td ltx_align_center">47.73</td>
<td id="S5.T3.1.9.9.6" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.9.9.7" class="ltx_td ltx_align_center">66.40</td>
</tr>
<tr id="S5.T3.1.10.10" class="ltx_tr">
<th id="S5.T3.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.1.10.10.1.1" class="ltx_text ltx_font_typewriter">mms-1b</span></th>
<td id="S5.T3.1.10.10.2" class="ltx_td ltx_align_center">39.65</td>
<td id="S5.T3.1.10.10.3" class="ltx_td ltx_align_center">28.37</td>
<td id="S5.T3.1.10.10.4" class="ltx_td ltx_align_center">34.60</td>
<td id="S5.T3.1.10.10.5" class="ltx_td ltx_align_center">26.85</td>
<td id="S5.T3.1.10.10.6" class="ltx_td ltx_align_center">46.44</td>
<td id="S5.T3.1.10.10.7" class="ltx_td ltx_align_center">42.46</td>
</tr>
<tr id="S5.T3.1.11.11" class="ltx_tr">
<th id="S5.T3.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.1.11.11.1.1" class="ltx_text ltx_font_typewriter">seamless-medium</span></th>
<td id="S5.T3.1.11.11.2" class="ltx_td ltx_align_center">30.06</td>
<td id="S5.T3.1.11.11.3" class="ltx_td ltx_align_center">19.41</td>
<td id="S5.T3.1.11.11.4" class="ltx_td ltx_align_center">24.18</td>
<td id="S5.T3.1.11.11.5" class="ltx_td ltx_align_center">20.59</td>
<td id="S5.T3.1.11.11.6" class="ltx_td ltx_align_center">22.33</td>
<td id="S5.T3.1.11.11.7" class="ltx_td ltx_align_center">20.01</td>
</tr>
<tr id="S5.T3.1.12.12" class="ltx_tr">
<th id="S5.T3.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T3.1.12.12.1.1" class="ltx_text ltx_font_typewriter">seamless-large</span></th>
<td id="S5.T3.1.12.12.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.12.12.2.1" class="ltx_text ltx_font_bold">23.97</span></td>
<td id="S5.T3.1.12.12.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.12.12.3.1" class="ltx_text ltx_font_bold">17.09</span></td>
<td id="S5.T3.1.12.12.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.12.12.4.1" class="ltx_text ltx_font_bold">20.57</span></td>
<td id="S5.T3.1.12.12.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.12.12.5.1" class="ltx_text ltx_font_bold">18.61</span></td>
<td id="S5.T3.1.12.12.6" class="ltx_td ltx_align_center ltx_border_bb">29.99</td>
<td id="S5.T3.1.12.12.7" class="ltx_td ltx_align_center ltx_border_bb">18.75</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>WER of ASR models on ARL and CSaLT dataset for Read Speech and on our dataset for Conversational Speech. For each category, bold represents the lower WER in the specified category.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Read Speech</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Read speech typically consists of well-articulated sentences with fewer disfluencies, making it a less complex task for ASR models compared to conversational speech. However, challenges such as dialectal variations and pronunciation differences still impact transcription accuracy.</p>
</div>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Quantitative Analysis</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">In both the ARL and CSaLT datasets, the evaluated ASR models reveal distinct patterns in performance based on their architecture and size. Smaller models, such as <span id="S5.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_typewriter">whisper-tiny</span> and <span id="S5.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_typewriter">whisper-base</span>, suffer from significant hallucination issues in their base versions, as indicated by the high WERs—116.92 and 96.57 on ARL for <span id="S5.SS1.SSS1.p1.1.3" class="ltx_text ltx_font_typewriter">whisper-tiny</span> and <span id="S5.SS1.SSS1.p1.1.4" class="ltx_text ltx_font_typewriter">whisper-base</span>, respectively. Similarly, on CSaLT, the WER for <span id="S5.SS1.SSS1.p1.1.5" class="ltx_text ltx_font_typewriter">whisper-tiny</span> is 96.57, indicating that these models struggle to handle Urdu without fine-tuning. This issue is depicted in Figure <a href="#S5.F1" title="Figure 1 ‣ 5.1.1 Quantitative Analysis ‣ 5.1 Read Speech ‣ 5 Results and Discussion ‣ WER We Stand: Benchmarking Urdu ASR Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, where the non-fine-tuned <span id="S5.SS1.SSS1.p1.1.6" class="ltx_text ltx_font_typewriter">whisper-tiny</span> generates incorrect transcriptions. However, fine-tuning substantially reduces these error rates, as seen with <span id="S5.SS1.SSS1.p1.1.7" class="ltx_text ltx_font_typewriter">whisper-tiny</span>’s drop to 45.59 on ARL and 42.12 on CSaLT, and <span id="S5.SS1.SSS1.p1.1.8" class="ltx_text ltx_font_typewriter">whisper-base</span>’s improvement to 39.84 and 38.86 on the respective datasets. Despite these improvements, the smaller models remain limited due to their relatively small parameter sizes—34M for <span id="S5.SS1.SSS1.p1.1.9" class="ltx_text ltx_font_typewriter">whisper-tiny</span> and 74M for <span id="S5.SS1.SSS1.p1.1.10" class="ltx_text ltx_font_typewriter">whisper-base</span>.</p>
</div>
<figure id="S5.F1" class="ltx_figure">
<p id="S5.F1.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F1.1.1.1" class="ltx_text"><img src="/html/2409.11252/assets/x1.png" id="S5.F1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="130" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The yellow and blue boxes represent the output of the same audio on general and fine-tuned <span id="S5.F1.3.1" class="ltx_text ltx_font_typewriter">whisper-tiny</span> models respectively</figcaption>
</figure>
<div id="S5.SS1.SSS1.p2" class="ltx_para">
<p id="S5.SS1.SSS1.p2.1" class="ltx_p">Moving to larger models, <span id="S5.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_typewriter">whisper-small</span> (244M parameters) and <span id="S5.SS1.SSS1.p2.1.2" class="ltx_text ltx_font_typewriter">whisper-medium</span> (769M parameters) demonstrate a notable improvement over their smaller counterparts. For instance, <span id="S5.SS1.SSS1.p2.1.3" class="ltx_text ltx_font_typewriter">whisper-small</span> reduces its WER from 48.70 to 28.60 on ARL after fine-tuning, while <span id="S5.SS1.SSS1.p2.1.4" class="ltx_text ltx_font_typewriter">whisper-medium</span> improves from 37.04 to 25.38. The trend is consistent on CSaLT, where <span id="S5.SS1.SSS1.p2.1.5" class="ltx_text ltx_font_typewriter">whisper-small</span> drops from a WER of 41.10 to 27.39 and <span id="S5.SS1.SSS1.p2.1.6" class="ltx_text ltx_font_typewriter">whisper-medium</span> from 33.39 to 24.15. However, the largest model, <span id="S5.SS1.SSS1.p2.1.7" class="ltx_text ltx_font_typewriter">whisper-large</span> with 1.55B parameters, performs better, achieving a WER of 23.79 on ARL and 22.35 on CSaLT after fine-tuning. Although <span id="S5.SS1.SSS1.p2.1.8" class="ltx_text ltx_font_typewriter">whisper-large</span> shows impressive results, the <span id="S5.SS1.SSS1.p2.1.9" class="ltx_text ltx_font_typewriter">seamless-m4t</span> models outperform it. For instance, <span id="S5.SS1.SSS1.p2.1.10" class="ltx_text ltx_font_typewriter">seamless-medium</span> with 1.2B parameters achieves a WER of 30.06 pre-fine-tuning and 19.41 post-fine-tuning on ARL. Additionally, fine-tuned <span id="S5.SS1.SSS1.p2.1.11" class="ltx_text ltx_font_typewriter">seamless-large</span> consistently outperforms all other models, with a WER of 17.09 on ARL and 18.61 on CSaLT, showcasing the efficiency of the Seamless-M4T architecture.</p>
</div>
<div id="S5.SS1.SSS1.p3" class="ltx_para">
<p id="S5.SS1.SSS1.p3.1" class="ltx_p">In contrast, the <span id="S5.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_typewriter">mms</span> models, despite their large parameter sizes, underperform. The <span id="S5.SS1.SSS1.p3.1.2" class="ltx_text ltx_font_typewriter">mms-300m</span> model struggles, with a WER of 51.48 on ARL and 47.73 on CSaLT datasets, whereas <span id="S5.SS1.SSS1.p3.1.3" class="ltx_text ltx_font_typewriter">mms-1b</span> achieves more competitive results after fine-tuning, reducing its WER from 39.65 to 28.37 on ARL and from 34.60 to 26.85 on CSaLT. <span id="S5.SS1.SSS1.p3.1.4" class="ltx_text ltx_font_typewriter">mms-300m</span> has a lower WER on both datasets compared to <span id="S5.SS1.SSS1.p3.1.5" class="ltx_text ltx_font_typewriter">whisper-tiny</span> with 39M parameters. The fine-tuned <span id="S5.SS1.SSS1.p3.1.6" class="ltx_text ltx_font_typewriter">mms-1b</span> has comparable performance to <span id="S5.SS1.SSS1.p3.1.7" class="ltx_text ltx_font_typewriter">whisper-small</span> with 244M parameters. This indicates the MMS family is outperformed by much smaller models from other model families, highlighting that model size alone does not guarantee better performance, and other factors such as architecture and training data play a crucial role.</p>
</div>
<div id="S5.SS1.SSS1.p4" class="ltx_para">
<p id="S5.SS1.SSS1.p4.1" class="ltx_p">The heatmap in Figure <a href="#S5.F2" title="Figure 2 ‣ 5.1.1 Quantitative Analysis ‣ 5.1 Read Speech ‣ 5 Results and Discussion ‣ WER We Stand: Benchmarking Urdu ASR Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> compares the most frequent errors (wrong words) on combined ARL and CSaLT datasets across five non-fine-tuned models: <span id="S5.SS1.SSS1.p4.1.1" class="ltx_text ltx_font_typewriter">mms-1b</span>, <span id="S5.SS1.SSS1.p4.1.2" class="ltx_text ltx_font_typewriter">whisper-medium</span>, <span id="S5.SS1.SSS1.p4.1.3" class="ltx_text ltx_font_typewriter">whisper-large</span>, <span id="S5.SS1.SSS1.p4.1.4" class="ltx_text ltx_font_typewriter">seamless-medium</span>, and <span id="S5.SS1.SSS1.p4.1.5" class="ltx_text ltx_font_typewriter">seamless-large</span>.</p>
</div>
<figure id="S5.F2" class="ltx_figure">
<p id="S5.F2.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F2.1.1.1" class="ltx_text"><img src="/html/2409.11252/assets/figures/heatmap_comparison-nft.png" id="S5.F2.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Comparison of wrong words across non-fine-tuned models.</figcaption>
</figure>
<div id="S5.SS1.SSS1.p5" class="ltx_para">
<p id="S5.SS1.SSS1.p5.1" class="ltx_p">Several words stand out as high-frequency errors across all models. Notably, "Ho ga" and "Chahiye" are consistently problematic, with particularly high error counts in <span id="S5.SS1.SSS1.p5.1.1" class="ltx_text ltx_font_typewriter">whisper-medium</span>, <span id="S5.SS1.SSS1.p5.1.2" class="ltx_text ltx_font_typewriter">whisper-large</span>, and <span id="S5.SS1.SSS1.p5.1.3" class="ltx_text ltx_font_typewriter">seamless-large</span>. The word "Chahiye" misspelled in Urdu: one with two "Yeh" and a "Hamza" characters and one with one "Yeh" and a "Hamza". These errors can likely be attributed to the training data, which often includes user-generated content such as internet text, where such misspellings are common. This noisy data likely contributes to the model’s tendency to replicate these errors.</p>
</div>
<div id="S5.SS1.SSS1.p6" class="ltx_para">
<p id="S5.SS1.SSS1.p6.1" class="ltx_p">"Chahiye" (correct one) has an error frequency of 1,269 in <span id="S5.SS1.SSS1.p6.1.1" class="ltx_text ltx_font_typewriter">whisper-medium</span> and 3,173 in <span id="S5.SS1.SSS1.p6.1.2" class="ltx_text ltx_font_typewriter">whisper-large</span>, indicating that these models struggle significantly with this specific word. <span id="S5.SS1.SSS1.p6.1.3" class="ltx_text ltx_font_typewriter">mms-1b</span> shows relatively fewer errors for many words in this specific analysis, but it has a higher WER for the ARL and CSaLT datasets. This indicates that, while fewer individual words may be highlighted as problematic here, the model struggles with a broader range of words overall, leading to a higher overall error rate . The word "Hoga" shows a high error rate across all models, as it should be written as "Ho ga" in the model transcriptions, but the test dataset incorrectly contains it as a single word, "Hoga." This mismatch leads to consistent false positives, where the models’ correct transcription is flagged as incorrect due to the way the word is represented in the test dataset. The Whisper family has the higher tendency to misrecognize the word "Hai" followed by MMS and then Seamless.</p>
</div>
<div id="S5.SS1.SSS1.p7" class="ltx_para">
<p id="S5.SS1.SSS1.p7.1" class="ltx_p">Figure <a href="#S5.F3" title="Figure 3 ‣ 5.1.1 Quantitative Analysis ‣ 5.1 Read Speech ‣ 5 Results and Discussion ‣ WER We Stand: Benchmarking Urdu ASR Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> compares the most frequent errors on ARL and CSaLT datasets across the fine-tuned models. In the fine-tuned models, certain high-frequency errors observed in the non-fine-tuned models have been reduced, indicating that fine-tuning has helped address some transcription challenges. However in some cases the error rate has gone up. For instance, the error rate for the word "Chahiye" (correct one) has increased after fine-tuning. We found 155 misspelled versions of this word in Common Voice and Fleurs dataset. For <span id="S5.SS1.SSS1.p7.1.1" class="ltx_text ltx_font_typewriter">seamless-medium</span>, the error has gone up from 0 to 1942. <span id="S5.SS1.SSS1.p7.1.2" class="ltx_text ltx_font_typewriter">whisper-large</span> still shows a high error with slight increase from 3173 to 3391. The word "Ho ga" also poses a challenge for fine-tuned models. There are 82 instances of "Ho ga" (correct) and 87 instances of "Hoga" (incorrect) in the training dataset. As a result, the false positives seen earlier are now turned into inconsistencies, where the models are confused between the two variations, leading to unpredictable outputs.</p>
</div>
<figure id="S5.F3" class="ltx_figure">
<p id="S5.F3.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F3.1.1.1" class="ltx_text"><img src="/html/2409.11252/assets/figures/heatmap_comparison-ft.png" id="S5.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison of wrong words across fine-tuned models.</figcaption>
</figure>
<div id="S5.SS1.SSS1.p8" class="ltx_para">
<p id="S5.SS1.SSS1.p8.1" class="ltx_p">The words like "Hi", "Ki" and "Ko" show improved performance in the fine-tuned models. While fine-tuning has led to notable improvements in transcription accuracy for several words, persistent challenges remain, particularly for high-frequency words and segmentation issues. This evaluation highlights the complexity of evaluating Urdu ASR models using quantitative metrics alone and underscores the need for a robust text normalization system to address variations in word forms and improve overall model accuracy.</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Qualitative Analysis</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">Fine-tuning leads to substantial improvements across all models, the <span id="S5.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_typewriter">seamless-large</span> emerges as the best performer across the ARL and CSaLT datasets. The performance of the <span id="S5.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_typewriter">seamless-medium</span> and <span id="S5.SS1.SSS2.p1.1.3" class="ltx_text ltx_font_typewriter">seamless-large</span> models can be attributed to several key differences in training data and architecture. Seamless-M4T benefited from over 470,000 hours of multimodal speech and text data, sourced from the SeamlessAlign dataset, which was specifically crafted to support a broad spectrum of languages. This extensive, diverse dataset, combined with the use of SONAR embeddings—designed to offer modality and language-agnostic representations—enhances Seamless-M4T’s ability to generalize effectively across various languages and speech domains. The architecture supports more complex multimodal learning, contributing to its edge in performance, particularly in tasks involving low-resource languages like Urdu.</p>
</div>
<div id="S5.SS1.SSS2.p2" class="ltx_para">
<p id="S5.SS1.SSS2.p2.1" class="ltx_p">In contrast, <span id="S5.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_typewriter">whisper-large</span> was trained on approximately 1 million hours of labeled data and 4 million hours of pseudo-labeled data, with a focus on automatic speech recognition (ASR) tasks. While Whisper is capable of handling numerous languages, its emphasis is primarily on transcription accuracy. Without the same multimodal and speech-to-speech training exposure found in Seamless-M4T, Whisper lacks the cross-modal adaptability that contributes to the superior results seen in Seamless-M4T’s performance on read speech tasks.</p>
</div>
<div id="S5.SS1.SSS2.p3" class="ltx_para">
<p id="S5.SS1.SSS2.p3.1" class="ltx_p">Similarly, while the MMS models were pretrained on 500,000 hours of speech data across 1,400 languages, their focus on Wav2Vec2-based self-supervised learning may explain their relative underperformance. MMS models are designed to excel at learning robust speech representations without labeled data, but they lack the comprehensive multimodal and multitask learning present in Seamless-M4T. This limits their performance on specific tasks like Urdu read speech, where cross-modal training and fine-tuning play a crucial role. Thus, despite extensive pretraining, MMS models are unable to match the fine-tuned, multimodal capabilities of Seamless-M4T in such applications.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Conversational Speech</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The results for conversational speech highlight distinct performance patterns among ASR models when compared to read speech, mainly due to the inherent challenges posed by spontaneous, natural dialogue. Conversational speech often includes disfluencies, overlaps, and variations in speaker styles, making it more complex to transcribe accurately.</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Quantitative Analysis</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">Most models show higher WERs in conversational speech, reflecting the increased difficulty of this task. Among the Whisper models, <span id="S5.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_typewriter">whisper-large</span> stands out with the lowest WER of 18.30 in its base version, slightly improving to 17.86 after fine-tuning. This positions it as the best-performing model for conversational speech. In contrast, smaller models like <span id="S5.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_typewriter">whisper-tiny</span> and <span id="S5.SS2.SSS1.p1.1.3" class="ltx_text ltx_font_typewriter">whisper-base</span> struggle significantly. Their base versions exhibit extremely high WERs—163.18 and 163.52, respectively—due to hallucinations and transcription errors. Fine-tuning reduces these errors, bringing their WERs down to 59.99 and 48.61, but they remain far behind the larger models. Mid-range models, such as <span id="S5.SS2.SSS1.p1.1.4" class="ltx_text ltx_font_typewriter">whisper-small</span> (244M parameters) and <span id="S5.SS2.SSS1.p1.1.5" class="ltx_text ltx_font_typewriter">whisper-medium</span> (769M parameters), also improve with fine-tuning, achieving WERs of 32.92 and 28.87, respectively.</p>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para">
<p id="S5.SS2.SSS1.p2.1" class="ltx_p">The MMS models, despite their large parameter sizes, underperform in conversational speech. The <span id="S5.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_typewriter">mms-1b</span> model, shows some improvement after fine-tuning, reducing its WER from 46.44 to 42.46, but it still lags behind even the smaller Whisper models like <span id="S5.SS2.SSS1.p2.1.2" class="ltx_text ltx_font_typewriter">whisper-medium</span>. The <span id="S5.SS2.SSS1.p2.1.3" class="ltx_text ltx_font_typewriter">mms-300m</span> model performs worse, with a post-fine-tuning WER of 66.40, suggesting that the MMS models’ pretraining data and objectives do not generalize well to conversational speech in low-resource languages such as Urdu.</p>
</div>
<div id="S5.SS2.SSS1.p3" class="ltx_para">
<p id="S5.SS2.SSS1.p3.1" class="ltx_p">The Seamless-M4T models also perform well on conversational speech, though their strengths lie primarily in read speech tasks. Notably, the smaller <span id="S5.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_typewriter">seamless-medium</span> model initially outperforms its larger counterpart, <span id="S5.SS2.SSS1.p3.1.2" class="ltx_text ltx_font_typewriter">seamless-large</span>, in its base version, achieving a WER of 22.33 compared to 29.99 for the large model. This suggests that the medium model, despite having fewer parameters, handles the nuances of conversational speech better. However, after fine-tuning, <span id="S5.SS2.SSS1.p3.1.3" class="ltx_text ltx_font_typewriter">seamless-large</span> shows more significant improvement, reducing its WER to 18.75, while <span id="S5.SS2.SSS1.p3.1.4" class="ltx_text ltx_font_typewriter">seamless-medium</span> only improves to 20.01. This indicates that while the medium model may have an edge in its base state, the larger model benefits more from fine-tuning, ultimately surpassing the performance of the medium model.</p>
</div>
<div id="S5.SS2.SSS1.p4" class="ltx_para">
<p id="S5.SS2.SSS1.p4.1" class="ltx_p">Overall, the results for conversational speech show that larger Whisper models and Seamless-M4T are better equipped to handle the complexities of spontaneous dialogue. Whisper models, particularly <span id="S5.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_typewriter">whisper-large</span>, exhibit a slight edge over Seamless-M4T, although the gap is narrower compared to their performance in read speech tasks. The MMS models, despite their large size, do not perform as well in this task, highlighting potential limitations in their training objectives. Fine-tuning proves to be essential for improving performance across all models when dealing with conversational speech.</p>
</div>
<div id="S5.SS2.SSS1.p5" class="ltx_para">
<p id="S5.SS2.SSS1.p5.1" class="ltx_p">The analysis of the conversational dataset on non-fine-tuned models reveals significant variability in error patterns across the different ASR models. While all models show high substitution errors, there are notable differences in the number of deletions and insertions. Figure <a href="#S5.F4" title="Figure 4 ‣ 5.2.1 Quantitative Analysis ‣ 5.2 Conversational Speech ‣ 5 Results and Discussion ‣ WER We Stand: Benchmarking Urdu ASR Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> gives the comparison of substitution errors and Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2.1 Quantitative Analysis ‣ 5.2 Conversational Speech ‣ 5 Results and Discussion ‣ WER We Stand: Benchmarking Urdu ASR Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> gives the comparison of deletion errors across models before and after fine-tuning. Substitutions dominate the total errors for all models, with counts ranging from 13,665 for <span id="S5.SS2.SSS1.p5.1.1" class="ltx_text ltx_font_typewriter">seamless-large</span> to 14,552 for <span id="S5.SS2.SSS1.p5.1.2" class="ltx_text ltx_font_typewriter">mms-1b</span>, indicating that non-fine-tuned models frequently mis-recognize words, leading to incorrect transcriptions. This is especially problematic for conversational datasets where context and word prediction are key factors for accurate transcription.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<p id="S5.F4.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F4.1.1.1" class="ltx_text"><img src="/html/2409.11252/assets/figures/substitutions_per_model_joint.png" id="S5.F4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="252" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparison of substitution errors across models before and after fine-tuning.</figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure">
<p id="S5.F5.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F5.1.1.1" class="ltx_text"><img src="/html/2409.11252/assets/figures/deletions_per_model_joint.png" id="S5.F5.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="252" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of deletion errors across models before and after fine-tuning.</figcaption>
</figure>
<div id="S5.SS2.SSS1.p6" class="ltx_para">
<p id="S5.SS2.SSS1.p6.1" class="ltx_p">Deletions also account for a substantial portion of errors, particularly for <span id="S5.SS2.SSS1.p6.1.1" class="ltx_text ltx_font_typewriter">whisper-large</span> (1,641) and <span id="S5.SS2.SSS1.p6.1.2" class="ltx_text ltx_font_typewriter">whisper-medium</span> (1,598). High deletion errors suggest that these models are often failing to transcribe words altogether, which can significantly distort the meaning of the conversation. Interestingly, <span id="S5.SS2.SSS1.p6.1.3" class="ltx_text ltx_font_typewriter">mms-1b</span> shows the lowest number of deletions (844), indicating that while it may substitute many words, it tends to capture more of the spoken content compared to other models. Insertions remain minimal across all models (non-fine-tuned and fine-tuned), with each registering only a few insertion errors (1 to 4). This suggests that these models rarely add extra words that were not spoken.</p>
</div>
<div id="S5.SS2.SSS1.p7" class="ltx_para">
<p id="S5.SS2.SSS1.p7.1" class="ltx_p">After fine-tuning, the overall performance of the ASR models shows notable changes, though the error patterns remain similar in terms of overall trends. Substitution errors continue to account for the majority of the total errors. We can observe in Figure <a href="#S5.F4" title="Figure 4 ‣ 5.2.1 Quantitative Analysis ‣ 5.2 Conversational Speech ‣ 5 Results and Discussion ‣ WER We Stand: Benchmarking Urdu ASR Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S5.F5" title="Figure 5 ‣ 5.2.1 Quantitative Analysis ‣ 5.2 Conversational Speech ‣ 5 Results and Discussion ‣ WER We Stand: Benchmarking Urdu ASR Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> that after fine-tuning the WER of Whisper models and MMS goes down because of reduction in deletion errors and the WER of Seamless models go down because of the reduction in substitution errors. Fine-tuning seems to reduce substitution errors for <span id="S5.SS2.SSS1.p7.1.1" class="ltx_text ltx_font_typewriter">seamless-large</span> to 13,543 and <span id="S5.SS2.SSS1.p7.1.2" class="ltx_text ltx_font_typewriter">seamless-medium</span> to 13,953 but it increased slightly for other models. Deletions error particularly, for <span id="S5.SS2.SSS1.p7.1.3" class="ltx_text ltx_font_typewriter">mms-1b</span> saw a drop from 844 to 338, and for <span id="S5.SS2.SSS1.p7.1.4" class="ltx_text ltx_font_typewriter">whisper-medium</span> dropped from 1,598 to 589. This indicates that fine-tuning helps the models transcribe more spoken content in their transcriptions.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Qualitative Analysis</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">A significant limitation across all ASR models is their inability to handle overlapping speech from multiple speakers. As illustrated in Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2.2 Qualitative Analysis ‣ 5.2 Conversational Speech ‣ 5 Results and Discussion ‣ WER We Stand: Benchmarking Urdu ASR Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the models tend to transcribe only the dominant speaker, ignoring other concurrent voices.</p>
</div>
<figure id="S5.F6" class="ltx_figure">
<p id="S5.F6.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F6.1.1.1" class="ltx_text"><img src="/html/2409.11252/assets/x2.png" id="S5.F6.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="230" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Example of multi-speaker audio.</figcaption>
</figure>
<div id="S5.SS2.SSS2.p2" class="ltx_para">
<p id="S5.SS2.SSS2.p2.1" class="ltx_p">The higher WER of base <span id="S5.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_typewriter">seamless-large</span> can be partly attributed to two distinct factors: its handling of English words within the Urdu conversational dataset and its tendency to paraphrase certain words. Instead of transliterating English words, it often translates them, leading to errors. For example, as shown in Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2.2 Qualitative Analysis ‣ 5.2 Conversational Speech ‣ 5 Results and Discussion ‣ WER We Stand: Benchmarking Urdu ASR Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, the word "Research" is translated to "Tehqeeq" rather than being transliterated. Additionally, the model tends to paraphrase words within the Urdu content, such as transcribing "Guftagu" as "Baat". These errors likely stem from the model’s training on a multimodal dataset that emphasizes cross-language translation and broader contextual understanding, rather than strict transcription accuracy. Seamless-M4T’s architecture, designed for tasks beyond ASR—such as speech-to-speech translation—encourages this behavior. The model prioritizes conveying meaning across languages, which leads to translations and paraphrasing when encountering multilingual inputs, as opposed to Whisper, which is primarily trained for transcription fidelity.</p>
</div>
<figure id="S5.F7" class="ltx_figure">
<p id="S5.F7.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F7.1.1.1" class="ltx_text"><img src="/html/2409.11252/assets/x3.png" id="S5.F7.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="87" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Translation and paraphrasing error in Seamless M4T. The yellow box represents the ground truth and the blue ones represents the prediction.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we present a comprehensive evaluation of three ASR model families—Whisper, MMS, and Seamless-M4T—on Urdu read and conversational speech datasets. Our findings highlight the challenges associated with developing robust ASR systems for low-resource languages like Urdu, particularly when faced with spontaneous conversational speech, disfluencies, and code-switching. Among the models, <span id="S6.p1.1.1" class="ltx_text ltx_font_typewriter">whisper-large</span> and <span id="S6.p1.1.2" class="ltx_text ltx_font_typewriter">seamless-large</span> stand out, with <span id="S6.p1.1.3" class="ltx_text ltx_font_typewriter">whisper-large</span> excelling in conversational contexts and <span id="S6.p1.1.4" class="ltx_text ltx_font_typewriter">seamless-large</span> demonstrating strong performance in read speech tasks. Despite improvements from fine-tuning, challenges such as handling overlapping speech and distinguishing between similar phonetic patterns remain prevalent across models, indicating the need for further refinement. Additionally, our error analysis reveals the importance of text normalization and highlights the potential of multimodal approaches to improve ASR accuracy. This study contributes valuable insights into the capabilities and limitations of current ASR models for Urdu and underscores the importance of designing specialized datasets and evaluation metrics. Our work will also be valuable for developers looking to build real-world applications, such as virtual assistants, voice-controlled devices, and transcription services.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Future Work</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This study opens several avenues for future exploration in Urdu ASR. Addressing overlapping speech in conversational settings remains a critical challenge. Future work could focus on evaluating speaker diarization using our dataset to improve multi-speaker recognition. Enhancing the handling of code-switching between Urdu and English, especially for models like <span id="S7.p1.1.1" class="ltx_text ltx_font_typewriter">seamless-large</span> that tend to translate rather than transliterate, could significantly boost transcription accuracy in real-world contexts where multilingualism is common. Exploring the potential of Large Language Models (LLMs), such as <span id="S7.p1.1.2" class="ltx_text ltx_font_typewriter">GPT-4o</span> and <span id="S7.p1.1.3" class="ltx_text ltx_font_typewriter">Llama-3.1</span>, for ASR output post-processing could further refine transcription quality by correcting errors and improving fluency. Finally, the integration of a robust text normalization system tailored for Urdu could address variations in spelling and word forms, improving the consistency of transcriptions. Extending this work to cross-modal tasks, such as speech-to-speech translation or text generation, could provide valuable insights for building more advanced systems that handle complex, multimodal language tasks for low-resource languages like Urdu.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We are grateful to our research interns: Muhammad Abubakar Mughal, Natiq Khan, Anum Javed, Aleena Saqib, Muhammad Abdullah Sohail, Bushra Zubair, Hamza Iqbal, Salaar Masood, Maham Javed, Faisal Haider, Muhammad Suhaib Rashid, and Muhammad Faizan Waris. Their dedication and hard work in creating and refining the dataset were instrumental to the success of this project. We sincerely appreciate their invaluable contributions and commitment to our research efforts.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdelhamid et al. (2020)</span>
<span class="ltx_bibblock">
Abdelaziz Abdelhamid, Hamzah Alsayadi, Islam Hegazy, and Zaki Fayed. 2020.

</span>
<span class="ltx_bibblock">End-to-end arabic speech recognition: A review.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adline Freeda et al. (2024)</span>
<span class="ltx_bibblock">
R. Adline Freeda, V. S. Krithikaa Venket, A. Anju, Gugan, Ragul, and Rakesh. 2024.

</span>
<span class="ltx_bibblock">Voice-based virtual assistant for windows using asr.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ICT: Smart Systems and Technologies</em>, pages 277–284, Singapore. Springer Nature Singapore.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ardila et al. (2020)</span>
<span class="ltx_bibblock">
R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber. 2020.

</span>
<span class="ltx_bibblock">Common voice: A massively-multilingual speech corpus.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)</em>, pages 4211–4215.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asadullah et al. (2016)</span>
<span class="ltx_bibblock">
Asadullah, Arslan Shaukat, Hazrat Ali, and Usman Akram. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICIVC.2016.7571287" title="" class="ltx_ref ltx_href">Automatic urdu speech recognition using hidden markov model</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2016 International Conference on Image, Vision and Computing (ICIVC)</em>, pages 135–139.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ashraf et al. (2010)</span>
<span class="ltx_bibblock">
Javed Ashraf, Naveed Iqbal, Naveed Sarfraz Khattak, and Ather Mohsin Zaidi. 2010.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-642-13881-2_14" title="" class="ltx_ref ltx_href">Speaker independent urdu speech recognition using hmm</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2010 The 7th International Conference on Informatics and Systems (INFOS)</em>, pages 1–5.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2006.11477" title="" class="ltx_ref ltx_href">wav2vec 2.0: A framework for self-supervised learning of speech representations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2006.11477.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caranica et al. (2017)</span>
<span class="ltx_bibblock">
Alexandru Caranica, Horia Cucu, Corneliu Burileanu, François Portet, and Michel Vacher. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/SPED.2017.7990438" title="" class="ltx_ref ltx_href">Speech recognition results for voice-controlled assistive applications</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2017 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)</em>, pages 1–8.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Zhang (2019)</span>
<span class="ltx_bibblock">
Hong Chen and Bo Zhang. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1088/1742-6596/1237/2/022133" title="" class="ltx_ref ltx_href">Application of automatic speech recognition (asr) algorithm in smart home</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Journal of Physics: Conference Series</em>, 1237(2):022133.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Communication (2023a)</span>
<span class="ltx_bibblock">
Seamless Communication. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2312.05187" title="" class="ltx_ref ltx_href">Seamless: Multilingual expressive and streaming speech translation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2312.05187.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Communication (2023b)</span>
<span class="ltx_bibblock">
Seamless Communication. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2308.11596" title="" class="ltx_ref ltx_href">Seamlessm4t: Massively multilingual and multimodal machine translation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2308.11596.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2022)</span>
<span class="ltx_bibblock">
Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2205.12446" title="" class="ltx_ref ltx_href">Fleurs: Few-shot learning evaluation of universal representations of speech</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2205.12446.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhouib et al. (2022)</span>
<span class="ltx_bibblock">
Amira Dhouib, Achraf Othman, Oussama El Ghoul, Mohamed Koutheair Khribi, and Aisha Al Sinani. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3390/app12178898" title="" class="ltx_ref ltx_href">Arabic automatic speech recognition: A systematic literature review</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, 12(17).

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2014)</span>
<span class="ltx_bibblock">
Maree Johnson, Samuel Lapkin, Vanessa Long, Paula Sanchez, Hanna Suominen, Jim Basilakis, and Linda Dawson. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1186/1472-6947-14-94" title="" class="ltx_ref ltx_href">A systematic review of speech recognition technology in health care</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">BMC medical informatics and decision making</em>, 14:94.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kadyan et al. (2019)</span>
<span class="ltx_bibblock">
Virender Kadyan, Archana Mantri, Rajesh Aggarwal, and Amitoj Singh. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/s10772-018-09577-3" title="" class="ltx_ref ltx_href">A comparative study of deep neural network based punjabi-asr system</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">International Journal of Speech Technology</em>, 22.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al. (2023)</span>
<span class="ltx_bibblock">
Muhammad Danyal Khan, Raheem Ali, and Arshad Aziz. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2307.12759" title="" class="ltx_ref ltx_href">Code-switched urdu asr for noisy telephonic environment using data centric approach with hybrid hmm and cnn-tdnn</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2307.12759.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maier et al. (2010)</span>
<span class="ltx_bibblock">
Andreas Maier, Haderlein Tino, Florian Stelzle, Elmar Noeth, Emeka Nkenke, Rosanowski Frank, Schützenberger Anne, and Maria Schuster. 2010.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1155/2010/926951" title="" class="ltx_ref ltx_href">Automatic speech recognition systems for the evaluation of voice and speech disorders in head and neck cancer</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">EURASIP Journal on Audio, Speech, and Music Processing</em>, 2010.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Munir et al. (2024)</span>
<span class="ltx_bibblock">
Sheza Munir, Wassay Sajjad, Mukeet Raza, Emaan Abbas, Abdul Hameed Azeemi, Ihsan Ayyub Qazi, and Agha Ali Raza. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2024.findings-acl.861" title="" class="ltx_ref ltx_href">Deepfake defense: Constructing and evaluating a specialized Urdu deepfake audio dataset</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics ACL 2024</em>, pages 14470–14480, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naeem et al. (2020)</span>
<span class="ltx_bibblock">
Saad Naeem, Majid Iqbal, Muhammad Saqib, Muhammad Saad, Muhammad Soban Raza, Zaid Ali, Naveed Akhtar, Mirza Omer Beg, Waseem Shahzad, and Muhhamad Umair Arshad. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICOSST51357.2020.9333026" title="" class="ltx_ref ltx_href">Subspace gaussian mixture model for continuous urdu speech recognition using kaldi</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2020 14th International Conference on Open Source Systems and Technologies (ICOSST)</em>, pages 1–7.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pratap et al. (2023)</span>
<span class="ltx_bibblock">
Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, and Michael Auli. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2305.13516" title="" class="ltx_ref ltx_href">Scaling speech technology to 1,000+ languages</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2305.13516.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rabiner (1997)</span>
<span class="ltx_bibblock">
L.R. Rabiner. 1997.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ASRU.1997.659129" title="" class="ltx_ref ltx_href">Applications of speech recognition in the area of telecommunications</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">1997 IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings</em>, pages 501–510.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2022)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2212.04356" title="" class="ltx_ref ltx_href">Robust speech recognition via large-scale weak supervision</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2212.04356.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2020)</span>
<span class="ltx_bibblock">
Amitoj Singh, Virender Kadyan, Munish Kumar, and Nancy Bassan. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/s10462-019-09775-8" title="" class="ltx_ref ltx_href">Asroil: a comprehensive survey for automatic speech recognition of indian languages</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Artif. Intell. Rev.</em>, 53(5):3673–3704.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Subhash et al. (2020)</span>
<span class="ltx_bibblock">
S Subhash, Prajwal N Srivatsa, S Siddesh, A Ullas, and B Santhosh. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/WorldS450073.2020.9210344" title="" class="ltx_ref ltx_href">Artificial intelligence-based voice assistant</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2020 Fourth World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4)</em>, pages 593–596.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toshniwal et al. (2018)</span>
<span class="ltx_bibblock">
Shubham Toshniwal, Tara N. Sainath, Ron J. Weiss, Bo Li, Pedro Moreno, Eugene Weinstein, and Kanishka Rao. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP.2018.8461972" title="" class="ltx_ref ltx_href">Multilingual speech recognition with a single end-to-end model</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 4904–4908.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tüske et al. (2013)</span>
<span class="ltx_bibblock">
Zoltán Tüske, Joel Pinto, Daniel Willett, and Ralf Schlüter. 2013.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP.2013.6639090" title="" class="ltx_ref ltx_href">Investigation on cross- and multilingual mlp features under matched and mismatched acoustical conditions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</em>, pages 7349–7353.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Unnibhavi and Jangamshetti (2016)</span>
<span class="ltx_bibblock">
Anand H. Unnibhavi and D. S. Jangamshetti. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/SCOPES.2016.7955616" title="" class="ltx_ref ltx_href">A survey of speech recognition on south indian languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">2016 International Conference on Signal Processing, Communication, Power and Embedded System (SCOPES)</em>, pages 1122–1126.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.11251" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.11252" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.11252">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.11252" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.11253" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 19:19:06 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
