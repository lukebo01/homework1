<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.14266] Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries</title><meta property="og:description" content="Recently, multiple applications of machine learning have been introduced. They include various possibilities arising when image analysis methods are applied to, broadly understood, video streams. In this context, a nov…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.14266">

<!--Generated on Sat Jul  6 00:09:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Anna Wróblewska, Marcel Witas, Kinga Frańczak, Arkadiusz Kniaź
<br class="ltx_break">Faculty of Mathematics and Information Sciences,
Warsaw University of Technology
<br class="ltx_break">Warsaw, Poland
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">anna.wroblewska1@pw.edu.pl
<br class="ltx_break"><span id="id1.1.id1.1" class="ltx_ERROR undefined">\And</span></span>Cheong Siew Ann
<br class="ltx_break">School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Seng Chee Tan 
<br class="ltx_break">National Institute of Education, Nanyang Technological Institute, Singapore
<br class="ltx_break"><span id="id3.3.id3" class="ltx_ERROR undefined">\And</span>Janusz Hołyst
<br class="ltx_break">Faculty of Physics, Warsaw University of Technology, Warsaw, Poland 
<br class="ltx_break"><span id="id4.4.id4" class="ltx_ERROR undefined">\And</span>Marcin Paprzycki
<br class="ltx_break">Systems Research Institute Polish Academy of Sciences, Warsaw, Poland
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Recently, multiple applications of machine learning have been introduced. They include various possibilities arising when image analysis methods are applied to, broadly understood, video streams. In this context, a novel tool, developed for academic educators to enhance the teaching process by automating, summarizing, and offering prompt feedback on conducting lectures, has been developed. The implemented prototype utilizes machine learning-based techniques to recognise selected didactic and behavioural teachers’ features within lecture video recordings.
Specifically, users (teachers) can upload their lecture videos, which are preprocessed and analysed using machine learning models. Next, users can view summaries of recognized didactic features through interactive charts and tables. Additionally, stored ML-based prediction results support comparisons between lectures based on their didactic content. In the developed application text-based models trained on lecture transcriptions, with enhancements to the transcription quality, by adopting an automatic speech recognition solution are applied. Furthermore, the system offers flexibility for (future) integration of new/additional machine-learning models and software modules for image and video analysis.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.3" class="ltx_p"><em id="p1.3.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.3.2" class="ltx_text ltx_font_bold">eywords</span> Artificial intelligence in education  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
Lecture evaluation
 <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
Natural language processing  <math id="p1.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation></semantics></math>
Machine learning.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">It is easy to notice that the majority of newly proposed solutions, adopting artificial intelligence (AI) in higher education, focus on supporting students. This involves student learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, measuring student performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, or assessing available learning sources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Moreover, since approximately 2022, great interest in utilising and evaluating use of generative AI in education, for learning and teaching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> has emerged.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">A much less frequently addressed subject is the support that could be provided for the teachers, by observing them while they are conducting lessons, and providing personalized feedback.
Only very few AI-based solutions have been developed to address this need, e.g. tools for observing students’ behaviour and cooperation in teamwork during class <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, as well as other activities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Another type of support – related to preparing lessons – includes AI-based tutors developed to support teachers in schools by directing them towards achieving educational goals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. One more recent tool, is described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and is closely related to the approach proposed in this contribution. Here, the work describes a system that includes teaching evaluation indicators that combines traditional assessment scales with new values derived from the computer vision, audio speech recognition, and machine learning. while quite interesting, it provides only a comprehensive behavioural classification of the teacher, e.g. teacher’s style (i.e. humorous, passionate, solemn), or teacher’s media usage (i.e. multimedia vs blackboard). Research reported here proposes to take the teacher support, using already available AI-based methods, a step further. Specifically, the main goal is to improve the academic lecturing process by concentrating on lecturers’ didactic behaviours, and other techniques that can be used to visualise the lecture topic. It is stipulated that this can be achieved by automating, summarising, and providing quick feedback to the teachers by using machine learning (ML) methods applied to the video recordings of their lectures.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">The overarching research question is: is it possible to apply already existing ML-based methods to design a system that will have as its input a recording of a lecture, and as an output a set of recommendations for the teacher.
To achieve this goal, the didactic features found in the lecture recordings are annotated, and machine learning models are trained to recognise them. Detailed experiments, dedicated to the selected ML models, are available in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. They have been used to set up the proposed approach, which can be summarized as follows:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">establishing a set of didactic features feasible to annotate and detect automatically in lecture videos,</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">collecting a dataset of lecture video recordings (that contain the selected features),</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.1" class="ltx_p">designing a set of deep learning models trained to determine the presence of the selected didactic feature in the collected dataset – the video recordings.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">This contribution more precisely describes the captured set of didactic features, and relates them to the teaching practices. Moreover, the reported research has been focused on designing and implementing a prototype, supported with ML models, to automatically recognise the selected didactic features and to prepare summaries and visualisation for the lecturers. Another functionality of the implemented prototype is the possibility of collecting additional datasets and annotating them automatically. This, in turn, allows the proposed system to be further extended and fine-tuned.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">The remainder of this work is structured as follows.
Section <a href="#S2" title="2 The Didactic Features and the Collected Dataset ‣ Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> describes the recognized didactic features and the collected dataset. Next, Sections <a href="#S3" title="3 System Design ‣ Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, <a href="#S4" title="4 Machine Learning Module ‣ Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and <a href="#S5" title="5 System Interface ‣ Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> introduce the system prototype and the results, including the interface for the feature visualisation. Finally, the work is summarized in Section <a href="#S6" title="6 Concluding Remarks ‣ Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. There the main findings, and limitations to be considered in the future research, are presented.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>The Didactic Features and the Collected Dataset</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Let us start the presentation by discussing the <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">didactic features</span> that have been selected and the reasons for their selection.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Didactic Features Captured in the Research</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">At first, the teaching practices have been thoroughly analysed, following the approaches described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. This allowed to establish which features can be used as standards for, objectively, assessing the lessons.
Next, a set of didactic features, particularly valuable from the teaching point of view have been selected, and adapted to the academic lecturing use case, considered in this work.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Obviously, it has to be possible to detect the selected didactic features, using the appropriately selected and adapted ML models, applied to the lecture video recordings. Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Didactic Features Captured in the Research ‣ 2 The Didactic Features and the Collected Dataset ‣ Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents an example of frames that were extracted from lecture recordings (found in the collected dataset). Note that layouts of such frames often consist of two regions – one originates from the camera view focused on the lecturer, while the second one is a direct input from a computer screen (typically, presentation slides).</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2406.14266/assets/images/example_video_2.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="134" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example frames from the recorded lectures in the collected dataset; this dataset has been also utilized in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></figcaption>
</figure>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">The results of analysis of pertinent literature have been summarized in Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Didactic Features Captured in the Research ‣ 2 The Didactic Features and the Collected Dataset ‣ Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Here the selected features are presented and their correspondence to the main categories of Singapore Teaching Practices taxonomy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, e.g., ”activating prior knowledge,” ”arousing interest”, indicated. In particular the set of actions, performed by the lecturer, such as ”giving class outline” or ”writing on a whiteboard” has been selected. In what follows, these teachers’ actions are also called the didactic features. Additionally, the slides’ features that help understand the lecture, have been collected.
Note that the qualitative and quantitative didactic features comprise (a) slide characteristics, and (b) different educational behaviours of teachers explaining scientific concepts, e.g. ”plots on slides”, ”asking questions to the students”, or ”actively explaining on the board”.
Taking into account the goal of capturing didactic features using ML-based approaches, in the Table, the selected features have been divided into three categories: (1) the ones that can be automatically detected from a video stream, i.e. images or videos; (2) those that need to be detected only from the lecture audio stream; and (3) those features that can be detected based on both sources.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.1.1.1" class="ltx_p" style="width:173.4pt;"><span id="S2.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Feature</span></span>
</span>
</td>
<td id="S2.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.2.1.1" class="ltx_p" style="width:260.2pt;"><span id="S2.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Singapore Teaching Practices areas</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2">Audio-based features</td>
</tr>
<tr id="S2.T1.1.3.3" class="ltx_tr">
<td id="S2.T1.1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.3.1.1.1" class="ltx_p" style="width:173.4pt;">Asking questions</span>
</span>
</td>
<td id="S2.T1.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.3.2.1.1" class="ltx_p" style="width:260.2pt;">Activating prior knowledge; Encouraging learner engagement; Using questions to deepen learning</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.4.4" class="ltx_tr">
<td id="S2.T1.1.4.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.4.1.1.1" class="ltx_p" style="width:173.4pt;">Giving questions to students: rhetorical, comprehension questions</span>
</span>
</td>
<td id="S2.T1.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.4.2.1.1" class="ltx_p" style="width:260.2pt;">Using questions to deepen learning; Arousing interest; Providing clear explanation</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.5.5" class="ltx_tr">
<td id="S2.T1.1.5.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.5.1.1.1" class="ltx_p" style="width:173.4pt;">Students are asking questions and generating their ideas</span>
</span>
</td>
<td id="S2.T1.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.5.2.1.1" class="ltx_p" style="width:260.2pt;">Facilitating collaborative learning; Empowering learners; Encouraging learner engagement</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.6.6" class="ltx_tr">
<td id="S2.T1.1.6.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.6.1.1.1" class="ltx_p" style="width:173.4pt;">Laughter</span>
</span>
</td>
<td id="S2.T1.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.6.2.1.1" class="ltx_p" style="width:260.2pt;">Arousing interest</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.7.7" class="ltx_tr">
<td id="S2.T1.1.7.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.7.1.1.1" class="ltx_p" style="width:173.4pt;">Discipline</span>
</span>
</td>
<td id="S2.T1.1.7.7.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.7.2.1.1" class="ltx_p" style="width:260.2pt;">Maintaining positive discipline</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.8.8" class="ltx_tr">
<td id="S2.T1.1.8.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.8.8.1.1.1" class="ltx_p" style="width:173.4pt;">Students’ discussion</span>
</span>
</td>
<td id="S2.T1.1.8.8.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.8.8.2.1.1" class="ltx_p" style="width:260.2pt;">Facilitating collaborative learning; Empowering learners; Encouraging learner engagement</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.9.9" class="ltx_tr">
<td id="S2.T1.1.9.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.9.9.1.1.1" class="ltx_p" style="width:173.4pt;">Use of voice intonation to emphasize more important issues/topics</span>
</span>
</td>
<td id="S2.T1.1.9.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.9.9.2.1.1" class="ltx_p" style="width:260.2pt;">Encouraging learner engagement; Pacing and maintaining momentum</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.10.10" class="ltx_tr">
<td id="S2.T1.1.10.10.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2">Visual features (that can be detected on slides or the view of the teacher)</td>
</tr>
<tr id="S2.T1.1.11.11" class="ltx_tr">
<td id="S2.T1.1.11.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.1.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.11.11.1.1.1" class="ltx_p" style="width:173.4pt;">Films or animations in slides</span>
</span>
</td>
<td id="S2.T1.1.11.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.1.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.11.11.2.1.1" class="ltx_p" style="width:260.2pt;">Providing clear explanation; Arousing interest</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.12.12" class="ltx_tr">
<td id="S2.T1.1.12.12.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.12.12.1.1.1" class="ltx_p" style="width:173.4pt;">Images in slides</span>
</span>
</td>
<td id="S2.T1.1.12.12.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.12.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.12.12.2.1.1" class="ltx_p" style="width:260.2pt;">Providing clear explanation; Arousing interest</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.13.13" class="ltx_tr">
<td id="S2.T1.1.13.13.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.13.13.1.1.1" class="ltx_p" style="width:173.4pt;">Charts in slides</span>
</span>
</td>
<td id="S2.T1.1.13.13.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.13.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.13.13.2.1.1" class="ltx_p" style="width:260.2pt;">Providing clear explanation; Arousing interest</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.14.14" class="ltx_tr">
<td id="S2.T1.1.14.14.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.14.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.14.14.1.1.1" class="ltx_p" style="width:173.4pt;">Showing websites</span>
</span>
</td>
<td id="S2.T1.1.14.14.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.14.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.14.14.2.1.1" class="ltx_p" style="width:260.2pt;">Providing clear explanation; Arousing interest</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.15.15" class="ltx_tr">
<td id="S2.T1.1.15.15.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.15.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.15.15.1.1.1" class="ltx_p" style="width:173.4pt;">An active teacher stands by slides and explains them</span>
</span>
</td>
<td id="S2.T1.1.15.15.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.15.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.15.15.2.1.1" class="ltx_p" style="width:260.2pt;">Encouraging learner engagement</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.16.16" class="ltx_tr">
<td id="S2.T1.1.16.16.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.16.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.16.16.1.1.1" class="ltx_p" style="width:173.4pt;">Movement across podium</span>
</span>
</td>
<td id="S2.T1.1.16.16.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.16.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.16.16.2.1.1" class="ltx_p" style="width:260.2pt;">Encouraging learner engagement</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.17.17" class="ltx_tr">
<td id="S2.T1.1.17.17.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.17.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.17.17.1.1.1" class="ltx_p" style="width:173.4pt;">Writing on a whiteboard</span>
</span>
</td>
<td id="S2.T1.1.17.17.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.17.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.17.17.2.1.1" class="ltx_p" style="width:260.2pt;">Providing clear explanation; Arousing interest</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.18.18" class="ltx_tr">
<td id="S2.T1.1.18.18.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.18.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.18.18.1.1.1" class="ltx_p" style="width:173.4pt;">Writing on slides</span>
</span>
</td>
<td id="S2.T1.1.18.18.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.18.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.18.18.2.1.1" class="ltx_p" style="width:260.2pt;">Providing clear explanation; Arousing interest</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.19.19" class="ltx_tr">
<td id="S2.T1.1.19.19.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.19.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.19.19.1.1.1" class="ltx_p" style="width:173.4pt;">Demonstration</span>
</span>
</td>
<td id="S2.T1.1.19.19.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.19.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.19.19.2.1.1" class="ltx_p" style="width:260.2pt;">Arousing interest; Providing clear explanation</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.20.20" class="ltx_tr">
<td id="S2.T1.1.20.20.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.20.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.20.20.1.1.1" class="ltx_p" style="width:173.4pt;">Eye contact</span>
</span>
</td>
<td id="S2.T1.1.20.20.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.20.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.20.20.2.1.1" class="ltx_p" style="width:260.2pt;">Encouraging learner engagement</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.21.21" class="ltx_tr">
<td id="S2.T1.1.21.21.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2">Visual and/or audio-based features</td>
</tr>
<tr id="S2.T1.1.22.22" class="ltx_tr">
<td id="S2.T1.1.22.22.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.1.22.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.22.22.1.1.1" class="ltx_p" style="width:173.4pt;">Referring to the bibliography, other research</span>
</span>
</td>
<td id="S2.T1.1.22.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.1.22.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.22.22.2.1.1" class="ltx_p" style="width:260.2pt;">Deciding on teaching aids and learning resources</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.23.23" class="ltx_tr">
<td id="S2.T1.1.23.23.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.23.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.23.23.1.1.1" class="ltx_p" style="width:173.4pt;">Session on tests</span>
</span>
</td>
<td id="S2.T1.1.23.23.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.23.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.23.23.2.1.1" class="ltx_p" style="width:260.2pt;">Activating prior knowledge; Checking for understanding and providing feedback; Assessment</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.24.24" class="ltx_tr">
<td id="S2.T1.1.24.24.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.24.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.24.24.1.1.1" class="ltx_p" style="width:173.4pt;">Giving hints on how to do something</span>
</span>
</td>
<td id="S2.T1.1.24.24.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.24.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.24.24.2.1.1" class="ltx_p" style="width:260.2pt;">Activating prior knowledge; Encouraging learner engagements</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.25.25" class="ltx_tr">
<td id="S2.T1.1.25.25.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.25.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.25.25.1.1.1" class="ltx_p" style="width:173.4pt;">Organization: giving class outline, clearly indicating a transition from one topic to another</span>
</span>
</td>
<td id="S2.T1.1.25.25.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.25.25.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.25.25.2.1.1" class="ltx_p" style="width:260.2pt;">Setting expectations and routines; Establishing interaction and rapport, determining lesson objectives; Deciding on teaching aids and learning resources; Pacing and maintaining momentum</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.26.26" class="ltx_tr">
<td id="S2.T1.1.26.26.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.26.26.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.26.26.1.1.1" class="ltx_p" style="width:173.4pt;">Assignments</span>
</span>
</td>
<td id="S2.T1.1.26.26.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.1.26.26.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.26.26.2.1.1" class="ltx_p" style="width:260.2pt;">Supporting self-directed learning; Setting meaningful assignments</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.27.27" class="ltx_tr">
<td id="S2.T1.1.27.27.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S2.T1.1.27.27.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.27.27.1.1.1" class="ltx_p" style="width:173.4pt;">Summing up</span>
</span>
</td>
<td id="S2.T1.1.27.27.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S2.T1.1.27.27.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.27.27.2.1.1" class="ltx_p" style="width:260.2pt;">Concluding the lesson</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Set of didactic features implemented in this and the previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Collecting Dataset</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">After conceptualizing the didactic features, a dataset with those features has been collected and annotated.
Preparing the annotation dataset is a challenging, complex, and time-consuming task. In the case reported here, the process started with designing a set of clear and understandable features (as described above), continued through annotator training and coding. Next, the annotation cleaning and verification ensued, and the process finished with selecting the gold standard for training and testing ML models. This required the combined efforts of multiple people and the application of programs and automation tools. In the annotation process, to facilitate annotations of life events (the features) occurring through time, BORIS – a free and open-source Behavioral Observation Research Interactive Software (BORIS is dedicated for video/audio coding and live observations) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> has been used
. (see, Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2 Collecting Dataset ‣ 2 The Didactic Features and the Collected Dataset ‣ Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2406.14266/assets/x1.png" id="S2.F2.g1" class="ltx_graphics ltx_img_landscape" width="664" height="262" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example annotations in the dataset, a view from BORIS; P – indicated point event, S – state event</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">Finally, the partner university (Nanyang Technological University in Singapore) provided 128 lectures related to the Physics discipline (delivered by multiple lecturers), recorded in English (on average, one lecture lasts an hour and a half).<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We have an ethical acceptance from the NTU to use this dataset in the reported experiments.</span></span></span> At least three independent research assistants annotated every lecture. Each individual data element is a full-length annotation of didactic features present in a given lectures. Cumulatively, 380 observations were collected.
Each feature was identified as either a state or point event. State events last a specific time, i.e., they have intervals with a precisely defined start time and end time. Point events occur at a given moment and show the changes in the behaviours or the occurrence of a feature, without its specific anchoring in a given time interval. An event means a single didactic feature in a given lecture, annotated by a research assistant. An observation is a set of events for one lecture annotated by one research assistant. An annotation is a set of all observations created for one lecture.
The annotations were then used to train machine learning models.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p">In the next project phase, various methods how to pre-process and prepare the deep learning models to detect the selected features and how to design ML models to cope with them have been tried. It was necessary to prepare methods that allowed to split two sources of information within the recordings, i.e. the slide and the teacher views. Afterwards, both or separate views have been used to design ML models to apply to the visual features. Moreover, ML techniques needed to identify features in the the audio streams have been selected. Moreover, transcription models have been applied to the collect texts – the audio stream of the lectures. Detailed results of tests run with all explored techniques have been reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Here, the models have been improved with the latest state-of-the-art techniques, which have been utilize to build the system prototype.
The overarching goal of the process was to design and implement an interface that can automatically help teachers to get feedback and to visualize their lectures so that they can derive ways to improve their performance.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>System Design</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In the proposed system design, it has been assumed that the automatically detected features will be used for the objective summary evaluation, to provide feedback for the academic teachers on their lecturing style and effectiveness.
To achieve this goal the prototype’s graphic interface allows to upload a lecture video and get a list of didactic features occurring within it. Overall, the developed application is organized in a way that does not require technical knowledge. The user should obtain the annotation of didactic features, view the visualisation of features detected by the model, and see which part of the lecture was automatically annotated. In addition, users should be able to see the visualisation of trends, based on all of the lectures uploaded and annotated in the previous sessions.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The prototype’s functional requirements were defined assuming that the developed application has two types of users: Lecturer (User) and Administrator. Without the need for ML knowledge, the lecturer uses the application to get feedback on their lecturers (see, Figure <a href="#S3.F3" title="Figure 3 ‣ 3 System Design ‣ Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The Administrator, on the other hand, has machine learning knowledge and can change or upgrade the models, available in the system.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2406.14266/assets/images/usecases.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="509" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>System use case diagram</figcaption>
</figure>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">Moreover, the system was designed to fulfil the standard non-functional requirements for web applications, i.e. (i) usability – intuitive interface, widely-used programming standards to reuse the application source code, reliability; (ii) performance – short time of response for ML-models; and (iii) supportability – easy-to-change ML models, easy extending source code with up-to-date standards (e.g. Python, dash, plotly, pytorch, whisper, sqlite3).</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">The implemented prototype comprises six modules: <span id="S3.p4.1.1" class="ltx_text ltx_font_italic">Controller, GUI, Preprocessing, ML Models, Database Connector</span> and <span id="S3.p4.1.2" class="ltx_text ltx_font_italic">Database</span> (see, Figure <a href="#S3.F4" title="Figure 4 ‣ 3 System Design ‣ Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). The <span id="S3.p4.1.3" class="ltx_text ltx_font_italic">Controller</span> module communicates with the other modules, and runs the application. The <span id="S3.p4.1.4" class="ltx_text ltx_font_italic">Preprocessing</span> module transforms video into a format that can be passed to the models that are being trained, e.g. a sequence of images, only audio stream. Preprocessing of the video consists of two steps: (A) the transcription is created with the <span id="S3.p4.1.5" class="ltx_text ltx_font_typewriter">openai-whisper</span> package; (B) the JSON file with generated transcription is transformed into a data frame with the start and the end timestamps of the sentence. The <span id="S3.p4.1.6" class="ltx_text ltx_font_italic">ML Models</span> module stores imported from pytorch binary files, with the model weights and other parameters, which are then used to create predictions.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2406.14266/assets/x2.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Implemented prototype’s design – module diagram</figcaption>
</figure>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p">The <span id="S3.p5.1.1" class="ltx_text ltx_font_italic">GUI</span> module is responsible for creating the graphic interface and handling input from the user. To make the interface, the <span id="S3.p5.1.2" class="ltx_text ltx_font_typewriter">dash</span> framework,<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://dash.plotly.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dash.plotly.com</a></span></span></span> was used, as it allows integration of interactive plots into the web application. The graphic interface allows the users to upload a new video, get transcription, and view the analysis of predicted didactic features. The <span id="S3.p5.1.3" class="ltx_text ltx_font_italic">Database</span> module is where all datasets (including videos uploaded by the users) are stored.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Machine Learning Module</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In the developed prototype, the audio was used to create the transcription and, in subsequent steps, the transcription was used for building text-based models. Let us now describe it in some more detail.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Transcription Process</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Even though the errors of the transcription models could be propagated to the classification model, the method based on transcription achieved generally better results than the method employing audio characteristics, and this transcription and training, or predicting, pipeline takes less computation power. The transcription results were tested on six short, manually transcribed lecture fragments. Ultimately, selected was the tool with the best word error rate (WER) metric results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> – see Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Transcription Process ‣ 4 Machine Learning Module ‣ Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Deepspeech</span></td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Azure</span></td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Watson</span></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Vosk</span></td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">A-S-R</span></td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">Jasper</span></td>
<td id="S4.T2.1.1.1.7" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.7.1" class="ltx_text ltx_font_bold">GCP</span></td>
<td id="S4.T2.1.1.1.8" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.1.8.1" class="ltx_text ltx_font_bold">Whisper</span></td>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<td id="S4.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">50.18</td>
<td id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">19.02</td>
<td id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">47.15</td>
<td id="S4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">43.62</td>
<td id="S4.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">63.04</td>
<td id="S4.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">49.97</td>
<td id="S4.T2.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">34.26</td>
<td id="S4.T2.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.2.8.1" class="ltx_text ltx_font_bold">16.81</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>WER on test dataset for lecture transcription</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Text-based Models</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">The transcriptions obtained from the existing transcriptions tools were used to train two machine learning models: for the binary classification task, noting whether the given sentence contains a question, and for the multiclass classification task, which marks all relevant text-based didactic features.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">The state-of-the-art transfer learning with Transformer BERT-based models provided by HuggingFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite><span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://huggingface.co/roberta-base</span></span></span>, was used.
The dataset classification tasks are selected as the downstream tasks each time the training procedure and the pretraining are performed on another, larger dataset, the BookCorpus dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite><span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://huggingface.co/datasets/bookcorpus</span></span></span>. A feed-forward neural network head is added at the end of the Transformers, and predicts classes from the dataset. Default uncased BERT-Base model hyperparameters were not changed during the hyperparameter optimization process, and instead, the focus was on fine-tuning the last layer, the feed-forward network.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p">The models used for the recognition of features regarding questions achieved a precision of about 20%, recall of about 50%, and F1 score of about 30%. It is worth mentioning that the model recognised six features at once, so the random results would be about 16%. Hence, the obtained results seem reasonable, mainly because the dataset was relatively small, and the task was challenging to annotate.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>System Interface</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The implemented prototype automatically produced summaries of the detected didactic features. Figure <a href="#S5.F5" title="Figure 5 ‣ 5 System Interface ‣ Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows an interface with the results for a single lecture recording with text-based features, e.g. ’asking questions’ and ’organizational issues’. This visualisation contains a number of occurrences of each feature during a given lecture and varying time spans for each feature. The first plot is a bar graph with the number of occurrences of every feature. The second plot’s x-axis shows time in minutes; on the y-axis, there are didactic features. Every point represents a single occurrence of a feature in the lecture.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2406.14266/assets/images/plots-text-model-results.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Displaying plots with a summary of didactic features that appeared during the selected lecture: number and time of occurrences of features predicted by the ML model</figcaption>
</figure>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">The user can also see and download the transcription results combined with the results of the text-based models, showing in which sentences any text-based features occurred. This functionality is available when the Text or the Questions model is selected. A dropdown list with features that have been recognized in the selected lecture(s) is displayed. After selecting one, the user can see a table with all moments where the model predicted the occurrence of the feature, with text, time of start, and time of the end of each occurrence. An example of a table look is shown in Figure <a href="#S5.F6" title="Figure 6 ‣ 5 System Interface ‣ Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2406.14266/assets/images/plot-transcription-with-features.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="154" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Displaying table with text fragments connected to chosen didactic feature and time interval of its occurrence.</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Concluding Remarks</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">The goal of the reported research was to develop and implement a prototype of an academic lecturer support system, based on application of ML-models to the lecture video recordings.
To achieve this goal a set of didactic features was selected and adapted to objectively and quantitatively evaluate and compare academic lectures. Moreover, the implemented prototype supports automating the annotation process of lecture video recordings using pre-trained machine-learning models. Utilizing these annotations, summaries and visualizations of the didactic features observed within the lectures have been created. The developed system is anchored in in-depth analysis of required system’s functionalities, which guided the design of its architecture, dataflow, and interfaces. The modularity of the designed system allows easy addition of ne ML models, if such models will be found to be promising for the task at hand.
Obtained results, validating the approach, are reasonable, taking into account the size of the dataset used for model training.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">The primary target of future research has to be the collection of a larger dataset to be used for model training. Here, what is worthy repeating is that the developed prototype can be used to form a partially-supervised continual learning loop. Specifically, the existing system can be used for preliminary didactic feature extraction, helping annotate future videos (human-in-the loop). These videos can then be used to up-train the model(s).</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p">The dataset and the system’s software is available upon request from the corresponding author. However, note that restrictions related to the GDPR (and other pertinent regulations) apply, and applicable documents will guide granting access to both the dataset and the code.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">This work is done in cooperation with Nanyang Technological University, in the frame of OMINO (Overcoming Multilevel Information Overload) grant (no 101086321) funded by the European Union under the Horizon Europe and by the Polish Ministry of Education and Science within the International Projects Co-Financed program.
(However, the views and opinions expressed are those of the authors only and do not necessarily reflect those of the European Union or the European Research Executive Agency. Neither the European Union nor the European Research Executive Agency can be held responsible for them.)</p>
</div>
<div id="Sx1.p2" class="ltx_para ltx_noindent">
<p id="Sx1.p2.1" class="ltx_p">This research was also carried out with the support of the Faculty of Mathematics and Information Science at Warsaw University of Technology, its Laboratory of Bioinformatics and Computational Genomics, and the High-Performance Computing Center.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Danijel Kucak, Vedran Juricic, and Goran Dambic.

</span>
<span class="ltx_bibblock">Machine learning in education - a survey of current research trends.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">DAAM Int. Symp. on Intelligent Manufacturing and Automation</span>, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Fengchun Miao and Wayne Holmes.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Guidance for generative AI in education and research</span>.

</span>
<span class="ltx_bibblock">UNESCO, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
VK Anand, SK Abdul Rahiman, E Ben George, and AS Huda.

</span>
<span class="ltx_bibblock">Recursive clustering technique for students’ performance evaluation in programming courses.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE Majan Int. Conf.</span>, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Xiaojin Zhu.

</span>
<span class="ltx_bibblock">Machine teaching: An inverse problem to machine learning and an approach toward optimal education.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">AAAI Conference on AI</span>, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Wayne Holmes, Sun Meng, and Li Yuan.

</span>
<span class="ltx_bibblock">Artificial intelligence and education: Digging beneath the surface.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">The Chinese Journal of ICT in Education</span>, 2023(2):16–26, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Avi Segal, Shaked Hindi, Naomi Prusak, Osama Swidan, Adva Livni, Alik Palatnic, Baruch Schwarz, and Ya’akov Gal.

</span>
<span class="ltx_bibblock">Keeping the teacher in the loop: Technologies for monitoring group learning in real-time.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence in Education: 18th International Conference, AIED 2017, Wuhan, China, June 28–July 1, 2017, Proceedings 18</span>, pages 64–76. Springer, 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
David M Broussard, Yitoshee Rahman, Arun K Kulshreshth, and Christoph W Borst.

</span>
<span class="ltx_bibblock">An interface for enhanced teacher awareness of student actions and attention in a vr classroom.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)</span>, pages 284–290. IEEE, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Eric Tarantini.

</span>
<span class="ltx_bibblock">Reflective teacher education in the digital age: 360° video reflection and ai-based developments.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Open and Inclusive Educational Practice in the Digital World</span>, pages 213–231. Springer, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Lauraine Langreo.

</span>
<span class="ltx_bibblock">Can AI do teacher observations.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Junqi Guo, Ludi Bai, Zehui Yu, Ziyun Zhao, and Boxin Wan.

</span>
<span class="ltx_bibblock">An ai-application-oriented in-class teaching evaluation model by using statistical modeling and ensemble learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Sensors</span>, 21(1):241, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Anna Wróblewska, Józef Jasek, Bogdan Jastrzebski, Stanisław Pawlak, Anna Grzywacz, Siew Ann Cheong, Seng Chee Tan, Tomasz Trzciński, and Janusz Hołyst.

</span>
<span class="ltx_bibblock">Deep learning for automatic detection of qualitative features of lecturing.

</span>
<span class="ltx_bibblock">In Maria Mercedes Rodrigo, Noburu Matsuda, Alexandra I. Cristea, and Vania Dimitrova, editors, <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence in Education</span>, pages 698–703, Cham, 2022. Springer International Publishing.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Michael Piburn and Daiyo Sawada.

</span>
<span class="ltx_bibblock">Reformed teaching observation protocol (RTOP) reference manual.

</span>
<span class="ltx_bibblock">Technical report, Arizona Collaborative for Excellence in the Preparation of Teachers (ERIC ED447205), 2000.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Singapore Ministry of Education.

</span>
<span class="ltx_bibblock">The Singapore Teaching Practice (STP), 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Olivier Friard and Marco Gamba.

</span>
<span class="ltx_bibblock">Boris: a free, versatile open-source event-logging software for video/audio coding and live observations.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Methods in Ecology and Evolution</span>, 2016.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Iain A McCowan, Darren Moore, John Dines, Daniel Gatica-Perez, Mike Flynn, Pierre Wellner, and Hervé Bourlard.

</span>
<span class="ltx_bibblock">On the use of information retrieval measures for speech recognition evaluation.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Reporte de investigación 04-73</span>, 2005.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Ro{bert}a: A robustly optimized {bert} pretraining approach, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.

</span>
<span class="ltx_bibblock">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IEEE ICCV</span>, 2015.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.14265" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.14266" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.14266">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.14266" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.14267" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Jul  6 00:09:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
