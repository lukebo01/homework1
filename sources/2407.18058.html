<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.18058] I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition</title><meta property="og:description" content="Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for cla…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.18058">

<!--Generated on Mon Aug  5 18:34:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">I can listen but cannot read: 
<br class="ltx_break">An evaluation of two-tower multimodal systems 
<br class="ltx_break">for instrument recognition</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Music two-tower multimodal systems integrate audio and text modalities into a joint audio-text space, enabling direct comparison between songs and their corresponding labels. These systems enable new approaches for classification and retrieval, leveraging both modalities. Despite the promising results they have shown for zero-shot classification and retrieval tasks, closer inspection of the embeddings is needed. This paper evaluates the inherent zero-shot properties of joint audio-text spaces for the case-study of instrument recognition. We present an evaluation and analysis of two-tower systems for zero-shot instrument recognition and a detailed analysis of the properties of the pre-joint and joint embedding spaces. Our findings suggest that audio encoders alone demonstrate good quality, while challenges remain within the text encoder or joint space projection. Specifically, two-tower systems exhibit sensitivity towards specific words, favoring generic prompts over musically informed ones. Despite the large size of textual encoders, they do not yet leverage additional textual context or infer instruments accurately from their descriptions. Lastly, a novel approach for quantifying the semantic meaningfulness of the textual space leveraging an instrument ontology is proposed. This method reveals deficiencies in the systems’ understanding of instruments and provides evidence of the need for fine-tuning text encoders on musical data.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Multiclass classification has been a heavily researched topic in Music Information Retrieval (MIR) with many concrete applications such as genre, instrument and emotion recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Despite the success of Deep (DL) systems for such tasks, recurring deficiencies persist among these problems. These are: (1) the limited availability of large-scale annotated datasets curated by experts, (2) the restricted capability of these systems to infer only a set of predefined classes.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<p id="S1.F1.1" class="ltx_p ltx_align_center"><span id="S1.F1.1.1" class="ltx_text"><img src="/html/2407.18058/assets/figs/final_two_tower_system_overview.jpg" id="S1.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="138" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text ltx_font_bold">Figure 1</span>: </span>Figure of a pipeline for two-tower multimodal systems. A separate model for each modality is used and their individual representations are projected to a joint audio-text space through a Multi-Layer Perceptron (MLP). This enables direct comparison between audio and textual data. We refer to embeddings obtained before joint-space projection as pre-joint space embeddings.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Music is an ever-evolving art form and as a result, there is an inherent need to make these systems adaptable to new terms/classes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and infer task-agnostic representations that can be useful for a plethora of downstream tasks with representation learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Zero-shot learning (ZSL) is focused on estimating a classifier capable of inferring unseen, new classes without annotated examples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. ZSL is often achieved in either of two ways: (1) by decomposing each class into attributes and inferring unseen classes through their related attributes (e.g. genres decomposed into presence or absence of instruments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>) or (2) using word embeddings from Language Models (LM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The success of contextualized Large Language Models (LLM) has driven the research community predominantly toward the second solution, as it doesn’t require experts to define attributes and the mapping between classes and attributes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">As a result, ZSL for audio classification is primarily focused on connecting the audio and semantic representation spaces. This interconnection can happen in 2 main ways: (1) mapping the audio representations to text space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, or (2) mapping both of the spaces to a new, joint audio-text space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The systems of the second category are named two-tower multimodal systems, where pre-trained audio models and LMs are used as the audio and text encoders respectively. Representations obtained from each modality are then mapped to a joint audio-text space and systems are jointly optimized such that the audio and text representation are close in the joint space (e.g. the phrase “A rock song track” is similar to the recording of a “rock” song). We will call such representations as embeddings from here on.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This work aims to better understand the properties of existing two-tower systems. We use instrument classification as a case-study to provide insights into the presence (or absence) of semantic properties in the audio, text or joint spaces in addition to reporting classification metrics. Concretely, we consider 3 systems: MusCALL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, a CLAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> model trained on speech and music datasets, and a CLAP model trained on music data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. We evaluate the performance of these systems on instrument classification using the TinySOL dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. We would like to highlight that multimodal DL models typically excel at simple tasks and datasets like this.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Furthermore, a novel approach for quantifying the semantic meaningfulness of textual encoders for instrument recognition is proposed.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">For reproducibility, our experiments are performed on open-source datasets and the code of our experiments is made publicly available<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/YannisBilly/i_can_listen_but_cannot_read</span></span></span></span>, such that they can be reproduced.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Zero-shot transfer</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">ZSL focuses on estimating classifiers for novel, unseen classes without annotated examples. Two-tower systems are not primarily optimized for ZSL but due to the pretrained textual encoder, novel words or phrases can be interpreted during inference. This property is known as zero-shot transfer (ZST) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Side-information can be used in multiple ways that fall into two categories: (1) decomposing classes into shared attributes and (2) using LMs to represent this information as a text embedding. Despite its success, the first solution requires experts to effectively estimate the relevance of attributes and several classes and is a costly activity. As a result and due to the remarkable results obtained through contextualized LLMs, research has focused on the second option.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Generally, the methodology can be broken down into 3 components: (1) an audio encoder, (2) a textual encoder and (3) a projection to a common space. General purpose audio DL models that have been used as the audio encoder include VGGish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, PANN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, HTS-AT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and Audio Spectrogram Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. For the textual encoder, distributional LMs like GloVE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, Word2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and contextualized LMs like BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> have been thoroughly tested. As each modality produces heterogeneous representations, different methods of establishing comparability have been tested. This is predominantly achieved through projecting audio to text/semantic space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> or a novel, joint audio-text space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Two-tower multimodal systems</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Multimodal systems aim to represent data with additional knowledge from multiple modalities. Examples are audio combined with images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> or a combination thereof.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Two-tower multimodal systems focus on combining the textual and audio modalities by projecting them in a joint audio-text space. In that space, words that are relevant to a specific song will produce embeddings that will be close in terms of some similarity metric. An illustration of a two-tower system is presented in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Information flowing through the audio encoder or textual encoder is referred to as the audio and textual branch respectively. We are also interested in the embeddings obtained through the encoders before projecting them into the joint audio-text space. We will call these the pre-joint spaces from now on.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">The text used during training is usually a description of a song and will be referred to as a caption. The text used during inference will be referred to as a prompt.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">MusCALL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> combined a ResNET-50 for audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> with a Transformer for text encoding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, optimized jointly over InfoNCE contrastive loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Additionally, a weighing mechanism based on caption-caption similarity was incorporated between negative audio-caption pairs. This is based on the premise that similar captions will be given to similar audio. The audio used is private but the training code is publicly available and used for this work.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">MuLan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> experimented with ResNET-50 as well as Audio Spectrogram Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> for audio encoding and a pretrained BERT model for the text branch. Both were jointly optimized over the Contrastive Multiview Coding loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, which is a cross-modal extension of InfoNCE. Neither the data nor the code is available.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">LAION-CLAP tested 6 different combinations of audio and text embedding models, the best one of which was HTS-AT with RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. The latter is the one that will be used in this paper. The LAION-Audio-630k dataset was formed by combining AudioCaps, CLotho and Audioset.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p">Generally, research for two-tower systems is limited to testing different combinations of audio and text encoders, optimized jointly over a form of contrastive loss and modalitity fusion. We believe that closer inspection of their embeddings and evaluation protocol is needed.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation of two-tower systems</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset and models</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We use the TinySOL dataset which contains 2913 audio clips with a single note played from a single instrument out of a set of 14 instrument classes. This dataset has been chosen as it has consistent recording settings without noise, it is a simple dataset for instrument recognition and finally, confounding factors (compression, sampling rate etc) are minimized.
We consider 3 models in total:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Music/Speech CLAP</span>: <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> A CLAP-based model trained on music/speech data<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>music_speech_epoch_15_esc_89.25.pt</span></span></span></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Music CLAP</span>: <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> A CLAP-based model trained on music data<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>music_audioset_epoch_15_esc_90.14.pt</span></span></span></p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">MusCALL</span>: A version of  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, retrained on music data<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/ilaria-manco/muscall</span></span></span></p>
</div>
</li>
</ol>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We use the two pretrained CLAP systems provided by LAION<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/LAION-AI/CLAP</span></span></span>.
For this work, the original MusCALL implementation was retrained from scratch, as both the data and trained models used in the original paper are not publicly available.
Instead, we train on the LPMusicCaps-MTT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> dataset, which is built by leveraging the audio and 188 tags from Magna Tag A Tune<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> to artificially generate captions through a GPT-3.5 model.
The audio is resampled to 44.1 and 16 KHz for CLAP and MusCALL respectively, and the pre-processing steps described in their respective code repositories are followed.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Zero-shot transfer for instrument classification</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.10" class="ltx_p">Given an unseen audio segment <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="x^{*}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msup id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">x</mi><mo id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝑥</ci><times id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">x^{*}</annotation></semantics></math>, a text label <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="l^{*}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msup id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">l</mi><mo id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑙</ci><times id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">l^{*}</annotation></semantics></math> and a two-tower system <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="f(x)" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.2" xref="S3.SS2.p1.3.m3.1.2.cmml"><mi id="S3.SS2.p1.3.m3.1.2.2" xref="S3.SS2.p1.3.m3.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.2.1" xref="S3.SS2.p1.3.m3.1.2.1.cmml">​</mo><mrow id="S3.SS2.p1.3.m3.1.2.3.2" xref="S3.SS2.p1.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.3.m3.1.2.3.2.1" xref="S3.SS2.p1.3.m3.1.2.cmml">(</mo><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS2.p1.3.m3.1.2.3.2.2" xref="S3.SS2.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.2.cmml" xref="S3.SS2.p1.3.m3.1.2"><times id="S3.SS2.p1.3.m3.1.2.1.cmml" xref="S3.SS2.p1.3.m3.1.2.1"></times><ci id="S3.SS2.p1.3.m3.1.2.2.cmml" xref="S3.SS2.p1.3.m3.1.2.2">𝑓</ci><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">f(x)</annotation></semantics></math>, we want to model the likelihood <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="P(l^{*}|x^{*})" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">​</mo><mrow id="S3.SS2.p1.4.m4.1.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p1.4.m4.1.1.1.1.2" xref="S3.SS2.p1.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.4.m4.1.1.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.1.1.cmml"><msup id="S3.SS2.p1.4.m4.1.1.1.1.1.2" xref="S3.SS2.p1.4.m4.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p1.4.m4.1.1.1.1.1.2.2" xref="S3.SS2.p1.4.m4.1.1.1.1.1.2.2.cmml">l</mi><mo id="S3.SS2.p1.4.m4.1.1.1.1.1.2.3" xref="S3.SS2.p1.4.m4.1.1.1.1.1.2.3.cmml">∗</mo></msup><mo fence="false" id="S3.SS2.p1.4.m4.1.1.1.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1.cmml">|</mo><msup id="S3.SS2.p1.4.m4.1.1.1.1.1.3" xref="S3.SS2.p1.4.m4.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p1.4.m4.1.1.1.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.1.1.1.3.2.cmml">x</mi><mo id="S3.SS2.p1.4.m4.1.1.1.1.1.3.3" xref="S3.SS2.p1.4.m4.1.1.1.1.1.3.3.cmml">∗</mo></msup></mrow><mo stretchy="false" id="S3.SS2.p1.4.m4.1.1.1.1.3" xref="S3.SS2.p1.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><times id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2"></times><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">𝑃</ci><apply id="S3.SS2.p1.4.m4.1.1.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.1">conditional</csymbol><apply id="S3.SS2.p1.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.2.2">𝑙</ci><times id="S3.SS2.p1.4.m4.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.2.3"></times></apply><apply id="S3.SS2.p1.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.3.2">𝑥</ci><times id="S3.SS2.p1.4.m4.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.1.1.1.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">P(l^{*}|x^{*})</annotation></semantics></math> based on the embeddings provided by <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="f(x)" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mrow id="S3.SS2.p1.5.m5.1.2" xref="S3.SS2.p1.5.m5.1.2.cmml"><mi id="S3.SS2.p1.5.m5.1.2.2" xref="S3.SS2.p1.5.m5.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m5.1.2.1" xref="S3.SS2.p1.5.m5.1.2.1.cmml">​</mo><mrow id="S3.SS2.p1.5.m5.1.2.3.2" xref="S3.SS2.p1.5.m5.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.5.m5.1.2.3.2.1" xref="S3.SS2.p1.5.m5.1.2.cmml">(</mo><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS2.p1.5.m5.1.2.3.2.2" xref="S3.SS2.p1.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.2.cmml" xref="S3.SS2.p1.5.m5.1.2"><times id="S3.SS2.p1.5.m5.1.2.1.cmml" xref="S3.SS2.p1.5.m5.1.2.1"></times><ci id="S3.SS2.p1.5.m5.1.2.2.cmml" xref="S3.SS2.p1.5.m5.1.2.2">𝑓</ci><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">f(x)</annotation></semantics></math>. In the general case, <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="f\mapsto\mathbb{R}^{F}" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mrow id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">f</mi><mo stretchy="false" id="S3.SS2.p1.6.m6.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.cmml">↦</mo><msup id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml"><mi id="S3.SS2.p1.6.m6.1.1.3.2" xref="S3.SS2.p1.6.m6.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS2.p1.6.m6.1.1.3.3" xref="S3.SS2.p1.6.m6.1.1.3.3.cmml">F</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="latexml" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1">maps-to</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">𝑓</ci><apply id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.3.1.cmml" xref="S3.SS2.p1.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.3.2.cmml" xref="S3.SS2.p1.6.m6.1.1.3.2">ℝ</ci><ci id="S3.SS2.p1.6.m6.1.1.3.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3.3">𝐹</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">f\mapsto\mathbb{R}^{F}</annotation></semantics></math> is a function that represents a two-tower system and maps audio or text information to a joint audio-text space, where <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">F</annotation></semantics></math> is the dimension of the joint space. Also, let <math id="S3.SS2.p1.8.m8.1" class="ltx_Math" alttext="\delta:(\mathbb{R}^{F}\text{ x }\mathbb{R}^{F})\rightarrow\mathbb{R}" display="inline"><semantics id="S3.SS2.p1.8.m8.1a"><mrow id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml"><mi id="S3.SS2.p1.8.m8.1.1.3" xref="S3.SS2.p1.8.m8.1.1.3.cmml">δ</mi><mo lspace="0.278em" rspace="0.278em" id="S3.SS2.p1.8.m8.1.1.2" xref="S3.SS2.p1.8.m8.1.1.2.cmml">:</mo><mrow id="S3.SS2.p1.8.m8.1.1.1" xref="S3.SS2.p1.8.m8.1.1.1.cmml"><mrow id="S3.SS2.p1.8.m8.1.1.1.1.1" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p1.8.m8.1.1.1.1.1.2" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.8.m8.1.1.1.1.1.1" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.cmml"><msup id="S3.SS2.p1.8.m8.1.1.1.1.1.1.2" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p1.8.m8.1.1.1.1.1.1.2.2" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.2.2.cmml">ℝ</mi><mi id="S3.SS2.p1.8.m8.1.1.1.1.1.1.2.3" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.2.3.cmml">F</mi></msup><mo lspace="0em" rspace="0em" id="S3.SS2.p1.8.m8.1.1.1.1.1.1.1" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.1.cmml">​</mo><mtext id="S3.SS2.p1.8.m8.1.1.1.1.1.1.3" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.3a.cmml"> x </mtext><mo lspace="0em" rspace="0em" id="S3.SS2.p1.8.m8.1.1.1.1.1.1.1a" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.1.cmml">​</mo><msup id="S3.SS2.p1.8.m8.1.1.1.1.1.1.4" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.4.cmml"><mi id="S3.SS2.p1.8.m8.1.1.1.1.1.1.4.2" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.4.2.cmml">ℝ</mi><mi id="S3.SS2.p1.8.m8.1.1.1.1.1.1.4.3" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.4.3.cmml">F</mi></msup></mrow><mo stretchy="false" id="S3.SS2.p1.8.m8.1.1.1.1.1.3" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.cmml">)</mo></mrow><mo stretchy="false" id="S3.SS2.p1.8.m8.1.1.1.2" xref="S3.SS2.p1.8.m8.1.1.1.2.cmml">→</mo><mi id="S3.SS2.p1.8.m8.1.1.1.3" xref="S3.SS2.p1.8.m8.1.1.1.3.cmml">ℝ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><apply id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1"><ci id="S3.SS2.p1.8.m8.1.1.2.cmml" xref="S3.SS2.p1.8.m8.1.1.2">:</ci><ci id="S3.SS2.p1.8.m8.1.1.3.cmml" xref="S3.SS2.p1.8.m8.1.1.3">𝛿</ci><apply id="S3.SS2.p1.8.m8.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1.1"><ci id="S3.SS2.p1.8.m8.1.1.1.2.cmml" xref="S3.SS2.p1.8.m8.1.1.1.2">→</ci><apply id="S3.SS2.p1.8.m8.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1.1.1.1"><times id="S3.SS2.p1.8.m8.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.1"></times><apply id="S3.SS2.p1.8.m8.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m8.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS2.p1.8.m8.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.2.2">ℝ</ci><ci id="S3.SS2.p1.8.m8.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.2.3">𝐹</ci></apply><ci id="S3.SS2.p1.8.m8.1.1.1.1.1.1.3a.cmml" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.3"><mtext id="S3.SS2.p1.8.m8.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.3"> x </mtext></ci><apply id="S3.SS2.p1.8.m8.1.1.1.1.1.1.4.cmml" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m8.1.1.1.1.1.1.4.1.cmml" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.4">superscript</csymbol><ci id="S3.SS2.p1.8.m8.1.1.1.1.1.1.4.2.cmml" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.4.2">ℝ</ci><ci id="S3.SS2.p1.8.m8.1.1.1.1.1.1.4.3.cmml" xref="S3.SS2.p1.8.m8.1.1.1.1.1.1.4.3">𝐹</ci></apply></apply><ci id="S3.SS2.p1.8.m8.1.1.1.3.cmml" xref="S3.SS2.p1.8.m8.1.1.1.3">ℝ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">\delta:(\mathbb{R}^{F}\text{ x }\mathbb{R}^{F})\rightarrow\mathbb{R}</annotation></semantics></math> be a function that measures similarity between joint space embeddings. In this approach, we model the <math id="S3.SS2.p1.9.m9.1" class="ltx_Math" alttext="P(l^{*}|x^{*})" display="inline"><semantics id="S3.SS2.p1.9.m9.1a"><mrow id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml"><mi id="S3.SS2.p1.9.m9.1.1.3" xref="S3.SS2.p1.9.m9.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.9.m9.1.1.2" xref="S3.SS2.p1.9.m9.1.1.2.cmml">​</mo><mrow id="S3.SS2.p1.9.m9.1.1.1.1" xref="S3.SS2.p1.9.m9.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p1.9.m9.1.1.1.1.2" xref="S3.SS2.p1.9.m9.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.9.m9.1.1.1.1.1" xref="S3.SS2.p1.9.m9.1.1.1.1.1.cmml"><msup id="S3.SS2.p1.9.m9.1.1.1.1.1.2" xref="S3.SS2.p1.9.m9.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p1.9.m9.1.1.1.1.1.2.2" xref="S3.SS2.p1.9.m9.1.1.1.1.1.2.2.cmml">l</mi><mo id="S3.SS2.p1.9.m9.1.1.1.1.1.2.3" xref="S3.SS2.p1.9.m9.1.1.1.1.1.2.3.cmml">∗</mo></msup><mo fence="false" id="S3.SS2.p1.9.m9.1.1.1.1.1.1" xref="S3.SS2.p1.9.m9.1.1.1.1.1.1.cmml">|</mo><msup id="S3.SS2.p1.9.m9.1.1.1.1.1.3" xref="S3.SS2.p1.9.m9.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p1.9.m9.1.1.1.1.1.3.2" xref="S3.SS2.p1.9.m9.1.1.1.1.1.3.2.cmml">x</mi><mo id="S3.SS2.p1.9.m9.1.1.1.1.1.3.3" xref="S3.SS2.p1.9.m9.1.1.1.1.1.3.3.cmml">∗</mo></msup></mrow><mo stretchy="false" id="S3.SS2.p1.9.m9.1.1.1.1.3" xref="S3.SS2.p1.9.m9.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><apply id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1"><times id="S3.SS2.p1.9.m9.1.1.2.cmml" xref="S3.SS2.p1.9.m9.1.1.2"></times><ci id="S3.SS2.p1.9.m9.1.1.3.cmml" xref="S3.SS2.p1.9.m9.1.1.3">𝑃</ci><apply id="S3.SS2.p1.9.m9.1.1.1.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p1.9.m9.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1.1.1.1.1">conditional</csymbol><apply id="S3.SS2.p1.9.m9.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.9.m9.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.9.m9.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.9.m9.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS2.p1.9.m9.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.9.m9.1.1.1.1.1.2.2">𝑙</ci><times id="S3.SS2.p1.9.m9.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p1.9.m9.1.1.1.1.1.2.3"></times></apply><apply id="S3.SS2.p1.9.m9.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.9.m9.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.9.m9.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p1.9.m9.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.9.m9.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p1.9.m9.1.1.1.1.1.3.2">𝑥</ci><times id="S3.SS2.p1.9.m9.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p1.9.m9.1.1.1.1.1.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">P(l^{*}|x^{*})</annotation></semantics></math> based on <math id="S3.SS2.p1.10.m10.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S3.SS2.p1.10.m10.1a"><mi id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b"><ci id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">\delta</annotation></semantics></math>, as in:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="P(l^{*}|x^{*})\propto\delta(f(x^{*}),f(l^{*}))" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><msup id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.cmml">l</mi><mo id="S3.E1.m1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml">∗</mo></msup><mo fence="false" id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">|</mo><msup id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.cmml">x</mi><mo id="S3.E1.m1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.cmml">∗</mo></msup></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.4" xref="S3.E1.m1.3.3.4.cmml">∝</mo><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><mi id="S3.E1.m1.3.3.3.4" xref="S3.E1.m1.3.3.3.4.cmml">δ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml">​</mo><mrow id="S3.E1.m1.3.3.3.2.2" xref="S3.E1.m1.3.3.3.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.3.2.2.3" xref="S3.E1.m1.3.3.3.2.3.cmml">(</mo><mrow id="S3.E1.m1.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.2.1.1.1.1.1.1.cmml">(</mo><msup id="S3.E1.m1.2.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.2.1.1.1.1.1.1.2.cmml">x</mi><mo id="S3.E1.m1.2.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.2.1.1.1.1.1.1.3.cmml">∗</mo></msup><mo stretchy="false" id="S3.E1.m1.2.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.3.2.2.4" xref="S3.E1.m1.3.3.3.2.3.cmml">,</mo><mrow id="S3.E1.m1.3.3.3.2.2.2" xref="S3.E1.m1.3.3.3.2.2.2.cmml"><mi id="S3.E1.m1.3.3.3.2.2.2.3" xref="S3.E1.m1.3.3.3.2.2.2.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.2.2.2.2" xref="S3.E1.m1.3.3.3.2.2.2.2.cmml">​</mo><mrow id="S3.E1.m1.3.3.3.2.2.2.1.1" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.3.2.2.2.1.1.2" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.cmml">(</mo><msup id="S3.E1.m1.3.3.3.2.2.2.1.1.1" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.3.3.3.2.2.2.1.1.1.2" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.2.cmml">l</mi><mo id="S3.E1.m1.3.3.3.2.2.2.1.1.1.3" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.3.cmml">∗</mo></msup><mo stretchy="false" id="S3.E1.m1.3.3.3.2.2.2.1.1.3" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.3.3.3.2.2.5" xref="S3.E1.m1.3.3.3.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><csymbol cd="latexml" id="S3.E1.m1.3.3.4.cmml" xref="S3.E1.m1.3.3.4">proportional-to</csymbol><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3">𝑃</ci><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2">𝑙</ci><times id="S3.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3"></times></apply><apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2">𝑥</ci><times id="S3.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3"></times></apply></apply></apply><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><times id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3"></times><ci id="S3.E1.m1.3.3.3.4.cmml" xref="S3.E1.m1.3.3.3.4">𝛿</ci><interval closure="open" id="S3.E1.m1.3.3.3.2.3.cmml" xref="S3.E1.m1.3.3.3.2.2"><apply id="S3.E1.m1.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1"><times id="S3.E1.m1.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.2"></times><ci id="S3.E1.m1.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.3">𝑓</ci><apply id="S3.E1.m1.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.1.1.2">𝑥</ci><times id="S3.E1.m1.2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.1.1.1.3"></times></apply></apply><apply id="S3.E1.m1.3.3.3.2.2.2.cmml" xref="S3.E1.m1.3.3.3.2.2.2"><times id="S3.E1.m1.3.3.3.2.2.2.2.cmml" xref="S3.E1.m1.3.3.3.2.2.2.2"></times><ci id="S3.E1.m1.3.3.3.2.2.2.3.cmml" xref="S3.E1.m1.3.3.3.2.2.2.3">𝑓</ci><apply id="S3.E1.m1.3.3.3.2.2.2.1.1.1.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1.1">superscript</csymbol><ci id="S3.E1.m1.3.3.3.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.2">𝑙</ci><times id="S3.E1.m1.3.3.3.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1.1.1.3"></times></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">P(l^{*}|x^{*})\propto\delta(f(x^{*}),f(l^{*}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Multiclass classification attributes the most probable class to each recording and equivalently, the one that has the maximum likelihood. Given our approximation, the output class for each recording is:</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.3" class="ltx_Math" alttext="c^{*}=\operatorname*{argmax}_{c\in C}\delta(f(x^{*}),f(c))" display="block"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml"><msup id="S3.E2.m1.3.3.4" xref="S3.E2.m1.3.3.4.cmml"><mi id="S3.E2.m1.3.3.4.2" xref="S3.E2.m1.3.3.4.2.cmml">c</mi><mo id="S3.E2.m1.3.3.4.3" xref="S3.E2.m1.3.3.4.3.cmml">∗</mo></msup><mo rspace="0.1389em" id="S3.E2.m1.3.3.3" xref="S3.E2.m1.3.3.3.cmml">=</mo><mrow id="S3.E2.m1.3.3.2" xref="S3.E2.m1.3.3.2.cmml"><mrow id="S3.E2.m1.3.3.2.4" xref="S3.E2.m1.3.3.2.4.cmml"><munder id="S3.E2.m1.3.3.2.4.1" xref="S3.E2.m1.3.3.2.4.1.cmml"><mo lspace="0.1389em" rspace="0.167em" id="S3.E2.m1.3.3.2.4.1.2" xref="S3.E2.m1.3.3.2.4.1.2.cmml">argmax</mo><mrow id="S3.E2.m1.3.3.2.4.1.3" xref="S3.E2.m1.3.3.2.4.1.3.cmml"><mi id="S3.E2.m1.3.3.2.4.1.3.2" xref="S3.E2.m1.3.3.2.4.1.3.2.cmml">c</mi><mo id="S3.E2.m1.3.3.2.4.1.3.1" xref="S3.E2.m1.3.3.2.4.1.3.1.cmml">∈</mo><mi id="S3.E2.m1.3.3.2.4.1.3.3" xref="S3.E2.m1.3.3.2.4.1.3.3.cmml">C</mi></mrow></munder><mi id="S3.E2.m1.3.3.2.4.2" xref="S3.E2.m1.3.3.2.4.2.cmml">δ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.2.3" xref="S3.E2.m1.3.3.2.3.cmml">​</mo><mrow id="S3.E2.m1.3.3.2.2.2" xref="S3.E2.m1.3.3.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.2.2.2.3" xref="S3.E2.m1.3.3.2.2.3.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">x</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml">∗</mo></msup><mo stretchy="false" id="S3.E2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.2.2.2.4" xref="S3.E2.m1.3.3.2.2.3.cmml">,</mo><mrow id="S3.E2.m1.3.3.2.2.2.2" xref="S3.E2.m1.3.3.2.2.2.2.cmml"><mi id="S3.E2.m1.3.3.2.2.2.2.2" xref="S3.E2.m1.3.3.2.2.2.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.2.2.2.2.1" xref="S3.E2.m1.3.3.2.2.2.2.1.cmml">​</mo><mrow id="S3.E2.m1.3.3.2.2.2.2.3.2" xref="S3.E2.m1.3.3.2.2.2.2.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.2.2.2.2.3.2.1" xref="S3.E2.m1.3.3.2.2.2.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">c</mi><mo stretchy="false" id="S3.E2.m1.3.3.2.2.2.2.3.2.2" xref="S3.E2.m1.3.3.2.2.2.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.3.3.2.2.2.5" xref="S3.E2.m1.3.3.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"><eq id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.3"></eq><apply id="S3.E2.m1.3.3.4.cmml" xref="S3.E2.m1.3.3.4"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.4.1.cmml" xref="S3.E2.m1.3.3.4">superscript</csymbol><ci id="S3.E2.m1.3.3.4.2.cmml" xref="S3.E2.m1.3.3.4.2">𝑐</ci><times id="S3.E2.m1.3.3.4.3.cmml" xref="S3.E2.m1.3.3.4.3"></times></apply><apply id="S3.E2.m1.3.3.2.cmml" xref="S3.E2.m1.3.3.2"><times id="S3.E2.m1.3.3.2.3.cmml" xref="S3.E2.m1.3.3.2.3"></times><apply id="S3.E2.m1.3.3.2.4.cmml" xref="S3.E2.m1.3.3.2.4"><apply id="S3.E2.m1.3.3.2.4.1.cmml" xref="S3.E2.m1.3.3.2.4.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.4.1.1.cmml" xref="S3.E2.m1.3.3.2.4.1">subscript</csymbol><ci id="S3.E2.m1.3.3.2.4.1.2.cmml" xref="S3.E2.m1.3.3.2.4.1.2">argmax</ci><apply id="S3.E2.m1.3.3.2.4.1.3.cmml" xref="S3.E2.m1.3.3.2.4.1.3"><in id="S3.E2.m1.3.3.2.4.1.3.1.cmml" xref="S3.E2.m1.3.3.2.4.1.3.1"></in><ci id="S3.E2.m1.3.3.2.4.1.3.2.cmml" xref="S3.E2.m1.3.3.2.4.1.3.2">𝑐</ci><ci id="S3.E2.m1.3.3.2.4.1.3.3.cmml" xref="S3.E2.m1.3.3.2.4.1.3.3">𝐶</ci></apply></apply><ci id="S3.E2.m1.3.3.2.4.2.cmml" xref="S3.E2.m1.3.3.2.4.2">𝛿</ci></apply><interval closure="open" id="S3.E2.m1.3.3.2.2.3.cmml" xref="S3.E2.m1.3.3.2.2.2"><apply id="S3.E2.m1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.2"></times><ci id="S3.E2.m1.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.3">𝑓</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2">𝑥</ci><times id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3"></times></apply></apply><apply id="S3.E2.m1.3.3.2.2.2.2.cmml" xref="S3.E2.m1.3.3.2.2.2.2"><times id="S3.E2.m1.3.3.2.2.2.2.1.cmml" xref="S3.E2.m1.3.3.2.2.2.2.1"></times><ci id="S3.E2.m1.3.3.2.2.2.2.2.cmml" xref="S3.E2.m1.3.3.2.2.2.2.2">𝑓</ci><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑐</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">c^{*}=\operatorname*{argmax}_{c\in C}\delta(f(x^{*}),f(c))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.2" class="ltx_p">for <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="c\in C" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mrow id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">c</mi><mo id="S3.SS2.p5.1.m1.1.1.1" xref="S3.SS2.p5.1.m1.1.1.1.cmml">∈</mo><mi id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><in id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1.1"></in><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">𝑐</ci><ci id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">c\in C</annotation></semantics></math>, where <math id="S3.SS2.p5.2.m2.4" class="ltx_Math" alttext="C=\{c_{1},c_{2},\cdots,c_{N}\}" display="inline"><semantics id="S3.SS2.p5.2.m2.4a"><mrow id="S3.SS2.p5.2.m2.4.4" xref="S3.SS2.p5.2.m2.4.4.cmml"><mi id="S3.SS2.p5.2.m2.4.4.5" xref="S3.SS2.p5.2.m2.4.4.5.cmml">C</mi><mo id="S3.SS2.p5.2.m2.4.4.4" xref="S3.SS2.p5.2.m2.4.4.4.cmml">=</mo><mrow id="S3.SS2.p5.2.m2.4.4.3.3" xref="S3.SS2.p5.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS2.p5.2.m2.4.4.3.3.4" xref="S3.SS2.p5.2.m2.4.4.3.4.cmml">{</mo><msub id="S3.SS2.p5.2.m2.2.2.1.1.1" xref="S3.SS2.p5.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS2.p5.2.m2.2.2.1.1.1.2" xref="S3.SS2.p5.2.m2.2.2.1.1.1.2.cmml">c</mi><mn id="S3.SS2.p5.2.m2.2.2.1.1.1.3" xref="S3.SS2.p5.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p5.2.m2.4.4.3.3.5" xref="S3.SS2.p5.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.SS2.p5.2.m2.3.3.2.2.2" xref="S3.SS2.p5.2.m2.3.3.2.2.2.cmml"><mi id="S3.SS2.p5.2.m2.3.3.2.2.2.2" xref="S3.SS2.p5.2.m2.3.3.2.2.2.2.cmml">c</mi><mn id="S3.SS2.p5.2.m2.3.3.2.2.2.3" xref="S3.SS2.p5.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.p5.2.m2.4.4.3.3.6" xref="S3.SS2.p5.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">⋯</mi><mo id="S3.SS2.p5.2.m2.4.4.3.3.7" xref="S3.SS2.p5.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.SS2.p5.2.m2.4.4.3.3.3" xref="S3.SS2.p5.2.m2.4.4.3.3.3.cmml"><mi id="S3.SS2.p5.2.m2.4.4.3.3.3.2" xref="S3.SS2.p5.2.m2.4.4.3.3.3.2.cmml">c</mi><mi id="S3.SS2.p5.2.m2.4.4.3.3.3.3" xref="S3.SS2.p5.2.m2.4.4.3.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="S3.SS2.p5.2.m2.4.4.3.3.8" xref="S3.SS2.p5.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.4b"><apply id="S3.SS2.p5.2.m2.4.4.cmml" xref="S3.SS2.p5.2.m2.4.4"><eq id="S3.SS2.p5.2.m2.4.4.4.cmml" xref="S3.SS2.p5.2.m2.4.4.4"></eq><ci id="S3.SS2.p5.2.m2.4.4.5.cmml" xref="S3.SS2.p5.2.m2.4.4.5">𝐶</ci><set id="S3.SS2.p5.2.m2.4.4.3.4.cmml" xref="S3.SS2.p5.2.m2.4.4.3.3"><apply id="S3.SS2.p5.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS2.p5.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p5.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS2.p5.2.m2.2.2.1.1.1.2">𝑐</ci><cn type="integer" id="S3.SS2.p5.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS2.p5.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS2.p5.2.m2.3.3.2.2.2.cmml" xref="S3.SS2.p5.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.3.3.2.2.2.1.cmml" xref="S3.SS2.p5.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.p5.2.m2.3.3.2.2.2.2.cmml" xref="S3.SS2.p5.2.m2.3.3.2.2.2.2">𝑐</ci><cn type="integer" id="S3.SS2.p5.2.m2.3.3.2.2.2.3.cmml" xref="S3.SS2.p5.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">⋯</ci><apply id="S3.SS2.p5.2.m2.4.4.3.3.3.cmml" xref="S3.SS2.p5.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.4.4.3.3.3.1.cmml" xref="S3.SS2.p5.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S3.SS2.p5.2.m2.4.4.3.3.3.2.cmml" xref="S3.SS2.p5.2.m2.4.4.3.3.3.2">𝑐</ci><ci id="S3.SS2.p5.2.m2.4.4.3.3.3.3.cmml" xref="S3.SS2.p5.2.m2.4.4.3.3.3.3">𝑁</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.4c">C=\{c_{1},c_{2},\cdots,c_{N}\}</annotation></semantics></math> is the set of classes that we are interested in.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">In our work, the embedding similarity function <math id="S3.SS2.p6.1.m1.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S3.SS2.p6.1.m1.1a"><mi id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><ci id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">\delta</annotation></semantics></math> is the cosine similarity:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.4" class="ltx_Math" alttext="\delta(e_{1},e_{2})=\frac{e_{1}\cdot e_{2}}{\lVert e_{1}\rVert\cdot\lVert e_{2}\rVert}" display="block"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml"><mrow id="S3.E3.m1.4.4.2" xref="S3.E3.m1.4.4.2.cmml"><mi id="S3.E3.m1.4.4.2.4" xref="S3.E3.m1.4.4.2.4.cmml">δ</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.2.3" xref="S3.E3.m1.4.4.2.3.cmml">​</mo><mrow id="S3.E3.m1.4.4.2.2.2" xref="S3.E3.m1.4.4.2.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.2.2.2.3" xref="S3.E3.m1.4.4.2.2.3.cmml">(</mo><msub id="S3.E3.m1.3.3.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.2.cmml">e</mi><mn id="S3.E3.m1.3.3.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E3.m1.4.4.2.2.2.4" xref="S3.E3.m1.4.4.2.2.3.cmml">,</mo><msub id="S3.E3.m1.4.4.2.2.2.2" xref="S3.E3.m1.4.4.2.2.2.2.cmml"><mi id="S3.E3.m1.4.4.2.2.2.2.2" xref="S3.E3.m1.4.4.2.2.2.2.2.cmml">e</mi><mn id="S3.E3.m1.4.4.2.2.2.2.3" xref="S3.E3.m1.4.4.2.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="S3.E3.m1.4.4.2.2.2.5" xref="S3.E3.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.3" xref="S3.E3.m1.4.4.3.cmml">=</mo><mfrac id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mrow id="S3.E3.m1.2.2.4" xref="S3.E3.m1.2.2.4.cmml"><msub id="S3.E3.m1.2.2.4.2" xref="S3.E3.m1.2.2.4.2.cmml"><mi id="S3.E3.m1.2.2.4.2.2" xref="S3.E3.m1.2.2.4.2.2.cmml">e</mi><mn id="S3.E3.m1.2.2.4.2.3" xref="S3.E3.m1.2.2.4.2.3.cmml">1</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.2.2.4.1" xref="S3.E3.m1.2.2.4.1.cmml">⋅</mo><msub id="S3.E3.m1.2.2.4.3" xref="S3.E3.m1.2.2.4.3.cmml"><mi id="S3.E3.m1.2.2.4.3.2" xref="S3.E3.m1.2.2.4.3.2.cmml">e</mi><mn id="S3.E3.m1.2.2.4.3.3" xref="S3.E3.m1.2.2.4.3.3.cmml">2</mn></msub></mrow><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.1.cmml">∥</mo><msub id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">e</mi><mn id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo fence="true" lspace="0em" rspace="0.055em" id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.2.1.cmml">∥</mo></mrow><mo id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml">⋅</mo><mrow id="S3.E3.m1.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.2.cmml"><mo fence="true" lspace="0.055em" rspace="0em" id="S3.E3.m1.2.2.2.2.1.2" xref="S3.E3.m1.2.2.2.2.2.1.cmml">∥</mo><msub id="S3.E3.m1.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.1.1.cmml"><mi id="S3.E3.m1.2.2.2.2.1.1.2" xref="S3.E3.m1.2.2.2.2.1.1.2.cmml">e</mi><mn id="S3.E3.m1.2.2.2.2.1.1.3" xref="S3.E3.m1.2.2.2.2.1.1.3.cmml">2</mn></msub><mo fence="true" lspace="0em" id="S3.E3.m1.2.2.2.2.1.3" xref="S3.E3.m1.2.2.2.2.2.1.cmml">∥</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4"><eq id="S3.E3.m1.4.4.3.cmml" xref="S3.E3.m1.4.4.3"></eq><apply id="S3.E3.m1.4.4.2.cmml" xref="S3.E3.m1.4.4.2"><times id="S3.E3.m1.4.4.2.3.cmml" xref="S3.E3.m1.4.4.2.3"></times><ci id="S3.E3.m1.4.4.2.4.cmml" xref="S3.E3.m1.4.4.2.4">𝛿</ci><interval closure="open" id="S3.E3.m1.4.4.2.2.3.cmml" xref="S3.E3.m1.4.4.2.2.2"><apply id="S3.E3.m1.3.3.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.2">𝑒</ci><cn type="integer" id="S3.E3.m1.3.3.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.3">1</cn></apply><apply id="S3.E3.m1.4.4.2.2.2.2.cmml" xref="S3.E3.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.2.2.2.2.1.cmml" xref="S3.E3.m1.4.4.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.4.4.2.2.2.2.2.cmml" xref="S3.E3.m1.4.4.2.2.2.2.2">𝑒</ci><cn type="integer" id="S3.E3.m1.4.4.2.2.2.2.3.cmml" xref="S3.E3.m1.4.4.2.2.2.2.3">2</cn></apply></interval></apply><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><divide id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2"></divide><apply id="S3.E3.m1.2.2.4.cmml" xref="S3.E3.m1.2.2.4"><ci id="S3.E3.m1.2.2.4.1.cmml" xref="S3.E3.m1.2.2.4.1">⋅</ci><apply id="S3.E3.m1.2.2.4.2.cmml" xref="S3.E3.m1.2.2.4.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.4.2.1.cmml" xref="S3.E3.m1.2.2.4.2">subscript</csymbol><ci id="S3.E3.m1.2.2.4.2.2.cmml" xref="S3.E3.m1.2.2.4.2.2">𝑒</ci><cn type="integer" id="S3.E3.m1.2.2.4.2.3.cmml" xref="S3.E3.m1.2.2.4.2.3">1</cn></apply><apply id="S3.E3.m1.2.2.4.3.cmml" xref="S3.E3.m1.2.2.4.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.4.3.1.cmml" xref="S3.E3.m1.2.2.4.3">subscript</csymbol><ci id="S3.E3.m1.2.2.4.3.2.cmml" xref="S3.E3.m1.2.2.4.3.2">𝑒</ci><cn type="integer" id="S3.E3.m1.2.2.4.3.3.cmml" xref="S3.E3.m1.2.2.4.3.3">2</cn></apply></apply><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><ci id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3">⋅</ci><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2">delimited-∥∥</csymbol><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">𝑒</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S3.E3.m1.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.1"><csymbol cd="latexml" id="S3.E3.m1.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.1.2">delimited-∥∥</csymbol><apply id="S3.E3.m1.2.2.2.2.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.2.2.1.1.2">𝑒</ci><cn type="integer" id="S3.E3.m1.2.2.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.2.2.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">\delta(e_{1},e_{2})=\frac{e_{1}\cdot e_{2}}{\lVert e_{1}\rVert\cdot\lVert e_{2}\rVert}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.3" class="ltx_p">where <math id="S3.SS2.p7.1.m1.1" class="ltx_Math" alttext="\lVert\cdot\rVert" display="inline"><semantics id="S3.SS2.p7.1.m1.1a"><mrow id="S3.SS2.p7.1.m1.1.2.2" xref="S3.SS2.p7.1.m1.1.2.1.cmml"><mo fence="true" rspace="0em" id="S3.SS2.p7.1.m1.1.2.2.1" xref="S3.SS2.p7.1.m1.1.2.1.1.cmml">∥</mo><mo lspace="0em" rspace="0em" id="S3.SS2.p7.1.m1.1.1" xref="S3.SS2.p7.1.m1.1.1.cmml">⋅</mo><mo fence="true" lspace="0em" id="S3.SS2.p7.1.m1.1.2.2.2" xref="S3.SS2.p7.1.m1.1.2.1.1.cmml">∥</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.1b"><apply id="S3.SS2.p7.1.m1.1.2.1.cmml" xref="S3.SS2.p7.1.m1.1.2.2"><csymbol cd="latexml" id="S3.SS2.p7.1.m1.1.2.1.1.cmml" xref="S3.SS2.p7.1.m1.1.2.2.1">delimited-∥∥</csymbol><ci id="S3.SS2.p7.1.m1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.1c">\lVert\cdot\rVert</annotation></semantics></math> is the <math id="S3.SS2.p7.2.m2.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S3.SS2.p7.2.m2.1a"><msub id="S3.SS2.p7.2.m2.1.1" xref="S3.SS2.p7.2.m2.1.1.cmml"><mi id="S3.SS2.p7.2.m2.1.1.2" xref="S3.SS2.p7.2.m2.1.1.2.cmml">L</mi><mn id="S3.SS2.p7.2.m2.1.1.3" xref="S3.SS2.p7.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.2.m2.1b"><apply id="S3.SS2.p7.2.m2.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.2.m2.1.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p7.2.m2.1.1.2.cmml" xref="S3.SS2.p7.2.m2.1.1.2">𝐿</ci><cn type="integer" id="S3.SS2.p7.2.m2.1.1.3.cmml" xref="S3.SS2.p7.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.2.m2.1c">L_{2}</annotation></semantics></math> norm and <math id="S3.SS2.p7.3.m3.1" class="ltx_Math" alttext="e_{i}\in\mathbb{R}^{F}" display="inline"><semantics id="S3.SS2.p7.3.m3.1a"><mrow id="S3.SS2.p7.3.m3.1.1" xref="S3.SS2.p7.3.m3.1.1.cmml"><msub id="S3.SS2.p7.3.m3.1.1.2" xref="S3.SS2.p7.3.m3.1.1.2.cmml"><mi id="S3.SS2.p7.3.m3.1.1.2.2" xref="S3.SS2.p7.3.m3.1.1.2.2.cmml">e</mi><mi id="S3.SS2.p7.3.m3.1.1.2.3" xref="S3.SS2.p7.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p7.3.m3.1.1.1" xref="S3.SS2.p7.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS2.p7.3.m3.1.1.3" xref="S3.SS2.p7.3.m3.1.1.3.cmml"><mi id="S3.SS2.p7.3.m3.1.1.3.2" xref="S3.SS2.p7.3.m3.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS2.p7.3.m3.1.1.3.3" xref="S3.SS2.p7.3.m3.1.1.3.3.cmml">F</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.3.m3.1b"><apply id="S3.SS2.p7.3.m3.1.1.cmml" xref="S3.SS2.p7.3.m3.1.1"><in id="S3.SS2.p7.3.m3.1.1.1.cmml" xref="S3.SS2.p7.3.m3.1.1.1"></in><apply id="S3.SS2.p7.3.m3.1.1.2.cmml" xref="S3.SS2.p7.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p7.3.m3.1.1.2.1.cmml" xref="S3.SS2.p7.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p7.3.m3.1.1.2.2.cmml" xref="S3.SS2.p7.3.m3.1.1.2.2">𝑒</ci><ci id="S3.SS2.p7.3.m3.1.1.2.3.cmml" xref="S3.SS2.p7.3.m3.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.p7.3.m3.1.1.3.cmml" xref="S3.SS2.p7.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p7.3.m3.1.1.3.1.cmml" xref="S3.SS2.p7.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.p7.3.m3.1.1.3.2.cmml" xref="S3.SS2.p7.3.m3.1.1.3.2">ℝ</ci><ci id="S3.SS2.p7.3.m3.1.1.3.3.cmml" xref="S3.SS2.p7.3.m3.1.1.3.3">𝐹</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.3.m3.1c">e_{i}\in\mathbb{R}^{F}</annotation></semantics></math> are embeddings.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<p id="S3.F2.1" class="ltx_p ltx_align_center"><span id="S3.F2.1.1" class="ltx_text"><img src="/html/2407.18058/assets/figs/prompt_evaluation.png" id="S3.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="538" height="359" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text ltx_font_bold">Figure 2</span>: </span>Metrics for 6 textual prompts (See Section <a href="#S3.SS3" title="3.3 Experiment 1: Are two-tower systems context dependent? ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>), 2 audio based label embeddings (See Section <a href="#S3.SS4" title="3.4 Experiment 2: Inspecting the cosine similarity distributions ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>) and the 3 two-tower multimodal systems. The top row contains top-1 through top-3 accuracy and the bottom ROC-AUC and PR-AUC. The red line represents random choice.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Experiment 1: Are two-tower systems context dependent?</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Two-tower systems are typically not trained on single words, but rather longer prompts. As a result, the embedding produced for a single-word text label (e.g. “guitar”) can be very different from the embedding for a longer prompt, with additional context (e.g. “a guitar track”). When using two-tower models for classification, the class label can be wrapped in a prompt such as “A &lt;label&gt; track” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, to better match the training distribution. Methods to introduce stochasticity in the prompts used during training have been empirically proven to lead to more robust results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Retraining the systems and testing different ways of augmenting captions used for training is left for future work, but works in image-text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and video-text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> two-tower systems provide some evidence for their usefulness.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The impact of different approaches for giving additional context to a single-word text label during inference has not been well-explored.
We explore the prompt sensitivity of each system by slightly changing the text prompt used for zero-shot classification in order to better understand to which extent these systems leverage contextual information. As far as we are aware, we are the first to evaluate the use of different types of prompts for two-tower systems during inference. Specifically, we evaluate 3 systems against 6 different prompts:</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<ol id="S3.I2" class="ltx_enumerate">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">MusCALL prompt: “<span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_italic">A &lt;label&gt; track</span>”</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Generated definition: “<span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_italic">The &lt;label&gt; is a …</span>”</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">Generated definitions without label words: “<span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_italic">The &lt;removed&gt; is a …</span>”</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p">Label word with random context: “<span id="S3.I2.i4.p1.1.1" class="ltx_text ltx_font_italic">&lt;label&gt; &lt;randomly selected lorem ipsum segment&gt;</span>”</p>
</div>
</li>
<li id="S3.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I2.i5.p1" class="ltx_para">
<p id="S3.I2.i5.p1.1" class="ltx_p">Musically informed #1: “<span id="S3.I2.i5.p1.1.1" class="ltx_text ltx_font_italic">This is a recording of a &lt;label&gt;</span>”</p>
</div>
</li>
<li id="S3.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S3.I2.i6.p1" class="ltx_para">
<p id="S3.I2.i6.p1.1" class="ltx_p">Musically informed #2: “<span id="S3.I2.i6.p1.1.1" class="ltx_text ltx_font_italic">Solo musical instrument sound of a &lt;label&gt;</span>”</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">The first prompt proposed is the prompt that was used in MusCALL. The second prompt is generated using GPT-3.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. The third prompt is the same as the second but we removed all instances of the label itself to evaluate the influence of the context on its own. To evaluate if the systems are sensitive to specific words and to further evaluate if the context is useful, the fourth prompt adds random words alongside the label. Lastly, we test 2 musically informed prompts.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">As a first metric, we consider Top-k accuracy with <math id="S3.SS3.p5.1.m1.3" class="ltx_Math" alttext="k=\{1,2,3\}" display="inline"><semantics id="S3.SS3.p5.1.m1.3a"><mrow id="S3.SS3.p5.1.m1.3.4" xref="S3.SS3.p5.1.m1.3.4.cmml"><mi id="S3.SS3.p5.1.m1.3.4.2" xref="S3.SS3.p5.1.m1.3.4.2.cmml">k</mi><mo id="S3.SS3.p5.1.m1.3.4.1" xref="S3.SS3.p5.1.m1.3.4.1.cmml">=</mo><mrow id="S3.SS3.p5.1.m1.3.4.3.2" xref="S3.SS3.p5.1.m1.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS3.p5.1.m1.3.4.3.2.1" xref="S3.SS3.p5.1.m1.3.4.3.1.cmml">{</mo><mn id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">1</mn><mo id="S3.SS3.p5.1.m1.3.4.3.2.2" xref="S3.SS3.p5.1.m1.3.4.3.1.cmml">,</mo><mn id="S3.SS3.p5.1.m1.2.2" xref="S3.SS3.p5.1.m1.2.2.cmml">2</mn><mo id="S3.SS3.p5.1.m1.3.4.3.2.3" xref="S3.SS3.p5.1.m1.3.4.3.1.cmml">,</mo><mn id="S3.SS3.p5.1.m1.3.3" xref="S3.SS3.p5.1.m1.3.3.cmml">3</mn><mo stretchy="false" id="S3.SS3.p5.1.m1.3.4.3.2.4" xref="S3.SS3.p5.1.m1.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.3b"><apply id="S3.SS3.p5.1.m1.3.4.cmml" xref="S3.SS3.p5.1.m1.3.4"><eq id="S3.SS3.p5.1.m1.3.4.1.cmml" xref="S3.SS3.p5.1.m1.3.4.1"></eq><ci id="S3.SS3.p5.1.m1.3.4.2.cmml" xref="S3.SS3.p5.1.m1.3.4.2">𝑘</ci><set id="S3.SS3.p5.1.m1.3.4.3.1.cmml" xref="S3.SS3.p5.1.m1.3.4.3.2"><cn type="integer" id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">1</cn><cn type="integer" id="S3.SS3.p5.1.m1.2.2.cmml" xref="S3.SS3.p5.1.m1.2.2">2</cn><cn type="integer" id="S3.SS3.p5.1.m1.3.3.cmml" xref="S3.SS3.p5.1.m1.3.3">3</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.3c">k=\{1,2,3\}</annotation></semantics></math>. We calculate the cosine similarity between each recording and instrument prompt and sort them. We assign zero-shot class labels as described in Section <a href="#S3.SS2" title="3.2 Zero-shot transfer for instrument classification ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> and check if the true label is present in the top-k assigned class labels. Furthermore, we calculate Receiver Operating Characteristic and Precision-Recall Area Under the Curve (ROC-AUC and PR-AUC respectively) following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Zero-shot transfer for instrument classification ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the zero-shot instrument recognition results for the three models across the 6 prompts, as well as the audio-only alternatives that will be described in Section <a href="#S3.SS4" title="3.4 Experiment 2: Inspecting the cosine similarity distributions ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>. Despite the focus on music data, Music CLAP doesn’t display very different results from Music/Speech CLAP. While music-specific systems are generally expected to perform better, this is not the case for two-tower systems. This might be an indication that music requires special treatment, as the metrics approach the state of the art in audio-text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and image-text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> two-tower systems.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p id="S3.SS3.p7.1" class="ltx_p">Top-1 accuracy is worse than random for 4 out of 6 textual prompts for MusCALL. This might be caused by the small size of training data used, the absence of instrument-specific captions or their underrepresentation in the captions used, as well as the absence of single-note recordings in LPMusicCaps-MTT. While the metrics are low for MusCALL in most of the cases, a relatively large performance is still evident for the audio-only scenario. This implies that the problem lies in the audio-text alignment or the text branch.</p>
</div>
<div id="S3.SS3.p8" class="ltx_para">
<p id="S3.SS3.p8.1" class="ltx_p">The performance of CLAP models seems to be heavily correlated with the instrument labels themselves. Removing the label from definitions provides evidence that relevant context cannot be leveraged properly. Also, using musically informed prompts doesn’t always result in greater or even comparable results. Specifically, top-1 accuracy drops when using the second musically informed prompt for CLAP models, despite the prompt being a more precise description of what is occurring in the audio.</p>
</div>
<div id="S3.SS3.p9" class="ltx_para">
<p id="S3.SS3.p9.1" class="ltx_p">These results suggest that CLAP models do not leverage extra context in the input prompt effectively. Both models performed worse when using relevant context without the instrument word, suggesting that the textual encoders put a lot more emphasis on the presence of specific words rather than the meaning of the prompts themselves. In addition, using a generic prompt provided better results than a musically informed one in most cases. Furthermore, any kind of context added at the prompts seems to harm the performance in most of the cases and provide more evidence that the model’s text encoder cannot properly decompose the sentence to its constituents and use these semantically. Despite this observation, using definitions (prompt 2) seems promising for Music CLAP and for every metric apart from top-1 accuracy.</p>
</div>
<div id="S3.SS3.p10" class="ltx_para">
<p id="S3.SS3.p10.1" class="ltx_p">In the following experiments, we will consider only the “MusCALL prompt” as it leads to the highest top-1 accuracy, when model accuracy surpasses random choice.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Experiment 2: Inspecting the cosine similarity distributions</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">As a next experiment, we calculate the cosine similarity between the joint space embeddings of each recording from TinySOL and the MusCALL prompt for each instrument, then compare the similarities of positive pairs vs negative pairs. We define a positive pair as an audio-label pair where the label corresponds to the instrument in the recording, and the negative pairs as all other pairs. Figure <a href="#S3.F3.sf1" title="In Figure 3 ‣ 3.4 Experiment 2: Inspecting the cosine similarity distributions ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a> presents histograms of similarities for positive and negative pairs when using text prompts. If the audio-text coherence is good, positive and negative histograms should be well separated.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.18058/assets/figs/muscall_prompt_histogram_of_positive_v_negative_similarities_subplot.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Histogram of cosine similarity between TinySOL data and MusCALL prompts in joint audio-text space.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.18058/assets/figs/final_audio_only_positive_negative_joint_space.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Histogram of cosine similarity between TinySOL audio data and the mean of intra-class embeddings in joint audio-text space.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text ltx_font_bold">Figure 3</span>: </span>Histograms of audio and label embeddings for positive and negative pairs. When using textual prompts (<a href="#S3.F3.sf1" title="In Figure 3 ‣ 3.4 Experiment 2: Inspecting the cosine similarity distributions ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>), the alignment is problematic, as can be seen from the overlap between positive and negative distributions.</figcaption>
</figure>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Positive and negative similarity distributions overlap greatly, as can be seen in Figure <a href="#S3.F3.sf1" title="In Figure 3 ‣ 3.4 Experiment 2: Inspecting the cosine similarity distributions ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>. As a result, retrieval is far from optimal. Fundamentally, a caption is a multi-faceted sentence. We suspect that treating a sentence as only one embedding point (mean of word embeddings) is fundamentally problematic and greatly hinders the semantic properties of the joint space. A hypothesis that needs testing is that by using composite sentences, a model cannot properly infer the relative embeddings of the sentence constituents.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<p id="S3.F4.1" class="ltx_p ltx_align_center"><span id="S3.F4.1.1" class="ltx_text"><img src="/html/2407.18058/assets/figs/final_top2_difference.png" id="S3.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.3.1.1" class="ltx_text ltx_font_bold">Figure 4</span>: </span>The histogram of top-2 class similarities for every song in TinySOL. The CLAP models tend to be not very confident while the metrics are greater than the overconfident MusCALL with the worst metrics.</figcaption>
</figure>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">To further evaluate if the audio encoder produces meaningful representations, we use the mean of intra-class song embeddings as the label embedding. This label embedding takes the role of the prompt embedding in the previous experiment. We generate the embeddings in joint audio-text space for each song. Then, we collect the songs that belong to k-th class <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="c_{k}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝑐</ci><ci id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">c_{k}</annotation></semantics></math> and estimate the mean of the embeddings. The latter serves as the optimal embedding that the text label would have to be mapped to in order to maximize performance and will be referred to as the “audio-only” label. Note that this embedding is only optimal in the case of TinySOL data.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">The resulting histograms for positive and negative pairs are shown in Figure <a href="#S3.F3.sf2" title="In Figure 3 ‣ 3.4 Experiment 2: Inspecting the cosine similarity distributions ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>. They are well separated, indicating that the audio-encoder itself produces meaningful, separable embeddings.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">Audio embeddings seem to be of good quality before and after the projection to the joint audio-text space, as the metrics are almost equal before and after projection in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Zero-shot transfer for instrument classification ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The metrics almost double when using any audio-only labels, which further provides evidence that the problem resides in the text branch, or joint-space projection and there remains a large performance gap to bridge.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Experiment 3: How confident are two-tower systems in their prediction?</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">We calculate the histogram of the difference between the top-2 candidate classes for each recording <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> to quantify the classification confidence. The similarity between each audio and instrument embeddings is estimated and they are sorted in descending order. The difference between top-2 similarities for each song is then calculated and a histogram of that difference is plotted in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.4 Experiment 2: Inspecting the cosine similarity distributions ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">MusCALL seems to be overly confident in its prediction, which is unwarranted given the metrics reported. The opposite can be stated for CLAP models, where despite their better performance, the difference has a median value of 0.05-0.08.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Experiment 4: Quantitative evaluation of the text branch</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">While there are datasets that can be used to quantify the semantic properties and/or quality of a LM, there isn’t one that focuses on music. To overcome this lack of text data for the case of instrument recognition, we can utilize instrument ontologies, which encompass semantic similarity of instruments at multiple levels. We propose to leverage them to quantify the semantic similarity between different instruments and instrument families. In this experiment we use the instrument ontology by Henry Doktorski<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://free-reed.net/</span></span></span> (HDIO). As every instrument ontology has its limitations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, repeating the same experiment with other ontologies is left for future work.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">We extract the tree based on HDIO and form every possible triplet of TinySOL instrument labels in the tree for a total of <math id="S3.SS6.p2.1.m1.3" class="ltx_Math" alttext="364={14\choose 3}" display="inline"><semantics id="S3.SS6.p2.1.m1.3a"><mrow id="S3.SS6.p2.1.m1.3.4" xref="S3.SS6.p2.1.m1.3.4.cmml"><mn id="S3.SS6.p2.1.m1.3.4.2" xref="S3.SS6.p2.1.m1.3.4.2.cmml">364</mn><mo id="S3.SS6.p2.1.m1.3.4.1" xref="S3.SS6.p2.1.m1.3.4.1.cmml">=</mo><mrow id="S3.SS6.p2.1.m1.3.3.5" xref="S3.SS6.p2.1.m1.3.3.4.cmml"><mo id="S3.SS6.p2.1.m1.3.3.5.1" xref="S3.SS6.p2.1.m1.1.1.1.1.1.cmml">(</mo><mfrac linethickness="0pt" id="S3.SS6.p2.1.m1.3.3.3.3" xref="S3.SS6.p2.1.m1.3.3.4.cmml"><mn id="S3.SS6.p2.1.m1.2.2.2.2.2" xref="S3.SS6.p2.1.m1.2.2.2.2.2.cmml">14</mn><mn id="S3.SS6.p2.1.m1.3.3.3.3.3" xref="S3.SS6.p2.1.m1.3.3.3.3.3.cmml">3</mn></mfrac><mo id="S3.SS6.p2.1.m1.3.3.5.2" xref="S3.SS6.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.1.m1.3b"><apply id="S3.SS6.p2.1.m1.3.4.cmml" xref="S3.SS6.p2.1.m1.3.4"><eq id="S3.SS6.p2.1.m1.3.4.1.cmml" xref="S3.SS6.p2.1.m1.3.4.1"></eq><cn type="integer" id="S3.SS6.p2.1.m1.3.4.2.cmml" xref="S3.SS6.p2.1.m1.3.4.2">364</cn><apply id="S3.SS6.p2.1.m1.3.3.4.cmml" xref="S3.SS6.p2.1.m1.3.3.5"><csymbol cd="latexml" id="S3.SS6.p2.1.m1.1.1.1.1.1.cmml" xref="S3.SS6.p2.1.m1.3.3.5.1">binomial</csymbol><cn type="integer" id="S3.SS6.p2.1.m1.2.2.2.2.2.cmml" xref="S3.SS6.p2.1.m1.2.2.2.2.2">14</cn><cn type="integer" id="S3.SS6.p2.1.m1.3.3.3.3.3.cmml" xref="S3.SS6.p2.1.m1.3.3.3.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.1.m1.3c">364={14\choose 3}</annotation></semantics></math> combinations of positive word pairs without repetition. The triplets are of the form <em id="S3.SS6.p2.1.1" class="ltx_emph ltx_font_italic">(&lt;anchor&gt;, &lt;positive&gt;, &lt;negative&gt;)</em> where the <em id="S3.SS6.p2.1.2" class="ltx_emph ltx_font_italic">&lt;anchor&gt;</em> label has to be more semantically similar according to HDIO to the <em id="S3.SS6.p2.1.3" class="ltx_emph ltx_font_italic">&lt;positive&gt;</em> than the <em id="S3.SS6.p2.1.4" class="ltx_emph ltx_font_italic">&lt;negative&gt;</em>, e.g (<em id="S3.SS6.p2.1.5" class="ltx_emph ltx_font_italic">“violin”</em>, <em id="S3.SS6.p2.1.6" class="ltx_emph ltx_font_italic">“violoncello”</em>, <em id="S3.SS6.p2.1.7" class="ltx_emph ltx_font_italic">“trumpet”</em>). Subsequently, every <em id="S3.SS6.p2.1.8" class="ltx_emph ltx_font_italic">(&lt;anchor&gt;, &lt;positive&gt;)</em> pair that is linked through the root node of HDIO is excluded. The number of remaining triplets is 273.
As a way to quantify semantic meaningfulness with respect to musical instruments, we calculate cosine similarity between the <em id="S3.SS6.p2.1.9" class="ltx_emph ltx_font_italic">(&lt;anchor&gt;, &lt;positive&gt;)</em> and <em id="S3.SS6.p2.1.10" class="ltx_emph ltx_font_italic">(&lt;anchor&gt;, &lt;negative&gt;)</em> pairs for each system. Triplets for which the similarity is higher for the first pair than the second are considered “correct”, and triplets where this is not the case are considered “incorrect”. We compute the accuracy score as the percentage of correct triples.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<p id="S3.SS6.p3.1" class="ltx_p">We repeat this procedure with every valid triplet from the full ontology, as opposed to just using the instrument labels appearing in TinySOL. This gives us <math id="S3.SS6.p3.1.m1.1" class="ltx_Math" alttext="\approx 443k" display="inline"><semantics id="S3.SS6.p3.1.m1.1a"><mrow id="S3.SS6.p3.1.m1.1.1" xref="S3.SS6.p3.1.m1.1.1.cmml"><mi id="S3.SS6.p3.1.m1.1.1.2" xref="S3.SS6.p3.1.m1.1.1.2.cmml"></mi><mo id="S3.SS6.p3.1.m1.1.1.1" xref="S3.SS6.p3.1.m1.1.1.1.cmml">≈</mo><mrow id="S3.SS6.p3.1.m1.1.1.3" xref="S3.SS6.p3.1.m1.1.1.3.cmml"><mn id="S3.SS6.p3.1.m1.1.1.3.2" xref="S3.SS6.p3.1.m1.1.1.3.2.cmml">443</mn><mo lspace="0em" rspace="0em" id="S3.SS6.p3.1.m1.1.1.3.1" xref="S3.SS6.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS6.p3.1.m1.1.1.3.3" xref="S3.SS6.p3.1.m1.1.1.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p3.1.m1.1b"><apply id="S3.SS6.p3.1.m1.1.1.cmml" xref="S3.SS6.p3.1.m1.1.1"><approx id="S3.SS6.p3.1.m1.1.1.1.cmml" xref="S3.SS6.p3.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S3.SS6.p3.1.m1.1.1.2.cmml" xref="S3.SS6.p3.1.m1.1.1.2">absent</csymbol><apply id="S3.SS6.p3.1.m1.1.1.3.cmml" xref="S3.SS6.p3.1.m1.1.1.3"><times id="S3.SS6.p3.1.m1.1.1.3.1.cmml" xref="S3.SS6.p3.1.m1.1.1.3.1"></times><cn type="integer" id="S3.SS6.p3.1.m1.1.1.3.2.cmml" xref="S3.SS6.p3.1.m1.1.1.3.2">443</cn><ci id="S3.SS6.p3.1.m1.1.1.3.3.cmml" xref="S3.SS6.p3.1.m1.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p3.1.m1.1c">\approx 443k</annotation></semantics></math> triplets.</p>
</div>
<div id="S3.SS6.p4" class="ltx_para">
<p id="S3.SS6.p4.1" class="ltx_p">The accuracy for triplets from TinySOL and the full HDIO ontology are both presented in Figure <a href="#S3.F5" title="Figure 5 ‣ 3.6 Experiment 4: Quantitative evaluation of the text branch ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We see that half of the triplets are “incorrect” and this means that abstract semantic relations between instruments are not effectively captured in the textual branch, indicating a need for fine-tuning textual encoders on music related data.
Note that the accuracy is roughly the same as we would obtain by creating arbitrary triplets, though it is important to highlight that several instruments and instrument categories are words that are not frequently used in English.
Closer examination of the validity and usefulness of specific triplet cases (e.g. “stringed”, “plucked”, “violin”) is left for future work.</p>
</div>
<figure id="S3.F5" class="ltx_figure">
<p id="S3.F5.1" class="ltx_p ltx_align_center"><span id="S3.F5.1.1" class="ltx_text"><img src="/html/2407.18058/assets/figs/model_accuracy_by_subset_multibar.png" id="S3.F5.1.1.g1" class="ltx_graphics ltx_img_landscape" width="538" height="323" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.3.1.1" class="ltx_text ltx_font_bold">Figure 5</span>: </span>Semantic meaningfulness quantification leveraging Henry Doktorski’s instrument ontology. We evaluated the systems over valid triplets obtained through TinySOL labels, as well as every available triplet obtained from the ontology’s labels. Accuracy ranges from 49-59% which stresses that the models do not properly understand musical instruments in depth.</figcaption>
</figure>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Experiment 5: Does joint space mapping introduce noise?</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">To further examine the origins of the problematic embedding alignment, we repeat zero-shot evaluation with audio-only labels described in Sections <a href="#S3.SS4" title="3.4 Experiment 2: Inspecting the cosine similarity distributions ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a> and semantic meaningfulness evaluation described in <a href="#S3.SS6" title="3.6 Experiment 4: Quantitative evaluation of the text branch ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.6</span></a> with the embeddings in the audio space and text space before the joint audio-text space mapping.</p>
</div>
<div id="S3.SS7.p2" class="ltx_para">
<p id="S3.SS7.p2.1" class="ltx_p">A minor performance increment can be seen when using the joint embedding instead of the pre-joint audio embedding, as can be seen in the last two columns of Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Zero-shot transfer for instrument classification ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, apart from MusCALL where the metrics remain almost the same. We believe that the reduction in dimensionality of the joint space compared to the separate spaces is the underlying cause of these increments.</p>
</div>
<div id="S3.SS7.p3" class="ltx_para">
<p id="S3.SS7.p3.1" class="ltx_p">On the other hand, the accuracy based on HDIO remains the same, except for a decrement observed for Music CLAP and TinySOL subset of HDIO triplets, as can be seen in Figure <a href="#S3.F5" title="Figure 5 ‣ 3.6 Experiment 4: Quantitative evaluation of the text branch ‣ 3 Evaluation of two-tower systems ‣ I can listen but cannot read: An evaluation of two-tower multimodal systems for instrument recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. This could be an indication that the MLP can effectively map knowledge to the joint space. This is a further hint that potentially the problem lies in the LM used and fine-tuning might be needed to enforce musical semantics to be better represented.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusions and Future Work</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we evaluated 3 two-tower multimodal systems for instrument classification. We provided a zero-shot classification analysis and an elaborate evaluation of the audio and text embeddings in the pre-joint and joint audio-text spaces. We also proposed a novel way to quantify the semantic meaningfulness of text embeddings based on triplets derived from an instrument ontology.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Generally, experiments showed that audio encoders are of good quality and hence, the alignment issue might be traced back to the text branch and/or the joint audio-text space mapping. Therefore, a solution could be to freeze the audio encoder and map the text information to audio space. Also, further attention to modality imbalance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> can be placed with weighing in negative and positive examples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. Additionally, to avoid sensitivity towards instrument labels and the inability to leverage context, we propose to use text augmentation over captions or masking/removing the words from them. It is important to state that the relation between sentence and word embeddings is not as straightforward as with bag-of-words Language Models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> and as a result, the way to utilize captions or put additional emphasis on their constituents have to be further tested.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">As a result, using two-tower systems might not be very useful for multi-class scenarios, given the large overlap between positive and negative histograms of cosine similarities shown in our experiments. We believe that it is essential for a music terminology similarity corpus to be established. The benefits will be two-fold: (1) it will provide a useful way of quantifying the semantic meaningfulness of the textual branch for two-tower model and (2) it can serve as a baseline to quantify the need for music-informed fine-tuning. Last but not least, genre and emotion ontologies can be used to further evaluate the semantic meaningfulness of language models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
N. Ndou, R. Ajoodha, and A. Jadhav, “Music genre classification: A review of deep-learning and traditional machine-learning approaches,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International IOT, Electronics and Mechatronics Conference (IEMTRONICS)</em>, 2021, pp. 1–6.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
D. Han, Y. Kong, J. Han, and G. Wang, “A survey of music emotion recognition,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Frontiers of Computer Science</em>, vol. 16, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M. Won, J. Spijkervet, and K. Choi, <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Music Classification: Beyond Supervised Learning, Towards Real-world Applications</em>.   https://music-classification.github.io/tutorial, November 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Z. Fu, G. Lu, K. M. Ting, and D. Zhang, “A survey of audio-based music classification and annotation,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Multimedia</em>, vol. 13, no. 2, pp. 303–319, 2011.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. C. Lena and R. A. Peterson, “Classification as culture: Types and trajectories of music genres,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">American Sociological Review</em>, vol. 73, no. 5, pp. 697–718, 2008.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. van Venrooij and V. Schmutz, “Categorical ambiguity in cultural fields: The effects of genre fuzziness in popular music,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Poetics</em>, vol. 66, pp. 1–18, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
R. Hennequin, J. Royo-Letelier, and M. Moussallam, “Audio based disambiguation of music genre tags,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Society for Music Information Retrieval Conference</em>, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, “PANNs: Large-scale pretrained audio neural networks for audio pattern recognition,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 28, pp. 2880–2894, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
K. Chen, X. Du, B. Zhu, Z. Ma, T. Berg-Kirkpatrick, and S. Dubnov, “HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP</em>, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
L. Wang, P. Luc, Y. Wu, A. Recasens, L. Smaira, A. Brock, A. Jaegle, J.-B. Alayrac, S. Dieleman, J. Carreira, and A. van den Oord, “Towards learning universal audio representations,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp. 4593–4597, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H. Xie, O. J. Räsänen, and T. Virtanen, “Zero-shot audio classification with factored linear and nonlinear acoustic-semantic projections,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp. 326–330, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Choi, J. Lee, J. Park, and J. Nam, “Zero-shot learning for audio-based music classification and tagging,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Society for Music Information Retrieval Conference</em>, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
H. Xie and T. Virtanen, “Zero-shot audio classification via semantic embeddings,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 29, pp. 1233–1242, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
H. Xie and V. Tuomas, “Zero-shot audio classification based on class label embeddings,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</em>, pp. 264–267, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S. Pratt, I. Covert, R. Liu, and A. Farhadi, “What does a platypus look like? generating customized prompts for zero-shot image classification,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>, October 2023, pp. 15 691–15 701.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y. Ge, J. Ren, A. Gallagher, Y. Wang, M.-H. Yang, H. Adam, L. Itti, B. Lakshminarayanan, and J. Zhao, “Improving zero-shot generalization and robustness of multi-modal models,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2023, pp. 11 093–11 101.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S. Goyal, A. Kumar, S. Garg, Z. Kolter, and A. Raghunathan, “Finetune like you pretrain: Improved finetuning of zero-shot vision models,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2023, pp. 19 338–19 347.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
I. Manco, E. Benetos, E. Quinton, and G. Fazekas, “Contrastive audio-language learning for music,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd International Society for Music Information Retrieval Conference (ISMIR)</em>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Q. Huang, A. Jansen, J. Lee, R. Ganti, J. Y. Li, and D. P. W. Ellis, “MuLan: A joint embedding of music audio and natural language,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">International Society for Music Information Retrieval Conference</em>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
B. Elizalde, S. Deshmukh, M. A. Ismail, and H. Wang, “CLAP: learning audio concepts from natural language supervision,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, “Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP</em>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
C. Emanuele, D. Ghisi, V. Lostanlen, F. Lévy, J. Fineberg, and Y. Maresz, “TinySOL: An audio dataset of isolated musical notes (5.0),” 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
X. Zhai, X. Wang, B. Mustafa, A. Steiner, D. Keysers, A. Kolesnikov, and L. Beyer, “LiT: Zero-shot transfer with locked-image text tuning,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp. 18 102–18 112, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1409.1556, 2014.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y. Gong, Y.-A. Chung, and J. Glass, “AST: Audio Spectrogram Transformer,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2021</em>, 2021, pp. 571–575.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J. Pennington, R. Socher, and C. Manning, “GloVe: Global vectors for word representation,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, A. Moschitti, B. Pang, and W. Daelemans, Eds.   Doha, Qatar: Association for Computational Linguistics, Oct. 2014, pp. 1532–1543.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
T. Mikolov, K. Chen, G. S. Corrado, and J. Dean, “Efficient estimation of word representations in vector space,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2013.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">North American Chapter of the Association for Computational Linguistics</em>, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
D. Dogan, H. Xie, T. Heittola, and T. Virtanen, “Zero-shot audio classification using image embeddings,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">2022 30th European Signal Processing Conference (EUSIPCO)</em>, pp. 1–5, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
X. Du, Z. Yu, J. Lin, B. Zhu, and Q. Kong, “Joint music and language attention models for zero-shot music tagging,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2024, pp. 1126–1130.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
K. He, Y. Wang, and J. Hopcroft, “A powerful generative model using random weights for the deep image representation,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th International Conference on Neural Information Processing Systems</em>, ser. NIPS’16.   Red Hook, NY, USA: Curran Associates Inc., 2016, p. 631–639.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Neural Information Processing Systems</em>, 2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. van den Oord, Y. Li, and O. Vinyals, “Representation learning with contrastive predictive coding,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1807.03748, 2018. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://arxiv.org/abs/1807.03748</span>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Y. Tian, D. Krishnan, and P. Isola, “Contrastive multiview coding,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “RoBERTa: A robustly optimized BERT pretraining approach,” <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1907.11692, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
S. Doh, K. Choi, J. Lee, and J. Nam, “LP-MusicCaps: LLM-based pseudo music captioning,” vol. abs/2307.16372, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
E. Law, K. West, M. I. Mandel, M. Bay, and J. S. Downie, “Evaluation of algorithms using games: The case of music tagging,” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">International Society for Music Information Retrieval Conference</em>, 2009.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
S. Doh, M. Won, K. Choi, and J. Nam, “Toward universal text-to-music retrieval,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
L. Zhen, P. Hu, X. Wang, and D. Peng, “Deep supervised cross-modal retrieval,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019, pp. 10 386–10 395.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
X. Wang, B. Ke, X. Li, F. Liu, M. Zhang, X. Liang, and Q. Xiao, “Modality-balanced embedding for video retrieval,” in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, ser. SIGIR ’22.   New York, NY, USA: Association for Computing Machinery, 2022, p. 2578–2582.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, “Language models are few-shot learners,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33.   Curran Associates, Inc., 2020, pp. 1877–1901.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
M. Won, A. Ferraro, D. Bogdanov, and X. Serra, “Evaluation of cnn-based automatic music tagging models,” in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proc. of 17th Sound and Music Computing</em>, 2020.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
M. J. Mirza, L. Karlinsky, W. Lin, H. Possegger, M. Kozinski, R. Feris, and H. Bischof, “Lafter: Label-free tuning of zero-shot classifier using language and unlabeled image collections,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., vol. 36.   Curran Associates, Inc., 2023, pp. 5765–5777.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
C. C. Aggarwal, <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Data Classification: Algorithms and Applications</em>, 1st ed.   Chapman &amp; Hall/CRC, 2014.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Şefki Kolozali, M. Barthet, G. Fazekas, and M. B. Sandler, “Knowledge representation issues in musical instrument ontology design,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">International Society for Music Information Retrieval Conference</em>, 2011.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
X. Wang, B. Ke, X. Li, F. Liu, M. Zhang, X. Liang, Q.-E. Xiao, and Y. Yu, “Modality-balanced embedding for video retrieval,” <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 2022.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
D. Oneaţă and H. Cucu, “Improving multimodal speech recognition by data augmentation and speech representations,” <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, pp. 4578–4587, 2022.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
K. Margatina, G. Vernikos, L. Barrault, and N. Aletras, “Active learning by acquiring contrastive examples,” in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Conference on Empirical Methods in Natural Language Processing</em>, 2021.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
J. Choi, S. Jang, H. Cho <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Towards proper contrastive self-supervised learning strategies for music audio representation,” in <em id="bib.bib49.2.2" class="ltx_emph ltx_font_italic">2022 IEEE International Conference on Multimedia and Expo (ICME)</em>.   IEEE, 2022, pp. 1–6.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
S. Ma, Z. Zeng, D. McDuff, and Y. Song, “Learning audio-visual representations with active contrastive coding,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2009.09805, 2020. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2009.09805</span>

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
M. Alian and A. Awajan, “Factors affecting sentence similarity and paraphrasing identification,” <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Int. J. Speech Technol.</em>, vol. 23, no. 4, p. 851–859, dec 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.18057" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.18058" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.18058">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.18058" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.18059" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 18:34:26 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
