<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.02243] A Novel Audio-Visual Information Fusion System for Mental Disorders Detection</title><meta property="og:description" content="Mental disorders are among the foremost contributors to the global healthcare challenge.
Research indicates that timely diagnosis and intervention are vital in treating various mental disorders. However, the early soma‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Novel Audio-Visual Information Fusion System for Mental Disorders Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Novel Audio-Visual Information Fusion System for Mental Disorders Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.02243">

<!--Generated on Sun Oct  6 01:51:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
mental disorder,  machine learning,  depression,  ADHD,  multimodal
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Novel Audio-Visual Information Fusion System for Mental Disorders Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yichun Li, Shuanglin Li, Syed Mohsen Naqvi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Intelligent Sensing and Communications Research Group, Newcastle University, UK
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Mental disorders are among the foremost contributors to the global healthcare challenge.
Research indicates that timely diagnosis and intervention are vital in treating various mental disorders. However, the early somatization symptoms of certain mental disorders may not be immediately evident, often resulting in their oversight and misdiagnosis. Additionally, the traditional diagnosis methods incur high time and cost.
Deep learning methods based on fMRI and EEG have improved the efficiency of the mental disorder detection process. However, the cost of the equipment and trained staff are generally huge. Moreover, most systems are only trained for a specific mental disorder and are not general-purpose.
Recently, physiological studies have shown that there are some speech and facial-related symptoms in a few mental disorders (e.g., depression and ADHD).
In this paper, we focus on the emotional expression features of mental disorders and introduce a multimodal mental disorder diagnosis system based on audio-visual information input. Our proposed system is based on spatial-temporal attention networks and innovative uses a less computationally intensive pre-train audio recognition network to fine-tune the video recognition module for better results. We also apply the unified system for multiple mental disorders (ADHD and depression) for the first time.
The proposed system achieves over 80% accuracy on the real multimodal ADHD dataset and achieves state-of-the-art results on the depression dataset AVEC 2014.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
mental disorder, machine learning, depression, ADHD, multimodal

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.2" class="ltx_p">Mental health encompasses an individual‚Äôs psychological, emotional, and social well-being, which includes the ability to cope with stress, manage emotions, maintain relationships, and make decisions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. It plays a crucial role in overall health and functioning, influencing thoughts, feelings, and actions in daily life.
Fig. 1 shows the age (years) and gender distribution of patients with a diagnosis of severe mental illness (SMI) compared with all patients recorded by the United Kingdom National Health Service (NHS), UK. The results suggest that approximately 5-15 % of recorded patient visits within the 20-60 years age group are impacted by severe mental illness, constituting a substantial overall figure.
Various factors, including genetics, environment, life experiences, and biological factors, can influence mental health. As a result, mental disorders such as depression, Attention Deficit Hyperactivity Disorder (ADHD), and anxiety are common, particularly among children and adolescents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. According to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, ADHD affects around 5-7 <math id="S1.p1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S1.p1.1.m1.1a"><mo id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><csymbol cd="latexml" id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">\%</annotation></semantics></math> of children and adolescents worldwide. Moreover, the global prevalence of depression was estimated at 28 <math id="S1.p1.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S1.p1.2.m2.1a"><mo id="S1.p1.2.m2.1.1" xref="S1.p1.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S1.p1.2.m2.1b"><csymbol cd="latexml" id="S1.p1.2.m2.1.1.cmml" xref="S1.p1.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.2.m2.1c">\%</annotation></semantics></math> in 2021. These disorders could have serious consequences for individuals, including learning difficulties, impaired social interactions, and emotional issues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.02243/assets/Images/Ages.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="260" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.2.1" class="ltx_text" style="font-size:80%;">Age (years) and gender distribution of patients with a diagnosis of severe mental illness (SMI) compared with all patients recorded by the United Kingdom National Health Service (NHS), UK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. </span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The traditional diagnosis of mental disorders typically relies on clinicians‚Äô observation, questioning, and consultation, guided by the Diagnostic and Statistical Manual of Mental Disorders (DSM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. However, this diagnostic process is time-consuming and heavily dependent on the clinician‚Äôs experience and judgment, including the long waiting time for the clinical appointment. The timely intervention significantly impacts the treatment of mental disorders and improves the quality of life for patients and their families <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Due to the time-consuming process and the shortage of experienced clinical consultants, it has been reported that the waiting period for diagnoses and treatment of certain mental disorders such as ADHD, depression, and Alzheimer‚Äôs can extend to several years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
Recently, there has been a growing interest in machine learning methods for mental disorders detection and diagnosis. The majority of research in this area relies on Magnetic Resonance Imaging (MRI) and Electroencephalography (EEG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. These methods efficiently detect and extract neurobiological symptoms and features, leveraging objective brain changes to diagnose subjects.
The conventional MRI and EEG-based methods have two limitations. Firstly, the expensive equipment and high operational costs limit the practical use of MRI and EEG in real-world diagnosis. MRI and EEG scanners cost ¬£150,000-1,000,000 and ¬£1,000-25,000, respectively, and the regular maintenance costs are also very expensive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Secondly, recent detection and diagnosis techniques are specific to one mental disorder. However, many different mental disorders share the same or similar behavioral symptoms, such as dodge expression and uncontrollable body shaking, which are often overlooked by these neurobiological diagnostic techniques.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2409.02243/assets/Images/emo2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="334" height="235" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S1.F2.2.1" class="ltx_text" style="font-size:80%;">The two-dimensional emotion space, which can be divided into four quadrants. Each quadrant is associated with various emotions. </span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Therefore, there is a growing demand for cost-effective and versatile psychiatric screening methods. In 1980, Russell introduced the concept of emotional states being represented as continuous numerical vectors in a two-dimensional space known as the Valence‚ÄìArousal (VA) space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Valence denotes positive and negative emotional states, while arousal indicates the intensity of emotions ranging from sleepiness to high excitement.
As shown in Fig. 2, depression typically occupies the third quadrant of the VA space, while ADHD is predominantly situated in the first and third quadrants. The distinct expression of various mental disorders within emotional space enables possible diagnosis and screening using a unified system. Many mental disorder symptoms manifest as observable emotional swings, which are reflected in both the patient‚Äôs speech and facial expressions.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Therefore, we introduce a novel diagnostic system for mental disorders based on emotion recognition and the classification of audio and facial video input.
The contributions of this paper are summarized as follows:</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><math id="S1.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.p5.1.m1.1a"><mo id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><ci id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">\bullet</annotation></semantics></math> A generalized diagnostic system for mental disorders is proposed, leveraging emotional recognition from raw RGB facial video and speech audio data. The performance of the system is also validated by the medical-approved NHS body.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><math id="S1.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.p6.1.m1.1a"><mo id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><ci id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\bullet</annotation></semantics></math> An efficient multimodal detection method is also proposed and applied to the mental disorder diagnosis for the first time. The innovative use of the simple pre-train audio model to fine-tune the video-based model to improve accuracy is provided.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><math id="S1.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.p7.1.m1.1a"><mo id="S1.p7.1.m1.1.1" xref="S1.p7.1.m1.1.1.cmml">‚àô</mo><annotation-xml encoding="MathML-Content" id="S1.p7.1.m1.1b"><ci id="S1.p7.1.m1.1.1.cmml" xref="S1.p7.1.m1.1.1">‚àô</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.m1.1c">\bullet</annotation></semantics></math> We demonstrate the effectiveness of our proposed method over both ADHD and depression datasets, compared to state-of-the-art benchmarks across the latest performance matrix.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The rest of the paper is organized as follows. The related work to mental disorders and diagnosis techniques is introduced in Section II. Then, the proposed method is described in Section III. The experimental settings and results are presented in Section IV. Finally, our work is concluded in Section V. It should be noted that this paper aims to explore the application of fusion systems based on audio-visual features in mental disorder assessment and detection. Further feature fusion experiments and more comprehensive detection results, will be addressed in the future journal paper of this work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Works</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Various physiological and psychological researchers have verified that lesions in certain brain areas can lead to behavior disorders. Therefore, for some psychological disorders, diagnosis and detection through emotional expression and behavioral characteristics have been proven feasible <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Depression is a common psychiatric disorder. The DSM-V characterizes depression as enduring sadness and diminished interest in previously enjoyed activities. It also highlights that individuals might encounter supplementary physical symptoms, including chronic pain or digestive problems. In the research conducted by Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, they employed a methodology that calculates the two
attributes of brain regions based on the multi-layer network of dynamic functional connections and fuses morphological and
anatomical network features to diagnose depression, resulting in a reasonable classification accuracy of 93.6%. In recent research, machine learning methods based on emotion recognition have also been used in depression assessment. In the research conducted by Niu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, they proposed a representation block to find a set of basis
vectors to construct the optimal transformation space and generate the transformation result.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2409.02243/assets/Images/frame4.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="687" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S2.F3.8.4" class="ltx_text" style="font-size:80%;">Illustration of the proposed mental disorders assessment and detection system. The spectral-temporal feature from the video is extracted with the Cov-Attention module, 1D-convolutional layers, and spatial-temporal networks. We use a pre-train Attention-CNN module based on audio to fine-tune the video input. The <math id="S2.F3.5.1.m1.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S2.F3.5.1.m1.1b"><mo id="S2.F3.5.1.m1.1.1" xref="S2.F3.5.1.m1.1.1.cmml">‚äï</mo><annotation-xml encoding="MathML-Content" id="S2.F3.5.1.m1.1c"><csymbol cd="latexml" id="S2.F3.5.1.m1.1.1.cmml" xref="S2.F3.5.1.m1.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.5.1.m1.1d">\oplus</annotation></semantics></math> symbol denotes the concatenation operation for the fusion loss. The variables <math id="S2.F3.6.2.m2.1" class="ltx_Math" alttext="\ell_{S}" display="inline"><semantics id="S2.F3.6.2.m2.1b"><msub id="S2.F3.6.2.m2.1.1" xref="S2.F3.6.2.m2.1.1.cmml"><mi mathvariant="normal" id="S2.F3.6.2.m2.1.1.2" xref="S2.F3.6.2.m2.1.1.2.cmml">‚Ñì</mi><mi id="S2.F3.6.2.m2.1.1.3" xref="S2.F3.6.2.m2.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F3.6.2.m2.1c"><apply id="S2.F3.6.2.m2.1.1.cmml" xref="S2.F3.6.2.m2.1.1"><csymbol cd="ambiguous" id="S2.F3.6.2.m2.1.1.1.cmml" xref="S2.F3.6.2.m2.1.1">subscript</csymbol><ci id="S2.F3.6.2.m2.1.1.2.cmml" xref="S2.F3.6.2.m2.1.1.2">‚Ñì</ci><ci id="S2.F3.6.2.m2.1.1.3.cmml" xref="S2.F3.6.2.m2.1.1.3">ùëÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.6.2.m2.1d">\ell_{S}</annotation></semantics></math>, <math id="S2.F3.7.3.m3.1" class="ltx_Math" alttext="\ell_{V}" display="inline"><semantics id="S2.F3.7.3.m3.1b"><msub id="S2.F3.7.3.m3.1.1" xref="S2.F3.7.3.m3.1.1.cmml"><mi mathvariant="normal" id="S2.F3.7.3.m3.1.1.2" xref="S2.F3.7.3.m3.1.1.2.cmml">‚Ñì</mi><mi id="S2.F3.7.3.m3.1.1.3" xref="S2.F3.7.3.m3.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F3.7.3.m3.1c"><apply id="S2.F3.7.3.m3.1.1.cmml" xref="S2.F3.7.3.m3.1.1"><csymbol cd="ambiguous" id="S2.F3.7.3.m3.1.1.1.cmml" xref="S2.F3.7.3.m3.1.1">subscript</csymbol><ci id="S2.F3.7.3.m3.1.1.2.cmml" xref="S2.F3.7.3.m3.1.1.2">‚Ñì</ci><ci id="S2.F3.7.3.m3.1.1.3.cmml" xref="S2.F3.7.3.m3.1.1.3">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.7.3.m3.1d">\ell_{V}</annotation></semantics></math>, and <math id="S2.F3.8.4.m4.1" class="ltx_Math" alttext="\ell_{B}" display="inline"><semantics id="S2.F3.8.4.m4.1b"><msub id="S2.F3.8.4.m4.1.1" xref="S2.F3.8.4.m4.1.1.cmml"><mi mathvariant="normal" id="S2.F3.8.4.m4.1.1.2" xref="S2.F3.8.4.m4.1.1.2.cmml">‚Ñì</mi><mi id="S2.F3.8.4.m4.1.1.3" xref="S2.F3.8.4.m4.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F3.8.4.m4.1c"><apply id="S2.F3.8.4.m4.1.1.cmml" xref="S2.F3.8.4.m4.1.1"><csymbol cd="ambiguous" id="S2.F3.8.4.m4.1.1.1.cmml" xref="S2.F3.8.4.m4.1.1">subscript</csymbol><ci id="S2.F3.8.4.m4.1.1.2.cmml" xref="S2.F3.8.4.m4.1.1.2">‚Ñì</ci><ci id="S2.F3.8.4.m4.1.1.3.cmml" xref="S2.F3.8.4.m4.1.1.3">ùêµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F3.8.4.m4.1d">\ell_{B}</annotation></semantics></math> denote the loss of the audio-based recognition model, the loss of the video-based recognition model and the fusion background loss, respectively. </span></figcaption>
</figure>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Brain MRI is the most widely used modality for ADHD diagnosis with machine learning. Most studies have sourced their MRI images from a single public database, namely, the Neuro Bureau ADHD-200 Reprocessed repository (ADHD-200) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. This dataset comprises structural and resting-state functional MRI images collected from 585 control individuals and 362 children and adolescents with ADHD. Numerous studies have identified structural differences between individuals with ADHD and controls. Based on this dataset, Tang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> achieve 99.6% accuracy by employing a modified auto-encoder network to extract discriminative features and enlarge the variability scores for the binary comparison.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Audio and video, as the most readily available multimodal signals, play a crucial role in various multimodal machine learning applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Their advantages include providing rich sensory information, enabling a better understanding of context, and facilitating natural interaction. Additionally, they greatly improve the performance and robustness of related systems by combining different features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Based on our investigation, the majority of mental disorders detection and diagnosis relies on fMRI and EEG tools, incurring high human and instrumental costs for application. Diagnostic and detection multimodal methods are highly limited, with most research on text and wearable sensors.
Therefore, developing a universal mental disorder detection system based on low-cost audio-visual signals has great potential for application.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Proposed Methods</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section briefly introduces the outlines of our systems and datasets utilized in this work. As our main emphasis lies on the multimodal fusion system based on audio and videos, details of networks utilized in the proposed system are also presented in this section.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Proposed Multimodal Mental Disorder Detection systems</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The proposed system contains three main parts: video-based facial expression detection, audio-based pre-train model, and classification and regression performance measurements.
Because of the particularity of medical and clinical-related information, open-source mental disorders datasets are relatively limited at this stage. We select multimodal datasets containing real facial video and audio encompassing a broad spectrum of mental disorders, including ADHD and depression.
Fig. 3 illustrates our proposed mental disorder detection system. Details will be introduced in the following subsections.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">This paper primarily focuses on attention deficit hyperactivity disorder (ADHD) and depression. These psychiatric conditions are selected due to their prevalence as the most common mental disorders, each characterized by specific emotional symptoms. Two challenging multimodal datasets, which serve as benchmarks for ADHD and depression detection and assessment, are utilized in the experiments. All datasets used to validate the detection and assessment performance of the proposed system are approved by certified medical authorities.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In our proposed system, we utilize the interview segments from our real multimodal ADHD dataset for the binary classification of ADHD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Each subject and control undergoes a data recording process lasting 10-20 minutes, involving 21 questions selected from the Diagnostic Interview for ADHD in Adults (DIVA) administered in English. Notably, DIVA is a standard questionnaire used by doctors for ADHD diagnosis.
The recording setup involves three GoPro cameras: a front-facing Camera 1 captures facial information, while side Cameras 2 and 3 record the left and right torsos and limbs at a resolution of <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="3840\times 2160" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">3840</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">2160</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">3840</cn><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">2160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">3840\times 2160</annotation></semantics></math> pixels. For our proposed mental disorder assessment and detection system, only facial information captured by Camera 1 is utilized.
Videos are segmented into 60-second clips, and each is labeled as either 0 (non-ADHD controls) or 1 (ADHD subjects). The ADHD dataset used in our proposed system comprises 188 video clips, partitioned into training, validation, and testing at a ratio of 6:1:3, respectively.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">For depression detection, we employ the AVEC 2014 dataset to train and evaluate our proposed system. This dataset consists of videos accompanied by BDI-II score labels, self-evaluated by the participants in each video. The labels span from 0 to 63 and are divided into four depression levels: minimal (0‚Äì13), mild (14‚Äì19), moderate (20‚Äì28), and severe (29‚Äì63). The AVEC 2014 dataset consists of a total of 300 video recordings,
which are divided into three categories: training, development, and testing sets. Each set contains two different types of video recordings: Freeform and Northwind. The Northwind task involves participants reading aloud the fable ‚ÄòThe North Wind and the Sun‚Äô in German. The Freeform task requires participants to answer a series of questions in German. The length of each video is approximately between 10 to 60 seconds.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Networks</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">As shown in Fig. 3, we design and present a two-stream multimodal system-based attention network with CNN and ResNet. The novelty is also in utilizing a pre-trained audio-based model to fine-tune the loss obtained from video input and get better results.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Based on the raw audio, we choose an attention-CNN structure as the main core network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
A simple attention module is added to the CNN structure. Compared to traditional CNN, it focuses on leveraging local feature connections while utilizing parallel computing to decrease training time. The weights in the convolution kernels are shared, and multiple convolution kernels can be used to extract multi-dimensional information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">There are 5 convolution layers that have 3<math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mo id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><times id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\times</annotation></semantics></math>3 kernels with 1 stride. Different from the original CNN audio classification network, we freeze the model after fully training the network. Then, we integrate this simple pre-train audio recognition model into the multimodal system to fine-tune the video recognition model.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">The loss of the audio-based recognition model (<math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="\ell_{S}" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><msub id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">‚Ñì</mi><mi id="S3.SS3.p4.1.m1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">‚Ñì</ci><ci id="S3.SS3.p4.1.m1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3">ùëÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\ell_{S}</annotation></semantics></math>) is to minimize the Mean Absolute Error (MAE) of the outputs and true label results:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\ell_{S}=\frac{1}{n}\sum_{i=1}^{n}\left|\hat{y}_{i}-y_{i}\right|" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi mathvariant="normal" id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">‚Ñì</mi><mi id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml">S</mi></msub><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mfrac id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mn id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.3.2.cmml">1</mn><mi id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><munderover id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E1.m1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.cmml">‚àë</mo><mrow id="S3.E1.m1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2.3.2" xref="S3.E1.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.1.1.1.1.2.2.3.1" xref="S3.E1.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.1.1.1.1.2.2.3.3" xref="S3.E1.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml">n</mi></munderover><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S3.E1.m1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.cmml">y</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.2.2.1" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">‚Ñì</ci><ci id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">ùëÜ</ci></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><divide id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3"></divide><cn type="integer" id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3.2">1</cn><ci id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3">ùëõ</ci></apply><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.E1.m1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2"></sum><apply id="S3.E1.m1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3"><eq id="S3.E1.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E1.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.2">ùëñ</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3">ùëõ</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><abs id="S3.E1.m1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"></abs><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2"><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.1">^</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2">ùë¶</ci></apply><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3">ùëñ</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2">ùë¶</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3">ùëñ</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\ell_{S}=\frac{1}{n}\sum_{i=1}^{n}\left|\hat{y}_{i}-y_{i}\right|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p4.5" class="ltx_p">where <math id="S3.SS3.p4.2.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.p4.2.m1.1a"><mi id="S3.SS3.p4.2.m1.1.1" xref="S3.SS3.p4.2.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m1.1b"><ci id="S3.SS3.p4.2.m1.1.1.cmml" xref="S3.SS3.p4.2.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m1.1c">n</annotation></semantics></math> means the number of samples in the dataset, the <math id="S3.SS3.p4.3.m2.1" class="ltx_Math" alttext="\hat{y}_{i}" display="inline"><semantics id="S3.SS3.p4.3.m2.1a"><msub id="S3.SS3.p4.3.m2.1.1" xref="S3.SS3.p4.3.m2.1.1.cmml"><mover accent="true" id="S3.SS3.p4.3.m2.1.1.2" xref="S3.SS3.p4.3.m2.1.1.2.cmml"><mi id="S3.SS3.p4.3.m2.1.1.2.2" xref="S3.SS3.p4.3.m2.1.1.2.2.cmml">y</mi><mo id="S3.SS3.p4.3.m2.1.1.2.1" xref="S3.SS3.p4.3.m2.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS3.p4.3.m2.1.1.3" xref="S3.SS3.p4.3.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m2.1b"><apply id="S3.SS3.p4.3.m2.1.1.cmml" xref="S3.SS3.p4.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m2.1.1.1.cmml" xref="S3.SS3.p4.3.m2.1.1">subscript</csymbol><apply id="S3.SS3.p4.3.m2.1.1.2.cmml" xref="S3.SS3.p4.3.m2.1.1.2"><ci id="S3.SS3.p4.3.m2.1.1.2.1.cmml" xref="S3.SS3.p4.3.m2.1.1.2.1">^</ci><ci id="S3.SS3.p4.3.m2.1.1.2.2.cmml" xref="S3.SS3.p4.3.m2.1.1.2.2">ùë¶</ci></apply><ci id="S3.SS3.p4.3.m2.1.1.3.cmml" xref="S3.SS3.p4.3.m2.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m2.1c">\hat{y}_{i}</annotation></semantics></math> and <math id="S3.SS3.p4.4.m3.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S3.SS3.p4.4.m3.1a"><msub id="S3.SS3.p4.4.m3.1.1" xref="S3.SS3.p4.4.m3.1.1.cmml"><mi id="S3.SS3.p4.4.m3.1.1.2" xref="S3.SS3.p4.4.m3.1.1.2.cmml">y</mi><mi id="S3.SS3.p4.4.m3.1.1.3" xref="S3.SS3.p4.4.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m3.1b"><apply id="S3.SS3.p4.4.m3.1.1.cmml" xref="S3.SS3.p4.4.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.4.m3.1.1.1.cmml" xref="S3.SS3.p4.4.m3.1.1">subscript</csymbol><ci id="S3.SS3.p4.4.m3.1.1.2.cmml" xref="S3.SS3.p4.4.m3.1.1.2">ùë¶</ci><ci id="S3.SS3.p4.4.m3.1.1.3.cmml" xref="S3.SS3.p4.4.m3.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m3.1c">y_{i}</annotation></semantics></math> are the predicted value and the true value of the <math id="S3.SS3.p4.5.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p4.5.m4.1a"><mi id="S3.SS3.p4.5.m4.1.1" xref="S3.SS3.p4.5.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m4.1b"><ci id="S3.SS3.p4.5.m4.1.1.cmml" xref="S3.SS3.p4.5.m4.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m4.1c">i</annotation></semantics></math>th sample, respectively.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.6" class="ltx_p">For the video-based recognition and classification network, we proposed a Cov-Attention module based on the ResNet backbone. This module aims to capture both global and local spatial-temporal information from video frames.
The network architecture is illustrated in Fig. 3.
In the first Cov1D layer, a convolution operation with a kernel size of 7<math id="S3.SS3.p5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.p5.1.m1.1a"><mo id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><times id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">\times</annotation></semantics></math>7<math id="S3.SS3.p5.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.p5.2.m2.1a"><mo id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><times id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">\times</annotation></semantics></math>7 and a stride of 1x2x2 is applied to extract and downsample low-level features. Subsequently, a 3<math id="S3.SS3.p5.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.p5.3.m3.1a"><mo id="S3.SS3.p5.3.m3.1.1" xref="S3.SS3.p5.3.m3.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.1b"><times id="S3.SS3.p5.3.m3.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.1c">\times</annotation></semantics></math>3<math id="S3.SS3.p5.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.p5.4.m4.1a"><mo id="S3.SS3.p5.4.m4.1.1" xref="S3.SS3.p5.4.m4.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.4.m4.1b"><times id="S3.SS3.p5.4.m4.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.4.m4.1c">\times</annotation></semantics></math>3 pooling layer with a stride of 1<math id="S3.SS3.p5.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.p5.5.m5.1a"><mo id="S3.SS3.p5.5.m5.1.1" xref="S3.SS3.p5.5.m5.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.5.m5.1b"><times id="S3.SS3.p5.5.m5.1.1.cmml" xref="S3.SS3.p5.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.5.m5.1c">\times</annotation></semantics></math>2<math id="S3.SS3.p5.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.p5.6.m6.1a"><mo id="S3.SS3.p5.6.m6.1.1" xref="S3.SS3.p5.6.m6.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.6.m6.1b"><times id="S3.SS3.p5.6.m6.1.1.cmml" xref="S3.SS3.p5.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.6.m6.1c">\times</annotation></semantics></math>2 is employed to further process the features. The processed features are then passed through a residual module, which comprises two bottleneck structures.
Notably, the attention module replaces the middle layer of the bottleneck and generates a weighted feature incorporating attention features. Following a stack of residual modules, an adaptive pooling layer resamples the feature into a fixed shape. Finally, the last fully connected layer predicts a score, serving as the output of the proposed system.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">The loss of the video-based recognition model (<math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="\ell_{V}" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><msub id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.p6.1.m1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.2.cmml">‚Ñì</mi><mi id="S3.SS3.p6.1.m1.1.1.3" xref="S3.SS3.p6.1.m1.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><apply id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.1.m1.1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p6.1.m1.1.1.2.cmml" xref="S3.SS3.p6.1.m1.1.1.2">‚Ñì</ci><ci id="S3.SS3.p6.1.m1.1.1.3.cmml" xref="S3.SS3.p6.1.m1.1.1.3">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">\ell_{V}</annotation></semantics></math>) is to minimize the Mean Absolute Error (MAE) of the video outputs and true label results:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\ell_{V}=\frac{1}{n}\sum_{j=1}^{n}\left|\hat{y}_{j}-y_{j}\right|" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi mathvariant="normal" id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">‚Ñì</mi><mi id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml">V</mi></msub><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mfrac id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml"><mn id="S3.E2.m1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.3.2.cmml">1</mn><mi id="S3.E2.m1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><munderover id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E2.m1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.cmml">‚àë</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S3.E2.m1.1.1.1.1.2.2.3.1" xref="S3.E2.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">n</mi></munderover><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S3.E2.m1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.2.cmml">y</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.2.2.1" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.2.3.cmml">j</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">‚Ñì</ci><ci id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3">ùëâ</ci></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><times id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3"><divide id="S3.E2.m1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.3"></divide><cn type="integer" id="S3.E2.m1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.3.2">1</cn><ci id="S3.E2.m1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.3.3">ùëõ</ci></apply><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3"><eq id="S3.E2.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3.2">ùëó</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.3">ùëõ</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><abs id="S3.E2.m1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"></abs><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2"><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.1">^</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.2">ùë¶</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.3">ùëó</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2">ùë¶</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3">ùëó</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\ell_{V}=\frac{1}{n}\sum_{j=1}^{n}\left|\hat{y}_{j}-y_{j}\right|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p6.5" class="ltx_p">where <math id="S3.SS3.p6.2.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.p6.2.m1.1a"><mi id="S3.SS3.p6.2.m1.1.1" xref="S3.SS3.p6.2.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m1.1b"><ci id="S3.SS3.p6.2.m1.1.1.cmml" xref="S3.SS3.p6.2.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m1.1c">n</annotation></semantics></math> means the number of samples in the dataset. The <math id="S3.SS3.p6.3.m2.1" class="ltx_Math" alttext="\hat{y}_{j}" display="inline"><semantics id="S3.SS3.p6.3.m2.1a"><msub id="S3.SS3.p6.3.m2.1.1" xref="S3.SS3.p6.3.m2.1.1.cmml"><mover accent="true" id="S3.SS3.p6.3.m2.1.1.2" xref="S3.SS3.p6.3.m2.1.1.2.cmml"><mi id="S3.SS3.p6.3.m2.1.1.2.2" xref="S3.SS3.p6.3.m2.1.1.2.2.cmml">y</mi><mo id="S3.SS3.p6.3.m2.1.1.2.1" xref="S3.SS3.p6.3.m2.1.1.2.1.cmml">^</mo></mover><mi id="S3.SS3.p6.3.m2.1.1.3" xref="S3.SS3.p6.3.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.3.m2.1b"><apply id="S3.SS3.p6.3.m2.1.1.cmml" xref="S3.SS3.p6.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.3.m2.1.1.1.cmml" xref="S3.SS3.p6.3.m2.1.1">subscript</csymbol><apply id="S3.SS3.p6.3.m2.1.1.2.cmml" xref="S3.SS3.p6.3.m2.1.1.2"><ci id="S3.SS3.p6.3.m2.1.1.2.1.cmml" xref="S3.SS3.p6.3.m2.1.1.2.1">^</ci><ci id="S3.SS3.p6.3.m2.1.1.2.2.cmml" xref="S3.SS3.p6.3.m2.1.1.2.2">ùë¶</ci></apply><ci id="S3.SS3.p6.3.m2.1.1.3.cmml" xref="S3.SS3.p6.3.m2.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.3.m2.1c">\hat{y}_{j}</annotation></semantics></math> and <math id="S3.SS3.p6.4.m3.1" class="ltx_Math" alttext="y_{j}" display="inline"><semantics id="S3.SS3.p6.4.m3.1a"><msub id="S3.SS3.p6.4.m3.1.1" xref="S3.SS3.p6.4.m3.1.1.cmml"><mi id="S3.SS3.p6.4.m3.1.1.2" xref="S3.SS3.p6.4.m3.1.1.2.cmml">y</mi><mi id="S3.SS3.p6.4.m3.1.1.3" xref="S3.SS3.p6.4.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.4.m3.1b"><apply id="S3.SS3.p6.4.m3.1.1.cmml" xref="S3.SS3.p6.4.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.4.m3.1.1.1.cmml" xref="S3.SS3.p6.4.m3.1.1">subscript</csymbol><ci id="S3.SS3.p6.4.m3.1.1.2.cmml" xref="S3.SS3.p6.4.m3.1.1.2">ùë¶</ci><ci id="S3.SS3.p6.4.m3.1.1.3.cmml" xref="S3.SS3.p6.4.m3.1.1.3">ùëó</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.4.m3.1c">y_{j}</annotation></semantics></math> are the predicted value and the true value of the <math id="S3.SS3.p6.5.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS3.p6.5.m4.1a"><mi id="S3.SS3.p6.5.m4.1.1" xref="S3.SS3.p6.5.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.5.m4.1b"><ci id="S3.SS3.p6.5.m4.1.1.cmml" xref="S3.SS3.p6.5.m4.1.1">ùëó</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.5.m4.1c">j</annotation></semantics></math>th sample, respectively.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p id="S3.SS3.p7.3" class="ltx_p">We propose to perform loss fusion in this system, using the loss <math id="S3.SS3.p7.1.m1.1" class="ltx_Math" alttext="\ell_{S}" display="inline"><semantics id="S3.SS3.p7.1.m1.1a"><msub id="S3.SS3.p7.1.m1.1.1" xref="S3.SS3.p7.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.p7.1.m1.1.1.2" xref="S3.SS3.p7.1.m1.1.1.2.cmml">‚Ñì</mi><mi id="S3.SS3.p7.1.m1.1.1.3" xref="S3.SS3.p7.1.m1.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.1.m1.1b"><apply id="S3.SS3.p7.1.m1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p7.1.m1.1.1.1.cmml" xref="S3.SS3.p7.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p7.1.m1.1.1.2.cmml" xref="S3.SS3.p7.1.m1.1.1.2">‚Ñì</ci><ci id="S3.SS3.p7.1.m1.1.1.3.cmml" xref="S3.SS3.p7.1.m1.1.1.3">ùëÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.1.m1.1c">\ell_{S}</annotation></semantics></math> of an independent pre-train audio recognition model to fine-tune the loss <math id="S3.SS3.p7.2.m2.1" class="ltx_Math" alttext="\ell_{V}" display="inline"><semantics id="S3.SS3.p7.2.m2.1a"><msub id="S3.SS3.p7.2.m2.1.1" xref="S3.SS3.p7.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.p7.2.m2.1.1.2" xref="S3.SS3.p7.2.m2.1.1.2.cmml">‚Ñì</mi><mi id="S3.SS3.p7.2.m2.1.1.3" xref="S3.SS3.p7.2.m2.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.2.m2.1b"><apply id="S3.SS3.p7.2.m2.1.1.cmml" xref="S3.SS3.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p7.2.m2.1.1.1.cmml" xref="S3.SS3.p7.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p7.2.m2.1.1.2.cmml" xref="S3.SS3.p7.2.m2.1.1.2">‚Ñì</ci><ci id="S3.SS3.p7.2.m2.1.1.3.cmml" xref="S3.SS3.p7.2.m2.1.1.3">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.2.m2.1c">\ell_{V}</annotation></semantics></math> on the video side to achieve more accurate recognition results. The fine-tune loss <math id="S3.SS3.p7.3.m3.1" class="ltx_Math" alttext="\ell_{B}" display="inline"><semantics id="S3.SS3.p7.3.m3.1a"><msub id="S3.SS3.p7.3.m3.1.1" xref="S3.SS3.p7.3.m3.1.1.cmml"><mi mathvariant="normal" id="S3.SS3.p7.3.m3.1.1.2" xref="S3.SS3.p7.3.m3.1.1.2.cmml">‚Ñì</mi><mi id="S3.SS3.p7.3.m3.1.1.3" xref="S3.SS3.p7.3.m3.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.3.m3.1b"><apply id="S3.SS3.p7.3.m3.1.1.cmml" xref="S3.SS3.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p7.3.m3.1.1.1.cmml" xref="S3.SS3.p7.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p7.3.m3.1.1.2.cmml" xref="S3.SS3.p7.3.m3.1.1.2">‚Ñì</ci><ci id="S3.SS3.p7.3.m3.1.1.3.cmml" xref="S3.SS3.p7.3.m3.1.1.3">ùêµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.3.m3.1c">\ell_{B}</annotation></semantics></math> is defined as:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\ell_{B}=\alpha\cdot\ell_{S}+\beta\cdot\ell_{V}" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml"><mi mathvariant="normal" id="S3.E3.m1.1.1.2.2" xref="S3.E3.m1.1.1.2.2.cmml">‚Ñì</mi><mi id="S3.E3.m1.1.1.2.3" xref="S3.E3.m1.1.1.2.3.cmml">B</mi></msub><mo id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mrow id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><mi id="S3.E3.m1.1.1.3.2.2" xref="S3.E3.m1.1.1.3.2.2.cmml">Œ±</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.1.3.2.1" xref="S3.E3.m1.1.1.3.2.1.cmml">‚ãÖ</mo><msub id="S3.E3.m1.1.1.3.2.3" xref="S3.E3.m1.1.1.3.2.3.cmml"><mi mathvariant="normal" id="S3.E3.m1.1.1.3.2.3.2" xref="S3.E3.m1.1.1.3.2.3.2.cmml">‚Ñì</mi><mi id="S3.E3.m1.1.1.3.2.3.3" xref="S3.E3.m1.1.1.3.2.3.3.cmml">S</mi></msub></mrow><mo id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml">Œ≤</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.1.3.3.1" xref="S3.E3.m1.1.1.3.3.1.cmml">‚ãÖ</mo><msub id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml"><mi mathvariant="normal" id="S3.E3.m1.1.1.3.3.3.2" xref="S3.E3.m1.1.1.3.3.3.2.cmml">‚Ñì</mi><mi id="S3.E3.m1.1.1.3.3.3.3" xref="S3.E3.m1.1.1.3.3.3.3.cmml">V</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"></eq><apply id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.2.2">‚Ñì</ci><ci id="S3.E3.m1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.2.3">ùêµ</ci></apply><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><plus id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></plus><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><ci id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2.1">‚ãÖ</ci><ci id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2">ùõº</ci><apply id="S3.E3.m1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.3.2.cmml" xref="S3.E3.m1.1.1.3.2.3.2">‚Ñì</ci><ci id="S3.E3.m1.1.1.3.2.3.3.cmml" xref="S3.E3.m1.1.1.3.2.3.3">ùëÜ</ci></apply></apply><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><ci id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.1">‚ãÖ</ci><ci id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2">ùõΩ</ci><apply id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.3.2">‚Ñì</ci><ci id="S3.E3.m1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3.3">ùëâ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\ell_{B}=\alpha\cdot\ell_{S}+\beta\cdot\ell_{V}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p7.5" class="ltx_p">where the <math id="S3.SS3.p7.4.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS3.p7.4.m1.1a"><mi id="S3.SS3.p7.4.m1.1.1" xref="S3.SS3.p7.4.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.4.m1.1b"><ci id="S3.SS3.p7.4.m1.1.1.cmml" xref="S3.SS3.p7.4.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.4.m1.1c">\alpha</annotation></semantics></math> and <math id="S3.SS3.p7.5.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS3.p7.5.m2.1a"><mi id="S3.SS3.p7.5.m2.1.1" xref="S3.SS3.p7.5.m2.1.1.cmml">Œ≤</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p7.5.m2.1b"><ci id="S3.SS3.p7.5.m2.1.1.cmml" xref="S3.SS3.p7.5.m2.1.1">ùõΩ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p7.5.m2.1c">\beta</annotation></semantics></math> are the mixing coefficients to achieve the highest recognition effect through grid search and set empirically to 0.6 and 0.4, respectively.</p>
</div>
<div id="S3.SS3.p8" class="ltx_para">
<p id="S3.SS3.p8.1" class="ltx_p">Based on this basic system, we adapt two popular networks, i.e., LSTM and 3D-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, to the video-based classification tasks. We also compare the proposed method with other commonly used audio-based recognition networks (LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. To ensure the fairness of the evaluation, we use the same fusion method and parameter settings as in our proposed system.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Preprocessing and Experimental Settings</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To optimize the system‚Äôs capacity to extract features from multimodal inputs, it is advantageous to perform preprocessing on the audio and video signals. By subjecting the preprocessing process, such as normalization, denoising, and feature extraction, the system can better discern relevant information from both modalities, thereby improving the performance of the system.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2409.02243/assets/Images/facial2.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S4.F4.2.1" class="ltx_text" style="font-size:80%;">Illustration of the facial reactions closely associated with mental disorders, i.e., ADHD (a) and depression (b), randomly selected subjects from multimodal ADHD data and AVEC 2014 data, respectively. </span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">For optimal preservation of short-term facial expressions, we initially extract the raw video input as individual images at a 1-frame interval. Our preprocessing utilizes the Dlib toolkit to precisely extract facial landmarks from the sequence of frames, effectively eliminating background interference and aligning human faces to minimize environmental disruptions. As shown in Fig. 4, during alignment, we center the facial features precisely between the eyes and adjust the vertical distance between the eyes and mouth to occupy one-third of the image‚Äôs height. Subsequently, the aligned facial images are resized to the dimension of 224 √ó 224 pixels for further processing.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">We extract audio data from the original recordings and proceed to eliminate background noise from the extracted audio samples. This process is achieved using the noisereduce() function in Python.
Fig. 5 shows the audio spectrograms separated by denoising from the original recordings. The noise threshold is determined based on statistical analysis performed across the audio clip.
Following the denoising process, the raw audios are segmented into clips of a 2-second duration and organized within the same structure as the corresponding video dataset.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2409.02243/assets/Images/audio8.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="663" height="491" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S4.F5.2.1" class="ltx_text" style="font-size:80%;">Illustration of the audio spectrogram from a randomly selected ADHD subject, depression subject, and neurotypical control. </span></figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.2" class="ltx_p">Our proposed system is trained on the AVEC 2014 and the real multimodal ADHD dataset. We subsequently validate and test it on the development/validation and testing sets of both datasets, respectively.
For the training period for the multimodal system, we randomly select a sequence of 64 frames from a given audio-visual recording at a stochastic position to form a training clip. To augment the input data during training, we apply random horizontal flips and adjust brightness, contrast, saturation, and hue within the range of 0 to 0.1 for all frames in one video clip.
During the testing phase, for a given testing audio-visual recording, we crop it into a sub-video with 64 frames and compare it with the indicated audio group, predicting each group to calculate the mean value of depression scores and the ADHD classification probability score. The training epochs for the audio model is 100, with the learning rate empirically set to <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="1\times 10^{-4}" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mn id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml">√ó</mo><msup id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml"><mn id="S4.SS1.p4.1.m1.1.1.3.2" xref="S4.SS1.p4.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p4.1.m1.1.1.3.3" xref="S4.SS1.p4.1.m1.1.1.3.3.cmml"><mo id="S4.SS1.p4.1.m1.1.1.3.3a" xref="S4.SS1.p4.1.m1.1.1.3.3.cmml">‚àí</mo><mn id="S4.SS1.p4.1.m1.1.1.3.3.2" xref="S4.SS1.p4.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><times id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">1</cn><apply id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p4.1.m1.1.1.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2">10</cn><apply id="S4.SS1.p4.1.m1.1.1.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3"><minus id="S4.SS1.p4.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS1.p4.1.m1.1.1.3.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">1\times 10^{-4}</annotation></semantics></math>. The training epochs for the multimodal network are 150, and the learning rate is empirically set to <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="1\times 10^{-3}" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mrow id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml"><mn id="S4.SS1.p4.2.m2.1.1.2" xref="S4.SS1.p4.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p4.2.m2.1.1.1" xref="S4.SS1.p4.2.m2.1.1.1.cmml">√ó</mo><msup id="S4.SS1.p4.2.m2.1.1.3" xref="S4.SS1.p4.2.m2.1.1.3.cmml"><mn id="S4.SS1.p4.2.m2.1.1.3.2" xref="S4.SS1.p4.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p4.2.m2.1.1.3.3" xref="S4.SS1.p4.2.m2.1.1.3.3.cmml"><mo id="S4.SS1.p4.2.m2.1.1.3.3a" xref="S4.SS1.p4.2.m2.1.1.3.3.cmml">‚àí</mo><mn id="S4.SS1.p4.2.m2.1.1.3.3.2" xref="S4.SS1.p4.2.m2.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><apply id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"><times id="S4.SS1.p4.2.m2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.p4.2.m2.1.1.2.cmml" xref="S4.SS1.p4.2.m2.1.1.2">1</cn><apply id="S4.SS1.p4.2.m2.1.1.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.1.1.3.1.cmml" xref="S4.SS1.p4.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p4.2.m2.1.1.3.2.cmml" xref="S4.SS1.p4.2.m2.1.1.3.2">10</cn><apply id="S4.SS1.p4.2.m2.1.1.3.3.cmml" xref="S4.SS1.p4.2.m2.1.1.3.3"><minus id="S4.SS1.p4.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.p4.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS1.p4.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.p4.2.m2.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">1\times 10^{-3}</annotation></semantics></math>. All experiments are run on a workstation with four Nvidia GTX 1080Ti GPUs.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Results and Discussions</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">As mentioned in Section III, our proposed system differs from traditional machine learning approaches based on fMRI and EEG. Instead, it concentrates on creating multimodal systems for detecting and assessing mental disorders using video and audio data collected by common sensors. These multimodal systems have the advantage of correcting or compensating for errors in detection that can arise from single-modality inputs, making them especially valuable for such related medical research.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Our proposed system‚Äôs performance is evaluated by comparing it with LSTM and 3D-CNN networks on this real multimodal ADHD dataset with precision, accuracy, and Area Under the Curve (AUC) to evaluate the classification performance of the system.
For the results of the AVEC 2014 dataset for depression, we utilize Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).
The experimental results are summarized in Table I and Table II.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span><span id="S4.T1.2.1" class="ltx_text" style="font-size:80%;">ADHD detection performance with different neural networks. <span id="S4.T1.2.1.1" class="ltx_text ltx_font_bold">Bold</span> indicates the best results. 
<br class="ltx_break"></span></figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.3.1.1" class="ltx_tr">
<th id="S4.T1.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.1.1" class="ltx_text ltx_font_bold">Audio</span></th>
<th id="S4.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.3.1.1.2.1" class="ltx_text ltx_font_bold">Video</span></th>
<th id="S4.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.3.1.1.3.1" class="ltx_text ltx_font_bold">Precision</span></th>
<th id="S4.T1.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.3.1.1.4.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
<th id="S4.T1.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.3.1.1.5.1" class="ltx_text ltx_font_bold">AUC</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.3.2.1" class="ltx_tr">
<th id="S4.T1.3.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">LSTM</th>
<th id="S4.T1.3.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">3D CNN</th>
<td id="S4.T1.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t">75.58</td>
<td id="S4.T1.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t">59.60</td>
<td id="S4.T1.3.2.1.5" class="ltx_td ltx_align_center ltx_border_t">64.84</td>
</tr>
<tr id="S4.T1.3.3.2" class="ltx_tr">
<th id="S4.T1.3.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">LSTM</th>
<th id="S4.T1.3.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">LSTM</th>
<td id="S4.T1.3.3.2.3" class="ltx_td ltx_align_center ltx_border_t">65.67</td>
<td id="S4.T1.3.3.2.4" class="ltx_td ltx_align_center ltx_border_t">47.98</td>
<td id="S4.T1.3.3.2.5" class="ltx_td ltx_align_center ltx_border_t">53.13</td>
</tr>
<tr id="S4.T1.3.4.3" class="ltx_tr">
<th id="S4.T1.3.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Attention CNN</th>
<th id="S4.T1.3.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">3D CNN</th>
<td id="S4.T1.3.4.3.3" class="ltx_td ltx_align_center ltx_border_t">76.19</td>
<td id="S4.T1.3.4.3.4" class="ltx_td ltx_align_center ltx_border_t">65.16</td>
<td id="S4.T1.3.4.3.5" class="ltx_td ltx_align_center ltx_border_t">68.38</td>
</tr>
<tr id="S4.T1.3.5.4" class="ltx_tr">
<th id="S4.T1.3.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Attention CNN</th>
<th id="S4.T1.3.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">LSTM</th>
<td id="S4.T1.3.5.4.3" class="ltx_td ltx_align_center ltx_border_t">71.71</td>
<td id="S4.T1.3.5.4.4" class="ltx_td ltx_align_center ltx_border_t">70.57</td>
<td id="S4.T1.3.5.4.5" class="ltx_td ltx_align_center ltx_border_t">67.20</td>
</tr>
<tr id="S4.T1.3.6.5" class="ltx_tr">
<th id="S4.T1.3.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.6.5.1.1" class="ltx_text ltx_font_bold">Attention CNN</span></th>
<th id="S4.T1.3.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.3.6.5.2.1" class="ltx_text ltx_font_bold">Cov-Attention</span></th>
<td id="S4.T1.3.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.3.6.5.3.1" class="ltx_text ltx_font_bold">81.08</span></td>
<td id="S4.T1.3.6.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.3.6.5.4.1" class="ltx_text ltx_font_bold">82.22</span></td>
<td id="S4.T1.3.6.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.3.6.5.5.1" class="ltx_text ltx_font_bold">77.35</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span><span id="S4.T2.2.1" class="ltx_text" style="font-size:80%;">Depression detection performance with different neural networks. <span id="S4.T2.2.1.1" class="ltx_text ltx_font_bold">Bold</span> indicates the best results. 
<br class="ltx_break"></span></figcaption>
<table id="S4.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.1.1" class="ltx_tr">
<th id="S4.T2.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.3.1.1.1.1" class="ltx_text ltx_font_bold">Audio</span></th>
<th id="S4.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.3.1.1.2.1" class="ltx_text ltx_font_bold">Video</span></th>
<th id="S4.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.3.1.1.3.1" class="ltx_text ltx_font_bold">MAE</span></th>
<th id="S4.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T2.3.1.1.4.1" class="ltx_text ltx_font_bold">RMSE</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.2.1" class="ltx_tr">
<th id="S4.T2.3.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">LSTM</th>
<th id="S4.T2.3.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">3D-CNN</th>
<td id="S4.T2.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t">8.81</td>
<td id="S4.T2.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t">10.04</td>
</tr>
<tr id="S4.T2.3.3.2" class="ltx_tr">
<th id="S4.T2.3.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">LSTM</th>
<th id="S4.T2.3.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">LSTM</th>
<td id="S4.T2.3.3.2.3" class="ltx_td ltx_align_center ltx_border_t">10.43</td>
<td id="S4.T2.3.3.2.4" class="ltx_td ltx_align_center ltx_border_t">12.11</td>
</tr>
<tr id="S4.T2.3.4.3" class="ltx_tr">
<th id="S4.T2.3.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Attention CNN</th>
<th id="S4.T2.3.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">3D CNN</th>
<td id="S4.T2.3.4.3.3" class="ltx_td ltx_align_center ltx_border_t">7.67</td>
<td id="S4.T2.3.4.3.4" class="ltx_td ltx_align_center ltx_border_t">9.24</td>
</tr>
<tr id="S4.T2.3.5.4" class="ltx_tr">
<th id="S4.T2.3.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Attention CNN</th>
<th id="S4.T2.3.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">LSTM</th>
<td id="S4.T2.3.5.4.3" class="ltx_td ltx_align_center ltx_border_t">8.52</td>
<td id="S4.T2.3.5.4.4" class="ltx_td ltx_align_center ltx_border_t">10.20</td>
</tr>
<tr id="S4.T2.3.6.5" class="ltx_tr">
<th id="S4.T2.3.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.3.6.5.1.1" class="ltx_text ltx_font_bold">Attention CNN</span></th>
<th id="S4.T2.3.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.3.6.5.2.1" class="ltx_text ltx_font_bold">Cov-Attention</span></th>
<td id="S4.T2.3.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.3.6.5.3.1" class="ltx_text ltx_font_bold">7.23</span></td>
<td id="S4.T2.3.6.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.3.6.5.4.1" class="ltx_text ltx_font_bold">9.36</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">From Table I, the proposed system shows good classification ability on the real multimodal ADHD dataset. At the same time, the results from Table II show that the proposed audio-visual attention network is significantly higher than the other fusion networks in MAE and RMSE.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">We also provided part of state-of-the-art results compared to our proposed method in Table III and Table IV. It should be emphasized that, due to medical confidentiality requirements, there is no publicly available ADHD multimodal dataset. Therefore, we evaluated the performance of state-of-the-art ADHD detection systems on various datasets containing EEG and daily activities videos.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span><span id="S4.T3.2.1" class="ltx_text" style="font-size:80%;">Comparison with STATE-OF-THE-ART METHODS for ADHD Detection.
<br class="ltx_break"></span></figcaption>
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.3.1.1" class="ltx_tr">
<th id="S4.T3.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Author</span></th>
<td id="S4.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Data Input</span></td>
<td id="S4.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Classifier</span></td>
<td id="S4.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Accuracy</span></td>
<td id="S4.T3.3.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Equipment Price ($)</span></td>
</tr>
<tr id="S4.T3.3.2.2" class="ltx_tr">
<th id="S4.T3.3.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><em id="S4.T3.3.2.2.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">Luo et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite></em></th>
<td id="S4.T3.3.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.2.2.2.1" class="ltx_text" style="font-size:70%;">MRI &amp; DTI</span></td>
<td id="S4.T3.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.2.2.3.1" class="ltx_text" style="font-size:70%;">CNN</span></td>
<td id="S4.T3.3.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.2.2.4.1" class="ltx_text" style="font-size:70%;">76.6%</span></td>
<td id="S4.T3.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.2.2.5.1" class="ltx_text" style="font-size:70%;">150,000-1,000,000</span></td>
</tr>
<tr id="S4.T3.3.3.3" class="ltx_tr">
<th id="S4.T3.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><em id="S4.T3.3.3.3.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">Peng et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite></em></th>
<td id="S4.T3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.3.3.2.1" class="ltx_text" style="font-size:70%;">fMRI</span></td>
<td id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.3.3.3.1" class="ltx_text" style="font-size:70%;">CNN</span></td>
<td id="S4.T3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.3.3.4.1" class="ltx_text" style="font-size:70%;">72.9%</span></td>
<td id="S4.T3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.3.3.5.1" class="ltx_text" style="font-size:70%;">150,000-1,000,000</span></td>
</tr>
<tr id="S4.T3.3.4.4" class="ltx_tr">
<th id="S4.T3.3.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><em id="S4.T3.3.4.4.1.1" class="ltx_emph ltx_font_italic" style="font-size:70%;">Vahid et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></em></th>
<td id="S4.T3.3.4.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.4.4.2.1" class="ltx_text" style="font-size:70%;">EEG</span></td>
<td id="S4.T3.3.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.4.4.3.1" class="ltx_text" style="font-size:70%;">CNN</span></td>
<td id="S4.T3.3.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.4.4.4.1" class="ltx_text" style="font-size:70%;">80.3%</span></td>
<td id="S4.T3.3.4.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.4.4.5.1" class="ltx_text" style="font-size:70%;">1,000-25,000</span></td>
</tr>
<tr id="S4.T3.3.5.5" class="ltx_tr">
<th id="S4.T3.3.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><em id="S4.T3.3.5.5.1.1" class="ltx_emph ltx_font_bold ltx_font_italic" style="font-size:70%;">Proposed method</em></th>
<td id="S4.T3.3.5.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.5.5.2.1" class="ltx_text" style="font-size:70%;">Audio-Visual</span></td>
<td id="S4.T3.3.5.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.5.5.3.1" class="ltx_text" style="font-size:70%;">Cov-attention</span></td>
<td id="S4.T3.3.5.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.5.5.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">82.22%</span></td>
<td id="S4.T3.3.5.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S4.T3.3.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">450</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span><span id="S4.T4.2.1" class="ltx_text" style="font-size:80%;">Comparison with state-of-the-art methods for Depression Assessment. 
<br class="ltx_break"></span></figcaption>
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.3.1.1" class="ltx_tr">
<th id="S4.T4.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T4.3.1.1.1.1" class="ltx_text ltx_font_bold">Author</span></th>
<td id="S4.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.3.1.1.2.1" class="ltx_text ltx_font_bold">Features</span></td>
<td id="S4.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.3.1.1.3.1" class="ltx_text ltx_font_bold">MAE</span></td>
</tr>
<tr id="S4.T4.3.2.2" class="ltx_tr">
<th id="S4.T4.3.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><em id="S4.T4.3.2.2.1.1" class="ltx_emph ltx_font_italic">Valstar et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite></em></th>
<td id="S4.T4.3.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Hand-craft features</td>
<td id="S4.T4.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t">10.03</td>
</tr>
<tr id="S4.T4.3.3.3" class="ltx_tr">
<th id="S4.T4.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><em id="S4.T4.3.3.3.1.1" class="ltx_emph ltx_font_italic">Niu et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></em></th>
<td id="S4.T4.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Fourier
spectrogram</td>
<td id="S4.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">7.65</td>
</tr>
<tr id="S4.T4.3.4.4" class="ltx_tr">
<th id="S4.T4.3.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><em id="S4.T4.3.4.4.1.1" class="ltx_emph ltx_font_italic">Niu et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite></em></th>
<td id="S4.T4.3.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mel spectrogram</td>
<td id="S4.T4.3.4.4.3" class="ltx_td ltx_align_center ltx_border_t">7.67</td>
</tr>
<tr id="S4.T4.3.5.5" class="ltx_tr">
<th id="S4.T4.3.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><em id="S4.T4.3.5.5.1.1" class="ltx_emph ltx_font_italic">Du et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite></em></th>
<td id="S4.T4.3.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Video</td>
<td id="S4.T4.3.5.5.3" class="ltx_td ltx_align_center ltx_border_t">7.28</td>
</tr>
<tr id="S4.T4.3.6.6" class="ltx_tr">
<th id="S4.T4.3.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.3.6.6.1.1" class="ltx_text ltx_font_bold">Proposed method</span></th>
<td id="S4.T4.3.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T4.3.6.6.2.1" class="ltx_text ltx_font_bold">Audio-Visual Information</span></td>
<td id="S4.T4.3.6.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T4.3.6.6.3.1" class="ltx_text ltx_font_bold">7.23</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">The proposed audio-visual fusion emphasizes extracting emotional information, specifically relevant symptom features of mental disorders, from both audio and video inputs. In this system, the Cov-Attention model captures expression and emotional cues across multiple contiguous frames, utilizing both spatial and temporal dimensions of video data. This model is crucial for accurate diagnosis and classification in related tasks. Additionally, the attention-CNN model in the audio recognition module effectively captures frequency and dimensional features present in speech signals,i.e., enhancing performance in speech-based detection and diagnosis while introducing the pre-train model. Overall, the proposed system exhibits strong performance in the detection tasks of the ADHD multimodal dataset, achieving high accuracy using only cost-effective audio-visual information data. Additionally, it demonstrates robust performance in the assessment of depression using the AVEC 2014 dataset.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">We conduct ablation studies to assess the diagnostic performance of each module within our proposed system on the same AVEC 2014 dataset and multimodal ADHD dataset. The corresponding results are presented in Table V and Fig. 6, respectively.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Ablation study of four contributions in the proposed method. <span id="S4.T5.2.1" class="ltx_text ltx_font_bold">Bold</span> indicates the best results.</figcaption>
<table id="S4.T5.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.3.1.1" class="ltx_tr">
<th id="S4.T5.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">Ablation Settings</th>
<th id="S4.T5.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" rowspan="2"><span id="S4.T5.3.1.1.2.1" class="ltx_text">Backbone</span></th>
<th id="S4.T5.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" rowspan="2"><span id="S4.T5.3.1.1.3.1" class="ltx_text">MAE</span></th>
</tr>
<tr id="S4.T5.3.2.2" class="ltx_tr">
<th id="S4.T5.3.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Audio</th>
<th id="S4.T5.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Video</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.3.3.1" class="ltx_tr">
<td id="S4.T5.3.3.1.1" class="ltx_td ltx_align_center ltx_border_t">‚úì</td>
<td id="S4.T5.3.3.1.2" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T5.3.3.1.3" class="ltx_td ltx_align_center ltx_border_t">Attention-CNN</td>
<td id="S4.T5.3.3.1.4" class="ltx_td ltx_align_center ltx_border_t">7.64</td>
</tr>
<tr id="S4.T5.3.4.2" class="ltx_tr">
<td id="S4.T5.3.4.2.1" class="ltx_td ltx_align_center">‚úó</td>
<td id="S4.T5.3.4.2.2" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T5.3.4.2.3" class="ltx_td ltx_align_center">Cov-Attention</td>
<td id="S4.T5.3.4.2.4" class="ltx_td ltx_align_center">7.58</td>
</tr>
<tr id="S4.T5.3.5.3" class="ltx_tr">
<td id="S4.T5.3.5.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">‚úì</td>
<td id="S4.T5.3.5.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">‚úì</td>
<td id="S4.T5.3.5.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T5.3.5.3.3.1" class="ltx_text ltx_font_bold">Attention+Cov-Attention</span></td>
<td id="S4.T5.3.5.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">7.23</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2409.02243/assets/Images/roc1.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S4.F6.2.1" class="ltx_text" style="font-size:80%;">ROC curve for ADHD classification ablation study result. </span></figcaption>
</figure>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p">Based on the results of the ablation studies, firstly, both the audio and video-based classification networks exhibit a notable level of robustness in assessing and detecting various psychological and emotional features, underscoring their efficacy in cross-mental disorders.
Secondly, fine-tuning the video model using simple pre-trained audio models leads to significant improvements in classification accuracy and performance across different experiments.
Thirdly, by leveraging the strengths of both network models and exploiting the features from different modalities, we have developed a comprehensive system for assessing and detecting various mental disorders. This integrated approach yields superior performance, particularly evident in depression assessment, where the MAE on the AVEC 2014 dataset is 7.23, and the AUC on the ADHD dataset is 0.77.</p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p">The aforementioned results highlight the similar related symptoms in emotional expression, including facial expressions and speech, across different mental disorders. They also indicate the feasibility of evaluating and screening multiple mental disorders through a unified multimodal system.
Moreover, while video-based depression assessment performs slightly better than only audio-based depression assessment, the opposite is observed for ADHD diagnosis. This discrepancy may be attributed to differences in symptom manifestation among various mental disorders. We note that the ADHD data are collected from interview videos, potentially amplifying the prominence of speech characteristics over facial expressions.
In our future work, we intend to delve deeper into these findings through more comprehensive experiments and introduce more multimodal data, such as EEG and fMRI, for fusion and evaluation of the related disorders.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper presented an innovative multimodal detection system for identifying and detecting various mental disorders. The proposed system demonstrated state-of-the-art assessment and classification capabilities on depression and ADHD datasets, respectively. By leveraging a simple pre-train audio model to fine-tune video data, our system achieved promising results, as evidenced by comparative and ablation study experiments. Compared to conventional machine learning methods based on EEG and fMRI, our system offered cost-effectiveness and broader applicability, pointing to a promising direction in clinical practice.
For future research, we aim to broaden the scope of our proposed system to encompass a wider array of mental disorders with a larger sample size. It should be noted that this paper aims to explore the application of fusion systems based on audio-visual features in mental disorder assessment and detection. Further feature fusion experiments and more comprehensive detection results will be addressed in this work‚Äôs future journal paper.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We would like to express our gratitude to Dr. Rejesh Nair from the United Kingdom National Health Service (NHS), UK, for his professional medical advice and help and all participants and volunteers for the multimodal ADHD data recording. Especially the Cumbria, Northumberland, Tyne, and Wear (CNTW) NHS Foundation Trust, one of the largest mental health and disability Trusts in England.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
C.¬†Nash, R.¬†Nair, and S.¬†M. Naqvi, ‚ÄúMachine learning and ADHD mental health detection-a short survey,‚Äù <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Information Fusion (FUSION)</em>, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Y.¬†Chen, Y.¬†Tang, C.¬†Wang, X.¬†Liu, L.¬†Zhao, and Z.¬†Wang, ‚ÄúADHD classification by dual subspace learning using resting-state functional connectivity,‚Äù <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence in medicine</em>, vol. 103, p. 101786, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M.¬†Sajjadian, R.¬†W. Lam, R.¬†Milev, S.¬†Rotzinger, B.¬†N. Frey, C.¬†N. Soares, S.¬†V. Parikh, J.¬†A. Foster, G.¬†Turecki, D.¬†J. M√ºller <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúMachine learning in the prediction of depression treatment outcomes: a systematic review and meta-analysis,‚Äù <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">Psychological Medicine</em>, vol.¬†51, no.¬†16, pp. 2742‚Äì2751, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S.¬†Li, Y.¬†Sun, R.¬†Nair, and S.¬†M. Naqvi, ‚ÄúEnhancing ADHD detection using DIVA interview-based audio signals and a two-stream network,‚Äù <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE International Performance Computing and Communications Conference (IPCC)</em>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C.¬†Nash, R.¬†Nair, and S.¬†M. Naqvi, ‚ÄúMachine learning in ADHD and depression mental health diagnosis: a survey,‚Äù <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol.¬†11, no.¬†2, pp. 86‚Äâ297‚Äì86‚Äâ317, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
C.¬†Ouyang, Y.¬†Chiu, C.¬†Chiang, R.¬†Wu, Y.¬†Lin, R.¬†Yang, and L.¬†Lin, ‚ÄúEvaluating therapeutic effects of ADHD medication objectively by movement quantification with a video-based skeleton analysis,‚Äù <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">International Journal of Environmental Research and Public Health</em>, vol.¬†18, no.¬†17, p. 9363, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S.¬†Li, R.¬†Nair, and M.¬†Naqvi, ‚ÄúAcoustic and text features analysis for adult ADHD screening: A data-driven approach utilizing DIVA interview,‚Äù <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Translational Engineering in Health and Medicine</em>, pp. 1‚Äì1, 2024.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
NHS, United Kingdom National Health Service, https://www.gov.uk/government/publications/severe-mental-illness-smi-physical-health-inequalities/severe-mental-illness-and-physical-health-inequalities-briefing.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A.¬†P. Americana, ‚ÄúDiagnostic and statistical manual of mental disorders,‚Äù <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">The American Psychiatric Association</em>, vol.¬†5, pp. 591‚Äì643, 2013.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y.¬†Li, R.¬†Nair, and S.¬†M. Naqvi, ‚ÄúVideo-based skeleton data analysis for ADHD detection,‚Äù in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Symposium Series on Computational Intelligence (SSCI)</em>, 2023, pp. 1‚Äì6.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
W.¬†C. De¬†Melo, E.¬†Granger, and M.¬†B. Lopez, ‚ÄúEncoding temporal information for automatic depression recognition from facial analysis,‚Äù in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y.¬†Li, S.¬†Li, C.¬†Nash, S.¬†M. Naqvi, and R.¬†Nair, ‚Äú24 intelligent sensing in ADHD trial ‚Äì pilot study,‚Äù <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Journal of Neurology, Neurosurgery &amp; Psychiatry</em>, vol.¬†94, no.¬†12, p.¬†e2, 2023. [Online]. Available: https://jnnp.bmj.com/content/94/12/e2.35

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
H.¬†W. Loh, C.¬†P. Ooi, P.¬†D. Barua, E.¬†E. Palmer, F.¬†Molinari, and U.¬†R. Acharya, ‚ÄúAutomated detection of ADHD: current trends and future perspective,‚Äù <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Computers in Biology and Medicine</em>, vol. 146, pp. 1‚Äì18, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J.¬†A. Russell, ‚ÄúMeasures of emotion,‚Äù in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">The measurement of emotions</em>.¬†¬†¬†Elsevier, 1989, pp. 83‚Äì111.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S.¬†Sharma, P.¬†Jasper, E.¬†Muth, and A.¬†Hoover, ‚ÄúThe impact of walking and resting on wrist motion for automated detection of meals,‚Äù <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Computing for Healthcare</em>, vol.¬†1, no.¬†4, pp. 1‚Äì19, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
E.¬†A. R√≠ssola, D.¬†E. Losada, and F.¬†Crestani, ‚ÄúA survey of computational methods for online mental state assessment on social media,‚Äù <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Computing for Healthcare</em>, vol.¬†2, no.¬†2, pp. 1‚Äì31, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y.¬†Tang, J.¬†Sun, C.¬†Wang, Y.¬†Zhong, A.¬†Jiang, G.¬†Liu, and X.¬†Liu, ‚ÄúAdhd classification using auto-encoding neural network and binary hypothesis testing,‚Äù <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence in Medicine</em>, vol. 123, pp. 102‚Äâ209‚Äì102‚Äâ230, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Q.¬†Li, F.¬†Dong, Q.¬†Gai, K.¬†Che, H.¬†Ma, F.¬†Zhao, T.¬†Chu, N.¬†Mao, and P.¬†Wang, ‚ÄúDiagnosis of major depressive disorder using machine learning based on multisequence mri neuroimaging features,‚Äù <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Journal of Magnetic Resonance Imaging</em>, vol.¬†58, no.¬†5, pp. 1420‚Äì1430, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M.¬†Niu, J.¬†Tao, Y.¬†Li, Y.¬†Qin, and Y.¬†Li, ‚ÄúWavdepressionnet: Automatic depression level prediction via raw speech signals,‚Äù <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>, vol.¬†15, no.¬†5, pp. 285‚Äì296, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
P.¬†Bellec, C.¬†Chu, F.¬†Chouinard-Decorte, Y.¬†Benhajali, D.¬†S. Margulies, and R.¬†C. Craddock, ‚ÄúThe neuro bureau ADHD-200 preprocessed repository,‚Äù <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Neuroimage</em>, vol. 144, pp. 275‚Äì286, 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y.¬†Tang, J.¬†Sun, C.¬†Wang, Y.¬†Zhong, A.¬†Jiang, G.¬†Liu, and X.¬†Liu, ‚ÄúAdhd classification using auto-encoding neural network and binary hypothesis testing,‚Äù <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence in Medicine</em>, vol. 123, p. 102209, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Y.¬†Li, Y.¬†Sun, and S.¬†Mohsen¬†Naqvi, ‚ÄúSingle-channel dereverberation and denoising based on lower band trained sa-lstms,‚Äù <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IET Signal Processing</em>, vol.¬†14, no.¬†10, pp. 774‚Äì782, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y.¬†Li, Y.¬†Sun, K.¬†Horoshenkov, and S.¬†M. Naqvi, ‚ÄúDomain adaptation and autoencoder-based unsupervised speech enhancement,‚Äù <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Artificial Intelligence</em>, vol.¬†3, no.¬†1, pp. 43‚Äì52, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y.¬†Pan, Y.¬†Shang, T.¬†Liu, Z.¬†Shao, G.¬†Guo, H.¬†Ding, and Q.¬†Hu, ‚ÄúSpatial‚Äìtemporal attention network for depression recognition from facial videos,‚Äù <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, vol. 237, p. 121410, 2024.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
D.¬†Tran, L.¬†Bourdev, R.¬†Fergus, L.¬†Torresani, and M.¬†Paluri, ‚ÄúLearning spatiotemporal features with 3D convolutional networks,‚Äù <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision (ICCV)</em>, 2015.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
D.¬†Tran, H.¬†Wang, L.¬†Torresani, J.¬†Ray, Y.¬†LeCun, D.¬†Paluri, M.Tran, H.¬†Wang, L.¬†Torresani, J.¬†Ray, Y.¬†LeCun, and M.¬†Paluri, ‚ÄúA closer look at spatiotemporal convolutions for action recognition,‚Äù <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Y.¬†Li, Y.¬†Sun, and S.¬†M. Naqvi, ‚ÄúPSD and signal approximation-lstm based speech enhancement,‚Äù in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">International Conference on Signal Processing and Communication Systems (ICSPCS)</em>, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y.¬†Luo, T.¬†L. Alvarez, J.¬†M. Halperin, and X.¬†Li, ‚ÄúMultimodal neuroimaging-based prediction of adult outcomes in childhood-onset ADHD using ensemble learning techniques,‚Äù <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">NeuroImage: Clinical</em>, vol.¬†26, p. 102238, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J.¬†Peng, M.¬†Debnath, and A.¬†K. Biswas, ‚ÄúEfficacy of novel summation-based synergetic artificial neural network in adhd diagnosis,‚Äù <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Machine Learning with Applications</em>, vol.¬†6, p. 100120, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A.¬†Vahid, A.¬†Bluschke, V.¬†Roessner, S.¬†Stober, and C.¬†Beste, ‚ÄúDeep learning based on event-related eeg differentiates children with adhd from healthy controls,‚Äù <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Journal of clinical medicine</em>, vol.¬†8, no.¬†7, p. 1055, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M.¬†Valstar, B.¬†Schuller, K.¬†Smith, T.¬†Almaev, F.¬†Eyben, J.¬†Krajewski, R.¬†Cowie, and M.¬†Pantic, ‚ÄúAvec 2014: 3d dimensional affect and depression recognition challenge,‚Äù in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 4th international workshop on audio/visual emotion challenge</em>, 2014.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M.¬†Niu, J.¬†Tao, B.¬†Liu, J.¬†Huang, and Z.¬†Lian, ‚ÄúMultimodal spatiotemporal representation for automatic depression level detection,‚Äù <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on affective computing</em>, vol.¬†14, no.¬†1, pp. 294‚Äì307, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
M.¬†Niu, J.¬†Tao, B.¬†Liu, and C.¬†Fan, ‚ÄúAutomatic depression level detection via lp-norm pooling,‚Äù <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH, Graz, Austria</em>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Z.¬†Du, W.¬†Li, D.¬†Huang, and Y.¬†Wang, ‚ÄúEncoding visual behaviors with attentive temporal convolution for depression prediction,‚Äù in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">IEEE international conference on automatic face &amp; gesture recognition (FG 2019)</em>.¬†¬†¬†IEEE, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.02242" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.02243" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.02243">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.02243" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.02244" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:51:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
