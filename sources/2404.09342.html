<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.09342] Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan</title><meta property="og:description" content="The advancements of technology have led to the use of multimodal systems in various real-world applications. Among them, the audio-visual systems are one of the widely used multimodal systems. In the recent years, asso…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.09342">

<!--Generated on Sun May  5 21:36:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\sidecaptionvpos</span>
<p id="p1.2" class="ltx_p">figuret


</p>
</div>
<h1 class="ltx_title ltx_title_document">Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Muhammad Saad Saeed<sup id="id17.17.id1" class="ltx_sup"><span id="id17.17.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Shah Nawaz<sup id="id18.18.id2" class="ltx_sup"><span id="id18.18.id2.1" class="ltx_text ltx_font_italic">2</span></sup>, Muhammad Salman Tahir<sup id="id19.19.id3" class="ltx_sup"><span id="id19.19.id3.1" class="ltx_text ltx_font_italic">1</span></sup>, Rohan Kumar Das<sup id="id20.20.id4" class="ltx_sup"><span id="id20.20.id4.1" class="ltx_text ltx_font_italic">3</span></sup>, Muhammad Zaigham Zaheer<sup id="id21.21.id5" class="ltx_sup"><span id="id21.21.id5.1" class="ltx_text ltx_font_italic">4</span></sup>, 
<br class="ltx_break">Marta Moscati<sup id="id22.22.id6" class="ltx_sup"><span id="id22.22.id6.1" class="ltx_text ltx_font_italic">5</span></sup>, Markus Schedl<sup id="id23.23.id7" class="ltx_sup"><span id="id23.23.id7.1" class="ltx_text ltx_font_italic">5,6</span></sup>, Muhammad Haris Khan<sup id="id24.24.id8" class="ltx_sup"><span id="id24.24.id8.1" class="ltx_text ltx_font_italic">4</span></sup>, Karthik Nandakumar<sup id="id25.25.id9" class="ltx_sup"><span id="id25.25.id9.1" class="ltx_text ltx_font_italic">4</span></sup>, Muhammad Haroon Yousaf<sup id="id26.26.id10" class="ltx_sup"><span id="id26.26.id10.1" class="ltx_text ltx_font_italic">1</span></sup> 
<br class="ltx_break"><sup id="id27.27.id11" class="ltx_sup"><span id="id27.27.id11.1" class="ltx_text ltx_font_italic">1</span></sup>Swarm Robotics Lab NCRA, University of Engineering and Technology Taxila,
<sup id="id28.28.id12" class="ltx_sup"><span id="id28.28.id12.1" class="ltx_text ltx_font_italic">2</span></sup>IMEC, Belgium,
<br class="ltx_break"><sup id="id29.29.id13" class="ltx_sup"><span id="id29.29.id13.1" class="ltx_text ltx_font_italic">3</span></sup>Fortemedia Singapore, Singapore,
<sup id="id30.30.id14" class="ltx_sup"><span id="id30.30.id14.1" class="ltx_text ltx_font_italic">4</span></sup>Mohamed bin Zayed University of Artificial Intelligence 
<br class="ltx_break"><sup id="id31.31.id15" class="ltx_sup"><span id="id31.31.id15.1" class="ltx_text ltx_font_italic">5</span></sup>Institute of Computational Perception, Johannes Kepler University Linz, Austria 
<br class="ltx_break"><sup id="id32.32.id16" class="ltx_sup"><span id="id32.32.id16.1" class="ltx_text ltx_font_italic">6</span></sup>Human-centered AI Group, AI Lab, Linz Institute of Technology, Austria 
<br class="ltx_break"><span id="id33.33.id17" class="ltx_text ltx_font_typewriter">{mavceleb@gmail.com}
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id34.id1" class="ltx_p">The advancements of technology have led to the use of multimodal systems in various real-world applications. Among them, the audio-visual systems are one of the widely used multimodal systems. In the recent years, associating face and voice of a person has gained attention due to presence of unique correlation between them. The Face-voice Association in Multilingual Environments (FAME) Challenge 2024 focuses on exploring face-voice association under a unique condition of multilingual scenario. This condition is inspired from the fact that half of the world’s population is bilingual and most often people communicate under multilingual scenario. The challenge uses a dataset namely, Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice association in multilingual environments. This report provides the details of the challenge, dataset, baselines and task details for the FAME Challenge.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The face and voice of a person have unique characteristics and they are well used as biometric measures for person authentication either as a unimodal or multimodal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. A strong correlation has been found between face and voice of a person, which has attracted significant research interest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
Though previous works have established association between faces and voices, none of these approaches investigated the effect of multiple languages on this task. As half of the population of world is bilingual and we are more often communicating in multilingual scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, therefore, it is essential to investigate the effect of language for associating faces with the voices.
Thus, the goal of the FAME challenge 2024 is to analyze the impact of multiple languages on face-voice association using cross-modal verification task. Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides an overview of the challenge.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In response, our prior research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> introduced a Multilingual Audio-Visual (MAV-Celeb) dataset to analyze the impact of language on face-voice association; it comprises of video and audio recordings of different celebrities speaking more than one language. For example, a celebrity named ‘Imran Khan’ has audio information in Urdu and English languages.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.09342/assets/ver_diag.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="182" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">(Left) Standard Face-voice association is established with a cross-modal verification task. (Right) The FAME challenge 2024 extends the verification task to analyze the impact of multiple of languages.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Challenge Objectives</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The FAME Challenge 2024 is planned with the primary objective to provide a common platform to academic and industrial researchers to develop and explore the impact of languages in face-voice association, which can be useful for various downstream tasks. The research conducted under this challenge is expected to spearhead the face-voice association task in one unique direction from the perspective of real-world scenarios. The challenge focuses on exploring the following, but not limited to:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">To study the influence of language information in face-voice association</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">To explore language specific knowledge in face-voice association</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">To explore language independent face-voice association models</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">Any other related information with respect to language mismatch in face-voice association</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Dataset</span>
</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2404.09342/assets/mavDataset.jpg" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="428" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Audio-visual samples selected from MAV-Celeb dataset. The visual data contains various variations such as pose, lighting condition and motion. (Left) It contains information of celebrities speaking English and the (Right) block presents data of the same celebrity in Hindi.</span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.5" class="ltx_p">MAV-Celeb dataset provide data of <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="154" display="inline"><semantics id="S3.p1.1.m1.1a"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">154</mn><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><cn type="integer" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">154</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">154</annotation></semantics></math> celebrities in <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.p1.2.m2.1a"><mn id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><cn type="integer" id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">3</annotation></semantics></math> languages (English, Hindi, Urdu).
These three languages have been selected because of several factors: i) They represent approximately 1.4 Billion bilingual/trilingual people; ii) The population is highly proficient in two or more languages; iii) There is a relevant corpus of different media that can be extracted from available online repositories (e.g. YouTube).
The collected videos cover a wide range of unconstrained, challenging multi-speaker environment including political debates, press conferences, outdoor interviews, quiet studio interviews, drama and movie clips.
Note that the visual data spans over a vast range of variations including poses, motion blur, background clutter, video quality, occlusions and lighting conditions. Moreover, videos are degraded with real-world noise like background chatter, music, overlapping speech and compression artifacts.
Fig. <a href="#S3.F2" title="Figure 2 ‣ III Dataset ‣ Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows some audio-visual samples while Table <a href="#S3.T1" title="TABLE I ‣ III Dataset ‣ Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> shows statistics of the dataset.
The dataset contains <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.p1.3.m3.1a"><mn id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><cn type="integer" id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">2</annotation></semantics></math> splits English–Urdu (V<math id="S3.p1.4.m4.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.p1.4.m4.1a"><mn id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><cn type="integer" id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">1</annotation></semantics></math>-EU) and English–Hindi (V<math id="S3.p1.5.m5.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.p1.5.m5.1a"><mn id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><cn type="integer" id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">2</annotation></semantics></math>-EH) to analyze performance measure across multiple languages.
Fig. <a href="#S3.F3" title="Figure 3 ‣ III Dataset ‣ Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the file structure of both splits.
The pipeline followed in creating the dataset is available in our prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.4.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S3.T1.5.2" class="ltx_text" style="font-size:90%;">Summary of MAV-Celeb dataset</span></figcaption>
<table id="S3.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.2.2" class="ltx_tr">
<th id="S3.T1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S3.T1.2.2.3.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.1.1.1" class="ltx_text ltx_font_bold">E/U/V<math id="S3.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.T1.1.1.1.1.m1.1a"><mn id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><cn type="integer" id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">1</annotation></semantics></math>-EU</span></th>
<th id="S3.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.2.2.2.1" class="ltx_text ltx_font_bold">E/H/V<math id="S3.T1.2.2.2.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.T1.2.2.2.1.m1.1a"><mn id="S3.T1.2.2.2.1.m1.1.1" xref="S3.T1.2.2.2.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.1.m1.1b"><cn type="integer" id="S3.T1.2.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.2.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.1.m1.1c">2</annotation></semantics></math>-EH</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.2.3.1" class="ltx_tr">
<th id="S3.T1.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"># of Celebrities</th>
<td id="S3.T1.2.3.1.2" class="ltx_td ltx_align_center ltx_border_tt">70</td>
<td id="S3.T1.2.3.1.3" class="ltx_td ltx_align_center ltx_border_tt">84</td>
</tr>
<tr id="S3.T1.2.4.2" class="ltx_tr">
<th id="S3.T1.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"># of male celebrities</th>
<td id="S3.T1.2.4.2.2" class="ltx_td ltx_align_center ltx_border_t">43</td>
<td id="S3.T1.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">56</td>
</tr>
<tr id="S3.T1.2.5.3" class="ltx_tr">
<th id="S3.T1.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"># of female celebrities</th>
<td id="S3.T1.2.5.3.2" class="ltx_td ltx_align_center ltx_border_t">27</td>
<td id="S3.T1.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t">28</td>
</tr>
<tr id="S3.T1.2.6.4" class="ltx_tr">
<th id="S3.T1.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"># of videos</th>
<td id="S3.T1.2.6.4.2" class="ltx_td ltx_align_center ltx_border_t">402/555/957</td>
<td id="S3.T1.2.6.4.3" class="ltx_td ltx_align_center ltx_border_t">646/484/1130</td>
</tr>
<tr id="S3.T1.2.7.5" class="ltx_tr">
<th id="S3.T1.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"># of hours</th>
<td id="S3.T1.2.7.5.2" class="ltx_td ltx_align_center ltx_border_t">30/54/84</td>
<td id="S3.T1.2.7.5.3" class="ltx_td ltx_align_center ltx_border_t">51/33/84</td>
</tr>
<tr id="S3.T1.2.8.6" class="ltx_tr">
<th id="S3.T1.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"># of utterances</th>
<td id="S3.T1.2.8.6.2" class="ltx_td ltx_align_center ltx_border_t">6850/12706/19556</td>
<td id="S3.T1.2.8.6.3" class="ltx_td ltx_align_center ltx_border_t">12579/8136/20715</td>
</tr>
<tr id="S3.T1.2.9.7" class="ltx_tr">
<th id="S3.T1.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Avg # of videos/celebrity</th>
<td id="S3.T1.2.9.7.2" class="ltx_td ltx_align_center ltx_border_t">6/8/14</td>
<td id="S3.T1.2.9.7.3" class="ltx_td ltx_align_center ltx_border_t">8/6/14</td>
</tr>
<tr id="S3.T1.2.10.8" class="ltx_tr">
<th id="S3.T1.2.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Avg # of utterances/celebrity</th>
<td id="S3.T1.2.10.8.2" class="ltx_td ltx_align_center ltx_border_t">98/182/280</td>
<td id="S3.T1.2.10.8.3" class="ltx_td ltx_align_center ltx_border_t">150/97/247</td>
</tr>
<tr id="S3.T1.2.11.9" class="ltx_tr">
<th id="S3.T1.2.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t">Avg length of utterance</th>
<td id="S3.T1.2.11.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">15.8/15.3/15.6</td>
<td id="S3.T1.2.11.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">14.6/14.6/14.6</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2404.09342/assets/dataset_structure.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="299" height="653" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">MAV-Celeb file structure.</span></figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Challenge Setup and Baseline</span>
</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.10" class="ltx_p"><span id="S4.p1.10.1" class="ltx_text ltx_font_bold">Challenge setup.</span>
The MAV-Celeb dataset is divided into train and test splits consisting of disjoint identities from the same language typically known as unseen-unheard configuration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows evaluation protocol at train and test time. At test time, the network is evaluated on a <span id="S4.p1.10.2" class="ltx_text ltx_font_italic">heard</span> and completely <span id="S4.p1.10.3" class="ltx_text ltx_font_italic">unheard</span> language on cross-modal verification task. The dataset splits V<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.p1.1.m1.1a"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><cn type="integer" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">1</annotation></semantics></math>-EU, V<math id="S4.p1.2.m2.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.p1.2.m2.1a"><mn id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><cn type="integer" id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">2</annotation></semantics></math>-EH contains <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S4.p1.3.m3.1a"><mn id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><cn type="integer" id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">64</annotation></semantics></math>–<math id="S4.p1.4.m4.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.p1.4.m4.1a"><mn id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><cn type="integer" id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">6</annotation></semantics></math>, <math id="S4.p1.5.m5.1" class="ltx_Math" alttext="78" display="inline"><semantics id="S4.p1.5.m5.1a"><mn id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml">78</mn><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.1b"><cn type="integer" id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1">78</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.1c">78</annotation></semantics></math>–<math id="S4.p1.6.m6.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.p1.6.m6.1a"><mn id="S4.p1.6.m6.1.1" xref="S4.p1.6.m6.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.p1.6.m6.1b"><cn type="integer" id="S4.p1.6.m6.1.1.cmml" xref="S4.p1.6.m6.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.6.m6.1c">6</annotation></semantics></math> identities for train and test respectively. V<math id="S4.p1.7.m7.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.p1.7.m7.1a"><mn id="S4.p1.7.m7.1.1" xref="S4.p1.7.m7.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.p1.7.m7.1b"><cn type="integer" id="S4.p1.7.m7.1.1.cmml" xref="S4.p1.7.m7.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.7.m7.1c">2</annotation></semantics></math>-EH split will be use in the progress phase. While, the final evaluation will be carried out in on test set of V<math id="S4.p1.8.m8.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.p1.8.m8.1a"><mn id="S4.p1.8.m8.1.1" xref="S4.p1.8.m8.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.p1.8.m8.1b"><cn type="integer" id="S4.p1.8.m8.1.1.cmml" xref="S4.p1.8.m8.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.8.m8.1c">1</annotation></semantics></math>-EU.
Each line from test file in V<math id="S4.p1.9.m9.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.p1.9.m9.1a"><mn id="S4.p1.9.m9.1.1" xref="S4.p1.9.m9.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.p1.9.m9.1b"><cn type="integer" id="S4.p1.9.m9.1.1.cmml" xref="S4.p1.9.m9.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.9.m9.1c">1</annotation></semantics></math> and V<math id="S4.p1.10.m10.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.p1.10.m10.1a"><mn id="S4.p1.10.m10.1.1" xref="S4.p1.10.m10.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.p1.10.m10.1b"><cn type="integer" id="S4.p1.10.m10.1.1.cmml" xref="S4.p1.10.m10.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.10.m10.1c">2</annotation></semantics></math> has the following format.</p>
</div>
<div id="S4.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter">ysuvkz41 voices/English/00000.wav faces/English/00000.jpg</span></p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter">tog3zj45 voices/English/00001.wav faces/English/00001.jpg</span></p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter">ky5xfj1d voices/English/00002.wav faces/English/00002.jpg</span></p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_typewriter">yx4nfa35 voices/English/01062.wav faces/English/01062.jpg</span></p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p"><span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_typewriter">bowsaf5e voices/English/01063.wav faces/English/01063.jpg</span></p>
</div>
</li>
</ul>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><a target="_blank" href="https://mavceleb.github.io/dataset" title="" class="ltx_ref ltx_href">MAV-Celeb</a> dataset is publicly available. We provided features from state-of-the-art methods representing faces and voices. Moreover, we created <a target="_blank" href="https://mavceleb.github.io/dataset/competition.html" title="" class="ltx_ref ltx_href">FAME Challenge</a> website for additional information on the task.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2404.09342/assets/twoBranch_fig1.jpg" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="5057" height="1701" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Overall architecture of our baseline method. Fundamentally, it is a two-stream pipeline which generates face and voice embeddings. We propose a light-weight, plug-and-play mechanism, dubbed as fusion and orthogonal projection (FOP) (shown in dotted red box).
</span></figcaption>
</figure>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Evaluation Metric.</span> We are considering equal error rate (EER) as the metric for evaluating the challenge performance. We expect the challenge participants to submit a output score file for every test pairs to indicate how confident the system believes to have a match between the face and voice or in other words, the face and voice belongs to the same person. The higher the score is, the larger is the confidence of being the face and voice from the same person. In real-world applications, people may set a threshold to determine the if the pair belongs to same or different person as binary output. With the threshold higher, the false acceptance rate (FAR) will become lower, and the false rejection rate (FRR) will become higher. The EER is that optial point when both the errors FAR and FRR are equal. Therefore, EER becomes suitable to evaluate the performance of systems than the conventional accuracy since it independent of the threshold. Finally, the lower the EER it can characterize a better system.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.4.1.1" class="ltx_text" style="font-size:113%;">TABLE II</span>: </span><span id="S4.T2.5.2" class="ltx_text" style="font-size:113%;">Cross-modal verification between faces and voices across multiple language on various test configurations of MAV-Celeb dataset. (EER: lower is better)</span></figcaption>
<table id="S4.T2.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.6.1.1" class="ltx_tr">
<th id="S4.T2.6.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T2.6.1.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T2.6.1.1.3" class="ltx_td ltx_border_t"></td>
<td id="S4.T2.6.1.1.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T2.6.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">V2-EH</span></td>
</tr>
<tr id="S4.T2.6.2.2" class="ltx_tr">
<th id="S4.T2.6.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T2.6.2.2.1.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<th id="S4.T2.6.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T2.6.2.2.2.1" class="ltx_text" style="font-size:80%;">Configuration</span></th>
<td id="S4.T2.6.2.2.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.6.2.2.3.1" class="ltx_text" style="font-size:80%;">Eng. test</span></td>
<td id="S4.T2.6.2.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.6.2.2.4.1" class="ltx_text" style="font-size:80%;">Hindi test</span></td>
<td id="S4.T2.6.2.2.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.6.2.2.5.1" class="ltx_text" style="font-size:80%;">Overall score</span></td>
</tr>
<tr id="S4.T2.6.3.3" class="ltx_tr">
<th id="S4.T2.6.3.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T2.6.3.3.2" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T2.6.3.3.3" class="ltx_td ltx_align_center"><span id="S4.T2.6.3.3.3.1" class="ltx_text" style="font-size:80%;">(EER)</span></td>
<td id="S4.T2.6.3.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.6.3.3.4.1" class="ltx_text" style="font-size:80%;">(EER)</span></td>
<td id="S4.T2.6.3.3.5" class="ltx_td ltx_align_center"><span id="S4.T2.6.3.3.5.1" class="ltx_text" style="font-size:80%;">(EER)</span></td>
</tr>
<tr id="S4.T2.6.4.4" class="ltx_tr">
<th id="S4.T2.6.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T2.6.4.4.1.1" class="ltx_text" style="font-size:80%;">FOP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></span></th>
<th id="S4.T2.6.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.6.4.4.2.1" class="ltx_text" style="font-size:80%;">Eng. train</span></th>
<td id="S4.T2.6.4.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.4.4.3.1" class="ltx_text" style="font-size:80%;">20.8</span></td>
<td id="S4.T2.6.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.4.4.4.1" class="ltx_text" style="font-size:80%;">24.0</span></td>
<td id="S4.T2.6.4.4.5" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T2.6.4.4.5.1" class="ltx_text" style="font-size:80%;">22.0</span></td>
</tr>
<tr id="S4.T2.6.5.5" class="ltx_tr">
<th id="S4.T2.6.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.6.5.5.1.1" class="ltx_text" style="font-size:80%;">Hindi train</span></th>
<td id="S4.T2.6.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T2.6.5.5.2.1" class="ltx_text" style="font-size:80%;">24.0</span></td>
<td id="S4.T2.6.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T2.6.5.5.3.1" class="ltx_text" style="font-size:80%;">19.3</span></td>
</tr>
<tr id="S4.T2.6.6.6" class="ltx_tr">
<th id="S4.T2.6.6.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T2.6.6.6.2" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S4.T2.6.6.6.3" class="ltx_td ltx_border_tt"></td>
<td id="S4.T2.6.6.6.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T2.6.6.6.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">V1-EU</span></td>
</tr>
<tr id="S4.T2.6.7.7" class="ltx_tr">
<th id="S4.T2.6.7.7.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T2.6.7.7.2" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S4.T2.6.7.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.7.7.3.1" class="ltx_text" style="font-size:80%;">Eng. test</span></td>
<td id="S4.T2.6.7.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.7.7.4.1" class="ltx_text" style="font-size:80%;">Urdu test</span></td>
<td id="S4.T2.6.7.7.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.7.7.5.1" class="ltx_text" style="font-size:80%;">Overall score</span></td>
</tr>
<tr id="S4.T2.6.8.8" class="ltx_tr">
<th id="S4.T2.6.8.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T2.6.8.8.2" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T2.6.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T2.6.8.8.3.1" class="ltx_text" style="font-size:80%;">(EER)</span></td>
<td id="S4.T2.6.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T2.6.8.8.4.1" class="ltx_text" style="font-size:80%;">(EER)</span></td>
<td id="S4.T2.6.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T2.6.8.8.5.1" class="ltx_text" style="font-size:80%;">(EER)</span></td>
</tr>
<tr id="S4.T2.6.9.9" class="ltx_tr">
<th id="S4.T2.6.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="2"><span id="S4.T2.6.9.9.1.1" class="ltx_text" style="font-size:80%;">FOP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></span></th>
<th id="S4.T2.6.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.6.9.9.2.1" class="ltx_text" style="font-size:80%;">Eng. train</span></th>
<td id="S4.T2.6.9.9.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.9.9.3.1" class="ltx_text" style="font-size:80%;">29.3</span></td>
<td id="S4.T2.6.9.9.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.9.9.4.1" class="ltx_text" style="font-size:80%;">37.9</span></td>
<td id="S4.T2.6.9.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" rowspan="2"><span id="S4.T2.6.9.9.5.1" class="ltx_text" style="font-size:80%;">33.4</span></td>
</tr>
<tr id="S4.T2.6.10.10" class="ltx_tr">
<th id="S4.T2.6.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T2.6.10.10.1.1" class="ltx_text" style="font-size:80%;">Urdu train</span></th>
<td id="S4.T2.6.10.10.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.6.10.10.2.1" class="ltx_text" style="font-size:80%;">40.4</span></td>
<td id="S4.T2.6.10.10.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.6.10.10.3.1" class="ltx_text" style="font-size:80%;">25.8</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Baseline method.</span> The baseline method employ a two-stream pipeline to obtain the respective embeddings of both face and voice inputs. The first stream corresponds to a pre-trained convolutional neural network (CNN) on face modality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
We take the penultimate layer’s output of this CNN as the feature embeddings for an input face image. Likewise, the second stream is a pre-trained audio encoding network that outputs a feature embedding for an input audio signal (typically a short-term spectrogram) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
The baseline method exploits complementary cues from both modality embeddings to form enriched fused embeddings and imposes orthogonal constraints on them for learning discriminative joint face-voice embeddings, as shown in Fig <a href="#S4.F4" title="Figure 4 ‣ IV Challenge Setup and Baseline ‣ Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. More information is available in our prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and <a target="_blank" href="https://github.com/mavceleb/mavceleb_baseline" title="" class="ltx_ref ltx_href">GitHub repository</a>.</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Baseline results.</span> Table <a href="#S4.T2" title="TABLE II ‣ IV Challenge Setup and Baseline ‣ Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> provides the baseline face-voice association results with the impact of multiple languages on both splits of MAV-Celeb. The FAME challenge <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="2024" display="inline"><semantics id="S4.p6.1.m1.1a"><mn id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml">2024</mn><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><cn type="integer" id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1">2024</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">2024</annotation></semantics></math> encourages participants to explore novel ideas to improve performance on heard and unheard languages.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Rules for System Development</span>
</h2>

<div id="S5.p1" class="ltx_para">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">A pretrained model on heard language is allowed.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">A pretrained model on unheard language is not allowed. The evaluation follows unheard-unseen and completely <span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">unheard</span> protocol. Each celebrity in the split has audio information in two languages; the model will be trained on one language (say Hindi) and then tested on heard language (Hindi) and completely <span id="S5.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">unheard</span> language (English).</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">The participants are required to submit a <math id="S5.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S5.I1.i3.p1.1.m1.1a"><mn id="S5.I1.i3.p1.1.m1.1.1" xref="S5.I1.i3.p1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.1.m1.1b"><cn type="integer" id="S5.I1.i3.p1.1.m1.1.1.cmml" xref="S5.I1.i3.p1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.1.m1.1c">2</annotation></semantics></math> page system description in the ACM template to the challenge organizers. Teams without system description will be disqualified from the challenge.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Registration Process</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The following Google Form to be used by participating teams for registration of their respective teams in the challenge.
<a target="_blank" href="https://forms.gle/TfCfwgxkSL56iRmW9" title="" class="ltx_ref ltx_href">Registration form</a></p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Submission of Results</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Within the directory containing the submission files, use zip archive.zip *.txt and do not zip the folder. Files should be named as:</p>
</div>
<div id="S7.p2" class="ltx_para">
<ul id="S7.I1" class="ltx_itemize">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.1" class="ltx_p"><span id="S7.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter">sub_score_English_heard.txt</span></p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.1" class="ltx_p"><span id="S7.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter">sub_score_English_unheard.txt</span></p>
</div>
</li>
<li id="S7.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i3.p1" class="ltx_para">
<p id="S7.I1.i3.p1.1" class="ltx_p"><span id="S7.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter">sub_score_Urdu_heard.txt</span></p>
</div>
</li>
<li id="S7.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i4.p1" class="ltx_para">
<p id="S7.I1.i4.p1.1" class="ltx_p"><span id="S7.I1.i4.p1.1.1" class="ltx_text ltx_font_typewriter">sub_score_Urdu_unheard.txt</span></p>
</div>
</li>
</ul>
</div>
<figure id="S7.T3" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S7.T3.4.1.1" class="ltx_text" style="font-size:113%;">TABLE III</span>: </span><span id="S7.T3.5.2" class="ltx_text" style="font-size:113%;">Configuration to analyze the impact of multiple languages on face-voice association with nomenclature.</span></figcaption>
<table id="S7.T3.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T3.6.1.1" class="ltx_tr">
<th id="S7.T3.6.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S7.T3.6.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2"><span id="S7.T3.6.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">V2-EH</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T3.6.2.1" class="ltx_tr">
<th id="S7.T3.6.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S7.T3.6.2.1.1.1" class="ltx_text" style="font-size:80%;">Configuration</span></th>
<td id="S7.T3.6.2.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S7.T3.6.2.1.2.1" class="ltx_text" style="font-size:80%;">Eng. test</span></td>
<td id="S7.T3.6.2.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S7.T3.6.2.1.3.1" class="ltx_text" style="font-size:80%;">Hindi test</span></td>
</tr>
<tr id="S7.T3.6.3.2" class="ltx_tr">
<th id="S7.T3.6.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S7.T3.6.3.2.1.1" class="ltx_text" style="font-size:80%;">Eng. train</span></th>
<td id="S7.T3.6.3.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T3.6.3.2.2.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">sub_score_English_heard.txt</span></td>
<td id="S7.T3.6.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T3.6.3.2.3.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">sub_score_Hindi_unheard.txt</span></td>
</tr>
<tr id="S7.T3.6.4.3" class="ltx_tr">
<th id="S7.T3.6.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S7.T3.6.4.3.1.1" class="ltx_text" style="font-size:80%;">Hindi train</span></th>
<td id="S7.T3.6.4.3.2" class="ltx_td ltx_align_center"><span id="S7.T3.6.4.3.2.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">sub_score_English_unheard.txt</span></td>
<td id="S7.T3.6.4.3.3" class="ltx_td ltx_align_center"><span id="S7.T3.6.4.3.3.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">sub_score_Hindi_heard.txt</span></td>
</tr>
<tr id="S7.T3.6.5.4" class="ltx_tr">
<th id="S7.T3.6.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S7.T3.6.5.4.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S7.T3.6.5.4.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">V1-EU</span></td>
</tr>
<tr id="S7.T3.6.6.5" class="ltx_tr">
<th id="S7.T3.6.6.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="S7.T3.6.6.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T3.6.6.5.2.1" class="ltx_text" style="font-size:80%;">Eng. test</span></td>
<td id="S7.T3.6.6.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T3.6.6.5.3.1" class="ltx_text" style="font-size:80%;">Urdu test</span></td>
</tr>
<tr id="S7.T3.6.7.6" class="ltx_tr">
<th id="S7.T3.6.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S7.T3.6.7.6.1.1" class="ltx_text" style="font-size:80%;">Eng. train</span></th>
<td id="S7.T3.6.7.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T3.6.7.6.2.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">sub_score_English_heard.txt</span></td>
<td id="S7.T3.6.7.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S7.T3.6.7.6.3.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">sub_score_Urdu_unheard.txt</span></td>
</tr>
<tr id="S7.T3.6.8.7" class="ltx_tr">
<th id="S7.T3.6.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S7.T3.6.8.7.1.1" class="ltx_text" style="font-size:80%;">Urdu train</span></th>
<td id="S7.T3.6.8.7.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S7.T3.6.8.7.2.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">sub_score_English_unheard.txt</span></td>
<td id="S7.T3.6.8.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S7.T3.6.8.7.3.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">sub_score_Urdu_heard.txt</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Moreover, Table <a href="#S7.T3" title="TABLE III ‣ VII Submission of Results ‣ Face-voice Association in Multilingual Environments (FAME) Challenge 2024 Evaluation Plan" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> provides nomenclature of submission files. For each file, we have kept the ground truth for fair evaluation during FAME challenge. Participants are expected to compute and submit text files including the <span id="S7.p3.1.1" class="ltx_text ltx_font_typewriter">id</span> and <span id="S7.p3.1.2" class="ltx_text ltx_font_typewriter">L2</span> scores in the following format:</p>
</div>
<div id="S7.p4" class="ltx_para">
<ul id="S7.I2" class="ltx_itemize">
<li id="S7.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I2.i1.p1" class="ltx_para">
<p id="S7.I2.i1.p1.1" class="ltx_p"><span id="S7.I2.i1.p1.1.1" class="ltx_text ltx_font_typewriter">ysuvkz41 0.9988</span></p>
</div>
</li>
<li id="S7.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I2.i2.p1" class="ltx_para">
<p id="S7.I2.i2.p1.1" class="ltx_p"><span id="S7.I2.i2.p1.1.1" class="ltx_text ltx_font_typewriter">tog3zj45 0.1146</span></p>
</div>
</li>
<li id="S7.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I2.i3.p1" class="ltx_para">
<p id="S7.I2.i3.p1.1" class="ltx_p"><span id="S7.I2.i3.p1.1.1" class="ltx_text ltx_font_typewriter">ky5xfj1d 0.6514</span></p>
</div>
</li>
<li id="S7.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I2.i4.p1" class="ltx_para">
<p id="S7.I2.i4.p1.1" class="ltx_p"><span id="S7.I2.i4.p1.1.1" class="ltx_text ltx_font_typewriter">yx4nfa35 1.5321</span></p>
</div>
</li>
<li id="S7.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I2.i5.p1" class="ltx_para">
<p id="S7.I2.i5.p1.1" class="ltx_p"><span id="S7.I2.i5.p1.1.1" class="ltx_text ltx_font_typewriter">bowsaf5e 1.6578</span></p>
</div>
</li>
</ul>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.3" class="ltx_p">Each file is submitted through <a target="_blank" href="https://codalab.lisn.upsaclay.fr/competitions/18534" title="" class="ltx_ref ltx_href">Codalab</a>. In the progress phase, each team will have <math id="S7.p5.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S7.p5.1.m1.1a"><mn id="S7.p5.1.m1.1.1" xref="S7.p5.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S7.p5.1.m1.1b"><cn type="integer" id="S7.p5.1.m1.1.1.cmml" xref="S7.p5.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p5.1.m1.1c">100</annotation></semantics></math> submissions with maximum <math id="S7.p5.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S7.p5.2.m2.1a"><mn id="S7.p5.2.m2.1.1" xref="S7.p5.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S7.p5.2.m2.1b"><cn type="integer" id="S7.p5.2.m2.1.1.cmml" xref="S7.p5.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p5.2.m2.1c">10</annotation></semantics></math> per day. While, we will overall <math id="S7.p5.3.m3.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S7.p5.3.m3.1a"><mn id="S7.p5.3.m3.1.1" xref="S7.p5.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S7.p5.3.m3.1b"><cn type="integer" id="S7.p5.3.m3.1.1.cmml" xref="S7.p5.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p5.3.m3.1c">5</annotation></semantics></math> submission in the evaluation phase. The overall score will be computed as:</p>
</div>
<div id="S7.p6" class="ltx_para">
<table id="S7.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S7.E1.m1.1" class="ltx_Math" alttext="\text{ Overall Score }=(\text{Sum of all EERs})/4" display="block"><semantics id="S7.E1.m1.1a"><mrow id="S7.E1.m1.1.2" xref="S7.E1.m1.1.2.cmml"><mtext id="S7.E1.m1.1.2.2" xref="S7.E1.m1.1.2.2a.cmml"> Overall Score </mtext><mo id="S7.E1.m1.1.2.1" xref="S7.E1.m1.1.2.1.cmml">=</mo><mrow id="S7.E1.m1.1.2.3" xref="S7.E1.m1.1.2.3.cmml"><mrow id="S7.E1.m1.1.2.3.2.2" xref="S7.E1.m1.1.1a.cmml"><mo stretchy="false" id="S7.E1.m1.1.2.3.2.2.1" xref="S7.E1.m1.1.1a.cmml">(</mo><mtext id="S7.E1.m1.1.1" xref="S7.E1.m1.1.1.cmml">Sum of all EERs</mtext><mo stretchy="false" id="S7.E1.m1.1.2.3.2.2.2" xref="S7.E1.m1.1.1a.cmml">)</mo></mrow><mo id="S7.E1.m1.1.2.3.1" xref="S7.E1.m1.1.2.3.1.cmml">/</mo><mn id="S7.E1.m1.1.2.3.3" xref="S7.E1.m1.1.2.3.3.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.E1.m1.1b"><apply id="S7.E1.m1.1.2.cmml" xref="S7.E1.m1.1.2"><eq id="S7.E1.m1.1.2.1.cmml" xref="S7.E1.m1.1.2.1"></eq><ci id="S7.E1.m1.1.2.2a.cmml" xref="S7.E1.m1.1.2.2"><mtext id="S7.E1.m1.1.2.2.cmml" xref="S7.E1.m1.1.2.2"> Overall Score </mtext></ci><apply id="S7.E1.m1.1.2.3.cmml" xref="S7.E1.m1.1.2.3"><divide id="S7.E1.m1.1.2.3.1.cmml" xref="S7.E1.m1.1.2.3.1"></divide><ci id="S7.E1.m1.1.1a.cmml" xref="S7.E1.m1.1.2.3.2.2"><mtext id="S7.E1.m1.1.1.cmml" xref="S7.E1.m1.1.1">Sum of all EERs</mtext></ci><cn type="integer" id="S7.E1.m1.1.2.3.3.cmml" xref="S7.E1.m1.1.2.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.E1.m1.1c">\text{ Overall Score }=(\text{Sum of all EERs})/4</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Paper Submission</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">The FAME challenge 2024 is one of the Grand Challenges in ACM Multimedia 2024. The participants of the challenge are invited to grand challenge papers to ACM Multimedia following the official website.</p>
</div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IX </span><span id="S9.1.1" class="ltx_text ltx_font_smallcaps">Important Dates</span>
</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">Tentative timeline of the challenge following ACM Grand Challenge submission:</p>
</div>
<div id="S9.p2" class="ltx_para">
<ul id="S9.I1" class="ltx_itemize">
<li id="S9.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S9.I1.i1.p1" class="ltx_para">
<p id="S9.I1.i1.p1.1" class="ltx_p">Registration Period: 15 April-1 June 2024</p>
</div>
</li>
<li id="S9.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S9.I1.i2.p1" class="ltx_para">
<p id="S9.I1.i2.p1.1" class="ltx_p">Progress Phase: 15 April- 14 June 2024</p>
</div>
</li>
<li id="S9.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S9.I1.i3.p1" class="ltx_para">
<p id="S9.I1.i3.p1.1" class="ltx_p">Evaluation Phase: 15 June- 21 June 2024</p>
</div>
</li>
<li id="S9.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S9.I1.i4.p1" class="ltx_para">
<p id="S9.I1.i4.p1.1" class="ltx_p">Challenge Results: 27 June 2024</p>
</div>
</li>
<li id="S9.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S9.I1.i5.p1" class="ltx_para">
<p id="S9.I1.i5.p1.1" class="ltx_p">Submission of System Descriptions: 30 June 2024</p>
</div>
</li>
<li id="S9.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S9.I1.i6.p1" class="ltx_para">
<p id="S9.I1.i6.p1.1" class="ltx_p">ACM Grand Challenge Paper Submission: TBA</p>
</div>
</li>
</ul>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A.K. Jain, A. Ross, and S. Prabhakar,

</span>
<span class="ltx_bibblock">“An introduction to biometric recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Circuits and Systems for Video Technology</span>,
vol. 14, no. 1, pp. 4–20, 2004.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Arun Ross and Anil K. Jain,

</span>
<span class="ltx_bibblock">“Multimodal biometrics: An overview,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">12th European Signal Processing Conference 2004</span>, 2004, pp.
1221–1224.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Saqlain Hussain Shah, Muhammad Saad Saeed, Shah Nawaz, and Muhammad Haroon
Yousaf,

</span>
<span class="ltx_bibblock">“Speaker recognition in realistic scenario using multimodal data,”

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.13033</span>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Miyuki Kamachi, Harold Hill, Karen Lander, and Eric Vatikiotis-Bateson,

</span>
<span class="ltx_bibblock">“‘putting the face to the voice’: Matching identity across
modality,”

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Current Biology</span>, vol. 13, no. 19, pp. 1709–1714, 2003.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Arsha Nagrani, Samuel Albanie, and Andrew Zisserman,

</span>
<span class="ltx_bibblock">“Seeing voices and hearing faces: Cross-modal biometric matching,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, 2018, pp. 8427–8436.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Shota Horiguchi, Naoyuki Kanda, and Kenji Nagamatsu,

</span>
<span class="ltx_bibblock">“Face-voice matching using cross-modal embeddings,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">ACM Multimedia Conference on Multimedia Conference 2018</span>,
Susanne Boll, Kyoung Mu Lee, Jiebo Luo, Wenwu Zhu, Hyeran Byun, Chang Wen
Chen, Rainer Lienhart, and Tao Mei, Eds. 2018, pp. 1011–1019, ACM.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Ruijie Tao, Rohan Kumar Das, and Haizhou Li,

</span>
<span class="ltx_bibblock">“Audio-Visual Speaker Recognition with a Cross-Modal Discriminative
Network,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2020</span>, 2020, pp. 2242–2246.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Muhammad Saad Saeed, Muhammad Haris Khan, Shah Nawaz, Muhammad Haroon Yousaf,
and Alessio Del Bue,

</span>
<span class="ltx_bibblock">“Fusion and orthogonal projection for improved face-voice
association,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>. IEEE, 2022, pp. 7057–7061.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Shah Nawaz, Muhammad Kamran Janjua, Ignazio Gallo, Arif Mahmood, and Alessandro
Calefati,

</span>
<span class="ltx_bibblock">“Deep latent space learning for cross-modal mapping of audio and
visual signals,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">2019 Digital Image Computing: Techniques and Applications
(DICTA)</span>. IEEE, 2019, pp. 1–7.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Muhammad Saad Saeed, Shah Nawaz, Muhammad Haris Khan, Muhammad Zaigham Zaheer,
Karthik Nandakumar, Muhammad Haroon Yousaf, and Arif Mahmood,

</span>
<span class="ltx_bibblock">“Single-branch network for multimodal training,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>. IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Jay Mathews,

</span>
<span class="ltx_bibblock">“Half of the world is bilingual. What’s our problem?,”
<a href="www.washingtonpost.com/local/education/half-the-world-is-bilingual-whats-our-problem/2019/04/24/1c2b0cc2-6625-11e9-a1b6-b29b90efa879_story" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.washingtonpost.com/local/education/half-the-world-is-bilingual-whats-our-problem/2019/04/24/1c2b0cc2-6625-11e9-a1b6-b29b90efa879_story</a>,
2019,

</span>
<span class="ltx_bibblock">[Online; accessed 16-April-2021].

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Shah Nawaz, Muhammad Saad Saeed, Pietro Morerio, Arif Mahmood, Ignazio Gallo,
Muhammad Haroon Yousaf, and Alessio Del Bue,

</span>
<span class="ltx_bibblock">“Cross-modal speaker verification and recognition: A multilingual
perspective,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, 2021, pp. 1682–1691.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Arsha Nagrani, Samuel Albanie, and Andrew Zisserman,

</span>
<span class="ltx_bibblock">“Learnable pins: Cross-modal embeddings for person identity,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, 2018, pp. 71–88.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman,

</span>
<span class="ltx_bibblock">“Deep face recognition,”

</span>
<span class="ltx_bibblock">2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Weidi Xie, Arsha Nagrani, Joon Son Chung, and Andrew Zisserman,

</span>
<span class="ltx_bibblock">“Utterance-level aggregation for speaker recognition in the wild,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>. IEEE, 2019, pp. 5791–5795.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.09341" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.09342" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.09342">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.09342" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.09343" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 21:36:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
