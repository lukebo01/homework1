<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.13018] Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings</title><meta property="og:description" content="Creating Automatic Speech Recognition (ASR) systems that are robust and resilient to classroom conditions is paramount to the development of AI tools to aid teachers and students. In this work, we study the efficacy of…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.13018">

<!--Generated on Wed Jun  5 14:55:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ahmed Adel Attia<sup id="id1.1.id1" class="ltx_sup">1</sup>,
Dorottya Demszky<sup id="id2.2.id2" class="ltx_sup">2</sup>,
Tolúlọpẹ́ Ògúnrẹ̀mí<sup id="id3.3.id3" class="ltx_sup">2</sup>,
Jing Liu<sup id="id4.4.id4" class="ltx_sup">1</sup>,
Carol Espy-Wilson<sup id="id5.5.id5" class="ltx_sup">1</sup>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">Creating Automatic Speech Recognition (ASR) systems that are robust and resilient to classroom conditions is paramount to the development of AI tools to aid teachers and students. In this work, we study the efficacy of continued pretraining (CPT) in adapting Wav2vec2.0 to the classroom domain. We show that CPT is a powerful tool in that regard and reduces the Word Error Rate (WER) of Wav2vec2.0-based models by upwards of 10%. More specifically, CPT improves the model’s robustness to different noises, microphones, classroom conditions as well as classroom demographics. Our CPT models show improved ability to generalize to different demographics unseen in the labeled finetuning data.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Ensuring equitable access to high quality educational opportunities remains a persistent challenge in the US education system <cite class="ltx_cite ltx_citemacro_citep">(Reardon <a href="#bib.bib35" title="" class="ltx_ref">2018</a>; Barrett et al. <a href="#bib.bib4" title="" class="ltx_ref">2021</a>; Xie, Fang, and Shauman <a href="#bib.bib44" title="" class="ltx_ref">2015</a>; Morrison, Annamma, and Jackson <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>. Disparities in instruction quality, and specifically teacher-student interactions, contribute significantly to systemic inequities <cite class="ltx_cite ltx_citemacro_citep">(Darling-Hammond <a href="#bib.bib8" title="" class="ltx_ref">2004</a>)</cite>. Providing teachers with feedback to improve their instruction has the potential to increase teacher effectiveness and reduce inequities <cite class="ltx_cite ltx_citemacro_citep">(Kraft, Blazar, and Hogan <a href="#bib.bib25" title="" class="ltx_ref">2018</a>; Link and Guskey <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>, but is highly resource intensive to implement at scale. AI has the potential to complement expert feedback and provide teachers with low-cost, consistent, automated feedback, which can improve their instruction and student outcomes <cite class="ltx_cite ltx_citemacro_citep">(Demszky and Liu <a href="#bib.bib10" title="" class="ltx_ref">2023a</a>; Demszky et al. <a href="#bib.bib12" title="" class="ltx_ref">2023</a>; Demszky and Liu <a href="#bib.bib11" title="" class="ltx_ref">2023b</a>; Jacobs et al. <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>. Such tools can foster active learning environments where students can contribute and feel heard, which in turn can foster equity in the classroom.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">Automatic Speech Recognition (ASR) is a critical component in the pipeline of tools needed to provide automated feedback. Transcripts generated by ASR systems can be analyzed on many levels to understand the dynamics in the classrooms — if they are sufficiently accurate <cite class="ltx_cite ltx_citemacro_citep">(Demszky et al. <a href="#bib.bib12" title="" class="ltx_ref">2023</a>; Jacobs et al. <a href="#bib.bib18" title="" class="ltx_ref">2022</a>, <a href="#bib.bib17" title="" class="ltx_ref">2024</a>)</cite>. Recent advancements in transformer models have allowed ASR systems to witness a major boom and approach human-level performance in transcribing clean American English speech <cite class="ltx_cite ltx_citemacro_citep">(Radford et al. <a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>. This level of performance is difficult to achieve for classroom ASR <cite class="ltx_cite ltx_citemacro_citep">(Southwell et al. <a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite>. Classrooms are uniquely characterized by the abundance of children’s speech and unique classroom noise, like children’s babble noise and as well as other conditions that are known to affect the accuracy of ASR systems, like far-field speech <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al. <a href="#bib.bib45" title="" class="ltx_ref">2024</a>)</cite>, and multi-speaker conditions <cite class="ltx_cite ltx_citemacro_citep">(Chang et al. <a href="#bib.bib5" title="" class="ltx_ref">2019</a>, <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<section id="Sx1.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Challenges of Children ASR</h3>

<div id="Sx1.SSx1.p1" class="ltx_para">
<p id="Sx1.SSx1.p1.1" class="ltx_p">ASR systems struggle with children’s speech out-of-the-box, even in clean conditions. ASR systems are mainly trained with adult speech and, therefore cannot deal with the fact that children speak less clearly than adults <cite class="ltx_cite ltx_citemacro_citep">(Lee, Potamianos, and Narayanan <a href="#bib.bib27" title="" class="ltx_ref">1999</a>)</cite> and that the basic acoustic and linguistic characteristics of children’s speech differ from that of adults <cite class="ltx_cite ltx_citemacro_citep">(Gerosa et al. <a href="#bib.bib14" title="" class="ltx_ref">2009</a>)</cite>. Additionally, children’s speech exhibits more inter-speaker variability due to varying developmental rates and intra-speaker variability due to underdeveloped pronunciation skills <cite class="ltx_cite ltx_citemacro_citep">(Koenig, Lucero, and Perlman <a href="#bib.bib24" title="" class="ltx_ref">2008</a>; Koenig and Lucero <a href="#bib.bib23" title="" class="ltx_ref">2008</a>; Lee, Potamianos, and Narayanan <a href="#bib.bib27" title="" class="ltx_ref">1999</a>, <a href="#bib.bib26" title="" class="ltx_ref">1997</a>; Vorperian and Kent <a href="#bib.bib43" title="" class="ltx_ref">2007</a>; Smith <a href="#bib.bib40" title="" class="ltx_ref">1992</a>)</cite>, or immigration from non-English-speaking countries. In fact, in the United States, a significant percentage of students are classified as English Language Learners 18% in California, 20% in Texas, 10% nationally <cite class="ltx_cite ltx_citemacro_citep">(NCES <a href="#bib.bib31" title="" class="ltx_ref">2018a</a>)</cite>.</p>
</div>
<div id="Sx1.SSx1.p2" class="ltx_para">
<p id="Sx1.SSx1.p2.1" class="ltx_p">In addition to the issues mentioned above, children’s speech exhibits unique linguistic properties that ASR systems are not adapted to. <cite class="ltx_cite ltx_citemacro_citet">Attia et al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> did an analysis the performance of Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford et al. <a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>, a popular transformer-based ASR system, on popular children’s speech corpora and found that the language model in Whisper struggles with the fact that children change topics multiple times in the sentence in unstructured spontaneous speech. They found that Whisper can achieve adult-level performance with simple and scripted prompts, proving that Whisper can adapt to the acoustic properties of children’s speech, but requires more work to adapt to the linguistic properties. Several other publications have shown that finetuning improves the performance of popular ASR systems, but a gap still exists between adult and children’s speech <cite class="ltx_cite ltx_citemacro_citep">(Attia et al. <a href="#bib.bib1" title="" class="ltx_ref">2023</a>; Shahnawazuddin et al. <a href="#bib.bib38" title="" class="ltx_ref">2024</a>; Jain et al. <a href="#bib.bib19" title="" class="ltx_ref">2023</a>; Southwell et al. <a href="#bib.bib42" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</section>
<section id="Sx1.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Challenges of Classroom environments</h3>

<div id="Sx1.SSx2.p1" class="ltx_para">
<p id="Sx1.SSx2.p1.1" class="ltx_p">In classrooms, however, the previously mentioned challenges are compounded by multiple factors, namely the presence of unique noises and the presence of multiple speakers.</p>
</div>
<div id="Sx1.SSx2.p2" class="ltx_para">
<p id="Sx1.SSx2.p2.1" class="ltx_p">In the US, classrooms hold around 20 students on average <cite class="ltx_cite ltx_citemacro_citep">(NCES <a href="#bib.bib32" title="" class="ltx_ref">2018b</a>)</cite>. In collaborative learning scenarios, children’s babble noise severely affects the performance of ASR. Babble noise, which is the noise from multiple speakers in the background, is considered one of the most challenging noises even in adult speech with adult babble <cite class="ltx_cite ltx_citemacro_citep">(Simic and Bocklet <a href="#bib.bib39" title="" class="ltx_ref">2024</a>)</cite>. However, children babble noise is even more challenging, as this kind of noise is unlikely to exist in public datasets used to train these models. The severity of noise increases with the number of students in the classroom and disproportionately affects overcrowded schools.</p>
</div>
<div id="Sx1.SSx2.p3" class="ltx_para">
<p id="Sx1.SSx2.p3.1" class="ltx_p">Additionally, classrooms are a multi-speaker environment, where an unpredetermined number of unseen speakers speak in the same classroom, possibly overlapping with each other. Multi-speaker ASR is an open area of research in and of itself, and a wide gap still exists between single and multi-speaker ASR <cite class="ltx_cite ltx_citemacro_citep">(Chang et al. <a href="#bib.bib5" title="" class="ltx_ref">2019</a>, <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>. In classrooms, there is an abundance of overlapping speech which complicates the multi-speaker ASR problem further. Far-field speech is another problem, for example, if a student asks a question while being far from the microphone. Far-field speech is another open area of research, and previous works have shown it to be a much more difficult task than near-field speech <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al. <a href="#bib.bib45" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</section>
<section id="Sx1.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Data Scarcity</h3>

<div id="Sx1.SSx3.p1" class="ltx_para">
<p id="Sx1.SSx3.p1.1" class="ltx_p">All of the previous challenges are compounded by the fact that transcribed classroom datasets are scarce, and those that exist are not public, partially due to the sensitivity of data representing minors. Although non-transcribed classroom recordings do exist, transcription can be prohibitively expensive. This low-resource setting lends itself well to self-supervised speech representation models like Wav2vec2.0 <cite class="ltx_cite ltx_citemacro_citep">(Baevski et al. <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>, where the untranscribed data can be used to pretrain Wav2vec2.0 and the limited transcribed data can be used for finetuning for ASR.</p>
</div>
</section>
<section id="Sx1.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">How Continued Pretraining Can Help</h3>

<div id="Sx1.SSx4.p1" class="ltx_para">
<p id="Sx1.SSx4.p1.1" class="ltx_p">In this paper, we propose continued pretraining (CPT) as an effective way to adapt Wav2vec2.0 to domain-specific noisy speech recognition, namely classroom speech recognition. We consider three different 300M parameter variations of Wav2vec2.0:Wav2vec2.0 pretrained on LibriVox-60, XLS-R <cite class="ltx_cite ltx_citemacro_citep">(Babu et al. <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite> pretrained on speech from 128 languages, and Robust Wav2vec2.0 pretrained on noisy English speech. We perform CPT on unlabeled classroom data for each model and then finetune the resultant models on labeled classroom data for ASR. In addition, we perform finetuning of the off-the-shelf Wav2vec2.0 models without CPT. We also pretrain Wav2vec2.0 from scratch on the same data used in CPT as it is the baseline used in similar publications <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al. <a href="#bib.bib46" title="" class="ltx_ref">2022</a>)</cite>. Our results show that CPT is the most effective way for adapting Wav2vec2.0 for noisy classroom speech and that the choice of starting point for CPT affects performance. In fact, our best-performing model outperforms Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford et al. <a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>
when finetuned on the same dataset.</p>
</div>
<div id="Sx1.SSx4.p2" class="ltx_para">
<p id="Sx1.SSx4.p2.1" class="ltx_p"><span id="Sx1.SSx4.p2.1.1" class="ltx_text ltx_font_bold">Our contributions</span> in this research can be summarized as follows:</p>
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">We show that CPT is the most effective tool to adapt Wav2vec2.0 to noisy conditions like classrooms.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">We show how CPT can improve the robustness of Wav2vec2.0, not only to noise but to different microphone configurations and demographics.</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">Our proposed methods are more robust to noise and different demographics than Whisper, both off-the-shelf and finetuned.</p>
</div>
</li>
<li id="Sx1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i4.p1" class="ltx_para">
<p id="Sx1.I1.i4.p1.1" class="ltx_p">We provide a detailed analysis showing how ASR models might be biased against minority teachers and how to mitigate these problems moving forward.</p>
</div>
</li>
<li id="Sx1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i5.p1" class="ltx_para">
<p id="Sx1.I1.i5.p1.1" class="ltx_p">We demonstrate the use of existing classroom text corpora for LM pretraining.</p>
</div>
</li>
</ul>
</div>
<div id="Sx1.SSx4.p3" class="ltx_para">
<p id="Sx1.SSx4.p3.1" class="ltx_p">To facilitate further research and reproducibility, our training code as well as our model checkpoints will be available at the time of the camera-ready submission.</p>
</div>
</section>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Background: Wav2vec2.0</h2>

<figure id="Sx2.F1" class="ltx_figure"><img src="/html/2405.13018/assets/Figures/Wav2vec2.0.png" id="Sx2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="1003" height="923" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Wav2vec2.0 pretraining architecture. Lowercase q in circles represents the quantization networks, with the green circle representing the positive sample and the red circles representing the negative samples. Adapted from <cite class="ltx_cite ltx_citemacro_citep">(Baevski et al. <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite></figcaption>
</figure>
<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">Wav2vec2.0 is a Self Supervised Learning (SSL) speech representation model developed by <cite class="ltx_cite ltx_citemacro_citet">Baevski et al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>, in a follow-up to Wav2vec <cite class="ltx_cite ltx_citemacro_citep">(Schneider et al. <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite>. Wav2vec2.0 utilizes the contextualization capabilities of transformers to learn contextual self-supervised speech representations from unlabeled audio in an SSL paradigm.</p>
</div>
<div id="Sx2.p2" class="ltx_para">
<p id="Sx2.p2.1" class="ltx_p">Supervised speech models, like Whisper, learn directly on task-specific labeled data. That means they need a large amount of human-transcribed data to achieve SOTA performance. On the flip side, SSL models can learn useful speech representations from unlabeled data, either through non-contrastive learning, by extracting targets from the input speech signal to perform predictive learning, like in the case of HuBERT <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al. <a href="#bib.bib15" title="" class="ltx_ref">2021a</a>)</cite>, or through contrastive learning by contrasting positive and negative examples, like in Wav2vec.</p>
</div>
<div id="Sx2.p3" class="ltx_para">
<p id="Sx2.p3.1" class="ltx_p">Wav2vec2.0’s architecture consists of a convolutional feature extractor that extracts latent representations from raw audio and a transformer network that produces contextual representations of speech. During self-supervised pretraining, the latent representations generated by the convolutional feature encoder are quantized and the unquantized latent features are fed into the transformer, with some frames masked. The model learns through a constrastive learning task which is to distinguish the quantized representations corresponding to the masked frames from the context. Additionally, a diversity loss is also applied to diversify the quantized representations.</p>
</div>
<div id="Sx2.p4" class="ltx_para">
<p id="Sx2.p4.1" class="ltx_p">For ASR finetuning, a single randomly initialized layer is added on top of the transformer network, whose size corresponds to the number of characters in the vocabulary plus a single word boundary token. The model is then optimized using a Connectionist Temporal Classification (CTC) loss.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Related Previous Works</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">In this section, we discuss previous works of interest related to noise robustness and domain adaptation in Wav2vec2.0</p>
</div>
<section id="Sx3.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Robust Wav2vec2.0</h3>

<div id="Sx3.SSx1.p1" class="ltx_para">
<p id="Sx3.SSx1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Hsu et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021b</a>)</cite> investigated the effect of using target domain data during the pretraining. Their findings suggest that adding in-domain data during pretraining improves performance when the resulting model is finetuned on that in-domain data. Additionally, they also found that adding more data even if it is out-of-domain still improves performance, but less so than in-domain data.</p>
</div>
<div id="Sx3.SSx1.p2" class="ltx_para">
<p id="Sx3.SSx1.p2.1" class="ltx_p">Most relevant to our work, are their experiments with CPT. They show that performing CPT with more unlabeled in-domain data and then finetuning on out-of-domain data improves the performance on an in-domain test set. These results serve as a strong motivation for our work, however, they do not sufficiently answer our question about the effectiveness of CPT for improving noise robustness for two main reasons: (1) they do not finetune on their in-domain dataset and, (2) their in-domain test dataset is Librispeech <span id="Sx3.SSx1.p2.1.1" class="ltx_text ltx_font_italic">dev-other</span>, and while it is more challenging than <span id="Sx3.SSx1.p2.1.2" class="ltx_text ltx_font_italic">dev-clean</span>, it is still considered a very clean dataset compared to the challenging domain of classroom ASR.</p>
</div>
</section>
<section id="Sx3.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Noise-Robust Wav2vec2.0</h3>

<div id="Sx3.SSx2.p1" class="ltx_para">
<p id="Sx3.SSx2.p1.1" class="ltx_p">The work proposed by <cite class="ltx_cite ltx_citemacro_citet">Zhu et al. (<a href="#bib.bib46" title="" class="ltx_ref">2022</a>)</cite> investigates the noise robustness of Wav2vec2.0 to noise and proposes a new pretraining paradigm that performs implicit speech enhancement on the latent representations in Wav2vec2.0. Simply put, during pretraining they feed a clean frame as well as a noise-augmented frame to the same copy of the convolutional encoder network and then force the output latent representations to be closer together via a consistency loss. They then feed the noisy latent representations to the transformer, but they sample the positive and negative samples from the quantized clean features.</p>
</div>
<div id="Sx3.SSx2.p2" class="ltx_para">
<p id="Sx3.SSx2.p2.1" class="ltx_p">Their experiments on vanilla-wav2vec2.0 show that pretraining on noisy speech improves the performance on both in-domain and out-of-domain finetuning ASR tasks when compared to clean pretraining. Their results show that their proposed pretraining structure achieves a 4-5% absolute improvement in Word Error Rate (WER) in noisy speech across several signal-to-noise ratio (SNR) values as well as less degradation in clean speech performance. The main drawback of that method is that it requires the use of clean-noisy pairs for pretraining and doesn’t generalize to naturally occurring noisy recordings like classroom recordings. However, their experiments show that even without any modifications, pretraining on noisy speech improves the performance in both in-domain and out-of-domain finetuning scenarios.</p>
</div>
</section>
<section id="Sx3.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Adaptation of Wav2vec2.0 to low-resource languages through CPT</h3>

<div id="Sx3.SSx3.p1" class="ltx_para">
<p id="Sx3.SSx3.p1.1" class="ltx_p">Several research papers have attempted to investigate the effectiveness of CPT in adapting good multi-lingual self-supervised ASR systems like XLSR53 <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al. <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite> and XLS-R <cite class="ltx_cite ltx_citemacro_citep">(Babu et al. <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>. The research by <cite class="ltx_cite ltx_citemacro_citep">(Nowakowski et al. <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite> aimed to develop an ASR system for Ainu, a critically endangered and low-resource language local to northern Japan, Sakhalin, and Kuril Islands. Starting from XLSR53 which was already pretrained on 56K hours from 53 languages, they performed CPT on 234 hours of Ainu recordings. Then they performed multiple finetuning experiments using different subsets of their labeled training data. They describe CPT as ”clearly the most effective way to adapt a speech representation model for a new language”. CPT decreased their WER by up to 40% relative to the unadapted model, an absolute improvement of about 20.6%.</p>
</div>
<div id="Sx3.SSx3.p2" class="ltx_para">
<p id="Sx3.SSx3.p2.1" class="ltx_p">Another interesting study in the field of low-resource languages <cite class="ltx_cite ltx_citemacro_citep">(San et al. <a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite> performed CPT on different languages, including the target language (Punjabi) as well as languages similar and different to it starting from XLS-R 128. Their best-performing setup was with CPT on the full 70 hours available of the target language (22.2% WER) and they found better performance when supplementing the target language with a related language like Hindi (23.4% WER) versus using unrelated languages like Malayalam, Bengali, Odia, or Tamil (25% WER), however, even CPT on a mixture of 10 hours of Punjabi and 60 hours of unrelated languages improved the performance relative to the unadapted XLS-R 128 model (30.8% WER).</p>
</div>
</section>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Datasets</h2>

<section id="Sx4.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Audio Datasets</h3>

<section id="Sx4.SSx1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">NCTE</h4>

<div id="Sx4.SSx1.SSSx1.p1" class="ltx_para">
<p id="Sx4.SSx1.SSSx1.p1.1" class="ltx_p">The NCTE dataset consists of video and audio recordings of 2128 4th and 5th-grade elementary math classrooms collected as part of the National Center for Teacher Effectiveness (NCTE) Main Study <cite class="ltx_cite ltx_citemacro_citep">(Kane, Hill, and Staiger <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>. The observations took place between 2010 and 2013 across four districts serving historically marginalized students.</p>
</div>
<div id="Sx4.SSx1.SSSx1.p2" class="ltx_para">
<p id="Sx4.SSx1.SSSx1.p2.1" class="ltx_p">Table <a href="#Sx4.T1" title="Table 1 ‣ NCTE ‣ Audio Datasets ‣ Datasets ‣ Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the key demographics of the populations of students and teachers in the recordings. The statistics show that the students are balanced by gender, but the vast majority of them come from minority racial backgrounds (72.3%). The majority of the students received free or reduced-price lunches (64.9%) indicating that they likely come from low-income households. A sizable percentage of the students exhibited limited English proficiency (19.7%) or special educational status (12.3%). On the other hand, the teachers were mostly female (82.4%) and White (64.2%). This disparity between students’ and teachers’ demographics is within the national statistics <cite class="ltx_cite ltx_citemacro_citep">(Demszky and Hill <a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<figure id="Sx4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Key demographics from the NCTE dataset. LEP indicates that the students have Limited English Proficiency, FRPL indicates that they receive Free or Reduced-Price Lunches in the given school year and SPED indicates Special Education status. </figcaption>
<p id="Sx4.T1.1" class="ltx_p ltx_align_center"><span id="Sx4.T1.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="Sx4.T1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:204.5pt;height:306pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="Sx4.T1.1.1.1.1" class="ltx_p"><span id="Sx4.T1.1.1.1.1.1" class="ltx_text">
<span id="Sx4.T1.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="Sx4.T1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_t"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span>Statistic</span>
<span id="Sx4.T1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Teachers</span>
<span id="Sx4.T1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Students</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.2.2" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_tt"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span>Number</span>
<span id="Sx4.T1.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">313</span>
<span id="Sx4.T1.1.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">12661</span></span>
</span>
<span class="ltx_tbody">
<span id="Sx4.T1.1.1.1.1.1.1.3.1" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span>% Male</span>
<span id="Sx4.T1.1.1.1.1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.3%</span>
<span id="Sx4.T1.1.1.1.1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">49.4%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.4.2" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr">% Female</span>
<span id="Sx4.T1.1.1.1.1.1.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r">82.4%</span>
<span id="Sx4.T1.1.1.1.1.1.1.4.2.3" class="ltx_td ltx_align_center">49.6%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.5.3" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr">% No data</span>
<span id="Sx4.T1.1.1.1.1.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r">1.3%</span>
<span id="Sx4.T1.1.1.1.1.1.1.5.3.3" class="ltx_td ltx_align_center">1%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.6.4" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span>% African-American</span>
<span id="Sx4.T1.1.1.1.1.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.4%</span>
<span id="Sx4.T1.1.1.1.1.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_t">42.3%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.7.5" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr">% Asian</span>
<span id="Sx4.T1.1.1.1.1.1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_r">2.6%</span>
<span id="Sx4.T1.1.1.1.1.1.1.7.5.3" class="ltx_td ltx_align_center">7.4%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.8.6" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr">% Hispanic</span>
<span id="Sx4.T1.1.1.1.1.1.1.8.6.2" class="ltx_td ltx_align_center ltx_border_r">2.9%</span>
<span id="Sx4.T1.1.1.1.1.1.1.8.6.3" class="ltx_td ltx_align_center">22.6%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.9.7" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr">% White</span>
<span id="Sx4.T1.1.1.1.1.1.1.9.7.2" class="ltx_td ltx_align_center ltx_border_r">64.2%</span>
<span id="Sx4.T1.1.1.1.1.1.1.9.7.3" class="ltx_td ltx_align_center">22.7%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.10.8" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr">% Other</span>
<span id="Sx4.T1.1.1.1.1.1.1.10.8.2" class="ltx_td ltx_align_center ltx_border_r">3.6%</span>
<span id="Sx4.T1.1.1.1.1.1.1.10.8.3" class="ltx_td ltx_align_center">3.9%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.11.9" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.11.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr">% No data</span>
<span id="Sx4.T1.1.1.1.1.1.1.11.9.2" class="ltx_td ltx_align_center ltx_border_r">0%</span>
<span id="Sx4.T1.1.1.1.1.1.1.11.9.3" class="ltx_td ltx_align_center">1%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.12.10" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.12.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span>% LEP</span>
<span id="Sx4.T1.1.1.1.1.1.1.12.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</span>
<span id="Sx4.T1.1.1.1.1.1.1.12.10.3" class="ltx_td ltx_align_center ltx_border_t">19.7%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.13.11" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr">% No data</span>
<span id="Sx4.T1.1.1.1.1.1.1.13.11.2" class="ltx_td ltx_align_center ltx_border_r">-</span>
<span id="Sx4.T1.1.1.1.1.1.1.13.11.3" class="ltx_td ltx_align_center">1.2%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.14.12" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.14.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span> % SPED</span>
<span id="Sx4.T1.1.1.1.1.1.1.14.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</span>
<span id="Sx4.T1.1.1.1.1.1.1.14.12.3" class="ltx_td ltx_align_center ltx_border_t">12.3%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.15.13" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.15.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr">% No data</span>
<span id="Sx4.T1.1.1.1.1.1.1.15.13.2" class="ltx_td ltx_align_center ltx_border_r">-</span>
<span id="Sx4.T1.1.1.1.1.1.1.15.13.3" class="ltx_td ltx_align_center">1.2%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.16.14" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.16.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span>% FRPL</span>
<span id="Sx4.T1.1.1.1.1.1.1.16.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</span>
<span id="Sx4.T1.1.1.1.1.1.1.16.14.3" class="ltx_td ltx_align_center ltx_border_t">64.9%</span></span>
<span id="Sx4.T1.1.1.1.1.1.1.17.15" class="ltx_tr">
<span id="Sx4.T1.1.1.1.1.1.1.17.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_rr">% No data</span>
<span id="Sx4.T1.1.1.1.1.1.1.17.15.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">-</span>
<span id="Sx4.T1.1.1.1.1.1.1.17.15.3" class="ltx_td ltx_align_center ltx_border_bb">1.2%</span></span>
</span>
</span></span></span>
</span></span></span></p>
</figure>
<figure id="Sx4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Key statistics from the transcribed portion of the NCTE dataset. % Teacher refers to the percentage of speech duration attributed to the teacher. AFAM refers to African-American. FF indicates a far-field microphone and NN indicates a near-field microphone.</figcaption>
<p id="Sx4.T2.1" class="ltx_p ltx_align_center"><span id="Sx4.T2.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="Sx4.T2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:628.8pt;height:289pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="Sx4.T2.1.1.1.1" class="ltx_p"><span id="Sx4.T2.1.1.1.1.1" class="ltx_text">
<span id="Sx4.T2.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="Sx4.T2.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_border_r ltx_border_t"></span>
<span id="Sx4.T2.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t ltx_colspan ltx_colspan_2"><span id="Sx4.T2.1.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Teacher</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t ltx_colspan ltx_colspan_7"><span id="Sx4.T2.1.1.1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Students</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_border_rr ltx_border_t"></span>
<span id="Sx4.T2.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_border_t"></span></span>
<span id="Sx4.T2.1.1.1.1.1.1.2.2" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_11"><span id="Sx4.T2.1.1.1.1.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Used for training and validation</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_border_t"></span></span>
<span id="Sx4.T2.1.1.1.1.1.1.3.3" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.3.3.1.1" class="ltx_text ltx_font_bold">Class ID</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.3.3.2.1" class="ltx_text ltx_font_bold">Gender</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.3.3.3.1" class="ltx_text ltx_font_bold">Race</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.3.3.4.1" class="ltx_text ltx_font_bold">% Male</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.3.3.5.1" class="ltx_text ltx_font_bold">% AFAM</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.3.3.6.1" class="ltx_text ltx_font_bold">% Asian</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.3.3.7.1" class="ltx_text ltx_font_bold">% Hispanic</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.3.3.8.1" class="ltx_text ltx_font_bold">% White</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.3.3.9.1" class="ltx_text ltx_font_bold">% Other</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.3.3.10.1" class="ltx_text ltx_font_bold"># Students</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.3.3.11.1" class="ltx_text ltx_font_bold">% Teacher</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.3.3.12.1" class="ltx_text ltx_font_bold">Microphone</span></span></span>
<span id="Sx4.T2.1.1.1.1.1.1.4.4" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.4.4.1.1" class="ltx_text ltx_font_bold">144</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">Female</span>
<span id="Sx4.T2.1.1.1.1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">White</span>
<span id="Sx4.T2.1.1.1.1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">85%</span>
<span id="Sx4.T2.1.1.1.1.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">62%</span>
<span id="Sx4.T2.1.1.1.1.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">0%</span>
<span id="Sx4.T2.1.1.1.1.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">8%</span>
<span id="Sx4.T2.1.1.1.1.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_t">31%</span>
<span id="Sx4.T2.1.1.1.1.1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0%</span>
<span id="Sx4.T2.1.1.1.1.1.1.4.4.10" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">13</span>
<span id="Sx4.T2.1.1.1.1.1.1.4.4.11" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">84%</span>
<span id="Sx4.T2.1.1.1.1.1.1.4.4.12" class="ltx_td ltx_align_center ltx_border_t">NF</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.5.5" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T2.1.1.1.1.1.1.5.5.1.1" class="ltx_text ltx_font_bold">622</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.5.5.2" class="ltx_td ltx_align_center">Female</span>
<span id="Sx4.T2.1.1.1.1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_rr">AFAM</span>
<span id="Sx4.T2.1.1.1.1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r">53%</span>
<span id="Sx4.T2.1.1.1.1.1.1.5.5.5" class="ltx_td ltx_align_center">16%</span>
<span id="Sx4.T2.1.1.1.1.1.1.5.5.6" class="ltx_td ltx_align_center">11%</span>
<span id="Sx4.T2.1.1.1.1.1.1.5.5.7" class="ltx_td ltx_align_center">47%</span>
<span id="Sx4.T2.1.1.1.1.1.1.5.5.8" class="ltx_td ltx_align_center">26%</span>
<span id="Sx4.T2.1.1.1.1.1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r">0%</span>
<span id="Sx4.T2.1.1.1.1.1.1.5.5.10" class="ltx_td ltx_align_center ltx_border_rr">19</span>
<span id="Sx4.T2.1.1.1.1.1.1.5.5.11" class="ltx_td ltx_align_center ltx_border_rr">79%</span>
<span id="Sx4.T2.1.1.1.1.1.1.5.5.12" class="ltx_td ltx_align_center">NF</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.6.6" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T2.1.1.1.1.1.1.6.6.1.1" class="ltx_text ltx_font_bold">2619</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.6.6.2" class="ltx_td ltx_align_center">Female</span>
<span id="Sx4.T2.1.1.1.1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_rr">White</span>
<span id="Sx4.T2.1.1.1.1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r">48%</span>
<span id="Sx4.T2.1.1.1.1.1.1.6.6.5" class="ltx_td ltx_align_center">76%</span>
<span id="Sx4.T2.1.1.1.1.1.1.6.6.6" class="ltx_td ltx_align_center">5%</span>
<span id="Sx4.T2.1.1.1.1.1.1.6.6.7" class="ltx_td ltx_align_center">0%</span>
<span id="Sx4.T2.1.1.1.1.1.1.6.6.8" class="ltx_td ltx_align_center">19%</span>
<span id="Sx4.T2.1.1.1.1.1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r">0%</span>
<span id="Sx4.T2.1.1.1.1.1.1.6.6.10" class="ltx_td ltx_align_center ltx_border_rr">21</span>
<span id="Sx4.T2.1.1.1.1.1.1.6.6.11" class="ltx_td ltx_align_center ltx_border_rr">84%</span>
<span id="Sx4.T2.1.1.1.1.1.1.6.6.12" class="ltx_td ltx_align_center">FF</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.7.7" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T2.1.1.1.1.1.1.7.7.1.1" class="ltx_text ltx_font_bold">2709</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.7.7.2" class="ltx_td ltx_align_center">Female</span>
<span id="Sx4.T2.1.1.1.1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_rr">White</span>
<span id="Sx4.T2.1.1.1.1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">63%</span>
<span id="Sx4.T2.1.1.1.1.1.1.7.7.5" class="ltx_td ltx_align_center">13%</span>
<span id="Sx4.T2.1.1.1.1.1.1.7.7.6" class="ltx_td ltx_align_center">29%</span>
<span id="Sx4.T2.1.1.1.1.1.1.7.7.7" class="ltx_td ltx_align_center">13%</span>
<span id="Sx4.T2.1.1.1.1.1.1.7.7.8" class="ltx_td ltx_align_center">42%</span>
<span id="Sx4.T2.1.1.1.1.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_r">4%</span>
<span id="Sx4.T2.1.1.1.1.1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_rr">24</span>
<span id="Sx4.T2.1.1.1.1.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_rr">68%</span>
<span id="Sx4.T2.1.1.1.1.1.1.7.7.12" class="ltx_td ltx_align_center">NF</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.8.8" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T2.1.1.1.1.1.1.8.8.1.1" class="ltx_text ltx_font_bold">2944</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.8.8.2" class="ltx_td ltx_align_center">Female</span>
<span id="Sx4.T2.1.1.1.1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_rr">White</span>
<span id="Sx4.T2.1.1.1.1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r">54%</span>
<span id="Sx4.T2.1.1.1.1.1.1.8.8.5" class="ltx_td ltx_align_center">46%</span>
<span id="Sx4.T2.1.1.1.1.1.1.8.8.6" class="ltx_td ltx_align_center">4%</span>
<span id="Sx4.T2.1.1.1.1.1.1.8.8.7" class="ltx_td ltx_align_center">8%</span>
<span id="Sx4.T2.1.1.1.1.1.1.8.8.8" class="ltx_td ltx_align_center">38%</span>
<span id="Sx4.T2.1.1.1.1.1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_r">4%</span>
<span id="Sx4.T2.1.1.1.1.1.1.8.8.10" class="ltx_td ltx_align_center ltx_border_rr">26</span>
<span id="Sx4.T2.1.1.1.1.1.1.8.8.11" class="ltx_td ltx_align_center ltx_border_rr">84%</span>
<span id="Sx4.T2.1.1.1.1.1.1.8.8.12" class="ltx_td ltx_align_center">NF</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.9.9" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T2.1.1.1.1.1.1.9.9.1.1" class="ltx_text ltx_font_bold">4724</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.9.9.2" class="ltx_td ltx_align_center">Female</span>
<span id="Sx4.T2.1.1.1.1.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_rr">White</span>
<span id="Sx4.T2.1.1.1.1.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r">46%</span>
<span id="Sx4.T2.1.1.1.1.1.1.9.9.5" class="ltx_td ltx_align_center">46%</span>
<span id="Sx4.T2.1.1.1.1.1.1.9.9.6" class="ltx_td ltx_align_center">0%</span>
<span id="Sx4.T2.1.1.1.1.1.1.9.9.7" class="ltx_td ltx_align_center">11%</span>
<span id="Sx4.T2.1.1.1.1.1.1.9.9.8" class="ltx_td ltx_align_center">43%</span>
<span id="Sx4.T2.1.1.1.1.1.1.9.9.9" class="ltx_td ltx_align_center ltx_border_r">0%</span>
<span id="Sx4.T2.1.1.1.1.1.1.9.9.10" class="ltx_td ltx_align_center ltx_border_rr">28</span>
<span id="Sx4.T2.1.1.1.1.1.1.9.9.11" class="ltx_td ltx_align_center ltx_border_rr">92%</span>
<span id="Sx4.T2.1.1.1.1.1.1.9.9.12" class="ltx_td ltx_align_center">NF</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.10.10" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.10.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.10.10.1.1" class="ltx_text ltx_font_bold">Total</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx4.T2.1.1.1.1.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">-</span>
<span id="Sx4.T2.1.1.1.1.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">56%</span>
<span id="Sx4.T2.1.1.1.1.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_t">42%</span>
<span id="Sx4.T2.1.1.1.1.1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_t">8%</span>
<span id="Sx4.T2.1.1.1.1.1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_t">14%</span>
<span id="Sx4.T2.1.1.1.1.1.1.10.10.8" class="ltx_td ltx_align_center ltx_border_t">34%</span>
<span id="Sx4.T2.1.1.1.1.1.1.10.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2%</span>
<span id="Sx4.T2.1.1.1.1.1.1.10.10.10" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">131</span>
<span id="Sx4.T2.1.1.1.1.1.1.10.10.11" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">NF</span>
<span id="Sx4.T2.1.1.1.1.1.1.10.10.12" class="ltx_td ltx_border_t"></span></span>
<span id="Sx4.T2.1.1.1.1.1.1.11.11" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.11.11.1" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_11"><span id="Sx4.T2.1.1.1.1.1.1.11.11.1.1" class="ltx_text ltx_font_bold">Used for testing and analysis</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.11.11.2" class="ltx_td ltx_border_tt"></span></span>
<span id="Sx4.T2.1.1.1.1.1.1.12.12" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.12.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.12.12.1.1" class="ltx_text ltx_font_bold">13</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_t">Male</span>
<span id="Sx4.T2.1.1.1.1.1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Asian</span>
<span id="Sx4.T2.1.1.1.1.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61%</span>
<span id="Sx4.T2.1.1.1.1.1.1.12.12.5" class="ltx_td ltx_align_center ltx_border_t">9%</span>
<span id="Sx4.T2.1.1.1.1.1.1.12.12.6" class="ltx_td ltx_align_center ltx_border_t">65%</span>
<span id="Sx4.T2.1.1.1.1.1.1.12.12.7" class="ltx_td ltx_align_center ltx_border_t">13%</span>
<span id="Sx4.T2.1.1.1.1.1.1.12.12.8" class="ltx_td ltx_align_center ltx_border_t">9%</span>
<span id="Sx4.T2.1.1.1.1.1.1.12.12.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4%</span>
<span id="Sx4.T2.1.1.1.1.1.1.12.12.10" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">23</span>
<span id="Sx4.T2.1.1.1.1.1.1.12.12.11" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">78%</span>
<span id="Sx4.T2.1.1.1.1.1.1.12.12.12" class="ltx_td ltx_align_center ltx_border_t">NF</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.13.13" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.13.13.1" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T2.1.1.1.1.1.1.13.13.1.1" class="ltx_text ltx_font_bold">4106</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.13.13.2" class="ltx_td ltx_align_center">Female</span>
<span id="Sx4.T2.1.1.1.1.1.1.13.13.3" class="ltx_td ltx_align_center ltx_border_rr">AFAM</span>
<span id="Sx4.T2.1.1.1.1.1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r">50%</span>
<span id="Sx4.T2.1.1.1.1.1.1.13.13.5" class="ltx_td ltx_align_center">14%</span>
<span id="Sx4.T2.1.1.1.1.1.1.13.13.6" class="ltx_td ltx_align_center">50%</span>
<span id="Sx4.T2.1.1.1.1.1.1.13.13.7" class="ltx_td ltx_align_center">21%</span>
<span id="Sx4.T2.1.1.1.1.1.1.13.13.8" class="ltx_td ltx_align_center">0%</span>
<span id="Sx4.T2.1.1.1.1.1.1.13.13.9" class="ltx_td ltx_align_center ltx_border_r">14%</span>
<span id="Sx4.T2.1.1.1.1.1.1.13.13.10" class="ltx_td ltx_align_center ltx_border_rr">12</span>
<span id="Sx4.T2.1.1.1.1.1.1.13.13.11" class="ltx_td ltx_align_center ltx_border_rr">81%</span>
<span id="Sx4.T2.1.1.1.1.1.1.13.13.12" class="ltx_td ltx_align_center">NF</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.14.14" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.14.14.1" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T2.1.1.1.1.1.1.14.14.1.1" class="ltx_text ltx_font_bold">4352</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.14.14.2" class="ltx_td ltx_align_center">Male</span>
<span id="Sx4.T2.1.1.1.1.1.1.14.14.3" class="ltx_td ltx_align_center ltx_border_rr">White</span>
<span id="Sx4.T2.1.1.1.1.1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r">41%</span>
<span id="Sx4.T2.1.1.1.1.1.1.14.14.5" class="ltx_td ltx_align_center">34%</span>
<span id="Sx4.T2.1.1.1.1.1.1.14.14.6" class="ltx_td ltx_align_center">10%</span>
<span id="Sx4.T2.1.1.1.1.1.1.14.14.7" class="ltx_td ltx_align_center">7%</span>
<span id="Sx4.T2.1.1.1.1.1.1.14.14.8" class="ltx_td ltx_align_center">48%</span>
<span id="Sx4.T2.1.1.1.1.1.1.14.14.9" class="ltx_td ltx_align_center ltx_border_r">0%</span>
<span id="Sx4.T2.1.1.1.1.1.1.14.14.10" class="ltx_td ltx_align_center ltx_border_rr">29</span>
<span id="Sx4.T2.1.1.1.1.1.1.14.14.11" class="ltx_td ltx_align_center ltx_border_rr">88%</span>
<span id="Sx4.T2.1.1.1.1.1.1.14.14.12" class="ltx_td ltx_align_center">NF</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.15.15" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.15.15.1" class="ltx_td ltx_align_center ltx_border_r"><span id="Sx4.T2.1.1.1.1.1.1.15.15.1.1" class="ltx_text ltx_font_bold">4651</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.15.15.2" class="ltx_td ltx_align_center">Female</span>
<span id="Sx4.T2.1.1.1.1.1.1.15.15.3" class="ltx_td ltx_align_center ltx_border_rr">AFAM</span>
<span id="Sx4.T2.1.1.1.1.1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r">55%</span>
<span id="Sx4.T2.1.1.1.1.1.1.15.15.5" class="ltx_td ltx_align_center">27%</span>
<span id="Sx4.T2.1.1.1.1.1.1.15.15.6" class="ltx_td ltx_align_center">14%</span>
<span id="Sx4.T2.1.1.1.1.1.1.15.15.7" class="ltx_td ltx_align_center">9%</span>
<span id="Sx4.T2.1.1.1.1.1.1.15.15.8" class="ltx_td ltx_align_center">50%</span>
<span id="Sx4.T2.1.1.1.1.1.1.15.15.9" class="ltx_td ltx_align_center ltx_border_r">0%</span>
<span id="Sx4.T2.1.1.1.1.1.1.15.15.10" class="ltx_td ltx_align_center ltx_border_rr">22</span>
<span id="Sx4.T2.1.1.1.1.1.1.15.15.11" class="ltx_td ltx_align_center ltx_border_rr">80%</span>
<span id="Sx4.T2.1.1.1.1.1.1.15.15.12" class="ltx_td ltx_align_center">FF</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.16.16" class="ltx_tr">
<span id="Sx4.T2.1.1.1.1.1.1.16.16.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="Sx4.T2.1.1.1.1.1.1.16.16.1.1" class="ltx_text ltx_font_bold">Total</span></span>
<span id="Sx4.T2.1.1.1.1.1.1.16.16.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">-</span>
<span id="Sx4.T2.1.1.1.1.1.1.16.16.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr ltx_border_t">-</span>
<span id="Sx4.T2.1.1.1.1.1.1.16.16.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">51%</span>
<span id="Sx4.T2.1.1.1.1.1.1.16.16.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">23%</span>
<span id="Sx4.T2.1.1.1.1.1.1.16.16.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">32%</span>
<span id="Sx4.T2.1.1.1.1.1.1.16.16.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">11%</span>
<span id="Sx4.T2.1.1.1.1.1.1.16.16.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">31%</span>
<span id="Sx4.T2.1.1.1.1.1.1.16.16.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">3%</span>
<span id="Sx4.T2.1.1.1.1.1.1.16.16.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr ltx_border_t">88</span>
<span id="Sx4.T2.1.1.1.1.1.1.16.16.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr ltx_border_t">NF</span>
<span id="Sx4.T2.1.1.1.1.1.1.16.16.12" class="ltx_td ltx_border_bb ltx_border_t"></span></span>
</span>
</span></span></span>
</span></span></span></p>
</figure>
<div id="Sx4.SSx1.SSSx1.p3" class="ltx_para">
<p id="Sx4.SSx1.SSSx1.p3.1" class="ltx_p">For each classroom, 2 to 3 video and audio recordings exist, from different angles and microphones, each lasting 45 minutes to an hour. The total duration of the recordings from all microphones and classrooms amounts to 5235 hours. We resampled the audio from the original 44.1KHz sampling rate to 16KHz and cut it into 20-second chunks. About 10% of the data was reserved for validation, and the rest was used as unlabeled pretraining data.</p>
</div>
<div id="Sx4.SSx1.SSSx1.p4" class="ltx_para">
<p id="Sx4.SSx1.SSSx1.p4.1" class="ltx_p">Out of these recordings, 6 were initially randomly chosen to be transcribed to create a low-resource unbalanced problem. The duration of this subset is about 5.15 hours, with the duration of each file between 45-60 minutes. Key statistics about the transcribed recordings are in Table <a href="#Sx4.T2" title="Table 2 ‣ NCTE ‣ Audio Datasets ‣ Datasets ‣ Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. All teachers are White women except for one who is an African-American woman. The student’s gender is mostly balanced throughout the entire dataset, with about 56% of the students being male. All classes but one have a 45-65% male population. Class 144 has 85% male students. The majority of the students in the dataset come from minority racial backgrounds (66%), with African-American students making up 42% of the total student population, and both Asian and Hispanic students making up 22%. This racial imbalance in the dataset presents a unique challenge and an interesting question to be answered: will pretraining on a racially diverse dataset improve the fairness and decrease the racial bias of the ASR system? All but one recording in the dataset was recorded by a near-field microphone and the recording of class 2619 comes from a far-field microphone. This distinction is by design, to test how well the ASR system generalizes to microphone configurations unseen in the training data and whether pretraining on different microphone set-ups improves this generalization.</p>
</div>
<div id="Sx4.SSx1.SSSx1.p5" class="ltx_para">
<p id="Sx4.SSx1.SSSx1.p5.1" class="ltx_p">An additional 4 classroom recordings were transcribed, and reserved for testing. In total, this subset is about 3.7 hours. The racial make-up of this subset is different than the one trained on, with white students representing roughly 31% of the population, African-American students representing 23%, and Asian students being the most represented group with 32% while being the least represented group in the training-validation subset, making up 65% of one class and 50% of another. Additionally, this subset differs from the train-validation subset in the presence of two male teachers, one of them being Asian. We use this dataset to answer the question: how well does the ASR system generalize to different make-ups than the ones seen in training? Does pretraining on a diverse dataset improve this generalization?</p>
</div>
</section>
<section id="Sx4.SSx1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">In house dataset</h4>

<div id="Sx4.SSx1.SSSx2.p1" class="ltx_para">
<p id="Sx4.SSx1.SSSx2.p1.1" class="ltx_p">We recorded 6 classrooms from 3 schools. We recorded two 8th-grade classes in a charter school in Washington, DC that serve predominantly African-American and Hispanic low-income students. Classes included at least one special education student and at least one English language learner. We refer to these recordings as <span id="Sx4.SSx1.SSSx2.p1.1.1" class="ltx_text ltx_font_bold">DC-1</span> and <span id="Sx4.SSx1.SSSx2.p1.1.2" class="ltx_text ltx_font_bold">DC-2</span>. Two 5th-grade classrooms were recorded in a school in Eastlake, Ohio, with predominantly White students. Classes included special education students. We refer to these recordings as <span id="Sx4.SSx1.SSSx2.p1.1.3" class="ltx_text ltx_font_bold">OH-1</span> and <span id="Sx4.SSx1.SSSx2.p1.1.4" class="ltx_text ltx_font_bold">OH-2</span>. The remaining two recordings came from 6th-grade classrooms at a private school in San Jose, California with predominantly White and Asian high-income students with no special education or English language learner students. We refer to these recordings as <span id="Sx4.SSx1.SSSx2.p1.1.5" class="ltx_text ltx_font_bold">CA-1</span> and <span id="Sx4.SSx1.SSSx2.p1.1.6" class="ltx_text ltx_font_bold">CA-2</span>. In each classroom, five microphones were placed at different places in the classroom and the audio streams were then added together. This resulted in a good capture of all the audio in the classroom but also extremely noisy audio in one particularly noisy classroom, CA-1. We keep this configuration to test the model’s ability to handle extremely noisy conditions. We use this dataset to test the effect of CPT on improving performance in slightly different but related domains than the one seen during CPT.</p>
</div>
</section>
<section id="Sx4.SSx1.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">NCTE-Text</h4>

<div id="Sx4.SSx1.SSSx3.p1" class="ltx_para">
<p id="Sx4.SSx1.SSSx3.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Demszky and Hill (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite> published a dataset of anonymized transcriptions of 1660 classrooms from the NCTE dataset for the purposes of developing computational measures of classroom discourse. These transcripts were not intended for ASR training and are not suitable for it, because all the teachers and students were anonymized to protect the subjects’ privacy, replacing every name with its initial. Additionally, the transcripts are not verbatim and do not capture the exact speech in the classroom word for word. Add to that that they were not properly time-stamped, making preprocessing very complicated. However, this text corpus still holds tremendous value for ASR.</p>
</div>
<div id="Sx4.SSx1.SSSx3.p2" class="ltx_para">
<p id="Sx4.SSx1.SSSx3.p2.1" class="ltx_p">One of the key differences between Wav2vec2.0 and Whisper is that Whisper has a built-in audio-conditional LM, while Wav2vec2.0 needs an external n-gram or transformer LM for beam-search. Even though end-to-end models are generally preferable, breaking down the ASR problem into an acoustic model (Wav2vec2.0) and an LM allows us to utilize unmappable representations like non-verbatim transcriptions better. In that sense, the NCTE-Text dataset was very useful for use in training a custom 5-gram language model for beam-search decoding with Wav2vec2.0</p>
</div>
<div id="Sx4.SSx1.SSSx3.p3" class="ltx_para">
<p id="Sx4.SSx1.SSSx3.p3.1" class="ltx_p">The corpus is de-identified, with teacher and student names replaced with initials such as ”Student K” and ”Mrs. D”. To make this data suitable for LM finetuning, we replaced the initials with randomly chosen names. To ensure that the names are unbiased towards race or gender, we referenced a list of the most popular baby names by race between 2011 and 2019 in New York City <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://data.cityofnewyork.us/Health/Popular-Baby-Names/25th-nujf/data</span></span></span></span>. For each classroom transcription and every initial, we uniformly sample gender, and then sampled a race uniformly from White, African-American, Hispanic, or Asian. After sampling the race and gender, we referred to the list of popular names of that gender and race based on its popularity according to the list. We then replaced all instances of that anonymized initial in that transcription. Race-aware deanonymization ensures that all races and both genders are equally represented in the text corpus. Our results show that race-aware deanonymization shows some improvement over naive gender-aware deanonymization which does not take race into account.</p>
</div>
</section>
</section>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experiments</h2>

<section id="Sx5.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">CPT Experiments</h3>

<div id="Sx5.SSx1.p1" class="ltx_para">
<p id="Sx5.SSx1.p1.1" class="ltx_p">We performed three CPT experiments in addition to contrast the effect of the starting checkpoint for CPT on domain adaptation. For the CPT experiments, we started from three 300M parameter checkpoints of Wav2vec2.0. The first was initially pretrained on 60K hours LibriVox <cite class="ltx_cite ltx_citemacro_citep">(Kearns <a href="#bib.bib22" title="" class="ltx_ref">2014</a>)</cite>, which we denote as <span id="Sx5.SSx1.p1.1.1" class="ltx_text ltx_font_bold">W2V-LV60</span>. The second model, <span id="Sx5.SSx1.p1.1.2" class="ltx_text ltx_font_bold">W2V-Robust</span> <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al. <a href="#bib.bib16" title="" class="ltx_ref">2021b</a>)</cite> was pretrained on LibriVox, as well as noisy English recordings. The third model is <span id="Sx5.SSx1.p1.1.3" class="ltx_text ltx_font_bold">XLS-R</span> 300M which was pretrained on 436K hours of speech from 128 languages, including English. The contrast between the performance of W2V-LV60 and W2V-Robust will showcase the effect of additional noisy data from a different domain during initial pretraining on the efficacy of CPT domain adaptation. The performance of XLS-R will showcase the impact of additional pretraining data from completely different domains and languages. We also pretrain a Wav2vec2.0 model from scratch to establish a baseline, which we denote as <span id="Sx5.SSx1.p1.1.4" class="ltx_text ltx_font_bold">W2V-SCR</span>, representing the baseline configuration from previous works.</p>
</div>
<div id="Sx5.SSx1.p2" class="ltx_para">
<p id="Sx5.SSx1.p2.1" class="ltx_p">We implemented our model using the official fairseq library <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://github.com/facebookresearch/fairseq</span></span></span></span>. Each of the models was pretrained for 1M steps on 3 NVIDIA-A100 GPUs using the configuration file obtained from the fairseq repository.</p>
</div>
</section>
<section id="Sx5.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Finetuning</h3>

<div id="Sx5.SSx2.p1" class="ltx_para">
<p id="Sx5.SSx2.p1.1" class="ltx_p">We perform two separate 6-fold cross-validation finetuning on each of the two datasets used for training, leaving one classroom recording for validation in each fold. For each fold, we train using a configuration file adapted from the 10-hour configuration file found on the fairseq repository. Each fold was trained on one NVIDIA-A6000 GPU using the fairseq library. We finetune each off-the-shelf Wav2vec2.0 model (W2V-LV60, W2V-Robust, and XLS-R), and their CPT counterparts as well as W2V-SCR. In total, we pretrain 4 models and finetune 84 models, 42 for NCTE and 42 for our in-house dataset.</p>
</div>
<div id="Sx5.SSx2.p2" class="ltx_para">
<p id="Sx5.SSx2.p2.1" class="ltx_p">We preprocess our data by cutting the audio into chunks of 30 seconds or less depending on the timestamps of the transcription. We also normalize the text using the WhipserNormalizer Library <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://pypi.org/project/whisper-normalizer/</span></span></span></span>.</p>
</div>
</section>
<section id="Sx5.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">N-Gram LM Training</h3>

<div id="Sx5.SSx3.p1" class="ltx_para">
<p id="Sx5.SSx3.p1.1" class="ltx_p">We train our 5-gram LM using the KenLM<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://github.com/kpu/kenlm</span></span></span></span> library. Our training data is the deanonymized NCTE-Text dataset as described above. We normalize the training data using the Whisper Normalizer library to match the transcription text. We use this LM for LM decoding of Wav2vec2.0 in all our experiments.</p>
</div>
<figure id="Sx5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Average cross-validation WER finetuning results starting from different Wav2vec2.0 checkpoints with and without continued pretraining, as well as comparison with the small English-only checkpoint of Whisper. Whisper-FT refers to finetuning Whisper on the target dataset. The standard deviation of the WER is between brackets. All models decoded with an n-gram LM use our LM trained on race-aware deanonymized NCTE-Text corpus.</figcaption>
<p id="Sx5.T3.1" class="ltx_p ltx_align_center"><span id="Sx5.T3.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="Sx5.T3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:296.2pt;height:378.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="Sx5.T3.1.1.1.1" class="ltx_p"><span id="Sx5.T3.1.1.1.1.1" class="ltx_text">
<span id="Sx5.T3.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="Sx5.T3.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></span>
<span id="Sx5.T3.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_border_t"></span>
<span id="Sx5.T3.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_2"><span id="Sx5.T3.1.1.1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">WER(STD)</span></span></span>
<span id="Sx5.T3.1.1.1.1.1.1.2.2" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T3.1.1.1.1.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Model</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center"><span id="Sx5.T3.1.1.1.1.1.1.2.2.2.1" class="ltx_text ltx_font_bold">LM</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx5.T3.1.1.1.1.1.1.2.2.3.1" class="ltx_text ltx_font_bold">NCTE</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx5.T3.1.1.1.1.1.1.2.2.4.1" class="ltx_text ltx_font_bold">In-house</span></span></span>
<span id="Sx5.T3.1.1.1.1.1.1.3.3" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T3.1.1.1.1.1.1.3.3.1.1" class="ltx_text ltx_font_bold">Whisper</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_tt">-</span>
<span id="Sx5.T3.1.1.1.1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">24.46(12.37)</span>
<span id="Sx5.T3.1.1.1.1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_tt">30.15(12.99)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.4.4" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T3.1.1.1.1.1.1.4.4.1.1" class="ltx_text ltx_font_bold">Whisper-FT</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.4.4.2" class="ltx_td ltx_align_center">-</span>
<span id="Sx5.T3.1.1.1.1.1.1.4.4.3" class="ltx_td ltx_align_center">19.14(6.77)</span>
<span id="Sx5.T3.1.1.1.1.1.1.4.4.4" class="ltx_td ltx_align_center">28.53(14.07)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.5.5" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt ltx_colspan ltx_colspan_4"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="Sx5.T3.1.1.1.1.1.1.5.5.1.1" class="ltx_text ltx_font_bold ltx_font_italic">No Continued Pretraining</span></span></span>
<span id="Sx5.T3.1.1.1.1.1.1.6.6" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_rowspan ltx_rowspan_2"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T3.1.1.1.1.1.1.6.6.1.1" class="ltx_text ltx_font_bold">W2V-LV60K</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.6.6.2" class="ltx_td ltx_align_center">None</span>
<span id="Sx5.T3.1.1.1.1.1.1.6.6.3" class="ltx_td ltx_align_center">39.11(13.01)</span>
<span id="Sx5.T3.1.1.1.1.1.1.6.6.4" class="ltx_td ltx_align_center">37.82(12.30)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.7.7" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.7.7.1" class="ltx_td ltx_align_center">5-gram LM</span>
<span id="Sx5.T3.1.1.1.1.1.1.7.7.2" class="ltx_td ltx_align_center">30.39(14.48)</span>
<span id="Sx5.T3.1.1.1.1.1.1.7.7.3" class="ltx_td ltx_align_center">33.56(10.86)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.8.8" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T3.1.1.1.1.1.1.8.8.1.1" class="ltx_text ltx_font_bold">XLS-R</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t">None</span>
<span id="Sx5.T3.1.1.1.1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t">38.19(10.39)</span>
<span id="Sx5.T3.1.1.1.1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t">39.12(13.60)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.9.9" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.9.9.1" class="ltx_td ltx_align_center">5-gram LM</span>
<span id="Sx5.T3.1.1.1.1.1.1.9.9.2" class="ltx_td ltx_align_center">29.02(10.96)</span>
<span id="Sx5.T3.1.1.1.1.1.1.9.9.3" class="ltx_td ltx_align_center">32.49(10.76)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.10.10" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T3.1.1.1.1.1.1.10.10.1.1" class="ltx_text ltx_font_bold">W2V-Robust</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_t">None</span>
<span id="Sx5.T3.1.1.1.1.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_t">35.07(11.85)</span>
<span id="Sx5.T3.1.1.1.1.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_t">36.36(11.54)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.11.11" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.11.11.1" class="ltx_td ltx_align_center">5-gram LM</span>
<span id="Sx5.T3.1.1.1.1.1.1.11.11.2" class="ltx_td ltx_align_center">27.99(13.28)</span>
<span id="Sx5.T3.1.1.1.1.1.1.11.11.3" class="ltx_td ltx_align_center">31.49(9.97)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.12.12" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt ltx_colspan ltx_colspan_4"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="Sx5.T3.1.1.1.1.1.1.12.12.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Pretraining from Scratch</span></span></span>
<span id="Sx5.T3.1.1.1.1.1.1.13.13" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.13.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_rowspan ltx_rowspan_2"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T3.1.1.1.1.1.1.13.13.1.1" class="ltx_text ltx_font_bold">W2V-SCR</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.13.13.2" class="ltx_td ltx_align_center">None</span>
<span id="Sx5.T3.1.1.1.1.1.1.13.13.3" class="ltx_td ltx_align_center">47.34(5.73)</span>
<span id="Sx5.T3.1.1.1.1.1.1.13.13.4" class="ltx_td ltx_align_center">51.39 (6.83)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.14.14" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.14.14.1" class="ltx_td ltx_align_center">5-gram LM</span>
<span id="Sx5.T3.1.1.1.1.1.1.14.14.2" class="ltx_td ltx_align_center">30.25(15.44)</span>
<span id="Sx5.T3.1.1.1.1.1.1.14.14.3" class="ltx_td ltx_align_center">38.59(12.93)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.15.15" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.15.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt ltx_colspan ltx_colspan_4"><span class="ltx_rule" style="width:0.0pt;height:12.9pt;background:black;display:inline-block;"></span><span id="Sx5.T3.1.1.1.1.1.1.15.15.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Continued Pretraining</span></span></span>
<span id="Sx5.T3.1.1.1.1.1.1.16.16" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.16.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_rowspan ltx_rowspan_2"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span> <span id="Sx5.T3.1.1.1.1.1.1.16.16.1.1" class="ltx_text ltx_font_bold">W2V-LV60K (CPT)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.16.16.2" class="ltx_td ltx_align_center">None</span>
<span id="Sx5.T3.1.1.1.1.1.1.16.16.3" class="ltx_td ltx_align_center">22.52(4.89)</span>
<span id="Sx5.T3.1.1.1.1.1.1.16.16.4" class="ltx_td ltx_align_center">32.26(8.92)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.17.17" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.17.17.1" class="ltx_td ltx_align_center">5-gram LM</span>
<span id="Sx5.T3.1.1.1.1.1.1.17.17.2" class="ltx_td ltx_align_center">18.13(5.50)</span>
<span id="Sx5.T3.1.1.1.1.1.1.17.17.3" class="ltx_td ltx_align_center">26.72(7.72)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.18.18" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.18.18.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span> <span id="Sx5.T3.1.1.1.1.1.1.18.18.1.1" class="ltx_text ltx_font_bold">XLS-R (CPT)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.18.18.2" class="ltx_td ltx_align_center ltx_border_t">None</span>
<span id="Sx5.T3.1.1.1.1.1.1.18.18.3" class="ltx_td ltx_align_center ltx_border_t">26.53(5.13)</span>
<span id="Sx5.T3.1.1.1.1.1.1.18.18.4" class="ltx_td ltx_align_center ltx_border_t">32.16(10.78)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.19.19" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.19.19.1" class="ltx_td ltx_align_center">5-gram LM</span>
<span id="Sx5.T3.1.1.1.1.1.1.19.19.2" class="ltx_td ltx_align_center">19.37(4.91)</span>
<span id="Sx5.T3.1.1.1.1.1.1.19.19.3" class="ltx_td ltx_align_center">26.80(8.00)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.20.20" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.20.20.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_2"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T3.1.1.1.1.1.1.20.20.1.1" class="ltx_text ltx_font_bold">W2V-Robust (CPT)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.20.20.2" class="ltx_td ltx_align_center ltx_border_t">None</span>
<span id="Sx5.T3.1.1.1.1.1.1.20.20.3" class="ltx_td ltx_align_center ltx_border_t">25.04(5.28)</span>
<span id="Sx5.T3.1.1.1.1.1.1.20.20.4" class="ltx_td ltx_align_center ltx_border_t">30.97(9.99)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.21.21" class="ltx_tr">
<span id="Sx5.T3.1.1.1.1.1.1.21.21.1" class="ltx_td ltx_align_center ltx_border_bb">5-gram LM</span>
<span id="Sx5.T3.1.1.1.1.1.1.21.21.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx5.T3.1.1.1.1.1.1.21.21.2.1" class="ltx_text ltx_font_bold">17.71(5.06)</span></span>
<span id="Sx5.T3.1.1.1.1.1.1.21.21.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx5.T3.1.1.1.1.1.1.21.21.3.1" class="ltx_text ltx_font_bold">26.50(8.09)</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</figure>
<figure id="Sx5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Detailed WER results on each fold of the cross-validation on both the NCTE and in-house datasets, for the off-the-shelf and the CPT version of W2V-Robust and the finetuned small English-only Whisper checkpoint.</figcaption>
<p id="Sx5.T4.1" class="ltx_p ltx_align_center"><span id="Sx5.T4.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="Sx5.T4.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:287.7pt;height:324pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="Sx5.T4.1.1.1.1" class="ltx_p"><span id="Sx5.T4.1.1.1.1.1" class="ltx_text">
<span id="Sx5.T4.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="Sx5.T4.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_colspan ltx_colspan_4"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">NCTE</span></span></span>
</span>
<span class="ltx_tbody">
<span id="Sx5.T4.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Fold</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx5.T4.1.1.1.1.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Whisper-FT</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx5.T4.1.1.1.1.1.1.2.1.3.1" class="ltx_text ltx_font_bold">W2V-Robust</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx5.T4.1.1.1.1.1.1.2.1.4.1" class="ltx_text ltx_font_bold">W2V-Robust (CPT)</span></span></span>
<span id="Sx5.T4.1.1.1.1.1.1.3.2" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.3.2.1.1" class="ltx_text ltx_font_bold">144</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx5.T4.1.1.1.1.1.1.3.2.2.1" class="ltx_text ltx_font_bold">14.78</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">19.86</span>
<span id="Sx5.T4.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">15.20</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.4.3" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.4.3.1.1" class="ltx_text ltx_font_bold">622</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_center"><span id="Sx5.T4.1.1.1.1.1.1.4.3.2.1" class="ltx_text ltx_font_bold">16.97</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_center">26.31</span>
<span id="Sx5.T4.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_center">18.77</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.5.4" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.5.4.1.1" class="ltx_text ltx_font_bold">2619</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.5.4.2" class="ltx_td ltx_align_center">28.4</span>
<span id="Sx5.T4.1.1.1.1.1.1.5.4.3" class="ltx_td ltx_align_center">54.03</span>
<span id="Sx5.T4.1.1.1.1.1.1.5.4.4" class="ltx_td ltx_align_center"><span id="Sx5.T4.1.1.1.1.1.1.5.4.4.1" class="ltx_text ltx_font_bold">27.15</span></span></span>
<span id="Sx5.T4.1.1.1.1.1.1.6.5" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.6.5.1.1" class="ltx_text ltx_font_bold">2709</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.6.5.2" class="ltx_td ltx_align_center"><span id="Sx5.T4.1.1.1.1.1.1.6.5.2.1" class="ltx_text ltx_font_bold">12.74</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.6.5.3" class="ltx_td ltx_align_center">19.09</span>
<span id="Sx5.T4.1.1.1.1.1.1.6.5.4" class="ltx_td ltx_align_center">13.12</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.7.6" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.7.6.1.1" class="ltx_text ltx_font_bold">2944</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.7.6.2" class="ltx_td ltx_align_center">26.98</span>
<span id="Sx5.T4.1.1.1.1.1.1.7.6.3" class="ltx_td ltx_align_center">28.07</span>
<span id="Sx5.T4.1.1.1.1.1.1.7.6.4" class="ltx_td ltx_align_center"><span id="Sx5.T4.1.1.1.1.1.1.7.6.4.1" class="ltx_text ltx_font_bold">17.61</span></span></span>
<span id="Sx5.T4.1.1.1.1.1.1.8.7" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.8.7.1.1" class="ltx_text ltx_font_bold">4724</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.8.7.2" class="ltx_td ltx_align_center">14.95</span>
<span id="Sx5.T4.1.1.1.1.1.1.8.7.3" class="ltx_td ltx_align_center">20.55</span>
<span id="Sx5.T4.1.1.1.1.1.1.8.7.4" class="ltx_td ltx_align_center"><span id="Sx5.T4.1.1.1.1.1.1.8.7.4.1" class="ltx_text ltx_font_bold">14.43</span></span></span>
<span id="Sx5.T4.1.1.1.1.1.1.9.8" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.9.8.1.1" class="ltx_text ltx_font_bold">Average</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_t">19.14</span>
<span id="Sx5.T4.1.1.1.1.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_t">27.99</span>
<span id="Sx5.T4.1.1.1.1.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx5.T4.1.1.1.1.1.1.9.8.4.1" class="ltx_text ltx_font_bold">17.71</span></span></span>
<span id="Sx5.T4.1.1.1.1.1.1.10.9" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt ltx_colspan ltx_colspan_4"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.10.9.1.1" class="ltx_text ltx_font_bold">In-House</span></span></span>
<span id="Sx5.T4.1.1.1.1.1.1.11.10" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.11.10.1.1" class="ltx_text ltx_font_bold">Fold</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.11.10.2" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx5.T4.1.1.1.1.1.1.11.10.2.1" class="ltx_text ltx_font_bold">Whisper-FT</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx5.T4.1.1.1.1.1.1.11.10.3.1" class="ltx_text ltx_font_bold">W2V-Robust</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx5.T4.1.1.1.1.1.1.11.10.4.1" class="ltx_text ltx_font_bold">W2V-Robust (CPT)</span></span></span>
<span id="Sx5.T4.1.1.1.1.1.1.12.11" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.12.11.1.1" class="ltx_text ltx_font_bold">OH-1</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.12.11.2" class="ltx_td ltx_align_center ltx_border_t">19.76</span>
<span id="Sx5.T4.1.1.1.1.1.1.12.11.3" class="ltx_td ltx_align_center ltx_border_t">18.55</span>
<span id="Sx5.T4.1.1.1.1.1.1.12.11.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx5.T4.1.1.1.1.1.1.12.11.4.1" class="ltx_text ltx_font_bold">15.42</span></span></span>
<span id="Sx5.T4.1.1.1.1.1.1.13.12" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.13.12.1.1" class="ltx_text ltx_font_bold">OH-2</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.13.12.2" class="ltx_td ltx_align_center"><span id="Sx5.T4.1.1.1.1.1.1.13.12.2.1" class="ltx_text ltx_font_bold">16.42</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.13.12.3" class="ltx_td ltx_align_center">19.89</span>
<span id="Sx5.T4.1.1.1.1.1.1.13.12.4" class="ltx_td ltx_align_center">16.88</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.14.13" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.14.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.14.13.1.1" class="ltx_text ltx_font_bold">DC-1</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.14.13.2" class="ltx_td ltx_align_center">28.79</span>
<span id="Sx5.T4.1.1.1.1.1.1.14.13.3" class="ltx_td ltx_align_center">29.42</span>
<span id="Sx5.T4.1.1.1.1.1.1.14.13.4" class="ltx_td ltx_align_center"><span id="Sx5.T4.1.1.1.1.1.1.14.13.4.1" class="ltx_text ltx_font_bold">24.90</span></span></span>
<span id="Sx5.T4.1.1.1.1.1.1.15.14" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.15.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.15.14.1.1" class="ltx_text ltx_font_bold">DC-2</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.15.14.2" class="ltx_td ltx_align_center"><span id="Sx5.T4.1.1.1.1.1.1.15.14.2.1" class="ltx_text ltx_font_bold">26.42</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.15.14.3" class="ltx_td ltx_align_center">37.32</span>
<span id="Sx5.T4.1.1.1.1.1.1.15.14.4" class="ltx_td ltx_align_center">30.34</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.16.15" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.16.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.16.15.1.1" class="ltx_text ltx_font_bold">CA-1</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.16.15.2" class="ltx_td ltx_align_center">55.77</span>
<span id="Sx5.T4.1.1.1.1.1.1.16.15.3" class="ltx_td ltx_align_center">49.03</span>
<span id="Sx5.T4.1.1.1.1.1.1.16.15.4" class="ltx_td ltx_align_center"><span id="Sx5.T4.1.1.1.1.1.1.16.15.4.1" class="ltx_text ltx_font_bold">41.54</span></span></span>
<span id="Sx5.T4.1.1.1.1.1.1.17.16" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.17.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.17.16.1.1" class="ltx_text ltx_font_bold">CA-2</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.17.16.2" class="ltx_td ltx_align_center"><span id="Sx5.T4.1.1.1.1.1.1.17.16.2.1" class="ltx_text ltx_font_bold">24.04</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.17.16.3" class="ltx_td ltx_align_center">34.47</span>
<span id="Sx5.T4.1.1.1.1.1.1.17.16.4" class="ltx_td ltx_align_center">29.91</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.18.17" class="ltx_tr">
<span id="Sx5.T4.1.1.1.1.1.1.18.17.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span class="ltx_rule" style="width:0.0pt;height:8.6pt;background:black;display:inline-block;"></span><span id="Sx5.T4.1.1.1.1.1.1.18.17.1.1" class="ltx_text ltx_font_bold">Average</span></span>
<span id="Sx5.T4.1.1.1.1.1.1.18.17.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">28.53</span>
<span id="Sx5.T4.1.1.1.1.1.1.18.17.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">31.45</span>
<span id="Sx5.T4.1.1.1.1.1.1.18.17.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx5.T4.1.1.1.1.1.1.18.17.4.1" class="ltx_text ltx_font_bold">26.50</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</figure>
<figure id="Sx5.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Test set WER results on the held-out files from the NCTE dataset not used in cross-validation training. Each entry is the average WER of all the cross-validation versions of a particular model when tested on a particular recording in the test set, with the standard deviation in brackets. Average shows the total average WER of each model.</figcaption>
<p id="Sx5.T5.1" class="ltx_p ltx_align_center"><span id="Sx5.T5.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="Sx5.T5.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:329.8pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="Sx5.T5.1.1.1.1" class="ltx_p"><span id="Sx5.T5.1.1.1.1.1" class="ltx_text">
<span id="Sx5.T5.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="Sx5.T5.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="Sx5.T5.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="Sx5.T5.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></span>
<span id="Sx5.T5.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="Sx5.T5.1.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Whisper</span></span>
<span id="Sx5.T5.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="Sx5.T5.1.1.1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Whisper-FT</span></span>
<span id="Sx5.T5.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="Sx5.T5.1.1.1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">W2V-Robust</span></span>
<span id="Sx5.T5.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="Sx5.T5.1.1.1.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">W2V-Robust(CPT)</span></span></span>
</span>
<span class="ltx_tbody">
<span id="Sx5.T5.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="Sx5.T5.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="Sx5.T5.1.1.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">13</span></span>
<span id="Sx5.T5.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">26.02</span>
<span id="Sx5.T5.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">25.89(1.70)</span>
<span id="Sx5.T5.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">25.55(0.66)</span>
<span id="Sx5.T5.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="Sx5.T5.1.1.1.1.1.1.2.1.5.1" class="ltx_text ltx_font_bold">25.07(0.73)</span></span></span>
<span id="Sx5.T5.1.1.1.1.1.1.3.2" class="ltx_tr">
<span id="Sx5.T5.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="Sx5.T5.1.1.1.1.1.1.3.2.1.1" class="ltx_text ltx_font_bold">4106</span></span>
<span id="Sx5.T5.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_center"><span id="Sx5.T5.1.1.1.1.1.1.3.2.2.1" class="ltx_text ltx_font_bold">25.10</span></span>
<span id="Sx5.T5.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_center">35.65(8.17)</span>
<span id="Sx5.T5.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_center">30.59(0.90)</span>
<span id="Sx5.T5.1.1.1.1.1.1.3.2.5" class="ltx_td ltx_align_center">25.79(0.26)</span></span>
<span id="Sx5.T5.1.1.1.1.1.1.4.3" class="ltx_tr">
<span id="Sx5.T5.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="Sx5.T5.1.1.1.1.1.1.4.3.1.1" class="ltx_text ltx_font_bold">4352</span></span>
<span id="Sx5.T5.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_center">22.27</span>
<span id="Sx5.T5.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_center">22.13(0.41)</span>
<span id="Sx5.T5.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_center">26.15(0.59)</span>
<span id="Sx5.T5.1.1.1.1.1.1.4.3.5" class="ltx_td ltx_align_center"><span id="Sx5.T5.1.1.1.1.1.1.4.3.5.1" class="ltx_text ltx_font_bold">22.30(0.26)</span></span></span>
<span id="Sx5.T5.1.1.1.1.1.1.5.4" class="ltx_tr">
<span id="Sx5.T5.1.1.1.1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="Sx5.T5.1.1.1.1.1.1.5.4.1.1" class="ltx_text ltx_font_bold">4651</span></span>
<span id="Sx5.T5.1.1.1.1.1.1.5.4.2" class="ltx_td ltx_align_center">50.74</span>
<span id="Sx5.T5.1.1.1.1.1.1.5.4.3" class="ltx_td ltx_align_center">39.82(6.88)</span>
<span id="Sx5.T5.1.1.1.1.1.1.5.4.4" class="ltx_td ltx_align_center">35.19(0.43)</span>
<span id="Sx5.T5.1.1.1.1.1.1.5.4.5" class="ltx_td ltx_align_center"><span id="Sx5.T5.1.1.1.1.1.1.5.4.5.1" class="ltx_text ltx_font_bold">29.47(0.40)</span></span></span>
<span id="Sx5.T5.1.1.1.1.1.1.6.5" class="ltx_tr">
<span id="Sx5.T5.1.1.1.1.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="Sx5.T5.1.1.1.1.1.1.6.5.1.1" class="ltx_text ltx_font_bold">Average</span></span>
<span id="Sx5.T5.1.1.1.1.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">31.03</span>
<span id="Sx5.T5.1.1.1.1.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">31.12</span>
<span id="Sx5.T5.1.1.1.1.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">30.12</span>
<span id="Sx5.T5.1.1.1.1.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="Sx5.T5.1.1.1.1.1.1.6.5.5.1" class="ltx_text ltx_font_bold">25.66</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</figure>
</section>
</section>
<section id="Sx6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Results and Discussion</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">In this section, we showcase our finetuning results of different Wav2vec2.0 checkpoints as well as the results from the small English-only Whisper checkpoint both off-the-shelf and finetuned in the leave-one-out cross-validation fashion described in the <span id="Sx6.p1.1.1" class="ltx_text ltx_font_bold">Experiments</span> section.</p>
</div>
<section id="Sx6.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Overall performance</h3>

<div id="Sx6.SSx1.p1" class="ltx_para">
<p id="Sx6.SSx1.p1.1" class="ltx_p">Table <a href="#Sx5.T3" title="Table 3 ‣ N-Gram LM Training ‣ Experiments ‣ Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the average cross-validation WER across all folds for every model. Without CPT, finetuning W2V-Robust yields superior performance. This shows that initial pretraining on noisy data improves the noise robustness to unseen noises in different domains. Additionally, XLS-R yields slightly better results than W2V-LV60K, which shows that training on much more data from other languages can be useful, especially since some of these recordings might be noisier than LV-60K. LM decoding improves results by 6% on average, which is in line with the results from the Appendix of the seminal Wav2vec2.0 paper <cite class="ltx_cite ltx_citemacro_citep">(Baevski et al. <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>. There is a slight performance gap between NCTE and In-house indicating that the NCTE task is slightly easier, which is in line with the noise levels observed upon manual inspection of the recordings. The standard deviation in the results is quite high, which shows that each recording has unique characteristics and thus the folds perform differently on each one, showing that although we are considering classroom recordings to be one domain, the kind of classroom environment be it collaborative learning or purely instructional affects the noise level and the amount of adult teacher’s speech versus students’ speech which affects the performance. One thing to note is that one of the files comes from a far-field microphone to test how well the ASR system generalizes to unseen microphone configuration which we expand on in Table <a href="#Sx5.T4" title="Table 4 ‣ N-Gram LM Training ‣ Experiments ‣ Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="Sx6.SSx1.p2" class="ltx_para">
<p id="Sx6.SSx1.p2.1" class="ltx_p">Looking at the third and fourth sections of Table <a href="#Sx5.T3" title="Table 3 ‣ N-Gram LM Training ‣ Experiments ‣ Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the advantage of CPT when compared to pretraining from scratch is immediately clear. Pretraining from scratch yields the worst performance in the entire table. Previous works <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al. <a href="#bib.bib46" title="" class="ltx_ref">2022</a>)</cite> have shown that pretraining on noisy data yields superior performance to pretraining on clean data, but in their tests, they pretrained both configurations on the same dataset but augmented the training data with noise for noisy pretraining. However, in our experiments, the clean pretraining data is at least 12 times bigger than the noisy pretraining data, which explains why clean self-supervised pretraining yields better performance in our experiments. One interesting thing to note is that the gap between the LM-decoded and the Viterbi-decoded results when pretraining from scratch is much higher than with other configurations. This suggests that the model does learn useful acoustical representations but does not learn enough linguistic properties during pretraining, which is accounted for by LM decoding. This result also suggests that initial pretraining on clean adult speech, even from other languages learns useful linguistic representations that are not sufficiently learned from noisy in-domain pretraining from scratch. Note the wide gap between the Viterbi decoded WER from W2V-LV60K(CPT) and W2V-SCR of almost 25%. In that regard, CPT has the benefit of utilizing linguistic representations from clean, and out-of-domain speech as well as learning the acoustic properties, and further learning useful linguistic properties from in-domain data.</p>
</div>
<div id="Sx6.SSx1.p3" class="ltx_para">
<p id="Sx6.SSx1.p3.1" class="ltx_p">Looking at CPT results in the fourth section of Table <a href="#Sx5.T3" title="Table 3 ‣ N-Gram LM Training ‣ Experiments ‣ Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we can see that for NCTE with LM decoding, CPT improves WER by 10.73% on average which proves that CPT is a powerful tool for domain adaptation when labeled data is scarce and unlabeled data is plentiful. For the In-house dataset where the domain is slightly different, CPT is still very effective, yielding a performance improvement of about 5% on average, proving that CPT is effective in domain adaptation in a generalizable fashion. It is also noticeable how the standard deviation decreases in both datasets, down to 5.06 in NCTE and 8.09 in the In-house dataset. This shows that performing CPT on diverse classroom environments and noise conditions improves the ability of the model to generalize to these conditions.</p>
</div>
<div id="Sx6.SSx1.p4" class="ltx_para">
<p id="Sx6.SSx1.p4.1" class="ltx_p">In terms of the choice of starting point for CPT, the order of performance with CPT does not follow the order observed in the performance without CPT. W2V-LV60K outperforms XLS-R with CPT, meaning that the performance edge XLS-R had from initially pretraining on more data from different languages does not carry forward with CPT, and initial pretraining on English-only datasets is always superior. This adds another dimension to the findings by <cite class="ltx_cite ltx_citemacro_citet">Hsu et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021b</a>)</cite> which showed that adding more pretraining data, even if out-of-domain improves performance. Our findings suggest that this does not extend to CPT. However, W2V-Robust still provides the best performance, showing that initial pretraining on noisy data even if from completely different domains yields better speech representations when adapted to other domains.</p>
</div>
<div id="Sx6.SSx1.p5" class="ltx_para">
<p id="Sx6.SSx1.p5.1" class="ltx_p">Finally, we can see that with CPT, W2V-Robust outperforms the Whisper checkpoint of comparable size, even with finetuning in both NCTE and In-house datasets. The nature of self-supervised speech models breaks down the problem into three parts: pretraining/CPT, finetuning, and LM decoding. This gives us more flexibility in situations when labeled data is scarce and expensive, but unlabeled speech data and text annotations from other domains are plentiful. Without CPT or LM decoding, Wav2vec2.0 performs much worse than Whisper, with WER being higher by 15-19% in the NCTE dataset when finetuned on the same data in the same manner. However, the flexibility that Wav2vec2.0 allows beyond supervised finetuning improved the performance by up to 19% through a combination of CPT and LM decoding.</p>
</div>
</section>
<section id="Sx6.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Detailed analysis of cross-validation results</h3>

<div id="Sx6.SSx2.p1" class="ltx_para">
<p id="Sx6.SSx2.p1.1" class="ltx_p">In this section, we do a more detailed analysis of the results. We start by discussing the results in Table <a href="#Sx5.T4" title="Table 4 ‣ N-Gram LM Training ‣ Experiments ‣ Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> which shows the WER of each fold of the cross-validation in Whisper-FT and W2V-Robust with and without CPT.</p>
</div>
<section id="Sx6.SSx2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">NCTE</h4>

<div id="Sx6.SSx2.SSSx1.p1" class="ltx_para">
<p id="Sx6.SSx2.SSSx1.p1.1" class="ltx_p">Looking at the results, it is apparent that CPT significantly improves the performance in each fold of the cross-validation with one notable example recording 2619. Starting with recording 2619, this recording is the only one that comes from a far-field microphone, with the entirety of the training data in this cross-validation fold coming from near-field microphones. It is thus no surprise that without CPT, the model performs much worse in this fold than the others. However, with CPT, the error is cut in half, proving that CPT helps the model generalize to microphone configurations unseen in the labeled data but present in the unlabeled pretraining data.</p>
</div>
<div id="Sx6.SSx2.SSSx1.p2" class="ltx_para">
<p id="Sx6.SSx2.SSSx1.p2.1" class="ltx_p">In terms of comparison with Whisper, looking at the NCTE results, CPT allows the model to achieve close performance or improve upon Whisper in every fold, with one major improvement noticed with recording 2944. Upon manual inspection, it was noted that this class started as an instructional class for 15 minutes and then the teacher assigned a set of questions to the students and started making rounds in the class. This resulted in a much noisier environment than other classrooms with a higher degree of children’s babble noise. Whisper was unable to deal with children’s babble noise, often interleaving the target speaker with whatever it could discern from the background noise and sometimes exhibiting characteristic Whisper hallucinations by repeating a single word or phrase, for example, <span id="Sx6.SSx2.SSSx1.p2.1.1" class="ltx_text ltx_font_italic">“how do you know that what what is the denominator what is the denominator the the the the the…”</span> with the word <span id="Sx6.SSx2.SSSx1.p2.1.2" class="ltx_text ltx_font_italic">”the”</span> repeating 206 times. Wav2vec does not suffer from the same hallucination problem, and with CPT, it’s much more capable of dealing with children’s babble noise and focusing on the target speaker correctly.</p>
</div>
</section>
<section id="Sx6.SSx2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">In-house dataset</h4>

<div id="Sx6.SSx2.SSSx2.p1" class="ltx_para">
<p id="Sx6.SSx2.SSSx2.p1.1" class="ltx_p">The differences in WER between Whisper and W2V-Robust with CPT on each fold are higher in the in-house dataset. W2V-Robust with CPT outperforms Whisper in 3 folds, with the main improvement coming from the recording of class CA-1. This recording is perhaps the noisiest of all the datasets used in this study, as discussed in the <span id="Sx6.SSx2.SSSx2.p1.1.1" class="ltx_text ltx_font_bold">Dataset</span> section. Both W2V-Robust and Whisper have high WER for this class recording, but even the non-CPT W2V-Robust outperformed Whisper. Upon manual inspection, we again see that Whisper suffers from extreme hallucinations with high children babble noise. On the other hand, W2V-Robust with CPT is more capable of handling extremely noisy conditions, outperforming Whisper in this fold by 14%.</p>
</div>
</section>
</section>
<section id="Sx6.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Performance on an unseen test set with different demographics</h3>

<div id="Sx6.SSx3.p1" class="ltx_para">
<p id="Sx6.SSx3.p1.1" class="ltx_p">In this section, we discuss the results from the unseen NCTE test set. As previously discussed, and according to Table <a href="#Sx4.T2" title="Table 2 ‣ NCTE ‣ Audio Datasets ‣ Datasets ‣ Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the test set has a different racial makeup with better representations of Asian students and the presence of two male teachers, one of them being Asian, while all the teachers in the train-validation subset are White women except for one African-American woman. This test shows how well the models generalize to different demographics, unseen during training.</p>
</div>
<div id="Sx6.SSx3.p2" class="ltx_para">
<p id="Sx6.SSx3.p2.1" class="ltx_p">From Table <a href="#Sx5.T5" title="Table 5 ‣ N-Gram LM Training ‣ Experiments ‣ Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, CPT improves the WER on average, from 30.12% to 25.66%, with the decreased standard deviation showing less variation in performance with different training data. This shows that CPT smoothes out the differences caused by different representations in the training data. When compared to Whisper, both the finetuned and non-finetuned versions of Whisper perform worse than both W2V-Robust models and finetuning seems to even harm the performance on average, indicating that Whisper overfits.</p>
</div>
<div id="Sx6.SSx3.p3" class="ltx_para">
<p id="Sx6.SSx3.p3.1" class="ltx_p">However, it’s important to note that even with CPT, the average WER with W2V-Robust is higher than the average cross-validation WER. Upon manual inspection, we noticed that the time stamps used to preprocess the audio into smaller chunks for testing had some inaccuracies. We attempted to account for that in our preprocessing, but we suspect that this issue still contributes to some extent to the gap in performance. However, our findings in comparing different models’ performance on the test set still stand. We are currently in the process of refining these timestamps for more accurate analysis to accurately pinpoint the cause of the performance gap.</p>
</div>
</section>
</section>
<section id="Sx7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Racial Bias in ASR systems</h2>

<div id="Sx7.p1" class="ltx_para">
<p id="Sx7.p1.1" class="ltx_p">Our analysis showed that ASR models might perform worse with minority teachers, in particular Asian and African American teachers. We note a pattern existing in both the validation and test results where the WER of classes with minority teachers is noticeably higher than other classes with similar or even higher noise levels and conditions.</p>
</div>
<div id="Sx7.p2" class="ltx_para">
<p id="Sx7.p2.1" class="ltx_p">In the training-validation dataset, we note that the performance of class 622 is worse than similar classes with the same noise conditions and levels. According to Table <a href="#Sx5.T4" title="Table 4 ‣ N-Gram LM Training ‣ Experiments ‣ Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, comparing class 622 to classes 144, 2709, and 4724, all of which have similar acoustic properties, we see that all three models presented in this table have higher WER for this particular file, with the worst offender being W2V-Robust without CPT. Our proposed CPT does narrow the gap and improves the performance by 8%, however the gap still exists. By referencing Table <a href="#Sx4.T2" title="Table 2 ‣ NCTE ‣ Audio Datasets ‣ Datasets ‣ Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we see that this class is the only class with an African-American teacher and has the highest percentage of Hispanic students and the second-lowest number of students in the dataset.</p>
</div>
<div id="Sx7.p3" class="ltx_para">
<p id="Sx7.p3.1" class="ltx_p">Looking at the test set results in Table <a href="#Sx5.T5" title="Table 5 ‣ N-Gram LM Training ‣ Experiments ‣ Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we see a similar pattern. The best performing class with all the models is class 4352 where the teacher is a White man. We note again the presence of some inaccuracies in the test-set transcriptions resulting from inaccurate timestamps, however, these inaccuracies are common and constant between all four classes, so the comparison between them still stands. Classes 4106 and 4651 score worse with all models, with the narrowest gap being with our W2V-Robust (CPT) model, about 10%. By referencing Table <a href="#Sx4.T2" title="Table 2 ‣ NCTE ‣ Audio Datasets ‣ Datasets ‣ Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we see that class 4106 has a female African-American teacher, with the majority of the students being Asian. Class 4651 also had a female African-American teacher with a majority of White students. Class 13 also scores lower than class 4352, where the teacher is an Asian man and upon manual inspection, we note the presence of a light accent. However, his speech is perfectly enunciated and clear. Upon further inspection of all the files, we find that these classes are not noisier than the 4352. The teacher in class 4352 wore a lanyard microphone which interfered with his shirt buttons and caused a clicking sound throughout the recording. Class 4352 also has the highest number of students in class. All of these findings suggest that the ASR system should perform <span id="Sx7.p3.1.1" class="ltx_text ltx_font_italic">worse</span> and not <span id="Sx7.p3.1.2" class="ltx_text ltx_font_italic">better</span> in class 4352, but they don’t.</p>
</div>
<div id="Sx7.p4" class="ltx_para">
<p id="Sx7.p4.1" class="ltx_p">Our findings suggest that ASR systems, our proposed systems included, are biased against minority teachers. Our findings are in line with previous research <cite class="ltx_cite ltx_citemacro_citep">(Martin and Wright <a href="#bib.bib29" title="" class="ltx_ref">2023</a>; Jain et al. <a href="#bib.bib20" title="" class="ltx_ref">2024</a>)</cite> which indicates that ASR systems are biased against African-American Language (AAL), as well as accented speech. These findings highlight the urgent need for better representations in training datasets, as this disparity in performance has been attributed to the absence of recordings from diverse racial groups and dialects in popular speech corpora. For example, in the TIMIT <cite class="ltx_cite ltx_citemacro_citep">(Garofolo <a href="#bib.bib13" title="" class="ltx_ref">1993</a>)</cite> dataset, out of 630 speakers, only 4 are African-American and 538 are White. This in part explains the bias found in ASR models, although more research is needed.</p>
</div>
</section>
<section id="Sx8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Conclusions and Future Work</h2>

<div id="Sx8.p1" class="ltx_para">
<p id="Sx8.p1.1" class="ltx_p">We have demonstrated how CPT is the best way to adapt Wav2vec2.0 models to different domains. By performing CPT on 5000 hours of classroom recordings, we’ve improved the model’s ability to generalize to (1) different noise conditions, (2) different microphones, and (3) different demographics than those existing in the labeled training data. We’ve shown that CPT can improve the WER on noisy classroom data by 10% on average and up to 27% in specific conditions. Our results suggest that CPT should be the baseline for Wav2vec2.0 domain adaptation experiments, especially in noise robustness applications as it is far superior to pretraining from scratch. We also propose a race-aware deanonymized classroom text dataset for LM training.</p>
</div>
<div id="Sx8.p2" class="ltx_para">
<p id="Sx8.p2.1" class="ltx_p">There is an urgent need for more balanced and fair labeled classroom datasets. To that end, we are developing tools to sample recordings from the unlabeled NCTE dataset in a way to ensures balanced demographics and fair representation. We are also working towards a larger CPT trial, with an additional 15K hours of unlabeled classroom recordings.</p>
</div>
<div id="Sx8.p3" class="ltx_para">
<p id="Sx8.p3.1" class="ltx_p">Lastly, we plan to expand on the work of <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al. <a href="#bib.bib46" title="" class="ltx_ref">2022</a>)</cite> of speech enhancement-based Wav2vec2.0 pretraining. We are working on simulating classroom noises to create a classroom noise dataset that can augment clean speech for use in such tools.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Attia et al. (2023)</span>
<span class="ltx_bibblock">
Attia, A. A.; Liu, J.; Ai, W.; Demszky, D.; and Espy-Wilson, C. 2023.

</span>
<span class="ltx_bibblock">Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.07927</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babu et al. (2021)</span>
<span class="ltx_bibblock">
Babu, A.; Wang, C.; Tjandra, A.; Lakhotia, K.; Xu, Q.; Goyal, N.; Singh, K.; von Platen, P.; Saraf, Y.; Pino, J.; et al. 2021.

</span>
<span class="ltx_bibblock">XLS-R: Self-supervised cross-lingual speech representation learning at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.09296</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. (2020)</span>
<span class="ltx_bibblock">
Baevski, A.; Zhou, Y.; Mohamed, A.; and Auli, M. 2020.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33: 12449–12460.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrett et al. (2021)</span>
<span class="ltx_bibblock">
Barrett, N.; McEachin, A.; Mills, J. N.; and Valant, J. 2021.

</span>
<span class="ltx_bibblock">Disparities and discrimination in student discipline by race and family income.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Journal of Human Resources</em>, 56(3): 711–748.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2019)</span>
<span class="ltx_bibblock">
Chang, X.; Qian, Y.; Yu, K.; and Watanabe, S. 2019.

</span>
<span class="ltx_bibblock">End-to-end monaural multi-speaker ASR system without pretraining.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 6256–6260. IEEE.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2020)</span>
<span class="ltx_bibblock">
Chang, X.; Zhang, W.; Qian, Y.; Le Roux, J.; and Watanabe, S. 2020.

</span>
<span class="ltx_bibblock">End-to-end multi-speaker speech recognition with transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 6134–6138. IEEE.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Conneau, A.; Baevski, A.; Collobert, R.; Mohamed, A.; and Auli, M. 2020.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning for speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.13979</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Darling-Hammond (2004)</span>
<span class="ltx_bibblock">
Darling-Hammond, L. 2004.

</span>
<span class="ltx_bibblock">Inequality and the right to learn: Access to qualified teachers in California’s public schools.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Teachers College Record</em>, 106(10): 1936–1966.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Demszky and Hill (2022)</span>
<span class="ltx_bibblock">
Demszky, D.; and Hill, H. 2022.

</span>
<span class="ltx_bibblock">The NCTE transcripts: A dataset of elementary math classroom transcripts.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.11772</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Demszky and Liu (2023a)</span>
<span class="ltx_bibblock">
Demszky, D.; and Liu, J. 2023a.

</span>
<span class="ltx_bibblock">M-powering teachers: natural language processing powered feedback improves 1:1 instruction and student outcomes.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ACM Conference on Learning</em>, 59–69. Copenhagen Denmark: ACM.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Demszky and Liu (2023b)</span>
<span class="ltx_bibblock">
Demszky, D.; and Liu, J. 2023b.

</span>
<span class="ltx_bibblock">M-Powering Teachers: Natural Language Processing Powered Feedback Improves 1:1 Instruction and Student Outcomes.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">L@S ’23: Proceedings of the Tenth ACM Conference on Learning @ Scale</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Demszky et al. (2023)</span>
<span class="ltx_bibblock">
Demszky, D.; Liu, J.; Hill, H. C.; Sanghi, S.; and Chung, A. 2023.

</span>
<span class="ltx_bibblock">Improving Teachers’ Questioning Quality through Automated Feedback: A Mixed-Methods Randomized Controlled Trial in Brick-and-Mortar Classrooms.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">EdWorkingPapers</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garofolo (1993)</span>
<span class="ltx_bibblock">
Garofolo, J. S. 1993.

</span>
<span class="ltx_bibblock">Timit acoustic phonetic continuous speech corpus.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Linguistic Data Consortium, 1993</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerosa et al. (2009)</span>
<span class="ltx_bibblock">
Gerosa, M.; Giuliani, D.; Narayanan, S.; and Potamianos, A. 2009.

</span>
<span class="ltx_bibblock">A review of ASR technologies for children’s speech.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2nd Workshop on Child, Computer and Interaction</em>, 1–8.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2021a)</span>
<span class="ltx_bibblock">
Hsu, W.-N.; Bolte, B.; Tsai, Y.-H. H.; Lakhotia, K.; Salakhutdinov, R.; and Mohamed, A. 2021a.

</span>
<span class="ltx_bibblock">Hubert: Self-supervised speech representation learning by masked prediction of hidden units.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 29: 3451–3460.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2021b)</span>
<span class="ltx_bibblock">
Hsu, W.-N.; Sriram, A.; Baevski, A.; Likhomanenko, T.; Xu, Q.; Pratap, V.; Kahn, J.; Lee, A.; Collobert, R.; Synnaeve, G.; et al. 2021b.

</span>
<span class="ltx_bibblock">Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.01027</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacobs et al. (2024)</span>
<span class="ltx_bibblock">
Jacobs, J.; Scornavacco, K.; Clevenger, C.; Suresh, A.; and Sumner, T. 2024.

</span>
<span class="ltx_bibblock">Automated feedback on discourse moves: teachers’ perceived utility of a professional learning tool.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Educational technology research and development</em>, 1–23.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacobs et al. (2022)</span>
<span class="ltx_bibblock">
Jacobs, J.; Scornavacco, K.; Harty, C.; Suresh, A.; Lai, V.; and Sumner, T. 2022.

</span>
<span class="ltx_bibblock">Promoting rich discussions in mathematics classrooms: Using personalized, automated feedback to support reflection and instructional change.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Teaching and Teacher Education</em>, 112: 103631.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. (2023)</span>
<span class="ltx_bibblock">
Jain, R.; Barcovschi, A.; Yiwere, M.; Corcoran, P.; and Cucu, H. 2023.

</span>
<span class="ltx_bibblock">Adaptation of Whisper models to child speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.13008</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. (2024)</span>
<span class="ltx_bibblock">
Jain, R.; Barcovschi, A.; Yiwere, M. Y.; Corcoran, P.; and Cucu, H. 2024.

</span>
<span class="ltx_bibblock">Exploring Native and Non-Native English Child Speech Recognition With Whisper.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 12: 41601–41610.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kane, Hill, and Staiger (2022)</span>
<span class="ltx_bibblock">
Kane, T.; Hill, H.; and Staiger, D. 2022.

</span>
<span class="ltx_bibblock">National Center for Teacher Effectiveness Main Study.

</span>
<span class="ltx_bibblock">Inter-university Consortium for Political and Social Research [distributor].

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kearns (2014)</span>
<span class="ltx_bibblock">
Kearns, J. 2014.

</span>
<span class="ltx_bibblock">Librivox: Free public domain audiobooks.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Reference Reviews</em>, 28(1): 7–8.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koenig and Lucero (2008)</span>
<span class="ltx_bibblock">
Koenig, L. L.; and Lucero, J. C. 2008.

</span>
<span class="ltx_bibblock">Stop consonant voicing and intraoral pressure contours in women and children.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, 123(2): 1077–1088.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koenig, Lucero, and Perlman (2008)</span>
<span class="ltx_bibblock">
Koenig, L. L.; Lucero, J. C.; and Perlman, E. 2008.

</span>
<span class="ltx_bibblock">Speech production variability in fricatives of children and adults: Results of functional data analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, 124(5): 3158–3170.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kraft, Blazar, and Hogan (2018)</span>
<span class="ltx_bibblock">
Kraft, M. A.; Blazar, D.; and Hogan, D. 2018.

</span>
<span class="ltx_bibblock">The effect of teacher coaching on instruction and achievement: A meta-analysis of the causal evidence.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Review of educational research</em>, 88(4): 547–588.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee, Potamianos, and Narayanan (1997)</span>
<span class="ltx_bibblock">
Lee, S.; Potamianos, A.; and Narayanan, S. 1997.

</span>
<span class="ltx_bibblock">Analysis of children’s speech: Duration, pitch and formants.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Fifth European Conference on Speech Communication and Technology</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee, Potamianos, and Narayanan (1999)</span>
<span class="ltx_bibblock">
Lee, S.; Potamianos, A.; and Narayanan, S. 1999.

</span>
<span class="ltx_bibblock">Acoustics of children’s speech: Developmental changes of temporal and spectral parameters.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, 105(3): 1455–1468.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Link and Guskey (2022)</span>
<span class="ltx_bibblock">
Link, L.; and Guskey, T. 2022.

</span>
<span class="ltx_bibblock">Grading and assessment survey.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martin and Wright (2023)</span>
<span class="ltx_bibblock">
Martin, J. L.; and Wright, K. E. 2023.

</span>
<span class="ltx_bibblock">Bias in automatic speech recognition: The case of African American Language.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Applied Linguistics</em>, 44(4): 613–630.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morrison, Annamma, and Jackson (2023)</span>
<span class="ltx_bibblock">
Morrison, D.; Annamma, S. A.; and Jackson, D. D. 2023.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Critical race spatial analysis: Mapping to understand and address educational inequity</em>.

</span>
<span class="ltx_bibblock">Taylor &amp; Francis.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NCES (2018a)</span>
<span class="ltx_bibblock">
NCES. 2018a.

</span>
<span class="ltx_bibblock">English Learners in Public Schools.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">National Center for Education Statistics (NCES) Home Page, a part of the U.S. Department of Education</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NCES (2018b)</span>
<span class="ltx_bibblock">
NCES. 2018b.

</span>
<span class="ltx_bibblock">National Teacher and Principal Survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">National Center for Education Statistics (NCES) Home Page, a part of the U.S. Department of Education</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nowakowski et al. (2023)</span>
<span class="ltx_bibblock">
Nowakowski, K.; Ptaszynski, M.; Murasaki, K.; and Nieuważny, J. 2023.

</span>
<span class="ltx_bibblock">Adapting multilingual speech representation model for a new, underresourced language through multilingual fine-tuning and continued pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Information Processing &amp; Management</em>, 60(2): 103148.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2023)</span>
<span class="ltx_bibblock">
Radford, A.; Kim, J. W.; Xu, T.; Brockman, G.; McLeavey, C.; and Sutskever, I. 2023.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 28492–28518. PMLR.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reardon (2018)</span>
<span class="ltx_bibblock">
Reardon, S. F. 2018.

</span>
<span class="ltx_bibblock">The widening academic achievement gap between the rich and the poor.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Social stratification</em>, 536–550. Routledge.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">San et al. (2024)</span>
<span class="ltx_bibblock">
San, N.; Paraskevopoulos, G.; Arora, A.; He, X.; Kaur, P.; Adams, O.; and Jurafsky, D. 2024.

</span>
<span class="ltx_bibblock">Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.02302</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schneider et al. (2019)</span>
<span class="ltx_bibblock">
Schneider, S.; Baevski, A.; Collobert, R.; and Auli, M. 2019.

</span>
<span class="ltx_bibblock">wav2vec: Unsupervised pre-training for speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.05862</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shahnawazuddin et al. (2024)</span>
<span class="ltx_bibblock">
Shahnawazuddin, S.; et al. 2024.

</span>
<span class="ltx_bibblock">Developing children’s ASR system under low-resource conditions using end-to-end architecture.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Digital Signal Processing</em>, 146: 104385.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simic and Bocklet (2024)</span>
<span class="ltx_bibblock">
Simic, C.; and Bocklet, T. 2024.

</span>
<span class="ltx_bibblock">Self-Supervised Adaptive AV Fusion Module for Pre-Trained ASR Models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 12787–12791. IEEE.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith (1992)</span>
<span class="ltx_bibblock">
Smith, B. L. 1992.

</span>
<span class="ltx_bibblock">Relationships between duration and temporal variability in children’s speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, 91(4): 2165–2174.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Southwell et al. (2022)</span>
<span class="ltx_bibblock">
Southwell, R.; Pugh, S.; Perkoff, E. M.; Clevenger, C.; Bush, J. B.; Lieber, R.; Ward, W.; Foltz, P.; and D’Mello, S. 2022.

</span>
<span class="ltx_bibblock">Challenges and Feasibility of Automatic Speech Recognition for Modeling Student Collaborative Discourse in Classrooms.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">International Educational Data Mining Society</em>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Southwell et al. (2024)</span>
<span class="ltx_bibblock">
Southwell, R.; Ward, W.; Trinh, V. A.; Clevenger, C.; Clevenger, C.; Watts, E.; Reitman, J.; D’Mello, S.; and Whitehill, J. 2024.

</span>
<span class="ltx_bibblock">Automatic Speech Recognition Tuned for Child Speech in the Classroom.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 12291–12295. IEEE.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vorperian and Kent (2007)</span>
<span class="ltx_bibblock">
Vorperian, H. K.; and Kent, R. D. 2007.

</span>
<span class="ltx_bibblock">Vowel acoustic space development in children: A synthesis of acoustic and anatomic data.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie, Fang, and Shauman (2015)</span>
<span class="ltx_bibblock">
Xie, Y.; Fang, M.; and Shauman, K. 2015.

</span>
<span class="ltx_bibblock">STEM education.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Annual review of sociology</em>, 41: 331–357.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2024)</span>
<span class="ltx_bibblock">
Zhu, Q.; Zhang, J.; Gu, Y.; Hu, Y.; and Dai, L. 2024.

</span>
<span class="ltx_bibblock">Multichannel AV-wav2vec2: A Framework for Learning Multichannel Multi-Modal Speech Representation.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.03468</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2022)</span>
<span class="ltx_bibblock">
Zhu, Q.-S.; Zhang, J.; Zhang, Z.-Q.; Wu, M.-H.; Fang, X.; and Dai, L.-R. 2022.

</span>
<span class="ltx_bibblock">A noise-robust self-supervised pre-training model based speech representation learning for automatic speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 3174–3178. IEEE.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix: Full Results Table</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">We provide the full cross-validation results table from the finetuning all the models mentioned in the manscript.</p>
</div>
<figure id="A1.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Full results from cross-validation finetuning off all finetuned Wav2vec2.0 models, both with and without CPT, as well as Whisper-small.en, both finetuned and not off-the-shelf.</figcaption>
<p id="A1.T6.1" class="ltx_p ltx_align_center"><span id="A1.T6.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="A1.T6.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:1106.6pt;height:324pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A1.T6.1.1.1.1" class="ltx_p"><span id="A1.T6.1.1.1.1.1" class="ltx_text">
<span id="A1.T6.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="A1.T6.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="A1.T6.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Fine-tuning data</span></span>
<span id="A1.T6.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_16"><span id="A1.T6.1.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">NCTE</span></span></span>
<span id="A1.T6.1.1.1.1.1.1.2.2" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="A1.T6.1.1.1.1.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Model</span></span>
<span id="A1.T6.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Whisper</span>
<span id="A1.T6.1.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r ltx_border_t">Whisper-FT</span>
<span id="A1.T6.1.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t ltx_colspan ltx_colspan_2">W2V-SCR</span>
<span id="A1.T6.1.1.1.1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t ltx_colspan ltx_colspan_2">W2V-LV60K</span>
<span id="A1.T6.1.1.1.1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t ltx_colspan ltx_colspan_2">W2V-LV60K (CPT)</span>
<span id="A1.T6.1.1.1.1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t ltx_colspan ltx_colspan_2">XLS-R</span>
<span id="A1.T6.1.1.1.1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t ltx_colspan ltx_colspan_2">XLS-R (CPT)</span>
<span id="A1.T6.1.1.1.1.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t ltx_colspan ltx_colspan_2">W2V-Robust</span>
<span id="A1.T6.1.1.1.1.1.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_2">W2V-Robust (CPT)</span></span>
<span id="A1.T6.1.1.1.1.1.1.3.3" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="A1.T6.1.1.1.1.1.1.3.3.1.1" class="ltx_text ltx_font_bold">LM</span></span>
<span id="A1.T6.1.1.1.1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T6.1.1.1.1.1.1.3.3.2.1" class="ltx_text ltx_font_bold">-</span></span>
<span id="A1.T6.1.1.1.1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r ltx_border_t">-</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">None</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">5-g LM</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">None</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">5-g LM</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">None</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">5-g LM</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">None</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">5-g LM</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">None</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.13" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">5-g LM</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">None</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.15" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">5-g LM</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">None</span>
<span id="A1.T6.1.1.1.1.1.1.3.3.17" class="ltx_td ltx_align_center ltx_border_t">5-g LM</span></span>
<span id="A1.T6.1.1.1.1.1.1.4.4" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="A1.T6.1.1.1.1.1.1.4.4.1.1" class="ltx_text ltx_font_bold">144</span></span>
<span id="A1.T6.1.1.1.1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.38</span>
<span id="A1.T6.1.1.1.1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r ltx_border_t"><span id="A1.T6.1.1.1.1.1.1.4.4.3.1" class="ltx_text ltx_font_bold">14.78</span></span>
<span id="A1.T6.1.1.1.1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">43.31</span>
<span id="A1.T6.1.1.1.1.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="A1.T6.1.1.1.1.1.1.4.4.5.1" class="ltx_text ltx_font_bold">27.85</span></span>
<span id="A1.T6.1.1.1.1.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.01</span>
<span id="A1.T6.1.1.1.1.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">21.75</span>
<span id="A1.T6.1.1.1.1.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.41</span>
<span id="A1.T6.1.1.1.1.1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">14.78</span>
<span id="A1.T6.1.1.1.1.1.1.4.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.12</span>
<span id="A1.T6.1.1.1.1.1.1.4.4.11" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">22.31</span>
<span id="A1.T6.1.1.1.1.1.1.4.4.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.57</span>
<span id="A1.T6.1.1.1.1.1.1.4.4.13" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">15.51</span>
<span id="A1.T6.1.1.1.1.1.1.4.4.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.81</span>
<span id="A1.T6.1.1.1.1.1.1.4.4.15" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">19.86</span>
<span id="A1.T6.1.1.1.1.1.1.4.4.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.48</span>
<span id="A1.T6.1.1.1.1.1.1.4.4.17" class="ltx_td ltx_align_center ltx_border_t">15.20</span></span>
<span id="A1.T6.1.1.1.1.1.1.5.5" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.5.5.1.1" class="ltx_text ltx_font_bold">622</span></span>
<span id="A1.T6.1.1.1.1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r">21.43</span>
<span id="A1.T6.1.1.1.1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r"><span id="A1.T6.1.1.1.1.1.1.5.5.3.1" class="ltx_text ltx_font_bold">16.97</span></span>
<span id="A1.T6.1.1.1.1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r">47.80</span>
<span id="A1.T6.1.1.1.1.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.5.5.5.1" class="ltx_text ltx_font_bold">30.55</span></span>
<span id="A1.T6.1.1.1.1.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r">36.08</span>
<span id="A1.T6.1.1.1.1.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_rr">27.66</span>
<span id="A1.T6.1.1.1.1.1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r">26.53</span>
<span id="A1.T6.1.1.1.1.1.1.5.5.9" class="ltx_td ltx_align_center ltx_border_rr">19.13</span>
<span id="A1.T6.1.1.1.1.1.1.5.5.10" class="ltx_td ltx_align_center ltx_border_r">36.72</span>
<span id="A1.T6.1.1.1.1.1.1.5.5.11" class="ltx_td ltx_align_center ltx_border_rr">28.09</span>
<span id="A1.T6.1.1.1.1.1.1.5.5.12" class="ltx_td ltx_align_center ltx_border_r">29.32</span>
<span id="A1.T6.1.1.1.1.1.1.5.5.13" class="ltx_td ltx_align_center ltx_border_rr">21.46</span>
<span id="A1.T6.1.1.1.1.1.1.5.5.14" class="ltx_td ltx_align_center ltx_border_r">33.22</span>
<span id="A1.T6.1.1.1.1.1.1.5.5.15" class="ltx_td ltx_align_center ltx_border_rr">26.31</span>
<span id="A1.T6.1.1.1.1.1.1.5.5.16" class="ltx_td ltx_align_center ltx_border_r">26.79</span>
<span id="A1.T6.1.1.1.1.1.1.5.5.17" class="ltx_td ltx_align_center">18.77</span></span>
<span id="A1.T6.1.1.1.1.1.1.6.6" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.6.6.1.1" class="ltx_text ltx_font_bold">2619</span></span>
<span id="A1.T6.1.1.1.1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">47.6</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r">28.4</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r">57.75</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.6.6.5.1" class="ltx_text ltx_font_bold">42.86</span></span>
<span id="A1.T6.1.1.1.1.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r">62.86</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_rr">58.48</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r">34.58</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_rr">28.39</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.10" class="ltx_td ltx_align_center ltx_border_r">57.77</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.11" class="ltx_td ltx_align_center ltx_border_rr">50.50</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.12" class="ltx_td ltx_align_center ltx_border_r">35.47</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.13" class="ltx_td ltx_align_center ltx_border_rr">28.04</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.14" class="ltx_td ltx_align_center ltx_border_r">57.58</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.15" class="ltx_td ltx_align_center ltx_border_rr">54.03</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.16" class="ltx_td ltx_align_center ltx_border_r">34.58</span>
<span id="A1.T6.1.1.1.1.1.1.6.6.17" class="ltx_td ltx_align_center"><span id="A1.T6.1.1.1.1.1.1.6.6.17.1" class="ltx_text ltx_font_bold">27.15</span></span></span>
<span id="A1.T6.1.1.1.1.1.1.7.7" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.7.7.1.1" class="ltx_text ltx_font_bold">2709</span></span>
<span id="A1.T6.1.1.1.1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r">14.34</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r"><span id="A1.T6.1.1.1.1.1.1.7.7.3.1" class="ltx_text ltx_font_bold">12.74</span></span>
<span id="A1.T6.1.1.1.1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">41.38</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_rr">22.84</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r">30.80</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_rr">20.60</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r">21.02</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_rr">13.03</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_r">30.68</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_rr">21.17</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.12" class="ltx_td ltx_align_center ltx_border_r">22.32</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.13" class="ltx_td ltx_align_center ltx_border_rr">14.75</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.14" class="ltx_td ltx_align_center ltx_border_r">27.24</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.15" class="ltx_td ltx_align_center ltx_border_rr">19.09</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.16" class="ltx_td ltx_align_center ltx_border_r">20.10</span>
<span id="A1.T6.1.1.1.1.1.1.7.7.17" class="ltx_td ltx_align_center">13.12</span></span>
<span id="A1.T6.1.1.1.1.1.1.8.8" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.8.8.1.1" class="ltx_text ltx_font_bold">2944</span></span>
<span id="A1.T6.1.1.1.1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r">27.98</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r">26.98</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r">48.20</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_rr">26.64</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r">45.02</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_rr">32.22</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r">25.65</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_rr">18.05</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.10" class="ltx_td ltx_align_center ltx_border_r">40.99</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.11" class="ltx_td ltx_align_center ltx_border_rr">28.56</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.12" class="ltx_td ltx_align_center ltx_border_r">26.20</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.13" class="ltx_td ltx_align_center ltx_border_rr">19.31</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.14" class="ltx_td ltx_align_center ltx_border_r">37.87</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.15" class="ltx_td ltx_align_center ltx_border_rr">28.07</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.16" class="ltx_td ltx_align_center ltx_border_r">25.17</span>
<span id="A1.T6.1.1.1.1.1.1.8.8.17" class="ltx_td ltx_align_center"><span id="A1.T6.1.1.1.1.1.1.8.8.17.1" class="ltx_text ltx_font_bold">17.61</span></span></span>
<span id="A1.T6.1.1.1.1.1.1.9.9" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.9.9.1.1" class="ltx_text ltx_font_bold">4724</span></span>
<span id="A1.T6.1.1.1.1.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r">15</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r">14.95</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r">45.60</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_rr">30.74</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r">28.91</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_rr">21.60</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r">22.94</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.9" class="ltx_td ltx_align_center ltx_border_rr">15.40</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.10" class="ltx_td ltx_align_center ltx_border_r">30.86</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.11" class="ltx_td ltx_align_center ltx_border_rr">23.46</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.12" class="ltx_td ltx_align_center ltx_border_r">23.29</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.13" class="ltx_td ltx_align_center ltx_border_rr">17.17</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.14" class="ltx_td ltx_align_center ltx_border_r">27.70</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.15" class="ltx_td ltx_align_center ltx_border_rr">20.55</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.16" class="ltx_td ltx_align_center ltx_border_r">22.14</span>
<span id="A1.T6.1.1.1.1.1.1.9.9.17" class="ltx_td ltx_align_center"><span id="A1.T6.1.1.1.1.1.1.9.9.17.1" class="ltx_text ltx_font_bold">14.43</span></span></span>
<span id="A1.T6.1.1.1.1.1.1.10.10" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.10.10.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.10.10.1.1" class="ltx_text ltx_font_bold">Average</span></span>
<span id="A1.T6.1.1.1.1.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r">24.46(12.37)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r">19.14(6.77)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r">47.34(5.73)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_rr">30.25(6.83)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r">39.11(13.01)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_rr">30.39(14.48)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.8" class="ltx_td ltx_align_center ltx_border_r">25.52(4.89)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.9" class="ltx_td ltx_align_center ltx_border_rr">18.13(5.50)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.10" class="ltx_td ltx_align_center ltx_border_r">38.19(10.39)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.11" class="ltx_td ltx_align_center ltx_border_rr">29.02(10.96)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.12" class="ltx_td ltx_align_center ltx_border_r">26.53(5.13)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.13" class="ltx_td ltx_align_center ltx_border_rr">19.37(4.91)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.14" class="ltx_td ltx_align_center ltx_border_r">35.07(11.85)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.15" class="ltx_td ltx_align_center ltx_border_rr">27.99(13.28)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.16" class="ltx_td ltx_align_center ltx_border_r">25.04(5.28)</span>
<span id="A1.T6.1.1.1.1.1.1.10.10.17" class="ltx_td ltx_align_center"><span id="A1.T6.1.1.1.1.1.1.10.10.17.1" class="ltx_text ltx_font_bold">17.71(5.06)</span></span></span>
<span id="A1.T6.1.1.1.1.1.1.11.11" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.11.11.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="A1.T6.1.1.1.1.1.1.11.11.1.1" class="ltx_text ltx_font_bold">Fine-tuning data</span></span>
<span id="A1.T6.1.1.1.1.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_16"><span id="A1.T6.1.1.1.1.1.1.11.11.2.1" class="ltx_text ltx_font_bold">In-House</span></span></span>
<span id="A1.T6.1.1.1.1.1.1.12.12" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.12.12.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><span id="A1.T6.1.1.1.1.1.1.12.12.1.1" class="ltx_text ltx_font_bold">OH-1</span></span>
<span id="A1.T6.1.1.1.1.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">27.65</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r ltx_border_tt">19.76</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">33.60</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.5" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">23.34</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">23.25</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.7" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">20.40</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">18.75</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.9" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">15.84</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">24.34</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.11" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">20.01</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">19.81</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.13" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">16.68</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">21.77</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.15" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">18.55</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.16" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">18.61</span>
<span id="A1.T6.1.1.1.1.1.1.12.12.17" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T6.1.1.1.1.1.1.12.12.17.1" class="ltx_text ltx_font_bold">15.42</span></span></span>
<span id="A1.T6.1.1.1.1.1.1.13.13" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.13.13.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.13.13.1.1" class="ltx_text ltx_font_bold">OH-2</span></span>
<span id="A1.T6.1.1.1.1.1.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r">16.84</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r"><span id="A1.T6.1.1.1.1.1.1.13.13.3.1" class="ltx_text ltx_font_bold">16.42</span></span>
<span id="A1.T6.1.1.1.1.1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r">33.53</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.5" class="ltx_td ltx_align_center ltx_border_rr">24.86</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.6" class="ltx_td ltx_align_center ltx_border_r">26.03</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.7" class="ltx_td ltx_align_center ltx_border_rr">24.04</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.8" class="ltx_td ltx_align_center ltx_border_r">21.14</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.9" class="ltx_td ltx_align_center ltx_border_rr">17.53</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.10" class="ltx_td ltx_align_center ltx_border_r">23.43</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.11" class="ltx_td ltx_align_center ltx_border_rr">21.40</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.12" class="ltx_td ltx_align_center ltx_border_r">18.64</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.13" class="ltx_td ltx_align_center ltx_border_rr">17.23</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.14" class="ltx_td ltx_align_center ltx_border_r">21.83</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.15" class="ltx_td ltx_align_center ltx_border_rr">19.97</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.16" class="ltx_td ltx_align_center ltx_border_r">17.80</span>
<span id="A1.T6.1.1.1.1.1.1.13.13.17" class="ltx_td ltx_align_center">16.88</span></span>
<span id="A1.T6.1.1.1.1.1.1.14.14" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.14.14.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.14.14.1.1" class="ltx_text ltx_font_bold">DC-1</span></span>
<span id="A1.T6.1.1.1.1.1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r"><span id="A1.T6.1.1.1.1.1.1.14.14.2.1" class="ltx_text ltx_font_bold">23.73</span></span>
<span id="A1.T6.1.1.1.1.1.1.14.14.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r">28.79</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r">52.01</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.5" class="ltx_td ltx_align_center ltx_border_rr">37.70</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.6" class="ltx_td ltx_align_center ltx_border_r">35.99</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.7" class="ltx_td ltx_align_center ltx_border_rr">32.43</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.8" class="ltx_td ltx_align_center ltx_border_r">31.33</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.9" class="ltx_td ltx_align_center ltx_border_rr">24.69</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.10" class="ltx_td ltx_align_center ltx_border_r">39.31</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.11" class="ltx_td ltx_align_center ltx_border_rr">32.50</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.12" class="ltx_td ltx_align_center ltx_border_r">30.18</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.13" class="ltx_td ltx_align_center ltx_border_rr">24.61</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.14" class="ltx_td ltx_align_center ltx_border_r">35.63</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.15" class="ltx_td ltx_align_center ltx_border_rr">29.44</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.16" class="ltx_td ltx_align_center ltx_border_r">30.62</span>
<span id="A1.T6.1.1.1.1.1.1.14.14.17" class="ltx_td ltx_align_center">24.90</span></span>
<span id="A1.T6.1.1.1.1.1.1.15.15" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.15.15.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.15.15.1.1" class="ltx_text ltx_font_bold">DC-2</span></span>
<span id="A1.T6.1.1.1.1.1.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r">32.76</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r"><span id="A1.T6.1.1.1.1.1.1.15.15.3.1" class="ltx_text ltx_font_bold">26.42</span></span>
<span id="A1.T6.1.1.1.1.1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r">59.55</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.5" class="ltx_td ltx_align_center ltx_border_rr">43.49</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.6" class="ltx_td ltx_align_center ltx_border_r">44.73</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.7" class="ltx_td ltx_align_center ltx_border_rr">37.03</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.8" class="ltx_td ltx_align_center ltx_border_r">37.27</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.9" class="ltx_td ltx_align_center ltx_border_rr">30.15</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.10" class="ltx_td ltx_align_center ltx_border_r">47.75</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.11" class="ltx_td ltx_align_center ltx_border_rr">36.05</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.12" class="ltx_td ltx_align_center ltx_border_r">39.50</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.13" class="ltx_td ltx_align_center ltx_border_rr">30.56</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.14" class="ltx_td ltx_align_center ltx_border_r">44.65</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.15" class="ltx_td ltx_align_center ltx_border_rr">37.70</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.16" class="ltx_td ltx_align_center ltx_border_r">36.81</span>
<span id="A1.T6.1.1.1.1.1.1.15.15.17" class="ltx_td ltx_align_center">30.34</span></span>
<span id="A1.T6.1.1.1.1.1.1.16.16" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.16.16.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.16.16.1.1" class="ltx_text ltx_font_bold">CA-1</span></span>
<span id="A1.T6.1.1.1.1.1.1.16.16.2" class="ltx_td ltx_align_center ltx_border_r">54.44</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r">55.77</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.4" class="ltx_td ltx_align_center ltx_border_r">72.88</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.5" class="ltx_td ltx_align_center ltx_border_rr">57.33</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.6" class="ltx_td ltx_align_center ltx_border_r">56.45</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.7" class="ltx_td ltx_align_center ltx_border_rr">50.99</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.8" class="ltx_td ltx_align_center ltx_border_r">48.33</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.9" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.16.16.9.1" class="ltx_text ltx_font_bold">40.69</span></span>
<span id="A1.T6.1.1.1.1.1.1.16.16.10" class="ltx_td ltx_align_center ltx_border_r">58.64</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.11" class="ltx_td ltx_align_center ltx_border_rr">49.09</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.12" class="ltx_td ltx_align_center ltx_border_r">51.23</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.13" class="ltx_td ltx_align_center ltx_border_rr">41.43</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.14" class="ltx_td ltx_align_center ltx_border_r">56.85</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.15" class="ltx_td ltx_align_center ltx_border_rr">49.42</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.16" class="ltx_td ltx_align_center ltx_border_r">48.61</span>
<span id="A1.T6.1.1.1.1.1.1.16.16.17" class="ltx_td ltx_align_center">41.54</span></span>
<span id="A1.T6.1.1.1.1.1.1.17.17" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.17.17.1" class="ltx_td ltx_align_center ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.17.17.1.1" class="ltx_text ltx_font_bold">CA-2</span></span>
<span id="A1.T6.1.1.1.1.1.1.17.17.2" class="ltx_td ltx_align_center ltx_border_r">25.48</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_r"><span id="A1.T6.1.1.1.1.1.1.17.17.3.1" class="ltx_text ltx_font_bold">24.04</span></span>
<span id="A1.T6.1.1.1.1.1.1.17.17.4" class="ltx_td ltx_align_center ltx_border_r">56.74</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.5" class="ltx_td ltx_align_center ltx_border_rr">44.80</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.6" class="ltx_td ltx_align_center ltx_border_r">40.44</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.7" class="ltx_td ltx_align_center ltx_border_rr">36.46</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.8" class="ltx_td ltx_align_center ltx_border_r">36.74</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.9" class="ltx_td ltx_align_center ltx_border_rr">31.54</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.10" class="ltx_td ltx_align_center ltx_border_r">41.26</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.11" class="ltx_td ltx_align_center ltx_border_rr">35.86</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.12" class="ltx_td ltx_align_center ltx_border_r">33.59</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.13" class="ltx_td ltx_align_center ltx_border_rr">30.30</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.14" class="ltx_td ltx_align_center ltx_border_r">37.43</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.15" class="ltx_td ltx_align_center ltx_border_rr">33.84</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.16" class="ltx_td ltx_align_center ltx_border_r">33.40</span>
<span id="A1.T6.1.1.1.1.1.1.17.17.17" class="ltx_td ltx_align_center">29.91</span></span>
<span id="A1.T6.1.1.1.1.1.1.18.18" class="ltx_tr">
<span id="A1.T6.1.1.1.1.1.1.18.18.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr"><span id="A1.T6.1.1.1.1.1.1.18.18.1.1" class="ltx_text ltx_font_bold">Averge</span></span>
<span id="A1.T6.1.1.1.1.1.1.18.18.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">30.15(12.99)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_r">28.53(14.07)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">51.39(15.44)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr">38.59(12.93)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">37.82(12.30)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr">33.56(10.86)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">32.26(8.92)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr">26.74(7.72)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">39.12(13.60)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr">32.49(10.76)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.12" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">32.16(10.78)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.13" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr">26.80(8.00)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.14" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">36.36(11.54)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.15" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr">31.49(9.74)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.16" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">30.97(9.99)</span>
<span id="A1.T6.1.1.1.1.1.1.18.18.17" class="ltx_td ltx_align_center ltx_border_b"><span id="A1.T6.1.1.1.1.1.1.18.18.17.1" class="ltx_text ltx_font_bold">26.50(8.09)</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.13017" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.13018" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.13018">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.13018" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.13019" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 14:55:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
