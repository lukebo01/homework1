<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.13585] FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation</title><meta property="og:description" content="Sign language translation has historically been peripheral to mainstream machine translation research. In order to help converge the fields, we introduce FLEURS-ASL, an extension of the multiway parallel benchmarks FLO…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.13585">

<!--Generated on Thu Sep  5 12:02:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Garrett Tanzer 
<br class="ltx_break">Google 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">gtanzer@google.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Sign language translation has historically been peripheral to mainstream machine translation research. In order to help converge the fields, we introduce FLEURS-ASL, an extension of the multiway parallel benchmarks FLORES (for text) and FLEURS (for speech) to support their first sign language (as video), American Sign Language, translated by 5 Certified Deaf Interpreters. FLEURS-ASL can be used to evaluate a variety of tasks—primarily sentence- and discourse-level translation—between ASL and 200 other languages as text, or 102 languages as speech. We provide baselines for tasks from ASL to English text using a unified modeling approach that incorporates timestamp tokens and previous text tokens in a 34-second context window, trained on random video clips from YouTube-ASL. This model meets or exceeds the performance of phrase-level baselines while supporting a multitude of new tasks. We also use FLEURS-ASL to show that multimodal frontier models have virtually no understanding of ASL, underscoring the importance of including sign languages in standard evaluation suites.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We publicly release FLEURS-ASL at <a target="_blank" href="https://www.kaggle.com/datasets/googleai/fleurs-asl" title="" class="ltx_ref ltx_href">this link</a> under CC BY-SA 4.0.
</span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Despite calls to include sign languages in mainstream natural language processing <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, sign language translation remains peripheral to the machine translation field. The sign language translation field uses individual benchmarks for each language—such as How2Sign <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> for ASL—constructed as train/test splits from one underlying dataset, often with overlapping signers. Meanwhile, mainstream machine translation has developed independently constructed massively multilingual benchmarks like FLORES <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> (a set of short documents translated into 200 languages), which has been extended to FLEURS <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> (speech for 102 languages) and Belebele <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> (reading comprehension questions for 122 languages). Multiway benchmarks admit better comparisons of translation quality across languages because they reduce irrelevant variation in topic or style. Independently constructed benchmarks give a more representative view of real-world performance because they test out-of-domain generalization rather than held-out generalization; fresh queries from users do not necessarily come from the training distribution.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While YouTube-ASL <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> takes a first step towards this kind of robust evaluation by reporting zero-shot scores on How2Sign (i.e., scores without finetuning on How2Sign), this test set (like others in sign language translation) is limited in many ways beyond just lack of standardization. For example, the content is translated live after only one look-over, by a combination of Hearing interpreters and Deaf signers (not specified to be professional Deaf interpreters), with errors and redos included in-stream but skipped with sentence-level clip boundaries (complicating discourse-level evaluation) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. Of course, zero-shot evaluation on a held-out test split also forces researchers to discard valuable training data.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we introduce FLEURS-ASL, an extension of FLORES/FLEURS to support their first sign language (as video), American Sign Language. FLEURS-ASL can be used to evaluate a variety of tasks—primarily sentence- and discourse-level translation, but also in principle caption alignment, retrieval, and multiple-choice receptive comprehension—between ASL and all the languages supported by FLORES and FLEURS. FLEURS-ASL was translated by 5 Certified Deaf Interpreters with an estimated 5-6 hours of preparation per 1 hour of translated content, ensuring a high quality bar.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We provide several sets of baselines. First, we provide human baselines for sentence-level and discourse-level translation, at 13.0 BLEU (64.6 BLEURT) and 13.5 BLEU respectively.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Second, as a model baseline for the suite of FLEURS-ASL tasks, we introduce a unified sign language to text modeling approach that builds upon YouTube-ASL <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>’s T5 baseline. Inspired by Whisper’s approach for speech to text tasks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, we extend the context window to 34 seconds of signing, include 256 tokens of prior text context, incorporate timestamp tokens as both input and output, and train on random video clips. The use of multi-caption random clips heeds the call of <cite class="ltx_cite ltx_citemacro_citet">Tanzer et al. [<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> to explore methods that can incorporate more context and are more tolerant to caption misalignment. This exploratory method meets or exceeds the performance of sentence-level baselines (3.7 vs. 2.9 BLEU, 37.2 vs. 33.6 BLEURT) while enabling a multitude of new tasks through chunked autoregression, such as discourse-level translation, timed translation, and (albeit poorly) caption alignment—despite lack of optimization due to the large possibility space.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">And third, we use FLEURS-ASL to show that recent multimodal frontier models such as GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro display virtually no understanding of ASL, underscoring the importance of including sign language tasks in standard evaluation suites.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We publicly release FLEURS-ASL at <a target="_blank" href="https://www.kaggle.com/datasets/googleai/fleurs-asl" title="" class="ltx_ref ltx_href">this link</a>. We hope that it will encourage development and help to evaluate more generally capable sign language models, perhaps drawing inspiration from our new unified modeling approach. We also hope that the practices and findings from our data collection will aid in the construction of high-quality eval sets for more of the world’s sign languages. See Section <a href="#S6" title="6 Limitations ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> for a discussion of limitations; while there is great value to standardized benchmarks, we expect that more evaluation will be needed specifically for sign languages to capture variety across vision and language aspects and ensure consistent performance.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Sign Language Translation Benchmarks</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Sign language translation has historically been evaluated on test splits of the same underlying dataset used for training. Early and well-worn datasets like RWTH-PHOENIX-Weather 2014 T <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> have narrow domains—which were initially useful for measuring modeling progress but are not representative of broader content—and consist of live hearing interpretations. Over time datasets have expanded to cover more diverse topics, levels of signing proficiency, recording environment, and so on, <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, but evaluation still tends to be performed on held-out test splits (often even with overlapping signers). This is true for the most related prior work, How2Sign <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, the canonical ASL to English translation benchmark. Beyond the nature of the test split, How2Sign has several issues with translation quality (live interpretation with quality that varies significantly between signers) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, though it remains valuable due to its unique domain of “how to” instructional videos.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The notable exception to this evaluation trend is YouTube-ASL <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> (and the subsequent works that use it <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>); YouTube-ASL provides no test split but instead evaluates zero-shot and finetuned results on How2Sign.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>YouTube data is also unsuitable as a benchmark because videos may be deleted over time.</span></span></span> Our goal for FLEURS-ASL is to enhance this kind of evaluation across a wide array of tasks, with a focus on quality over quantity.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Machine Translation Benchmarks</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In mainstream machine translation, this shift came with benchmarks like FLORES <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, an independently constructed, massively multilingual multiway translation benchmark for over 200 languages as text, which has been extended to FLEURS <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> for 102 languages as speech, Belebele <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> with reading comprehension questions for 122 languages, and more through the <a target="_blank" href="https://github.com/openlanguagedata/flores" title="" class="ltx_ref ltx_href">FLORES+</a> open source project.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">These are by no means the only machine translation benchmarks—there is still space for benchmarks serving other purposes, such as avoiding contamination <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, hosting fair <a target="_blank" href="https://www2.statmt.org/wmt23/" title="" class="ltx_ref ltx_href">WMT</a> competitions in various domains, exercising particular grammatical <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> or social aspects <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, and testing extreme new model capabilities <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, among others—but FLORES/FLEURS serve as a valuable foundation for evaluating breadth and robustness of coverage in multilingual models, measured across languages on the same (independent) domain.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2408.13585/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="193" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S2.F1.2.1" class="ltx_text ltx_font_bold">FLEURS-ASL dataset splits.</span> The sentences are divided among 5 interpreters, and 3 sets of splits: zero-shot (“zs”), signer-independent finetuning (“si”), and signer-dependent finetuning (“sd”). We blur the interpreters’ faces in this paper for privacy, but the underlying dataset is unblurred because facial expressions are an essential component of the grammar of sign languages.</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Sign Language Translation Methods</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The bulk of work on methods for sign language translation focuses on alternative featurization, architectures, pretraining, or losses for the same underlying sentence-level translation task framing <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">To the best of our knowledge, the only prior work that has trained models for context-aware sign language translation is <cite class="ltx_cite ltx_citemacro_citet">Sincan et al. [<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, which incorporates prior text or sign spottings as conditioning for sentence-level translation; the training and test examples still rely on sentence-level alignments being available. BOBSL <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and WMT-SLT 23 <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> include large amounts of weakly supervised data that to our knowledge have not been specially utilized. <cite class="ltx_cite ltx_citemacro_citet">Tanzer et al. [<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> argues based on linguistic survey and an empirical case study that isolated sentence-level clips of the kind in How2Sign are often not even comprehensible to human signers; this motivates our new longer-context modeling approach where we train a mixture of tasks, including conditioning on context and generating timed tracks as outputs, on random crops<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Naturally, works that perform self-supervised pretraining on uncaptioned videos already use random clips. <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></span></span></span> of videos with captions that are generally well-aligned but do not strictly contain the corresponding signing.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The FLEURS-ASL Benchmark</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The FLORES benchmark consists of 3001 sentences across 842 articles drawn from English Wikipedia, split into a dev set of 997 sentences (281 articles), devtest set of 1012 sentences (281 articles), and test set of 992 sentences (280 articles). However, they do not release the test set so—like FLEURS—we build on only the dev and devtest sets. We randomly split these sets into halves (4 chunks in total), each intended to be translated by a different interpreter. This strikes a compromise between signer diversity and the feasibility of recruiting/onboarding qualified interpreters.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Collection</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We set out specifically to produce translations with a high quality bar, which meant recruiting <a target="_blank" href="https://www.casli.org/certified-deaf-interpreter-exam-cdi/" title="" class="ltx_ref ltx_href">Certified Deaf Interpreters</a> (CDIs: Deaf or Hard of Hearing signers with native or near-native fluency, cultural competence, and professional certification for their interpreting qualifications) who would research and plan translations with substantial preparation time, then perform and record them (with a ratio of 5-6 hours of prep time for each 1 hour of recorded content). Therefore our selection criteria were as follows: a) <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">required</span>: CDI, b) <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">required</span>: expertise in translation from English text,<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Much of the work that CDIs do is actually translating from a Hearing interpreter’s ASL into more natural, culturally aware Deaf ASL (with particular focus on facial expressions and body language), so translating the kind of English text found in FLORES is not necessarily within everyone’s area of expertise.</span></span></span> c) <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">required</span>: has their own professional recording environment, and d) <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_italic">preferred</span>: not prolific on YouTube (to maintain zero-shotness).<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Unfortunately, it could not be a hard requirement that the interpreter is completely absent from public web data because the pool of qualified interpreters, especially those who are comfortable interpreting content that will be recorded and made public, is limited.</span></span></span></p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We ran an initial pilot through a sign language interpretation vendor, which resulted in signer #0’s chunk. Then we rerandomized the chunks<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We had assumed that the FLORES articles were presented in random order but realized that this is not the case (though the effect seems minor). This rerandomization means that signer #0’s translated content overlaps with signer #1 and #2, which may be useful for studying generalization across signers or the variation in translations by different people.</span></span></span> and partnered with the <a target="_blank" href="https://dpan.tv/" title="" class="ltx_ref ltx_href">Deaf Professional Arts Network (DPAN)</a>, a Deaf-led organization that specializes in American Sign Language media, to recruit 4 more interpreters (signers #1-4). Across both of these vendors, candidates were recruited targeted to the above criteria and then screened based on (paid) sample translations they produced for one article. For each candidate that was ultimately selected, several others were considered who either withdrew for logistical reasons or were not the best fit to translate this content.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>In addition to the fact that not all CDIs have expertise in translation from English text, the content of FLORES is relatively difficult to translate because it has many relatively short articles in different domains. Sign language interpreters are often specialized to certain domains or prepare for a domain ahead of an event—but these events are much longer than the short documents being translated here, so the overhead is less burdensome. For text translation benchmarks, different domains can be allocated to different translators because the differences between individual translators are less visible, whereas here we want to be able to study variation across signers without interference from domain correlations.</span></span></span> At a high level, the interpreters were told the project’s expectations for the amount of preparation and asked to sign as naturally as possible, flowing naturally between sentences (but preserving the original ordering, which unfortunately limits discourse phenomena to some extent) so that caption alignment wouldn’t be artificially easy.<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Despite this instruction, there are sometimes still unnaturally long pauses between sentences due to the performance/memory characteristics of sign language translation, so we do not expect caption alignment to be as challenging as in real data.</span></span></span> See Appendix <a href="#A1" title="Appendix A Translation Guidance ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> for more details on the guidance interpreters were given. They then proceeded to translate their content. All interpreters gave informed consent for the use of this data in machine learning and were paid at market rates.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Each interpreter contributed a certain number of hours of work to the task, but there was wide variation in the speed of the interpreters—both in terms of preparation time, and also literally the speed of their signing. For the faster interpreters (#1 and #3), we went through a round of revisions where the first author (a non-native but proficient signer) quality-checked all the content and sent feedback to DPAN for rerecordings—minor performance errors like unintentionally omitting details, not high-level changes to the translations, which were of course up to the discretion of the expert interpreters.<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>This is another way in which high-quality sign language translation is more difficult than translation for text: the translations are performed in real-time (often based on nonstandardized notes) and cannot be edited later. If there is an error (and if we care about discourse-level tasks, which we do here), the entire video must be rerecorded, which might introduce new errors in the process. This difficulty—which Deaf signers experience when producing new professional-quality content, not just translations—motivates more machine learning research on seamless editing for sign language videos, which is an understudied problem.</span></span></span> Given time constraints, for the other interpreters (#2 and #4) we decided to target fewer hours of translated content and do fewer revisions. This means that signer #1 and #3 have the most complete and thoroughly revised content, but the others still meet a high quality bar and offer more variation for evaluation. The first author manually annotated caption alignments for all content while performing quality checks.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"></td>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;">#</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;">#</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;">#</th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;">sent. length (chars)</th>
<th id="S3.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;">sent. duration (secs)</th>
<th id="S3.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;">#</th>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.2.2.1" class="ltx_td" style="padding-left:5.5pt;padding-right:5.5pt;"></td>
<th id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;">signers</th>
<th id="S3.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;">discourses</th>
<th id="S3.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;">sentences</th>
<th id="S3.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;">(0, 10, 50, 90, 100)%</th>
<th id="S3.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;">(0, 10, 50, 90, 100)%</th>
<th id="S3.T1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.5pt;padding-right:5.5pt;">hours</th>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<th id="S3.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">How2Sign</th>
<th id="S3.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">11</th>
<th id="S3.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">2456</th>
<th id="S3.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">35263</th>
<th id="S3.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">3, 27, 76, 171, 977</th>
<th id="S3.T1.1.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">0.0, 1.7, 5.3, 13.6, 143.0</th>
<th id="S3.T1.1.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">79.1</th>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">FLEURS-ASL</td>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">5</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">495</td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">1749</td>
<td id="S3.T1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">28, 76, 122, 186, 368</td>
<td id="S3.T1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">1.6, 7.4, 13.3, 25.0, 72.8</td>
<td id="S3.T1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">7.49</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.5.5.1" class="ltx_td ltx_align_left" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S3.T1.1.5.5.1.1" class="ltx_text ltx_font_italic">signers</span></td>
<td id="S3.T1.1.5.5.2" class="ltx_td" style="padding-left:5.5pt;padding-right:5.5pt;"></td>
<td id="S3.T1.1.5.5.3" class="ltx_td" style="padding-left:5.5pt;padding-right:5.5pt;"></td>
<td id="S3.T1.1.5.5.4" class="ltx_td" style="padding-left:5.5pt;padding-right:5.5pt;"></td>
<td id="S3.T1.1.5.5.5" class="ltx_td" style="padding-left:5.5pt;padding-right:5.5pt;"></td>
<td id="S3.T1.1.5.5.6" class="ltx_td" style="padding-left:5.5pt;padding-right:5.5pt;"></td>
<td id="S3.T1.1.5.5.7" class="ltx_td" style="padding-left:5.5pt;padding-right:5.5pt;"></td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.6.6.1" class="ltx_td ltx_align_left" style="padding-left:5.5pt;padding-right:5.5pt;">      #0</td>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">1</td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">103</td>
<td id="S3.T1.1.6.6.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">350</td>
<td id="S3.T1.1.6.6.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">35, 71, 122, 190, 328</td>
<td id="S3.T1.1.6.6.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">4.4, 8.1, 15.1, 24.4, 48.5</td>
<td id="S3.T1.1.6.6.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">1.64</td>
</tr>
<tr id="S3.T1.1.7.7" class="ltx_tr">
<td id="S3.T1.1.7.7.1" class="ltx_td ltx_align_left" style="padding-left:5.5pt;padding-right:5.5pt;">      #1</td>
<td id="S3.T1.1.7.7.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">1</td>
<td id="S3.T1.1.7.7.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">141</td>
<td id="S3.T1.1.7.7.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">508</td>
<td id="S3.T1.1.7.7.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">35, 81, 123, 183, 368</td>
<td id="S3.T1.1.7.7.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">1.6, 6.9, 12.0, 19.6, 46.0</td>
<td id="S3.T1.1.7.7.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">1.84</td>
</tr>
<tr id="S3.T1.1.8.8" class="ltx_tr">
<td id="S3.T1.1.8.8.1" class="ltx_td ltx_align_left" style="padding-left:5.5pt;padding-right:5.5pt;">      #2</td>
<td id="S3.T1.1.8.8.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">1</td>
<td id="S3.T1.1.8.8.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">70</td>
<td id="S3.T1.1.8.8.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">252</td>
<td id="S3.T1.1.8.8.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">52, 75, 124, 190, 328</td>
<td id="S3.T1.1.8.8.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">4.4, 12.6, 22.7, 37.0, 72.8</td>
<td id="S3.T1.1.8.8.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">1.72</td>
</tr>
<tr id="S3.T1.1.9.9" class="ltx_tr">
<td id="S3.T1.1.9.9.1" class="ltx_td ltx_align_left" style="padding-left:5.5pt;padding-right:5.5pt;">      #3</td>
<td id="S3.T1.1.9.9.2" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">1</td>
<td id="S3.T1.1.9.9.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">141</td>
<td id="S3.T1.1.9.9.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">494</td>
<td id="S3.T1.1.9.9.5" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">28, 74, 123, 184, 290</td>
<td id="S3.T1.1.9.9.6" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">2.4, 6.6, 10.3, 15.4, 30.5</td>
<td id="S3.T1.1.9.9.7" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;">1.57</td>
</tr>
<tr id="S3.T1.1.10.10" class="ltx_tr">
<td id="S3.T1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">      #4</td>
<td id="S3.T1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">1</td>
<td id="S3.T1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">40</td>
<td id="S3.T1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">145</td>
<td id="S3.T1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">33, 80, 117, 165, 246</td>
<td id="S3.T1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">4.3, 8.6, 15.5, 25.6, 38.5</td>
<td id="S3.T1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;">0.72</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S3.T1.3.1" class="ltx_text ltx_font_bold">FLEURS-ASL summary statistics</span> and comparisons to How2Sign (all splits included). # of signers, # of discourses, # of sentences, sentence length &amp; duration (across 0th, 10th, 50th, 90th, and 100th percentiles) measured in characters and seconds respectively, and # of hours.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dataset Statistics</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">See Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Data Collection ‣ 3 The FLEURS-ASL Benchmark ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for the final statistics for the translated content in FLEURS-ASL, broken down per signer. Of the 5 signers, 2 are men and 3 are women. Signers #0-3 are right-handed, and #4 is left-handed. There is limited variation in skin tone and age; see discussion in Section <a href="#S6" title="6 Limitations ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Note the variation in median sentence duration across signers (from 10.3 seconds to 22.7 seconds) despite minimal variation in median sentence length (i.e., signing speed varies across interpreters), and the increased median sentence duration for FLEURS-ASL compared to How2Sign (13.2 seconds vs. 5.3 seconds) due to the different domain (web articles vs. informal instructional narratives). See Appendix <a href="#A3" title="Appendix C Disaggregated Analysis ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> for more description of the individual signers. The short duration of How2Sign clips (especially in the left tail of the distribution) can make them difficult to understand for sentence-level translation in the absence of context <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Benchmark Tasks</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">See Figure <a href="#S2.F1" title="Figure 1 ‣ 2.2 Machine Translation Benchmarks ‣ 2 Related Work ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for a depiction of the final dataset splits. We provide splits for three settings: zero-shot, signer-independent finetuning (i.e., there is no signer overlap between the train and test sets), and signer-dependent finetuning (i.e., all signers overlap).<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>We ensure that the signer-dependent splits have disjoint train and test sets in light of the content overlap for signers #0 and #1+#2.</span></span></span> We expect that the signer-dependent splits will be used less than the others, but they may be useful for studying the effects of per-signer personalization.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">FLEURS-ASL supports many tasks by virtue of being grafted onto FLORES/FLEURS/Belebele, but we expect that the most common will be sentence- and discourse-level translation from ASL to English. We recommend scoring sentence-level translation with both BLEU <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> (with sacreBLEU <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> with <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">intl</span> tokenization) and BLEURT <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> (with BLEURT-20 <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>). Untimed discourse-level translation can only be scored with BLEU because BLEURT is not intended for long outputs. We score timed translation using Timed BLEU <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (and by extension, BLEURT), where we temporally interpolate at the granularity of characters rather than tokens for convenience.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Baselines</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We provide three sets of baselines. First, we give human baselines for sentence- and discourse-level translation. Second, we train new versions of the YouTube-ASL baselines that can support many FLEURS-ASL sign<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.p1.1.m1.1a"><mo stretchy="false" id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\rightarrow</annotation></semantics></math>text tasks using a unified multitask mixture, integrating prior text history, timestamps, and a longer context window (rather than just caption-level translation). And third, we evaluate current multimodal frontier models and show that they have virtually no understanding of ASL.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Human Baseline</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We recruited a native Deaf ASL signer with professional training/experience in sign language education through DPAN to perform the human baseline. The annotator was informed of the purpose of their translations (to be released for use in ML research, to understand how well models perform at ASL<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mo stretchy="false" id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\rightarrow</annotation></semantics></math>English translation vs. humans) and paid at market rates.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We sampled 8 articles uniformly at random from the signer-dependent test set for each signer. Within each article, we sampled one sentence. For each article, the annotator was tasked with translating that sampled sentence’s ASL clip back to English, and then (once finished) was presented with the entire article’s ASL video and asked to translate it to English in its entirety. This gives us human baselines for sentence- and discourse-level translation across 40 instances each, where like <cite class="ltx_cite ltx_citemacro_citet">Tanzer et al. [<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> the translations for each setting are performed by the same person (to reduce irrelevant variation) without contaminating the sentence-level translations with discourse context. See Appendix <a href="#A2" title="Appendix B Human Baseline Instructions ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> for the task instructions.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">The scores are 13.0 BLEU (64.6 BLEURT) for sentence-level translation and 13.5 BLEU for discourse-level translation. Like in <cite class="ltx_cite ltx_citemacro_citet">Tanzer et al. [<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, these metrics (in particular BLEU) may sound low, but qualitatively the human translations look good. The discrepancies with the references come from a variety of sources (natural changes due to backtranslation through a sign language, errors in one of the translation directions) but the most interesting come from visual ambiguity in the sign language translation, such as when “6,500” was perceived as “2,500” because the 6 was angled such that the ring and middle finger were aligned, making it look like only 2 fingers were raised without close inspection. See Appendix <a href="#A4" title="Appendix D Complete Human Baseline ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> for the full set of human translations.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2408.13585/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="350" height="247" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
<span id="S4.F2.40.2" class="ltx_text ltx_font_bold">Unified multitask document-level sign to text training.</span> 
<br class="ltx_break"><span id="S4.F2.41.3" class="ltx_text ltx_font_bold ltx_font_italic">Top left:<span id="S4.F2.41.3.1" class="ltx_text ltx_font_upright"> Model architecture.</span></span> Input text tokens (token embeddings) and up to 512 frames of half-frame-rate linearly projected MediaPipe Holistic landmarks (subset of 85 3D points) are the inputs to T5v1.1-Base (a pretrained encoder-decoder Transformer), finetuned on YouTube-ASL as described below.  
<br class="ltx_break"><span id="S4.F2.42.4" class="ltx_text ltx_font_bold ltx_font_italic">Right:<span id="S4.F2.42.4.1" class="ltx_text ltx_font_upright"> Preprocessing steps to sample training clips from long videos.</span></span> In order to sample training clips uniformly in proportion to video duration, we perform the following steps. (This is only necessary because SeqIO <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> does not support sampling training examples in proportion to scalar values attached to them, and because we want to support multi-epoch training with random crops without duplicating the underlying data many times.) We chunk arbitrary length captioned training videos into training examples on disk of <math id="S4.F2.18.m1.1" class="ltx_Math" alttext="2n" display="inline"><semantics id="S4.F2.18.m1.1b"><mrow id="S4.F2.18.m1.1.1" xref="S4.F2.18.m1.1.1.cmml"><mn id="S4.F2.18.m1.1.1.2" xref="S4.F2.18.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.F2.18.m1.1.1.1" xref="S4.F2.18.m1.1.1.1.cmml">​</mo><mi id="S4.F2.18.m1.1.1.3" xref="S4.F2.18.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.18.m1.1c"><apply id="S4.F2.18.m1.1.1.cmml" xref="S4.F2.18.m1.1.1"><times id="S4.F2.18.m1.1.1.1.cmml" xref="S4.F2.18.m1.1.1.1"></times><cn type="integer" id="S4.F2.18.m1.1.1.2.cmml" xref="S4.F2.18.m1.1.1.2">2</cn><ci id="S4.F2.18.m1.1.1.3.cmml" xref="S4.F2.18.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.18.m1.1d">2n</annotation></semantics></math> seconds, preprocess them with MediaPipe Holistic, and carry along the timed caption track, including the previous and next <math id="S4.F2.19.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.F2.19.m2.1b"><mi id="S4.F2.19.m2.1.1" xref="S4.F2.19.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.F2.19.m2.1c"><ci id="S4.F2.19.m2.1.1.cmml" xref="S4.F2.19.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.19.m2.1d">n</annotation></semantics></math> seconds of captions. (Here, <math id="S4.F2.20.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.F2.20.m3.1b"><mi id="S4.F2.20.m3.1.1" xref="S4.F2.20.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.F2.20.m3.1c"><ci id="S4.F2.20.m3.1.1.cmml" xref="S4.F2.20.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.20.m3.1d">n</annotation></semantics></math> is 34 seconds so that 15 Hz input fits in 512 tokens.) For each training example, the start position of the clip is sampled from the first half of the chunk uniformly at random, and with probability 0.2 the duration of the clip is truncated to between <math id="S4.F2.21.m4.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S4.F2.21.m4.1b"><mi id="S4.F2.21.m4.1.1" xref="S4.F2.21.m4.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.F2.21.m4.1c"><ci id="S4.F2.21.m4.1.1.cmml" xref="S4.F2.21.m4.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.21.m4.1d">m</annotation></semantics></math> and <math id="S4.F2.22.m5.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.F2.22.m5.1b"><mi id="S4.F2.22.m5.1.1" xref="S4.F2.22.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.F2.22.m5.1c"><ci id="S4.F2.22.m5.1.1.cmml" xref="S4.F2.22.m5.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.22.m5.1d">n</annotation></semantics></math> seconds. (Here, <math id="S4.F2.23.m6.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S4.F2.23.m6.1b"><mi id="S4.F2.23.m6.1.1" xref="S4.F2.23.m6.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S4.F2.23.m6.1c"><ci id="S4.F2.23.m6.1.1.cmml" xref="S4.F2.23.m6.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.23.m6.1d">m</annotation></semantics></math> is 17 seconds). We modify this scheme slightly at the beginning and end of videos; by default, the first and last <math id="S4.F2.24.m7.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.F2.24.m7.1b"><mi id="S4.F2.24.m7.1.1" xref="S4.F2.24.m7.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.F2.24.m7.1c"><ci id="S4.F2.24.m7.1.1.cmml" xref="S4.F2.24.m7.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.24.m7.1d">n</annotation></semantics></math> seconds of the video would be extremely undersampled because they do not overlap with other examples. We make the chunk duration <math id="S4.F2.25.m8.1" class="ltx_Math" alttext="\frac{3}{2}n" display="inline"><semantics id="S4.F2.25.m8.1b"><mrow id="S4.F2.25.m8.1.1" xref="S4.F2.25.m8.1.1.cmml"><mfrac id="S4.F2.25.m8.1.1.2" xref="S4.F2.25.m8.1.1.2.cmml"><mn id="S4.F2.25.m8.1.1.2.2" xref="S4.F2.25.m8.1.1.2.2.cmml">3</mn><mn id="S4.F2.25.m8.1.1.2.3" xref="S4.F2.25.m8.1.1.2.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S4.F2.25.m8.1.1.1" xref="S4.F2.25.m8.1.1.1.cmml">​</mo><mi id="S4.F2.25.m8.1.1.3" xref="S4.F2.25.m8.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.25.m8.1c"><apply id="S4.F2.25.m8.1.1.cmml" xref="S4.F2.25.m8.1.1"><times id="S4.F2.25.m8.1.1.1.cmml" xref="S4.F2.25.m8.1.1.1"></times><apply id="S4.F2.25.m8.1.1.2.cmml" xref="S4.F2.25.m8.1.1.2"><divide id="S4.F2.25.m8.1.1.2.1.cmml" xref="S4.F2.25.m8.1.1.2"></divide><cn type="integer" id="S4.F2.25.m8.1.1.2.2.cmml" xref="S4.F2.25.m8.1.1.2.2">3</cn><cn type="integer" id="S4.F2.25.m8.1.1.2.3.cmml" xref="S4.F2.25.m8.1.1.2.3">2</cn></apply><ci id="S4.F2.25.m8.1.1.3.cmml" xref="S4.F2.25.m8.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.25.m8.1d">\frac{3}{2}n</annotation></semantics></math> seconds rather than <math id="S4.F2.26.m9.1" class="ltx_Math" alttext="2n" display="inline"><semantics id="S4.F2.26.m9.1b"><mrow id="S4.F2.26.m9.1.1" xref="S4.F2.26.m9.1.1.cmml"><mn id="S4.F2.26.m9.1.1.2" xref="S4.F2.26.m9.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.F2.26.m9.1.1.1" xref="S4.F2.26.m9.1.1.1.cmml">​</mo><mi id="S4.F2.26.m9.1.1.3" xref="S4.F2.26.m9.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.26.m9.1c"><apply id="S4.F2.26.m9.1.1.cmml" xref="S4.F2.26.m9.1.1"><times id="S4.F2.26.m9.1.1.1.cmml" xref="S4.F2.26.m9.1.1.1"></times><cn type="integer" id="S4.F2.26.m9.1.1.2.cmml" xref="S4.F2.26.m9.1.1.2">2</cn><ci id="S4.F2.26.m9.1.1.3.cmml" xref="S4.F2.26.m9.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.26.m9.1d">2n</annotation></semantics></math> and sample the start position <math id="S4.F2.27.m10.3" class="ltx_Math" alttext="\sim max(0,U[-\frac{n}{2},n])" display="inline"><semantics id="S4.F2.27.m10.3b"><mrow id="S4.F2.27.m10.3.3" xref="S4.F2.27.m10.3.3.cmml"><mi id="S4.F2.27.m10.3.3.3" xref="S4.F2.27.m10.3.3.3.cmml"></mi><mo id="S4.F2.27.m10.3.3.2" xref="S4.F2.27.m10.3.3.2.cmml">∼</mo><mrow id="S4.F2.27.m10.3.3.1" xref="S4.F2.27.m10.3.3.1.cmml"><mi id="S4.F2.27.m10.3.3.1.3" xref="S4.F2.27.m10.3.3.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.F2.27.m10.3.3.1.2" xref="S4.F2.27.m10.3.3.1.2.cmml">​</mo><mi id="S4.F2.27.m10.3.3.1.4" xref="S4.F2.27.m10.3.3.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.F2.27.m10.3.3.1.2b" xref="S4.F2.27.m10.3.3.1.2.cmml">​</mo><mi id="S4.F2.27.m10.3.3.1.5" xref="S4.F2.27.m10.3.3.1.5.cmml">x</mi><mo lspace="0em" rspace="0em" id="S4.F2.27.m10.3.3.1.2c" xref="S4.F2.27.m10.3.3.1.2.cmml">​</mo><mrow id="S4.F2.27.m10.3.3.1.1.1" xref="S4.F2.27.m10.3.3.1.1.2.cmml"><mo stretchy="false" id="S4.F2.27.m10.3.3.1.1.1.2" xref="S4.F2.27.m10.3.3.1.1.2.cmml">(</mo><mn id="S4.F2.27.m10.2.2" xref="S4.F2.27.m10.2.2.cmml">0</mn><mo id="S4.F2.27.m10.3.3.1.1.1.3" xref="S4.F2.27.m10.3.3.1.1.2.cmml">,</mo><mrow id="S4.F2.27.m10.3.3.1.1.1.1" xref="S4.F2.27.m10.3.3.1.1.1.1.cmml"><mi id="S4.F2.27.m10.3.3.1.1.1.1.3" xref="S4.F2.27.m10.3.3.1.1.1.1.3.cmml">U</mi><mo lspace="0em" rspace="0em" id="S4.F2.27.m10.3.3.1.1.1.1.2" xref="S4.F2.27.m10.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S4.F2.27.m10.3.3.1.1.1.1.1.1" xref="S4.F2.27.m10.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.F2.27.m10.3.3.1.1.1.1.1.1.2" xref="S4.F2.27.m10.3.3.1.1.1.1.1.2.cmml">[</mo><mrow id="S4.F2.27.m10.3.3.1.1.1.1.1.1.1" xref="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.cmml"><mo id="S4.F2.27.m10.3.3.1.1.1.1.1.1.1b" xref="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.cmml">−</mo><mfrac id="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2" xref="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2.cmml"><mi id="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2.2" xref="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2.2.cmml">n</mi><mn id="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2.3" xref="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2.3.cmml">2</mn></mfrac></mrow><mo id="S4.F2.27.m10.3.3.1.1.1.1.1.1.3" xref="S4.F2.27.m10.3.3.1.1.1.1.1.2.cmml">,</mo><mi id="S4.F2.27.m10.1.1" xref="S4.F2.27.m10.1.1.cmml">n</mi><mo stretchy="false" id="S4.F2.27.m10.3.3.1.1.1.1.1.1.4" xref="S4.F2.27.m10.3.3.1.1.1.1.1.2.cmml">]</mo></mrow></mrow><mo stretchy="false" id="S4.F2.27.m10.3.3.1.1.1.4" xref="S4.F2.27.m10.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.27.m10.3c"><apply id="S4.F2.27.m10.3.3.cmml" xref="S4.F2.27.m10.3.3"><csymbol cd="latexml" id="S4.F2.27.m10.3.3.2.cmml" xref="S4.F2.27.m10.3.3.2">similar-to</csymbol><csymbol cd="latexml" id="S4.F2.27.m10.3.3.3.cmml" xref="S4.F2.27.m10.3.3.3">absent</csymbol><apply id="S4.F2.27.m10.3.3.1.cmml" xref="S4.F2.27.m10.3.3.1"><times id="S4.F2.27.m10.3.3.1.2.cmml" xref="S4.F2.27.m10.3.3.1.2"></times><ci id="S4.F2.27.m10.3.3.1.3.cmml" xref="S4.F2.27.m10.3.3.1.3">𝑚</ci><ci id="S4.F2.27.m10.3.3.1.4.cmml" xref="S4.F2.27.m10.3.3.1.4">𝑎</ci><ci id="S4.F2.27.m10.3.3.1.5.cmml" xref="S4.F2.27.m10.3.3.1.5">𝑥</ci><interval closure="open" id="S4.F2.27.m10.3.3.1.1.2.cmml" xref="S4.F2.27.m10.3.3.1.1.1"><cn type="integer" id="S4.F2.27.m10.2.2.cmml" xref="S4.F2.27.m10.2.2">0</cn><apply id="S4.F2.27.m10.3.3.1.1.1.1.cmml" xref="S4.F2.27.m10.3.3.1.1.1.1"><times id="S4.F2.27.m10.3.3.1.1.1.1.2.cmml" xref="S4.F2.27.m10.3.3.1.1.1.1.2"></times><ci id="S4.F2.27.m10.3.3.1.1.1.1.3.cmml" xref="S4.F2.27.m10.3.3.1.1.1.1.3">𝑈</ci><interval closure="closed" id="S4.F2.27.m10.3.3.1.1.1.1.1.2.cmml" xref="S4.F2.27.m10.3.3.1.1.1.1.1.1"><apply id="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.cmml" xref="S4.F2.27.m10.3.3.1.1.1.1.1.1.1"><minus id="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.1.cmml" xref="S4.F2.27.m10.3.3.1.1.1.1.1.1.1"></minus><apply id="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2"><divide id="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2"></divide><ci id="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2.2">𝑛</ci><cn type="integer" id="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S4.F2.27.m10.3.3.1.1.1.1.1.1.1.2.3">2</cn></apply></apply><ci id="S4.F2.27.m10.1.1.cmml" xref="S4.F2.27.m10.1.1">𝑛</ci></interval></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.27.m10.3d">\sim max(0,U[-\frac{n}{2},n])</annotation></semantics></math> for the first chunk and <math id="S4.F2.28.m11.4" class="ltx_Math" alttext="\sim min(\frac{n}{2},U[0,\frac{3n}{2}])" display="inline"><semantics id="S4.F2.28.m11.4b"><mrow id="S4.F2.28.m11.4.4" xref="S4.F2.28.m11.4.4.cmml"><mi id="S4.F2.28.m11.4.4.3" xref="S4.F2.28.m11.4.4.3.cmml"></mi><mo id="S4.F2.28.m11.4.4.2" xref="S4.F2.28.m11.4.4.2.cmml">∼</mo><mrow id="S4.F2.28.m11.4.4.1" xref="S4.F2.28.m11.4.4.1.cmml"><mi id="S4.F2.28.m11.4.4.1.3" xref="S4.F2.28.m11.4.4.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.F2.28.m11.4.4.1.2" xref="S4.F2.28.m11.4.4.1.2.cmml">​</mo><mi id="S4.F2.28.m11.4.4.1.4" xref="S4.F2.28.m11.4.4.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.F2.28.m11.4.4.1.2b" xref="S4.F2.28.m11.4.4.1.2.cmml">​</mo><mi id="S4.F2.28.m11.4.4.1.5" xref="S4.F2.28.m11.4.4.1.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.F2.28.m11.4.4.1.2c" xref="S4.F2.28.m11.4.4.1.2.cmml">​</mo><mrow id="S4.F2.28.m11.4.4.1.1.1" xref="S4.F2.28.m11.4.4.1.1.2.cmml"><mo stretchy="false" id="S4.F2.28.m11.4.4.1.1.1.2" xref="S4.F2.28.m11.4.4.1.1.2.cmml">(</mo><mfrac id="S4.F2.28.m11.3.3" xref="S4.F2.28.m11.3.3.cmml"><mi id="S4.F2.28.m11.3.3.2" xref="S4.F2.28.m11.3.3.2.cmml">n</mi><mn id="S4.F2.28.m11.3.3.3" xref="S4.F2.28.m11.3.3.3.cmml">2</mn></mfrac><mo id="S4.F2.28.m11.4.4.1.1.1.3" xref="S4.F2.28.m11.4.4.1.1.2.cmml">,</mo><mrow id="S4.F2.28.m11.4.4.1.1.1.1" xref="S4.F2.28.m11.4.4.1.1.1.1.cmml"><mi id="S4.F2.28.m11.4.4.1.1.1.1.2" xref="S4.F2.28.m11.4.4.1.1.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S4.F2.28.m11.4.4.1.1.1.1.1" xref="S4.F2.28.m11.4.4.1.1.1.1.1.cmml">​</mo><mrow id="S4.F2.28.m11.4.4.1.1.1.1.3.2" xref="S4.F2.28.m11.4.4.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S4.F2.28.m11.4.4.1.1.1.1.3.2.1" xref="S4.F2.28.m11.4.4.1.1.1.1.3.1.cmml">[</mo><mn id="S4.F2.28.m11.1.1" xref="S4.F2.28.m11.1.1.cmml">0</mn><mo id="S4.F2.28.m11.4.4.1.1.1.1.3.2.2" xref="S4.F2.28.m11.4.4.1.1.1.1.3.1.cmml">,</mo><mfrac id="S4.F2.28.m11.2.2" xref="S4.F2.28.m11.2.2.cmml"><mrow id="S4.F2.28.m11.2.2.2" xref="S4.F2.28.m11.2.2.2.cmml"><mn id="S4.F2.28.m11.2.2.2.2" xref="S4.F2.28.m11.2.2.2.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S4.F2.28.m11.2.2.2.1" xref="S4.F2.28.m11.2.2.2.1.cmml">​</mo><mi id="S4.F2.28.m11.2.2.2.3" xref="S4.F2.28.m11.2.2.2.3.cmml">n</mi></mrow><mn id="S4.F2.28.m11.2.2.3" xref="S4.F2.28.m11.2.2.3.cmml">2</mn></mfrac><mo stretchy="false" id="S4.F2.28.m11.4.4.1.1.1.1.3.2.3" xref="S4.F2.28.m11.4.4.1.1.1.1.3.1.cmml">]</mo></mrow></mrow><mo stretchy="false" id="S4.F2.28.m11.4.4.1.1.1.4" xref="S4.F2.28.m11.4.4.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.28.m11.4c"><apply id="S4.F2.28.m11.4.4.cmml" xref="S4.F2.28.m11.4.4"><csymbol cd="latexml" id="S4.F2.28.m11.4.4.2.cmml" xref="S4.F2.28.m11.4.4.2">similar-to</csymbol><csymbol cd="latexml" id="S4.F2.28.m11.4.4.3.cmml" xref="S4.F2.28.m11.4.4.3">absent</csymbol><apply id="S4.F2.28.m11.4.4.1.cmml" xref="S4.F2.28.m11.4.4.1"><times id="S4.F2.28.m11.4.4.1.2.cmml" xref="S4.F2.28.m11.4.4.1.2"></times><ci id="S4.F2.28.m11.4.4.1.3.cmml" xref="S4.F2.28.m11.4.4.1.3">𝑚</ci><ci id="S4.F2.28.m11.4.4.1.4.cmml" xref="S4.F2.28.m11.4.4.1.4">𝑖</ci><ci id="S4.F2.28.m11.4.4.1.5.cmml" xref="S4.F2.28.m11.4.4.1.5">𝑛</ci><interval closure="open" id="S4.F2.28.m11.4.4.1.1.2.cmml" xref="S4.F2.28.m11.4.4.1.1.1"><apply id="S4.F2.28.m11.3.3.cmml" xref="S4.F2.28.m11.3.3"><divide id="S4.F2.28.m11.3.3.1.cmml" xref="S4.F2.28.m11.3.3"></divide><ci id="S4.F2.28.m11.3.3.2.cmml" xref="S4.F2.28.m11.3.3.2">𝑛</ci><cn type="integer" id="S4.F2.28.m11.3.3.3.cmml" xref="S4.F2.28.m11.3.3.3">2</cn></apply><apply id="S4.F2.28.m11.4.4.1.1.1.1.cmml" xref="S4.F2.28.m11.4.4.1.1.1.1"><times id="S4.F2.28.m11.4.4.1.1.1.1.1.cmml" xref="S4.F2.28.m11.4.4.1.1.1.1.1"></times><ci id="S4.F2.28.m11.4.4.1.1.1.1.2.cmml" xref="S4.F2.28.m11.4.4.1.1.1.1.2">𝑈</ci><interval closure="closed" id="S4.F2.28.m11.4.4.1.1.1.1.3.1.cmml" xref="S4.F2.28.m11.4.4.1.1.1.1.3.2"><cn type="integer" id="S4.F2.28.m11.1.1.cmml" xref="S4.F2.28.m11.1.1">0</cn><apply id="S4.F2.28.m11.2.2.cmml" xref="S4.F2.28.m11.2.2"><divide id="S4.F2.28.m11.2.2.1.cmml" xref="S4.F2.28.m11.2.2"></divide><apply id="S4.F2.28.m11.2.2.2.cmml" xref="S4.F2.28.m11.2.2.2"><times id="S4.F2.28.m11.2.2.2.1.cmml" xref="S4.F2.28.m11.2.2.2.1"></times><cn type="integer" id="S4.F2.28.m11.2.2.2.2.cmml" xref="S4.F2.28.m11.2.2.2.2">3</cn><ci id="S4.F2.28.m11.2.2.2.3.cmml" xref="S4.F2.28.m11.2.2.2.3">𝑛</ci></apply><cn type="integer" id="S4.F2.28.m11.2.2.3.cmml" xref="S4.F2.28.m11.2.2.3">2</cn></apply></interval></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.28.m11.4d">\sim min(\frac{n}{2},U[0,\frac{3n}{2}])</annotation></semantics></math> for the last chunk. For videos whose duration doesn’t divide evenly by <math id="S4.F2.29.m12.1" class="ltx_Math" alttext="2n" display="inline"><semantics id="S4.F2.29.m12.1b"><mrow id="S4.F2.29.m12.1.1" xref="S4.F2.29.m12.1.1.cmml"><mn id="S4.F2.29.m12.1.1.2" xref="S4.F2.29.m12.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.F2.29.m12.1.1.1" xref="S4.F2.29.m12.1.1.1.cmml">​</mo><mi id="S4.F2.29.m12.1.1.3" xref="S4.F2.29.m12.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.29.m12.1c"><apply id="S4.F2.29.m12.1.1.cmml" xref="S4.F2.29.m12.1.1"><times id="S4.F2.29.m12.1.1.1.cmml" xref="S4.F2.29.m12.1.1.1"></times><cn type="integer" id="S4.F2.29.m12.1.1.2.cmml" xref="S4.F2.29.m12.1.1.2">2</cn><ci id="S4.F2.29.m12.1.1.3.cmml" xref="S4.F2.29.m12.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.29.m12.1d">2n</annotation></semantics></math> seconds, we overlap the final two chunks so they are between <math id="S4.F2.30.m13.1" class="ltx_Math" alttext="\frac{3n}{2}" display="inline"><semantics id="S4.F2.30.m13.1b"><mfrac id="S4.F2.30.m13.1.1" xref="S4.F2.30.m13.1.1.cmml"><mrow id="S4.F2.30.m13.1.1.2" xref="S4.F2.30.m13.1.1.2.cmml"><mn id="S4.F2.30.m13.1.1.2.2" xref="S4.F2.30.m13.1.1.2.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S4.F2.30.m13.1.1.2.1" xref="S4.F2.30.m13.1.1.2.1.cmml">​</mo><mi id="S4.F2.30.m13.1.1.2.3" xref="S4.F2.30.m13.1.1.2.3.cmml">n</mi></mrow><mn id="S4.F2.30.m13.1.1.3" xref="S4.F2.30.m13.1.1.3.cmml">2</mn></mfrac><annotation-xml encoding="MathML-Content" id="S4.F2.30.m13.1c"><apply id="S4.F2.30.m13.1.1.cmml" xref="S4.F2.30.m13.1.1"><divide id="S4.F2.30.m13.1.1.1.cmml" xref="S4.F2.30.m13.1.1"></divide><apply id="S4.F2.30.m13.1.1.2.cmml" xref="S4.F2.30.m13.1.1.2"><times id="S4.F2.30.m13.1.1.2.1.cmml" xref="S4.F2.30.m13.1.1.2.1"></times><cn type="integer" id="S4.F2.30.m13.1.1.2.2.cmml" xref="S4.F2.30.m13.1.1.2.2">3</cn><ci id="S4.F2.30.m13.1.1.2.3.cmml" xref="S4.F2.30.m13.1.1.2.3">𝑛</ci></apply><cn type="integer" id="S4.F2.30.m13.1.1.3.cmml" xref="S4.F2.30.m13.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.30.m13.1d">\frac{3n}{2}</annotation></semantics></math> and <math id="S4.F2.31.m14.1" class="ltx_Math" alttext="2n" display="inline"><semantics id="S4.F2.31.m14.1b"><mrow id="S4.F2.31.m14.1.1" xref="S4.F2.31.m14.1.1.cmml"><mn id="S4.F2.31.m14.1.1.2" xref="S4.F2.31.m14.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.F2.31.m14.1.1.1" xref="S4.F2.31.m14.1.1.1.cmml">​</mo><mi id="S4.F2.31.m14.1.1.3" xref="S4.F2.31.m14.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.31.m14.1c"><apply id="S4.F2.31.m14.1.1.cmml" xref="S4.F2.31.m14.1.1"><times id="S4.F2.31.m14.1.1.1.cmml" xref="S4.F2.31.m14.1.1.1"></times><cn type="integer" id="S4.F2.31.m14.1.1.2.cmml" xref="S4.F2.31.m14.1.1.2">2</cn><ci id="S4.F2.31.m14.1.1.3.cmml" xref="S4.F2.31.m14.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.31.m14.1d">2n</annotation></semantics></math> seconds long.  
<br class="ltx_break"><span id="S4.F2.43.5" class="ltx_text ltx_font_bold ltx_font_italic">Bottom left:<span id="S4.F2.43.5.1" class="ltx_text ltx_font_upright"> Control token format and mixture.</span></span> For implementation convenience, we instantiate control tokens as regular text. In order to save compute, task mixture weights are chosen by intuition without hyperparameter sweeps; we expect that optimal weights would change depending on dataset size and application priorities anyway.
<br class="ltx_break">Captions are represented as text spans delimited by separator tokens (either <span id="S4.F2.44.6" class="ltx_text ltx_font_typewriter">‘ ’</span> or <span id="S4.F2.32.1" class="ltx_text ltx_font_typewriter">‘<math id="S4.F2.32.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S4.F2.32.1.m1.1b"><mo id="S4.F2.32.1.m1.1.1" xref="S4.F2.32.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S4.F2.32.1.m1.1c"><ci id="S4.F2.32.1.m1.1.1.cmml" xref="S4.F2.32.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.32.1.m1.1d">\backslash</annotation></semantics></math>n’</span>), optionally with start and end timestamps prepended. There are three sets of captions: the “curr” captions fully contained within the input clip, the “prev” captions starting in the <math id="S4.F2.33.m15.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.F2.33.m15.1b"><mi id="S4.F2.33.m15.1.1" xref="S4.F2.33.m15.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.F2.33.m15.1c"><ci id="S4.F2.33.m15.1.1.cmml" xref="S4.F2.33.m15.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.33.m15.1d">n</annotation></semantics></math> seconds prior to the input clip, and the “next” captions ending in the <math id="S4.F2.34.m16.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.F2.34.m16.1b"><mi id="S4.F2.34.m16.1.1" xref="S4.F2.34.m16.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.F2.34.m16.1c"><ci id="S4.F2.34.m16.1.1.cmml" xref="S4.F2.34.m16.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.34.m16.1d">n</annotation></semantics></math> seconds after it. There are two main branches of input tokens: caption alignment (with probability 0.04) and translation (0.96). 
<br class="ltx_break">The caption alignment branch supports two modes, “choose sep” (0.5) where the input specifies caption breaks, and “copy sep” (0.5), where the model predicts them. In the latter case, the average caption duration across the source video can be provided as conditioning (0.5) as a form of controllability. Then the “left edge” caption timestamp is provided, i.e. the end timestamp of any caption that crosses the left boundary of the landmark input, so that the model knows where to start predicting outputs. Finally, two more modes for either “overflow” (0.8), where the provided captions to align may spill past the right boundary of the clip, and “subset” (0.2), where the captions are a random prefix of those covering the clip, in order to support whole-video and human-in-the-loop caption alignment respectively. The target tokens are unconditionally timed, newline-separated captions.
<br class="ltx_break">The translation branch supports untimed (0.2) and timed (0.8) translation (determining whether the target captions include timestamp tokens), with the latter also allowing duration conditioning (0.5). Either no captions (0.2), previous captions (0.64), or previous and next captions (0.16) are provided as context, to support isolated translation, blockwise autoregressive translation, and infilling.
</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Unified Sign to Text Modeling</h3>

<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S4.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T2.2.3.1.2.1" class="ltx_text ltx_font_bold">How2Sign</span></th>
<td id="S4.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T2.2.3.1.3.1" class="ltx_text ltx_font_bold">FLEURS-ASL</span></td>
</tr>
<tr id="S4.T2.2.4.2" class="ltx_tr">
<th id="S4.T2.2.4.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<th id="S4.T2.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">sentence-level</th>
<td id="S4.T2.2.4.2.3" class="ltx_td ltx_align_center">sentence-level</td>
<td id="S4.T2.2.4.2.4" class="ltx_td ltx_align_center">context+sentence-level</td>
</tr>
<tr id="S4.T2.2.5.3" class="ltx_tr">
<th id="S4.T2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Caption-level</th>
<th id="S4.T2.2.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">4.0 (34.4) / - / 12.1 (45.4)</th>
<td id="S4.T2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_t">2.9 (33.6) / 4.8 (38.6) / 5.4 (40.3)</td>
<td id="S4.T2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T2.2.6.4" class="ltx_tr">
<th id="S4.T2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Ours</th>
<th id="S4.T2.2.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">5.1 (36.9) / - / 14.4 (46.3)</th>
<td id="S4.T2.2.6.4.3" class="ltx_td ltx_align_center">3.7 (37.2) / 4.9 (39.7) / 6.3 (42.2)</td>
<td id="S4.T2.2.6.4.4" class="ltx_td ltx_align_center">3.9 (38.3) / 5.3 (41.4) / 6.8 (43.4)</td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">Human</th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">
<math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\sim</annotation></semantics></math> 19.8 (56.6) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</th>
<td id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<math id="S4.T2.2.2.2.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.T2.2.2.2.m1.1a"><mo id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\sim</annotation></semantics></math> 13.0 (64.6)</td>
<td id="S4.T2.2.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S4.T2.4.1" class="ltx_text ltx_font_bold">Baseline results for sentence-level translation from ASL to English (zero-shot / signer-independent finetuning / signer-dependent finetuning)</span>, measured in BLEU (BLEURT in parentheses). We also report scores on How2Sign as a point of reference. Note that the human baseline scores are not perfectly comparable for either benchmark because they are for a subset of the data that is not necessarily representative due to the sampling method and small size.</figcaption>
</figure>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Method</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Inspired by Whisper <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and heeding the call of <cite class="ltx_cite ltx_citemacro_citet">Tanzer et al. [<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> to move beyond sentence-level sign language translation, we present a new unified modeling approach to flexibly support many FLEURS-ASL tasks (and more) in one model. We integrate timestamp tokens and previous text tokens with a 34-second context window trained on random clips from captioned videos. This approach fits into the typical encoder-decoder framework without architectural modifications, only changes to data preparation. See Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1 Human Baseline ‣ 4 Baselines ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for a complete description of this procedure. The key point is that, in contrast to prior work in sign language, our training examples contain caption <span id="S4.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_italic">tracks</span> as supervision rather than individual captions, and the timing of these tracks is used to adapt to random video clip boundaries, rather than determine the boundaries of those clips. This disregard for preordained alignments in our clip sampling enables us to handle arbitrary clipping at inference time: in particular, we can perform transformations of long videos in autoregressive chunks, where the next chunk starts after the previous chunk’s predicted timestamps.<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>Specifically: in addition to providing caption history in the input text tokens, we maintain the first 4 seconds and last 10 seconds of the 34-second context window as regions where predicted captions should not start or stop; if a caption is predicted in that range, we shift the window for autoregression to ensure that the model has more context to perform its translation, and to tolerate misalignment. This heuristic was determined interactively by observing when the autoregression got “stuck” on certain videos (not from the test set, which would cause leakage).</span></span></span></p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Experiments</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">We build off YouTube-ASL <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>’s baseline, which linearly projects 85 3D points from MediaPipe Holistic <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> skeletons into the encoder of T5 <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> at half frame rate, and predicts text tokens from the decoder (with greedy sampling with beam size 5). We compare to a fresh baseline for sentence-level translation so that results are meaningful in light of dataset churn, and report results on context+sentence-level translation (sentence-level translation given prior text history) and discourse-level translation as well. We train each model on 64 TPUv3s for 200k steps with batch size 128 and Adafactor <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> learning rate 0.001 using the T5X framework <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> (10k steps = <math id="S4.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mo id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">\sim</annotation></semantics></math>1 hour), with validation BLEU as the checkpoint selection criterion. We finetune on How2Sign for at most 10k steps and FLEURS-ASL for at most 100 steps. We first train the model on caption-level data, as in YouTube-ASL, for 100k steps, then mix in the multitask mixture with weight 0.5.<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>This could be seen as a naive kind of context window extension training <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</span></span></span> See Tables <a href="#S4.T2" title="Table 2 ‣ 4.2 Unified Sign to Text Modeling ‣ 4 Baselines ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <a href="#S4.T3" title="Table 3 ‣ 4.2.2 Experiments ‣ 4.2 Unified Sign to Text Modeling ‣ 4 Baselines ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, and <a href="#S4.T4" title="Table 4 ‣ 4.2.2 Experiments ‣ 4.2 Unified Sign to Text Modeling ‣ 4 Baselines ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for quantitative results; see Appendix <a href="#A3" title="Appendix C Disaggregated Analysis ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> for expanded experimental results disaggregated for each individual signer, following <cite class="ltx_cite ltx_citemacro_citet">Tanzer et al. [<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>; and see Table <a href="#S4.T6" title="Table 6 ‣ 4.3 Frontier Models ‣ 4 Baselines ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> for qualitative examples.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">The highlights are that our method performs at least as well as the caption-level baseline at sentence-level translation (3.7 vs. 2.9 BLEU, 37.2 vs. 33.6 BLEURT) while being able to tackle additional tasks. The models improve with signer-independent and then signer-dependent finetuning (but much less than the gap on How2Sign), as well as with additional context.
Results are still far below human performance of 13.0 BLEU (64.6 BLEURT). This approach also shows poor performance on caption alignment—far below a strong model-free baseline where caption duration is scaled in proportion to caption length—because the model can decohere from the original input (especially when captions are long relative to the context window) as the model autoregresses. This is not an issue for translation because it does not have to follow along with a given input track.
</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">discourse-level</span></td>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<th id="S4.T3.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S4.T3.1.2.2.2" class="ltx_td ltx_align_center">untimed</td>
<td id="S4.T3.1.2.2.3" class="ltx_td ltx_align_center">timed</td>
</tr>
<tr id="S4.T3.1.3.3" class="ltx_tr">
<th id="S4.T3.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Ours</th>
<td id="S4.T3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">4.0 (-)</td>
<td id="S4.T3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">3.1 (31.7)</td>
</tr>
<tr id="S4.T3.1.4.4" class="ltx_tr">
<th id="S4.T3.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">Human</th>
<td id="S4.T3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">13.5 (-)</td>
<td id="S4.T3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S4.T3.3.1" class="ltx_text ltx_font_bold">Baseline results for discourse-level translation from ASL to English (untimed vs. timed)</span>. The untimed column is measured in BLEU, because BLEURT is not intended for long outputs, whereas the timed column is measured in timed BLEU (timed BLEURT in parentheses) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. We perform the temporal interpolation for timed BLEU/BLEURT at the character level.</figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">frame-acc</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<th id="S4.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Length scaling</th>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">88.5%</td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<th id="S4.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Ours</th>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb">24.3%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S4.T4.3.1" class="ltx_text ltx_font_bold">Baseline results for ASL-English caption alignment</span>, measured in frame accuracy as in <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. “Length scaling” refers to a model-free baseline where the caption boundaries are linearly interpolated along the video duration according to the length (in characters) of the corresponding text. Our model often decoheres from the input track while autoregressing and therefore drastically underperforms the length rescaling baseline.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Frontier Models</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Finally, we evaluate 3 multimodal frontier models—Claude 3 Opus <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, GPT-4o <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, and Gemini 1.5 Pro <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>—on the same FLEURS-ASL sample we used for the human baseline. While Claude 3 Opus only claims support for image inputs (but accepts up to 20 image inputs), GPT-4o and Gemini 1.5 Pro support video, at least at low frame rates. We prompt the model to “Translate the following video from American Sign Language to English. Give only the translation without any extra explanations.” and upload the given video at 1 Hz, sampling with default parameters. See quantitative results in Table <a href="#S4.T5" title="Table 5 ‣ 4.3 Frontier Models ‣ 4 Baselines ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and qualitative results in Table <a href="#S4.T6" title="Table 6 ‣ 4.3 Frontier Models ‣ 4 Baselines ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. None of the models show any appreciable sign language understanding. Claude 3 sometimes acknowledges that it cannot understand the videos and chooses not to produce a translation.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Overselling technological progress is all too common in the sign language space and is a <a target="_blank" href="https://www.theatlantic.com/technology/archive/2017/11/why-sign-language-gloves-dont-help-deaf-people/545441/" title="" class="ltx_ref ltx_href">point of frustration</a> for the Deaf community. In the brief time that these models have been out, we have personally witnessed multiple independent instances of users testing sign language examples and then declaring that the models must understand sign language, when the examples are either trivial/given away by context or the translations are completely incorrect. We hope that FLEURS-ASL will become a standard addition to frontier model evaluation suites to increase the visibility of sign language tasks and calibrate expectations for them.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">BLEU</th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">BLEURT</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<th id="S4.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GPT-4o</th>
<td id="S4.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.1</td>
<td id="S4.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">32.2</td>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<th id="S4.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Claude 3 Opus</th>
<td id="S4.T5.1.3.2.2" class="ltx_td ltx_align_center">0.4</td>
<td id="S4.T5.1.3.2.3" class="ltx_td ltx_align_center">32.0</td>
</tr>
<tr id="S4.T5.1.4.3" class="ltx_tr">
<th id="S4.T5.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Gemini 1.5 Pro</th>
<td id="S4.T5.1.4.3.2" class="ltx_td ltx_align_center">0.2</td>
<td id="S4.T5.1.4.3.3" class="ltx_td ltx_align_center">20.6</td>
</tr>
<tr id="S4.T5.1.5.4" class="ltx_tr">
<th id="S4.T5.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Human</th>
<td id="S4.T5.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">13.0</td>
<td id="S4.T5.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">64.6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span><span id="S4.T5.3.1" class="ltx_text ltx_font_bold">Performance of frontier multimodal models on sentence-level translation from ASL to English.</span> None of the models demonstrate any meaningful understanding of ASL; the variation in scores reflects artifacts of output length.</figcaption>
</figure>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.1.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" rowspan="4"><span id="S4.T6.1.1.1.1.1" class="ltx_text">(1)</span></td>
<td id="S4.T6.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T6.1.1.1.2.1" class="ltx_text ltx_font_bold">Reference</span></td>
<td id="S4.T6.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T6.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.1.1.3.1.1" class="ltx_p" style="width:261.8pt;">During the 1980s he worked on shows such as Taxi, Cheers, and The Tracy Ullman Show.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.2.2" class="ltx_tr">
<td id="S4.T6.1.2.2.1" class="ltx_td ltx_align_left">Gemini 1.5 Pro</td>
<td id="S4.T6.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.2.2.2.1.1" class="ltx_p" style="width:261.8pt;">I am learning sign language.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.3.3" class="ltx_tr">
<td id="S4.T6.1.3.3.1" class="ltx_td ltx_align_left">Ours (zero-shot)</td>
<td id="S4.T6.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.3.3.2.1.1" class="ltx_p" style="width:261.8pt;">In the 1980s, she worked in theaters like taxesi, cheesy, and tracy.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.4.4" class="ltx_tr">
<td id="S4.T6.1.4.4.1" class="ltx_td ltx_align_left">Ours (signer-indep ft)</td>
<td id="S4.T6.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.4.4.2.1.1" class="ltx_p" style="width:261.8pt;">During the 1980s, he worked in theaters such as Axios, Chruas, and the Tracy Ultra Show.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.5.5" class="ltx_tr">
<td id="S4.T6.1.5.5.1" class="ltx_td"></td>
<td id="S4.T6.1.5.5.2" class="ltx_td ltx_align_left">Ours (signer-dep ft)</td>
<td id="S4.T6.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.5.5.3.1.1" class="ltx_p" style="width:261.8pt;">During the 1980s, he worked in theaters like taxesi, Cheruns, and the Tracy Ullman Show.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.6.6" class="ltx_tr">
<td id="S4.T6.1.6.6.1" class="ltx_td"></td>
<td id="S4.T6.1.6.6.2" class="ltx_td ltx_align_left">Human</td>
<td id="S4.T6.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.6.6.3.1.1" class="ltx_p" style="width:261.8pt;">During the 1980s, they worked in shows like Taxi, Cheers, and The Tracy Ullman show.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.7.7" class="ltx_tr">
<td id="S4.T6.1.7.7.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T6.1.7.7.1.1" class="ltx_text">(2)</span></td>
<td id="S4.T6.1.7.7.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.1.7.7.2.1" class="ltx_text ltx_font_bold">Reference</span></td>
<td id="S4.T6.1.7.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T6.1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.7.7.3.1.1" class="ltx_p" style="width:261.8pt;">The rise of new technologies allows us to see and investigate brain structures and processes never seen before.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.8.8" class="ltx_tr">
<td id="S4.T6.1.8.8.1" class="ltx_td ltx_align_left">Gemini 1.5 Pro</td>
<td id="S4.T6.1.8.8.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.8.8.2.1.1" class="ltx_p" style="width:261.8pt;">You must think! Use your brain!</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.9.9" class="ltx_tr">
<td id="S4.T6.1.9.9.1" class="ltx_td ltx_align_left">Ours (zero-shot)</td>
<td id="S4.T6.1.9.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.9.9.2.1.1" class="ltx_p" style="width:261.8pt;">There is a new technique to detect brains and vision.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.10.10" class="ltx_tr">
<td id="S4.T6.1.10.10.1" class="ltx_td ltx_align_left">Ours (signer-indep ft)</td>
<td id="S4.T6.1.10.10.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.10.10.2.1.1" class="ltx_p" style="width:261.8pt;">The increase in new technologies allows a better understanding of the brain and its visual properties.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.11.11" class="ltx_tr">
<td id="S4.T6.1.11.11.1" class="ltx_td"></td>
<td id="S4.T6.1.11.11.2" class="ltx_td ltx_align_left">Ours (signer-dep ft)</td>
<td id="S4.T6.1.11.11.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.11.11.3.1.1" class="ltx_p" style="width:261.8pt;">The increase in new technology allows the researchers to recognize brain athletics and brain memory.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.12.12" class="ltx_tr">
<td id="S4.T6.1.12.12.1" class="ltx_td"></td>
<td id="S4.T6.1.12.12.2" class="ltx_td ltx_align_left">Human</td>
<td id="S4.T6.1.12.12.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.12.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.12.12.3.1.1" class="ltx_p" style="width:261.8pt;">The rise of new technology allows for investigation of brain structure and processes never seen before.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.13.13" class="ltx_tr">
<td id="S4.T6.1.13.13.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T6.1.13.13.1.1" class="ltx_text">(3)</span></td>
<td id="S4.T6.1.13.13.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.1.13.13.2.1" class="ltx_text ltx_font_bold">Reference</span></td>
<td id="S4.T6.1.13.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T6.1.13.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.13.13.3.1.1" class="ltx_p" style="width:261.8pt;">The Articles required unanimous consent from all the states before they could be amended and states took the central government so lightly that their representatives were often absent.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.14.14" class="ltx_tr">
<td id="S4.T6.1.14.14.1" class="ltx_td ltx_align_left">Gemini 1.5 Pro</td>
<td id="S4.T6.1.14.14.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.14.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.14.14.2.1.1" class="ltx_p" style="width:261.8pt;">It is September. It’s time to go back to school!</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.15.15" class="ltx_tr">
<td id="S4.T6.1.15.15.1" class="ltx_td ltx_align_left">Ours (zero-shot)</td>
<td id="S4.T6.1.15.15.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.15.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.15.15.2.1.1" class="ltx_p" style="width:261.8pt;">The law requires all states to agree on a standard and that it is a legal requirement.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.16.16" class="ltx_tr">
<td id="S4.T6.1.16.16.1" class="ltx_td ltx_align_left">Ours (signer-indep ft)</td>
<td id="S4.T6.1.16.16.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.16.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.16.16.2.1.1" class="ltx_p" style="width:261.8pt;">The law requires that all states must agree on the same legal regulations.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.17.17" class="ltx_tr">
<td id="S4.T6.1.17.17.1" class="ltx_td"></td>
<td id="S4.T6.1.17.17.2" class="ltx_td ltx_align_left">Ours (signer-dep ft)</td>
<td id="S4.T6.1.17.17.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T6.1.17.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.17.17.3.1.1" class="ltx_p" style="width:261.8pt;">The law requires all states to agree on the same regulations that are not legal.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.18.18" class="ltx_tr">
<td id="S4.T6.1.18.18.1" class="ltx_td ltx_border_bb"></td>
<td id="S4.T6.1.18.18.2" class="ltx_td ltx_align_left ltx_border_bb">Human</td>
<td id="S4.T6.1.18.18.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T6.1.18.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.18.18.3.1.1" class="ltx_p" style="width:261.8pt;">The article mandated all states to create a uniform agreement as an amendment to the law for revisions. The states look at the process as minor and many of the state representatives did not make an effort to attend those meetings.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span><span id="S4.T6.3.1" class="ltx_text ltx_font_bold">Qualitative examples from our sentence-level baselines for ASL to English translation on FLEURS-ASL.</span> Examples selected randomly without cherrypicking. In (2), see that Gemini 1.5 Pro does translate “brain”, but this is because the sign is iconic (pointing to the head). See Appendix <a href="#A4" title="Appendix D Complete Human Baseline ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> for the complete set of examples from the human baseline.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we presented FLEURS-ASL, an extension of the standard FLORES/FLEURS translation benchmarks to their first sign language, American Sign Language. FLEURS-ASL distinguishes itself from prior sign language benchmarks not just with its flexibility (supporting multiple tasks and translation directions by virtue of extending a larger benchmark) but also its domain-specific quality considerations (use of Certified Deaf Interpreters translating English text with substantial preparation time, rather than semi-live interpretation following a video). We provided human baselines, domain-specific model baselines (using a new unified multitask approach that operates on random clips), and multimodal frontier model baselines; these collectively show that there is still a long way to go to reach usable performance on ASL to English translation. We hope that FLEURS-ASL will encourage more focus on this task—framed in a way that deemphasizes strictly aligned captions and therefore admits use of noisier datasets—and serve as scaffolding to extend FLORES/FLEURS to even more of the world’s sign languages.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">While FLEURS-ASL improves upon the quality and flexibility of prior evaluations for sign language understanding, it still has a number of limitations, both in language and vision aspects.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In terms of language: The FLORES <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> sentences were originally drawn from Wikipedia and therefore have a limited domain, favoring third-person over first- and second-person (in monologues only; no dialogues) and lacking Deaf-centric vocabulary and topics. This content was translated from English into ASL, which means that (despite efforts to maintain a high quality bar) the content will surely differ linguistically in some respects from spontaneously uttered ASL. As described in Section <a href="#S3.SS1" title="3.1 Data Collection ‣ 3 The FLEURS-ASL Benchmark ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, in order to support sentence-level tasks and caption alignment, we imposed restrictions on reordering of content across sentences that limit discourse structure, and the translation guidelines were tailored primarily to elicit data suitable for evaluating sign language understanding rather than generation.<span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>Different generation domains demand different styles, which are out of scope here. However, given that the content was translated from English into ASL, it is in some sense actually a closer match for generation than understanding.</span></span></span> FLEURS-ASL currently lacks finer-grained annotations than sentence alignments; we hope that the community could contribute these over time if useful. And of course, FLEURS-ASL only covers American Sign Language, one of the <a target="_blank" href="https://www.un.org/en/observances/sign-languages-day" title="" class="ltx_ref ltx_href">&gt;300 sign languages</a> used worldwide.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">In terms of vision: The FLEURS-ASL translations are performed in professional recording environments with clean backgrounds, which means that issues relating to camera quality/perspective or environment complexity are not captured by the benchmark. Our interpreters do not cover all phenotypic variation (e.g., skin tone), so the benchmark cannot guarantee fairness across demographics; this is impossible in a set of only 5 signers anyway. We circumvent this problem somewhat in our baselines by using MediaPipe Holistic, which has its own fairness evaluations in its model cards <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, but the benchmark is intended to be method-agnostic. Many variations in physical appearance correspond with sociolects (e.g., race, gender, and age <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>), so this is not strictly a vision problem. Note that modeling poses of the kind produced by MediaPipe Holistic is also a fundamentally flawed approach in general because it does not capture semantically relevant features like the tongue (and from the perspective of fairness, cannot handle people with atypically numbered or formed fingers and hands).</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Additionally, in terms of truly zero-shot evaluation: Because FLORES/FLEURS is seeded from Wikipedia and widely used for finetuning or evaluation, the text/speech references for FLEURS-ASL may be contaminated in pretraining data. The FLEURS-ASL interpreters may also appear in public data, such as on YouTube, due to the limited pool of suitably qualified candidiates.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The interpreters who translated FLEURS-ASL were paid at market rates and consented to their video translations being published for use in machine learning research by the broader community. While facial expressions are essential to sign language grammar and therefore must be included in the dataset, we ask that you blur the interpreters’ faces when including examples in publications (as we do in this paper). You should not attempt to reidentify the interpreters or use their likenesses to generate and publish other content (deepfakes).</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">We would like to thank Sean Forbes, Sam Sepah, and Manfred Georg for contributions to the data collection logistics; Thad Starner, Caroline Pantofaru, Hadar Shemtov, and many others for institutional support; David Uthus and the T5X team for general help with infrastructure; Chris Dyer and Caroline Pantofaru for giving feedback on drafts of this paper; and of course the FLEURS-ASL interpreters and annotators for their participation.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/google/mediapipe/blob/master/docs/solutions/models.md" title="" class="ltx_ref ltx_href">Mediapipe models and model cards</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Albanie et al. [2021]</span>
<span class="ltx_bibblock">
Samuel Albanie, Gül Varol, Liliane Momeni, Hannah Bull, Triantafyllos Afouras,
Himel Chowdhury, Neil Fox, Bencie Woll, Rob Cooper, Andrew McParland, and
Andrew Zisserman. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2111.03635" title="" class="ltx_ref ltx_href">Bbc-oxford british
sign language dataset</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic [2024]</span>
<span class="ltx_bibblock">
Anthropic. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf" title="" class="ltx_ref ltx_href">The claude 3 model family: Opus, sonnet, haiku</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bandarkar et al. [2023]</span>
<span class="ltx_bibblock">
Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan
Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and
Madian Khabsa. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.16884" title="" class="ltx_ref ltx_href">The belebele benchmark: a
parallel reading comprehension dataset in 122 language variants</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bapna et al. [2022]</span>
<span class="ltx_bibblock">
Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya
Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey,
Theresa Breiner, Vera Axelrod, Jason Riesa, Yuan Cao, Mia Xu Chen, Klaus
Macherey, Maxim Krikun, Pidong Wang, Alexander Gutkin, Apurva Shah, Yanping
Huang, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2205.03983" title="" class="ltx_ref ltx_href">Building machine translation
systems for the next thousand languages</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bawden et al. [2018]</span>
<span class="ltx_bibblock">
Rachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N18-1118" title="" class="ltx_ref ltx_href">Evaluating discourse
phenomena in neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers)</em>, pages 1304–1313, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bull et al. [2021]</span>
<span class="ltx_bibblock">
Hannah Bull, Triantafyllos Afouras, Gül Varol, Samuel Albanie, Liliane Momeni,
and Andrew Zisserman. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2105.02877" title="" class="ltx_ref ltx_href">Aligning subtitles in sign
language videos</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Camgoz et al. [2018]</span>
<span class="ltx_bibblock">
Necati Cihan Camgoz, Simon Hadfield, Oscar Koller, Hermann Ney, and Richard
Bowden. 2018.

</span>
<span class="ltx_bibblock">Neural sign language translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ceil Lucas and Bayley [2001]</span>
<span class="ltx_bibblock">
Clayton Valli Ceil Lucas and Robert Bayley. 2001.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Sociolinguistic Variation in American Sign Language</em>.

</span>
<span class="ltx_bibblock">Gallaudet University Press.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023]</span>
<span class="ltx_bibblock">
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2306.15595" title="" class="ltx_ref ltx_href">Extending context window of
large language models via positional interpolation</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cherry et al. [2021]</span>
<span class="ltx_bibblock">
Colin Cherry, Naveen Arivazhagan, Dirk Padfield, and Maxim Krikun. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/Interspeech.2021-744" title="" class="ltx_ref ltx_href">Subtitle
Translation as Markup Translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2021</em>, pages 2237–2241.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. [2022]</span>
<span class="ltx_bibblock">
Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth
Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2205.12446" title="" class="ltx_ref ltx_href">Fleurs: Few-shot learning
evaluation of universal representations of speech</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussà et al. [2022]</span>
<span class="ltx_bibblock">
Marta R. Costa-jussà, Christine Basta, and Gerard I. Gállego. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2010.14465" title="" class="ltx_ref ltx_href">Evaluating gender bias in
speech translation</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De Coster et al. [2021]</span>
<span class="ltx_bibblock">
Mathieu De Coster, Karel D’Oosterlinck, Marija Pizurica, Paloma Rabaey,
Severine Verlinden, Mieke Van Herreweghe, and Joni Dambre. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2021.mtsummit-at4ssl.10" title="" class="ltx_ref ltx_href">Frozen
pretrained transformers for neural sign language translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st International Workshop on Automatic
Translation for Signed and Spoken Languages (AT4SSL)</em>, pages 88–97, Virtual.
Association for Machine Translation in the Americas.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duarte et al. [2021]</span>
<span class="ltx_bibblock">
Amanda Duarte, Shruti Palaskar, Lucas Ventura, Deepti Ghadiyaram, Kenneth
DeHaan, Florian Metze, Jordi Torres, and Xavier Giro-i Nieto. 2021.

</span>
<span class="ltx_bibblock">How2Sign: A Large-scale Multimodal Dataset for Continuous American
Sign Language.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Conference on Computer Vision and Pattern Recognition
(CVPR)</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. [2021]</span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek,
Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and
Angela Fan. 2021.

</span>
<span class="ltx_bibblock">The flores-101 evaluation benchmark for low-resource and multilingual
machine translation.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grishchenko and Bazarevsky [2020]</span>
<span class="ltx_bibblock">
Ivan Grishchenko and Valentin Bazarevsky. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ai.googleblog.com/2020/12/mediapipe-holistic-simultaneous-face.html" title="" class="ltx_ref ltx_href">Mediapipe holistic - simultaneous face, hand and pose prediction, on
device</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gueuwou et al. [2024]</span>
<span class="ltx_bibblock">
Shester Gueuwou, Xiaodan Du, Greg Shakhnarovich, and Karen Livescu. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2406.06907" title="" class="ltx_ref ltx_href">Signmusketeers: An efficient
multi-stream approach for sign language translation at scale</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gueuwou et al. [2023]</span>
<span class="ltx_bibblock">
Shester Gueuwou, Sophie Siake, Colin Leong, and Mathias Müller. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2311.10174" title="" class="ltx_ref ltx_href">Jwsign: A highly
multilingual corpus of bible translations for more diversity in sign language
processing</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guzmán et al. [2019]</span>
<span class="ltx_bibblock">
Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume Lample,
Philipp Koehn, Vishrav Chaudhary, and Marc’Aurelio Ranzato. 2019.

</span>
<span class="ltx_bibblock">Two new evaluation datasets for low-resource machine translation:
Nepali-english and sinhala-english.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hanke et al. [2020]</span>
<span class="ltx_bibblock">
Thomas Hanke, Marc Schulder, Reiner Konrad, and Elena Jahn. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2020.signlang-1.12" title="" class="ltx_ref ltx_href">Extending the
Public DGS Corpus in size and depth</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the LREC2020 9th Workshop on the
Representation and Processing of Sign Languages: Sign Language Resources in
the Service of the Language Community, Technological Challenges and
Application Perspectives</em>, pages 75–82, Marseille, France. European Language
Resources Association (ELRA).

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2021]</span>
<span class="ltx_bibblock">
Hezhen Hu, Weichao Zhao, Wengang Zhou, Yuechen Wang, and Houqiang Li. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2110.05382" title="" class="ltx_ref ltx_href">Signbert: Pre-training of
hand-model-aware representation for sign language recognition</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. [2023]</span>
<span class="ltx_bibblock">
Abhinav Joshi, Susmit Agrawal, and Ashutosh Modi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2307.05440" title="" class="ltx_ref ltx_href">Isltranslate: Dataset for
translating indian sign language</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lugaresi et al. [2019]</span>
<span class="ltx_bibblock">
Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja,
Michael Hays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee,
Wan-Teh Chang, Wei Hua, Manfred Georg, and Matthias Grundmann. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1906.08172" title="" class="ltx_ref ltx_href">MediaPipe: A framework for
building perception pipelines</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moryossef et al. [2021]</span>
<span class="ltx_bibblock">
Amit Moryossef, Ioannis Tsochantaridis, Joe Dinn, Necati Cihan Camgöz, Richard
Bowden, Tao Jiang, Annette Rios, Mathias Müller, and Sarah Ebling. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2104.10166" title="" class="ltx_ref ltx_href">Evaluating the immediate
applicability of pose estimation for sign language recognition</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Müller et al. [2023]</span>
<span class="ltx_bibblock">
Mathias Müller, Malihe Alikhani, Eleftherios Avramidis, et al. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.wmt-1.4" title="" class="ltx_ref ltx_href">Findings of the
second WMT shared task on sign language translation (WMT-SLT23)</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eighth Conference on Machine
Translation</em>, pages 68–94, Singapore. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Müller et al. [2022]</span>
<span class="ltx_bibblock">
Mathias Müller, Sarah Ebling, Eleftherios Avramidis, Alessia Battisti,
Michèle Berger, Richard Bowden, Annelies Braffort, Necati
Cihan Camgöz, Cristina España-bonet, Roman Grundkiewicz, Zifan Jiang,
Oscar Koller, Amit Moryossef, Regula Perrollaz, Sabine Reinhard, Annette
Rios, Dimitar Shterionov, Sandra Sidler-miserez, and Katja Tissi. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.wmt-1.71" title="" class="ltx_ref ltx_href">Findings of the first
WMT shared task on sign language translation (WMT-SLT22)</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Seventh Conference on Machine Translation
(WMT)</em>, pages 744–772, Abu Dhabi, United Arab Emirates (Hybrid). Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NLLB Team [2022]</span>
<span class="ltx_bibblock">
James Cross Onur Çelebi Maha Elbayad Kenneth Heafield Kevin Heffernan Elahe
Kalbassi Janice Lam Daniel Licht Jean Maillard Anna Sun Skyler Wang Guillaume
Wenzek Al Youngblood Bapi Akula Loic Barrault Gabriel Mejia Gonzalez
Prangthip Hansanti John Hoffman Semarley Jarrett Kaushik Ram Sadagopan Dirk
Rowe Shannon Spruit Chau Tran Pierre Andrews Necip Fazil Ayan Shruti Bhosale
Sergey Edunov Angela Fan Cynthia Gao Vedanuj Goswami Francisco Guzmán
Philipp Koehn Alexandre Mourachko Christophe Ropers Safiyyah Saleem Holger
Schwenk Jeff Wang NLLB Team, Marta R. Costa-jussà. 2022.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2024]</span>
<span class="ltx_bibblock">
OpenAI. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/gpt-4o-contributions/" title="" class="ltx_ref ltx_href">Gpt-4o</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. [2002]</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/1073083.1073135" title="" class="ltx_ref ltx_href">Bleu: a method for
automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. [2023]</span>
<span class="ltx_bibblock">
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2309.00071" title="" class="ltx_ref ltx_href">Yarn: Efficient context
window extension of large language models</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post [2018]</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aclweb.org/anthology/W18-6319" title="" class="ltx_ref ltx_href">A call for clarity
in reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 186–191, Belgium, Brussels. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pu et al. [2021]</span>
<span class="ltx_bibblock">
Amy Pu, Hyung Won Chung, Ankur P Parikh, Sebastian Gehrmann, and Thibault
Sellam. 2021.

</span>
<span class="ltx_bibblock">Learning compact metrics for mt.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of EMNLP</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2022]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
Ilya Sutskever. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2212.04356" title="" class="ltx_ref ltx_href">Robust speech recognition
via large-scale weak supervision</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. [2020]</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1910.10683" title="" class="ltx_ref ltx_href">Exploring the limits of
transfer learning with a unified text-to-text transformer</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts et al. [2022]</span>
<span class="ltx_bibblock">
Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury,
Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin,
Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin,
Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko,
Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo
Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan
Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten
Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick,
Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea
Gesmundo. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2203.17189" title="" class="ltx_ref ltx_href">Scaling up models and data
with <span id="bib.bib36.3.3.1" class="ltx_text ltx_markedasmath ltx_font_typewriter">t5x</span> and <span id="bib.bib36.4.4.2" class="ltx_text ltx_markedasmath ltx_font_typewriter">seqio</span></a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.17189</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rust et al. [2024]</span>
<span class="ltx_bibblock">
Phillip Rust, Bowen Shi, Skyler Wang, Necati Cihan Camgöz, and Jean Maillard.
2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.09611" title="" class="ltx_ref ltx_href">Towards privacy-aware sign
language translation at scale</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sellam et al. [2020]</span>
<span class="ltx_bibblock">
Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2004.04696" title="" class="ltx_ref ltx_href">Bleurt: Learning robust
metrics for text generation</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer and Stern [2018]</span>
<span class="ltx_bibblock">
Noam Shazeer and Mitchell Stern. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1804.04235" title="" class="ltx_ref ltx_href">Adafactor: Adaptive learning
rates with sublinear memory cost</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. [2022]</span>
<span class="ltx_bibblock">
Bowen Shi, Diane Brentari, Greg Shakhnarovich, and Karen Livescu. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2205.12870" title="" class="ltx_ref ltx_href">Open-domain sign
language translation learned from online video</a>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sincan et al. [2023]</span>
<span class="ltx_bibblock">
Ozge Mercanoglu Sincan, Necati Cihan Camgoz, and Richard Bowden. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.09622" title="" class="ltx_ref ltx_href">Is context all you need?
scaling neural sign language translation to large domains of discourse</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stanovsky et al. [2019]</span>
<span class="ltx_bibblock">
Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1906.00591" title="" class="ltx_ref ltx_href">Evaluating gender bias in
machine translation</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tanzer et al. [2024a]</span>
<span class="ltx_bibblock">
Garrett Tanzer, Maximus Shengelia, Ken Harrenstien, and David Uthus.
2024a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2406.11049" title="" class="ltx_ref ltx_href">Reconsidering sentence-level
sign language translation</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tanzer et al. [2024b]</span>
<span class="ltx_bibblock">
Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, and Luke
Melas-Kyriazi. 2024b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2309.16575" title="" class="ltx_ref ltx_href">A benchmark for learning to
translate a new language from one grammar book</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team [2024]</span>
<span class="ltx_bibblock">
Gemini Team. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2403.05530" title="" class="ltx_ref ltx_href">Gemini 1.5: Unlocking
multimodal understanding across millions of tokens of context</a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Uthus et al. [2023]</span>
<span class="ltx_bibblock">
David Uthus, Garrett Tanzer, and Manfred Georg. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2306.15162" title="" class="ltx_ref ltx_href">Youtube-asl: A large-scale,
open-domain american sign language-english parallel corpus</a>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. [2021]</span>
<span class="ltx_bibblock">
Kayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, and Malihe Alikhani.
2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.570" title="" class="ltx_ref ltx_href">Including
signed languages in natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 7347–7360,
Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin and Read [2020]</span>
<span class="ltx_bibblock">
Kayo Yin and Jesse Read. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2004.00588" title="" class="ltx_ref ltx_href">Better sign language
translation with stmc-transformer</a>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023]</span>
<span class="ltx_bibblock">
Biao Zhang, Mathias Müller, and Rico Sennrich. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.01778" title="" class="ltx_ref ltx_href">Sltunet: A simple unified
model for sign language translation</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Translation Guidance</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">While sign language interpreting and translation is a mature field with comprehensive training and certification processes, the desiderata for a machine translation benchmark are somewhat unique compared to the variety of contexts in which interpreting and translation are typically performed. We gave the following guidance to the interpreters to consistently elicit the kind of content we were looking for (not necessarily verbatim; some guidance was provided live during onboarding or as feedback).</p>
</div>
<div id="A1.p2" class="ltx_para">
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">This is a challenging set of content to translate, with relatively short narratives about many different topics. We expect that you will spend 5-6 hours preparing the translations for every 1 hour of recorded content to ensure a high quality standard.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">Try to sign as naturally as possible. We are looking for content that looks like it was originally signed in ASL rather than translated, as much as possible. For example, this means that you don’t need to add explanations of concepts or entities that aren’t present in the source content, like you might for a translation of a public service announcement. You should sign as you would in a professional environment to another signer with similar background to you.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">Implied by this, do not leave intentional pauses between sentences.</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p">However, you should not reorder content between sentences. i.e., You should be able to take the original sentences and align them to your translation as a caption track, and the captions should be a reasonable translation of the signed content.</p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p">If there is any ambiguity in the English sentence when you’re translating into ASL, act as if the information was already known from context. For example, if the sentence starts with “he”, you should not fingerspell “he” to clarify the gender, but instead just point and assume that gender is known from context.</p>
</div>
</li>
<li id="A1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i6.p1" class="ltx_para">
<p id="A1.I1.i6.p1.1" class="ltx_p">If there is anything that needs to be specified when translating into ASL, pick one interpretation and stick with it throughout the narrative. For example, if the sentence is describing a “crash”, without specifying what kind of vehicle crashed (car, plane, etc.), pick the most reasonable interpretation given the context and go with it.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Human Baseline Instructions</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">Your task today will be to perform ASL<math id="A2.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A2.p1.1.m1.1a"><mo stretchy="false" id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><ci id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">\rightarrow</annotation></semantics></math>English translation tasks on a sample of 40 videos we provide to you. We will use/release these translations so that we and the broader AI research community can compare how well AI translates ASL vs. a human (and get a better understanding of its limitations).</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p"><span id="A2.p2.1.1" class="ltx_text ltx_font_bold">You’re allowed to watch the video multiple times, slow down the video, etc. to help you.</span> But don’t agonize too much over it.</p>
</div>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.1" class="ltx_p"><span id="A2.p3.1.1" class="ltx_text ltx_font_bold">You must perform the tasks in a particular order, in order to ensure the validity of the results.</span> Specifically, we will ask you to:</p>
</div>
<div id="A2.p4" class="ltx_para">
<ul id="A2.I1" class="ltx_itemize">
<li id="A2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="A2.I1.ix1.p1" class="ltx_para">
<p id="A2.I1.ix1.p1.1" class="ltx_p">Look at a clip containing 1 sentence, and translate that clip.</p>
</div>
</li>
<li id="A2.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="A2.I1.ix2.p1" class="ltx_para">
<p id="A2.I1.ix2.p1.1" class="ltx_p">Look at a full video containing that sentence plus others around it, and translate the entire video given the additional context.</p>
</div>
</li>
</ul>
</div>
<div id="A2.p5" class="ltx_para">
<p id="A2.p5.1" class="ltx_p">First, we’ll go through an example.</p>
</div>
<div id="A2.p6" class="ltx_para">
<p id="A2.p6.1" class="ltx_p">a) Watch the following clip. The task is to translate the clip. My translation of the clip is:</p>
</div>
<div id="A2.p7" class="ltx_para">
<p id="A2.p7.1" class="ltx_p"><span id="A2.p7.1.1" class="ltx_text ltx_font_italic">If you stand at the edge of the water, you’ll look down to the bottom and see pebbles and gunk.</span></p>
</div>
<div id="A2.p8" class="ltx_para">
<p id="A2.p8.1" class="ltx_p">b) Watch the following clip. The task is to translate the entire video. (Note that in light of additional context, you may refine your translation from step a for step b.) My translation of the video:</p>
</div>
<div id="A2.p9" class="ltx_para">
<p id="A2.p9.1" class="ltx_p"><span id="A2.p9.1.1" class="ltx_text ltx_font_italic">It will behave similar to water. It’s transparent like water. If you stand at the shore and look down, you’ll see through it to the pebbles and gunk on the bottom. Stofan adds, “As far as we know, only one planet is more active than Titan: the planet Earth.”</span></p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Disaggregated Analysis</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">First, some subjective observations about the different signers’ styles:</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p">Signers #0, #3, and #4 tend to sign closer to the original English syntax when there is no strong reason not to, whereas signers #1 and #2 more freely restructure the sentences. Some individual points of distinction:</p>
</div>
<div id="A3.p3" class="ltx_para">
<ul id="A3.I1" class="ltx_itemize">
<li id="A3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i1.p1" class="ltx_para">
<p id="A3.I1.i1.p1.1" class="ltx_p">Signer #0 performs a swish to separate individual words in a fingerspelled phrase, which is uncommon.</p>
</div>
</li>
<li id="A3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i2.p1" class="ltx_para">
<p id="A3.I1.i2.p1.1" class="ltx_p">Signer #1 tends to use more rare signs (such as cities/countries) and vanguard language practices (such as using the endogenous BSL sign for England rather than the traditional ASL sign).</p>
</div>
</li>
<li id="A3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i3.p1" class="ltx_para">
<p id="A3.I1.i3.p1.1" class="ltx_p">Signer #2 signs the slowest.</p>
</div>
</li>
<li id="A3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i4.p1" class="ltx_para">
<p id="A3.I1.i4.p1.1" class="ltx_p">Signer #3 signs the fastest.</p>
</div>
</li>
<li id="A3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i5.p1" class="ltx_para">
<p id="A3.I1.i5.p1.1" class="ltx_p">Signer #4 signs primarily left-handed, but is more ambidextrous than the others.</p>
</div>
</li>
</ul>
</div>
<div id="A3.p4" class="ltx_para">
<p id="A3.p4.1" class="ltx_p">Second, see Table <a href="#A3.T7" title="Table 7 ‣ Appendix C Disaggregated Analysis ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> for disaggregated sentence-level ASL to English translation results. As expected due to our more stringent set of requirements for recruiting interpreters, the human baseline scores (especially BLEURT) in FLEURS-ASL are more consistent across signers than the scores on How2Sign provided by <cite class="ltx_cite ltx_citemacro_citet">Tanzer et al. [<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. The BLEURT scores are also higher overall; we speculate that this is due factors such as the source sentences being longer, the translations into ASL being higher quality/more faithful, and the reference domain being more neutral. The variation in scores that remains in FLEURS-ASL does not seem to reflect underlying translation quality, but just the depth of the translation (and e.g., amount of fingerspelling).</p>
</div>
<figure id="A3.T7" class="ltx_table">
<table id="A3.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T7.1.1.1" class="ltx_tr">
<th id="A3.T7.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="A3.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">sentence-level</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T7.1.2.1" class="ltx_tr">
<th id="A3.T7.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Caption-level</th>
<td id="A3.T7.1.2.1.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A3.T7.1.3.2" class="ltx_tr">
<th id="A3.T7.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #0</th>
<td id="A3.T7.1.3.2.2" class="ltx_td ltx_align_center">3.6 (34.8)</td>
</tr>
<tr id="A3.T7.1.4.3" class="ltx_tr">
<th id="A3.T7.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #1</th>
<td id="A3.T7.1.4.3.2" class="ltx_td ltx_align_center">2.0 (31.8)</td>
</tr>
<tr id="A3.T7.1.5.4" class="ltx_tr">
<th id="A3.T7.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #2</th>
<td id="A3.T7.1.5.4.2" class="ltx_td ltx_align_center">3.6 (35.7)</td>
</tr>
<tr id="A3.T7.1.6.5" class="ltx_tr">
<th id="A3.T7.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #3</th>
<td id="A3.T7.1.6.5.2" class="ltx_td ltx_align_center">4.2 (36.2)</td>
</tr>
<tr id="A3.T7.1.7.6" class="ltx_tr">
<th id="A3.T7.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #4</th>
<td id="A3.T7.1.7.6.2" class="ltx_td ltx_align_center">3.9 (33.8)</td>
</tr>
<tr id="A3.T7.1.8.7" class="ltx_tr">
<th id="A3.T7.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Ours</th>
<td id="A3.T7.1.8.7.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A3.T7.1.9.8" class="ltx_tr">
<th id="A3.T7.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #0</th>
<td id="A3.T7.1.9.8.2" class="ltx_td ltx_align_center">5.1 (39.1)</td>
</tr>
<tr id="A3.T7.1.10.9" class="ltx_tr">
<th id="A3.T7.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #1</th>
<td id="A3.T7.1.10.9.2" class="ltx_td ltx_align_center">2.4 (35.1)</td>
</tr>
<tr id="A3.T7.1.11.10" class="ltx_tr">
<th id="A3.T7.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #2</th>
<td id="A3.T7.1.11.10.2" class="ltx_td ltx_align_center">4.3 (38.8)</td>
</tr>
<tr id="A3.T7.1.12.11" class="ltx_tr">
<th id="A3.T7.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #3</th>
<td id="A3.T7.1.12.11.2" class="ltx_td ltx_align_center">5.3 (39.5)</td>
</tr>
<tr id="A3.T7.1.13.12" class="ltx_tr">
<th id="A3.T7.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #4</th>
<td id="A3.T7.1.13.12.2" class="ltx_td ltx_align_center">4.7 (39.1)</td>
</tr>
<tr id="A3.T7.1.14.13" class="ltx_tr">
<th id="A3.T7.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Human</th>
<td id="A3.T7.1.14.13.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A3.T7.1.15.14" class="ltx_tr">
<th id="A3.T7.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #0</th>
<td id="A3.T7.1.15.14.2" class="ltx_td ltx_align_center">12.1 (63.4)</td>
</tr>
<tr id="A3.T7.1.16.15" class="ltx_tr">
<th id="A3.T7.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #1</th>
<td id="A3.T7.1.16.15.2" class="ltx_td ltx_align_center">9.0 (60.6)</td>
</tr>
<tr id="A3.T7.1.17.16" class="ltx_tr">
<th id="A3.T7.1.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #2</th>
<td id="A3.T7.1.17.16.2" class="ltx_td ltx_align_center">6.5 (63.1)</td>
</tr>
<tr id="A3.T7.1.18.17" class="ltx_tr">
<th id="A3.T7.1.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">      #3</th>
<td id="A3.T7.1.18.17.2" class="ltx_td ltx_align_center">16.1 (68.5)</td>
</tr>
<tr id="A3.T7.1.19.18" class="ltx_tr">
<th id="A3.T7.1.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">      #4</th>
<td id="A3.T7.1.19.18.2" class="ltx_td ltx_align_center ltx_border_bb">21.7 (67.6)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span><span id="A3.T7.3.1" class="ltx_text ltx_font_bold">Results for zero-shot sentence-level translation from ASL to English disaggregated by signer</span>, measured in BLEU (BLEURT in parentheses).</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Complete Human Baseline</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">See Table <a href="#A4.T8" title="Table 8 ‣ Appendix D Complete Human Baseline ‣ FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> for the complete set of FLEURS-ASL human baseline translations, also available at <a target="_blank" href="https://www.kaggle.com/datasets/googleai/fleurs-asl" title="" class="ltx_ref ltx_href">this link</a>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A4.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span><span id="A4.T8.2.1" class="ltx_text ltx_font_bold">Complete set of FLEURS-ASL human baseline translations</span>. We provide one sentence-level translation (performed in isolation) and then the complete discourse-level translation for each document sequentially.</figcaption>
<table id="A4.T8.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T8.3.1.1" class="ltx_tr">
<th id="A4.T8.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A4.T8.3.1.1.1.1" class="ltx_text ltx_font_bold">signer</span></th>
<th id="A4.T8.3.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.T8.3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.1.1.2.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.T8.3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">reference</span></span>
</span>
</th>
<th id="A4.T8.3.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.T8.3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.1.1.3.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.T8.3.1.1.3.1.1.1" class="ltx_text ltx_font_bold">human prediction</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T8.3.2.1" class="ltx_tr">
<th id="A4.T8.3.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">0</th>
<td id="A4.T8.3.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.T8.3.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.2.1.2.1.1" class="ltx_p" style="width:199.2pt;">As NSA, he assisted Carter in diplomatically handling world affairs, such as the Camp David Accords, 1978; normalizing US–China relations thought the late 1970s; the Iranian Revolution, which led to the Iran hostage crisis, 1979; and the Soviet invasion in Afghanistan, 1979.</span>
</span>
</td>
<td id="A4.T8.3.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.T8.3.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.2.1.3.1.1" class="ltx_p" style="width:199.2pt;">NSL’s work role supported Carter’s cabinet to handle world events. For example, Camp David Accords of 1978, National Customs of USA and China relations in late 1970s, the Iraqi revolution that resulted in the Iraqi hostage crisis in 1979, and the Soviet takeover of Afghanistan in 1979.</span>
</span>
</td>
</tr>
<tr id="A4.T8.3.3.2" class="ltx_tr">
<th id="A4.T8.3.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.T8.3.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.T8.3.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.3.2.2.1.1" class="ltx_p" style="width:199.2pt;">Throughout 1960s, Brzezinski worked for John F. Kennedy as his advisor and then the Lyndon B. Johnson administration. During the 1976 selections he advised Carter on foreign policy, then served as National Security Advisor (NSA) from 1977 to 1981, succeeding Henry Kissinger. As NSA, he assisted Carter in diplomatically handling world affairs, such as the Camp David Accords, 1978; normalizing US–China relations thought the late 1970s; the Iranian Revolution, which led to the Iran hostage crisis, 1979; and the Soviet invasion in Afghanistan, 1979.</span>
</span>
</td>
<td id="A4.T8.3.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.T8.3.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.3.2.3.1.1" class="ltx_p" style="width:199.2pt;">In the 1960s, a person named Burzezinski worked for John K. F. Kennedy in his role as an advisor. His role as advisor continued with the next president, Lyndon B Johnson. During the 1976 selection, he advised Carter on foreign policy and then served a role in NSA, National Security Advisory from 1977 to 1981, and then eventually handed his role over to Henry Kissinger. His role in NSA was to support Carter in policies handling world events. For example, Camp David Accords of 1978, National Customs between USA and China relations in late 1970s, Iraqi war that resulted in the Iraqi hostage crisis in 1979, and the Soviet takeover of Afghanistan in 1979.</span>
</span>
</td>
</tr>
<tr id="A4.T8.3.4.3" class="ltx_tr">
<th id="A4.T8.3.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">0</th>
<td id="A4.T8.3.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.T8.3.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.4.3.2.1.1" class="ltx_p" style="width:199.2pt;">Late on Sunday, the United States President Donald Trump, in a statement delivered via the press secretary, announced US troops would be leaving Syria.</span>
</span>
</td>
<td id="A4.T8.3.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.T8.3.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.4.3.3.1.1" class="ltx_p" style="width:199.2pt;">All day into the night, late at night Sunday, the United States president Donald Trump sent a note to the press secretary to announce that the US troops will go overseas to Syria.</span>
</span>
</td>
</tr>
<tr id="A4.T8.3.5.4" class="ltx_tr">
<th id="A4.T8.3.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.T8.3.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.T8.3.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.5.4.2.1.1" class="ltx_p" style="width:199.2pt;">Late on Sunday, the United States President Donald Trump, in a statement delivered via the press secretary, announced US troops would be leaving Syria. The announcement was made after Trump had a phone conversation with Turkish President Recep Tayyip Erdoğan. Turkey would also take over guarding captured ISIS fighters which, the statement said, European nations have refused to repatriate.</span>
</span>
</td>
<td id="A4.T8.3.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.T8.3.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.5.4.3.1.1" class="ltx_p" style="width:199.2pt;">All day into the night, late Sunday, president Donald Trump sent a note to the press secretary to announce that US troops will leave Syria. After the announcement, Trump met with Turkey’s president Recep Tayyip Erdogăn. Turkey will also take responsibility in watching over the ISIS militant captives. There was a note that European countries will not allow these captives back into their countries.</span>
</span>
</td>
</tr>
<tr id="A4.T8.3.6.5" class="ltx_tr">
<th id="A4.T8.3.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">0</th>
<td id="A4.T8.3.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.T8.3.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.6.5.2.1.1" class="ltx_p" style="width:199.2pt;">Mrs. Kirchner announced her intention to run for president at the Argentine Theatre, the same location she used to start her 2005 campaign for the Senate as member of the Buenos Aires province delegation.</span>
</span>
</td>
<td id="A4.T8.3.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.T8.3.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.6.5.3.1.1" class="ltx_p" style="width:199.2pt;">They announced their intention of their campaign for president at a guitar theater. Understand that the guitar theater is the same area as the 2005 campaign as a senator. Same member started the regional president demonstration.</span>
</span>
</td>
</tr>
<tr id="A4.T8.3.7.6" class="ltx_tr">
<th id="A4.T8.3.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.T8.3.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.T8.3.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.7.6.2.1.1" class="ltx_p" style="width:199.2pt;">Current senator and Argentine First Lady Cristina Fernandez de Kirchner announced her presidential candidacy yesterday evening in La Plata, a city 50 kilometers (31 miles) away from Buenos Aires. Mrs. Kirchner announced her intention to run for president at the Argentine Theatre, the same location she used to start her 2005 campaign for the Senate as member of the Buenos Aires province delegation.</span>
</span>
</td>
<td id="A4.T8.3.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.T8.3.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.7.6.3.1.1" class="ltx_p" style="width:199.2pt;">The current senate is also the Argentine president’s wife, Cristina Fernandez de Kirchner. She announced presidential candidacy last night at La Plata about 50 km, or 31 miles, from Buenos Aires. She announced her intention of running for presidency at a theater. Understand the theater is the same place where she announced she was running for candidacy for senate in 2005 as a member of Buenos Aires Local President Demonstration.</span>
</span>
</td>
</tr>
<tr id="A4.T8.3.8.7" class="ltx_tr">
<th id="A4.T8.3.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">0</th>
<td id="A4.T8.3.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.T8.3.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.8.7.2.1.1" class="ltx_p" style="width:199.2pt;">The National Hurricane Center (NHC) says that at this point Jerry poses no threat to land.</span>
</span>
</td>
<td id="A4.T8.3.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.T8.3.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.8.7.3.1.1" class="ltx_p" style="width:199.2pt;">NHC, National Hurricane Center, commented that today Jerry was not severe in affecting the land.</span>
</span>
</td>
</tr>
<tr id="A4.T8.3.9.8" class="ltx_tr">
<th id="A4.T8.3.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t"></th>
<td id="A4.T8.3.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.T8.3.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.9.8.2.1.1" class="ltx_p" style="width:199.2pt;">The tenth named storm of the Atlantic Hurricane season, Subtropical Storm Jerry, formed in the Atlantic Ocean today. The National Hurricane Center (NHC) says that at this point Jerry poses no threat to land.</span>
</span>
</td>
<td id="A4.T8.3.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.T8.3.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.3.9.8.3.1.1" class="ltx_p" style="width:199.2pt;">The tenth storm during the Atlantic Hurricane season, named Jerry, was first named as a subtropical storm in the Atlantic ocean. NHC, National Hurricane Center, commented today that Jerry was not severe.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A4.tab1" class="ltx_table">
<table id="A4.tab1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.tab1.1.1.1" class="ltx_tr">
<th id="A4.tab1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A4.tab1.1.1.1.1.1" class="ltx_text ltx_font_bold">signer</span></th>
<th id="A4.tab1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.1.1.2.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">reference</span></span>
</span>
</th>
<th id="A4.tab1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.1.1.3.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">human prediction</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.tab1.1.2.1" class="ltx_tr">
<th id="A4.tab1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">0</th>
<td id="A4.tab1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.2.1.2.1.1" class="ltx_p" style="width:199.2pt;">According to the lawsuit, waste from the UN camp was not properly sanitized, causing bacteria to enter the tributary of the Artibonite River, one of Haiti’s largest.</span>
</span>
</td>
<td id="A4.tab1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.2.1.3.1.1" class="ltx_p" style="width:199.2pt;">The lawsuit follows the United Nations camp and their waste as it is not properly sanitized. That caused the pollution to enter the Artibonite River, which is one of Haiti’s largest river.</span>
</span>
</td>
</tr>
<tr id="A4.tab1.1.3.2" class="ltx_tr">
<th id="A4.tab1.1.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.3.2.2.1.1" class="ltx_p" style="width:199.2pt;">UN peacekeepers, whom arrived in Haiti after the 2010 earthquake, are being blamed for the spread of the disease which started near the troop’s encampment. According to the lawsuit, waste from the UN camp was not properly sanitized, causing bacteria to enter the tributary of the Artibonite River, one of Haiti’s largest. Prior to the arrival of troops, Haiti had not encountered problems related to the disease since the 1800s. The Haitian Institute for Justice and Democracy has referenced independent studies that suggest the Nepalese UN peacekeeping battalion unknowingly brought the disease to Haiti. Danielle Lantagne, a UN expert on the disease, stated the outbreak was likely caused by the peacekeepers.</span>
</span>
</td>
<td id="A4.tab1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.3.2.3.1.1" class="ltx_p" style="width:199.2pt;">The United Nations Peacekeepers arrived in Haiti and began monitoring after the 2010 earthquake, where they started to blame the spread of disease on the military. The lawsuit documents that the United Nations camp’s waste is not properly sanitized so it caused the pollution to enter the Artibonite River, one of Haiti’s largest rivers. Before the military arrived, there were no issues with Haiti since the 1800s. So now once the military arrived, the issue came up. The Nile Institute Justice Department was involved with studies and suggested that the Nepal United Nations military were unaware that they were bringing diseases to Haiti. The United Nations disease expert named Danielle Lantagne made a statement that the spread was started by the United Nations Peacekeepers.</span>
</span>
</td>
</tr>
<tr id="A4.tab1.1.4.3" class="ltx_tr">
<th id="A4.tab1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">0</th>
<td id="A4.tab1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.4.3.2.1.1" class="ltx_p" style="width:199.2pt;">During the 1980s he worked on shows such as Taxi, Cheers, and The Tracy Ullman Show.</span>
</span>
</td>
<td id="A4.tab1.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.4.3.3.1.1" class="ltx_p" style="width:199.2pt;">During the 1980s, they worked in shows like Taxi, Cheers, and The Tracy Ullman show.</span>
</span>
</td>
</tr>
<tr id="A4.tab1.1.5.4" class="ltx_tr">
<th id="A4.tab1.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.5.4.2.1.1" class="ltx_p" style="width:199.2pt;">Before The Simpsons Simon had worked on several shows in various positions. During the 1980s he worked on shows such as Taxi, Cheers, and The Tracy Ullman Show. In 1989 he helped create The Simpsons with Brooks and Groening, and was responsible for hiring the show’s first writing team. Despite leaving the show in 1993 he kept the title of executive producer, and continued to receive tens of millions of dollars every season in royalties.</span>
</span>
</td>
<td id="A4.tab1.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab1.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.5.4.3.1.1" class="ltx_p" style="width:199.2pt;">Before The Simpsons, Simon worked in several different shows in different roles. During the 1980s, he worked in different shows like Taxi, Cheers and the Tracy Ullman show. In 1989, he helped implement The Simpsons with Brooks and Groening. He was responsible for hiring the first writers for the show. In 1993, even though he was very successful, he maintained his title as Executive Producer and earned tens of millions of dollars in royalty every season.</span>
</span>
</td>
</tr>
<tr id="A4.tab1.1.6.5" class="ltx_tr">
<th id="A4.tab1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">0</th>
<td id="A4.tab1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.6.5.2.1.1" class="ltx_p" style="width:199.2pt;">Her other race, the Giant Slalom, saw her finish in tenth in the women’s sitting group with a combined run time of 4:41.30, 2:11.60 minutes slower than first place finisher Austrian Claudia Loesch and 1:09.02 minutes slower than the ninth place finisher Gyöngyi Dani of Hungary.</span>
</span>
</td>
<td id="A4.tab1.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.6.5.3.1.1" class="ltx_p" style="width:199.2pt;">Another person who competed is Giant Slalom who finished tenth place. The women who were sitting in the group, their total time together was 4 hours, 41 minutes and 30 seconds. In comparison with the first place winner, who was from Amsterdam named Claudia Loesch, they were about 2 hours, 11 minutes, and 60 seconds slower. 9th place was Giöngyi Dani from Hungary, who was 1 hour, 9 minutes, and 2 seconds slower by comparison.</span>
</span>
</td>
</tr>
<tr id="A4.tab1.1.7.6" class="ltx_tr">
<th id="A4.tab1.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.7.6.2.1.1" class="ltx_p" style="width:199.2pt;">Beyond Wednesday’s event, Carpanedo competed in two individual races at the Championships. Her first was the Slalom, where she earned a Did Not Finish in her first run. 36 of the 116 competitors had the same result in that race. Her other race, the Giant Slalom, saw her finish in tenth in the women’s sitting group with a combined run time of 4:41.30, 2:11.60 minutes slower than first place finisher Austrian Claudia Loesch and 1:09.02 minutes slower than the ninth place finisher Gyöngyi Dani of Hungary. Four skiers in the women’s sitting group failed to finish their runs, and 45 of the 117 total skiers in the Giant Slalom failed to rank in the race.</span>
</span>
</td>
<td id="A4.tab1.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab1.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.7.6.3.1.1" class="ltx_p" style="width:199.2pt;">In Wednesday’s event in Carpenedo there were two individuals in a championship tournament. The first competitor was Slalom who earned the title of ‘Did Not Finish’ in the first run. Out of 116 competitions, 36 got the same result of ‘Did Not Finish’. The other competitor named Giant Slalom finished tenth place. The women in the sitting group’s total time competing was 4 hours, 41 minutes and 30 seconds. In comparison with the first place winner, who was from Amsterdam named Claudia Loesch, they were about 2 hours, 11 minutes and 60 seconds slower. Also in comparison with the 9th place winner who was Giöngyi Dani from Hungary, they were 1 hour, 9 minutes, and 2 seconds slower. Four of the skiers in the women sitting group flunked out of the competition. Out of 117 competitors, 45 failed to move up in the tournament. Giant Slalom also failed and never moved up in the tournament.</span>
</span>
</td>
</tr>
<tr id="A4.tab1.1.8.7" class="ltx_tr">
<th id="A4.tab1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">0</th>
<td id="A4.tab1.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.8.7.2.1.1" class="ltx_p" style="width:199.2pt;">The cabbage juice changes color depending on how acidic or basic (alkaline) the chemical is.</span>
</span>
</td>
<td id="A4.tab1.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab1.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.8.7.3.1.1" class="ltx_p" style="width:199.2pt;">Cabbage juice changes colors depending on how acidic it is and how basic, or the alkaline chemical, it is.</span>
</span>
</td>
</tr>
<tr id="A4.tab1.1.9.8" class="ltx_tr">
<th id="A4.tab1.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t"></th>
<td id="A4.tab1.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.9.8.2.1.1" class="ltx_p" style="width:199.2pt;">This is called a chemical’s pH. You can make an indicator using red cabbage juice. The cabbage juice changes color depending on how acidic or basic (alkaline) the chemical is. The pH level is indicated by the amount of Hydrogen (the H in pH) ions in the tested chemical. Hydrogen ions are protons that had their electrons stripped off them (since Hydrogen atoms consist of one proton and one electron).</span>
</span>
</td>
<td id="A4.tab1.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab1.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab1.1.9.8.3.1.1" class="ltx_p" style="width:199.2pt;">This is called chemical pH. You can compute pH levels by using red cabbage juice. The juice will change colors depending on the mix of two components, where the first is acidic and the second is basic, and the two put together is the alkaline chemical that influences the color change in cabbage juice. The pH levels are detected by the amount of hydrogen within the pH ions. Hydrogen ions are protons but the electrons will take ions away. The hydrogen ion only has one proton and one electron.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A4.tab2" class="ltx_table">
<table id="A4.tab2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.tab2.1.1.1" class="ltx_tr">
<th id="A4.tab2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A4.tab2.1.1.1.1.1" class="ltx_text ltx_font_bold">signer</span></th>
<th id="A4.tab2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.1.1.2.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab2.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">reference</span></span>
</span>
</th>
<th id="A4.tab2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.1.1.3.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab2.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">human prediction</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.tab2.1.2.1" class="ltx_tr">
<th id="A4.tab2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">1</th>
<td id="A4.tab2.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab2.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.2.1.2.1.1" class="ltx_p" style="width:199.2pt;">Tournament top seeds South Africa started on the right note when they had a comfortable 26 - 00 win against 5th seeded Zambia.</span>
</span>
</td>
<td id="A4.tab2.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab2.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.2.1.3.1.1" class="ltx_p" style="width:199.2pt;">In the tournament bracket, South Africa moved forward strong and won 26 to 0 against Zambia, who was 5th.</span>
</span>
</td>
</tr>
<tr id="A4.tab2.1.3.2" class="ltx_tr">
<th id="A4.tab2.1.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab2.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab2.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.3.2.2.1.1" class="ltx_p" style="width:199.2pt;">The games kicked off at 10:00am with great weather and apart from mid morning drizzle which quickly cleared up, it was a perfect day for 7’s rugby. Tournament top seeds South Africa started on the right note when they had a comfortable 26 - 00 win against 5th seeded Zambia. Looking decidedly rusty in the game against their southern sisters, South Africa however steadily improved as the tournament progressed. Their disciplined defence, ball handling skills and excellent team work made them stand out and it was clear that this was the team to beat.</span>
</span>
</td>
<td id="A4.tab2.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab2.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.3.2.3.1.1" class="ltx_p" style="width:199.2pt;">The game started at 10am and the weather was perfect, even though in the morning it was a bit hazy, but has since then disappeared. The rugby game was a 7-on-7 and they followed a tournament bracket. South Africa progressed nicely in the tournament and won 26-0 against Zambia, who was 5th. In the beginning of the game, it was a bit rough but eventually smoothed over. South Africa, regardless, improved throughout the tournament and became stronger as they progressed. Their defense worked seamlessly together and handled the ball nicely. The team showed solidarity. That’s the team that beat the other.</span>
</span>
</td>
</tr>
<tr id="A4.tab2.1.4.3" class="ltx_tr">
<th id="A4.tab2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">1</th>
<td id="A4.tab2.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab2.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.4.3.2.1.1" class="ltx_p" style="width:199.2pt;">Others are alleged to have been brought up by animals; some are said to have lived in the wild on their own.</span>
</span>
</td>
<td id="A4.tab2.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab2.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.4.3.3.1.1" class="ltx_p" style="width:199.2pt;">Some of them ventured off and joined the animals while others were isolated.</span>
</span>
</td>
</tr>
<tr id="A4.tab2.1.5.4" class="ltx_tr">
<th id="A4.tab2.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab2.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab2.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.5.4.2.1.1" class="ltx_p" style="width:199.2pt;">One of the most common methods used to illustrate the importance of socialization is to draw upon the few unfortunate cases of children who were, through neglect, misfortune, or wilful abuse, not socialized by adults while they were growing up. Such children are called "feral" or wild. Some feral children have been confined by people (usually their own parents); in some cases this child abandonment was due to the parents’ rejection of a child’s severe intellectual or physical impairment. Feral children may have experienced severe child abuse or trauma before being abandoned or running away. Others are alleged to have been brought up by animals; some are said to have lived in the wild on their own. When completely brought up by non-human animals, the feral child exhibits behaviors (within physical limits) almost entirely like those of the particular care-animal, such as its fear of or indifference to humans.</span>
</span>
</td>
<td id="A4.tab2.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab2.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.5.4.3.1.1" class="ltx_p" style="width:199.2pt;">There are discussions on the importance of socializing. It always goes back to children who grow up with no exposure to healthy interactions with adults, and this can happen for various reasons, some being neglect, bad luck, or just abuse. When children don’t get social exposure, they become what you call feral, or “wild child”. Isolating their children may be the result of some people being ashamed of these children because of their mental state or their physical appearance. Sometimes it’s their own parents doing this. What happens is they may sometimes escape captive because of trauma from mental abuse, sometimes severe. Sometimes these children escape and end up living with animals, being the only human. There is a trend that these children will mimic the body language and behavior of the animal they’ve interacted with. Sometimes these feral children are afraid of other humans while others do not care to make any connections with humans.</span>
</span>
</td>
</tr>
<tr id="A4.tab2.1.6.5" class="ltx_tr">
<th id="A4.tab2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">1</th>
<td id="A4.tab2.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab2.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.6.5.2.1.1" class="ltx_p" style="width:199.2pt;">His teammate Fernando Alonso was in the lead for most of the race, but ended it right after his pit-stop, probably because a badly tucked right front wheel.</span>
</span>
</td>
<td id="A4.tab2.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab2.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.6.5.3.1.1" class="ltx_p" style="width:199.2pt;">The other team, the same as Fernado Alonso, drove and eventually spun out. He had to pull over to the pit stop because one of his tires was bent inward. They had to remove the car from the race.</span>
</span>
</td>
</tr>
<tr id="A4.tab2.1.7.6" class="ltx_tr">
<th id="A4.tab2.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab2.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab2.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.7.6.2.1.1" class="ltx_p" style="width:199.2pt;">Giancarlo Fisichella lost control of his car and ended the race very soon after the start. His teammate Fernando Alonso was in the lead for most of the race, but ended it right after his pit-stop, probably because a badly tucked right front wheel. Michael Schumacher ended his race not long after Alonso, because of the suspension damage in the numerous battles during the race.</span>
</span>
</td>
<td id="A4.tab2.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab2.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.7.6.3.1.1" class="ltx_p" style="width:199.2pt;">In the car race, Giancauldo Fischla spun out and his car was damaged. He was out. The other team, like him, Fernando Alonso, also spun out of control and he had to pull over to the pit stop. One of the tires was bent inward and they determined that the driver was also out of the race. Another car, Michael Schumacher, after the other two were already out, was also out because his suspensions were not functioning properly.</span>
</span>
</td>
</tr>
<tr id="A4.tab2.1.8.7" class="ltx_tr">
<th id="A4.tab2.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">1</th>
<td id="A4.tab2.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab2.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.8.7.2.1.1" class="ltx_p" style="width:199.2pt;">Today many Sámi work in modern trades. Tourism is an important income in Sápmi, the Sámi area.</span>
</span>
</td>
<td id="A4.tab2.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab2.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.8.7.3.1.1" class="ltx_p" style="width:199.2pt;">Today many Sami work in their world as their job. Tourism, for example, Sapmi is a cash cow as a form of unification.</span>
</span>
</td>
</tr>
<tr id="A4.tab2.1.9.8" class="ltx_tr">
<th id="A4.tab2.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t"></th>
<td id="A4.tab2.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab2.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.9.8.2.1.1" class="ltx_p" style="width:199.2pt;">Reindeer husbandry is an important livelihood among the Sámi and the culture surrounding the trade is important also for many with other professions. Even traditionally, though, not all Sámi have been involved in big scale reindeer husbandry, but lived from fishing, hunting and similar, having reindeer mostly as draft animals. Today many Sámi work in modern trades. Tourism is an important income in Sápmi, the Sámi area.</span>
</span>
</td>
<td id="A4.tab2.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab2.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab2.1.9.8.3.1.1" class="ltx_p" style="width:199.2pt;">Caring and nurturing reindeer to ensure growth is important. Sami people and their culture depend on reindeer. It just so happens that other work fields are also associated with reindeer. It is not intentional for all Sami people to be involved with reindeer but living from fishing and hunting included the reliance and use of reindeer. Now in today’s world, many Sami workers in tourism, for example, bring in a lot of money, Sapmi, and they are unified by that too.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A4.tab3" class="ltx_table">
<table id="A4.tab3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.tab3.1.1.1" class="ltx_tr">
<th id="A4.tab3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A4.tab3.1.1.1.1.1" class="ltx_text ltx_font_bold">signer</span></th>
<th id="A4.tab3.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab3.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.1.1.2.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab3.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">reference</span></span>
</span>
</th>
<th id="A4.tab3.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab3.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.1.1.3.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab3.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">human prediction</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.tab3.1.2.1" class="ltx_tr">
<th id="A4.tab3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">1</th>
<td id="A4.tab3.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab3.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.2.1.2.1.1" class="ltx_p" style="width:199.2pt;">But Prime Minister John Howard has said the act was only to safeguard the facilities of the hospital from being downgraded by the Tasmanian government, in giving an extra AUD$45 million.</span>
</span>
</td>
<td id="A4.tab3.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab3.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.2.1.3.1.1" class="ltx_p" style="width:199.2pt;">The priest John Howard said the act is for protecting the hospital from worsening. They cherish it and want to put up 45 million AUD.</span>
</span>
</td>
</tr>
<tr id="A4.tab3.1.3.2" class="ltx_tr">
<th id="A4.tab3.1.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab3.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab3.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.3.2.2.1.1" class="ltx_p" style="width:199.2pt;">Prime Minister Stephen Harper has agreed to send the government’s ’Clean Air Act’ to an all-party committee for review, before its second reading, after Tuesday’s 25 minute meeting with NDP leader Jack Layton at the PMO. Layton had asked for changes to the conservatives’ environmental bill during the meeting with the PM, asking for a "thorough and complete rewriting" of the Conservative party’s environmental bill. Ever since the Federal Government stepped in to take over funding of the Mersey hospital in Devonport, Tasmania, the state government and some federal MPs have criticised this act as a stunt in the prelude to the federal election to be called by November. But Prime Minister John Howard has said the act was only to safeguard the facilities of the hospital from being downgraded by the Tasmanian government, in giving an extra AUD$45 million.</span>
</span>
</td>
<td id="A4.tab3.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab3.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.3.2.3.1.1" class="ltx_p" style="width:199.2pt;">The minister Stephen Harper agreed he will send his government’s Clean Air Act to all political parties to be looked over. There was a brief 25-minute meeting last week Tuesday with NDP’s leader, Jaclyn Pmo, and Jacklyn said the act needed to be reviewed and revised because it doesn’t suit their conservative environment. Understand that the government has been involved with funding for Mersey Hospital in Deveonport, Tasmania. The government MP oversaw the whole process and came to the conclusion that the NDP intentionally shifted their focus due to elections coming up in November. Something was not right. However, the priest John Howard clarified that the act is for protecting the hospital from worsening. They cherish it and want to send forth 45 million AUD.</span>
</span>
</td>
</tr>
<tr id="A4.tab3.1.4.3" class="ltx_tr">
<th id="A4.tab3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">1</th>
<td id="A4.tab3.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab3.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.4.3.2.1.1" class="ltx_p" style="width:199.2pt;">If you booked your flights and accommodation for 2020 before the postponement was announced, you may have a tricky situation.</span>
</span>
</td>
<td id="A4.tab3.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab3.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.4.3.3.1.1" class="ltx_p" style="width:199.2pt;">Have you already booked a flight and hotel for a trip in 2020? Before they had announced postponement, maybe you were faced with challenges and how to resolve them.</span>
</span>
</td>
</tr>
<tr id="A4.tab3.1.5.4" class="ltx_tr">
<th id="A4.tab3.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab3.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab3.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.5.4.2.1.1" class="ltx_p" style="width:199.2pt;">The Paralympics will take place from 24 August to 5 September 2021. Some events will be held in other locations throughout Japan. Tokyo will be the only Asian city to have hosted two summer Olympics, having hosted the games in 1964. If you booked your flights and accommodation for 2020 before the postponement was announced, you may have a tricky situation. Cancellation policies vary, but as of late March most coronavirus-based cancellation policies don’t extend to July 2020, when the Olympics had been scheduled. It’s expected that most event tickets will cost between ¥2,500 and ¥130,000, with typical tickets costing around ¥7,000.</span>
</span>
</td>
<td id="A4.tab3.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab3.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.5.4.3.1.1" class="ltx_p" style="width:199.2pt;">The Paralympics will be hosted from August 24th to September 5th, 2021 in Japan. Tokyo is the only Asian city that will have hosted two summer games, the first time being in 1964. If you already booked a flight and hotel for 2020 before the announcement of the postponement, maybe you were faced with challenges and what to do. The cancellation policies were all over the place. Recently in March, they had determined that during the time of COVID affecting cancellations, it doesn’t apply to or affect their schedule announced in July 2020. It is expected that most tickets will cost 2,500 to 130,000 yen, with the most average cost being 7,000 yen.</span>
</span>
</td>
</tr>
<tr id="A4.tab3.1.6.5" class="ltx_tr">
<th id="A4.tab3.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">1</th>
<td id="A4.tab3.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab3.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.6.5.2.1.1" class="ltx_p" style="width:199.2pt;">The rise of new technologies allows us to see and investigate brain structures and processes never seen before.</span>
</span>
</td>
<td id="A4.tab3.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab3.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.6.5.3.1.1" class="ltx_p" style="width:199.2pt;">The rise of new technology allows for investigation of brain structure and processes never seen before.</span>
</span>
</td>
</tr>
<tr id="A4.tab3.1.7.6" class="ltx_tr">
<th id="A4.tab3.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab3.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab3.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.7.6.2.1.1" class="ltx_p" style="width:199.2pt;">Neurobiological data provide physical evidence for a theoretical approach to the investigation of cognition. Therefore it narrows the research area and makes it much more exact. The correlation between brain pathology and behaviour supports scientists in their research. It has been known for a long time that different types of brain damage, traumas, lesions, and tumours affect behaviour and cause changes in some mental functions. The rise of new technologies allows us to see and investigate brain structures and processes never seen before. This provides us with a lot of information and material to build simulation models which help us to understand processes in our mind.</span>
</span>
</td>
<td id="A4.tab3.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab3.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.7.6.3.1.1" class="ltx_p" style="width:199.2pt;">Neurobiological data provides evidence of theoretical approaches and investigation related to cognition, more specifically for the research area to be exact. There has been an established connection between brain pathology and behavior that has supported science and their research so far. It has already been proven that brain damage, trauma, lesions and tumors affect behaviors. These affect some changes to occur in mental functions. The rises in new technology allows for investigation of brain structure and processes never seen before. It provides new information and resources to build a model that we can mimic to help us understand the brain processes that are happening.</span>
</span>
</td>
</tr>
<tr id="A4.tab3.1.8.7" class="ltx_tr">
<th id="A4.tab3.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">1</th>
<td id="A4.tab3.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab3.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.8.7.2.1.1" class="ltx_p" style="width:199.2pt;">Courier companies are well paid for delivering things quickly. Frequently, time is very important with business documents, merchandise or spare parts for an urgent repair.</span>
</span>
</td>
<td id="A4.tab3.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab3.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.8.7.3.1.1" class="ltx_p" style="width:199.2pt;">Courier companies are paid well for fast delivery. What happens often is these things are time sensitive such as documents related to business, materials and extra parts for quick repair.</span>
</span>
</td>
</tr>
<tr id="A4.tab3.1.9.8" class="ltx_tr">
<th id="A4.tab3.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t"></th>
<td id="A4.tab3.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab3.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.9.8.2.1.1" class="ltx_p" style="width:199.2pt;">Courier companies are well paid for delivering things quickly. Frequently, time is very important with business documents, merchandise or spare parts for an urgent repair. On some routes, the larger companies have their own planes, but for other routes and smaller firms there was a problem. If they sent things by air freight, on some routes it may have taken days to get through unloading and customs. The only way to get it through faster was to send it as checked luggage. Airline regulations will not allow them to send luggage without a passenger, which is where you come in.</span>
</span>
</td>
<td id="A4.tab3.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab3.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab3.1.9.8.3.1.1" class="ltx_p" style="width:199.2pt;">Courier companies are paid well for fast delivery. Oftentimes, these things being delivered are time sensitive such as business documents, materials and extra materials for quick repair. Larger companies have the luxury of convenient route options such as flights while others have limited options, like smaller companies with less options. Problems may arise for them. When these companies ship via flight, some will have to go through customs and investigations prolonging the process, so they expedite the process by putting these items in their own luggages onto a flight. However, flight regulations do require that luggages travel with their person and that is what the role of a courier is.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A4.tab4" class="ltx_table">
<table id="A4.tab4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.tab4.1.1.1" class="ltx_tr">
<th id="A4.tab4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A4.tab4.1.1.1.1.1" class="ltx_text ltx_font_bold">signer</span></th>
<th id="A4.tab4.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab4.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.1.1.2.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab4.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">reference</span></span>
</span>
</th>
<th id="A4.tab4.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab4.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.1.1.3.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab4.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">human prediction</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.tab4.1.2.1" class="ltx_tr">
<th id="A4.tab4.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">2</th>
<td id="A4.tab4.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab4.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.2.1.2.1.1" class="ltx_p" style="width:199.2pt;">The Articles required unanimous consent from all the states before they could be amended and states took the central government so lightly that their representatives were often absent.</span>
</span>
</td>
<td id="A4.tab4.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab4.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.2.1.3.1.1" class="ltx_p" style="width:199.2pt;">A legal article requires that all states must create an agreement. That can be an amendment to the law for revisions. The states look at the law as minor. Many state representatives come together regularly, but many do not make an effort to attend the meetings.</span>
</span>
</td>
</tr>
<tr id="A4.tab4.1.3.2" class="ltx_tr">
<th id="A4.tab4.1.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab4.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab4.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.3.2.2.1.1" class="ltx_p" style="width:199.2pt;">During the Revolutionary War, the thirteen states first formed a weak central government—with the Congress being its only component—under the Articles of Confederation. Congress lacked any power to impose taxes, and, because there was no national executive or judiciary, it relied on state authorities, who were often uncooperative, to enforce all its acts. It also had no authority to override tax laws and tariffs between states. The Articles required unanimous consent from all the states before they could be amended and states took the central government so lightly that their representatives were often absent.</span>
</span>
</td>
<td id="A4.tab4.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab4.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.3.2.3.1.1" class="ltx_p" style="width:199.2pt;">During the Revolutionary War, there were 13 states that came together to create a central government. The government was weak because there was one congress that was established by the Article of Confederation. Congress had no power to enforce taxation on people because there was no Executive branch, where the president falls, and then the other branch which is Judicial. So the government relies on the states’ authority for taxes. However, the government can try to enforce the law but the people won’t cooperate. The government struggled with oppression in the different states and their tax laws and their tariffs between the states. The article mandated all states to create a uniform agreement as an amendment to the law for revisions. The states look at the process as minor and many of the state representatives did not make an effort to attend those meetings.</span>
</span>
</td>
</tr>
<tr id="A4.tab4.1.4.3" class="ltx_tr">
<th id="A4.tab4.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">2</th>
<td id="A4.tab4.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab4.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.4.3.2.1.1" class="ltx_p" style="width:199.2pt;">The largest tournament of the year takes place in December at the polo fields in Las Cañitas.</span>
</span>
</td>
<td id="A4.tab4.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab4.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.4.3.3.1.1" class="ltx_p" style="width:199.2pt;">The largest tournament of the year is in Las Canitas in December.</span>
</span>
</td>
</tr>
<tr id="A4.tab4.1.5.4" class="ltx_tr">
<th id="A4.tab4.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab4.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab4.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.5.4.2.1.1" class="ltx_p" style="width:199.2pt;">Argentina is well known for having one of the best polo teams and players in the world. The largest tournament of the year takes place in December at the polo fields in Las Cañitas. Smaller tournaments and matches can also be seen here at other times of the year. For news on tournaments and where to buy tickets for polo matches, check Asociacion Argentina de Polo.</span>
</span>
</td>
<td id="A4.tab4.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab4.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.5.4.3.1.1" class="ltx_p" style="width:199.2pt;">In Argentina, they are well known for having one of the best polo teams and players in the world. The largest tournament of the year is in Las Canitas in December. Year round, at any time of the year, there are smaller tournaments that take place. If you want to know more information about the tournaments and where you can buy tickets for the polo games, check out Association of Argentina de Polo.</span>
</span>
</td>
</tr>
<tr id="A4.tab4.1.6.5" class="ltx_tr">
<th id="A4.tab4.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">2</th>
<td id="A4.tab4.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab4.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.6.5.2.1.1" class="ltx_p" style="width:199.2pt;">After seeing the horrors and atrocities of war during World War I, nations desired to avoid such a situation again in the future.</span>
</span>
</td>
<td id="A4.tab4.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab4.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.6.5.3.1.1" class="ltx_p" style="width:199.2pt;">During WWI, there were a lot of battles and people were there to witness the violence, cruelty, horror, and disruption. After the war, countries did their best to avoid any conflicts or bad situations from happening again. That was resolved.</span>
</span>
</td>
</tr>
<tr id="A4.tab4.1.7.6" class="ltx_tr">
<th id="A4.tab4.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab4.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab4.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.7.6.2.1.1" class="ltx_p" style="width:199.2pt;">During the 1920s, the prevailing attitudes of most citizens and nations was that of pacifism and isolation. After seeing the horrors and atrocities of war during World War I, nations desired to avoid such a situation again in the future.</span>
</span>
</td>
<td id="A4.tab4.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab4.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.7.6.3.1.1" class="ltx_p" style="width:199.2pt;">Back in the 1920s, during that time most people had a relaxed attitude and were open with each other and had honest dialogues. It was a peaceful time. Even at the country level, things were civil and peaceful. Issues that arose were resolved. There were unique situations where certain countries were isolated. Then during WWI, there were tense battles and many people saw the violence, cruelty, horror and disruptions that occurred. After the war was over, countries did their best to avoid the same situation happening again. That was resolved.</span>
</span>
</td>
</tr>
<tr id="A4.tab4.1.8.7" class="ltx_tr">
<th id="A4.tab4.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">2</th>
<td id="A4.tab4.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab4.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.8.7.2.1.1" class="ltx_p" style="width:199.2pt;">Children are placed in Foster Care for a wide variety of reasons that range from neglect, to abuse, and even to extortion.</span>
</span>
</td>
<td id="A4.tab4.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab4.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.8.7.3.1.1" class="ltx_p" style="width:199.2pt;">Children are in foster care for different reasons like neglect, abuse, sexual abuse, and even for forcing, threatening, bribing or grooming.</span>
</span>
</td>
</tr>
<tr id="A4.tab4.1.9.8" class="ltx_tr">
<th id="A4.tab4.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t"></th>
<td id="A4.tab4.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab4.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.9.8.2.1.1" class="ltx_p" style="width:199.2pt;">Children are placed in Foster Care for a wide variety of reasons that range from neglect, to abuse, and even to extortion. No child should ever have to grow up in an environment that is not nurturing, caring, and educational, but they do. We perceive the Foster Care System to be a safety zone for these children. Our foster care system is supposed to provide safe homes, loving caregivers, stable education, and reliable health care. Foster care is supposed to provide all the necessities that were lacking in the home they were previously taken from.</span>
</span>
</td>
<td id="A4.tab4.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab4.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab4.1.9.8.3.1.1" class="ltx_p" style="width:199.2pt;">Children are in foster care for different reasons like neglect, physical abuse, sexual abuse, and even for forcing, threatening, bribing or grooming. A child should not grow up in an environment that is not nurturing or thriving in self esteem or education. It is unfortunate for those who grow up in that environment. It is important to observe children are in a safe area when in foster care. Foster care all over must provide a loving, caring and stable environment, as well as reliable health care. Recently a child was taken from their home due to their needs not being met. Foster care is supposed to provide the needs that their original home did not provide.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A4.tab5" class="ltx_table">
<table id="A4.tab5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.tab5.1.1.1" class="ltx_tr">
<th id="A4.tab5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A4.tab5.1.1.1.1.1" class="ltx_text ltx_font_bold">signer</span></th>
<th id="A4.tab5.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab5.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.1.1.2.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab5.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">reference</span></span>
</span>
</th>
<th id="A4.tab5.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab5.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.1.1.3.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab5.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">human prediction</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.tab5.1.2.1" class="ltx_tr">
<th id="A4.tab5.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">2</th>
<td id="A4.tab5.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab5.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.2.1.2.1.1" class="ltx_p" style="width:199.2pt;">One of the most noteworthy recent examples of this was the North Atlantic campaign of WWII. The Americans were trying to move men and materials across the Atlantic Ocean to help Britain.</span>
</span>
</td>
<td id="A4.tab5.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab5.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.2.1.3.1.1" class="ltx_p" style="width:199.2pt;">One important, recent event to recognize is in World War II and their North Atlantic Campaign, North America brought over their men and materials over the Atlantic Ocean with the intention of helping Britain.</span>
</span>
</td>
</tr>
<tr id="A4.tab5.1.3.2" class="ltx_tr">
<th id="A4.tab5.1.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab5.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab5.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.3.2.2.1.1" class="ltx_p" style="width:199.2pt;">Using ships to transport goods is by far the most efficient way to move large amounts of people and goods across oceans. The job of navies has traditionally been to ensure that your country maintains the ability to move your people and goods, while at the same time, interfering with your enemy’s ability to move his people and goods. One of the most noteworthy recent examples of this was the North Atlantic campaign of WWII. The Americans were trying to move men and materials across the Atlantic Ocean to help Britain. At the same time, the German navy, using mainly U-boats, was trying to stop this traffic. Had the Allies failed, Germany probably would have been able to conquer Britain as it had the rest of Europe.</span>
</span>
</td>
<td id="A4.tab5.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab5.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.3.2.3.1.1" class="ltx_p" style="width:199.2pt;">There is the ocean and then land on both sides. A lot of people brought materials over the ocean via ship as it was the most effective method of transportation. The navy warships would oversee the process of transporting their people and materials. Also, when they would see the enemy transporting their people and materials, they would interfere with their transport. One important event recognized recently is during World War II and their North Atlantic campaign, where they would bring over their men and materials overseas to Britain in order to help them, Germany attempted to interfere with transport using their U-boats. If the alliance and transportation lines fell apart, that means there would be the possibility of Germany having been able to take over Britain and eventually the rest of Europe.</span>
</span>
</td>
</tr>
<tr id="A4.tab5.1.4.3" class="ltx_tr">
<th id="A4.tab5.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">2</th>
<td id="A4.tab5.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab5.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.4.3.2.1.1" class="ltx_p" style="width:199.2pt;">This term derives from ancient familiarity with Bed-bugs, which are insects highly adapted to parasitize humans.</span>
</span>
</td>
<td id="A4.tab5.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab5.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.4.3.3.1.1" class="ltx_p" style="width:199.2pt;">The word bug comes from the word “bed bug”, which is an insect that pierces and burrows under the skin, and becomes a parasite.</span>
</span>
</td>
</tr>
<tr id="A4.tab5.1.5.4" class="ltx_tr">
<th id="A4.tab5.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab5.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab5.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.5.4.2.1.1" class="ltx_p" style="width:199.2pt;">The term bug is used by entomologists in a formal sense for this group of insects. This term derives from ancient familiarity with Bed-bugs, which are insects highly adapted to parasitize humans. Both Assassin-bugs and Bed-bugs are nidicolous, adapted to living in nest or housing of their host.</span>
</span>
</td>
<td id="A4.tab5.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab5.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.5.4.3.1.1" class="ltx_p" style="width:199.2pt;">People who study insects were looking over a subgroup of insects that included the word ‘bug’. The word comes from an insect called ‘Bed Bugs’ that would pierce and burrow under the skin, and become a parasite. There are two different types of these bugs, Assassin Bugs and Bed Bugs, which tend to adapt and remain in the body. They essentially nest in the body for as long as they can.</span>
</span>
</td>
</tr>
<tr id="A4.tab5.1.6.5" class="ltx_tr">
<th id="A4.tab5.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">2</th>
<td id="A4.tab5.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab5.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.6.5.2.1.1" class="ltx_p" style="width:199.2pt;">For over a thousand years the Christian religion had bound European states together despite differences in language and customs. I</span>
</span>
</td>
<td id="A4.tab5.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab5.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.6.5.3.1.1" class="ltx_p" style="width:199.2pt;">Thousands of years in Europe and in their different regions, Christians stayed together regardless of different languages or customs.</span>
</span>
</td>
</tr>
<tr id="A4.tab5.1.7.6" class="ltx_tr">
<th id="A4.tab5.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab5.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab5.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.7.6.2.1.1" class="ltx_p" style="width:199.2pt;">During this period of European history, the Catholic Church, which had become rich and powerful, came under scrutiny. For over a thousand years the Christian religion had bound European states together despite differences in language and customs. I Its all-pervading power affected everyone from king to commoner. One of the main Christian tenets is that wealth should be used to alleviate suffering and poverty and that the monetary funds of the church are there specifically for that reason. The central authority of the church had been in Rome for over a thousand years and this concentration of power and money led many to question whether this tenet was being met.</span>
</span>
</td>
<td id="A4.tab5.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab5.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.7.6.3.1.1" class="ltx_p" style="width:199.2pt;">In Europe’s historical time period where catholic churches were wealthy and powerful, all eyes were on them. In the thousands of years in Europe and their different regions, Christians stuck together regardless of their different languages or customs. Religion was powerful and had influence everywhere such as monarchy and even peasants. Christians had a belief system and one part says that the wealthy shall help the poor and reduce their suffering. WIth this in mind, the Church would save their money with the intention of reducing suffering in the poor. Over the thousands of years, the central authority figure in churches everywhere was in Rome and they focused on the power of money and how it had influence on others. It caused people to question whether they were truly following their belief system.</span>
</span>
</td>
</tr>
<tr id="A4.tab5.1.8.7" class="ltx_tr">
<th id="A4.tab5.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">2</th>
<td id="A4.tab5.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab5.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.8.7.2.1.1" class="ltx_p" style="width:199.2pt;">Plants make oxygen which humans breathe, and they take in carbon-dioxide which humans exhale (that is, breathe out).</span>
</span>
</td>
<td id="A4.tab5.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab5.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.8.7.3.1.1" class="ltx_p" style="width:199.2pt;">Plants create oxygen, of which people use and breathe in. Then people breathe out carbon dioxide which plants need. It is a transactional process.</span>
</span>
</td>
</tr>
<tr id="A4.tab5.1.9.8" class="ltx_tr">
<th id="A4.tab5.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t"></th>
<td id="A4.tab5.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab5.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.9.8.2.1.1" class="ltx_p" style="width:199.2pt;">Plants make oxygen which humans breathe, and they take in carbon-dioxide which humans exhale (that is, breathe out). Plants make their food from the sun by photosynthesis. They also provide shade. We make our houses from plants and make clothes from plants. Most foods that we eat are plants. Without plants, animals could not survive.</span>
</span>
</td>
<td id="A4.tab5.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab5.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab5.1.9.8.3.1.1" class="ltx_p" style="width:199.2pt;">Plants create oxygen of which people breathe in. Then when people breathe out, they are letting out carbon dioxide which plants need. It’s a transactional process. Plants rely on the sun where it helps them produce their own food, and this process is called photosynthesis. Plants also provide shades, housing, materials, clothes, and even most food come from plants. Animals cannot survive without plants.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A4.tab6" class="ltx_table">
<table id="A4.tab6.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.tab6.1.1.1" class="ltx_tr">
<th id="A4.tab6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A4.tab6.1.1.1.1.1" class="ltx_text ltx_font_bold">signer</span></th>
<th id="A4.tab6.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab6.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.1.1.2.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab6.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">reference</span></span>
</span>
</th>
<th id="A4.tab6.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab6.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.1.1.3.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab6.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">human prediction</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.tab6.1.2.1" class="ltx_tr">
<th id="A4.tab6.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">3</th>
<td id="A4.tab6.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab6.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.2.1.2.1.1" class="ltx_p" style="width:199.2pt;">They begin as funnels descending from storm clouds, and become "tornadoes" when they touch the ground.</span>
</span>
</td>
<td id="A4.tab6.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab6.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.2.1.3.1.1" class="ltx_p" style="width:199.2pt;">A tornado starts when a wind cloud funnels downward until it touches the ground. Once on ground, it moves forward.</span>
</span>
</td>
</tr>
<tr id="A4.tab6.1.3.2" class="ltx_tr">
<th id="A4.tab6.1.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab6.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab6.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.3.2.2.1.1" class="ltx_p" style="width:199.2pt;">A tornado is a spinning column of very low-pressure air, which sucks the surrounding air inward and upward. They generate high winds (often 100-200 miles/hour) and can lift heavy objects into the air, carrying them as the tornado moves. They begin as funnels descending from storm clouds, and become "tornadoes" when they touch the ground.</span>
</span>
</td>
<td id="A4.tab6.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab6.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.3.2.3.1.1" class="ltx_p" style="width:199.2pt;">A tornado is a funnel of low pressure air where air from outside of the funnel is sucked in and channels upward, creating the spin. A tornado forms high wind of around 100-200 mph with heavy items being pulled up into the funnel. A tornado starts when a wind cloud forms a funnel and makes its way downward onto the ground. Once it touches the ground, it moves forward and we have a tornado.</span>
</span>
</td>
</tr>
<tr id="A4.tab6.1.4.3" class="ltx_tr">
<th id="A4.tab6.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">3</th>
<td id="A4.tab6.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab6.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.4.3.2.1.1" class="ltx_p" style="width:199.2pt;">The setting might be an historic old building with antique furnishings, manicured grounds and a swimming pool.</span>
</span>
</td>
<td id="A4.tab6.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab6.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.4.3.3.1.1" class="ltx_p" style="width:199.2pt;">The setting may be an old historical building with antique furniture. The yard may have clean cut grass and a swimming pool.</span>
</span>
</td>
</tr>
<tr id="A4.tab6.1.5.4" class="ltx_tr">
<th id="A4.tab6.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab6.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab6.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.5.4.2.1.1" class="ltx_p" style="width:199.2pt;">In developed countries today, providing deluxe bed and breakfasts has been raised to a sort of art-form. At the top end, B&amp;Bs obviously compete mainly on two main things: bedding and breakfast. Accordingly, at the finest such establishments one is apt to find the most luxurious bedding, maybe a handmade quilt or an antique bed. Breakfast may include seasonal delights of the region or the host’s speciality dish. The setting might be an historic old building with antique furnishings, manicured grounds and a swimming pool.</span>
</span>
</td>
<td id="A4.tab6.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab6.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.5.4.3.1.1" class="ltx_p" style="width:199.2pt;">The country has a strong economy that now provides a Bed and Breakfast that eventually became an art form. Their competitive advantage is that they provide a bed that they already have but also a breakfast. These are fancy places where you may find luxurious, antique beds with hand sewn quilts. Breakfast may be within their food season and the host may cook their specialized dish. The setting may be an old historical building with antique furniture. The yard may have clean cut grass and a swimming pool.</span>
</span>
</td>
</tr>
<tr id="A4.tab6.1.6.5" class="ltx_tr">
<th id="A4.tab6.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">3</th>
<td id="A4.tab6.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab6.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.6.5.2.1.1" class="ltx_p" style="width:199.2pt;">Chiao and Sharipov reported being a safe distance from the attitude adjustment thrusters.</span>
</span>
</td>
<td id="A4.tab6.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab6.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.6.5.3.1.1" class="ltx_p" style="width:199.2pt;">Chiao and Sharipok were informed that during attitude adjustment that it’s best for safety reasons to step back away from the throwers.</span>
</span>
</td>
</tr>
<tr id="A4.tab6.1.7.6" class="ltx_tr">
<th id="A4.tab6.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab6.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab6.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.7.6.2.1.1" class="ltx_p" style="width:199.2pt;">The station maintained its attitude, despite the loss of a gyroscope earlier in the space station mission, until the end of the spacewalk. Chiao and Sharipov reported being a safe distance from the attitude adjustment thrusters. Russian ground control activated the jets and normal attitude of the station was regained.</span>
</span>
</td>
<td id="A4.tab6.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab6.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.7.6.3.1.1" class="ltx_p" style="width:199.2pt;">The Space Station maintained their altitude position all throughout to the end of the space walk. It didn’t matter if they lost the gigatode in the earlier stages of the space mission. Chiao and Sharipok were informed that during the altitude adjustments, it was best for safety reasons to back away from the thrusters. Russian grounds control the activity jets and the station was adjusted back to their normal altitude.</span>
</span>
</td>
</tr>
<tr id="A4.tab6.1.8.7" class="ltx_tr">
<th id="A4.tab6.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">3</th>
<td id="A4.tab6.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab6.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.8.7.2.1.1" class="ltx_p" style="width:199.2pt;">Bird flu, or more formally avian influenza, can infect both birds and mammals.</span>
</span>
</td>
<td id="A4.tab6.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab6.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.8.7.3.1.1" class="ltx_p" style="width:199.2pt;">The bird flu, formally known as the Avian influenza, affects both birds and mammals.</span>
</span>
</td>
</tr>
<tr id="A4.tab6.1.9.8" class="ltx_tr">
<th id="A4.tab6.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab6.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab6.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.9.8.2.1.1" class="ltx_p" style="width:199.2pt;">Bird flu, or more formally avian influenza, can infect both birds and mammals. Fewer than a thousand cases have ever been reported in humans, but some of them have been fatal. Most have involved people who work with poultry, but there is also some risk to birdwatchers.</span>
</span>
</td>
<td id="A4.tab6.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab6.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.9.8.3.1.1" class="ltx_p" style="width:199.2pt;">The bird flu, formally known as the Avian Influenza, can affect both birds and mammals. Under 1000 cases were reported in humans and some of them died. Most of the people who contracted the disease worked with poultry but bird watchers also are at risk.</span>
</span>
</td>
</tr>
<tr id="A4.tab6.1.10.9" class="ltx_tr">
<th id="A4.tab6.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">3</th>
<td id="A4.tab6.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab6.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.10.9.2.1.1" class="ltx_p" style="width:199.2pt;">On the accordion, to get extra volume, you use the bellows with more pressure or speed.</span>
</span>
</td>
<td id="A4.tab6.1.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab6.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.10.9.3.1.1" class="ltx_p" style="width:199.2pt;">The volume of the accordion is turned up by increasing the speed of which the accordion is pressed.</span>
</span>
</td>
</tr>
<tr id="A4.tab6.1.11.10" class="ltx_tr">
<th id="A4.tab6.1.11.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab6.1.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab6.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.11.10.2.1.1" class="ltx_p" style="width:199.2pt;">Make sure your hand is as relaxed as possible while still hitting all the notes correctly - also try not to make much extraneous motion with your fingers. This way, you will tire yourself out as little as possible. Remember there’s no need to hit the keys with a lot of force for extra volume like on the piano. On the accordion, to get extra volume, you use the bellows with more pressure or speed.</span>
</span>
</td>
<td id="A4.tab6.1.11.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab6.1.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.11.10.3.1.1" class="ltx_p" style="width:199.2pt;">Make sure that when you are pressing the buttons on the accordion, make sure you are relaxed and pressing while in a comfortable position. Pressing with intensity and high speed causes premature fatigue and it is not necessary, just like while you are playing on the piano. Also, the volume of the accordion can be turned up by increasing the speed of which you are pressing the accordion.</span>
</span>
</td>
</tr>
<tr id="A4.tab6.1.12.11" class="ltx_tr">
<th id="A4.tab6.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">3</th>
<td id="A4.tab6.1.12.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab6.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.12.11.2.1.1" class="ltx_p" style="width:199.2pt;">While immigration check is usually absent or a formality when you arrive in your homeland, customs control can be a hassle.</span>
</span>
</td>
<td id="A4.tab6.1.12.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab6.1.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.12.11.3.1.1" class="ltx_p" style="width:199.2pt;">When you arrive in your homeland, immigration customs will inspect you. Sometimes they are casual while at other times they are rigid and follow protocol. Customs regulations can be a nuisance.</span>
</span>
</td>
</tr>
<tr id="A4.tab6.1.13.12" class="ltx_tr">
<th id="A4.tab6.1.13.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t"></th>
<td id="A4.tab6.1.13.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab6.1.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.13.12.2.1.1" class="ltx_p" style="width:199.2pt;">While immigration check is usually absent or a formality when you arrive in your homeland, customs control can be a hassle. Make sure you know what you can and cannot bring in and declare anything over the legal limits.</span>
</span>
</td>
<td id="A4.tab6.1.13.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab6.1.13.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab6.1.13.12.3.1.1" class="ltx_p" style="width:199.2pt;">When you arrive in your homeland, immigration will do checks where it is sometimes casual but at other times more of a formality. Customs regulations can be a nuisance. Just make sure you know what you can and can’t bring and announce anything that is above the legal limit.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A4.tab7" class="ltx_table">
<table id="A4.tab7.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.tab7.1.1.1" class="ltx_tr">
<th id="A4.tab7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A4.tab7.1.1.1.1.1" class="ltx_text ltx_font_bold">signer</span></th>
<th id="A4.tab7.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab7.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.1.1.2.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab7.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">reference</span></span>
</span>
</th>
<th id="A4.tab7.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab7.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.1.1.3.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab7.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">human prediction</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.tab7.1.2.1" class="ltx_tr">
<th id="A4.tab7.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">3</th>
<td id="A4.tab7.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab7.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.2.1.2.1.1" class="ltx_p" style="width:199.2pt;">During the summer, also watch out for the Nordic mosquitoes. Although they do not transmit any diseases, they can be irritating.</span>
</span>
</td>
<td id="A4.tab7.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab7.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.2.1.3.1.1" class="ltx_p" style="width:199.2pt;">During the summer, be aware of Nordic mosquitoes. They don’t spread diseases but they are a pest.</span>
</span>
</td>
</tr>
<tr id="A4.tab7.1.3.2" class="ltx_tr">
<th id="A4.tab7.1.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab7.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab7.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.3.2.2.1.1" class="ltx_p" style="width:199.2pt;">Crime, and ill-will toward foreigners in general, is virtually unknown in Greenland. Even in the towns, there are no "rough areas." Cold weather is perhaps the only real danger the unprepared will face. If you visit Greenland during cold seasons (considering that the further north you go, the colder it will be), it is essential to bring warm enough clothing. The very long days in the summer can lead to problems getting sufficient sleep and associated health issues. During the summer, also watch out for the Nordic mosquitoes. Although they do not transmit any diseases, they can be irritating.</span>
</span>
</td>
<td id="A4.tab7.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab7.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.3.2.3.1.1" class="ltx_p" style="width:199.2pt;">Crime towards foreigners in general never happens in Greenland, regardless of different cities not having any “rough” areas. Cold is the only real danger for those who are unprepared to face it. If you visit Greenland during the cold seasons, the north is dangerously frigid and it’s important to bring essential items such as warm clothing. In the summer months, the days are long and it leads to lack of sleep which is also connected to health issues. Be wary in the summer months of the Nordic mosquitoes. They don’t spread diseases but they are truly a nuisance.</span>
</span>
</td>
</tr>
<tr id="A4.tab7.1.4.3" class="ltx_tr">
<th id="A4.tab7.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">3</th>
<td id="A4.tab7.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab7.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.4.3.2.1.1" class="ltx_p" style="width:199.2pt;">But the Royal Navy was still much stronger than the German Navy (“Kriegsmarine”) and could have destroyed any invasion fleet sent across the English Channel.</span>
</span>
</td>
<td id="A4.tab7.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab7.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.4.3.3.1.1" class="ltx_p" style="width:199.2pt;">The Royal Navy is stronger than the German navy, called the Kriegsmarine. They can defeat and overpower any invasion sent across the English channel.</span>
</span>
</td>
</tr>
<tr id="A4.tab7.1.5.4" class="ltx_tr">
<th id="A4.tab7.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab7.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab7.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.5.4.2.1.1" class="ltx_p" style="width:199.2pt;">With the battle for France over, Germany began to get ready to invade the island of Britain. Germany code-named the attack “Operation Sealion”. Most of the British Army’s heavy weapons and supplies had been lost when it evacuated from Dunkirk, so the army was fairly weak. But the Royal Navy was still much stronger than the German Navy (“Kriegsmarine”) and could have destroyed any invasion fleet sent across the English Channel. However, very few Royal Navy ships were based near the likely invasion routes as the admirals were afraid they would be sunk by German air attack.</span>
</span>
</td>
<td id="A4.tab7.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab7.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.5.4.3.1.1" class="ltx_p" style="width:199.2pt;">After the German battle against France, they were ready to overtake the British Isle. The German army’s code name for the takeover was Operation Sea Lion. Most British armies had heavy weaponry lost after their evasion from Dunkirk so they were weakened. The royal navy is still stronger than the German navy, called the Kriegsmarine. They can destroy and defeat any invasions sent across the English Channel. However, where there were a few royal navy ships near the location for planned takeover, officers feared Germans would fly over and sink those ships.</span>
</span>
</td>
</tr>
<tr id="A4.tab7.1.6.5" class="ltx_tr">
<th id="A4.tab7.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">4</th>
<td id="A4.tab7.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab7.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.6.5.2.1.1" class="ltx_p" style="width:199.2pt;">However, this shouldn’t really be off your concern, because often tourists are shuffled around to fill the cars.</span>
</span>
</td>
<td id="A4.tab7.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab7.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.6.5.3.1.1" class="ltx_p" style="width:199.2pt;">Really most trips tend to be a collection of cars, vans, buses and others.</span>
</span>
</td>
</tr>
<tr id="A4.tab7.1.7.6" class="ltx_tr">
<th id="A4.tab7.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab7.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab7.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.7.6.2.1.1" class="ltx_p" style="width:199.2pt;">Tours are cheaper for larger groups, so if you’re by yourself or with just one friend, try to meet other people and form a group of four to six for a better per-person rate. However, this shouldn’t really be off your concern, because often tourists are shuffled around to fill the cars. It seems actually to be more a way of tricking people into believing they have to pay more.</span>
</span>
</td>
<td id="A4.tab7.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab7.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.7.6.3.1.1" class="ltx_p" style="width:199.2pt;">Today I am going to talk about tours. Oftentimes, if you go on a tour as a group, they will give you a discounted rate rather than if you were alone or as a couple, it costs more. If there were four or six of us, we can all chip in on the cost and it’s cheaper. In the process, you may get a chance to meet other people too. Most tours tend to include cars, vans, buses and others of that source. The concept influenced me to think that being alone is more expensive and therefore tours alone are not necessary. The tour industry has influenced me to believe that.</span>
</span>
</td>
</tr>
<tr id="A4.tab7.1.8.7" class="ltx_tr">
<th id="A4.tab7.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">4</th>
<td id="A4.tab7.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab7.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.8.7.2.1.1" class="ltx_p" style="width:199.2pt;">If you want to see the world on the cheap, for necessity, lifestyle or challenge, there are some ways to do that.</span>
</span>
</td>
<td id="A4.tab7.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab7.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.8.7.3.1.1" class="ltx_p" style="width:199.2pt;">Suppose you wanted to travel and see the world on a low budget, whether you chose lifestyle or facing challenges along the way, there are different ways to do it.</span>
</span>
</td>
</tr>
<tr id="A4.tab7.1.9.8" class="ltx_tr">
<th id="A4.tab7.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t"></th>
<td id="A4.tab7.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab7.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.9.8.2.1.1" class="ltx_p" style="width:199.2pt;">If you want to see the world on the cheap, for necessity, lifestyle or challenge, there are some ways to do that. Basically, they fall into two categories: Either work while you travel or try and limit your expenses. This article is focused on the latter. For those willing to sacrifice comfort, time and predictability to push expenses down close to zero, see minimum budget travel. The advice assumes that travellers do not steal, trespass, participate in the illegal market, beg, or otherwise exploit other people for their own gain.</span>
</span>
</td>
<td id="A4.tab7.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab7.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab7.1.9.8.3.1.1" class="ltx_p" style="width:199.2pt;">Suppose you wanted to travel and see the world on a low budget, whether you chose between lifestyle or face challenges, there are different ways to do it. There are basically two ways to do it. There is work-study travel and that will limit your spending. The second way to do it, I will go into depth later. So, for those of you who are willing to sacrifice the luxury of comfort and a predictable schedule, you can reduce your expenditure to be very low and can travel on a minimal travel budget. My advice to you is don’t steal, don’t trespass on property, don’t participate in the illegal market, don’t beg or don’t take advantage of others. For your own good, just don’t.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A4.tab8" class="ltx_table">
<table id="A4.tab8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.tab8.1.1.1" class="ltx_tr">
<th id="A4.tab8.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A4.tab8.1.1.1.1.1" class="ltx_text ltx_font_bold">signer</span></th>
<th id="A4.tab8.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab8.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.1.1.2.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab8.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">reference</span></span>
</span>
</th>
<th id="A4.tab8.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab8.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.1.1.3.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab8.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">human prediction</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.tab8.1.2.1" class="ltx_tr">
<th id="A4.tab8.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">4</th>
<td id="A4.tab8.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab8.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.2.1.2.1.1" class="ltx_p" style="width:199.2pt;">Protests also took place in Paris, Sofia in Bulgaria, Vilnius in Lithuania, Valetta in Malta, Tallinn in Estonia, and Edinburgh and Glasgow in Scotland.</span>
</span>
</td>
<td id="A4.tab8.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab8.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.2.1.3.1.1" class="ltx_p" style="width:199.2pt;">Protests also happened at other locations in Paris, Sofia in Bulgaria, Vilnius in Lithuania, Valletta in Malta, Tallinn in Estonia, and Edinburgh as well as Glasgow in Scotland. There were many locations.</span>
</span>
</td>
</tr>
<tr id="A4.tab8.1.3.2" class="ltx_tr">
<th id="A4.tab8.1.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab8.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab8.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.3.2.2.1.1" class="ltx_p" style="width:199.2pt;">Organisers of the protest said about 100,000 people turned up in German cities such as Berlin, Cologne, Hamburg, and Hanover. In Berlin, police estimated 6,500 protestors. Protests also took place in Paris, Sofia in Bulgaria, Vilnius in Lithuania, Valetta in Malta, Tallinn in Estonia, and Edinburgh and Glasgow in Scotland. In London, about 200 people protested outside some major copyright holders’ offices. Last month, there were major protests in Poland when that country signed ACTA, which has led to the Polish government deciding not to ratify the agreement, for now. Latvia and Slovakia have both delayed the process of joining ACTA.</span>
</span>
</td>
<td id="A4.tab8.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab8.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.3.2.3.1.1" class="ltx_p" style="width:199.2pt;">Organizations of the protests said about 100,000 people showed up in four different German towns, which are Berlin, Cologne, Hamburg, and Hanover. In Berlin, the police estimated about 2,500 protesters. There were protests happening in other locations too. Other locations include Paris, Sofia in Bulgaria, Vilnius in Lithuania, Valletta in Malta, Tallinn in Estonia, and Edinburgh as well as Glasgow in Scotland. There were many locations. In London, around 200 people protested there. Outside of some big copyright holder offices where there were several larger protests last month. In Poland, when that country signed ACTA, that caused other governments to be indecisive about signing an agreement. Latvia and Slovakia also had a delayed response in the process of joining ACTA.</span>
</span>
</td>
</tr>
<tr id="A4.tab8.1.4.3" class="ltx_tr">
<th id="A4.tab8.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">4</th>
<td id="A4.tab8.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab8.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.4.3.2.1.1" class="ltx_p" style="width:199.2pt;">"We have a year-long financial crisis, which has had its most acute moment in the past two months, and I think now the financial markets are beginning to recover."</span>
</span>
</td>
<td id="A4.tab8.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab8.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.4.3.3.1.1" class="ltx_p" style="width:199.2pt;">In the past year, there was a long financial crisis. The past two months were the worst but I think we are finally beginning to recover.</span>
</span>
</td>
</tr>
<tr id="A4.tab8.1.5.4" class="ltx_tr">
<th id="A4.tab8.1.5.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab8.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab8.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.5.4.2.1.1" class="ltx_p" style="width:199.2pt;">"Regarding the global financial situation, Zapatero continued by saying that "the financial system is a part of the economy, a crucial part. We have a year-long financial crisis, which has had its most acute moment in the past two months, and I think now the financial markets are beginning to recover."</span>
</span>
</td>
<td id="A4.tab8.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab8.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.5.4.3.1.1" class="ltx_p" style="width:199.2pt;">On the global financial situation, Zapatero said the financial system is a very important part of the economic system. The past year was a long financial crisis, especially the past two months. I think we are finally beginning to recover.</span>
</span>
</td>
</tr>
<tr id="A4.tab8.1.6.5" class="ltx_tr">
<th id="A4.tab8.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">4</th>
<td id="A4.tab8.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab8.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.6.5.2.1.1" class="ltx_p" style="width:199.2pt;">Ten years later, he led the Soviet part of the Apollo–Soyuz mission symbolizing that the Space Race was over.</span>
</span>
</td>
<td id="A4.tab8.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab8.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.6.5.3.1.1" class="ltx_p" style="width:199.2pt;">Ten years later, they were leading as shown in the Soviet Apollo Soyuz mission, which was representative of how their space race process worked effectively.</span>
</span>
</td>
</tr>
<tr id="A4.tab8.1.7.6" class="ltx_tr">
<th id="A4.tab8.1.7.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab8.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab8.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.7.6.2.1.1" class="ltx_p" style="width:199.2pt;">Leonov, also known as "cosmonaut No. 11", was part of the Soviet Union’s original team of cosmonauts. On March 18, 1965, he performed the first manned extravehicular activity (EVA), or "spacewalk", remaining alone outside the spacecraft for just over twelve minutes. He received the "Hero of the Soviet Union", the Soviet Union’s highest honor, for his work. Ten years later, he led the Soviet part of the Apollo–Soyuz mission symbolizing that the Space Race was over.</span>
</span>
</td>
<td id="A4.tab8.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab8.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.7.6.3.1.1" class="ltx_p" style="width:199.2pt;">Leonov is a well known as cosmonaut #11 who is part of the Soviet Union’s original team for Astronauts. On March 18, 1965, he was the first person to be involved with an extravehicular activity, EVA, where he went into outer space and did a spacewalk for 12 minutes before he returned. He was recognized as a hero in the Soviet Union and received highest honors for his work. Ten years later, he led the Soviet Apollo Soyuz mission, demonstrative of their space race process and how it worked smoothly and effectively.</span>
</span>
</td>
</tr>
<tr id="A4.tab8.1.8.7" class="ltx_tr">
<th id="A4.tab8.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">4</th>
<td id="A4.tab8.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab8.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.8.7.2.1.1" class="ltx_p" style="width:199.2pt;">However, in June 1956, Krushchev’s promises were put to the test when riots in Poland, where workers were protesting against food shortages and wage cuts, turned into a general protest against Communism.</span>
</span>
</td>
<td id="A4.tab8.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab8.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.8.7.3.1.1" class="ltx_p" style="width:199.2pt;">In the month of June in 1956, Krush Chev made promises which caused disruptions because people and workers in Poland were revolting due to food shortage and low earnings. It resulted in a revolt against communism.</span>
</span>
</td>
</tr>
<tr id="A4.tab8.1.9.8" class="ltx_tr">
<th id="A4.tab8.1.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<td id="A4.tab8.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab8.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.9.8.2.1.1" class="ltx_p" style="width:199.2pt;">However, in June 1956, Krushchev’s promises were put to the test when riots in Poland, where workers were protesting against food shortages and wage cuts, turned into a general protest against Communism. Although in the end, Krushchev sent in tanks to restore order, he did give way to some economic demands and agreed to appoint the popular Wladyslaw Gomulka as the new prime minister.</span>
</span>
</td>
<td id="A4.tab8.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.tab8.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.9.8.3.1.1" class="ltx_p" style="width:199.2pt;">In June of 1956, Khrushchev made empty promises to his people that resulted in disruptions. People in Poland were protesting due to food shortages and low earnings and that also subsequently resulted in a revolt against communism. So in the midst of the revolts, Khrushchev deployed military and battle tanks to restore order and control in the people. He consented to demands from the people for economic stability, food and increased earnings, and also appointed a popular prime minister named Wladyslaw Gomulka.</span>
</span>
</td>
</tr>
<tr id="A4.tab8.1.10.9" class="ltx_tr">
<th id="A4.tab8.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">4</th>
<td id="A4.tab8.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab8.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.10.9.2.1.1" class="ltx_p" style="width:199.2pt;">However, Charles went to university at Trinity College, Cambridge where he studied Anthropology and Archaeology, and later History, earning a 2:2 (a lower second class degree).</span>
</span>
</td>
<td id="A4.tab8.1.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab8.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.10.9.3.1.1" class="ltx_p" style="width:199.2pt;">However, Charles went to college at Trinity College in Cambridge. He studied Anthropology and Archaeology. He also eventually studies History. He earned a 2.2 score, which is really low, so he earned a second class degree.</span>
</span>
</td>
</tr>
<tr id="A4.tab8.1.11.10" class="ltx_tr">
<th id="A4.tab8.1.11.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t"></th>
<td id="A4.tab8.1.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab8.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.11.10.2.1.1" class="ltx_p" style="width:199.2pt;">Traditionally, the heir to the throne would go straight into the military after finishing school. However, Charles went to university at Trinity College, Cambridge where he studied Anthropology and Archaeology, and later History, earning a 2:2 (a lower second class degree). Charles was the first member of the British Royal Family to be awarded a degree.</span>
</span>
</td>
<td id="A4.tab8.1.11.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab8.1.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab8.1.11.10.3.1.1" class="ltx_p" style="width:199.2pt;">Tradition of the heir to the throne is you go to school and then you immediately go to the military. However, Charles went to university at Trinity College in Cambridge where he studied Anthropology and Archaeology there. He also eventually studied History as well. There he earned a 2.2 score, which is very low, and is a second class degree. Charles is the first member of the British royal family to have been awarded a degree.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A4.tab9" class="ltx_table">
<table id="A4.tab9.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.tab9.1.1.1" class="ltx_tr">
<th id="A4.tab9.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A4.tab9.1.1.1.1.1" class="ltx_text ltx_font_bold">signer</span></th>
<th id="A4.tab9.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab9.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab9.1.1.1.2.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab9.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">reference</span></span>
</span>
</th>
<th id="A4.tab9.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.tab9.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab9.1.1.1.3.1.1" class="ltx_p" style="width:199.2pt;"><span id="A4.tab9.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">human prediction</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.tab9.1.2.1" class="ltx_tr">
<th id="A4.tab9.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">4</th>
<td id="A4.tab9.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab9.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab9.1.2.1.2.1.1" class="ltx_p" style="width:199.2pt;">He recently lost against Raonic in the Brisbane Open.</span>
</span>
</td>
<td id="A4.tab9.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.tab9.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab9.1.2.1.3.1.1" class="ltx_p" style="width:199.2pt;">Recently, there was a loss against Raonic in the Brisbane Opening.</span>
</span>
</td>
</tr>
<tr id="A4.tab9.1.3.2" class="ltx_tr">
<th id="A4.tab9.1.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t"></th>
<td id="A4.tab9.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab9.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab9.1.3.2.2.1.1" class="ltx_p" style="width:199.2pt;">Nadal’s head to head record against the Canadian is 7–2. He recently lost against Raonic in the Brisbane Open. Nadal bagged 88% net points in the match winning 76 points in the first serve. After the match, King of Clay said, "I am just excited about being back in the final rounds of the most important events. I am here to try to win this.</span>
</span>
</td>
<td id="A4.tab9.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.tab9.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.tab9.1.3.2.3.1.1" class="ltx_p" style="width:199.2pt;">Nadal’s record against Canada in tennis was 7-2. The recent loss against Raonic in the Brisbane opening was a disappointment. Nadal impacted 88% of the points in the match since being awarded 76 points. After the game was over, the King of Clay said he was excited to return to watch the last round of the most important game event. He said a win is a must!</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.13584" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.13585" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.13585">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.13585" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.13586" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 12:02:50 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
