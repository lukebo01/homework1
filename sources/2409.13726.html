<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.13726] Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement</title><meta property="og:description" content="Non-verbal behavior is a central challenge in understanding the dynamics of a conversation and the affective states between interlocutors arising from the interaction. Although psychological research has demonstrated t…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.13726">

<!--Generated on Sat Oct  5 22:20:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Multimodal Dataset; Cultural Comparison; Engagement Prediction; Non-verbal Communication; Dyadic Interaction">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document"> Multilingual Dyadic Interaction Corpus NoXi+J: 
<br class="ltx_break">Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marius Funk
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">University of Augsburg</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Augsburg</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">Germany</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:marius.funk@uni-a.de">marius.funk@uni-a.de</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0008-3064-599X" title="ORCID identifier" class="ltx_ref">0009-0008-3064-599X</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shogo Okada
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">Japan Advanced Institute of Science and Technology</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_city">Nomi</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_country">Japan</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:okada-s@jaist.ac.jp">okada-s@jaist.ac.jp</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-9260-0403" title="ORCID identifier" class="ltx_ref">0000-0002-9260-0403</a></span>
</span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elisabeth André
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">University of Augsburg</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_city">Augsburg</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_country">Germany</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:andre@informatik.uni-augsburg.de">andre@informatik.uni-augsburg.de</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-2367-162X" title="ORCID identifier" class="ltx_ref">0000-0002-2367-162X</a></span>
</span></span>
</div>
<div class="ltx_dates">(2024; May 10th, 2024; July 18th, 2024; August 16th, 2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id10.id1" class="ltx_p">Non-verbal behavior is a central challenge in understanding the dynamics of a conversation and the affective states between interlocutors arising from the interaction. Although psychological research has demonstrated that non-verbal behaviors vary across cultures, limited computational analysis has been conducted to clarify these differences and assess their impact on engagement recognition.
To gain a greater understanding of engagement and non-verbal behaviors among a wide range of cultures and language spheres, in this study we conduct a multilingual computational analysis of non-verbal features and investigate their role in engagement and engagement prediction. To achieve this goal, we first expanded the NoXi dataset, which contains interaction data from participants living in France, Germany, and the United Kingdom, by collecting session data of dyadic conversations in Japanese and Chinese, resulting in the enhanced dataset NoXi+J. Next, we extracted multimodal non-verbal features, including speech acoustics, facial expressions, backchanneling and gestures, via various pattern recognition techniques and algorithms. Then, we conducted a statistical analysis of listening behaviors and backchannel patterns to identify culturally dependent and independent features in each language and common features among multiple languages. These features were also correlated with the engagement shown by the interlocutors. Finally, we analyzed the influence of cultural differences in the input features of LSTM models trained to predict engagement for five language datasets. A SHAP analysis combined with transfer learning confirmed a considerable correlation between the importance of input features for a language set and the significant cultural characteristics analyzed.</p>
</div>
<div class="ltx_keywords">Multimodal Dataset; Cultural Comparison; Engagement Prediction; Non-verbal Communication; Dyadic Interaction
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION; November 4–8, 2024; San Jose, Costa Rica</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION (ICMI ’24), November 4–8, 2024, San Jose, Costa Rica</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3678957.3685757</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0462-8/24/11</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Social and professional topics Cultural characteristics</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Transfer learning</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Empirical studies in HCI</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Considering cultural differences in non-verbal behavior is essential for seamless conversations in different languages. This problem has been discussed as far back as Edward T. Hall in 1959, who stressed the importance of <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">”the non-verbal language which exists in every
country of the world and among the various groups within
each country”</span> <cite class="ltx_cite ltx_citemacro_citep">(Hall, <a href="#bib.bib28" title="" class="ltx_ref">1959</a>)</cite> to understand the ways of interaction between different cultures.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The field of non-verbal behavior and backchannels in culture-dependent human-human interaction has since been extensively studied, whereas non-verbal behavior in human-computer interaction has focused predominantly on single-culture human-computer interaction <cite class="ltx_cite ltx_citemacro_citep">(Müller et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2023</a>; Artiran et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2021</a>; Newn et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2019</a>; Iizuka and Otsuka, <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>. Even though it is acknowledged as important, cultural characteristics have not been a focus in Social Signal Processing <cite class="ltx_cite ltx_citemacro_citep">(Vinciarelli et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2012</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we present a computational analysis of the cultural characteristics of multimodal non-verbal features and the effects these differences have on engagement and its prediction. For this purpose, we introduce a new multilingual multimodal interaction dataset, <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">NoXi-J</span>, which enhances the existing <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">NO</span>vice e<span id="S1.p3.1.3" class="ltx_text ltx_font_bold">X</span>pert <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">I</span>nteraction database <span id="S1.p3.1.5" class="ltx_text ltx_font_bold">NoXi</span> <cite class="ltx_cite ltx_citemacro_citep">(Cafaro et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite> by recording sessions in <span id="S1.p3.1.6" class="ltx_text ltx_font_bold">J</span>apanese and Chinese, thereby creating an enriched dataset referred to as <span id="S1.p3.1.7" class="ltx_text ltx_font_bold">NoXi+J</span>. We also extract and analyze the non-verbal features and vocal backchannels of all predominant languages (German, English, French, Japanese, and Chinese) using various machine learning models and pattern recognition techniques. We study the individual multimodal non-verbal features and investigate the differences between language sets and their correlation with engagement.
Finally, we highlight the importance of culture-sensitive approaches with machine learning engagement prediction models. We evaluate the performance of six models trained on different language speaker subsets of the NoXi+J dataset, test the model performance on other language speaker subsets and compare the performance of the model depending on the relevant non-verbal and backchanneling features.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To the best of our knowledge, there is no other comprehensive data-based analysis of the cultural differences in backchanneling and non-verbal communication and their influences on engagement in a recorded multimodal database. The addition of the Japanese and Chinese language recordings also makes it the largest openly available multicultural multimodal corpus of dyadic interaction of which we are aware.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In the following pages, we briefly describe <span id="S1.p5.1.1" class="ltx_text ltx_font_bold">NoXi+J</span>, its design process, recording system, and data; we focus on the newly collected data, and outline the manually created affective annotations. Next, we computationally analyze the data, focusing on non-verbal features, backchanneling, and speaking state and their influences on engagement. Finally, we describe a set of engagement prediction models trained on various language subsets of the complete dataset, how the difference in their performance correlated with the results of the analysis and how transfer learning affects their performance.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Scientific Background</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Non-verbal communication</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Non-verbal communication in the context of conversations involves gestures, postures, touch, facial expressions, gaze, and vocal behavior beyond the meaning of words <cite class="ltx_cite ltx_citemacro_citep">(Knapp et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">1972</a>)</cite>. Non-verbal communication can manage the flow of a conversation and therefore the turn-taking <cite class="ltx_cite ltx_citemacro_citep">(Matsumoto et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2012</a>)</cite> and influence engagement by conveying emotional states and signaling interpersonal attitudes <cite class="ltx_cite ltx_citemacro_citep">(LaFrance and Mayo, <a href="#bib.bib43" title="" class="ltx_ref">1978</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Cultural differences in non-verbal behavior have been acknowledged for a long time <cite class="ltx_cite ltx_citemacro_citep">(Hall, <a href="#bib.bib27" title="" class="ltx_ref">1976</a>; Andersen, <a href="#bib.bib3" title="" class="ltx_ref">1998</a>)</cite>. Research often focuses on facial emotions <cite class="ltx_cite ltx_citemacro_citep">(Matsumoto and Kudoh, <a href="#bib.bib50" title="" class="ltx_ref">1993</a>; Ekman, <a href="#bib.bib18" title="" class="ltx_ref">1994</a>)</cite> and differences between East Asian and European facial expressions <cite class="ltx_cite ltx_citemacro_citep">(Jack et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2012</a>; Matsumoto and Kudoh, <a href="#bib.bib50" title="" class="ltx_ref">1993</a>)</cite>.
Another focus of non-verbal communication is head-nodding, where cultural differences, especially the prevalence of Japanese head nods, are often noted <cite class="ltx_cite ltx_citemacro_citep">(Freiermuth and Hamzah, <a href="#bib.bib20" title="" class="ltx_ref">2023</a>; Kita and Ide, <a href="#bib.bib40" title="" class="ltx_ref">2007</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Backchanneling</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The concept of listener utterances that do not lead to turn-taking has been discussed as far back as Fries in 1952 <cite class="ltx_cite ltx_citemacro_citep">(Fries, <a href="#bib.bib21" title="" class="ltx_ref">1952</a>)</cite>, whereas the term <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">backchannel</span> to describe this kind of verbalisation was introduced by Yngve in 1970 <cite class="ltx_cite ltx_citemacro_citep">(Yngve, <a href="#bib.bib78" title="" class="ltx_ref">1970</a>)</cite>. Backchannels often consist of utterances such as the English <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">uh huh</span> and <span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_italic">yeah</span> <cite class="ltx_cite ltx_citemacro_citep">(White, <a href="#bib.bib76" title="" class="ltx_ref">1989</a>)</cite> but can also be longer phrases such as the Japanese <span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_italic">sou desu ne</span> <cite class="ltx_cite ltx_citemacro_citep">(Hanzawa, <a href="#bib.bib29" title="" class="ltx_ref">2012</a>)</cite>. They can also include head nods <cite class="ltx_cite ltx_citemacro_citep">(Maynard, <a href="#bib.bib51" title="" class="ltx_ref">1987</a>)</cite> or laughter and exhaling sounds <cite class="ltx_cite ltx_citemacro_citep">(Ichinohara, <a href="#bib.bib35" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Studies often highlight that Japanese interlocutors use backchannels with much higher frequency compared to English or Chinese speakers<cite class="ltx_cite ltx_citemacro_citep">(White, <a href="#bib.bib76" title="" class="ltx_ref">1989</a>; Maynard, <a href="#bib.bib52" title="" class="ltx_ref">1990</a>; Cutrone, <a href="#bib.bib15" title="" class="ltx_ref">2005</a>; Kita and Ide, <a href="#bib.bib40" title="" class="ltx_ref">2007</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Turn-taking</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Turn-taking describes the changing of the active speaker in a conversation. Each time such a change takes place is classified as an instance of turn-taking <cite class="ltx_cite ltx_citemacro_citep">(Wiemann and Knapp, <a href="#bib.bib77" title="" class="ltx_ref">2006</a>)</cite>. The role division of the active speaker and listener and the issues that arrive when overlapping talk occurs significantly impact the course of conversations <cite class="ltx_cite ltx_citemacro_citep">(Schegloff, <a href="#bib.bib65" title="" class="ltx_ref">2000</a>)</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Turn-taking is crucial for the management of the flow of conversations and is often indicated by non-verbal communication <cite class="ltx_cite ltx_citemacro_citep">(Skantze, <a href="#bib.bib67" title="" class="ltx_ref">2021</a>)</cite>. Pauses can also indicate turn-taking, although short pauses between speech are common without turn-taking occurring <cite class="ltx_cite ltx_citemacro_citep">(ten Bosch et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2005</a>)</cite>. Turn-taking and its timing have been shown to noticeably influence engagement <cite class="ltx_cite ltx_citemacro_citep">(Chao and Thomaz, <a href="#bib.bib14" title="" class="ltx_ref">2012</a>; Cafaro et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Engagement</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Engagement refers to the interest a person shows in an ongoing conversation or interaction.
It can be measured either continuously or at specific interaction points. It can be assessed between participants or for individual interlocutors separately. Engagement may be directed toward a human interlocutor, a system, or an artificial agent <cite class="ltx_cite ltx_citemacro_citep">(Glas and Pelachaud, <a href="#bib.bib24" title="" class="ltx_ref">2015a</a>)</cite>.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">One of the earliest definitions in the context of human-computer interaction comes from Sidner et al. <cite class="ltx_cite ltx_citemacro_citep">(Sidner et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2005</a>)</cite>, who describes it as
<span id="S2.SS4.p2.1.1" class="ltx_text ltx_font_italic">”the process by which individuals in an interaction start, maintain and end their perceived connection to one another”</span>. Sidner et al. emphasize the role of non-verbal behavior and turn-taking as indicators of engagement.
In the context of dyadic conversations, Poggi <cite class="ltx_cite ltx_citemacro_citep">(Poggi, <a href="#bib.bib61" title="" class="ltx_ref">2007</a>)</cite> defines engagement as
<span id="S2.SS4.p2.1.2" class="ltx_text ltx_font_italic">”the value that a participant in an interaction attributes to the goal of being together with the other participant(s) and of continuing the interaction”</span>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Related Work</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In recent years, the analysis and prediction of non-verbal communication, turn-taking and backchannels have gained importance in interaction modeling <cite class="ltx_cite ltx_citemacro_citep">(Ortega et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2023</a>; Jain et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2021</a>; Brusco et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2020</a>; Levinson and Torreira, <a href="#bib.bib44" title="" class="ltx_ref">2015</a>)</cite>. Researchers have focused on gaze and its role in recognizing intention <cite class="ltx_cite ltx_citemacro_citep">(Newn et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2019</a>)</cite>, how non-verbal actions signal human preferences <cite class="ltx_cite ltx_citemacro_citep">(Candon et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>, the estimation of agreement <cite class="ltx_cite ltx_citemacro_citep">(Müller et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2022</a>)</cite>, head nod detection <cite class="ltx_cite ltx_citemacro_citep">(Artiran et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite> and backchannel prediction using multimodal approaches <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2024</a>; Iizuka and Otsuka, <a href="#bib.bib36" title="" class="ltx_ref">2023</a>; Müller et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Attempts have been made to equip virtual agents and robots with culture-specific behaviors. In this context, we refer to a survey that reports on how emotions are portrayed in different cultures and explores how virtual agents and robots can simulate culture-specific emotional behaviors <cite class="ltx_cite ltx_citemacro_citep">(André, <a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>. Endrass et al. <cite class="ltx_cite ltx_citemacro_citep">(Endrass et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2013</a>)</cite> developed computational models to replicate prototypical behaviors of German and Japanese cultures in virtual agents, taking into account verbal behavior, communication management, and non-verbal behavior. Meixuan et al. <cite class="ltx_cite ltx_citemacro_citep">(LI et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2020</a>)</cite> collected annotated voice responses in three languages — Chinese, English, and Japanese — with the aim of developing emotionally attuned robot models.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Many multimodal datasets focus on behavioral and emotional analysis, such as the Cardiff Conversation Dataset <cite class="ltx_cite ltx_citemacro_citep">(Aubrey et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2013</a>)</cite>, which contains 30 conversations with annotation for head movement, speaker activity, and non-verbal utterances, or SEMAINE <cite class="ltx_cite ltx_citemacro_citep">(McKeown et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2010</a>)</cite>, which features 150 recordings with emotional annotations. However, multicultural conversation datasets for comprehensive non-verbal analysis are rare as researchers often focus on text analysis <cite class="ltx_cite ltx_citemacro_citep">(Qiu et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2022</a>)</cite> or present study results without making their datasets publicly available <cite class="ltx_cite ltx_citemacro_citep">(He and Huang, <a href="#bib.bib31" title="" class="ltx_ref">2014</a>)</cite>. A few examples of datasets featuring multicultural interactions are the RECOLA Dataset <cite class="ltx_cite ltx_citemacro_citep">(Ringeval et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2013</a>)</cite>, which features collaborative and affective interactions with French, German, Italian and Portuguese participants, and
the Sentiment Analysis in the Wild (SEWA) dataset, which contains recordings of
British, German, Hungarian, Greek, Serbian, and Chinese participants <cite class="ltx_cite ltx_citemacro_citep">(Kossaifi et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Several reviews indicate the growing interest in engagement and its significance in human-computer interaction <cite class="ltx_cite ltx_citemacro_citep">(Doherty and Doherty, <a href="#bib.bib17" title="" class="ltx_ref">2018</a>; Glas and Pelachaud, <a href="#bib.bib25" title="" class="ltx_ref">2015b</a>; Oertel et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2020</a>)</cite>. Various techniques for engagement prediction <cite class="ltx_cite ltx_citemacro_citep">(Bohus and Horvitz, <a href="#bib.bib9" title="" class="ltx_ref">2009</a>; Inoue et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2019</a>; Nakano and Ishii, <a href="#bib.bib56" title="" class="ltx_ref">2010</a>)</cite> have been developed as part of interactive systems.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Research on the prediction of engagement has already been conducted on the original NoXi dataset <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>; Dermouche and Pelachaud, <a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>. However, the focus of this work was on the performance in the prediction task and not on the analysis of culture-specific aspects of the dataset.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Data Collection</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The original NoXi database, first introduced by Cafaro et al. <cite class="ltx_cite ltx_citemacro_citep">(Cafaro et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>, contains recordings of dyadic conversations between two interlocutors in the roles of expert and novice. In a session ranging between seven and 31 minutes the expert talks about one or more topics they are passionate or knowledgeable about, whereas the novice listens and discusses the introduced topic with the expert. The idea was to obtain a dataset of natural interactions in an expert-novice knowledge-sharing context.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The database contains conversations primarily in German, English, and French. NoXi-J extends it by adding 48 conversations in Japanese and 18 conversations in Chinese to provide a more culturally diverse dataset.
The newly recorded sessions follow the same structure as the original sessions. We will briefly explain the design principles and the recording system used to record the new sessions. For a more detailed explanation, see the initial paper <cite class="ltx_cite ltx_citemacro_citep">(Cafaro et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Design Principle</h3>

<figure id="S4.F1" class="ltx_figure">
<p id="S4.F1.1" class="ltx_p ltx_align_center"><span id="S4.F1.1.1" class="ltx_text"><img src="/html/2409.13726/assets/setup.jpg" id="S4.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="225" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>NoXi recording. Expert (left) and novice (right).</figcaption>
</figure>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Setting</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">Screen-mediated recording was chosen for two purposes: to record a face-to-face conversation without requiring multiple cameras recording from different angles and to create a setup more similar to an interaction between a human and a virtual agent. To ensure the capture of facial expressions, gestures, speech and full body movements (e.g. head touch), the participants were recorded in a standing position.
The setup used for NoXi and NoXi-J was nearly the same, with minor changes, such as the use of headphones instead of speakers to reduce echo in the case of NoXi-J.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Interaction</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">The recordings consist of spontaneous interactions that are focused on knowledge transfer and information retrieval but also include planned occurrences of unexpected events (e.g. interruptions). The conversations were not interrupted after the target length of 10 minutes, leading to some interactions exceeding 30 minutes. The actual setup of the interactions for the Asian recordings can be seen in Figure <a href="#S4.F1" title="Figure 1 ‣ 4.1. Design Principle ‣ 4. Data Collection ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Participants</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">In the European sessions, participants were recruited from local research facilities and their immediate social circles. For the newly recorded Asian sessions, participants were also recruited from local research facilities and, for many Japanese sessions, through an employment agency. This approach provided a wide variety of relationships between expert-novice dyads, ranging from zero-acquaintance situations <cite class="ltx_cite ltx_citemacro_citep">(Ambady et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">1995</a>)</cite> to spouses.</p>
</div>
</section>
<section id="S4.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4. </span>Unexpected Events</h4>

<div id="S4.SS1.SSS4.p1" class="ltx_para">
<p id="S4.SS1.SSS4.p1.1" class="ltx_p">One of the goals was to obtain occurrences of unexpected events. In addition to events such as spontaneous debates during the session, we artificially injected unexpected events during the recording sessions. These events were one of two types of interruptions. Approximately five minutes after the start of a session we either called the novice on their mobile phone (i.e., CALL-IN) or physically entered the recording room to adjust the microphone or ask them to hand over something (i.e., WALK-IN). The novice was informed about the possibility of one of these events occurring prior to the session, in contrast to the expert, who was intended to be surprised and annoyed by the interruption.</p>
</div>
</section>
<section id="S4.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.5. </span>Recording Protocol</h4>

<div id="S4.SS1.SSS5.p1" class="ltx_para">
<p id="S4.SS1.SSS5.p1.1" class="ltx_p">The recording protocol had slight differences between the European and Asian parts of the corpus. For the European recordings, the participants were received and instructed in different rooms, whereas for the Asian recordings, the initial explanation was given in a shared room before the participants were split into different rooms.
We then primed the novice about the functional interruption, set up their microphone, and indicated where the participants had to stand (also indicated by a marker on the floor or wall). The session was monitored in a separate room.
After the conversation concluded naturally, the participants were given questionnaires (see Section <a href="#S4.SS3" title="4.3. Collected Data ‣ 4. Data Collection ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>), informed about their compensation, and debriefed.</p>
</div>
<div id="S4.SS1.SSS5.p2" class="ltx_para">
<p id="S4.SS1.SSS5.p2.1" class="ltx_p">Both participants gave their informed consent before the start of the recording. They consented to the use of the recorded data for scientific research and noncommercial applications. The participants had three choices regarding the usage of their data. All participants agreed to (1) the use of their data within the PANORAMA project consortium.
Additionally, most participants agreed to (2) the usage of the data in academic conferences, publications and/or as part of teaching material and to (3) the usage of the data for academic and non-commercial applications to third-party users internationally, provided that those parties upholding the same ethical standards as the PANORAMA Project.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Recording System</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The data were recorded using Microsoft Kinect 2 devices for full HD video streams and ambient noise capture. Furthermore, low-noise recordings of voices were obtained using dynamic head-set microphones (Shure WH20XLR connected via a TASCAM US-322). The Kinect devices were placed over 55” flat screens. Both were connected to PCs (i7, 16GB-32GB of RAM). Each room’s system captured and stored the recorded footage and signal streams locally. A third PC observed the interaction from another room. All PCs were connected over LAN.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">To sync the recordings, a two-step synchronization was used. Once all the sensors were connected and the setup was completed, we used a network broadcast from the observer room PC to start recording in the expert’s room and novice’s room simultaneously. The system was implemented with the Social Signal Interpretation framework <cite class="ltx_cite ltx_citemacro_citep">(Wagner et al<span class="ltx_text">.</span>, <a href="#bib.bib73" title="" class="ltx_ref">2013</a>)</cite>. For more details regarding the setup and the frameworks used, we refer to the introductory paper of NoXi <cite class="ltx_cite ltx_citemacro_citep">(Cafaro et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Collected Data</h3>

<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Lang.</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Ses.</span></th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Part.</span></th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Avg. Dur.</span></th>
<th id="S4.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Std. Dur.</span></th>
<th id="S4.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">Tot. Dur.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">DE</th>
<th id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">19</th>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29 (5/24)</td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17:56</td>
<td id="S4.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">05:56</td>
<td id="S4.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">05:38</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">FR</th>
<th id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">21</th>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">32 (8/20)</td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">20:12</td>
<td id="S4.T1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">06:35</td>
<td id="S4.T1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r">07:20</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">EN</th>
<th id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">32</th>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">26 (11/14)</td>
<td id="S4.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">16:49</td>
<td id="S4.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">05:55</td>
<td id="S4.T1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r">08:35</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">JP</th>
<th id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">48</th>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">48 (18/30)</td>
<td id="S4.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">14:30</td>
<td id="S4.T1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r">04:06</td>
<td id="S4.T1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r">11:36</td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<th id="S4.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">ZH</th>
<th id="S4.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">18</th>
<td id="S4.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">18 (10/8)</td>
<td id="S4.T1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">15:16</td>
<td id="S4.T1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r">03:22</td>
<td id="S4.T1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r">04:35</td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<th id="S4.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Other</th>
<th id="S4.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">12</th>
<td id="S4.T1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r">18 (5/5)</td>
<td id="S4.T1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">17:37</td>
<td id="S4.T1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r">07:09</td>
<td id="S4.T1.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r">03:28</td>
</tr>
<tr id="S4.T1.1.8.7" class="ltx_tr">
<th id="S4.T1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Total</th>
<th id="S4.T1.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">150</th>
<td id="S4.T1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">153 (54/99)</td>
<td id="S4.T1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">16:36</td>
<td id="S4.T1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">05:44</td>
<td id="S4.T1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">41:11</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Overview of all the recorded NoXi sessions. From left to right: Language of the recording, number of recording sessions, number of participants (female/male), average and standard deviation of the recording duration (mm:ss), and the total duration (hh:mm). Some participants were used in sessions of multiple languages.</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The experiment was administered in four countries, with NoXi being conducted in France, Germany, and the UK, and NoXi-J being conducted in Japan. In addition to the Japanese sessions, we decided to increase the diversity and to supplement the dataset with Chinese sessions, as many native Chinese speakers were available at the recording location. Besides the five primary languages, a smaller number of recordings of four other languages (Spanish, Indonesian, Italian and Arabic) was also collected. A summary of the recorded sessions divided by primary language can be found in Table <a href="#S4.T1" title="Table 1 ‣ 4.3. Collected Data ‣ 4. Data Collection ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
In addition to session details, we recorded demographic information about the participants including their age and gender as well as their self-assigned cultural identity. A breakdown of the five primary languages and their age distributions can be seen in Figure <a href="#S4.F2" title="Figure 2 ‣ 4.3. Collected Data ‣ 4. Data Collection ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
We collected the discussed topics, proficiency of the language spoken for both participants, and the social relationship level between the two participants (e.g. zero acquaintance, friends). All participants provided a self-assessment of their personality on the basis of the Big 5 model <cite class="ltx_cite ltx_citemacro_citep">(Goldberg, <a href="#bib.bib26" title="" class="ltx_ref">1990</a>)</cite> by using descriptions for Saucier’s Mini-Markers set of adjectives (consisting of 40 adjectives) <cite class="ltx_cite ltx_citemacro_citep">(Saucier, <a href="#bib.bib64" title="" class="ltx_ref">1994</a>)</cite>.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<p id="S4.F2.1" class="ltx_p ltx_align_center"><span id="S4.F2.1.1" class="ltx_text"><img src="/html/2409.13726/assets/x1.png" id="S4.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="221" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Age distribution of the speakers of the 5 primary recorded languages: German (DE), French (FR), English (EN), Japanese (JP) and Chinese (ZH). <cite class="ltx_cite ltx_citemacro_citep">(Cafaro et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite></figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Anonymized data are available for the NoXi+J dataset upon request from the authors at the e-mail address noxi+j@hcai.eu.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Annotations</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Over 40 annotators from 4 countries (Germany, France, the UK, and Japan) were involved in the annotation of the NoXi+J database. Annotations were made and managed using the freely available annotation tool NOVA<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/hcmlab/nova</span></span></span>.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">In this paper, we focus exclusively on the manually created engagement annotations (see Section <a href="#S2.SS4" title="2.4. Engagement ‣ 2. Scientific Background ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>). Similar to the original NoXi corpus, engagement in the NoXi-J dataset has been annotated by three or more individuals. However, this excludes the complete Chinese language set and many Japanese language sessions, which were only recently recorded, and contain fewer annotations. To minimize annotator bias, the average of all annotations for each session and role (expert or novice) is used for further analysis. The engagement annotations assign values between 0 and 1 to every frame of the dataset for the perceived engagement at that moment.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">Some differences may arise from annotator bias <cite class="ltx_cite ltx_citemacro_citep">(Hovy and Prabhumoye, <a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>.
This is unavoidable as establishing a definitive ground truth is not possible, not even with self-reporting <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite>.
To determine the extent of annotator bias, we calculated the intercoder reliability <cite class="ltx_cite ltx_citemacro_citep">(O’Connor and Joffe, <a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite>. Using the ICCk3 value of the intraclass correlation coefficient, we calculated an overall intercoder agreement of 0.63. The Mean Absolute Error between annotators was between 0.14 and 0.15 for the engagement scores of 0-1 for all annotated languages.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Data Analysis</h2>

<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>List of all used features for novice and expert.</figcaption>
<div id="S5.T2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:367pt;vertical-align:-7.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.6pt,0.5pt) scale(0.997462794408074,0.997462794408074) ;">
<table id="S5.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Feature</span></td>
<td id="S5.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Dim</span></td>
<td id="S5.T2.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Explanation</span></td>
</tr>
<tr id="S5.T2.1.1.2.2" class="ltx_tr">
<td id="S5.T2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Engagement</td>
<td id="S5.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S5.T2.1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Continuous annotation of engagement</td>
</tr>
<tr id="S5.T2.1.1.3.3" class="ltx_tr">
<td id="S5.T2.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Body Properties</td>
<td id="S5.T2.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20</td>
<td id="S5.T2.1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">20 different body properties such
as <span id="S5.T2.1.1.3.3.3.1" class="ltx_text ltx_font_italic">Arms open</span>, <span id="S5.T2.1.1.3.3.3.2" class="ltx_text ltx_font_italic">Energy Head</span>, etc.</td>
</tr>
<tr id="S5.T2.1.1.4.4" class="ltx_tr">
<td id="S5.T2.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Action Units</td>
<td id="S5.T2.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17</td>
<td id="S5.T2.1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">AU26 (Jaw Open)</td>
</tr>
<tr id="S5.T2.1.1.5.5" class="ltx_tr">
<td id="S5.T2.1.1.5.5.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S5.T2.1.1.5.5.2" class="ltx_td ltx_border_r"></td>
<td id="S5.T2.1.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r">AU18 (Lip Pucker)</td>
</tr>
<tr id="S5.T2.1.1.6.6" class="ltx_tr">
<td id="S5.T2.1.1.6.6.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S5.T2.1.1.6.6.2" class="ltx_td ltx_border_r"></td>
<td id="S5.T2.1.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r">AU30 (Jaw Slide)</td>
</tr>
<tr id="S5.T2.1.1.7.7" class="ltx_tr">
<td id="S5.T2.1.1.7.7.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S5.T2.1.1.7.7.2" class="ltx_td ltx_border_r"></td>
<td id="S5.T2.1.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r">AU20 (Lip Stretcher) (Left,Right)</td>
</tr>
<tr id="S5.T2.1.1.8.8" class="ltx_tr">
<td id="S5.T2.1.1.8.8.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S5.T2.1.1.8.8.2" class="ltx_td ltx_border_r"></td>
<td id="S5.T2.1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_r">AU12 (Lip Corner Puller) (Left,Right)</td>
</tr>
<tr id="S5.T2.1.1.9.9" class="ltx_tr">
<td id="S5.T2.1.1.9.9.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S5.T2.1.1.9.9.2" class="ltx_td ltx_border_r"></td>
<td id="S5.T2.1.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r">AU15 (Lip Corner Depressor) (Left,Right)</td>
</tr>
<tr id="S5.T2.1.1.10.10" class="ltx_tr">
<td id="S5.T2.1.1.10.10.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S5.T2.1.1.10.10.2" class="ltx_td ltx_border_r"></td>
<td id="S5.T2.1.1.10.10.3" class="ltx_td ltx_align_left ltx_border_r">AU16 (Lower Lip Depressor) (Left,Right)</td>
</tr>
<tr id="S5.T2.1.1.11.11" class="ltx_tr">
<td id="S5.T2.1.1.11.11.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S5.T2.1.1.11.11.2" class="ltx_td ltx_border_r"></td>
<td id="S5.T2.1.1.11.11.3" class="ltx_td ltx_align_left ltx_border_r">AU13 (Cheek Puff) (Left,Right)</td>
</tr>
<tr id="S5.T2.1.1.12.12" class="ltx_tr">
<td id="S5.T2.1.1.12.12.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S5.T2.1.1.12.12.2" class="ltx_td ltx_border_r"></td>
<td id="S5.T2.1.1.12.12.3" class="ltx_td ltx_align_left ltx_border_r">AU43 (Eye Closed) (Left,Right)</td>
</tr>
<tr id="S5.T2.1.1.13.13" class="ltx_tr">
<td id="S5.T2.1.1.13.13.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S5.T2.1.1.13.13.2" class="ltx_td ltx_border_r"></td>
<td id="S5.T2.1.1.13.13.3" class="ltx_td ltx_align_left ltx_border_r">AU4 (Eyebrow Lowerer) (Left,Right)</td>
</tr>
<tr id="S5.T2.1.1.14.14" class="ltx_tr">
<td id="S5.T2.1.1.14.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Fluidity</td>
<td id="S5.T2.1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S5.T2.1.1.14.14.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Fluidity of body movements</td>
</tr>
<tr id="S5.T2.1.1.15.15" class="ltx_tr">
<td id="S5.T2.1.1.15.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Head Rotations</td>
<td id="S5.T2.1.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3</td>
<td id="S5.T2.1.1.15.15.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pitch, Yaw, and Roll of the head</td>
</tr>
<tr id="S5.T2.1.1.16.16" class="ltx_tr">
<td id="S5.T2.1.1.16.16.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Spatial Extent</td>
<td id="S5.T2.1.1.16.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S5.T2.1.1.16.16.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Usage of space by movement</td>
</tr>
<tr id="S5.T2.1.1.17.17" class="ltx_tr">
<td id="S5.T2.1.1.17.17.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Overall Activation</td>
<td id="S5.T2.1.1.17.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S5.T2.1.1.17.17.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Overall movement</td>
</tr>
<tr id="S5.T2.1.1.18.18" class="ltx_tr">
<td id="S5.T2.1.1.18.18.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Voice Activation</td>
<td id="S5.T2.1.1.18.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S5.T2.1.1.18.18.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Instances of human speech</td>
</tr>
<tr id="S5.T2.1.1.19.19" class="ltx_tr">
<td id="S5.T2.1.1.19.19.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Transcription</td>
<td id="S5.T2.1.1.19.19.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S5.T2.1.1.19.19.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Transcription of speech (Only for reference)</td>
</tr>
<tr id="S5.T2.1.1.20.20" class="ltx_tr">
<td id="S5.T2.1.1.20.20.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Head nod</td>
<td id="S5.T2.1.1.20.20.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1</td>
<td id="S5.T2.1.1.20.20.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Instances of intense vertical head movement</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S5.F3" class="ltx_figure">
<p id="S5.F3.1" class="ltx_p ltx_align_center"><span id="S5.F3.1.1" class="ltx_text"><img src="/html/2409.13726/assets/x2.png" id="S5.F3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="121" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Schematic conversation depicting turn-taking, the division of the data by speaking state, engagement and instances of high positive and high negative engagement correlation. <span id="S5.F3.3.1" class="ltx_text ltx_font_bold">VBC</span> describes vocal backchanneling instances.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Features</h3>

<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1. </span>Features of the NoXi corpus</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">For the following analysis of intercultural differences of non-verbal features, engagement, and their mutual dependencies, we decided to use a total of 94 features (see Table <a href="#S5.T2" title="Table 2 ‣ 5. Data Analysis ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). We focus on the novice’s non-verbal
characteristics, their behavior, and the impact on engagement. However, we also analyzed expert features in relation to the novice’s engagement. Engagement has not yet been annotated for the Chinese language session. Therefore only the evaluation of the feature differences will consider these recordings.</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2. </span>Feature extractions</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">In addition to the immediate output stream of Kinect such as video and joint position data
, we extracted body properties indicative of the expression of emotion <cite class="ltx_cite ltx_citemacro_citep">(Wallbott, <a href="#bib.bib74" title="" class="ltx_ref">1998</a>)</cite> such as <span id="S5.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">Head Touch</span>, <span id="S5.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_italic">Arms crossed</span>, <span id="S5.SS1.SSS2.p1.1.3" class="ltx_text ltx_font_italic">Energy Head</span>, <span id="S5.SS1.SSS2.p1.1.4" class="ltx_text ltx_font_italic">Fluidity</span>, <span id="S5.SS1.SSS2.p1.1.5" class="ltx_text ltx_font_italic">Spatial Extent</span>, <span id="S5.SS1.SSS2.p1.1.6" class="ltx_text ltx_font_italic">Energy Hands</span> and <span id="S5.SS1.SSS2.p1.1.7" class="ltx_text ltx_font_italic">Overall Activation</span> of the body using NOVA’s integrated extraction tools.
For the computation of gestural expressivity features (fluidity, overall activation, spatial extent, and energy) we refer to the appendix of an earlier paper describing the NOVA annotation tool <cite class="ltx_cite ltx_citemacro_citep">(Baur et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2015</a>)</cite>. We also used an external Nova Server<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/hcmlab/nova-server</span></span></span> with integrated Pyannote<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/pyannote/pyannote-audio</span></span></span> for Voice Activity Detection (VAD) and whisperX<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/m-bain/whisperX</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Bain et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite> for the transcription of speech.</p>
</div>
</section>
<section id="S5.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3. </span>Computed features</h4>

<div id="S5.SS1.SSS3.p1" class="ltx_para">
<p id="S5.SS1.SSS3.p1.1" class="ltx_p">Additional features for the analysis of these data were computed. We used the extracted voice activation data to determine turns and distinguish active speech from vocal backchanneling. We attributed the turn to the first speaker, who holds it until both interlocutors either become silent (i.e. no voice activation is determined) for two seconds (50 frames) or the speaker becomes silent after the interlocutor has spoken for more than two seconds. In the first case, no one holds the turn, and we move on to the next speaker. In the second case, turn-taking takes place. All voice activation instances in between are classified as vocal backchanneling (VBC) (see Figure <a href="#S5.F3" title="Figure 3 ‣ 5. Data Analysis ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The idea for this division was influenced by Bosch <cite class="ltx_cite ltx_citemacro_citep">(ten Bosch et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2005</a>)</cite> and his discussion of overlap and turn-taking.</p>
</div>
<div id="S5.SS1.SSS3.p2" class="ltx_para">
<p id="S5.SS1.SSS3.p2.1" class="ltx_p">Head nods were identified by using the pitch, yaw, and roll angles of the head position extracted from the <span id="S5.SS1.SSS3.p2.1.1" class="ltx_text ltx_font_italic">head.stream</span> files. Rapid switches of over 2.5 degrees between up and down movement without extensive other movements were classified as head nods. Changes in the threshold led to different absolute numbers, while the distribution between cultures remained similar.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Intercultural data comparison</h3>

<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1. </span>Initial cultural comparison</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">Our first focus was on the general assessment of the recorded features. To reduce outliers and make the data more comparable, we calculated the standard score (z-score) for every data point. We then performed an initial Analysis of Variance (ANOVA) <cite class="ltx_cite ltx_citemacro_citep">(St»hle and Wold, <a href="#bib.bib68" title="" class="ltx_ref">1989</a>)</cite> for every feature of the session averages between languages.
The results show an average F-value of <math id="S5.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS2.SSS1.p1.1.m1.1a"><mo id="S5.SS2.SSS1.p1.1.m1.1.1" xref="S5.SS2.SSS1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p1.1.m1.1b"><csymbol cd="latexml" id="S5.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p1.1.m1.1c">\sim</annotation></semantics></math>43.000 with
a minimum F-value of 15.5, revealing severe differences in the features between datasets.
Table <a href="#S5.T3" title="Table 3 ‣ 5.2.1. Initial cultural comparison ‣ 5.2. Intercultural data comparison ‣ 5. Data Analysis ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows clear similarities for the inner-groups European language (NoXi) and Asian language (NoXi-J) in a subsequent pairwise Tukey-Kramer test <cite class="ltx_cite ltx_citemacro_citep">(Tukey, <a href="#bib.bib71" title="" class="ltx_ref">1949</a>; Hayter, <a href="#bib.bib30" title="" class="ltx_ref">1984</a>)</cite>.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Sum of the absolute values of all the Tukey-Kramer test averages between the five languages.</figcaption>
<div id="S5.T3.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:90.5pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.1pt,0.2pt) scale(0.994761540693509,0.994761540693509) ;">
<table id="S5.T3.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.1.2.1" class="ltx_text ltx_font_bold">German (DE)</span></td>
<td id="S5.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">French (FR)</span></td>
<td id="S5.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.1.4.1" class="ltx_text ltx_font_bold">English (EN)</span></td>
<td id="S5.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Japanese (JP)</span></td>
<td id="S5.T3.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Chinese (ZH)</span></td>
</tr>
<tr id="S5.T3.1.1.2.2" class="ltx_tr">
<td id="S5.T3.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">DE</td>
<td id="S5.T3.1.1.2.2.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.8</td>
<td id="S5.T3.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18.5</td>
<td id="S5.T3.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25.3</td>
<td id="S5.T3.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28.2</td>
</tr>
<tr id="S5.T3.1.1.3.3" class="ltx_tr">
<td id="S5.T3.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FR</td>
<td id="S5.T3.1.1.3.3.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.3.3.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.4</td>
<td id="S5.T3.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.4</td>
<td id="S5.T3.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.7</td>
</tr>
<tr id="S5.T3.1.1.4.4" class="ltx_tr">
<td id="S5.T3.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">EN</td>
<td id="S5.T3.1.1.4.4.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.4.4.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.4.4.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.5</td>
<td id="S5.T3.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.0</td>
</tr>
<tr id="S5.T3.1.1.5.5" class="ltx_tr">
<td id="S5.T3.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">JP</td>
<td id="S5.T3.1.1.5.5.2" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.5.5.3" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.5.5.4" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.5.5.5" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">16.1</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2. </span>Cultural differences between feature averages</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">We found many noteworthy feature differences between cultures. The Chinese participants activated AU12, which is commonly associated with a smile, the least, followed closely by the English-speaking participants. In contrast, the Japanese smiled the most. German novices held their arms in open poses observably longer than any other group of participants. In contrast, Japanese and Chinese interlocutors adopted an arms-crossed pose for noticeably longer periods, specifically during backchanneling, while otherwise displaying behaviors similar to that of the European participants.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Engagement in intercultural data comparison</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Next, we analysed engagement in the annotated sessions. Overall we found that the Japanese sessions exhibit a noticeably higher average level of annotated engagement compared to the European sessions (see Figure <a href="#S5.F6" title="Figure 6 ‣ 5.3.4. Mutual engagement and speaking turns ‣ 5.3. Engagement in intercultural data comparison ‣ 5. Data Analysis ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). As no Chinese language session engagement annotations have yet been created, we discuss only German, English, French and Japanese language data from this point on.</p>
</div>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1. </span>Engagement correlation with recorded and extracted features</h4>

<figure id="S5.F4" class="ltx_figure">
<p id="S5.F4.1" class="ltx_p ltx_align_center"><span id="S5.F4.1.1" class="ltx_text"><img src="/html/2409.13726/assets/x3.png" id="S5.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="183" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Correlations between annotated novice engagement and a selection of relevant features.</figcaption>
</figure>
<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.1" class="ltx_p">We began by examining the relationship between input features and engagement within the inter-lingual dataset. Specifically, we calculated the Pearson correlation coefficient <span id="S5.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_italic">r</span> between features and novice engagement as our primary metric. We found significant differences in the correlation of features and engagement between the NoXi and NoXi-J recordings (see Figure <a href="#S5.F4" title="Figure 4 ‣ 5.3.1. Engagement correlation with recorded and extracted features ‣ 5.3. Engagement in intercultural data comparison ‣ 5. Data Analysis ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). All correlations presented in this section are statistically significant with p-values of less than 0.001. <span id="S5.SS3.SSS1.p1.1.2" class="ltx_text ltx_font_italic">N</span> is the total number of frame values for each feature in each language (DE=511,200, FR=666,950, EN=784,675, JP=1,043,700).</p>
</div>
<div id="S5.SS3.SSS1.p2" class="ltx_para">
<p id="S5.SS3.SSS1.p2.1" class="ltx_p">Activation of AU12 (Smile) shows the strongest correlation with engagement in the Japanese dataset (r=0.27) compared to German (r=0.11), French (r=0.14) and English (r=0.12).
In contrast, overall activation is more strongly correlated with engagement the French (r=0.22), German (r=0.15) and English (r=0.22) datasets, but shows almost no correlation with the Japanese engagement (r=0.04).</p>
</div>
<div id="S5.SS3.SSS1.p3" class="ltx_para">
<p id="S5.SS3.SSS1.p3.1" class="ltx_p">A notable difference between German and French engagement correlations is observed in the energy level of the hands. While the German data show a negative correlation between engagement and hand energy (r=-0.13), the French data reveal a positive correlation (r=0.12). The other languages show no noteworthy correlation in this regard.</p>
</div>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2. </span>Head nods</h4>

<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Computed features for the novice, from left to right: Session language, registered head nods (HN), HN per minute, time ratio spent listening, and time ratio spent verbal backchanneling (VBC).</figcaption>
<div id="S5.T4.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:109.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(3.4pt,-0.9pt) scale(1.01589826017218,1.01589826017218) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Language</span></th>
<th id="S5.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Head Nods</span></th>
<th id="S5.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Head Nods per minute</span></th>
<th id="S5.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Listening ratio</span></th>
<th id="S5.T4.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.1.5.1" class="ltx_text ltx_font_bold">VBC ratio</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.2.1" class="ltx_tr">
<th id="S5.T4.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">DE</th>
<th id="S5.T4.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">1830</th>
<td id="S5.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.5</td>
<td id="S5.T4.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.0%</td>
<td id="S5.T4.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.9%</td>
</tr>
<tr id="S5.T4.1.1.3.2" class="ltx_tr">
<th id="S5.T4.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">FR</th>
<th id="S5.T4.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">2380</th>
<td id="S5.T4.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.1</td>
<td id="S5.T4.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75.3%</td>
<td id="S5.T4.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.1%</td>
</tr>
<tr id="S5.T4.1.1.4.3" class="ltx_tr">
<th id="S5.T4.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">EN</th>
<th id="S5.T4.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">4239</th>
<td id="S5.T4.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.9</td>
<td id="S5.T4.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79,4%</td>
<td id="S5.T4.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.5%</td>
</tr>
<tr id="S5.T4.1.1.5.4" class="ltx_tr">
<th id="S5.T4.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">JP</th>
<th id="S5.T4.1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">5851</th>
<td id="S5.T4.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.0</td>
<td id="S5.T4.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.5%</td>
<td id="S5.T4.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.5%</td>
</tr>
<tr id="S5.T4.1.1.6.5" class="ltx_tr">
<th id="S5.T4.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">ZH</th>
<th id="S5.T4.1.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">2167</th>
<td id="S5.T4.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">7.5</td>
<td id="S5.T4.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">89.6%</td>
<td id="S5.T4.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">8.9%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p id="S5.SS3.SSS2.p1.1" class="ltx_p">We calculated the frequency of head nods on the basis of length of the recorded sessions and the recognized head nod count. We found a noteworthy difference in head nod frequency between recorded data of European participants and Asian participants (see Table <a href="#S5.T4" title="Table 4 ‣ 5.3.2. Head nods ‣ 5.3. Engagement in intercultural data comparison ‣ 5. Data Analysis ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div id="S5.SS3.SSS2.p2" class="ltx_para">
<p id="S5.SS3.SSS2.p2.1" class="ltx_p">The high frequency of the session with the English-speaking participants was caused by frequent head nodding of the participants with an Asian cultural background. The utilized algorithm could not detect small head nods obscured by static noise in the facial recognition data extracted from Kinect. An investigation of the data verified that many small head nods, especially in the Japanese dataset, were not identified.</p>
</div>
</section>
<section id="S5.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.3. </span>Vocal backchannels</h4>

<div id="S5.SS3.SSS3.p1" class="ltx_para">
<p id="S5.SS3.SSS3.p1.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 ‣ 5.3.2. Head nods ‣ 5.3. Engagement in intercultural data comparison ‣ 5. Data Analysis ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> also shows the calculated ratio of listening and the time spent vocally backchanneling separately. French language interlocutors spent the least amount of time listening while engaging the most in vocal back-channeling, with similar values observed in the English-speaking sessions.
The Japanese are unique in exhibiting a very high listening ratio and a high ratio of vocal back-channeling.</p>
</div>
</section>
<section id="S5.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.4. </span>Mutual engagement and speaking turns</h4>

<figure id="S5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F6.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:217.8pt;"><img src="/html/2409.13726/assets/x4.png" id="S5.F6.1.g1" class="ltx_graphics ltx_img_square" width="461" height="405" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Correlations between expert and novice engagement.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F6.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:206.3pt;"><img src="/html/2409.13726/assets/x5.png" id="S5.F6.2.g1" class="ltx_graphics ltx_img_square" width="461" height="427" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Novice engagement annotation averages overall and by speaking state.</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S5.SS3.SSS4.p1" class="ltx_para">
<p id="S5.SS3.SSS4.p1.4" class="ltx_p">Finally, we observed differences in the correlations between expert and novice engagement. In all European language sessions, the engagement of the novice and the engagement of the expert have a negative correlation coefficient (DE: r=<math id="S5.SS3.SSS4.p1.1.m1.1" class="ltx_Math" alttext="-0.19" display="inline"><semantics id="S5.SS3.SSS4.p1.1.m1.1a"><mrow id="S5.SS3.SSS4.p1.1.m1.1.1" xref="S5.SS3.SSS4.p1.1.m1.1.1.cmml"><mo id="S5.SS3.SSS4.p1.1.m1.1.1a" xref="S5.SS3.SSS4.p1.1.m1.1.1.cmml">−</mo><mn id="S5.SS3.SSS4.p1.1.m1.1.1.2" xref="S5.SS3.SSS4.p1.1.m1.1.1.2.cmml">0.19</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS4.p1.1.m1.1b"><apply id="S5.SS3.SSS4.p1.1.m1.1.1.cmml" xref="S5.SS3.SSS4.p1.1.m1.1.1"><minus id="S5.SS3.SSS4.p1.1.m1.1.1.1.cmml" xref="S5.SS3.SSS4.p1.1.m1.1.1"></minus><cn type="float" id="S5.SS3.SSS4.p1.1.m1.1.1.2.cmml" xref="S5.SS3.SSS4.p1.1.m1.1.1.2">0.19</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS4.p1.1.m1.1c">-0.19</annotation></semantics></math>, FR: r=<math id="S5.SS3.SSS4.p1.2.m2.1" class="ltx_Math" alttext="-0.05" display="inline"><semantics id="S5.SS3.SSS4.p1.2.m2.1a"><mrow id="S5.SS3.SSS4.p1.2.m2.1.1" xref="S5.SS3.SSS4.p1.2.m2.1.1.cmml"><mo id="S5.SS3.SSS4.p1.2.m2.1.1a" xref="S5.SS3.SSS4.p1.2.m2.1.1.cmml">−</mo><mn id="S5.SS3.SSS4.p1.2.m2.1.1.2" xref="S5.SS3.SSS4.p1.2.m2.1.1.2.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS4.p1.2.m2.1b"><apply id="S5.SS3.SSS4.p1.2.m2.1.1.cmml" xref="S5.SS3.SSS4.p1.2.m2.1.1"><minus id="S5.SS3.SSS4.p1.2.m2.1.1.1.cmml" xref="S5.SS3.SSS4.p1.2.m2.1.1"></minus><cn type="float" id="S5.SS3.SSS4.p1.2.m2.1.1.2.cmml" xref="S5.SS3.SSS4.p1.2.m2.1.1.2">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS4.p1.2.m2.1c">-0.05</annotation></semantics></math>, EN: r=<math id="S5.SS3.SSS4.p1.3.m3.1" class="ltx_Math" alttext="-0.07" display="inline"><semantics id="S5.SS3.SSS4.p1.3.m3.1a"><mrow id="S5.SS3.SSS4.p1.3.m3.1.1" xref="S5.SS3.SSS4.p1.3.m3.1.1.cmml"><mo id="S5.SS3.SSS4.p1.3.m3.1.1a" xref="S5.SS3.SSS4.p1.3.m3.1.1.cmml">−</mo><mn id="S5.SS3.SSS4.p1.3.m3.1.1.2" xref="S5.SS3.SSS4.p1.3.m3.1.1.2.cmml">0.07</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS4.p1.3.m3.1b"><apply id="S5.SS3.SSS4.p1.3.m3.1.1.cmml" xref="S5.SS3.SSS4.p1.3.m3.1.1"><minus id="S5.SS3.SSS4.p1.3.m3.1.1.1.cmml" xref="S5.SS3.SSS4.p1.3.m3.1.1"></minus><cn type="float" id="S5.SS3.SSS4.p1.3.m3.1.1.2.cmml" xref="S5.SS3.SSS4.p1.3.m3.1.1.2">0.07</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS4.p1.3.m3.1c">-0.07</annotation></semantics></math>). This suggests a slight tendency for one interlocutor’s engagement to increase as the other’s decreases and vice versa, although the weak correlations indicate that this mutual influence is minimal (see Figure <a href="#S5.F6" title="Figure 6 ‣ 5.3.4. Mutual engagement and speaking turns ‣ 5.3. Engagement in intercultural data comparison ‣ 5. Data Analysis ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). The minor adversarial effect may be explained by the difference in the average engagement of novices between when they hold the speaking turn compared to when they are silent, with a difference of more than 0.15 points on average for the German and English novices (see Figure <a href="#S5.F6" title="Figure 6 ‣ 5.3.4. Mutual engagement and speaking turns ‣ 5.3. Engagement in intercultural data comparison ‣ 5. Data Analysis ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) .
The Japanese session, however, shows a high positive correlation coefficient (r=<math id="S5.SS3.SSS4.p1.4.m4.1" class="ltx_Math" alttext="0.51" display="inline"><semantics id="S5.SS3.SSS4.p1.4.m4.1a"><mn id="S5.SS3.SSS4.p1.4.m4.1.1" xref="S5.SS3.SSS4.p1.4.m4.1.1.cmml">0.51</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS4.p1.4.m4.1b"><cn type="float" id="S5.SS3.SSS4.p1.4.m4.1.1.cmml" xref="S5.SS3.SSS4.p1.4.m4.1.1">0.51</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS4.p1.4.m4.1c">0.51</annotation></semantics></math>) for mutual engagement. The data also show only a slight decline in engagement during silent intervals and no difference when vocally backchanneling in comparison to having the speaking turn.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Engagement Prediction Experiment</h2>

<figure id="S6.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>Results of the initial trained models applied to every dataset. Engagement is abbreviated as <span id="S6.T5.4.1" class="ltx_text ltx_font_italic">E</span>, Mean Squared Error loss as <span id="S6.T5.5.2" class="ltx_text ltx_font_italic">MSE</span>. The marked models were trained on the same language as the test set. LSG describes a <span id="S6.T5.6.3" class="ltx_text ltx_font_italic">Language Speaker Group</span>.</figcaption>
<div id="S6.T5.7" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:90.9pt;vertical-align:-5.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-102.7pt,20.3pt) scale(0.67851667029789,0.67851667029789) ;">
<table id="S6.T5.7.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.7.1.1.1" class="ltx_tr">
<th id="S6.T5.7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T5.7.1.1.1.1.1" class="ltx_text ltx_font_bold">Model training set</span></th>
<th id="S6.T5.7.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T5.7.1.1.1.2.1" class="ltx_text ltx_font_bold">MSE German LSG</span></th>
<th id="S6.T5.7.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T5.7.1.1.1.3.1" class="ltx_text ltx_font_bold">MSE French LSG</span></th>
<th id="S6.T5.7.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T5.7.1.1.1.4.1" class="ltx_text ltx_font_bold">MSE English LSG</span></th>
<th id="S6.T5.7.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T5.7.1.1.1.5.1" class="ltx_text ltx_font_bold">MSE Japanese LSG</span></th>
<th id="S6.T5.7.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T5.7.1.1.1.6.1" class="ltx_text ltx_font_bold">MSE European LSG</span></th>
<th id="S6.T5.7.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T5.7.1.1.1.7.1" class="ltx_text ltx_font_bold">MSE Global</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.7.1.2.1" class="ltx_tr">
<td id="S6.T5.7.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">German LSG</td>
<td id="S6.T5.7.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T5.7.1.2.1.2.1" class="ltx_text ltx_font_bold">0.008</span></td>
<td id="S6.T5.7.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.020</td>
<td id="S6.T5.7.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.024</td>
<td id="S6.T5.7.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.045</td>
<td id="S6.T5.7.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.016</td>
<td id="S6.T5.7.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.023</td>
</tr>
<tr id="S6.T5.7.1.3.2" class="ltx_tr">
<td id="S6.T5.7.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">French LSG</td>
<td id="S6.T5.7.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.009</td>
<td id="S6.T5.7.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T5.7.1.3.2.3.1" class="ltx_text ltx_font_bold">0.014</span></td>
<td id="S6.T5.7.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.021</td>
<td id="S6.T5.7.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">0.027</td>
<td id="S6.T5.7.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r">0.013</td>
<td id="S6.T5.7.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r">0.017</td>
</tr>
<tr id="S6.T5.7.1.4.3" class="ltx_tr">
<td id="S6.T5.7.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">English LSG</td>
<td id="S6.T5.7.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.009</td>
<td id="S6.T5.7.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">0.019</td>
<td id="S6.T5.7.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T5.7.1.4.3.4.1" class="ltx_text ltx_font_bold">0.020</span></td>
<td id="S6.T5.7.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">0.026</td>
<td id="S6.T5.7.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r">0.016</td>
<td id="S6.T5.7.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r">0.018</td>
</tr>
<tr id="S6.T5.7.1.5.4" class="ltx_tr">
<td id="S6.T5.7.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Japanese LSG</td>
<td id="S6.T5.7.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">0.044</td>
<td id="S6.T5.7.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">0.046</td>
<td id="S6.T5.7.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">0.046</td>
<td id="S6.T5.7.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T5.7.1.5.4.5.1" class="ltx_text ltx_font_bold">0.017</span></td>
<td id="S6.T5.7.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r">0.047</td>
<td id="S6.T5.7.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r">0.040</td>
</tr>
<tr id="S6.T5.7.1.6.5" class="ltx_tr">
<td id="S6.T5.7.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">European LSG</td>
<td id="S6.T5.7.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.008</td>
<td id="S6.T5.7.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.015</td>
<td id="S6.T5.7.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.020</td>
<td id="S6.T5.7.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.033</td>
<td id="S6.T5.7.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T5.7.1.6.5.6.1" class="ltx_text ltx_font_bold">0.013</span></td>
<td id="S6.T5.7.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.018</td>
</tr>
<tr id="S6.T5.7.1.7.6" class="ltx_tr">
<td id="S6.T5.7.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Global</td>
<td id="S6.T5.7.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.011</td>
<td id="S6.T5.7.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.019</td>
<td id="S6.T5.7.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.022</td>
<td id="S6.T5.7.1.7.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.014</td>
<td id="S6.T5.7.1.7.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.016</td>
<td id="S6.T5.7.1.7.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S6.T5.7.1.7.6.7.1" class="ltx_text ltx_font_bold">0.016</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S6.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6. </span>Results after transfer learning to every cross reference dataset. Engagement is abbreviated as <span id="S6.T6.4.1" class="ltx_text ltx_font_italic">E</span>, Mean Squared Error loss as <span id="S6.T6.5.2" class="ltx_text ltx_font_italic">MSE</span>. The best results for each test set are marked. LSG describes a <span id="S6.T6.6.3" class="ltx_text ltx_font_italic">Language Speaker Group</span>.</figcaption>
<div id="S6.T6.7" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:84.1pt;vertical-align:-5.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-128.7pt,23.5pt) scale(0.627574709245293,0.627574709245293) ;">
<table id="S6.T6.7.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T6.7.1.1.1" class="ltx_tr">
<th id="S6.T6.7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T6.7.1.1.1.1.1" class="ltx_text ltx_font_bold">Initial model training set</span></th>
<th id="S6.T6.7.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T6.7.1.1.1.2.1" class="ltx_text ltx_font_bold">MSE German LSG</span></th>
<th id="S6.T6.7.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T6.7.1.1.1.3.1" class="ltx_text ltx_font_bold">MSE French LSG</span></th>
<th id="S6.T6.7.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T6.7.1.1.1.4.1" class="ltx_text ltx_font_bold">MSE English LSG</span></th>
<th id="S6.T6.7.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T6.7.1.1.1.5.1" class="ltx_text ltx_font_bold">MSE Japanese LSG</span></th>
<th id="S6.T6.7.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T6.7.1.1.1.6.1" class="ltx_text ltx_font_bold">MSE European LSG</span></th>
<th id="S6.T6.7.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S6.T6.7.1.1.1.7.1" class="ltx_text ltx_font_bold">MSE Global LSG</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T6.7.1.2.1" class="ltx_tr">
<td id="S6.T6.7.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">German LSG</td>
<td id="S6.T6.7.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.7.1.2.1.2.1" class="ltx_text ltx_font_bold">-</span></td>
<td id="S6.T6.7.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.015</td>
<td id="S6.T6.7.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.020</td>
<td id="S6.T6.7.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.015</td>
<td id="S6.T6.7.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.013</td>
<td id="S6.T6.7.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.016</td>
</tr>
<tr id="S6.T6.7.1.3.2" class="ltx_tr">
<td id="S6.T6.7.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">French LSG</td>
<td id="S6.T6.7.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.009</td>
<td id="S6.T6.7.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T6.7.1.3.2.3.1" class="ltx_text ltx_font_bold">-</span></td>
<td id="S6.T6.7.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.020</td>
<td id="S6.T6.7.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">0.016</td>
<td id="S6.T6.7.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r">0.014</td>
<td id="S6.T6.7.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r">0.016</td>
</tr>
<tr id="S6.T6.7.1.4.3" class="ltx_tr">
<td id="S6.T6.7.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">English LSG</td>
<td id="S6.T6.7.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T6.7.1.4.3.2.1" class="ltx_text ltx_font_bold">0.008</span></td>
<td id="S6.T6.7.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T6.7.1.4.3.3.1" class="ltx_text ltx_font_bold">0.014</span></td>
<td id="S6.T6.7.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T6.7.1.4.3.4.1" class="ltx_text ltx_font_bold">-</span></td>
<td id="S6.T6.7.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T6.7.1.4.3.5.1" class="ltx_text ltx_font_bold">0.015</span></td>
<td id="S6.T6.7.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T6.7.1.4.3.6.1" class="ltx_text ltx_font_bold">0.013</span></td>
<td id="S6.T6.7.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T6.7.1.4.3.7.1" class="ltx_text ltx_font_bold">0.016</span></td>
</tr>
<tr id="S6.T6.7.1.5.4" class="ltx_tr">
<td id="S6.T6.7.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Japanese LSG</td>
<td id="S6.T6.7.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">0.011</td>
<td id="S6.T6.7.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">0.016</td>
<td id="S6.T6.7.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">0.021</td>
<td id="S6.T6.7.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T6.7.1.5.4.5.1" class="ltx_text ltx_font_bold">-</span></td>
<td id="S6.T6.7.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r">0.014</td>
<td id="S6.T6.7.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r">0.019</td>
</tr>
<tr id="S6.T6.7.1.6.5" class="ltx_tr">
<td id="S6.T6.7.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">European LSG</td>
<td id="S6.T6.7.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.008</td>
<td id="S6.T6.7.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.015</td>
<td id="S6.T6.7.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.7.1.6.5.4.1" class="ltx_text ltx_font_bold">0.019</span></td>
<td id="S6.T6.7.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.015</td>
<td id="S6.T6.7.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.7.1.6.5.6.1" class="ltx_text ltx_font_bold">-</span></td>
<td id="S6.T6.7.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T6.7.1.6.5.7.1" class="ltx_text ltx_font_bold">0.016</span></td>
</tr>
<tr id="S6.T6.7.1.7.6" class="ltx_tr">
<td id="S6.T6.7.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Global</td>
<td id="S6.T6.7.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.010</td>
<td id="S6.T6.7.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.018</td>
<td id="S6.T6.7.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.022</td>
<td id="S6.T6.7.1.7.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.017</td>
<td id="S6.T6.7.1.7.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.015</td>
<td id="S6.T6.7.1.7.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S6.T6.7.1.7.6.7.1" class="ltx_text ltx_font_bold">-</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we examine the effect of the correlation between engagement and non-verbal behavior (see Figure <a href="#S5.F4" title="Figure 4 ‣ 5.3.1. Engagement correlation with recorded and extracted features ‣ 5.3. Engagement in intercultural data comparison ‣ 5. Data Analysis ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S5.F6" title="Figure 6 ‣ 5.3.4. Mutual engagement and speaking turns ‣ 5.3. Engagement in intercultural data comparison ‣ 5. Data Analysis ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) on the accuracy of machine learning models in predicting engagement across different language speaker groups, hereafter called <span id="S6.p1.1.1" class="ltx_text ltx_font_bold">LSG</span>s.
Initially, we investigate how the differences in non-verbal behavior among different LSGs influence the accuracy of engagement prediction in a cross-corpus scenario (Section <a href="#S6.SS3.SSS1" title="6.3.1. Cross-language corpus experiments ‣ 6.3. Results ‣ 6. Engagement Prediction Experiment ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3.1</span></a>).
Subsequently, we assess the transfer-ability of model knowledge (parameters) related to non-verbal behavior across different LSGs using transfer learning. We discuss the adaptability of the non-verbal behavior-based estimation model across different LSGs by examining the improvements in accuracy facilitated by the transfer learning methodology (Section <a href="#S6.SS3.SSS2" title="6.3.2. Transfer learning results ‣ 6.3. Results ‣ 6. Engagement Prediction Experiment ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3.2</span></a>).</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Finally, we analyze feature importance on the engagement prediction with SHAP values, showing clear consistency between the results of the data analysis and the importance the models assigns to their input features (Section <a href="#S6.SS3.SSS3" title="6.3.3. Feature analysis with SHAP values ‣ 6.3. Results ‣ 6. Engagement Prediction Experiment ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3.3</span></a>).</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Features</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">The prediction models were trained using 49 feature streams: 17 for facial action units, 20 for body properties, 3 for head angle movements, 3 for additional properties, expert engagement, and the computed features head nods, silence, vocal back-channeling, and speaking turn.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">The models were trained with engagement annotations by frame as the target value. To minimize annotator bias, we used the average of three annotators for all sessions except for 37 Japanese sessions. While these sessions are used for training the Japanese based prediction model, we do not use them for transfer learning.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>ML model and training procedure</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">We designed 4-celled LSTM models using a 30-frame window, each containing values for the 49 features to predict the engagement of the following frame. The frame window represents a temporal dimension that can capture changes such as head moments. We trained a regressive model and used the MSE to highlight performance differences between models and cultures. The data were split into training and testing set, with the first four sessions of each LSG serving as the testing set. Cross-culture predictions were also tested on the first four sessions of each specified LSG.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.2" class="ltx_p">The dropout rate was set to 0.3 to prevent overfitting. The Data were randomized before training. For the initial training, the models were each trained for 5 epochs, with a learning rate of <math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="4*10^{-4}" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><mrow id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml"><mn id="S6.SS2.p2.1.m1.1.1.2" xref="S6.SS2.p2.1.m1.1.1.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S6.SS2.p2.1.m1.1.1.1" xref="S6.SS2.p2.1.m1.1.1.1.cmml">∗</mo><msup id="S6.SS2.p2.1.m1.1.1.3" xref="S6.SS2.p2.1.m1.1.1.3.cmml"><mn id="S6.SS2.p2.1.m1.1.1.3.2" xref="S6.SS2.p2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S6.SS2.p2.1.m1.1.1.3.3" xref="S6.SS2.p2.1.m1.1.1.3.3.cmml"><mo id="S6.SS2.p2.1.m1.1.1.3.3a" xref="S6.SS2.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S6.SS2.p2.1.m1.1.1.3.3.2" xref="S6.SS2.p2.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><apply id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1"><times id="S6.SS2.p2.1.m1.1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S6.SS2.p2.1.m1.1.1.2.cmml" xref="S6.SS2.p2.1.m1.1.1.2">4</cn><apply id="S6.SS2.p2.1.m1.1.1.3.cmml" xref="S6.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S6.SS2.p2.1.m1.1.1.3.1.cmml" xref="S6.SS2.p2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S6.SS2.p2.1.m1.1.1.3.2.cmml" xref="S6.SS2.p2.1.m1.1.1.3.2">10</cn><apply id="S6.SS2.p2.1.m1.1.1.3.3.cmml" xref="S6.SS2.p2.1.m1.1.1.3.3"><minus id="S6.SS2.p2.1.m1.1.1.3.3.1.cmml" xref="S6.SS2.p2.1.m1.1.1.3.3"></minus><cn type="integer" id="S6.SS2.p2.1.m1.1.1.3.3.2.cmml" xref="S6.SS2.p2.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">4*10^{-4}</annotation></semantics></math> .
For the transfer learning to another LSG, we fine-tuned the models by adjusting the training length to 1 epoch and reducing the learning rate to <math id="S6.SS2.p2.2.m2.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S6.SS2.p2.2.m2.1a"><msup id="S6.SS2.p2.2.m2.1.1" xref="S6.SS2.p2.2.m2.1.1.cmml"><mn id="S6.SS2.p2.2.m2.1.1.2" xref="S6.SS2.p2.2.m2.1.1.2.cmml">10</mn><mrow id="S6.SS2.p2.2.m2.1.1.3" xref="S6.SS2.p2.2.m2.1.1.3.cmml"><mo id="S6.SS2.p2.2.m2.1.1.3a" xref="S6.SS2.p2.2.m2.1.1.3.cmml">−</mo><mn id="S6.SS2.p2.2.m2.1.1.3.2" xref="S6.SS2.p2.2.m2.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.2.m2.1b"><apply id="S6.SS2.p2.2.m2.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS2.p2.2.m2.1.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1">superscript</csymbol><cn type="integer" id="S6.SS2.p2.2.m2.1.1.2.cmml" xref="S6.SS2.p2.2.m2.1.1.2">10</cn><apply id="S6.SS2.p2.2.m2.1.1.3.cmml" xref="S6.SS2.p2.2.m2.1.1.3"><minus id="S6.SS2.p2.2.m2.1.1.3.1.cmml" xref="S6.SS2.p2.2.m2.1.1.3"></minus><cn type="integer" id="S6.SS2.p2.2.m2.1.1.3.2.cmml" xref="S6.SS2.p2.2.m2.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.2.m2.1c">10^{-4}</annotation></semantics></math>.
We determined hyperparameters such as the learning rate and the number of epochs via grid search.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Results</h3>

<section id="S6.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1. </span>Cross-language corpus experiments</h4>

<div id="S6.SS3.SSS1.p1" class="ltx_para">
<p id="S6.SS3.SSS1.p1.1" class="ltx_p">The models were initially trained only for a single LSG. Most models perform best for the test set of the LSG they were trained on (see Table <a href="#S6.T5" title="Table 5 ‣ 6. Engagement Prediction Experiment ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). The worst-performing model is the English LSG model with a loss of 0.020, which performed only slightly better on the English test set than the French LSG model with a loss of 0.021. This is not surprising, as the English LSG training set is by far the most culturally diverse, with people from over 10 cultural backgrounds participating in the recordings, whereas German and French LSG recordings only have 3 and 4 cultural backgrounds respectively. This cultural diversity most likely also contributed to the English LSG model performing best on the Japanese LSG test set out of all European LSG models with a loss of 0.026. The Japanese LSG model performed poorly on all the other models, with a loss of 0.017 for the Japanese LSG test set, and a loss of over 0.040 for all other test sets.</p>
</div>
<div id="S6.SS3.SSS1.p2" class="ltx_para">
<p id="S6.SS3.SSS1.p2.1" class="ltx_p">We trained a model containing all the languages with 16 test sessions, named <span id="S6.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Global</span> model, to revise the necessity of single LSG models. We also trained a model with training data from German,English and French LSG sessions and 12 test sessions called <span id="S6.SS3.SSS1.p2.1.2" class="ltx_text ltx_font_bold">European LSG</span> model, as the model performance, in addition to the ANOVA results, highlights a clear distinction between the European LSG and Japanese LSG parts of the dataset. While the European LSG model proved very adept in predicting engagement for all the European LSG sessions, even performing equally to the English LSG model on the English LSG test set with a loss of 0.020, it performed worse on the Japanese LSG test set than the French and English LSG models did. The Global model is more accurate than any other model on the Japanese LSG test set with a loss of 0.014, but is less capable of predicting engagement for the German, French and English LSGs than the European LSG model is.</p>
</div>
</section>
<section id="S6.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.2. </span>Transfer learning results</h4>

<div id="S6.SS3.SSS2.p1" class="ltx_para">
<p id="S6.SS3.SSS2.p1.1" class="ltx_p">We then transfer learned each model to all other models and tested them on the respective training set.
The results (see Table <a href="#S6.T6" title="Table 6 ‣ 6. Engagement Prediction Experiment ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) show only minor improvements for inner European LSG model transfer learning. The German LSG model improved the most after the transfer learning on the French LSG training set, from a loss of 0.020 to a loss of 0.015 on the French test set.</p>
</div>
<div id="S6.SS3.SSS2.p2" class="ltx_para">
<p id="S6.SS3.SSS2.p2.1" class="ltx_p">Overall, transfer learning was most successful on the Japanese LSG training set, reducing the loss from 0.040–0.047 to 0.015–0.017.
Transfer learning on the Japanese LSG training set also substantially improved the performance of all other models on the Japanese LSG test set, sometimes outperforming the original Japanese LSG model.
Surprisingly, the Global model’s performance declined after transfer learning, with the loss increasing from 0.014 before transfer learning to 0.017 afterward.</p>
</div>
</section>
<section id="S6.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.3. </span>Feature analysis with SHAP values</h4>

<div id="S6.SS3.SSS3.p1" class="ltx_para">
<p id="S6.SS3.SSS3.p1.1" class="ltx_p">Finally, we aimed to investigate which features influenced the models’ decision-making processes. We used the SHAP (SHapley Additive exPlanations) method<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/shap/shap</span></span></span> to extract the SHAP values, which quantify the weight a model gives to each input feature, and compared them across the models (see Table <a href="#S6.T7" title="Table 7 ‣ 6.3.3. Feature analysis with SHAP values ‣ 6.3. Results ‣ 6. Engagement Prediction Experiment ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<div id="S6.SS3.SSS3.p2" class="ltx_para">
<p id="S6.SS3.SSS3.p2.8" class="ltx_p">First, we noticed that every model used fluidity as its primary factor for engagement prediction. This is surprising, given that fluidity showed a significant lack of correlation with engagement in the initial analysis (DE: r=<math id="S6.SS3.SSS3.p2.1.m1.1" class="ltx_Math" alttext="0.02" display="inline"><semantics id="S6.SS3.SSS3.p2.1.m1.1a"><mn id="S6.SS3.SSS3.p2.1.m1.1.1" xref="S6.SS3.SSS3.p2.1.m1.1.1.cmml">0.02</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p2.1.m1.1b"><cn type="float" id="S6.SS3.SSS3.p2.1.m1.1.1.cmml" xref="S6.SS3.SSS3.p2.1.m1.1.1">0.02</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p2.1.m1.1c">0.02</annotation></semantics></math>, p¡<math id="S6.SS3.SSS3.p2.2.m2.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S6.SS3.SSS3.p2.2.m2.1a"><mn id="S6.SS3.SSS3.p2.2.m2.1.1" xref="S6.SS3.SSS3.p2.2.m2.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p2.2.m2.1b"><cn type="float" id="S6.SS3.SSS3.p2.2.m2.1.1.cmml" xref="S6.SS3.SSS3.p2.2.m2.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p2.2.m2.1c">0.001</annotation></semantics></math>; FR: r=<math id="S6.SS3.SSS3.p2.3.m3.1" class="ltx_Math" alttext="-0.02" display="inline"><semantics id="S6.SS3.SSS3.p2.3.m3.1a"><mrow id="S6.SS3.SSS3.p2.3.m3.1.1" xref="S6.SS3.SSS3.p2.3.m3.1.1.cmml"><mo id="S6.SS3.SSS3.p2.3.m3.1.1a" xref="S6.SS3.SSS3.p2.3.m3.1.1.cmml">−</mo><mn id="S6.SS3.SSS3.p2.3.m3.1.1.2" xref="S6.SS3.SSS3.p2.3.m3.1.1.2.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p2.3.m3.1b"><apply id="S6.SS3.SSS3.p2.3.m3.1.1.cmml" xref="S6.SS3.SSS3.p2.3.m3.1.1"><minus id="S6.SS3.SSS3.p2.3.m3.1.1.1.cmml" xref="S6.SS3.SSS3.p2.3.m3.1.1"></minus><cn type="float" id="S6.SS3.SSS3.p2.3.m3.1.1.2.cmml" xref="S6.SS3.SSS3.p2.3.m3.1.1.2">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p2.3.m3.1c">-0.02</annotation></semantics></math>, p¡<math id="S6.SS3.SSS3.p2.4.m4.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S6.SS3.SSS3.p2.4.m4.1a"><mn id="S6.SS3.SSS3.p2.4.m4.1.1" xref="S6.SS3.SSS3.p2.4.m4.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p2.4.m4.1b"><cn type="float" id="S6.SS3.SSS3.p2.4.m4.1.1.cmml" xref="S6.SS3.SSS3.p2.4.m4.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p2.4.m4.1c">0.001</annotation></semantics></math>; JP: r=<math id="S6.SS3.SSS3.p2.5.m5.1" class="ltx_Math" alttext="0.01" display="inline"><semantics id="S6.SS3.SSS3.p2.5.m5.1a"><mn id="S6.SS3.SSS3.p2.5.m5.1.1" xref="S6.SS3.SSS3.p2.5.m5.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p2.5.m5.1b"><cn type="float" id="S6.SS3.SSS3.p2.5.m5.1.1.cmml" xref="S6.SS3.SSS3.p2.5.m5.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p2.5.m5.1c">0.01</annotation></semantics></math>, p¡<math id="S6.SS3.SSS3.p2.6.m6.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S6.SS3.SSS3.p2.6.m6.1a"><mn id="S6.SS3.SSS3.p2.6.m6.1.1" xref="S6.SS3.SSS3.p2.6.m6.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p2.6.m6.1b"><cn type="float" id="S6.SS3.SSS3.p2.6.m6.1.1.cmml" xref="S6.SS3.SSS3.p2.6.m6.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p2.6.m6.1c">0.001</annotation></semantics></math>; EN: r=<math id="S6.SS3.SSS3.p2.7.m7.1" class="ltx_Math" alttext="0.00" display="inline"><semantics id="S6.SS3.SSS3.p2.7.m7.1a"><mn id="S6.SS3.SSS3.p2.7.m7.1.1" xref="S6.SS3.SSS3.p2.7.m7.1.1.cmml">0.00</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p2.7.m7.1b"><cn type="float" id="S6.SS3.SSS3.p2.7.m7.1.1.cmml" xref="S6.SS3.SSS3.p2.7.m7.1.1">0.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p2.7.m7.1c">0.00</annotation></semantics></math>, p=<math id="S6.SS3.SSS3.p2.8.m8.1" class="ltx_Math" alttext="0.01" display="inline"><semantics id="S6.SS3.SSS3.p2.8.m8.1a"><mn id="S6.SS3.SSS3.p2.8.m8.1.1" xref="S6.SS3.SSS3.p2.8.m8.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.SSS3.p2.8.m8.1b"><cn type="float" id="S6.SS3.SSS3.p2.8.m8.1.1.cmml" xref="S6.SS3.SSS3.p2.8.m8.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.SSS3.p2.8.m8.1c">0.01</annotation></semantics></math>).</p>
</div>
<figure id="S6.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7. </span>Results of the SHAP analysis for all features for which a model showed a weight of 0.01 or higher.</figcaption>
<div id="S6.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:160.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.3pt,10.4pt) scale(0.884611422868442,0.884611422868442) ;">
<table id="S6.T7.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T7.1.1.1.1" class="ltx_tr">
<th id="S6.T7.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<td id="S6.T7.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">German LSG</td>
<td id="S6.T7.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">French LSG</td>
<td id="S6.T7.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">English LSG</td>
<td id="S6.T7.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Japanese LSG</td>
<td id="S6.T7.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Europe LSG</td>
<td id="S6.T7.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Global</td>
</tr>
<tr id="S6.T7.1.1.2.2" class="ltx_tr">
<th id="S6.T7.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Fluidity</th>
<td id="S6.T7.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.2.2.2.1" class="ltx_text ltx_font_bold">0.062</span></td>
<td id="S6.T7.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.2.2.3.1" class="ltx_text ltx_font_bold">0.055</span></td>
<td id="S6.T7.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.2.2.4.1" class="ltx_text ltx_font_bold">0.024</span></td>
<td id="S6.T7.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.2.2.5.1" class="ltx_text ltx_font_bold">0.097</span></td>
<td id="S6.T7.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.2.2.6.1" class="ltx_text ltx_font_bold">0.111</span></td>
<td id="S6.T7.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.2.2.7.1" class="ltx_text ltx_font_bold">0.044</span></td>
</tr>
<tr id="S6.T7.1.1.3.3" class="ltx_tr">
<th id="S6.T7.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Active speaking turn</th>
<td id="S6.T7.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.3.3.2.1" class="ltx_text ltx_font_bold">0.013</span></td>
<td id="S6.T7.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.009</td>
<td id="S6.T7.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.009</td>
<td id="S6.T7.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.003</td>
<td id="S6.T7.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.3.3.6.1" class="ltx_text ltx_font_bold">0.011</span></td>
<td id="S6.T7.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.3.3.7.1" class="ltx_text ltx_font_bold">0.010</span></td>
</tr>
<tr id="S6.T7.1.1.4.4" class="ltx_tr">
<th id="S6.T7.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Silence</th>
<td id="S6.T7.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.4.4.2.1" class="ltx_text ltx_font_bold">0.011</span></td>
<td id="S6.T7.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.009</td>
<td id="S6.T7.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.008</td>
<td id="S6.T7.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.005</td>
<td id="S6.T7.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.4.4.6.1" class="ltx_text ltx_font_bold">0.011</span></td>
<td id="S6.T7.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.008</td>
</tr>
<tr id="S6.T7.1.1.5.5" class="ltx_tr">
<th id="S6.T7.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Spatial extent</th>
<td id="S6.T7.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.009</td>
<td id="S6.T7.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.007</td>
<td id="S6.T7.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.5.5.4.1" class="ltx_text ltx_font_bold">0.010</span></td>
<td id="S6.T7.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.5.5.5.1" class="ltx_text ltx_font_bold">0.015</span></td>
<td id="S6.T7.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.004</td>
<td id="S6.T7.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.5.5.7.1" class="ltx_text ltx_font_bold">0.010</span></td>
</tr>
<tr id="S6.T7.1.1.6.6" class="ltx_tr">
<th id="S6.T7.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Head yaw</th>
<td id="S6.T7.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.009</td>
<td id="S6.T7.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.009</td>
<td id="S6.T7.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.002</td>
<td id="S6.T7.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.6.6.5.1" class="ltx_text ltx_font_bold">0.011</span></td>
<td id="S6.T7.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.008</td>
<td id="S6.T7.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.006</td>
</tr>
<tr id="S6.T7.1.1.7.7" class="ltx_tr">
<th id="S6.T7.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Head roll</th>
<td id="S6.T7.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.008</td>
<td id="S6.T7.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.008</td>
<td id="S6.T7.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.007</td>
<td id="S6.T7.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.7.7.5.1" class="ltx_text ltx_font_bold">0.026</span></td>
<td id="S6.T7.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.007</td>
<td id="S6.T7.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.001</td>
</tr>
<tr id="S6.T7.1.1.8.8" class="ltx_tr">
<th id="S6.T7.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Overall activation</th>
<td id="S6.T7.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.007</td>
<td id="S6.T7.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.007</td>
<td id="S6.T7.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.009</td>
<td id="S6.T7.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.8.8.5.1" class="ltx_text ltx_font_bold">0.026</span></td>
<td id="S6.T7.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.01</td>
<td id="S6.T7.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.002</td>
</tr>
<tr id="S6.T7.1.1.9.9" class="ltx_tr">
<th id="S6.T7.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Energy hands</th>
<td id="S6.T7.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.007</td>
<td id="S6.T7.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.009</td>
<td id="S6.T7.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.006</td>
<td id="S6.T7.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T7.1.1.9.9.5.1" class="ltx_text ltx_font_bold">0.011</span></td>
<td id="S6.T7.1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.006</td>
<td id="S6.T7.1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.002</td>
</tr>
<tr id="S6.T7.1.1.10.10" class="ltx_tr">
<th id="S6.T7.1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Expert Engagement</th>
<td id="S6.T7.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0</td>
<td id="S6.T7.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0</td>
<td id="S6.T7.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.002</td>
<td id="S6.T7.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S6.T7.1.1.10.10.5.1" class="ltx_text ltx_font_bold">0.010</span></td>
<td id="S6.T7.1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.002</td>
<td id="S6.T7.1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.007</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S6.SS3.SSS3.p3" class="ltx_para">
<p id="S6.SS3.SSS3.p3.1" class="ltx_p">Second, the models with the European language speakers rely on <span id="S6.SS3.SSS3.p3.1.1" class="ltx_text ltx_font_italic">Speaking Turn</span> and <span id="S6.SS3.SSS3.p3.1.2" class="ltx_text ltx_font_italic">Silence</span> information (Section <a href="#S5.SS1.SSS3" title="5.1.3. Computed features ‣ 5.1. Features ‣ 5. Data Analysis ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>) as relevant features with weights of between 0.008 and 0.013, whereas the model trained on the Japanese LSG attributes only had a marginal weight of 0.003 and 0.005 for those features.</p>
</div>
<div id="S6.SS3.SSS3.p4" class="ltx_para">
<p id="S6.SS3.SSS3.p4.1" class="ltx_p">The model trained on Japanese speakers considers head movement, energy of the hands, overall activation, and spatial extent as relevant features, which are also influential on all the European models to a lesser degree. The most prominent input feature of the Japanese LSG model is expert engagement, with a weight of 0.010, which the other LSG models consider irrelevant.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Discussion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We first noticed substantial differences between the European and Asian language sessions in the results of
the Tukey-Kramer test. These results are in line with general findings such as Hall <cite class="ltx_cite ltx_citemacro_citep">(Hall, <a href="#bib.bib27" title="" class="ltx_ref">1976</a>)</cite>.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">We found substantial differences in smiling frequency, especially a lower frequency in Chinese participants, similar to observations by Talhelm et al. <cite class="ltx_cite ltx_citemacro_citep">(Talhelm et al<span class="ltx_text">.</span>, <a href="#bib.bib69" title="" class="ltx_ref">2019</a>)</cite> and Lu et al. <cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2011</a>)</cite>. Additionally, there was a slightly greater frequency of smiles in the Japanese recordings that was correlated with engagement, which is not supported by literature, as many researchers deny a high correlation between smiles and the engagement of Japanese people <cite class="ltx_cite ltx_citemacro_citep">(Matsumoto and Kudoh, <a href="#bib.bib50" title="" class="ltx_ref">1993</a>)</cite>.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">The importance of factors such as overall activation, energy of the hands, the fluidity of movement, and the expression of emotion were already recognized by Wallbott <cite class="ltx_cite ltx_citemacro_citep">(Wallbott, <a href="#bib.bib74" title="" class="ltx_ref">1998</a>)</cite> and found to have a substantial impact on engagement prediction. The lack of significant correlation between the fluidity of movement and engagement in the general analysis of the data might be ascribed to the model being able to recognize patterns over its 30 frame window that were missed in a frame-wise comparison.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">The computed head nods were not a relevant factor in engagement prediction. However, head movements in general were a relevant factor for all engagement models and target LSGs, and were most relevant for the Japanese language sessions. This is reflected in the extracted SHAP value attributed to head movement of the Japanese LSG model. While these findings are unambiguous, they do not completely reflect the literature, which suggests a strong disparity in backchanneling behavior and especially head nods in the Japanese data in comparison to the European data as described in Chapter <a href="#S2.SS2" title="2.2. Backchanneling ‣ 2. Scientific Background ‣ Multilingual Dyadic Interaction Corpus NoXi+J: Toward Understanding Asian-European Non-verbal Cultural Characteristics and their Influences on Engagement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p">Turn-taking has been found to have a substantial influence on engagement <cite class="ltx_cite ltx_citemacro_citep">(Hsiao et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2012</a>)</cite>. While we noticed a considerable difference in annotated engagement for the European sessions in the average of engagement for each speaking state, there were far less pronounced in the Japanese conversation annotations. This constitutes a considerable finding for the difference in engagement between cultures.</p>
</div>
<div id="S7.p6" class="ltx_para">
<p id="S7.p6.1" class="ltx_p">Finally, we observed a significant positive correlation between novice engagement and expert engagement for the Japanese recordings which was not present in the original NoXi sessions. This suggests a stronger need for harmony among the Japanese participants, leading them to conform more closely to the mood of their interlocutors. This findings aligns with Hofstede’s theory of cultural dimensions <cite class="ltx_cite ltx_citemacro_citep">(Hofstede, <a href="#bib.bib32" title="" class="ltx_ref">2001</a>)</cite>, which attributes a higher degree of conformity to Japan than to Germany, France, or the UK.</p>
</div>
<div id="S7.p7" class="ltx_para">
<p id="S7.p7.1" class="ltx_p">We have found that the statistical findings of cultural differences in features are mostly reflected in the engagement prediction models, their accuracy and the improvement of model results after transfer learning.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">We introduced <span id="S8.p1.1.1" class="ltx_text ltx_font_bold">Noxi-J</span>, a new addition to the publicly available multi-lingual dyadic interaction corpus NoXi, which consists of a multimodal dataset featuring Japanese and Chinese speakers. Furthermore, we investigated the cultural variations in non-verbal features and their impact on engagement across different language groups and conducted comparative analyses. Finally, we trained an LSTM model for engagement prediction to verify the insights of the data analysis.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">We focused on computed and automatically extracted features. Although the inclusion of manually annotated features might have helped identify further culture-specific variations, the high costs made this impractical for every feature of interest. Additionally, inner-group differences, especially within the dataset of English speaking participants, highlight the potential benefits of segregating the data on the basis of the participants’ home culture.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">The need for engaging and connecting with artificial systems is growing <cite class="ltx_cite ltx_citemacro_citep">(Ge et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2024</a>)</cite>. Research has revealed issues in communication between different cultures caused by non-verbal communication <cite class="ltx_cite ltx_citemacro_citep">(Li, <a href="#bib.bib45" title="" class="ltx_ref">2006</a>)</cite>. Comprehensive data based analyses of cultural differences in non-verbal communication and backchanneling, as conducted here, are essential for the development of culturally sensitive agents and systems.
This paper serves as an introduction, providing a baseline for more optimized engagement prediction models and acting as a reference point for further research into cultural differences in AI agents.</p>
</div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>Acknowledgments</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">The research presented in the paper was conducted in the trilateral PANORAMA project and was partially funded by Deutsche Forschungsgemeinschaft (DFG), Project No. 442607480, and JST AIP Trilateral AI Research, Japan, Project No. JPMJCR20G6.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ambady et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (1995)</span>
<span class="ltx_bibblock">
Nalini Ambady, Mark Hallahan, and Robert Rosenthal. 1995.

</span>
<span class="ltx_bibblock">On judging and being judged accurately in zero-acquaintance situations.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Journal of Personality and Social Psychology</em> 69, 3 (Sept. 1995), 518–529.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/0022-3514.69.3.518" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/0022-3514.69.3.518</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andersen (1998)</span>
<span class="ltx_bibblock">
Peter A Andersen. 1998.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Nonverbal communication</em>.

</span>
<span class="ltx_bibblock">Mayfield Publishing, Maidenhead, England.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">André (2015)</span>
<span class="ltx_bibblock">
Elisabeth André. 2015.

</span>
<span class="ltx_bibblock">Preparing Emotional Agents for Intercultural Communication.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">The Oxford Handbook of Affective Computing</em>. Oxford University Press.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1093/oxfordhb/9780199942237.013.008" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1093/oxfordhb/9780199942237.013.008</a>
arXiv:https://academic.oup.com/book/0/chapter/212019627/chapter-ag-pdf/44596627/book_28057_section_212019627.ag.pdf

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artiran et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Saygin Artiran, Leanne Chukoskie, Ara Jung, Ian Miller, and Pamela Cosman. 2021.

</span>
<span class="ltx_bibblock">HMM-based Detection of Head Nods to Evaluate Conversational Engagement from Head Motion Data. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">2021 29th European Signal Processing Conference (EUSIPCO)</em>. 1301–1305.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.23919/EUSIPCO54536.2021.9615999" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.23919/EUSIPCO54536.2021.9615999</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aubrey et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Andrew J. Aubrey, David Marshall, Paul L. Rosin, Jason Vandeventer, Douglas W. Cunningham, and Christian Wallraven. 2013.

</span>
<span class="ltx_bibblock">Cardiff Conversation Database (CCDb): A Database of Natural Dyadic Conversations. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops</em>. 277–282.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/CVPRW.2013.48" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CVPRW.2013.48</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bain et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman. 2023.

</span>
<span class="ltx_bibblock">WhisperX: Time-Accurate Speech Transcription of Long-Form Audio.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.00747 [cs.SD]

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baur et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Tobias Baur, Gregor Mehlmann, Ionut Damian, Florian Lingenfelser, Johannes Wagner, Birgit Lugrin, Elisabeth André, and Patrick Gebhard. 2015.

</span>
<span class="ltx_bibblock">Context-Aware Automated Analysis and Annotation of Social Human-Agent Interactions.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">ACM Trans. Interact. Intell. Syst.</em> 5, 2 (2015), 11:1–11:33.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/2764921" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2764921</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bohus and Horvitz (2009)</span>
<span class="ltx_bibblock">
Dan Bohus and Eric Horvitz. 2009.

</span>
<span class="ltx_bibblock">Learning to Predict Engagement with a Spoken Dialog System in Open-World Settings. In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the SIGDIAL 2009 Conference</em>, Patrick Healey, Roberto Pieraccini, Donna Byron, Steve Young, and Matthew Purver (Eds.). Association for Computational Linguistics, London, UK, 244–252.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://aclanthology.org/W09-3935" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/W09-3935</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brusco et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Pablo Brusco, Jazmín Vidal, Štefan Beňuš, and Agustín Gravano. 2020.

</span>
<span class="ltx_bibblock">A cross-linguistic analysis of the temporal dynamics of turn-taking cues using machine learning as a descriptive tool.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Speech Communication</em> 125 (2020), 24–40.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.specom.2020.09.004" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.specom.2020.09.004</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cafaro et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Angelo Cafaro, Nadine Glas, and Catherine Pelachaud. 2016.

</span>
<span class="ltx_bibblock">The Effects of Interrupting Behavior on Interpersonal Attitude and Engagement in Dyadic Interactions. In <em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 International Conference on Autonomous Agents &amp; Multiagent Systems</em> (Singapore, Singapore) <em id="bib.bib11.4.2" class="ltx_emph ltx_font_italic">(AAMAS ’16)</em>. International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, 911–920.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cafaro et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Angelo Cafaro, Johannes Wagner, Tobias Baur, Soumia Dermouche, Mercedes Torres Torres, Catherine Pelachaud, Elisabeth André, and Michel Valstar. 2017.

</span>
<span class="ltx_bibblock">The NoXi database: multimodal recordings of mediated novice-expert interactions. In <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th ACM International Conference on Multimodal Interaction</em> (Glasgow, UK) <em id="bib.bib12.4.2" class="ltx_emph ltx_font_italic">(ICMI ’17)</em>. Association for Computing Machinery, New York, NY, USA, 350–359.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3136755.3136780" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3136755.3136780</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Candon et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Kate Candon, Jesse Chen, Yoony Kim, Zoe Hsu, Nathan Tsoi, and Marynel Vázquez. 2023.

</span>
<span class="ltx_bibblock">Nonverbal Human Signals Can Help Autonomous Agents Infer Human Preferences for Their Behavior. In <em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems</em> (London, United Kingdom) <em id="bib.bib13.4.2" class="ltx_emph ltx_font_italic">(AAMAS ’23)</em>. International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, 307–316.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chao and Thomaz (2012)</span>
<span class="ltx_bibblock">
Crystal Chao and Andrea L. Thomaz. 2012.

</span>
<span class="ltx_bibblock">Timing in multimodal turn-taking interactions: control and analysis using timed Petri nets.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">J. Hum.-Robot Interact.</em> 1, 1 (jul 2012), 4–25.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.5898/JHRI.1.1.Chao" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5898/JHRI.1.1.Chao</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cutrone (2005)</span>
<span class="ltx_bibblock">
Pino Cutrone. 2005.

</span>
<span class="ltx_bibblock">A case study examining backchannels in conversations between Japanese–British dyads.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Multilingua</em> 24, 3 (2005), 237–274.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1515/mult.2005.24.3.237" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1515/mult.2005.24.3.237</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dermouche and Pelachaud (2019)</span>
<span class="ltx_bibblock">
Soumia Dermouche and Catherine Pelachaud. 2019.

</span>
<span class="ltx_bibblock">Engagement Modeling in Dyadic Interaction. 440–445.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3340555.3353765" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3340555.3353765</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doherty and Doherty (2018)</span>
<span class="ltx_bibblock">
Kevin Doherty and Gavin Doherty. 2018.

</span>
<span class="ltx_bibblock">Engagement in HCI: Conception, Theory and Measurement.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ACM Comput. Surv.</em> 51, 5, Article 99 (nov 2018), 39 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3234149" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3234149</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ekman (1994)</span>
<span class="ltx_bibblock">
Paul Ekman. 1994.

</span>
<span class="ltx_bibblock">Strong evidence for universals in facial expressions: A reply to Russell’s mistaken critique.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Psychological Bulletin</em> 115, 2 (1994), 268–287.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/0033-2909.115.2.268" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/0033-2909.115.2.268</a>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Endrass et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Birgit Endrass, Elisabeth André, Matthias Rehm, and Yukiko Nakano. 2013.

</span>
<span class="ltx_bibblock">Investigating culture-related aspects of behavior for virtual characters.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Autonomous Agents and Multi-Agent Systems</em> 27, 2 (Jan. 2013), 277–304.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s10458-012-9218-5" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/s10458-012-9218-5</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freiermuth and Hamzah (2023)</span>
<span class="ltx_bibblock">
Mark R. Freiermuth and Nurul Huda Hamzah. 2023.

</span>
<span class="ltx_bibblock">“I agree!” empathetic head-nodding and its role in cultural competences development.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Lingua</em> 296 (2023), 103629.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.lingua.2023.103629" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.lingua.2023.103629</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fries (1952)</span>
<span class="ltx_bibblock">
C.C. Fries. 1952.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">The Structure of English: An Introduction to the Construction of English Sentences</em>.

</span>
<span class="ltx_bibblock">Harcourt, Brace.

</span>
<span class="ltx_bibblock">


<a target="_blank" href="https://books.google.co.jp/books?id=YEw3AAAAIAAJ" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://books.google.co.jp/books?id=YEw3AAAAIAAJ</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Nan Gao, Mohammad Saiedur Rahaman, Wei Shao, and Flora D Salim. 2021.

</span>
<span class="ltx_bibblock">Investigating the Reliability of Self-report Data in the Wild: The Quest for Ground Truth. In <em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers</em> (Virtual, USA) <em id="bib.bib22.4.2" class="ltx_emph ltx_font_italic">(UbiComp/ISWC ’21 Adjunct)</em>. Association for Computing Machinery, New York, NY, USA, 237–242.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3460418.3479338" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3460418.3479338</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiao Ge, Chunchen Xu, Daigo Misaki, Hazel Rose Markus, and Jeanne L Tsai. 2024.

</span>
<span class="ltx_bibblock">How Culture Shapes What People Want From AI.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2403.05104" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2403.05104</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glas and Pelachaud (2015a)</span>
<span class="ltx_bibblock">
Nadine Glas and Catherine Pelachaud. 2015a.

</span>
<span class="ltx_bibblock">Definitions of engagement in human-agent interaction. In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">2015 International Conference on Affective Computing and Intelligent Interaction (ACII)</em>. 944–949.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ACII.2015.7344688" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ACII.2015.7344688</a>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glas and Pelachaud (2015b)</span>
<span class="ltx_bibblock">
Nadine Glas and Catherine Pelachaud. 2015b.

</span>
<span class="ltx_bibblock">Definitions of Engagement in Human-Agent Interaction. 944–949.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ACII.2015.7344688" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ACII.2015.7344688</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldberg (1990)</span>
<span class="ltx_bibblock">
Lewis R. Goldberg. 1990.

</span>
<span class="ltx_bibblock">An alternative “description of personality”: The Big-Five factor structure.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Journal of Personality and Social Psychology</em> 59, 6 (1990), 1216–1229.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/0022-3514.59.6.1216" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/0022-3514.59.6.1216</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hall (1976)</span>
<span class="ltx_bibblock">
E.T. Hall. 1976.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Beyond Culture</em>.

</span>
<span class="ltx_bibblock">Knopf Doubleday Publishing Group.

</span>
<span class="ltx_bibblock">


<a target="_blank" href="https://books.google.co.jp/books?id=sgiNzwEACAAJ" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://books.google.co.jp/books?id=sgiNzwEACAAJ</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hall (1959)</span>
<span class="ltx_bibblock">
Edward Twitchell Hall. 1959.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">The Silent Language</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:143072138" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:143072138</a>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hanzawa (2012)</span>
<span class="ltx_bibblock">
Chiemi Hanzawa. 2012.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Listening behaviors in Japanese: Aizuchi and head nod use by native speakers and second language learners</em>.

</span>
<span class="ltx_bibblock">Ph. D. Dissertation. The University of Iowa.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.17077/etd.p4yv50ow" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.17077/etd.p4yv50ow</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hayter (1984)</span>
<span class="ltx_bibblock">
Anthony J Hayter. 1984.

</span>
<span class="ltx_bibblock">A proof of the conjecture that the Tukey-Kramer multiple comparisons procedure is conservative.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">The Annals of Statistics</em> (1984), 61–75.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He and Huang (2014)</span>
<span class="ltx_bibblock">
Helen Ai He and Elaine M. Huang. 2014.

</span>
<span class="ltx_bibblock">A qualitative study of workplace intercultural communication tensions in dyadic face-to-face and computer-mediated interactions. In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 Conference on Designing Interactive Systems</em> (Vancouver, BC, Canada) <em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic">(DIS ’14)</em>. Association for Computing Machinery, New York, NY, USA, 415–424.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2598510.2598594" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2598510.2598594</a>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hofstede (2001)</span>
<span class="ltx_bibblock">
Geert Hofstede. 2001.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Culture’s consequences</em> (2 ed.).

</span>
<span class="ltx_bibblock">SAGE Publications, Thousand Oaks, CA.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hovy and Prabhumoye (2021)</span>
<span class="ltx_bibblock">
Dirk Hovy and Shrimai Prabhumoye. 2021.

</span>
<span class="ltx_bibblock">Five sources of bias in natural language processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Language and Linguistics Compass</em> 15, 8 (Aug. 2021).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/lnc3.12432" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1111/lnc3.12432</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsiao et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Joey Chiao Yin Hsiao, Wan Rong Jih, and Jane Yung Jen Hsu. 2012.

</span>
<span class="ltx_bibblock">Recognizing Continuous Social Engagement Level in Dyadic Conversation by Using Turn-taking and Speech Emotion Patterns. In <em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">Activity Context Representation</em> <em id="bib.bib34.4.2" class="ltx_emph ltx_font_italic">(AAAI Workshop - Technical Report)</em>. 40–43.

</span>
<span class="ltx_bibblock">

</span>
<span class="ltx_bibblock">2012 AAAI Workshop ; Conference date: 23-07-2012 Through 23-07-2012.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ichinohara (2015)</span>
<span class="ltx_bibblock">
Kazue Ichinohara. 2015.

</span>
<span class="ltx_bibblock">Back-Channeling in Japanese Conversations:Features of Back-Channeling that Create Good Impressions.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Tokyo Woman’s Christian University studies in language and culture: Studies in language and culture</em> 23 (03 2015), 1–15.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://cir.nii.ac.jp/crid/1050845762588661248" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cir.nii.ac.jp/crid/1050845762588661248</a>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iizuka and Otsuka (2023)</span>
<span class="ltx_bibblock">
Kaito Iizuka and Kazuhiro Otsuka. 2023.

</span>
<span class="ltx_bibblock">Analyzing Synergetic Functions of Listener’s Head Movements and Aizuchi in Conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Transactions of the Japanese Society for Artificial Intelligence</em> 38, 3 (2023).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1527/tjsai.38-3_J-M91" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1527/tjsai.38-3_J-M91</a>

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Inoue et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Koji Inoue, Divesh Lala, Katsuya Takanashi, and Tatsuya Kawahara. 2019.

</span>
<span class="ltx_bibblock">Latent Character Model for Engagement Recognition Based on Multimodal Behaviors. In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">9th International Workshop on Spoken Dialogue System Technology</em>, Luis Fernando D’Haro, Rafael E. Banchs, and Haizhou Li (Eds.). Springer Singapore, Singapore, 119–130.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jack et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Rachael E. Jack, Oliver G. B. Garrod, Hui Yu, Roberto Caldara, and Philippe G. Schyns. 2012.

</span>
<span class="ltx_bibblock">Facial expressions of emotion are not culturally universal.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em> 109, 19 (April 2012), 7241–7244.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1073/pnas.1200155109" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1073/pnas.1200155109</a>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Vidit Jain, Maitree Leekha, Rajiv Ratn Shah, and Jainendra Shukla. 2021.

</span>
<span class="ltx_bibblock">Exploring Semi-Supervised Learning for Predicting Listener Backchannels. In <em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em> (, Yokohama, Japan,) <em id="bib.bib39.4.2" class="ltx_emph ltx_font_italic">(CHI ’21)</em>. Association for Computing Machinery, New York, NY, USA, Article 395, 12 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3411764.3445449" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3411764.3445449</a>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kita and Ide (2007)</span>
<span class="ltx_bibblock">
Sotaro Kita and Sachiko Ide. 2007.

</span>
<span class="ltx_bibblock">Nodding, aizuchi, and final particles in Japanese conversation: How conversation reflects the ideology of communication and social relationships.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Journal of Pragmatics</em> 39, 7 (2007), 1242–1254.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.pragma.2007.02.009" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.pragma.2007.02.009</a>

</span>
<span class="ltx_bibblock">Nodding, Aizuchi, and Final Particles in Japanese Conversation.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Knapp et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (1972)</span>
<span class="ltx_bibblock">
M.L. Knapp, J.A. Hall, and T.G. Horgan. 1972.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">Nonverbal Communication in Human Interaction</em>.

</span>
<span class="ltx_bibblock">Cengage Learning.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://books.google.co.jp/books?id=rWoWAAAAQBAJ" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://books.google.co.jp/books?id=rWoWAAAAQBAJ</a>

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kossaifi et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jean Kossaifi, Robert Walecki, Yannis Panagakis, Jie Shen, Maximilian Schmitt, Fabien Ringeval, Jing Han, Vedhas Pandit, Antoine Toisoul, Bjorn Schuller, Kam Star, Elnar Hajiyev, and Maja Pantic. 2021.

</span>
<span class="ltx_bibblock">SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 43, 3 (March 2021), 1022–1040.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/tpami.2019.2944808" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/tpami.2019.2944808</a>

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LaFrance and Mayo (1978)</span>
<span class="ltx_bibblock">
Marianne LaFrance and Clara Mayo. 1978.

</span>
<span class="ltx_bibblock">Cultural aspects of nonverbal communication.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">International Journal of Intercultural Relations</em> 2, 1 (1978), 71–89.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/0147-1767(78)90029-9" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/0147-1767(78)90029-9</a>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levinson and Torreira (2015)</span>
<span class="ltx_bibblock">
Stephen C. Levinson and Francisco Torreira. 2015.

</span>
<span class="ltx_bibblock">Timing in turn-taking and its implications for processing models of language.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Frontiers in Psychology</em> 6 (2015).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3389/fpsyg.2015.00731" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3389/fpsyg.2015.00731</a>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li (2006)</span>
<span class="ltx_bibblock">
Han Z. Li. 2006.

</span>
<span class="ltx_bibblock">Backchannel Responses as Misleading Feedback in Intercultural Discourse.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Journal of Intercultural Communication Research</em> 35, 2 (2006), 99–116.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1080/17475750600909253" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1080/17475750600909253</a>
arXiv:https://doi.org/10.1080/17475750600909253

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LI et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Meixuan LI, Defu ZHANG, Eri SATO-SHIMOKAWARA, and Toru YAMAGUCHI. 2020.

</span>
<span class="ltx_bibblock">Analysis of Perceptions towards Strong Emotions in Intercultural Communication.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference of JSAI</em> JSAI2020 (2020).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.11517/pjsai.JSAI2020.0_3F5ES204" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.11517/pjsai.JSAI2020.0_3F5ES204</a>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiguang Li, Candy Olivia Mawalim, and Shogo Okada. 2023.

</span>
<span class="ltx_bibblock">Inter-person Intra-modality Attention Based Model for Dyadic Interaction Engagement Prediction. In <em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">Social Computing and Social Media</em>, Adela Coman and Simona Vasilache (Eds.). Springer Nature Switzerland, Cham, 91–105.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
Jia Lu, Jens Allwood, and Elisabeth Ahlsen. 2011.

</span>
<span class="ltx_bibblock">A Study on Cultural Variations of Smile Based on Empirical Recordings of Chinese and Swedish First Encounters.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">Workshop on Multimodal Corpora at ICMI-MLMI, Alicante, Spain</em> (2011).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matsumoto et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
David Matsumoto, Mark G Frank, and Hyi Sung Hwang. 2012.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Nonverbal communication: Science and applications</em>.

</span>
<span class="ltx_bibblock">Sage Publications.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matsumoto and Kudoh (1993)</span>
<span class="ltx_bibblock">
David Matsumoto and Tsutomu Kudoh. 1993.

</span>
<span class="ltx_bibblock">American-Japanese cultural differences in attributions of personality based on smiles.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Journal of Nonverbal Behavior</em> 17, 4 (1993), 231–243.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/bf00987239" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/bf00987239</a>

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maynard (1987)</span>
<span class="ltx_bibblock">
Senko K. Maynard. 1987.

</span>
<span class="ltx_bibblock">Interactional functions of a nonverbal sign Head movement in japanese dyadic casual conversation.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Journal of Pragmatics</em> 11, 5 (1987), 589–606.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/0378-2166(87)90181-0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/0378-2166(87)90181-0</a>

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maynard (1990)</span>
<span class="ltx_bibblock">
Senko K. Maynard. 1990.

</span>
<span class="ltx_bibblock">Conversation management in contrast: Listener response in Japanese and American English.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Journal of Pragmatics</em> 14, 3 (1990), 397–412.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/0378-2166(90)90097-W" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/0378-2166(90)90097-W</a>

</span>
<span class="ltx_bibblock">Special Issue: ’Selected papers from The International Pragmatics Conference, Antwerp, 17-22 August, 1987’.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McKeown et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Gary McKeown, Michel F. Valstar, Roderick Cowie, and Maja Pantic. 2010.

</span>
<span class="ltx_bibblock">The SEMAINE corpus of emotionally coloured character interactions. In <em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">2010 IEEE International Conference on Multimedia and Expo</em>. 1079–1084.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICME.2010.5583006" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICME.2010.5583006</a>

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Müller et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Philipp Müller, Michal Balazia, Tobias Baur, Michael Dietz, Alexander Heimerl, Dominik Schiller, Mohammed Guermal, Dominike Thomas, François Brémond, Jan Alexandersson, Elisabeth André, and Andreas Bulling. 2023.

</span>
<span class="ltx_bibblock">MultiMediate ’23: Engagement Estimation and Bodily Behaviour Recognition in Social Interactions. In <em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October 2023- 3 November 2023</em>, Abdulmotaleb El-Saddik, Tao Mei, Rita Cucchiara, Marco Bertini, Diana Patricia Tobon Vallejo, Pradeep K. Atrey, and M. Shamim Hossain (Eds.). ACM, 9640–9645.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3581783.3613851" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3581783.3613851</a>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Müller et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Philipp Müller, Michael Dietz, Dominik Schiller, Dominike Thomas, Hali Lindsay, Patrick Gebhard, Elisabeth André, and Andreas Bulling. 2022.

</span>
<span class="ltx_bibblock">MultiMediate’22: Backchannel Detection and Agreement Estimation in Group Interactions. In <em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM International Conference on Multimedia</em> <em id="bib.bib55.4.2" class="ltx_emph ltx_font_italic">(MM ’22)</em>. ACM.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3503161.3551589" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3503161.3551589</a>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano and Ishii (2010)</span>
<span class="ltx_bibblock">
Yukiko I. Nakano and Ryo Ishii. 2010.

</span>
<span class="ltx_bibblock">Estimating user’s engagement from eye-gaze behaviors in human-agent conversations. In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th International Conference on Intelligent User Interfaces</em> (Hong Kong, China) <em id="bib.bib56.2.2" class="ltx_emph ltx_font_italic">(IUI ’10)</em>. Association for Computing Machinery, New York, NY, USA, 139–148.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/1719970.1719990" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/1719970.1719990</a>

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Newn et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Joshua Newn, Ronal Singh, Fraser Allison, Prashan Madumal, Eduardo Velloso, and Frank Vetere. 2019.

</span>
<span class="ltx_bibblock">Designing Interactions with Intention-Aware Gaze1-Enabled Artificial Agents. In <em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">Human-Computer Interaction – INTERACT 2019</em>, David Lamas, Fernando Loizides, Lennart Nacke, Helen Petrie, Marco Winckler, and Panayiotis Zaphiris (Eds.). Springer International Publishing, Cham, 255–281.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oertel et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Catharine Oertel, Ginevra Castellano, Mohamed Chetouani, Jauwairia Nasir, Mohammad Obaid, Catherine Pelachaud, and Christopher Peters. 2020.

</span>
<span class="ltx_bibblock">Engagement in Human-Agent Interaction: An Overview.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">Frontiers in Robotics and AI</em> 7 (Aug. 2020).

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3389/frobt.2020.00092" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3389/frobt.2020.00092</a>

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ortega et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Daniel Ortega, Sarina Meyer, Antje Schweitzer, and Ngoc Thang Vu. 2023.

</span>
<span class="ltx_bibblock">Modeling Speaker-Listener Interaction for Backchannel Prediction.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2304.04472 [cs.CL]

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">O’Connor and Joffe (2020)</span>
<span class="ltx_bibblock">
Cliodhna O’Connor and Helene Joffe. 2020.

</span>
<span class="ltx_bibblock">Intercoder Reliability in Qualitative Research: Debates and Practical Guidelines.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">International Journal of Qualitative Methods</em> 19 (2020), 1609406919899220.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1177/1609406919899220" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1177/1609406919899220</a>
arXiv:https://doi.org/10.1177/1609406919899220

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poggi (2007)</span>
<span class="ltx_bibblock">
I. Poggi. 2007.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Mind, Hands, Face and Body: A Goal and Belief View of Multimodal Communication</em>.

</span>
<span class="ltx_bibblock">Weidler.

</span>
<span class="ltx_bibblock">


<a target="_blank" href="https://books.google.co.jp/books?id=_xjoOAAACAAJ" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://books.google.co.jp/books?id=_xjoOAAACAAJ</a>

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Liang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin Peng, Jianfeng Gao, and Song-Chun Zhu. 2022.

</span>
<span class="ltx_bibblock">ValueNet: A New Dataset for Human Value Driven Dialogue System.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em> 36, 10 (Jun. 2022), 11183–11191.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1609/aaai.v36i10.21368" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1609/aaai.v36i10.21368</a>

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ringeval et al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Fabien Ringeval, Andreas Sonderegger, Jürgen Sauer, and Denis Lalanne. 2013.

</span>
<span class="ltx_bibblock">Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2013</em>, 1–8.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/FG.2013.6553805" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/FG.2013.6553805</a>

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saucier (1994)</span>
<span class="ltx_bibblock">
Gerard Saucier. 1994.

</span>
<span class="ltx_bibblock">Mini-Markers: A Brief Version of Goldberg’s Unipolar Big-Five Markers.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Journal of Personality Assessment</em> 63, 3 (Dec. 1994), 506–516.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1207/s15327752jpa6303_8" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1207/s15327752jpa6303_8</a>

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schegloff (2000)</span>
<span class="ltx_bibblock">
Emanuel A. Schegloff. 2000.

</span>
<span class="ltx_bibblock">Overlapping talk and the organization of turn-taking for conversation.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Language in Society</em> 29, 1 (2000), 1–63.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1017/S0047404500001019" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1017/S0047404500001019</a>

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sidner et al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (2005)</span>
<span class="ltx_bibblock">
Candace L. Sidner, Christopher Lee, Cory D. Kidd, Neal Lesh, and Charles Rich. 2005.

</span>
<span class="ltx_bibblock">Explorations in engagement for humans and robots.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.3.1" class="ltx_emph ltx_font_italic">Artificial Intelligence</em> 166, 1 (2005), 140–164.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.artint.2005.03.005" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.artint.2005.03.005</a>

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Skantze (2021)</span>
<span class="ltx_bibblock">
Gabriel Skantze. 2021.

</span>
<span class="ltx_bibblock">Turn-taking in Conversational Systems and Human-Robot Interaction: A Review.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em> 67 (2021), 101178.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.csl.2020.101178" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.csl.2020.101178</a>

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">St»hle and Wold (1989)</span>
<span class="ltx_bibblock">
Lars St»hle and Svante Wold. 1989.

</span>
<span class="ltx_bibblock">Analysis of variance (ANOVA).

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Chemometrics and Intelligent Laboratory Systems</em> 6, 4 (1989), 259–272.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/0169-7439(89)80095-4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/0169-7439(89)80095-4</a>

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talhelm et al<span id="bib.bib69.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Thomas Talhelm, Shigehiro Oishi, and Xuemin Zhang. 2019.

</span>
<span class="ltx_bibblock">Who smiles while alone? Rates of smiling lower in China than U.S.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.3.1" class="ltx_emph ltx_font_italic">Emotion</em> 19, 4 (June 2019), 741–745.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1037/emo0000459" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1037/emo0000459</a>

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ten Bosch et al<span id="bib.bib70.2.2.1" class="ltx_text">.</span> (2005)</span>
<span class="ltx_bibblock">
Louis ten Bosch, Nelleke Oostdijk, and Lou Boves. 2005.

</span>
<span class="ltx_bibblock">On temporal aspects of turn taking in conversational dialogues.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.3.1" class="ltx_emph ltx_font_italic">Speech Communication</em> 47, 1 (2005), 80–86.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.specom.2005.05.009" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.specom.2005.05.009</a>

</span>
<span class="ltx_bibblock">In Honour of Louis Pols.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tukey (1949)</span>
<span class="ltx_bibblock">
John W. Tukey. 1949.

</span>
<span class="ltx_bibblock">Comparing Individual Means in the Analysis of Variance.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Biometrics</em> 5, 2 (June 1949), 99.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.2307/3001913" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.2307/3001913</a>

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vinciarelli et al<span id="bib.bib72.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Alessandro Vinciarelli, Maja Pantic, Dirk Heylen, Catherine Pelachaud, Isabella Poggi, Francesca D’Errico, and Marc Schroeder. 2012.

</span>
<span class="ltx_bibblock">Bridging the Gap between Social Animal and Unsocial Machine: A Survey of Social Signal Processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em> 3, 1 (2012), 69–87.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/T-AFFC.2011.27" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/T-AFFC.2011.27</a>

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wagner et al<span id="bib.bib73.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Johannes Wagner, Florian Lingenfelser, Tobias Baur, Ionut Damian, Felix Kistler, and Elisabeth André. 2013.

</span>
<span class="ltx_bibblock">The social signal interpretation (SSI) framework: multimodal signal processing and recognition in real-time. In <em id="bib.bib73.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st ACM International Conference on Multimedia</em> (Barcelona, Spain) <em id="bib.bib73.4.2" class="ltx_emph ltx_font_italic">(MM ’13)</em>. Association for Computing Machinery, New York, NY, USA, 831–834.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2502081.2502223" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2502081.2502223</a>

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wallbott (1998)</span>
<span class="ltx_bibblock">
Harald G. Wallbott. 1998.

</span>
<span class="ltx_bibblock">Bodily expression of emotion.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">European Journal of Social Psychology</em> 28, 6 (1998), 879–896.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1002/(SICI)1099-0992(1998110)28:6&lt;879::AID-EJSP901&gt;3.0.CO;2-W" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1002/(SICI)1099-0992(1998110)28:6&lt;879::AID-EJSP901&gt;3.0.CO;2-W</a>

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib75.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Jinhan Wang, Long Chen, Aparna Khare, Anirudh Raju, Pranav Dheram, Di He, Minhua Wu, Andreas Stolcke, and Venkatesh Ravichandran. 2024.

</span>
<span class="ltx_bibblock">Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2401.14717 [cs.CL]

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">White (1989)</span>
<span class="ltx_bibblock">
Sheida White. 1989.

</span>
<span class="ltx_bibblock">Backchannels across cultures: A study of Americans and Japanese.

</span>
<span class="ltx_bibblock"><em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Language in Society</em> 18, 1 (1989), 59–76.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1017/S0047404500013270" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1017/S0047404500013270</a>

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wiemann and Knapp (2006)</span>
<span class="ltx_bibblock">
John M. Wiemann and Mark L. Knapp. 2006.

</span>
<span class="ltx_bibblock">Turn-taking in Conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Journal of Communication</em> 25, 2 (02 2006), 75–92.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/j.1460-2466.1975.tb00582.x" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1111/j.1460-2466.1975.tb00582.x</a>

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yngve (1970)</span>
<span class="ltx_bibblock">
Victor H. Yngve. 1970.

</span>
<span class="ltx_bibblock">On getting a word in edgewise. In <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">CLS-70</em>. University of Chicago, 567–577.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.13725" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.13726" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.13726">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.13726" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.13728" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 22:20:12 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
