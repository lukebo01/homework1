<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.01087] Towards Accurate Lip-to-Speech Synthesis in-the-Wild</title><meta property="og:description" content="In this paper, we introduce a novel approach to address the task of synthesizing speech from silent videos of any in-the-wild speaker solely based on lip movements. The traditional approach of directly generating speec…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Accurate Lip-to-Speech Synthesis in-the-Wild">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Accurate Lip-to-Speech Synthesis in-the-Wild">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.01087">

<!--Generated on Fri Apr  5 17:45:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Lip-reading,  Lip-to-Speech,  Assistive Technology,  Speech Generation">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Towards Accurate Lip-to-Speech Synthesis in-the-Wild</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sindhu Hegde
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:sindhu@robots.ox.ac.uk">sindhu@robots.ox.ac.uk</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">VGG, University of Oxford</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_country">United Kingdom</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rudrabha Mukhopadhyay
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:radrabha.m@research.iiit.ac.in">radrabha.m@research.iiit.ac.in</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_affiliation_institution">CVIT, IIIT Hyderabad</span><span id="id4.2.id2" class="ltx_text ltx_affiliation_country">India</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">C.V Jawahar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jawahar@iiit.ac.in">jawahar@iiit.ac.in</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">CVIT, IIIT Hyderabad</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_country">India</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vinay Namboodiri
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:vpn22@bath.ac.uk">vpn22@bath.ac.uk</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">University of Bath</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_country">United Kingdom</span>
</span></span></span>
</div>
<div class="ltx_dates">(2018; 2023)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id9.id1" class="ltx_p">In this paper, we introduce a novel approach to address the task of synthesizing speech from silent videos of any in-the-wild speaker solely based on lip movements. The traditional approach of directly generating speech from lip videos faces the challenge of not being able to learn a robust language model from speech alone, resulting in unsatisfactory outcomes. To overcome this issue, we propose incorporating noisy text supervision using a state-of-the-art lip-to-text network that instills language information into our model. The noisy text is generated using a pre-trained lip-to-text model, enabling our approach to work without text annotations during inference. We design a visual text-to-speech network that utilizes the visual stream to generate accurate speech, which is in-sync with the silent input video. We perform extensive experiments and ablation studies, demonstrating our approach’s superiority over the current state-of-the-art methods on various benchmark datasets. Further, we demonstrate an essential practical application of our method in assistive technology by generating speech for an ALS patient who has lost the voice but can make mouth movements. Our demo video, code, and additional details can be found at <a target="_blank" href="http://cvit.iiit.ac.in/research/projects/cvit-projects/ms-l2s-itw" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://cvit.iiit.ac.in/research/projects/cvit-projects/ms-l2s-itw</a>.</p>
</div>
<div class="ltx_keywords">Lip-reading, Lip-to-Speech, Assistive Technology, Speech Generation
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>ACM Multimedia; Oct 28–Nov 05,
2023; Ottawa, Canada</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_price"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_submissionid"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">submissionid: </span>409</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2023</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span id="id10" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 31st ACM International Conference on Multimedia; October 29-November 3, 2023; Ottawa, ON, Canada</span></span></span><span id="id11" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 31st ACM International Conference on Multimedia (MM ’23), October 29-November 3, 2023, Ottawa, ON, Canada</span></span></span><span id="id12" class="ltx_note ltx_note_frontmatter ltx_role_price"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span id="id13" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3581783.3611787</span></span></span><span id="id14" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0108-5/23/10</span></span></span><span id="id15" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Neural networks</span></span></span><span id="id16" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Computer vision tasks</span></span></span>
<figure id="S0.F1" class="ltx_figure ltx_teaserfigure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.01087/assets/images/banner.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="538" height="236" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>We propose a novel approach for multi-speaker lip-to-speech synthesis in the wild. Prior works try to learn a language model directly from raw speech, which only provides weak supervision due to the presence of other acoustic variations such as voice, accents, and prosody. We solve this problem by relying on recent advancements in lip-to-text generation. We condition on the noisy text outputs and lip video to generate natural speech with clearly pronounced words.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S0.F1.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As multi-sensory beings, humans have the ability to experience the world through various senses, adapting to make the most of what we perceive. When one sense is diminished, like hearing, the brain compensates by relying more on other senses, like vision, to maintain independence and navigate the environment. One of the most prevalent ways for deaf individuals to comprehend speech is through lip reading - the process of understanding spoken content purely from silent lip movements. Therefore, lip reading has numerous applications, ranging from searching through old silent films to being used as an assistive technology for people who are unable to speak. For example, people with Amyotrophic Lateral Sclerosis (ALS) often lose their ability to speak due to issues with their vocal cords but can mouth words. Therefore, developing automated lip reading technologies can improve the communication abilities of people with ALS. In the past few years, previous works have shown that it is far easier for deep learning models to perform lip reading than it is for humans <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite>. There has been a growing interest in the research community <cite class="ltx_cite ltx_citemacro_citep">(Afouras et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018a</a>, <a href="#bib.bib4" title="" class="ltx_ref">b</a>; Chung and Zisserman, <a href="#bib.bib9" title="" class="ltx_ref">2016a</a>; Chung et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> to solve the task of transcribing silent lip movements into text. While giant strides are being made to solve lip-to-text generation, only a handful of works go a step further to generate speech from silent lip movements. The widening gap between these two closely related tasks is becoming increasingly evident. In other words, while lip-to-text models are reaching word error rates as low as <math id="S1.p1.1.m1.1" class="ltx_Math" alttext="17\%" display="inline"><semantics id="S1.p1.1.m1.1a"><mrow id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml"><mn id="S1.p1.1.m1.1.1.2" xref="S1.p1.1.m1.1.1.2.cmml">17</mn><mo id="S1.p1.1.m1.1.1.1" xref="S1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><apply id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1"><csymbol cd="latexml" id="S1.p1.1.m1.1.1.1.cmml" xref="S1.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S1.p1.1.m1.1.1.2.cmml" xref="S1.p1.1.m1.1.1.2">17</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">17\%</annotation></semantics></math> WER <cite class="ltx_cite ltx_citemacro_citep">(Serdyuk et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite>, there are no practically usable lip-to-speech systems that generate natural, meaningful speech for in-the-wild identities.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Applications for lip-to-speech synthesis</h5>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p1.1" class="ltx_p">Lip-to-Speech generation is not only more exciting and challenging but also has more far-reaching applications. For instance, having a live video call with a person who has lost their voice is more engaging than reading the text transcription of their silent lip movements. Speech can convey more: it is possible to express emotions through silent lip movements if it is converted to speech rather than text. Speech is also more instantaneous for the listener: it is easier and faster to hear than to read text. The immersive user experience is a significant impetus for developing algorithms that translates lip movements directly to speech. Lip-to-Speech can also be applied in forensic investigations, where silent footage can be analyzed to determine what was being said or generate speech from archival film footage. Our work showcases the practical impact of lip-to-speech generation by generating speech for the silent lip movements of an ALS patient, a feat not previously demonstrated by other models in this field. We also show results on reading mouth movements of people suffering from hearing loss to demonstrate the effectiveness of accurate lip-to-speech synthesis further. More details about the applications can be found in Section <a href="#S4.SS3" title="4.3. Applications in Assistive Technology ‣ 4. Experiments ‣ Towards Accurate Lip-to-Speech Synthesis in-the-Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">The challenges in lip-to-speech synthesis</h5>

<div id="S1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px2.p1.1" class="ltx_p">However, synthesizing speech from silent talking-face videos is far more challenging than lip-to-text generation. Not only does the model first needs to distill the content from the lip movements, but it also needs to generate the final speech by accurately modeling the speaker attributes like voice, accent, and style. Owing to these difficulties, until recently, most of the existing works <cite class="ltx_cite ltx_citemacro_citep">(Ephrat and Peleg, <a href="#bib.bib13" title="" class="ltx_ref">2017</a>; Vougioukas et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2019</a>; Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite> focused on building either single-speaker models or constrained multi-speaker models with limited corpus size as well as limited vocabulary. While more recent efforts like <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite> have extended lip-to-speech to “in-the-wild” environments, most of these models are speaker-specific in nature, i.e., they only work on speakers they are trained on. The speaker-independent models <cite class="ltx_cite ltx_citemacro_citep">(Hegde et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>; Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022</a>; Mira et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2022</a>; Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite> suffer from numerous weaknesses and fail to accurately learn language and speech attributes like voice, prosody, etc. Due to the sheer difficulty of the task, they turn out to be well below expectations for an end-user application.</p>
</div>
</section>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1. </span>Our Contributions</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">First, we would like to clearly state that our goal is to be able to generate speech for a silent lip video of <span id="S1.SS1.p1.1.1" class="ltx_text ltx_font_italic">any</span> speaker in the wild and in <span id="S1.SS1.p1.1.2" class="ltx_text ltx_font_italic">any</span> desired target voice. The fundamental premise of this paper is that current models trying to solve this task struggle to learn language and speech attributes because they try to learn both of these solely from speech supervision. They inadvertently disregard the advancements that have been made in the sibling task of the lip-to-text generation. We argue that learning lip-to-speech without any understanding/supervision of text is very challenging to achieve, which can be observed in all the current models, where they fail to produce clear and intelligible outputs in unconstrained settings. Our key idea is to use the fact that lip-to-text models already have learned the task of extracting content from lips, as they are trained with text supervision. We show that we can achieve accurate, natural, and high-quality speech outputs by using the silent video input and the noisy text transcriptions from a pre-trained lip-to-text model. We train a visual text-to-speech model to synthesize speech that syncs with the input video. Our approach outperforms the existing multi-speaker lip-to-speech models by a significant margin in terms of both qualitative and quantitative evaluations. We highlight our key contributions below.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Lip-to-Speech generation is challenging with only speech supervision, and current models struggle to learn language and speech attributes. To address this, we propose using a pre-trained lip-to-text model’s output to aid in lip-to-speech generation.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Our approach involves training a visual text-to-speech model to synthesize speech that syncs with the input video. Using this approach, we outperform existing multi-speaker lip-to-speech models in qualitative and quantitative evaluations.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Our proposed approach enables accurate and natural speech output for silent lip videos of any speaker in any desired target voice.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Works</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Lip-to-Speech</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Generating speech directly from lip movements has always been a challenge for researchers due to the ill-posed nature of the task. While recognizing content from lip movements itself is challenging due to the one-to-many relation present between visemes and phonemes, lip-to-speech synthesis also needs to model speech-related variations like voice, accents, and prosody. Therefore, it was initially attempted in laboratory setups <cite class="ltx_cite ltx_citemacro_citep">(Cooke et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2006</a>)</cite> using datasets with a minimal vocabulary. The initial works to <cite class="ltx_cite ltx_citemacro_citep">(Ephrat et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2017</a>; Ephrat and Peleg, <a href="#bib.bib13" title="" class="ltx_ref">2017</a>; Vougioukas et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2019</a>; Akbari et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite> used fully convolutional neural networks to learn a mapping between lip movements and speech. These models were also trained on speaker-specific data and did not work for unseen speakers. Moreover, such models were incapable of handling “in-the-wild” settings. To mitigate this issue, a sequence-to-sequence model, Lip2Wav <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite>, was proposed that was trained on large amounts of speaker-specific data taken from in-the-wild YouTube videos. Lip2Wav worked reasonably well on speakers seen during training but did not extend to handling unseen speakers. Several more recent studies <cite class="ltx_cite ltx_citemacro_citep">(Hegde et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>; Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022</a>, <a href="#bib.bib22" title="" class="ltx_ref">2023</a>; Mira et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite> have attempted to develop end-to-end lip-to-speech synthesis models on large datasets containing data from hundreds of speakers. For instance, in <cite class="ltx_cite ltx_citemacro_citep">(Hegde et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>, authors proposed a variational approach that matches the distributions of lip movements and speech segments to project them into a shared space, which allows for handling the high variations of in-the-wild speakers to some extent. Meanwhile, both <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022</a>; Mira et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite> utilized a transformer-based approach to convert lip-to-speech synthesis into a sequence-to-sequence problem, where a sequence of lip movements is translated into a sequence of speech tokens. Additionally, <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite> treated lip-to-speech synthesis as a multi-tasking problem, where the model predicts text transcripts in addition to speech forcing the model to learn better content information. However, all of these models produce sub-par outputs with words that are barely uttered and often unintelligible with unnatural voice and prosody. We argue that these works are limited because they do not use the recent advancements in the sibling task, lip-to-text, which we discuss below.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Lip-to-Text</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Lip reading has become synonymous with lip-to-text, and several advancements have been made to achieve highly accurate text transcriptions of silent lip videos. The very first works <cite class="ltx_cite ltx_citemacro_citep">(Assael et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite> used datasets <cite class="ltx_cite ltx_citemacro_citep">(Cooke et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2006</a>; Harte and Gillen, <a href="#bib.bib16" title="" class="ltx_ref">2015</a>)</cite> collected in a laboratory setup and trained simple RNN-based architectures. One of the initial approaches <cite class="ltx_cite ltx_citemacro_citep">(Chung and Zisserman, <a href="#bib.bib9" title="" class="ltx_ref">2016a</a>)</cite> handling the “in-the-wild” setup posed the problem in a word-level classification setting and continued using RNN-based models. Subsequent approaches <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> have attempted to tackle this task by posing it as a sequence-to-sequence problem. The goal here is to translate a sequence of lip movements into a sequence of characters. Approaches like <cite class="ltx_cite ltx_citemacro_citep">(Afouras et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018a</a>)</cite> compared multiple sequence-to-sequence architectures, including transformers, to achieve favorable word error rates (WER). A more recent work <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite> uses a variant of transformer and specific attention to the mouth region through visual transformer pooling and achieves state-of-the-art results in lip reading. Other works such as AV-HuBERT <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite> have also improved the state-of-the-art for in-the-wild lip reading, paving the way for practical applications. Our work demonstrates the importance of pre-trained lip-to-text models in achieving precise lip-to-speech synthesis in uncontrolled environments.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Text-to-speech</h5>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Our proposed approach uses text as an intermediate output and converts it into speech. Text-to-Speech has long interested the speech community and has historically seen a lot of progress. Modern deep learning architectures <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>; Ping et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2018</a>; Ren et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite> enable several industrial and multimedia applications. They are accurate in terms of uttered content and have natural speaking styles, prosody, and voice. The efforts slowly shifted towards multi-speaker scenario, which is far more challenging due to the sheer variation in prosody, voice, and other speaker-related attributes like pitch and tone. Most of these methods are provided with a separate voice token <cite class="ltx_cite ltx_citemacro_citep">(Jia et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite> containing identity information about the target speaker. The voice tokens are generated from a short speech segment of a target speaker and often only capture the voice information while failing to capture the prosody. Thus, there has also been a push for using additional information like lip movements <cite class="ltx_cite ltx_citemacro_citep">(Hassid et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2022</a>; Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> to help TTS models generate higher-quality outputs. Since lip-to-speech synthesis requires us to generate accurate prosody and speaking style, our approach is inspired by this line of work. We design a visual text-to-speech model that works uses both text and visual features from state-of-the-art lip reading network <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite> to generate natural, accurate speech. Our Visual TTS is integral to generating speech that perfectly syncs with the input silent lip video.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Lip-to-Speech Synthesis in the Wild</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2403.01087/assets/images/arch.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="538" height="223" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Overview of our approach. We first extract the visual features and the text predictions from a pre-trained lip-to-text network. Using a visual text-to-speech (TTS) model, we can generate speech outputs that sync with the silent video input. The visual TTS encodes the visual and textual (in the form of phonemes) inputs and aligns them in time using the scaled dot-product attention. For each query video time-step, we retrieve the phoneme to utter at that time using this attention mechanism. After adding the speaker identity embedding, these are then upsampled and decoded into melspectrograms. The melspectrograms are converted into natural waveforms using a pre-trained vocoder.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We start the discussion by highlighting the key issues and challenges in the existing lip-to-speech works, leading to their poor performance in unconstrained settings. We then present our framework with a detailed description of the modules involved.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Issues and challenges in existing works</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Learning language from speech</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">As discussed previously, all the current lip-to-speech works directly generate speech from the lips. It is known that learning a language model is crucial for reading the lips accurately. However, the current multi-speaker lip-to-speech models are sub-par because they try to learn a language model in the speech modality, which contains a large diversity of speaker identities, styles, accents, and prosody. Thus, we argue that we need some other way of incorporating language knowledge.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>The missing block of lip-to-speech: lip-to-text</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">Lip-to-Speech synthesis models have two tasks: (i) inferring the content from lips and (ii) inferring the style in which that content is spoken. If we have the content that is being spoken, then our task is now reduced to just generating the speech that matches the silent lip video. This is the premise of our paper. But how do we get this text information, especially when we only have a silent lip video as input? We show that we can get this text information from pre-trained lip-to-text models: a class of models closely related to our task at hand but have been mainly ignored in previous works on lip-to-speech synthesis. We design an approach that can build upon the current works <cite class="ltx_cite ltx_citemacro_citep">(Afouras et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018a</a>; Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite> in lip reading, i.e., pre-trained lip-to-text models, and generate far more accurate speech outputs.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3. </span>Achieving accurate lip-sync</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">Now that we have the text, the next step is to generate the “right” kind of speech output. That is, the generated speech must match the input lip sequence. There are many ways to utter the same sentence, but only one will match the input lip video. Thus, it is worth noting that a trivial text-to-speech will not serve our task. Instead, we need a text-to-speech model that is also conditioned on video input.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Our Approach</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.4" class="ltx_p">The task of lip-to-speech synthesis is formulated as follows: given a silent video frame sequence of a person talking <math id="S3.SS2.p1.1.m1.4" class="ltx_Math" alttext="S_{v}=\{I_{1},I_{2},...,I_{p}\}" display="inline"><semantics id="S3.SS2.p1.1.m1.4a"><mrow id="S3.SS2.p1.1.m1.4.4" xref="S3.SS2.p1.1.m1.4.4.cmml"><msub id="S3.SS2.p1.1.m1.4.4.5" xref="S3.SS2.p1.1.m1.4.4.5.cmml"><mi id="S3.SS2.p1.1.m1.4.4.5.2" xref="S3.SS2.p1.1.m1.4.4.5.2.cmml">S</mi><mi id="S3.SS2.p1.1.m1.4.4.5.3" xref="S3.SS2.p1.1.m1.4.4.5.3.cmml">v</mi></msub><mo id="S3.SS2.p1.1.m1.4.4.4" xref="S3.SS2.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S3.SS2.p1.1.m1.4.4.3.3" xref="S3.SS2.p1.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.4.4.3.3.4" xref="S3.SS2.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="S3.SS2.p1.1.m1.2.2.1.1.1" xref="S3.SS2.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.2.2.1.1.1.2" xref="S3.SS2.p1.1.m1.2.2.1.1.1.2.cmml">I</mi><mn id="S3.SS2.p1.1.m1.2.2.1.1.1.3" xref="S3.SS2.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p1.1.m1.4.4.3.3.5" xref="S3.SS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS2.p1.1.m1.3.3.2.2.2" xref="S3.SS2.p1.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS2.p1.1.m1.3.3.2.2.2.2" xref="S3.SS2.p1.1.m1.3.3.2.2.2.2.cmml">I</mi><mn id="S3.SS2.p1.1.m1.3.3.2.2.2.3" xref="S3.SS2.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.p1.1.m1.4.4.3.3.6" xref="S3.SS2.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">…</mi><mo id="S3.SS2.p1.1.m1.4.4.3.3.7" xref="S3.SS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS2.p1.1.m1.4.4.3.3.3" xref="S3.SS2.p1.1.m1.4.4.3.3.3.cmml"><mi id="S3.SS2.p1.1.m1.4.4.3.3.3.2" xref="S3.SS2.p1.1.m1.4.4.3.3.3.2.cmml">I</mi><mi id="S3.SS2.p1.1.m1.4.4.3.3.3.3" xref="S3.SS2.p1.1.m1.4.4.3.3.3.3.cmml">p</mi></msub><mo stretchy="false" id="S3.SS2.p1.1.m1.4.4.3.3.8" xref="S3.SS2.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.4b"><apply id="S3.SS2.p1.1.m1.4.4.cmml" xref="S3.SS2.p1.1.m1.4.4"><eq id="S3.SS2.p1.1.m1.4.4.4.cmml" xref="S3.SS2.p1.1.m1.4.4.4"></eq><apply id="S3.SS2.p1.1.m1.4.4.5.cmml" xref="S3.SS2.p1.1.m1.4.4.5"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.4.4.5.1.cmml" xref="S3.SS2.p1.1.m1.4.4.5">subscript</csymbol><ci id="S3.SS2.p1.1.m1.4.4.5.2.cmml" xref="S3.SS2.p1.1.m1.4.4.5.2">𝑆</ci><ci id="S3.SS2.p1.1.m1.4.4.5.3.cmml" xref="S3.SS2.p1.1.m1.4.4.5.3">𝑣</ci></apply><set id="S3.SS2.p1.1.m1.4.4.3.4.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3"><apply id="S3.SS2.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1.2">𝐼</ci><cn type="integer" id="S3.SS2.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS2.p1.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2.2.2">𝐼</ci><cn type="integer" id="S3.SS2.p1.1.m1.3.3.2.2.2.3.cmml" xref="S3.SS2.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">…</ci><apply id="S3.SS2.p1.1.m1.4.4.3.3.3.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS2.p1.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3.3.2">𝐼</ci><ci id="S3.SS2.p1.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS2.p1.1.m1.4.4.3.3.3.3">𝑝</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.4c">S_{v}=\{I_{1},I_{2},...,I_{p}\}</annotation></semantics></math>, the goal is to generate the corresponding speech melspectrogram sequence, <math id="S3.SS2.p1.2.m2.4" class="ltx_Math" alttext="S_{m}=\{M_{1},M_{2},...,M_{q}\}" display="inline"><semantics id="S3.SS2.p1.2.m2.4a"><mrow id="S3.SS2.p1.2.m2.4.4" xref="S3.SS2.p1.2.m2.4.4.cmml"><msub id="S3.SS2.p1.2.m2.4.4.5" xref="S3.SS2.p1.2.m2.4.4.5.cmml"><mi id="S3.SS2.p1.2.m2.4.4.5.2" xref="S3.SS2.p1.2.m2.4.4.5.2.cmml">S</mi><mi id="S3.SS2.p1.2.m2.4.4.5.3" xref="S3.SS2.p1.2.m2.4.4.5.3.cmml">m</mi></msub><mo id="S3.SS2.p1.2.m2.4.4.4" xref="S3.SS2.p1.2.m2.4.4.4.cmml">=</mo><mrow id="S3.SS2.p1.2.m2.4.4.3.3" xref="S3.SS2.p1.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS2.p1.2.m2.4.4.3.3.4" xref="S3.SS2.p1.2.m2.4.4.3.4.cmml">{</mo><msub id="S3.SS2.p1.2.m2.2.2.1.1.1" xref="S3.SS2.p1.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS2.p1.2.m2.2.2.1.1.1.2" xref="S3.SS2.p1.2.m2.2.2.1.1.1.2.cmml">M</mi><mn id="S3.SS2.p1.2.m2.2.2.1.1.1.3" xref="S3.SS2.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p1.2.m2.4.4.3.3.5" xref="S3.SS2.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.SS2.p1.2.m2.3.3.2.2.2" xref="S3.SS2.p1.2.m2.3.3.2.2.2.cmml"><mi id="S3.SS2.p1.2.m2.3.3.2.2.2.2" xref="S3.SS2.p1.2.m2.3.3.2.2.2.2.cmml">M</mi><mn id="S3.SS2.p1.2.m2.3.3.2.2.2.3" xref="S3.SS2.p1.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.p1.2.m2.4.4.3.3.6" xref="S3.SS2.p1.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">…</mi><mo id="S3.SS2.p1.2.m2.4.4.3.3.7" xref="S3.SS2.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.SS2.p1.2.m2.4.4.3.3.3" xref="S3.SS2.p1.2.m2.4.4.3.3.3.cmml"><mi id="S3.SS2.p1.2.m2.4.4.3.3.3.2" xref="S3.SS2.p1.2.m2.4.4.3.3.3.2.cmml">M</mi><mi id="S3.SS2.p1.2.m2.4.4.3.3.3.3" xref="S3.SS2.p1.2.m2.4.4.3.3.3.3.cmml">q</mi></msub><mo stretchy="false" id="S3.SS2.p1.2.m2.4.4.3.3.8" xref="S3.SS2.p1.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.4b"><apply id="S3.SS2.p1.2.m2.4.4.cmml" xref="S3.SS2.p1.2.m2.4.4"><eq id="S3.SS2.p1.2.m2.4.4.4.cmml" xref="S3.SS2.p1.2.m2.4.4.4"></eq><apply id="S3.SS2.p1.2.m2.4.4.5.cmml" xref="S3.SS2.p1.2.m2.4.4.5"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.4.4.5.1.cmml" xref="S3.SS2.p1.2.m2.4.4.5">subscript</csymbol><ci id="S3.SS2.p1.2.m2.4.4.5.2.cmml" xref="S3.SS2.p1.2.m2.4.4.5.2">𝑆</ci><ci id="S3.SS2.p1.2.m2.4.4.5.3.cmml" xref="S3.SS2.p1.2.m2.4.4.5.3">𝑚</ci></apply><set id="S3.SS2.p1.2.m2.4.4.3.4.cmml" xref="S3.SS2.p1.2.m2.4.4.3.3"><apply id="S3.SS2.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS2.p1.2.m2.2.2.1.1.1.2">𝑀</ci><cn type="integer" id="S3.SS2.p1.2.m2.2.2.1.1.1.3.cmml" xref="S3.SS2.p1.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS2.p1.2.m2.3.3.2.2.2.cmml" xref="S3.SS2.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.3.3.2.2.2.1.cmml" xref="S3.SS2.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.3.3.2.2.2.2.cmml" xref="S3.SS2.p1.2.m2.3.3.2.2.2.2">𝑀</ci><cn type="integer" id="S3.SS2.p1.2.m2.3.3.2.2.2.3.cmml" xref="S3.SS2.p1.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">…</ci><apply id="S3.SS2.p1.2.m2.4.4.3.3.3.cmml" xref="S3.SS2.p1.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.4.4.3.3.3.1.cmml" xref="S3.SS2.p1.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S3.SS2.p1.2.m2.4.4.3.3.3.2.cmml" xref="S3.SS2.p1.2.m2.4.4.3.3.3.2">𝑀</ci><ci id="S3.SS2.p1.2.m2.4.4.3.3.3.3.cmml" xref="S3.SS2.p1.2.m2.4.4.3.3.3.3">𝑞</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.4c">S_{m}=\{M_{1},M_{2},...,M_{q}\}</annotation></semantics></math>. An overview of our proposed two-stage framework is depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3. Lip-to-Speech Synthesis in the Wild ‣ Towards Accurate Lip-to-Speech Synthesis in-the-Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We use a state-of-the-art lip-to-text model <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite> to obtain the lip features and noisy text transcriptions for the given silent lip video <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="S_{v}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">S</mi><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝑆</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">S_{v}</annotation></semantics></math>. We design a visual text-to-speech model conditioned on (i) the noisy text and (ii) the lip features to produce high-quality speech outputs <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="S_{m}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msub id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">S</mi><mi id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">𝑆</ci><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">S_{m}</annotation></semantics></math> that are in sync with the input silent lip video. This solves the task of lip-to-speech synthesis.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Pre-trained Lip-to-Text</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Lip-to-Text networks ingest the silent talking face videos as input and transcribe what is being uttered. The lip-to-text models <cite class="ltx_cite ltx_citemacro_citep">(Afouras et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2018c</a>; Chung and Zisserman, <a href="#bib.bib9" title="" class="ltx_ref">2016a</a>; Chung et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>; Afouras et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018b</a>, <a href="#bib.bib3" title="" class="ltx_ref">a</a>; Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite> are now capable of generating text transcripts with word error rates as low as <math id="S3.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="20-30\%" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mrow id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml">20</mn><mo id="S3.SS2.SSS1.p1.1.m1.1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml">−</mo><mrow id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml"><mn id="S3.SS2.SSS1.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml">30</mn><mo id="S3.SS2.SSS1.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><minus id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1"></minus><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2">20</cn><apply id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S3.SS2.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.2">30</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">20-30\%</annotation></semantics></math>. One such recent work trained on public datasets is VTP <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>. Specifically, this work stands out due to its two unique aspects: (i) it has been shown to be data efficient, (ii) it contains a strong visual backbone that can attend to and extract accurate lip features. These visual features have been shown to work well in several other tasks such as visual keyword spotting <cite class="ltx_cite ltx_citemacro_citep">(R et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite> and even spotting mouth movements in sign language <cite class="ltx_cite ltx_citemacro_citep">(Momeni et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>. The sub-word units considered to learn the text representations help to model the ambiguities of the task significantly better than using characters. Also, the sub-word tokens are semantically meaningful and provide a language prior, thereby resulting in a significant performance boost. In addition, the visual representations learned using the backbone network track and aggregate the spatio-temporal lip movement features, mainly due to the strong attention-based pooling mechanism. We thus propose to adopt this model as our first module to generate text and visual features from silent lip videos.</p>
</div>
<section id="S3.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Overview of VTP <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>:</h5>

<div id="S3.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p1.9" class="ltx_p">The model inputs a sequence of <math id="S3.SS2.SSS1.Px1.p1.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.1.m1.1a"><mn id="S3.SS2.SSS1.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.p1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.1.m1.1b"><cn type="integer" id="S3.SS2.SSS1.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.1.m1.1c">5</annotation></semantics></math> consecutive frames, <math id="S3.SS2.SSS1.Px1.p1.2.m2.1" class="ltx_Math" alttext="S_{v}\in\mathbb{R}^{T\times H\times W\times 3}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.2.m2.1a"><mrow id="S3.SS2.SSS1.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.cmml"><msub id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.2" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.2.cmml">S</mi><mi id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.3" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.3.cmml">v</mi></msub><mo id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.cmml"><mi id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.2" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.1" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.3" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.1a" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.4" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.4.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.1b" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.5" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.5.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1"><in id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.1"></in><apply id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.2">𝑆</ci><ci id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.3">𝑣</ci></apply><apply id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3"><times id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.1"></times><ci id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.2">𝑇</ci><ci id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.3">𝐻</ci><ci id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.4.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.4">𝑊</ci><cn type="integer" id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.5.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.3.5">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.2.m2.1c">S_{v}\in\mathbb{R}^{T\times H\times W\times 3}</annotation></semantics></math> (<math id="S3.SS2.SSS1.Px1.p1.3.m3.1" class="ltx_Math" alttext="T=5" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.3.m3.1a"><mrow id="S3.SS2.SSS1.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.2" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.2.cmml">T</mi><mo id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.1" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.3" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.3.m3.1b"><apply id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1"><eq id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.1"></eq><ci id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.2">𝑇</ci><cn type="integer" id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.3.m3.1c">T=5</annotation></semantics></math>) and extracts low-level features using a spatio-temporal residual CNN block. These individual frame-wise features are then processed using the visual transformer pooling block, which consists of a series of transformer layers. The output from this block, <math id="S3.SS2.SSS1.Px1.p1.4.m4.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.4.m4.1a"><msub id="S3.SS2.SSS1.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS1.Px1.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.4.m4.1.1.2" xref="S3.SS2.SSS1.Px1.p1.4.m4.1.1.2.cmml">z</mi><mi id="S3.SS2.SSS1.Px1.p1.4.m4.1.1.3" xref="S3.SS2.SSS1.Px1.p1.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.4.m4.1b"><apply id="S3.SS2.SSS1.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.4.m4.1.1.2">𝑧</ci><ci id="S3.SS2.SSS1.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.4.m4.1c">z_{t}</annotation></semantics></math> is a self-attended feature map, which is used along with a learnable query vector, <math id="S3.SS2.SSS1.Px1.p1.5.m5.1" class="ltx_Math" alttext="Q_{att}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.5.m5.1a"><msub id="S3.SS2.SSS1.Px1.p1.5.m5.1.1" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.2" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.2.cmml">Q</mi><mrow id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.cmml"><mi id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.2" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.1" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.3" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.1a" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.4" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.4.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.5.m5.1b"><apply id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.2">𝑄</ci><apply id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3"><times id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.1"></times><ci id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.2.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.2">𝑎</ci><ci id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.3.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.3">𝑡</ci><ci id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.4.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.3.4">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.5.m5.1c">Q_{att}</annotation></semantics></math> to obtain the spatially weighted average of <math id="S3.SS2.SSS1.Px1.p1.6.m6.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.6.m6.1a"><msub id="S3.SS2.SSS1.Px1.p1.6.m6.1.1" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.6.m6.1.1.2" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1.2.cmml">z</mi><mi id="S3.SS2.SSS1.Px1.p1.6.m6.1.1.3" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.6.m6.1b"><apply id="S3.SS2.SSS1.Px1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.Px1.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1.2">𝑧</ci><ci id="S3.SS2.SSS1.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.6.m6.1c">z_{t}</annotation></semantics></math>. These compact per-frame visual representations are stacked along the temporal dimension to obtain a temporal embedding sequence, <math id="S3.SS2.SSS1.Px1.p1.7.m7.1" class="ltx_Math" alttext="g\in\mathbb{R}^{T\times fd}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.7.m7.1a"><mrow id="S3.SS2.SSS1.Px1.p1.7.m7.1.1" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2.cmml">g</mi><mo id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.1" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.cmml"><mi id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.2" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.cmml"><mrow id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.cmml"><mi id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.2" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.1" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.1.cmml">×</mo><mi id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.3" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.3.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.1" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.3" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.7.m7.1b"><apply id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1"><in id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.1"></in><ci id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.2">𝑔</ci><apply id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.1.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.2.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.2">ℝ</ci><apply id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3"><times id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.1.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.1"></times><apply id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2"><times id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.1.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.1"></times><ci id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.2.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.2">𝑇</ci><ci id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.3.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.2.3">𝑓</ci></apply><ci id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.3.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.7.m7.1c">g\in\mathbb{R}^{T\times fd}</annotation></semantics></math>, where <math id="S3.SS2.SSS1.Px1.p1.8.m8.1" class="ltx_Math" alttext="fd" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.8.m8.1a"><mrow id="S3.SS2.SSS1.Px1.p1.8.m8.1.1" xref="S3.SS2.SSS1.Px1.p1.8.m8.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.8.m8.1.1.2" xref="S3.SS2.SSS1.Px1.p1.8.m8.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p1.8.m8.1.1.1" xref="S3.SS2.SSS1.Px1.p1.8.m8.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p1.8.m8.1.1.3" xref="S3.SS2.SSS1.Px1.p1.8.m8.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.8.m8.1b"><apply id="S3.SS2.SSS1.Px1.p1.8.m8.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.8.m8.1.1"><times id="S3.SS2.SSS1.Px1.p1.8.m8.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.8.m8.1.1.1"></times><ci id="S3.SS2.SSS1.Px1.p1.8.m8.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.8.m8.1.1.2">𝑓</ci><ci id="S3.SS2.SSS1.Px1.p1.8.m8.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.8.m8.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.8.m8.1c">fd</annotation></semantics></math> is the transformer feature dimension. Finally, the learned visual representations are passed to the transformer encoder-decoder network to predict the text outputs in the form of sub-word tokens in an auto-regressive fashion. The model uses beam search decoding <cite class="ltx_cite ltx_citemacro_citep">(Freitag and Al-Onaizan, <a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite> and language model rescoring <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2016</a>)</cite> strategies to obtain the final sentence outputs <math id="S3.SS2.SSS1.Px1.p1.9.m9.1" class="ltx_Math" alttext="S_{t}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.9.m9.1a"><msub id="S3.SS2.SSS1.Px1.p1.9.m9.1.1" xref="S3.SS2.SSS1.Px1.p1.9.m9.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.9.m9.1.1.2" xref="S3.SS2.SSS1.Px1.p1.9.m9.1.1.2.cmml">S</mi><mi id="S3.SS2.SSS1.Px1.p1.9.m9.1.1.3" xref="S3.SS2.SSS1.Px1.p1.9.m9.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.9.m9.1b"><apply id="S3.SS2.SSS1.Px1.p1.9.m9.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.9.m9.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.Px1.p1.9.m9.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m9.1.1.2">𝑆</ci><ci id="S3.SS2.SSS1.Px1.p1.9.m9.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m9.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.9.m9.1c">S_{t}</annotation></semantics></math>. We refer the reader to <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite> for more details on the architecture and training strategies.</p>
</div>
</section>
<section id="S3.SS2.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Adopting VTP for our task:</h5>

<div id="S3.SS2.SSS1.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS1.Px2.p1.2" class="ltx_p">For our task at hand, we use the pre-trained VTP network to obtain: (i) text predictions - the final decoded output of the model <math id="S3.SS2.SSS1.Px2.p1.1.m1.1" class="ltx_Math" alttext="S_{t}" display="inline"><semantics id="S3.SS2.SSS1.Px2.p1.1.m1.1a"><msub id="S3.SS2.SSS1.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS1.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.Px2.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.Px2.p1.1.m1.1.1.2.cmml">S</mi><mi id="S3.SS2.SSS1.Px2.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.Px2.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px2.p1.1.m1.1b"><apply id="S3.SS2.SSS1.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.Px2.p1.1.m1.1.1.2">𝑆</ci><ci id="S3.SS2.SSS1.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.Px2.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px2.p1.1.m1.1c">S_{t}</annotation></semantics></math>, and (ii) per-frame visual representations - <math id="S3.SS2.SSS1.Px2.p1.2.m2.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS2.SSS1.Px2.p1.2.m2.1a"><mi id="S3.SS2.SSS1.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS1.Px2.p1.2.m2.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px2.p1.2.m2.1b"><ci id="S3.SS2.SSS1.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.Px2.p1.2.m2.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px2.p1.2.m2.1c">g</annotation></semantics></math>. The text predictions directly act as input to our speech generation module. The visual representations serve as a condition for speech generation, which is crucial to obtain speech that is in-sync with the silent input video.</p>
</div>
</section>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>Visual Text-to-Speech</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.2" class="ltx_p">Once we have the accurate text predictions, the next step is to generate the corresponding speech sequence <math id="S3.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="S_{m}" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><msub id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml">S</mi><mi id="S3.SS2.SSS2.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><apply id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.2">𝑆</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">S_{m}</annotation></semantics></math>, which is in-sync with the input video clip <math id="S3.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="S_{v}" display="inline"><semantics id="S3.SS2.SSS2.p1.2.m2.1a"><msub id="S3.SS2.SSS2.p1.2.m2.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p1.2.m2.1.1.2" xref="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml">S</mi><mi id="S3.SS2.SSS2.p1.2.m2.1.1.3" xref="S3.SS2.SSS2.p1.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.1b"><apply id="S3.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.2">𝑆</ci><ci id="S3.SS2.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.1c">S_{v}</annotation></semantics></math>. As explained in Section <a href="#S3.SS1.SSS3" title="3.1.3. Achieving accurate lip-sync ‣ 3.1. Issues and challenges in existing works ‣ 3. Lip-to-Speech Synthesis in the Wild ‣ Towards Accurate Lip-to-Speech Synthesis in-the-Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.3</span></a>, directly using state-of-the-art TTS models to synthesize speech from text inputs will generate out-of-sync speech that does not match the input video. We thus design a TTS network by conditioning the model on the input video features. Our visual TTS network majorly comprises five components: (i) Text Encoder, (ii) Visual Encoder, (iii) Visual-Text Attention, (iv) Speaker Embedding, and (v) Spectrogram Decoder. We delve into each of these components below.</p>
</div>
<section id="S3.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Text Encoder:</h5>

<div id="S3.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px1.p1.3" class="ltx_p">As followed in most of the TTS networks <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite>, we extract phoneme representations from text input, which is then given as input to the Transformer encoder layers. Our text encoder block is similar to the one used in FastSpeech2 <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite>, which consists of a positional encoding layer and Feed-Forward Transformer (FFT) layers. The phonemes are transformed to encode the semantic representation and output the text embedding vectors, <math id="S3.SS2.SSS2.Px1.p1.1.m1.1" class="ltx_Math" alttext="E_{text}" display="inline"><semantics id="S3.SS2.SSS2.Px1.p1.1.m1.1a"><msub id="S3.SS2.SSS2.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.2.cmml">E</mi><mrow id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.1a" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.4" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.1b" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.5" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px1.p1.1.m1.1b"><apply id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.2">𝐸</ci><apply id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3"><times id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.2">𝑡</ci><ci id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.4.cmml" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.4">𝑥</ci><ci id="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.5.cmml" xref="S3.SS2.SSS2.Px1.p1.1.m1.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px1.p1.1.m1.1c">E_{text}</annotation></semantics></math> of dimension: <math id="S3.SS2.SSS2.Px1.p1.2.m2.1" class="ltx_Math" alttext="N\times d" display="inline"><semantics id="S3.SS2.SSS2.Px1.p1.2.m2.1a"><mrow id="S3.SS2.SSS2.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS2.Px1.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS2.Px1.p1.2.m2.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS2.Px1.p1.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS2.Px1.p1.2.m2.1.1.3" xref="S3.SS2.SSS2.Px1.p1.2.m2.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS2.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.Px1.p1.2.m2.1.1"><times id="S3.SS2.SSS2.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.Px1.p1.2.m2.1.1.1"></times><ci id="S3.SS2.SSS2.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.Px1.p1.2.m2.1.1.2">𝑁</ci><ci id="S3.SS2.SSS2.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.Px1.p1.2.m2.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px1.p1.2.m2.1c">N\times d</annotation></semantics></math>, where <math id="S3.SS2.SSS2.Px1.p1.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS2.SSS2.Px1.p1.3.m3.1a"><mi id="S3.SS2.SSS2.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS2.Px1.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px1.p1.3.m3.1b"><ci id="S3.SS2.SSS2.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.Px1.p1.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px1.p1.3.m3.1c">d</annotation></semantics></math> is the transformer feature dimension.</p>
</div>
</section>
<section id="S3.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Visual Encoder:</h5>

<div id="S3.SS2.SSS2.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px2.p1.4" class="ltx_p">The input to our visual encoder is the visual feature sequence <math id="S3.SS2.SSS2.Px2.p1.1.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS2.SSS2.Px2.p1.1.m1.1a"><mi id="S3.SS2.SSS2.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS2.Px2.p1.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px2.p1.1.m1.1b"><ci id="S3.SS2.SSS2.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.Px2.p1.1.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px2.p1.1.m1.1c">g</annotation></semantics></math> obtained from the lip-to-text network. We highlight that these visual features, which capture the lip shape and motion, play a crucial role in generating speech that syncs with the input video. Since these representations were also learned using text supervision, they are likely to reflect accurate content information. This starkly contrasts previous works that directly learn visual representations from speech supervision only, which can lead to sub-par visual representations that might contain other unnecessary information, such as the input face identity. The superiority of these representations is one of the critical reasons for our overall network’s performance. The extracted <math id="S3.SS2.SSS2.Px2.p1.2.m2.1" class="ltx_Math" alttext="N\times T\times fd" display="inline"><semantics id="S3.SS2.SSS2.Px2.p1.2.m2.1a"><mrow id="S3.SS2.SSS2.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.cmml"><mrow id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.2" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.1" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.1.cmml">×</mo><mi id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.3" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.3.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.1a" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.1.cmml">×</mo><mi id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.4" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.4.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.1" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.3" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px2.p1.2.m2.1b"><apply id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1"><times id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.1"></times><apply id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2"><times id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.1"></times><ci id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.2">𝑁</ci><ci id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.3">𝑇</ci><ci id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.4.cmml" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.2.4">𝑓</ci></apply><ci id="S3.SS2.SSS2.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.Px2.p1.2.m2.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px2.p1.2.m2.1c">N\times T\times fd</annotation></semantics></math> dimensional representations are given as input to the Transformer encoder layers as shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3. Lip-to-Speech Synthesis in the Wild ‣ Towards Accurate Lip-to-Speech Synthesis in-the-Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Similar to the text encoder network, the visual encoder consists of a positional encoding followed by a series of FFT blocks. The encoder network outputs the learned visual embeddings, <math id="S3.SS2.SSS2.Px2.p1.3.m3.1" class="ltx_Math" alttext="E_{vis}" display="inline"><semantics id="S3.SS2.SSS2.Px2.p1.3.m3.1a"><msub id="S3.SS2.SSS2.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.2" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.2.cmml">E</mi><mrow id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.1" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.1a" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.4" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px2.p1.3.m3.1b"><apply id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.2">𝐸</ci><apply id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3"><times id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.1"></times><ci id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.2">𝑣</ci><ci id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.3">𝑖</ci><ci id="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.4.cmml" xref="S3.SS2.SSS2.Px2.p1.3.m3.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px2.p1.3.m3.1c">E_{vis}</annotation></semantics></math> of dimension: <math id="S3.SS2.SSS2.Px2.p1.4.m4.1" class="ltx_Math" alttext="N\times T\times d" display="inline"><semantics id="S3.SS2.SSS2.Px2.p1.4.m4.1a"><mrow id="S3.SS2.SSS2.Px2.p1.4.m4.1.1" xref="S3.SS2.SSS2.Px2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS2.Px2.p1.4.m4.1.1.2" xref="S3.SS2.SSS2.Px2.p1.4.m4.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.Px2.p1.4.m4.1.1.1" xref="S3.SS2.SSS2.Px2.p1.4.m4.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS2.Px2.p1.4.m4.1.1.3" xref="S3.SS2.SSS2.Px2.p1.4.m4.1.1.3.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS2.Px2.p1.4.m4.1.1.1a" xref="S3.SS2.SSS2.Px2.p1.4.m4.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS2.Px2.p1.4.m4.1.1.4" xref="S3.SS2.SSS2.Px2.p1.4.m4.1.1.4.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px2.p1.4.m4.1b"><apply id="S3.SS2.SSS2.Px2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS2.Px2.p1.4.m4.1.1"><times id="S3.SS2.SSS2.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS2.Px2.p1.4.m4.1.1.1"></times><ci id="S3.SS2.SSS2.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS2.Px2.p1.4.m4.1.1.2">𝑁</ci><ci id="S3.SS2.SSS2.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS2.Px2.p1.4.m4.1.1.3">𝑇</ci><ci id="S3.SS2.SSS2.Px2.p1.4.m4.1.1.4.cmml" xref="S3.SS2.SSS2.Px2.p1.4.m4.1.1.4">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px2.p1.4.m4.1c">N\times T\times d</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS2.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Visual-Text Attention:</h5>

<div id="S3.SS2.SSS2.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px3.p1.2" class="ltx_p">Once we obtain the text and the visual embeddings, the next and the most important step is to find the alignment between these embeddings in time: which phoneme must be uttered when? The generated speech must take the content from the text embeddings and simultaneously, it should also temporally align (sync) it with the video frames. In order to achieve this, we employ a scaled-dot product attention <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2017</a>)</cite> mechanism to learn the correspondence between text and video frames. Specifically, the visual embeddings <math id="S3.SS2.SSS2.Px3.p1.1.m1.1" class="ltx_Math" alttext="E_{vis}" display="inline"><semantics id="S3.SS2.SSS2.Px3.p1.1.m1.1a"><msub id="S3.SS2.SSS2.Px3.p1.1.m1.1.1" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.2.cmml">E</mi><mrow id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.1a" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.4" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px3.p1.1.m1.1b"><apply id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.2">𝐸</ci><apply id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3"><times id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.2">𝑣</ci><ci id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.3">𝑖</ci><ci id="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.4.cmml" xref="S3.SS2.SSS2.Px3.p1.1.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px3.p1.1.m1.1c">E_{vis}</annotation></semantics></math> act as query, and the text embeddings <math id="S3.SS2.SSS2.Px3.p1.2.m2.1" class="ltx_Math" alttext="E_{text}" display="inline"><semantics id="S3.SS2.SSS2.Px3.p1.2.m2.1a"><msub id="S3.SS2.SSS2.Px3.p1.2.m2.1.1" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.2" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.2.cmml">E</mi><mrow id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.1" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.1a" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.4" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.1b" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.5" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px3.p1.2.m2.1b"><apply id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.2">𝐸</ci><apply id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3"><times id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.1"></times><ci id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.2">𝑡</ci><ci id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.3">𝑒</ci><ci id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.4.cmml" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.4">𝑥</ci><ci id="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.5.cmml" xref="S3.SS2.SSS2.Px3.p1.2.m2.1.1.3.5">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px3.p1.2.m2.1c">E_{text}</annotation></semantics></math> act as keys and values.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.62" class="ltx_Math" alttext="Attention(Q,K,V)=Scaled-Dot\&gt;Product\\
Attention(E_{vis},E_{text},E_{text})\in\mathbb{R}^{T\times d}" display="block"><semantics id="S3.E1.m1.62a"><mtable displaystyle="true" rowspacing="0pt" id="S3.E1.m1.62.62.6" xref="S3.E1.m1.59.59.3.cmml"><mtr id="S3.E1.m1.62.62.6a" xref="S3.E1.m1.59.59.3.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.62.62.6b" xref="S3.E1.m1.59.59.3.cmml"><mrow id="S3.E1.m1.34.34.34.34.34" xref="S3.E1.m1.59.59.3.cmml"><mrow id="S3.E1.m1.34.34.34.34.34.35" xref="S3.E1.m1.59.59.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.35.1" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.35.1a" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.35.1b" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.4.4.4.4.4.4" xref="S3.E1.m1.4.4.4.4.4.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.35.1c" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.5.5.5.5.5.5" xref="S3.E1.m1.5.5.5.5.5.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.35.1d" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.6.6.6.6.6.6" xref="S3.E1.m1.6.6.6.6.6.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.35.1e" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.7.7.7.7.7.7" xref="S3.E1.m1.7.7.7.7.7.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.35.1f" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.8.8.8.8.8.8" xref="S3.E1.m1.8.8.8.8.8.8.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.35.1g" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.9.9.9.9.9.9" xref="S3.E1.m1.9.9.9.9.9.9.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.35.1h" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mrow id="S3.E1.m1.34.34.34.34.34.35.2" xref="S3.E1.m1.59.59.3.cmml"><mo stretchy="false" id="S3.E1.m1.10.10.10.10.10.10" xref="S3.E1.m1.59.59.3a.cmml">(</mo><mi id="S3.E1.m1.11.11.11.11.11.11" xref="S3.E1.m1.11.11.11.11.11.11.cmml">Q</mi><mo id="S3.E1.m1.12.12.12.12.12.12" xref="S3.E1.m1.59.59.3a.cmml">,</mo><mi id="S3.E1.m1.13.13.13.13.13.13" xref="S3.E1.m1.13.13.13.13.13.13.cmml">K</mi><mo id="S3.E1.m1.14.14.14.14.14.14" xref="S3.E1.m1.59.59.3a.cmml">,</mo><mi id="S3.E1.m1.15.15.15.15.15.15" xref="S3.E1.m1.15.15.15.15.15.15.cmml">V</mi><mo stretchy="false" id="S3.E1.m1.16.16.16.16.16.16" xref="S3.E1.m1.59.59.3a.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.17.17.17.17.17.17" xref="S3.E1.m1.17.17.17.17.17.17.cmml">=</mo><mrow id="S3.E1.m1.34.34.34.34.34.36" xref="S3.E1.m1.59.59.3.cmml"><mrow id="S3.E1.m1.34.34.34.34.34.36.1" xref="S3.E1.m1.59.59.3.cmml"><mi id="S3.E1.m1.18.18.18.18.18.18" xref="S3.E1.m1.18.18.18.18.18.18.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.1.1" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.19.19.19.19.19.19" xref="S3.E1.m1.19.19.19.19.19.19.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.1.1a" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.20.20.20.20.20.20" xref="S3.E1.m1.20.20.20.20.20.20.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.1.1b" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.21.21.21.21.21.21" xref="S3.E1.m1.21.21.21.21.21.21.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.1.1c" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.22.22.22.22.22.22" xref="S3.E1.m1.22.22.22.22.22.22.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.1.1d" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.23.23.23.23.23.23" xref="S3.E1.m1.23.23.23.23.23.23.cmml">d</mi></mrow><mo id="S3.E1.m1.24.24.24.24.24.24" xref="S3.E1.m1.24.24.24.24.24.24.cmml">−</mo><mrow id="S3.E1.m1.34.34.34.34.34.36.2" xref="S3.E1.m1.59.59.3.cmml"><mi id="S3.E1.m1.25.25.25.25.25.25" xref="S3.E1.m1.25.25.25.25.25.25.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.2.1" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.26.26.26.26.26.26" xref="S3.E1.m1.26.26.26.26.26.26.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.2.1a" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.27.27.27.27.27.27" xref="S3.E1.m1.27.27.27.27.27.27.cmml">t</mi><mo lspace="0.220em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.2.1b" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.28.28.28.28.28.28" xref="S3.E1.m1.28.28.28.28.28.28.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.2.1c" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.29.29.29.29.29.29" xref="S3.E1.m1.29.29.29.29.29.29.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.2.1d" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.30.30.30.30.30.30" xref="S3.E1.m1.30.30.30.30.30.30.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.2.1e" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.31.31.31.31.31.31" xref="S3.E1.m1.31.31.31.31.31.31.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.2.1f" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.32.32.32.32.32.32" xref="S3.E1.m1.32.32.32.32.32.32.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.2.1g" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.33.33.33.33.33.33" xref="S3.E1.m1.33.33.33.33.33.33.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.34.34.34.34.34.36.2.1h" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.34.34.34.34.34.34" xref="S3.E1.m1.34.34.34.34.34.34.cmml">t</mi></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E1.m1.62.62.6c" xref="S3.E1.m1.59.59.3.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.62.62.6d" xref="S3.E1.m1.59.59.3.cmml"><mrow id="S3.E1.m1.62.62.6.59.25.25" xref="S3.E1.m1.59.59.3.cmml"><mrow id="S3.E1.m1.62.62.6.59.25.25.25" xref="S3.E1.m1.59.59.3.cmml"><mi id="S3.E1.m1.35.35.35.1.1.1" xref="S3.E1.m1.35.35.35.1.1.1.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.62.62.6.59.25.25.25.4" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.36.36.36.2.2.2" xref="S3.E1.m1.36.36.36.2.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.62.62.6.59.25.25.25.4a" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.37.37.37.3.3.3" xref="S3.E1.m1.37.37.37.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.62.62.6.59.25.25.25.4b" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.38.38.38.4.4.4" xref="S3.E1.m1.38.38.38.4.4.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.62.62.6.59.25.25.25.4c" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.39.39.39.5.5.5" xref="S3.E1.m1.39.39.39.5.5.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.62.62.6.59.25.25.25.4d" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.40.40.40.6.6.6" xref="S3.E1.m1.40.40.40.6.6.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.62.62.6.59.25.25.25.4e" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.41.41.41.7.7.7" xref="S3.E1.m1.41.41.41.7.7.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.62.62.6.59.25.25.25.4f" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.42.42.42.8.8.8" xref="S3.E1.m1.42.42.42.8.8.8.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.62.62.6.59.25.25.25.4g" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mi id="S3.E1.m1.43.43.43.9.9.9" xref="S3.E1.m1.43.43.43.9.9.9.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.62.62.6.59.25.25.25.4h" xref="S3.E1.m1.59.59.3a.cmml">​</mo><mrow id="S3.E1.m1.62.62.6.59.25.25.25.3.3" xref="S3.E1.m1.59.59.3.cmml"><mo stretchy="false" id="S3.E1.m1.44.44.44.10.10.10" xref="S3.E1.m1.59.59.3a.cmml">(</mo><msub id="S3.E1.m1.60.60.4.57.23.23.23.1.1.1" xref="S3.E1.m1.59.59.3.cmml"><mi id="S3.E1.m1.45.45.45.11.11.11" xref="S3.E1.m1.45.45.45.11.11.11.cmml">E</mi><mrow id="S3.E1.m1.46.46.46.12.12.12.1" xref="S3.E1.m1.46.46.46.12.12.12.1.cmml"><mi id="S3.E1.m1.46.46.46.12.12.12.1.2" xref="S3.E1.m1.46.46.46.12.12.12.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.46.46.46.12.12.12.1.1" xref="S3.E1.m1.46.46.46.12.12.12.1.1.cmml">​</mo><mi id="S3.E1.m1.46.46.46.12.12.12.1.3" xref="S3.E1.m1.46.46.46.12.12.12.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.46.46.46.12.12.12.1.1a" xref="S3.E1.m1.46.46.46.12.12.12.1.1.cmml">​</mo><mi id="S3.E1.m1.46.46.46.12.12.12.1.4" xref="S3.E1.m1.46.46.46.12.12.12.1.4.cmml">s</mi></mrow></msub><mo id="S3.E1.m1.47.47.47.13.13.13" xref="S3.E1.m1.59.59.3a.cmml">,</mo><msub id="S3.E1.m1.61.61.5.58.24.24.24.2.2.2" xref="S3.E1.m1.59.59.3.cmml"><mi id="S3.E1.m1.48.48.48.14.14.14" xref="S3.E1.m1.48.48.48.14.14.14.cmml">E</mi><mrow id="S3.E1.m1.49.49.49.15.15.15.1" xref="S3.E1.m1.49.49.49.15.15.15.1.cmml"><mi id="S3.E1.m1.49.49.49.15.15.15.1.2" xref="S3.E1.m1.49.49.49.15.15.15.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.49.49.49.15.15.15.1.1" xref="S3.E1.m1.49.49.49.15.15.15.1.1.cmml">​</mo><mi id="S3.E1.m1.49.49.49.15.15.15.1.3" xref="S3.E1.m1.49.49.49.15.15.15.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.49.49.49.15.15.15.1.1a" xref="S3.E1.m1.49.49.49.15.15.15.1.1.cmml">​</mo><mi id="S3.E1.m1.49.49.49.15.15.15.1.4" xref="S3.E1.m1.49.49.49.15.15.15.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.49.49.49.15.15.15.1.1b" xref="S3.E1.m1.49.49.49.15.15.15.1.1.cmml">​</mo><mi id="S3.E1.m1.49.49.49.15.15.15.1.5" xref="S3.E1.m1.49.49.49.15.15.15.1.5.cmml">t</mi></mrow></msub><mo id="S3.E1.m1.50.50.50.16.16.16" xref="S3.E1.m1.59.59.3a.cmml">,</mo><msub id="S3.E1.m1.62.62.6.59.25.25.25.3.3.3" xref="S3.E1.m1.59.59.3.cmml"><mi id="S3.E1.m1.51.51.51.17.17.17" xref="S3.E1.m1.51.51.51.17.17.17.cmml">E</mi><mrow id="S3.E1.m1.52.52.52.18.18.18.1" xref="S3.E1.m1.52.52.52.18.18.18.1.cmml"><mi id="S3.E1.m1.52.52.52.18.18.18.1.2" xref="S3.E1.m1.52.52.52.18.18.18.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.52.52.52.18.18.18.1.1" xref="S3.E1.m1.52.52.52.18.18.18.1.1.cmml">​</mo><mi id="S3.E1.m1.52.52.52.18.18.18.1.3" xref="S3.E1.m1.52.52.52.18.18.18.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.52.52.52.18.18.18.1.1a" xref="S3.E1.m1.52.52.52.18.18.18.1.1.cmml">​</mo><mi id="S3.E1.m1.52.52.52.18.18.18.1.4" xref="S3.E1.m1.52.52.52.18.18.18.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.52.52.52.18.18.18.1.1b" xref="S3.E1.m1.52.52.52.18.18.18.1.1.cmml">​</mo><mi id="S3.E1.m1.52.52.52.18.18.18.1.5" xref="S3.E1.m1.52.52.52.18.18.18.1.5.cmml">t</mi></mrow></msub><mo stretchy="false" id="S3.E1.m1.53.53.53.19.19.19" xref="S3.E1.m1.59.59.3a.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.54.54.54.20.20.20" xref="S3.E1.m1.54.54.54.20.20.20.cmml">∈</mo><msup id="S3.E1.m1.62.62.6.59.25.25.26" xref="S3.E1.m1.59.59.3.cmml"><mi id="S3.E1.m1.55.55.55.21.21.21" xref="S3.E1.m1.55.55.55.21.21.21.cmml">ℝ</mi><mrow id="S3.E1.m1.56.56.56.22.22.22.1" xref="S3.E1.m1.56.56.56.22.22.22.1.cmml"><mi id="S3.E1.m1.56.56.56.22.22.22.1.2" xref="S3.E1.m1.56.56.56.22.22.22.1.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.56.56.56.22.22.22.1.1" xref="S3.E1.m1.56.56.56.22.22.22.1.1.cmml">×</mo><mi id="S3.E1.m1.56.56.56.22.22.22.1.3" xref="S3.E1.m1.56.56.56.22.22.22.1.3.cmml">d</mi></mrow></msup></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.62b"><apply id="S3.E1.m1.59.59.3.cmml" xref="S3.E1.m1.62.62.6"><and id="S3.E1.m1.59.59.3a.cmml" xref="S3.E1.m1.34.34.34.34.34.35.1"></and><apply id="S3.E1.m1.59.59.3b.cmml" xref="S3.E1.m1.62.62.6"><eq id="S3.E1.m1.17.17.17.17.17.17.cmml" xref="S3.E1.m1.17.17.17.17.17.17"></eq><apply id="S3.E1.m1.59.59.3.5.cmml" xref="S3.E1.m1.62.62.6"><times id="S3.E1.m1.59.59.3.5.1.cmml" xref="S3.E1.m1.34.34.34.34.34.35.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">𝐴</ci><ci id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2">𝑡</ci><ci id="S3.E1.m1.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3">𝑡</ci><ci id="S3.E1.m1.4.4.4.4.4.4.cmml" xref="S3.E1.m1.4.4.4.4.4.4">𝑒</ci><ci id="S3.E1.m1.5.5.5.5.5.5.cmml" xref="S3.E1.m1.5.5.5.5.5.5">𝑛</ci><ci id="S3.E1.m1.6.6.6.6.6.6.cmml" xref="S3.E1.m1.6.6.6.6.6.6">𝑡</ci><ci id="S3.E1.m1.7.7.7.7.7.7.cmml" xref="S3.E1.m1.7.7.7.7.7.7">𝑖</ci><ci id="S3.E1.m1.8.8.8.8.8.8.cmml" xref="S3.E1.m1.8.8.8.8.8.8">𝑜</ci><ci id="S3.E1.m1.9.9.9.9.9.9.cmml" xref="S3.E1.m1.9.9.9.9.9.9">𝑛</ci><vector id="S3.E1.m1.59.59.3.5.11.cmml" xref="S3.E1.m1.62.62.6"><ci id="S3.E1.m1.11.11.11.11.11.11.cmml" xref="S3.E1.m1.11.11.11.11.11.11">𝑄</ci><ci id="S3.E1.m1.13.13.13.13.13.13.cmml" xref="S3.E1.m1.13.13.13.13.13.13">𝐾</ci><ci id="S3.E1.m1.15.15.15.15.15.15.cmml" xref="S3.E1.m1.15.15.15.15.15.15">𝑉</ci></vector></apply><apply id="S3.E1.m1.59.59.3.3.cmml" xref="S3.E1.m1.62.62.6"><minus id="S3.E1.m1.24.24.24.24.24.24.cmml" xref="S3.E1.m1.24.24.24.24.24.24"></minus><apply id="S3.E1.m1.59.59.3.3.5.cmml" xref="S3.E1.m1.62.62.6"><times id="S3.E1.m1.59.59.3.3.5.1.cmml" xref="S3.E1.m1.34.34.34.34.34.35.1"></times><ci id="S3.E1.m1.18.18.18.18.18.18.cmml" xref="S3.E1.m1.18.18.18.18.18.18">𝑆</ci><ci id="S3.E1.m1.19.19.19.19.19.19.cmml" xref="S3.E1.m1.19.19.19.19.19.19">𝑐</ci><ci id="S3.E1.m1.20.20.20.20.20.20.cmml" xref="S3.E1.m1.20.20.20.20.20.20">𝑎</ci><ci id="S3.E1.m1.21.21.21.21.21.21.cmml" xref="S3.E1.m1.21.21.21.21.21.21">𝑙</ci><ci id="S3.E1.m1.22.22.22.22.22.22.cmml" xref="S3.E1.m1.22.22.22.22.22.22">𝑒</ci><ci id="S3.E1.m1.23.23.23.23.23.23.cmml" xref="S3.E1.m1.23.23.23.23.23.23">𝑑</ci></apply><apply id="S3.E1.m1.59.59.3.3.3.cmml" xref="S3.E1.m1.62.62.6"><times id="S3.E1.m1.59.59.3.3.3.4.cmml" xref="S3.E1.m1.34.34.34.34.34.35.1"></times><ci id="S3.E1.m1.25.25.25.25.25.25.cmml" xref="S3.E1.m1.25.25.25.25.25.25">𝐷</ci><ci id="S3.E1.m1.26.26.26.26.26.26.cmml" xref="S3.E1.m1.26.26.26.26.26.26">𝑜</ci><ci id="S3.E1.m1.27.27.27.27.27.27.cmml" xref="S3.E1.m1.27.27.27.27.27.27">𝑡</ci><ci id="S3.E1.m1.28.28.28.28.28.28.cmml" xref="S3.E1.m1.28.28.28.28.28.28">𝑃</ci><ci id="S3.E1.m1.29.29.29.29.29.29.cmml" xref="S3.E1.m1.29.29.29.29.29.29">𝑟</ci><ci id="S3.E1.m1.30.30.30.30.30.30.cmml" xref="S3.E1.m1.30.30.30.30.30.30">𝑜</ci><ci id="S3.E1.m1.31.31.31.31.31.31.cmml" xref="S3.E1.m1.31.31.31.31.31.31">𝑑</ci><ci id="S3.E1.m1.32.32.32.32.32.32.cmml" xref="S3.E1.m1.32.32.32.32.32.32">𝑢</ci><ci id="S3.E1.m1.33.33.33.33.33.33.cmml" xref="S3.E1.m1.33.33.33.33.33.33">𝑐</ci><ci id="S3.E1.m1.34.34.34.34.34.34.cmml" xref="S3.E1.m1.34.34.34.34.34.34">𝑡</ci><ci id="S3.E1.m1.35.35.35.1.1.1.cmml" xref="S3.E1.m1.35.35.35.1.1.1">𝐴</ci><ci id="S3.E1.m1.36.36.36.2.2.2.cmml" xref="S3.E1.m1.36.36.36.2.2.2">𝑡</ci><ci id="S3.E1.m1.37.37.37.3.3.3.cmml" xref="S3.E1.m1.37.37.37.3.3.3">𝑡</ci><ci id="S3.E1.m1.38.38.38.4.4.4.cmml" xref="S3.E1.m1.38.38.38.4.4.4">𝑒</ci><ci id="S3.E1.m1.39.39.39.5.5.5.cmml" xref="S3.E1.m1.39.39.39.5.5.5">𝑛</ci><ci id="S3.E1.m1.40.40.40.6.6.6.cmml" xref="S3.E1.m1.40.40.40.6.6.6">𝑡</ci><ci id="S3.E1.m1.41.41.41.7.7.7.cmml" xref="S3.E1.m1.41.41.41.7.7.7">𝑖</ci><ci id="S3.E1.m1.42.42.42.8.8.8.cmml" xref="S3.E1.m1.42.42.42.8.8.8">𝑜</ci><ci id="S3.E1.m1.43.43.43.9.9.9.cmml" xref="S3.E1.m1.43.43.43.9.9.9">𝑛</ci><vector id="S3.E1.m1.59.59.3.3.3.3.4.cmml" xref="S3.E1.m1.62.62.6"><apply id="S3.E1.m1.57.57.1.1.1.1.1.1.cmml" xref="S3.E1.m1.62.62.6"><csymbol cd="ambiguous" id="S3.E1.m1.57.57.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.62.62.6">subscript</csymbol><ci id="S3.E1.m1.45.45.45.11.11.11.cmml" xref="S3.E1.m1.45.45.45.11.11.11">𝐸</ci><apply id="S3.E1.m1.46.46.46.12.12.12.1.cmml" xref="S3.E1.m1.46.46.46.12.12.12.1"><times id="S3.E1.m1.46.46.46.12.12.12.1.1.cmml" xref="S3.E1.m1.46.46.46.12.12.12.1.1"></times><ci id="S3.E1.m1.46.46.46.12.12.12.1.2.cmml" xref="S3.E1.m1.46.46.46.12.12.12.1.2">𝑣</ci><ci id="S3.E1.m1.46.46.46.12.12.12.1.3.cmml" xref="S3.E1.m1.46.46.46.12.12.12.1.3">𝑖</ci><ci id="S3.E1.m1.46.46.46.12.12.12.1.4.cmml" xref="S3.E1.m1.46.46.46.12.12.12.1.4">𝑠</ci></apply></apply><apply id="S3.E1.m1.58.58.2.2.2.2.2.2.cmml" xref="S3.E1.m1.62.62.6"><csymbol cd="ambiguous" id="S3.E1.m1.58.58.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.62.62.6">subscript</csymbol><ci id="S3.E1.m1.48.48.48.14.14.14.cmml" xref="S3.E1.m1.48.48.48.14.14.14">𝐸</ci><apply id="S3.E1.m1.49.49.49.15.15.15.1.cmml" xref="S3.E1.m1.49.49.49.15.15.15.1"><times id="S3.E1.m1.49.49.49.15.15.15.1.1.cmml" xref="S3.E1.m1.49.49.49.15.15.15.1.1"></times><ci id="S3.E1.m1.49.49.49.15.15.15.1.2.cmml" xref="S3.E1.m1.49.49.49.15.15.15.1.2">𝑡</ci><ci id="S3.E1.m1.49.49.49.15.15.15.1.3.cmml" xref="S3.E1.m1.49.49.49.15.15.15.1.3">𝑒</ci><ci id="S3.E1.m1.49.49.49.15.15.15.1.4.cmml" xref="S3.E1.m1.49.49.49.15.15.15.1.4">𝑥</ci><ci id="S3.E1.m1.49.49.49.15.15.15.1.5.cmml" xref="S3.E1.m1.49.49.49.15.15.15.1.5">𝑡</ci></apply></apply><apply id="S3.E1.m1.59.59.3.3.3.3.3.3.cmml" xref="S3.E1.m1.62.62.6"><csymbol cd="ambiguous" id="S3.E1.m1.59.59.3.3.3.3.3.3.1.cmml" xref="S3.E1.m1.62.62.6">subscript</csymbol><ci id="S3.E1.m1.51.51.51.17.17.17.cmml" xref="S3.E1.m1.51.51.51.17.17.17">𝐸</ci><apply id="S3.E1.m1.52.52.52.18.18.18.1.cmml" xref="S3.E1.m1.52.52.52.18.18.18.1"><times id="S3.E1.m1.52.52.52.18.18.18.1.1.cmml" xref="S3.E1.m1.52.52.52.18.18.18.1.1"></times><ci id="S3.E1.m1.52.52.52.18.18.18.1.2.cmml" xref="S3.E1.m1.52.52.52.18.18.18.1.2">𝑡</ci><ci id="S3.E1.m1.52.52.52.18.18.18.1.3.cmml" xref="S3.E1.m1.52.52.52.18.18.18.1.3">𝑒</ci><ci id="S3.E1.m1.52.52.52.18.18.18.1.4.cmml" xref="S3.E1.m1.52.52.52.18.18.18.1.4">𝑥</ci><ci id="S3.E1.m1.52.52.52.18.18.18.1.5.cmml" xref="S3.E1.m1.52.52.52.18.18.18.1.5">𝑡</ci></apply></apply></vector></apply></apply></apply><apply id="S3.E1.m1.59.59.3c.cmml" xref="S3.E1.m1.62.62.6"><in id="S3.E1.m1.54.54.54.20.20.20.cmml" xref="S3.E1.m1.54.54.54.20.20.20"></in><share href="#S3.E1.m1.59.59.3.3.cmml" id="S3.E1.m1.59.59.3d.cmml" xref="S3.E1.m1.34.34.34.34.34.35.1"></share><apply id="S3.E1.m1.59.59.3.8.cmml" xref="S3.E1.m1.62.62.6"><csymbol cd="ambiguous" id="S3.E1.m1.59.59.3.8.1.cmml" xref="S3.E1.m1.34.34.34.34.34.35.1">superscript</csymbol><ci id="S3.E1.m1.55.55.55.21.21.21.cmml" xref="S3.E1.m1.55.55.55.21.21.21">ℝ</ci><apply id="S3.E1.m1.56.56.56.22.22.22.1.cmml" xref="S3.E1.m1.56.56.56.22.22.22.1"><times id="S3.E1.m1.56.56.56.22.22.22.1.1.cmml" xref="S3.E1.m1.56.56.56.22.22.22.1.1"></times><ci id="S3.E1.m1.56.56.56.22.22.22.1.2.cmml" xref="S3.E1.m1.56.56.56.22.22.22.1.2">𝑇</ci><ci id="S3.E1.m1.56.56.56.22.22.22.1.3.cmml" xref="S3.E1.m1.56.56.56.22.22.22.1.3">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.62c">Attention(Q,K,V)=Scaled-Dot\&gt;Product\\
Attention(E_{vis},E_{text},E_{text})\in\mathbb{R}^{T\times d}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.Px3.p1.5" class="ltx_p">Through this attention module, the network learns the video-text temporal alignment, which synchronizes the generated speech with the input video frames. Now between the video sequence and melspectrograms, it is known that there exists a natural temporal alignment. The length of the melspectrograms is a constant <math id="S3.SS2.SSS2.Px3.p1.3.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.SSS2.Px3.p1.3.m1.1a"><mi id="S3.SS2.SSS2.Px3.p1.3.m1.1.1" xref="S3.SS2.SSS2.Px3.p1.3.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px3.p1.3.m1.1b"><ci id="S3.SS2.SSS2.Px3.p1.3.m1.1.1.cmml" xref="S3.SS2.SSS2.Px3.p1.3.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px3.p1.3.m1.1c">n</annotation></semantics></math> times the length of the video. Thus, the attention output <math id="S3.SS2.SSS2.Px3.p1.4.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS2.SSS2.Px3.p1.4.m2.1a"><mi id="S3.SS2.SSS2.Px3.p1.4.m2.1.1" xref="S3.SS2.SSS2.Px3.p1.4.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px3.p1.4.m2.1b"><ci id="S3.SS2.SSS2.Px3.p1.4.m2.1.1.cmml" xref="S3.SS2.SSS2.Px3.p1.4.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px3.p1.4.m2.1c">A</annotation></semantics></math> is up-sampled <math id="S3.SS2.SSS2.Px3.p1.5.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.SSS2.Px3.p1.5.m3.1a"><mi id="S3.SS2.SSS2.Px3.p1.5.m3.1.1" xref="S3.SS2.SSS2.Px3.p1.5.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px3.p1.5.m3.1b"><ci id="S3.SS2.SSS2.Px3.p1.5.m3.1.1.cmml" xref="S3.SS2.SSS2.Px3.p1.5.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px3.p1.5.m3.1c">n</annotation></semantics></math> times to directly obtain the melspectrogram duration. This eliminates the need to train a separate duration predictor as done in FastSpeech2 <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite>. In other words, the text-to-video alignment network already determines the duration of each phoneme in the speech output.</p>
</div>
</section>
<section id="S3.SS2.SSS2.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Speaker Embedding:</h5>

<div id="S3.SS2.SSS2.Px4.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px4.p1.1" class="ltx_p">Unlike single-speaker models, we want our model to generate speech for any arbitrary speaker in the wild. Thus, our model also needs the voice input of the target speaker to generate the speech in his/her voice. We consider a random one-second audio segment of the target speaker and extract the speaker embedding vector <math id="S3.SS2.SSS2.Px4.p1.1.m1.1" class="ltx_Math" alttext="E_{voice}" display="inline"><semantics id="S3.SS2.SSS2.Px4.p1.1.m1.1a"><msub id="S3.SS2.SSS2.Px4.p1.1.m1.1.1" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.2.cmml">E</mi><mrow id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.1a" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.4" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.1b" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.5" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.1c" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.6" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.6.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px4.p1.1.m1.1b"><apply id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.2">𝐸</ci><apply id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3"><times id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.2">𝑣</ci><ci id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.3">𝑜</ci><ci id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.4.cmml" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.4">𝑖</ci><ci id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.5.cmml" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.5">𝑐</ci><ci id="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.6.cmml" xref="S3.SS2.SSS2.Px4.p1.1.m1.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px4.p1.1.m1.1c">E_{voice}</annotation></semantics></math> using the pre-trained identity network<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a href="github.com/CorentinJ/Real-Time-Voice-Cloning" title="" class="ltx_ref ltx_url ltx_font_typewriter">github.com/CorentinJ/Real-Time-Voice-Cloning</a></span></span></span>.</p>
</div>
</section>
<section id="S3.SS2.SSS2.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Spectrogram Decoder:</h5>

<div id="S3.SS2.SSS2.Px5.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px5.p1.3" class="ltx_p">The speaker embedding vector <math id="S3.SS2.SSS2.Px5.p1.1.m1.1" class="ltx_Math" alttext="E_{voice}" display="inline"><semantics id="S3.SS2.SSS2.Px5.p1.1.m1.1a"><msub id="S3.SS2.SSS2.Px5.p1.1.m1.1.1" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.2.cmml">E</mi><mrow id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.1a" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.4" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.1b" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.5" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.1c" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.6" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.6.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px5.p1.1.m1.1b"><apply id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.2">𝐸</ci><apply id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3"><times id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.2">𝑣</ci><ci id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.3">𝑜</ci><ci id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.4.cmml" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.4">𝑖</ci><ci id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.5.cmml" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.5">𝑐</ci><ci id="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.6.cmml" xref="S3.SS2.SSS2.Px5.p1.1.m1.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px5.p1.1.m1.1c">E_{voice}</annotation></semantics></math> is added to the upsampled attention output <math id="S3.SS2.SSS2.Px5.p1.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS2.SSS2.Px5.p1.2.m2.1a"><mi id="S3.SS2.SSS2.Px5.p1.2.m2.1.1" xref="S3.SS2.SSS2.Px5.p1.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px5.p1.2.m2.1b"><ci id="S3.SS2.SSS2.Px5.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.Px5.p1.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px5.p1.2.m2.1c">A</annotation></semantics></math> to obtain voice-aware content representation. The spectrogram decoder, consisting of transformer decoder layers, ingests this representation and generates the melspectrogram sequence <math id="S3.SS2.SSS2.Px5.p1.3.m3.1" class="ltx_Math" alttext="S_{m}" display="inline"><semantics id="S3.SS2.SSS2.Px5.p1.3.m3.1a"><msub id="S3.SS2.SSS2.Px5.p1.3.m3.1.1" xref="S3.SS2.SSS2.Px5.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS2.Px5.p1.3.m3.1.1.2" xref="S3.SS2.SSS2.Px5.p1.3.m3.1.1.2.cmml">S</mi><mi id="S3.SS2.SSS2.Px5.p1.3.m3.1.1.3" xref="S3.SS2.SSS2.Px5.p1.3.m3.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.Px5.p1.3.m3.1b"><apply id="S3.SS2.SSS2.Px5.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.Px5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.Px5.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.Px5.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.Px5.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.Px5.p1.3.m3.1.1.2">𝑆</ci><ci id="S3.SS2.SSS2.Px5.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.Px5.p1.3.m3.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.Px5.p1.3.m3.1c">S_{m}</annotation></semantics></math>. To further improve speech quality, as done in most of the TTS networks, we adopt a pre-trained neural vocoder model BigVGAN <cite class="ltx_cite ltx_citemacro_citep">(gil Lee et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite>, to synthesize the speech from the melspectrogram output. Note that this step is only used during inference to obtain high-quality speech outputs.</p>
</div>
</section>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Datasets and Training Settings</h3>

<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>Datasets</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.6" class="ltx_p">We evaluate both constrained and unconstrained datasets to analyze the model’s performance. The first corpus we experiment with is the TCD-TIMIT <cite class="ltx_cite ltx_citemacro_citep">(Harte and Gillen, <a href="#bib.bib16" title="" class="ltx_ref">2015</a>)</cite> lip speaker dataset, which comprises lab-recorded videos of <math id="S3.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS3.SSS1.p1.1.m1.1a"><mn id="S3.SS3.SSS1.p1.1.m1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.1b"><cn type="integer" id="S3.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.1c">3</annotation></semantics></math> speakers. Next, we consider the word-level LRW <cite class="ltx_cite ltx_citemacro_citep">(Chung and Zisserman, <a href="#bib.bib9" title="" class="ltx_ref">2016a</a>)</cite> dataset, consisting of around <math id="S3.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="150" display="inline"><semantics id="S3.SS3.SSS1.p1.2.m2.1a"><mn id="S3.SS3.SSS1.p1.2.m2.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.cmml">150</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.2.m2.1b"><cn type="integer" id="S3.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1">150</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.2.m2.1c">150</annotation></semantics></math> hours of single-word utterances from hundreds of speakers. We then move on to the more challenging large-scale datasets: LRS2 <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> and LRS3 <cite class="ltx_cite ltx_citemacro_citep">(Afouras et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018b</a>)</cite>. The LRS2 data comprises thousands of speakers from BBC programs with a vocabulary of <math id="S3.SS3.SSS1.p1.3.m3.1" class="ltx_Math" alttext="59k" display="inline"><semantics id="S3.SS3.SSS1.p1.3.m3.1a"><mrow id="S3.SS3.SSS1.p1.3.m3.1.1" xref="S3.SS3.SSS1.p1.3.m3.1.1.cmml"><mn id="S3.SS3.SSS1.p1.3.m3.1.1.2" xref="S3.SS3.SSS1.p1.3.m3.1.1.2.cmml">59</mn><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.3.m3.1.1.1" xref="S3.SS3.SSS1.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS1.p1.3.m3.1.1.3" xref="S3.SS3.SSS1.p1.3.m3.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.3.m3.1b"><apply id="S3.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1"><times id="S3.SS3.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS3.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1.2">59</cn><ci id="S3.SS3.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.3.m3.1c">59k</annotation></semantics></math> and around <math id="S3.SS3.SSS1.p1.4.m4.1" class="ltx_Math" alttext="230" display="inline"><semantics id="S3.SS3.SSS1.p1.4.m4.1a"><mn id="S3.SS3.SSS1.p1.4.m4.1.1" xref="S3.SS3.SSS1.p1.4.m4.1.1.cmml">230</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.4.m4.1b"><cn type="integer" id="S3.SS3.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1">230</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.4.m4.1c">230</annotation></semantics></math> hours of video clips (both “train” and “pre-train” sets together). The LRS3 dataset, on the other hand, is also a large-scale dataset with a total of approximately <math id="S3.SS3.SSS1.p1.5.m5.1" class="ltx_Math" alttext="430" display="inline"><semantics id="S3.SS3.SSS1.p1.5.m5.1a"><mn id="S3.SS3.SSS1.p1.5.m5.1.1" xref="S3.SS3.SSS1.p1.5.m5.1.1.cmml">430</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.5.m5.1b"><cn type="integer" id="S3.SS3.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p1.5.m5.1.1">430</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.5.m5.1c">430</annotation></semantics></math> hours (“train” and “pre-train” sets) of video data with <math id="S3.SS3.SSS1.p1.6.m6.1" class="ltx_Math" alttext="150k" display="inline"><semantics id="S3.SS3.SSS1.p1.6.m6.1a"><mrow id="S3.SS3.SSS1.p1.6.m6.1.1" xref="S3.SS3.SSS1.p1.6.m6.1.1.cmml"><mn id="S3.SS3.SSS1.p1.6.m6.1.1.2" xref="S3.SS3.SSS1.p1.6.m6.1.1.2.cmml">150</mn><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.6.m6.1.1.1" xref="S3.SS3.SSS1.p1.6.m6.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS1.p1.6.m6.1.1.3" xref="S3.SS3.SSS1.p1.6.m6.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.6.m6.1b"><apply id="S3.SS3.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS1.p1.6.m6.1.1"><times id="S3.SS3.SSS1.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS1.p1.6.m6.1.1.1"></times><cn type="integer" id="S3.SS3.SSS1.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS1.p1.6.m6.1.1.2">150</cn><ci id="S3.SS3.SSS1.p1.6.m6.1.1.3.cmml" xref="S3.SS3.SSS1.p1.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.6.m6.1c">150k</annotation></semantics></math> utterances. It consists of thousands of spoken sentences from TED and TEDx talks in English. We train and test the performance of our network using the official splits of LRW, LRS2 and LRS3 datasets and use the train-test split proposed in Lip2Wav <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite> for TIMIT dataset <cite class="ltx_cite ltx_citemacro_citep">(Harte and Gillen, <a href="#bib.bib16" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span>Data pre-processing</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.5" class="ltx_p">We sample the video frames at <math id="S3.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S3.SS3.SSS2.p1.1.m1.1a"><mn id="S3.SS3.SSS2.p1.1.m1.1.1" xref="S3.SS3.SSS2.p1.1.m1.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.1.m1.1b"><cn type="integer" id="S3.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.1.m1.1c">25</annotation></semantics></math> FPS and follow the pre-processing procedure of VTP <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite> to obtain the face crops. For the speech segments, we compute STFT and then melspectrograms of <math id="S3.SS3.SSS2.p1.2.m2.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S3.SS3.SSS2.p1.2.m2.1a"><mn id="S3.SS3.SSS2.p1.2.m2.1.1" xref="S3.SS3.SSS2.p1.2.m2.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.2.m2.1b"><cn type="integer" id="S3.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.2.m2.1c">80</annotation></semantics></math> mel-bands, with a hop length of <math id="S3.SS3.SSS2.p1.3.m3.1" class="ltx_Math" alttext="10ms" display="inline"><semantics id="S3.SS3.SSS2.p1.3.m3.1a"><mrow id="S3.SS3.SSS2.p1.3.m3.1.1" xref="S3.SS3.SSS2.p1.3.m3.1.1.cmml"><mn id="S3.SS3.SSS2.p1.3.m3.1.1.2" xref="S3.SS3.SSS2.p1.3.m3.1.1.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="S3.SS3.SSS2.p1.3.m3.1.1.1" xref="S3.SS3.SSS2.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS2.p1.3.m3.1.1.3" xref="S3.SS3.SSS2.p1.3.m3.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS2.p1.3.m3.1.1.1a" xref="S3.SS3.SSS2.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS2.p1.3.m3.1.1.4" xref="S3.SS3.SSS2.p1.3.m3.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.3.m3.1b"><apply id="S3.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1"><times id="S3.SS3.SSS2.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS3.SSS2.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.2">10</cn><ci id="S3.SS3.SSS2.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.3">𝑚</ci><ci id="S3.SS3.SSS2.p1.3.m3.1.1.4.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.4">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.3.m3.1c">10ms</annotation></semantics></math> and a window length of <math id="S3.SS3.SSS2.p1.4.m4.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S3.SS3.SSS2.p1.4.m4.1a"><mn id="S3.SS3.SSS2.p1.4.m4.1.1" xref="S3.SS3.SSS2.p1.4.m4.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.4.m4.1b"><cn type="integer" id="S3.SS3.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.4.m4.1c">25</annotation></semantics></math>ms, sampled at <math id="S3.SS3.SSS2.p1.5.m5.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S3.SS3.SSS2.p1.5.m5.1a"><mn id="S3.SS3.SSS2.p1.5.m5.1.1" xref="S3.SS3.SSS2.p1.5.m5.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.5.m5.1b"><cn type="integer" id="S3.SS3.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS2.p1.5.m5.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.5.m5.1c">16</annotation></semantics></math>kHz. We use an open-source grapheme-to-phoneme tool for text processing to obtain the phoneme inputs for our Visual TTS model.</p>
</div>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3. </span>Model configuration and training</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.13" class="ltx_p">Our Visual TTS model comprises <math id="S3.SS3.SSS3.p1.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS3.SSS3.p1.1.m1.1a"><mn id="S3.SS3.SSS3.p1.1.m1.1.1" xref="S3.SS3.SSS3.p1.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.1.m1.1b"><cn type="integer" id="S3.SS3.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS3.p1.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.1.m1.1c">4</annotation></semantics></math> FFT blocks in the text and visual encoders and <math id="S3.SS3.SSS3.p1.2.m2.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS3.SSS3.p1.2.m2.1a"><mn id="S3.SS3.SSS3.p1.2.m2.1.1" xref="S3.SS3.SSS3.p1.2.m2.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.2.m2.1b"><cn type="integer" id="S3.SS3.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS3.p1.2.m2.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.2.m2.1c">6</annotation></semantics></math> FFT blocks in the spectrogram decoder network. The visual embeddings obtained from VTP are <math id="S3.SS3.SSS3.p1.3.m3.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S3.SS3.SSS3.p1.3.m3.1a"><mn id="S3.SS3.SSS3.p1.3.m3.1.1" xref="S3.SS3.SSS3.p1.3.m3.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.3.m3.1b"><cn type="integer" id="S3.SS3.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS3.p1.3.m3.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.3.m3.1c">512</annotation></semantics></math>-dimensional embeddings for each frame. In the video-text attention sub-network, the upsample factor, <math id="S3.SS3.SSS3.p1.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.SSS3.p1.4.m4.1a"><mi id="S3.SS3.SSS3.p1.4.m4.1.1" xref="S3.SS3.SSS3.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.4.m4.1b"><ci id="S3.SS3.SSS3.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS3.p1.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.4.m4.1c">n</annotation></semantics></math> is set to <math id="S3.SS3.SSS3.p1.5.m5.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS3.SSS3.p1.5.m5.1a"><mn id="S3.SS3.SSS3.p1.5.m5.1.1" xref="S3.SS3.SSS3.p1.5.m5.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.5.m5.1b"><cn type="integer" id="S3.SS3.SSS3.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS3.p1.5.m5.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.5.m5.1c">4</annotation></semantics></math>. For the speaker embedding, the identity network outputs a <math id="S3.SS3.SSS3.p1.6.m6.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S3.SS3.SSS3.p1.6.m6.1a"><mn id="S3.SS3.SSS3.p1.6.m6.1.1" xref="S3.SS3.SSS3.p1.6.m6.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.6.m6.1b"><cn type="integer" id="S3.SS3.SSS3.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS3.p1.6.m6.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.6.m6.1c">256</annotation></semantics></math> dimensional vector for each speech sample. For the lip-to-text network, we use the publicly released pre-trained model<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/prajwalkr/vtp" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/prajwalkr/vtp</a></span></span></span> (trained on LRS2 <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> and LRS3 <cite class="ltx_cite ltx_citemacro_citep">(Afouras et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018b</a>)</cite>). Our visual TTS model is trained on a single NVIDIA <math id="S3.SS3.SSS3.p1.7.m7.1" class="ltx_Math" alttext="2080" display="inline"><semantics id="S3.SS3.SSS3.p1.7.m7.1a"><mn id="S3.SS3.SSS3.p1.7.m7.1.1" xref="S3.SS3.SSS3.p1.7.m7.1.1.cmml">2080</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.7.m7.1b"><cn type="integer" id="S3.SS3.SSS3.p1.7.m7.1.1.cmml" xref="S3.SS3.SSS3.p1.7.m7.1.1">2080</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.7.m7.1c">2080</annotation></semantics></math> Ti GPU. We use the Adam optimizer <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a href="#bib.bib23" title="" class="ltx_ref">2015</a>)</cite> with <math id="S3.SS3.SSS3.p1.8.m8.1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><semantics id="S3.SS3.SSS3.p1.8.m8.1a"><mrow id="S3.SS3.SSS3.p1.8.m8.1.1" xref="S3.SS3.SSS3.p1.8.m8.1.1.cmml"><msub id="S3.SS3.SSS3.p1.8.m8.1.1.2" xref="S3.SS3.SSS3.p1.8.m8.1.1.2.cmml"><mi id="S3.SS3.SSS3.p1.8.m8.1.1.2.2" xref="S3.SS3.SSS3.p1.8.m8.1.1.2.2.cmml">β</mi><mn id="S3.SS3.SSS3.p1.8.m8.1.1.2.3" xref="S3.SS3.SSS3.p1.8.m8.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS3.SSS3.p1.8.m8.1.1.1" xref="S3.SS3.SSS3.p1.8.m8.1.1.1.cmml">=</mo><mn id="S3.SS3.SSS3.p1.8.m8.1.1.3" xref="S3.SS3.SSS3.p1.8.m8.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.8.m8.1b"><apply id="S3.SS3.SSS3.p1.8.m8.1.1.cmml" xref="S3.SS3.SSS3.p1.8.m8.1.1"><eq id="S3.SS3.SSS3.p1.8.m8.1.1.1.cmml" xref="S3.SS3.SSS3.p1.8.m8.1.1.1"></eq><apply id="S3.SS3.SSS3.p1.8.m8.1.1.2.cmml" xref="S3.SS3.SSS3.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS3.p1.8.m8.1.1.2.1.cmml" xref="S3.SS3.SSS3.p1.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS3.p1.8.m8.1.1.2.2.cmml" xref="S3.SS3.SSS3.p1.8.m8.1.1.2.2">𝛽</ci><cn type="integer" id="S3.SS3.SSS3.p1.8.m8.1.1.2.3.cmml" xref="S3.SS3.SSS3.p1.8.m8.1.1.2.3">1</cn></apply><cn type="float" id="S3.SS3.SSS3.p1.8.m8.1.1.3.cmml" xref="S3.SS3.SSS3.p1.8.m8.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.8.m8.1c">\beta_{1}=0.9</annotation></semantics></math>, <math id="S3.SS3.SSS3.p1.9.m9.1" class="ltx_Math" alttext="\beta_{2}=0.98" display="inline"><semantics id="S3.SS3.SSS3.p1.9.m9.1a"><mrow id="S3.SS3.SSS3.p1.9.m9.1.1" xref="S3.SS3.SSS3.p1.9.m9.1.1.cmml"><msub id="S3.SS3.SSS3.p1.9.m9.1.1.2" xref="S3.SS3.SSS3.p1.9.m9.1.1.2.cmml"><mi id="S3.SS3.SSS3.p1.9.m9.1.1.2.2" xref="S3.SS3.SSS3.p1.9.m9.1.1.2.2.cmml">β</mi><mn id="S3.SS3.SSS3.p1.9.m9.1.1.2.3" xref="S3.SS3.SSS3.p1.9.m9.1.1.2.3.cmml">2</mn></msub><mo id="S3.SS3.SSS3.p1.9.m9.1.1.1" xref="S3.SS3.SSS3.p1.9.m9.1.1.1.cmml">=</mo><mn id="S3.SS3.SSS3.p1.9.m9.1.1.3" xref="S3.SS3.SSS3.p1.9.m9.1.1.3.cmml">0.98</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.9.m9.1b"><apply id="S3.SS3.SSS3.p1.9.m9.1.1.cmml" xref="S3.SS3.SSS3.p1.9.m9.1.1"><eq id="S3.SS3.SSS3.p1.9.m9.1.1.1.cmml" xref="S3.SS3.SSS3.p1.9.m9.1.1.1"></eq><apply id="S3.SS3.SSS3.p1.9.m9.1.1.2.cmml" xref="S3.SS3.SSS3.p1.9.m9.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS3.p1.9.m9.1.1.2.1.cmml" xref="S3.SS3.SSS3.p1.9.m9.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS3.p1.9.m9.1.1.2.2.cmml" xref="S3.SS3.SSS3.p1.9.m9.1.1.2.2">𝛽</ci><cn type="integer" id="S3.SS3.SSS3.p1.9.m9.1.1.2.3.cmml" xref="S3.SS3.SSS3.p1.9.m9.1.1.2.3">2</cn></apply><cn type="float" id="S3.SS3.SSS3.p1.9.m9.1.1.3.cmml" xref="S3.SS3.SSS3.p1.9.m9.1.1.3">0.98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.9.m9.1c">\beta_{2}=0.98</annotation></semantics></math>, and <math id="S3.SS3.SSS3.p1.10.m10.1" class="ltx_Math" alttext="\epsilon=10^{-9}" display="inline"><semantics id="S3.SS3.SSS3.p1.10.m10.1a"><mrow id="S3.SS3.SSS3.p1.10.m10.1.1" xref="S3.SS3.SSS3.p1.10.m10.1.1.cmml"><mi id="S3.SS3.SSS3.p1.10.m10.1.1.2" xref="S3.SS3.SSS3.p1.10.m10.1.1.2.cmml">ϵ</mi><mo id="S3.SS3.SSS3.p1.10.m10.1.1.1" xref="S3.SS3.SSS3.p1.10.m10.1.1.1.cmml">=</mo><msup id="S3.SS3.SSS3.p1.10.m10.1.1.3" xref="S3.SS3.SSS3.p1.10.m10.1.1.3.cmml"><mn id="S3.SS3.SSS3.p1.10.m10.1.1.3.2" xref="S3.SS3.SSS3.p1.10.m10.1.1.3.2.cmml">10</mn><mrow id="S3.SS3.SSS3.p1.10.m10.1.1.3.3" xref="S3.SS3.SSS3.p1.10.m10.1.1.3.3.cmml"><mo id="S3.SS3.SSS3.p1.10.m10.1.1.3.3a" xref="S3.SS3.SSS3.p1.10.m10.1.1.3.3.cmml">−</mo><mn id="S3.SS3.SSS3.p1.10.m10.1.1.3.3.2" xref="S3.SS3.SSS3.p1.10.m10.1.1.3.3.2.cmml">9</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.10.m10.1b"><apply id="S3.SS3.SSS3.p1.10.m10.1.1.cmml" xref="S3.SS3.SSS3.p1.10.m10.1.1"><eq id="S3.SS3.SSS3.p1.10.m10.1.1.1.cmml" xref="S3.SS3.SSS3.p1.10.m10.1.1.1"></eq><ci id="S3.SS3.SSS3.p1.10.m10.1.1.2.cmml" xref="S3.SS3.SSS3.p1.10.m10.1.1.2">italic-ϵ</ci><apply id="S3.SS3.SSS3.p1.10.m10.1.1.3.cmml" xref="S3.SS3.SSS3.p1.10.m10.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS3.p1.10.m10.1.1.3.1.cmml" xref="S3.SS3.SSS3.p1.10.m10.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS3.SSS3.p1.10.m10.1.1.3.2.cmml" xref="S3.SS3.SSS3.p1.10.m10.1.1.3.2">10</cn><apply id="S3.SS3.SSS3.p1.10.m10.1.1.3.3.cmml" xref="S3.SS3.SSS3.p1.10.m10.1.1.3.3"><minus id="S3.SS3.SSS3.p1.10.m10.1.1.3.3.1.cmml" xref="S3.SS3.SSS3.p1.10.m10.1.1.3.3"></minus><cn type="integer" id="S3.SS3.SSS3.p1.10.m10.1.1.3.3.2.cmml" xref="S3.SS3.SSS3.p1.10.m10.1.1.3.3.2">9</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.10.m10.1c">\epsilon=10^{-9}</annotation></semantics></math> and follow the same learning rate schedule as done in <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>. We set the batch size to be <math id="S3.SS3.SSS3.p1.11.m11.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S3.SS3.SSS3.p1.11.m11.1a"><mn id="S3.SS3.SSS3.p1.11.m11.1.1" xref="S3.SS3.SSS3.p1.11.m11.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.11.m11.1b"><cn type="integer" id="S3.SS3.SSS3.p1.11.m11.1.1.cmml" xref="S3.SS3.SSS3.p1.11.m11.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.11.m11.1c">16</annotation></semantics></math> for all the datasets and train the model using <math id="S3.SS3.SSS3.p1.12.m12.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS3.SSS3.p1.12.m12.1a"><msub id="S3.SS3.SSS3.p1.12.m12.1.1" xref="S3.SS3.SSS3.p1.12.m12.1.1.cmml"><mi id="S3.SS3.SSS3.p1.12.m12.1.1.2" xref="S3.SS3.SSS3.p1.12.m12.1.1.2.cmml">L</mi><mn id="S3.SS3.SSS3.p1.12.m12.1.1.3" xref="S3.SS3.SSS3.p1.12.m12.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.12.m12.1b"><apply id="S3.SS3.SSS3.p1.12.m12.1.1.cmml" xref="S3.SS3.SSS3.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS3.p1.12.m12.1.1.1.cmml" xref="S3.SS3.SSS3.p1.12.m12.1.1">subscript</csymbol><ci id="S3.SS3.SSS3.p1.12.m12.1.1.2.cmml" xref="S3.SS3.SSS3.p1.12.m12.1.1.2">𝐿</ci><cn type="integer" id="S3.SS3.SSS3.p1.12.m12.1.1.3.cmml" xref="S3.SS3.SSS3.p1.12.m12.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.12.m12.1c">L_{1}</annotation></semantics></math> reconstruction loss for approximately <math id="S3.SS3.SSS3.p1.13.m13.1" class="ltx_Math" alttext="900k" display="inline"><semantics id="S3.SS3.SSS3.p1.13.m13.1a"><mrow id="S3.SS3.SSS3.p1.13.m13.1.1" xref="S3.SS3.SSS3.p1.13.m13.1.1.cmml"><mn id="S3.SS3.SSS3.p1.13.m13.1.1.2" xref="S3.SS3.SSS3.p1.13.m13.1.1.2.cmml">900</mn><mo lspace="0em" rspace="0em" id="S3.SS3.SSS3.p1.13.m13.1.1.1" xref="S3.SS3.SSS3.p1.13.m13.1.1.1.cmml">​</mo><mi id="S3.SS3.SSS3.p1.13.m13.1.1.3" xref="S3.SS3.SSS3.p1.13.m13.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.13.m13.1b"><apply id="S3.SS3.SSS3.p1.13.m13.1.1.cmml" xref="S3.SS3.SSS3.p1.13.m13.1.1"><times id="S3.SS3.SSS3.p1.13.m13.1.1.1.cmml" xref="S3.SS3.SSS3.p1.13.m13.1.1.1"></times><cn type="integer" id="S3.SS3.SSS3.p1.13.m13.1.1.2.cmml" xref="S3.SS3.SSS3.p1.13.m13.1.1.2">900</cn><ci id="S3.SS3.SSS3.p1.13.m13.1.1.3.cmml" xref="S3.SS3.SSS3.p1.13.m13.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.13.m13.1c">900k</annotation></semantics></math> steps (until convergence). We also train BigVGAN vocoder <cite class="ltx_cite ltx_citemacro_citep">(gil Lee et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> and use it during inference to generate the speech from the output melspectrograms.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We present our approach’s quantitative results and comparisons with the existing methods. As automatic speech metrics are imperfect, we also show MOS scores using human evaluation. Finally, we demonstrate a real-world application of lip-to-speech for the first time by voicing the silent lip movements of an ALS patient.</p>
</div>
<section id="S4.SS1" class="ltx_subsection ltx_pruned_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Quantitative Evaluations</h3>

<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Metrics:</span>
We measure the quality of the generated speech using the standard speech metrics: Perceptual Evaluation of Speech Quality (PESQ), Short-Time Objective Intelligibility measure (STOI), and its extended version (ESTOI). PESQ measures the clarity and overall perceptual quality of speech and STOI and ESTOI measure the intelligibility of speech. Further, as discussed previously, it is very crucial to generate speech that is in sync with the input video. We use the lip-sync metrics, Lip-Sync Error - Confidence (LSE-C) and Lip-Sync Error - Distance (LSE-D) <cite class="ltx_cite ltx_citemacro_citep">(Chung and Zisserman, <a href="#bib.bib10" title="" class="ltx_ref">2016b</a>)</cite> to evaluate whether the output speech matches the input lip movements. We use the public implementations of all the above metrics for a fair comparison.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection ltx_pruned_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Speech Synthesis in Constrained Settings</h4>

<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p"><span id="S4.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Comparisons:</span>
To evaluate lip-to-speech methods on the constrained single-speaker TCD-TIMIT dataset, we compare four existing approaches: (i) GAN-based <cite class="ltx_cite ltx_citemacro_citep">(Vougioukas et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite>, (ii) Lip2Wav <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite>, (iii) VAE-GAN <cite class="ltx_cite ltx_citemacro_citep">(Hegde et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>, and (iv) VCA-GAN <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>. We adopt the same settings as Lip2Wav <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite> and report the scores <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite> and VCA-GAN <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>. We use the Wav2Lip <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020b</a>)</cite> repository to compute the LSE-C and LSE-D scores for each method. We have excluded the metrics for a particular model that were not mentioned in the original papers or for which no publicly available pre-trained checkpoint exists.</p>
</div>
<div id="S4.SS1.SSS1.p4" class="ltx_para">
<p id="S4.SS1.SSS1.p4.1" class="ltx_p"><span id="S4.SS1.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Results:</span>
Table <a href="#S4.T1" title="Table 1 ‣ 4.1.1. Speech Synthesis in Constrained Settings ‣ 4.1. Quantitative Evaluations ‣ 4. Experiments ‣ Towards Accurate Lip-to-Speech Synthesis in-the-Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> contains the results on the TCD-TIMIT dataset. We observe that our approach achieves comparable results to previous methods in constrained settings with minimal data of only <math id="S4.SS1.SSS1.p4.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS1.SSS1.p4.1.m1.1a"><mn id="S4.SS1.SSS1.p4.1.m1.1.1" xref="S4.SS1.SSS1.p4.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p4.1.m1.1b"><cn type="integer" id="S4.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p4.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p4.1.m1.1c">3</annotation></semantics></math> speakers. However, the significant benefits of our approach can be seen in unconstrained settings, which we describe below.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:50%;"><span class="ltx_tag ltx_tag_table">Table 1. </span>We compare against state-of-the-art methods on several standard multi-speaker benchmarks using standard metrics. The generated outputs from our model are most natural (PESQ), most accurate (STOI, ESTOI) and in perfect sync with the video input (LSE-C, LSE-D) in the in-the-wild videos of LRW <cite class="ltx_cite ltx_citemacro_citep">(Chung and Zisserman, <a href="#bib.bib9" title="" class="ltx_ref">2016a</a>)</cite>, LRS2 <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> and LRS3 <cite class="ltx_cite ltx_citemacro_citep">(Afouras et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018b</a>)</cite>.</figcaption>
<div id="S4.T1.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:420.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.6pt,32.6pt) scale(0.865711233725961,0.865711233725961) ;">
<table id="S4.T1.5.5" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.5.5.5" class="ltx_tr">
<th id="S4.T1.5.5.5.6" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.5.5.5.6.1" class="ltx_text ltx_font_bold" style="font-size:50%;">Dataset</span></th>
<th id="S4.T1.5.5.5.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.5.5.5.7.1" class="ltx_text ltx_font_bold" style="font-size:50%;">Method</span></th>
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">PESQ<math id="S4.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:50%;">STOI<math id="S4.T1.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T1.2.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.1.m1.1b"><ci id="S4.T1.2.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:50%;">ESTOI<math id="S4.T1.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S4.T1.3.3.3.3.1.m1.1.1" xref="S4.T1.3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.1.m1.1b"><ci id="S4.T1.3.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S4.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.4.4.4.1" class="ltx_text ltx_font_bold" style="font-size:50%;">LSE-C<math id="S4.T1.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.4.4.4.4.1.m1.1a"><mo stretchy="false" id="S4.T1.4.4.4.4.1.m1.1.1" xref="S4.T1.4.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.1.m1.1b"><ci id="S4.T1.4.4.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S4.T1.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:50%;">LSE-D<math id="S4.T1.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.5.5.5.5.1.m1.1a"><mo stretchy="false" id="S4.T1.5.5.5.5.1.m1.1.1" xref="S4.T1.5.5.5.5.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.5.1.m1.1b"><ci id="S4.T1.5.5.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.5.5.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.5.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
</tr>
<tr id="S4.T1.5.5.6.1" class="ltx_tr">
<th id="S4.T1.5.5.6.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="5"><span id="S4.T1.5.5.6.1.1.1" class="ltx_text" style="font-size:50%;">TCD-TIMIT <cite class="ltx_cite ltx_citemacro_citep">(Harte and Gillen, <a href="#bib.bib16" title="" class="ltx_ref">2015</a>)</cite></span></th>
<th id="S4.T1.5.5.6.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T1.5.5.6.1.2.1" class="ltx_text" style="font-size:50%;">GAN-based </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.6.1.2.2.1" class="ltx_text" style="font-size:50%;">(</span>Vougioukas et al<span class="ltx_text">.</span><span id="S4.T1.5.5.6.1.2.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib37" title="" class="ltx_ref">2019</a><span id="S4.T1.5.5.6.1.2.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.6.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.6.1.3.1" class="ltx_text" style="font-size:50%;">1.22</span></td>
<td id="S4.T1.5.5.6.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.6.1.4.1" class="ltx_text" style="font-size:50%;">0.51</span></td>
<td id="S4.T1.5.5.6.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.6.1.5.1" class="ltx_text" style="font-size:50%;">0.32</span></td>
<td id="S4.T1.5.5.6.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.6.1.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T1.5.5.6.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.6.1.7.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T1.5.5.7.2" class="ltx_tr">
<th id="S4.T1.5.5.7.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.7.2.1.1" class="ltx_text" style="font-size:50%;">Lip2Wav </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.7.2.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Prajwal et al<span class="ltx_text">.</span><span id="S4.T1.5.5.7.2.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib28" title="" class="ltx_ref">2020a</a><span id="S4.T1.5.5.7.2.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.7.2.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.7.2.2.1" class="ltx_text" style="font-size:50%;">1.35</span></td>
<td id="S4.T1.5.5.7.2.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.7.2.3.1" class="ltx_text" style="font-size:50%;">0.56</span></td>
<td id="S4.T1.5.5.7.2.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.7.2.4.1" class="ltx_text" style="font-size:50%;">0.36</span></td>
<td id="S4.T1.5.5.7.2.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.7.2.5.1" class="ltx_text" style="font-size:50%;">6.610</span></td>
<td id="S4.T1.5.5.7.2.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.7.2.6.1" class="ltx_text" style="font-size:50%;">7.815</span></td>
</tr>
<tr id="S4.T1.5.5.8.3" class="ltx_tr">
<th id="S4.T1.5.5.8.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.8.3.1.1" class="ltx_text" style="font-size:50%;">VCA-GAN </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.8.3.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Kim et al<span class="ltx_text">.</span><span id="S4.T1.5.5.8.3.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib21" title="" class="ltx_ref">2022</a><span id="S4.T1.5.5.8.3.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.8.3.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.8.3.2.1" class="ltx_text ltx_font_bold" style="font-size:50%;">1.43</span></td>
<td id="S4.T1.5.5.8.3.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.8.3.3.1" class="ltx_text" style="font-size:50%;">0.58</span></td>
<td id="S4.T1.5.5.8.3.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.8.3.4.1" class="ltx_text" style="font-size:50%;">0.40</span></td>
<td id="S4.T1.5.5.8.3.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.8.3.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T1.5.5.8.3.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.8.3.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T1.5.5.9.4" class="ltx_tr">
<th id="S4.T1.5.5.9.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.9.4.1.1" class="ltx_text" style="font-size:50%;">VAE-GAN </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.9.4.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Hegde et al<span class="ltx_text">.</span><span id="S4.T1.5.5.9.4.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib18" title="" class="ltx_ref">2022</a><span id="S4.T1.5.5.9.4.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.9.4.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.9.4.2.1" class="ltx_text" style="font-size:50%;">1.35</span></td>
<td id="S4.T1.5.5.9.4.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.9.4.3.1" class="ltx_text" style="font-size:50%;">0.55</span></td>
<td id="S4.T1.5.5.9.4.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.9.4.4.1" class="ltx_text" style="font-size:50%;">0.35</span></td>
<td id="S4.T1.5.5.9.4.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.9.4.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T1.5.5.9.4.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.9.4.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T1.5.5.10.5" class="ltx_tr">
<th id="S4.T1.5.5.10.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.5.5.10.5.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">Ours</span></th>
<td id="S4.T1.5.5.10.5.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.10.5.2.1" class="ltx_text" style="font-size:50%;">1.34</span></td>
<td id="S4.T1.5.5.10.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.10.5.3.1" class="ltx_text ltx_font_bold" style="font-size:50%;">0.61</span></td>
<td id="S4.T1.5.5.10.5.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.10.5.4.1" class="ltx_text ltx_font_bold" style="font-size:50%;">0.42</span></td>
<td id="S4.T1.5.5.10.5.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.10.5.5.1" class="ltx_text ltx_font_bold" style="font-size:50%;">6.623</span></td>
<td id="S4.T1.5.5.10.5.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.10.5.6.1" class="ltx_text ltx_font_bold" style="font-size:50%;">6.901</span></td>
</tr>
<tr id="S4.T1.5.5.11.6" class="ltx_tr">
<th id="S4.T1.5.5.11.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="8"><span id="S4.T1.5.5.11.6.1.1" class="ltx_text" style="font-size:50%;">LRW <cite class="ltx_cite ltx_citemacro_citep">(Chung and Zisserman, <a href="#bib.bib9" title="" class="ltx_ref">2016a</a>)</cite></span></th>
<th id="S4.T1.5.5.11.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">
<span id="S4.T1.5.5.11.6.2.1" class="ltx_text" style="font-size:50%;">GAN-based </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.11.6.2.2.1" class="ltx_text" style="font-size:50%;">(</span>Vougioukas et al<span class="ltx_text">.</span><span id="S4.T1.5.5.11.6.2.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib37" title="" class="ltx_ref">2019</a><span id="S4.T1.5.5.11.6.2.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.11.6.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.5.5.11.6.3.1" class="ltx_text" style="font-size:50%;">0.72</span></td>
<td id="S4.T1.5.5.11.6.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.5.5.11.6.4.1" class="ltx_text" style="font-size:50%;">0.10</span></td>
<td id="S4.T1.5.5.11.6.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.5.5.11.6.5.1" class="ltx_text" style="font-size:50%;">0.02</span></td>
<td id="S4.T1.5.5.11.6.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.5.5.11.6.6.1" class="ltx_text" style="font-size:50%;">1.983</span></td>
<td id="S4.T1.5.5.11.6.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.5.5.11.6.7.1" class="ltx_text" style="font-size:50%;">9.426</span></td>
</tr>
<tr id="S4.T1.5.5.12.7" class="ltx_tr">
<th id="S4.T1.5.5.12.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.12.7.1.1" class="ltx_text" style="font-size:50%;">Lip2Wav </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.12.7.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Prajwal et al<span class="ltx_text">.</span><span id="S4.T1.5.5.12.7.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib28" title="" class="ltx_ref">2020a</a><span id="S4.T1.5.5.12.7.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.12.7.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.12.7.2.1" class="ltx_text" style="font-size:50%;">1.19</span></td>
<td id="S4.T1.5.5.12.7.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.12.7.3.1" class="ltx_text" style="font-size:50%;">0.54</span></td>
<td id="S4.T1.5.5.12.7.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.12.7.4.1" class="ltx_text" style="font-size:50%;">0.34</span></td>
<td id="S4.T1.5.5.12.7.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.12.7.5.1" class="ltx_text" style="font-size:50%;">2.526</span></td>
<td id="S4.T1.5.5.12.7.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.12.7.6.1" class="ltx_text" style="font-size:50%;">8.286</span></td>
</tr>
<tr id="S4.T1.5.5.13.8" class="ltx_tr">
<th id="S4.T1.5.5.13.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.13.8.1.1" class="ltx_text" style="font-size:50%;">VAE-GAN </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.13.8.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Hegde et al<span class="ltx_text">.</span><span id="S4.T1.5.5.13.8.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib18" title="" class="ltx_ref">2022</a><span id="S4.T1.5.5.13.8.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.13.8.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.13.8.2.1" class="ltx_text" style="font-size:50%;">0.78</span></td>
<td id="S4.T1.5.5.13.8.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.13.8.3.1" class="ltx_text" style="font-size:50%;">0.15</span></td>
<td id="S4.T1.5.5.13.8.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.13.8.4.1" class="ltx_text" style="font-size:50%;">0.03</span></td>
<td id="S4.T1.5.5.13.8.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.13.8.5.1" class="ltx_text" style="font-size:50%;">2.538</span></td>
<td id="S4.T1.5.5.13.8.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.13.8.6.1" class="ltx_text" style="font-size:50%;">8.173</span></td>
</tr>
<tr id="S4.T1.5.5.14.9" class="ltx_tr">
<th id="S4.T1.5.5.14.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.14.9.1.1" class="ltx_text" style="font-size:50%;">VCA-GAN </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.14.9.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Kim et al<span class="ltx_text">.</span><span id="S4.T1.5.5.14.9.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib21" title="" class="ltx_ref">2022</a><span id="S4.T1.5.5.14.9.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.14.9.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.14.9.2.1" class="ltx_text" style="font-size:50%;">1.33</span></td>
<td id="S4.T1.5.5.14.9.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.14.9.3.1" class="ltx_text" style="font-size:50%;">0.56</span></td>
<td id="S4.T1.5.5.14.9.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.14.9.4.1" class="ltx_text" style="font-size:50%;">0.36</span></td>
<td id="S4.T1.5.5.14.9.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.14.9.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T1.5.5.14.9.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.14.9.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T1.5.5.15.10" class="ltx_tr">
<th id="S4.T1.5.5.15.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.15.10.1.1" class="ltx_text" style="font-size:50%;">SVTS </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.15.10.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Mira et al<span class="ltx_text">.</span><span id="S4.T1.5.5.15.10.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib24" title="" class="ltx_ref">2022</a><span id="S4.T1.5.5.15.10.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.15.10.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.15.10.2.1" class="ltx_text" style="font-size:50%;">1.49</span></td>
<td id="S4.T1.5.5.15.10.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.15.10.3.1" class="ltx_text" style="font-size:50%;">0.64</span></td>
<td id="S4.T1.5.5.15.10.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.15.10.4.1" class="ltx_text" style="font-size:50%;">0.48</span></td>
<td id="S4.T1.5.5.15.10.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.15.10.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T1.5.5.15.10.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.15.10.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T1.5.5.16.11" class="ltx_tr">
<th id="S4.T1.5.5.16.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.16.11.1.1" class="ltx_text" style="font-size:50%;">Multi-task L2S </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.16.11.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Kim et al<span class="ltx_text">.</span><span id="S4.T1.5.5.16.11.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib22" title="" class="ltx_ref">2023</a><span id="S4.T1.5.5.16.11.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.16.11.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.16.11.2.1" class="ltx_text" style="font-size:50%;">1.56</span></td>
<td id="S4.T1.5.5.16.11.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.16.11.3.1" class="ltx_text" style="font-size:50%;">0.64</span></td>
<td id="S4.T1.5.5.16.11.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.16.11.4.1" class="ltx_text" style="font-size:50%;">0.47</span></td>
<td id="S4.T1.5.5.16.11.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.16.11.5.1" class="ltx_text" style="font-size:50%;">4.876</span></td>
<td id="S4.T1.5.5.16.11.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.16.11.6.1" class="ltx_text" style="font-size:50%;">8.102</span></td>
</tr>
<tr id="S4.T1.5.5.17.12" class="ltx_tr">
<th id="S4.T1.5.5.17.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.5.5.17.12.1.1" class="ltx_text" style="font-size:50%;">Lip-to-Text + TTS baseline</span></th>
<td id="S4.T1.5.5.17.12.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.17.12.2.1" class="ltx_text" style="font-size:50%;">0.69</span></td>
<td id="S4.T1.5.5.17.12.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.17.12.3.1" class="ltx_text" style="font-size:50%;">0.10</span></td>
<td id="S4.T1.5.5.17.12.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.17.12.4.1" class="ltx_text" style="font-size:50%;">0.01</span></td>
<td id="S4.T1.5.5.17.12.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.17.12.5.1" class="ltx_text" style="font-size:50%;">1.993</span></td>
<td id="S4.T1.5.5.17.12.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.17.12.6.1" class="ltx_text" style="font-size:50%;">12.872</span></td>
</tr>
<tr id="S4.T1.5.5.18.13" class="ltx_tr">
<th id="S4.T1.5.5.18.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.5.5.18.13.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">Ours</span></th>
<td id="S4.T1.5.5.18.13.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.18.13.2.1" class="ltx_text ltx_font_bold" style="font-size:50%;">1.61</span></td>
<td id="S4.T1.5.5.18.13.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.18.13.3.1" class="ltx_text ltx_font_bold" style="font-size:50%;">0.71</span></td>
<td id="S4.T1.5.5.18.13.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.18.13.4.1" class="ltx_text ltx_font_bold" style="font-size:50%;">0.56</span></td>
<td id="S4.T1.5.5.18.13.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.18.13.5.1" class="ltx_text ltx_font_bold" style="font-size:50%;">6.812</span></td>
<td id="S4.T1.5.5.18.13.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.18.13.6.1" class="ltx_text ltx_font_bold" style="font-size:50%;">6.974</span></td>
</tr>
<tr id="S4.T1.5.5.19.14" class="ltx_tr">
<th id="S4.T1.5.5.19.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="7"><span id="S4.T1.5.5.19.14.1.1" class="ltx_text" style="font-size:50%;">LRS2 <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite></span></th>
<th id="S4.T1.5.5.19.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T1.5.5.19.14.2.1" class="ltx_text" style="font-size:50%;">Lip2Wav </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.19.14.2.2.1" class="ltx_text" style="font-size:50%;">(</span>Prajwal et al<span class="ltx_text">.</span><span id="S4.T1.5.5.19.14.2.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib28" title="" class="ltx_ref">2020a</a><span id="S4.T1.5.5.19.14.2.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.19.14.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.19.14.3.1" class="ltx_text" style="font-size:50%;">0.58</span></td>
<td id="S4.T1.5.5.19.14.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.19.14.4.1" class="ltx_text" style="font-size:50%;">0.28</span></td>
<td id="S4.T1.5.5.19.14.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.19.14.5.1" class="ltx_text" style="font-size:50%;">0.11</span></td>
<td id="S4.T1.5.5.19.14.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.19.14.6.1" class="ltx_text" style="font-size:50%;">1.874</span></td>
<td id="S4.T1.5.5.19.14.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.19.14.7.1" class="ltx_text" style="font-size:50%;">11.48</span></td>
</tr>
<tr id="S4.T1.5.5.20.15" class="ltx_tr">
<th id="S4.T1.5.5.20.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.20.15.1.1" class="ltx_text" style="font-size:50%;">VAE-GAN </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.20.15.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Vougioukas et al<span class="ltx_text">.</span><span id="S4.T1.5.5.20.15.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib37" title="" class="ltx_ref">2019</a><span id="S4.T1.5.5.20.15.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.20.15.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.20.15.2.1" class="ltx_text" style="font-size:50%;">0.60</span></td>
<td id="S4.T1.5.5.20.15.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.20.15.3.1" class="ltx_text" style="font-size:50%;">0.34</span></td>
<td id="S4.T1.5.5.20.15.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.20.15.4.1" class="ltx_text" style="font-size:50%;">0.17</span></td>
<td id="S4.T1.5.5.20.15.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.20.15.5.1" class="ltx_text" style="font-size:50%;">2.507</span></td>
<td id="S4.T1.5.5.20.15.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.20.15.6.1" class="ltx_text" style="font-size:50%;">8.155</span></td>
</tr>
<tr id="S4.T1.5.5.21.16" class="ltx_tr">
<th id="S4.T1.5.5.21.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.21.16.1.1" class="ltx_text" style="font-size:50%;">VCA-GAN </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.21.16.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Kim et al<span class="ltx_text">.</span><span id="S4.T1.5.5.21.16.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib21" title="" class="ltx_ref">2022</a><span id="S4.T1.5.5.21.16.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.21.16.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.21.16.2.1" class="ltx_text" style="font-size:50%;">1.24</span></td>
<td id="S4.T1.5.5.21.16.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.21.16.3.1" class="ltx_text" style="font-size:50%;">0.40</span></td>
<td id="S4.T1.5.5.21.16.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.21.16.4.1" class="ltx_text" style="font-size:50%;">0.13</span></td>
<td id="S4.T1.5.5.21.16.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.21.16.5.1" class="ltx_text" style="font-size:50%;">4.016</span></td>
<td id="S4.T1.5.5.21.16.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.21.16.6.1" class="ltx_text" style="font-size:50%;">7.914</span></td>
</tr>
<tr id="S4.T1.5.5.22.17" class="ltx_tr">
<th id="S4.T1.5.5.22.17.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.22.17.1.1" class="ltx_text" style="font-size:50%;">SVTS </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.22.17.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Mira et al<span class="ltx_text">.</span><span id="S4.T1.5.5.22.17.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib24" title="" class="ltx_ref">2022</a><span id="S4.T1.5.5.22.17.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.22.17.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.22.17.2.1" class="ltx_text" style="font-size:50%;">1.34</span></td>
<td id="S4.T1.5.5.22.17.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.22.17.3.1" class="ltx_text" style="font-size:50%;">0.49</span></td>
<td id="S4.T1.5.5.22.17.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.22.17.4.1" class="ltx_text" style="font-size:50%;">0.29</span></td>
<td id="S4.T1.5.5.22.17.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.22.17.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T1.5.5.22.17.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.22.17.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T1.5.5.23.18" class="ltx_tr">
<th id="S4.T1.5.5.23.18.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.23.18.1.1" class="ltx_text" style="font-size:50%;">Multi-task L2S </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.23.18.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Kim et al<span class="ltx_text">.</span><span id="S4.T1.5.5.23.18.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib22" title="" class="ltx_ref">2023</a><span id="S4.T1.5.5.23.18.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.23.18.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.23.18.2.1" class="ltx_text" style="font-size:50%;">1.36</span></td>
<td id="S4.T1.5.5.23.18.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.23.18.3.1" class="ltx_text" style="font-size:50%;">0.52</span></td>
<td id="S4.T1.5.5.23.18.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.23.18.4.1" class="ltx_text" style="font-size:50%;">0.34</span></td>
<td id="S4.T1.5.5.23.18.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.23.18.5.1" class="ltx_text" style="font-size:50%;">4.001</span></td>
<td id="S4.T1.5.5.23.18.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.23.18.6.1" class="ltx_text" style="font-size:50%;">8.192</span></td>
</tr>
<tr id="S4.T1.5.5.24.19" class="ltx_tr">
<th id="S4.T1.5.5.24.19.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.5.5.24.19.1.1" class="ltx_text" style="font-size:50%;">Lip-to-Text + TTS baseline</span></th>
<td id="S4.T1.5.5.24.19.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.24.19.2.1" class="ltx_text" style="font-size:50%;">0.53</span></td>
<td id="S4.T1.5.5.24.19.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.24.19.3.1" class="ltx_text" style="font-size:50%;">0.19</span></td>
<td id="S4.T1.5.5.24.19.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.24.19.4.1" class="ltx_text" style="font-size:50%;">0.02</span></td>
<td id="S4.T1.5.5.24.19.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.24.19.5.1" class="ltx_text" style="font-size:50%;">2.013</span></td>
<td id="S4.T1.5.5.24.19.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.24.19.6.1" class="ltx_text" style="font-size:50%;">15.891</span></td>
</tr>
<tr id="S4.T1.5.5.25.20" class="ltx_tr">
<th id="S4.T1.5.5.25.20.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.5.5.25.20.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">Ours</span></th>
<td id="S4.T1.5.5.25.20.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.25.20.2.1" class="ltx_text ltx_font_bold" style="font-size:50%;">1.47</span></td>
<td id="S4.T1.5.5.25.20.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.25.20.3.1" class="ltx_text ltx_font_bold" style="font-size:50%;">0.65</span></td>
<td id="S4.T1.5.5.25.20.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.25.20.4.1" class="ltx_text ltx_font_bold" style="font-size:50%;">0.47</span></td>
<td id="S4.T1.5.5.25.20.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.25.20.5.1" class="ltx_text ltx_font_bold" style="font-size:50%;">8.083</span></td>
<td id="S4.T1.5.5.25.20.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.25.20.6.1" class="ltx_text ltx_font_bold" style="font-size:50%;">6.586</span></td>
</tr>
<tr id="S4.T1.5.5.26.21" class="ltx_tr">
<th id="S4.T1.5.5.26.21.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" rowspan="6"><span id="S4.T1.5.5.26.21.1.1" class="ltx_text" style="font-size:50%;">LRS3 <cite class="ltx_cite ltx_citemacro_citep">(Afouras et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018b</a>)</cite></span></th>
<th id="S4.T1.5.5.26.21.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T1.5.5.26.21.2.1" class="ltx_text" style="font-size:50%;">VAE-GAN </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.26.21.2.2.1" class="ltx_text" style="font-size:50%;">(</span>Vougioukas et al<span class="ltx_text">.</span><span id="S4.T1.5.5.26.21.2.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib37" title="" class="ltx_ref">2019</a><span id="S4.T1.5.5.26.21.2.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.26.21.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.26.21.3.1" class="ltx_text" style="font-size:50%;">0.51</span></td>
<td id="S4.T1.5.5.26.21.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.26.21.4.1" class="ltx_text" style="font-size:50%;">0.30</span></td>
<td id="S4.T1.5.5.26.21.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.26.21.5.1" class="ltx_text" style="font-size:50%;">0.15</span></td>
<td id="S4.T1.5.5.26.21.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.26.21.6.1" class="ltx_text" style="font-size:50%;">2.063</span></td>
<td id="S4.T1.5.5.26.21.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.26.21.7.1" class="ltx_text" style="font-size:50%;">8.256</span></td>
</tr>
<tr id="S4.T1.5.5.27.22" class="ltx_tr">
<th id="S4.T1.5.5.27.22.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.27.22.1.1" class="ltx_text" style="font-size:50%;">VCA-GAN </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.27.22.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Kim et al<span class="ltx_text">.</span><span id="S4.T1.5.5.27.22.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib21" title="" class="ltx_ref">2022</a><span id="S4.T1.5.5.27.22.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.27.22.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.27.22.2.1" class="ltx_text" style="font-size:50%;">1.23</span></td>
<td id="S4.T1.5.5.27.22.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.27.22.3.1" class="ltx_text" style="font-size:50%;">0.47</span></td>
<td id="S4.T1.5.5.27.22.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.27.22.4.1" class="ltx_text" style="font-size:50%;">0.20</span></td>
<td id="S4.T1.5.5.27.22.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.27.22.5.1" class="ltx_text" style="font-size:50%;">3.905</span></td>
<td id="S4.T1.5.5.27.22.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.27.22.6.1" class="ltx_text" style="font-size:50%;">8.392</span></td>
</tr>
<tr id="S4.T1.5.5.28.23" class="ltx_tr">
<th id="S4.T1.5.5.28.23.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.28.23.1.1" class="ltx_text" style="font-size:50%;">SVTS </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.28.23.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Mira et al<span class="ltx_text">.</span><span id="S4.T1.5.5.28.23.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib24" title="" class="ltx_ref">2022</a><span id="S4.T1.5.5.28.23.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.28.23.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.28.23.2.1" class="ltx_text" style="font-size:50%;">1.25</span></td>
<td id="S4.T1.5.5.28.23.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.28.23.3.1" class="ltx_text" style="font-size:50%;">0.50</span></td>
<td id="S4.T1.5.5.28.23.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.28.23.4.1" class="ltx_text" style="font-size:50%;">0.27</span></td>
<td id="S4.T1.5.5.28.23.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.28.23.5.1" class="ltx_text" style="font-size:50%;">-</span></td>
<td id="S4.T1.5.5.28.23.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.28.23.6.1" class="ltx_text" style="font-size:50%;">-</span></td>
</tr>
<tr id="S4.T1.5.5.29.24" class="ltx_tr">
<th id="S4.T1.5.5.29.24.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.5.5.29.24.1.1" class="ltx_text" style="font-size:50%;">Multi-task L2S </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.5.5.29.24.1.2.1" class="ltx_text" style="font-size:50%;">(</span>Kim et al<span class="ltx_text">.</span><span id="S4.T1.5.5.29.24.1.3.2.1.1" class="ltx_text" style="font-size:50%;">, </span><a href="#bib.bib22" title="" class="ltx_ref">2023</a><span id="S4.T1.5.5.29.24.1.4.3" class="ltx_text" style="font-size:50%;">)</span></cite>
</th>
<td id="S4.T1.5.5.29.24.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.29.24.2.1" class="ltx_text" style="font-size:50%;">1.31</span></td>
<td id="S4.T1.5.5.29.24.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.29.24.3.1" class="ltx_text" style="font-size:50%;">0.48</span></td>
<td id="S4.T1.5.5.29.24.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.29.24.4.1" class="ltx_text" style="font-size:50%;">0.26</span></td>
<td id="S4.T1.5.5.29.24.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.29.24.5.1" class="ltx_text" style="font-size:50%;">3.876</span></td>
<td id="S4.T1.5.5.29.24.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.29.24.6.1" class="ltx_text" style="font-size:50%;">8.677</span></td>
</tr>
<tr id="S4.T1.5.5.30.25" class="ltx_tr">
<th id="S4.T1.5.5.30.25.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.5.5.30.25.1.1" class="ltx_text" style="font-size:50%;">Lip-to-Text + TTS baseline</span></th>
<td id="S4.T1.5.5.30.25.2" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.30.25.2.1" class="ltx_text" style="font-size:50%;">0.42</span></td>
<td id="S4.T1.5.5.30.25.3" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.30.25.3.1" class="ltx_text" style="font-size:50%;">0.16</span></td>
<td id="S4.T1.5.5.30.25.4" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.30.25.4.1" class="ltx_text" style="font-size:50%;">0.01</span></td>
<td id="S4.T1.5.5.30.25.5" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.30.25.5.1" class="ltx_text" style="font-size:50%;">1.771</span></td>
<td id="S4.T1.5.5.30.25.6" class="ltx_td ltx_align_center"><span id="S4.T1.5.5.30.25.6.1" class="ltx_text" style="font-size:50%;">17.882</span></td>
</tr>
<tr id="S4.T1.5.5.31.26" class="ltx_tr">
<th id="S4.T1.5.5.31.26.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S4.T1.5.5.31.26.1.1" class="ltx_text ltx_font_bold" style="font-size:50%;">Ours</span></th>
<td id="S4.T1.5.5.31.26.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.5.5.31.26.2.1" class="ltx_text ltx_font_bold" style="font-size:50%;">1.39</span></td>
<td id="S4.T1.5.5.31.26.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.5.5.31.26.3.1" class="ltx_text ltx_font_bold" style="font-size:50%;">0.58</span></td>
<td id="S4.T1.5.5.31.26.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.5.5.31.26.4.1" class="ltx_text ltx_font_bold" style="font-size:50%;">0.37</span></td>
<td id="S4.T1.5.5.31.26.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.5.5.31.26.5.1" class="ltx_text ltx_font_bold" style="font-size:50%;">7.886</span></td>
<td id="S4.T1.5.5.31.26.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.5.5.31.26.6.1" class="ltx_text ltx_font_bold" style="font-size:50%;">6.850</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection ltx_pruned_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Speech Synthesis in Unconstrained Settings</h4>

<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p"><span id="S4.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Comparisons:</span>
In order to assess the performance of lip-to-speech methods in unconstrained scenarios, we employ three datasets: word-level LRW <cite class="ltx_cite ltx_citemacro_citep">(Chung and Zisserman, <a href="#bib.bib9" title="" class="ltx_ref">2016a</a>)</cite>, sentence-level LRS2 <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite>, and LRS3 <cite class="ltx_cite ltx_citemacro_citep">(Afouras et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018b</a>)</cite>. While the authors of VAE-GAN <cite class="ltx_cite ltx_citemacro_citep">(Hegde et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite> have re-trained the GAN-based <cite class="ltx_cite ltx_citemacro_citep">(Vougioukas et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite> and Lip2Wav <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite> models in a multi-speaker context, we present the scores from their original study for comparison. For VCA-GAN <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>, SVTS <cite class="ltx_cite ltx_citemacro_citep">(Mira et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>, and Multi-task Lip-to-Speech synthesis <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>, we adopt the speech metric (PESQ, STOI and ESTOI) scores from <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>. Further, we use publicly accessible pre-trained checkpoints for VCA-GAN<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/ms-dot-k/Visual-Context-Attentional-GAN" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ms-dot-k/Visual-Context-Attentional-GAN</a></span></span></span> and Multitask-L2S<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/ms-dot-k/Lip-to-Speech-Synthesis-in-the-Wild" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ms-dot-k/Lip-to-Speech-Synthesis-in-the-Wild</a></span></span></span> to generate speech on LRS2 and LRS3 test sets for the former, and LRS2, LRS3, and LRW test sets for the latter. We utilize these generations to compute the LSE metrics for both techniques, wherever applicable. Lastly, we include results for a baseline approach that leverages lip-to-text conversion followed by multi-speaker TTS without a visual stream in the TTS model. Our evaluation does not include all metrics that were not originally reported in the papers or for which no pre-trained model is publicly available.</p>
</div>
<div id="S4.SS1.SSS2.p4" class="ltx_para">
<p id="S4.SS1.SSS2.p4.1" class="ltx_p"><span id="S4.SS1.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Results:</span>
We present the results on the challenging LRW, LRS2, and LRS3 datasets in Table <a href="#S4.T1" title="Table 1 ‣ 4.1.1. Speech Synthesis in Constrained Settings ‣ 4.1. Quantitative Evaluations ‣ 4. Experiments ‣ Towards Accurate Lip-to-Speech Synthesis in-the-Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Our model consistently outperforms the existing methods by a significant margin on all these datasets. Since GAN-based <cite class="ltx_cite ltx_citemacro_citep">(Vougioukas et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite> model was proposed to work for constrained laboratory recorded datasets, we can observe that extending this model in unconstrained settings does not yield satisfactory results. Lip2Wav <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite> performs decently on the word-level LRW dataset; however, it fails to learn the audio-visual alignment on the LRS2 dataset, thus leading to very poor performance. We discard this model for further comparison on the LRS3 dataset. VAE-GAN <cite class="ltx_cite ltx_citemacro_citep">(Hegde et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>, VCA-GAN <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>, SVTS <cite class="ltx_cite ltx_citemacro_citep">(Mira et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite> and Multitask-L2S <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite> generate speech that is in-sync with the input video; however, they fail to synthesize accurate content. The quality of the generated speech is often non-intelligible and leads to lower scores in speech quality metrics. The Lip-to-Text <math id="S4.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S4.SS1.SSS2.p4.1.m1.1a"><mo id="S4.SS1.SSS2.p4.1.m1.1.1" xref="S4.SS1.SSS2.p4.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.1.m1.1b"><plus id="S4.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.1.m1.1c">+</annotation></semantics></math> TTS baseline model is on the opposite spectrum, where the model generates the content well but fails to capture the lip-sync, mainly because the model cannot infer the speed, prosody, and accents of speakers just from the text input. Our model, on the other hand, is capable of generating both the actual spoken content as well as maintaining precise lip synchronization. As we can see from the table, we outperform the previous methods in all the speech quality metrics, indicating the robustness and superiority of our approach. In Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1.2. Speech Synthesis in Unconstrained Settings ‣ 4.1. Quantitative Evaluations ‣ 4. Experiments ‣ Towards Accurate Lip-to-Speech Synthesis in-the-Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we depict how our model temporally aligns video and text sequences in the process of generating speech. We encourage the reader to view our demo video comprising multiple qualitative samples and comparisons.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2403.01087/assets/images/attn_plot.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="419" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>We visualize the video-text alignment from the scaled dot product attention step of our model. We observe that the model learns a strong monotonic near-diagonal attention, as expected.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Human Evaluations</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.7" class="ltx_p">To evaluate the applicability of our method in real-world scenarios, we conduct subjective human evaluations. We ask <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="integer" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">25</annotation></semantics></math> volunteers to assess the quality of speech generations. The participant group has an almost equal male-female ratio, spanning an age group of <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="20-45" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">20</mn><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">−</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">45</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><minus id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></minus><cn type="integer" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">20</cn><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">45</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">20-45</annotation></semantics></math> years. We randomly select <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mn id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><cn type="integer" id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">10</annotation></semantics></math> long sentences (<math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mn id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><cn type="integer" id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">10</annotation></semantics></math> seconds or longer) from the test set of LRS3 <cite class="ltx_cite ltx_citemacro_citep">(Afouras et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018b</a>)</cite> and present the results from different methods to the participants. We ask them to rate the samples on a scale of <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="1-5" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mrow id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mn id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">1</mn><mo id="S4.SS2.p1.5.m5.1.1.1" xref="S4.SS2.p1.5.m5.1.1.1.cmml">−</mo><mn id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><minus id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1.1"></minus><cn type="integer" id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">1</cn><cn type="integer" id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">1-5</annotation></semantics></math> based on the following criteria: (A) Intelligibility (is the speech meaningful?), (B) Content clarity (are the words clear?), (C) Sync Accuracy, (D) Overall perceptual quality of the talking head video <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mo id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><plus id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">+</annotation></semantics></math> audio. We report the mean opinion scores in Table <a href="#S4.T2" title="Table 2 ‣ 4.2. Human Evaluations ‣ 4. Experiments ‣ Towards Accurate Lip-to-Speech Synthesis in-the-Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. On par with the quantitative evaluations, our method is highly rated over other approaches in all the criteria listed above. As expected, the speech intelligibility is slightly rated higher for the Lip-to-Text <math id="S4.SS2.p1.7.m7.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S4.SS2.p1.7.m7.1a"><mo id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><plus id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">+</annotation></semantics></math> TTS baseline. However, the overall perceptual quality for this baseline sharply falls due to the lack of sync between the spoken content and the lip movements. Overall, Table <a href="#S4.T2" title="Table 2 ‣ 4.2. Human Evaluations ‣ 4. Experiments ‣ Towards Accurate Lip-to-Speech Synthesis in-the-Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> clearly signifies that our network is able to generate speech with more clarity, which sounds more natural and is of considerably higher quality.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>(A) Intelligibility, (B) Content clarity, (C) Sync Accuracy, (D) Overall perceptual quality. Our model produces natural and realistic speech outputs that is largely preferred by the users in comparison to other approaches.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Method</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">(A)</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">(B)</th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">(C)</th>
<th id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">(D)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GAN-based <cite class="ltx_cite ltx_citemacro_citep">(Vougioukas et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">2.05</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">1.87</td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">1.99</td>
<td id="S4.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">2.12</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Lip2Wav <cite class="ltx_cite ltx_citemacro_citep">(Prajwal et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2020a</a>)</cite>
</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center">1.01</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center">1.03</td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_center">1.34</td>
<td id="S4.T2.1.3.2.5" class="ltx_td ltx_align_center">1.01</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VAE-GAN <cite class="ltx_cite ltx_citemacro_citep">(Hegde et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center">1.07</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center">1.33</td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_center">2.18</td>
<td id="S4.T2.1.4.3.5" class="ltx_td ltx_align_center">2.57</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VCA-GAN <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center">2.18</td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_center">1.88</td>
<td id="S4.T2.1.5.4.4" class="ltx_td ltx_align_center">2.97</td>
<td id="S4.T2.1.5.4.5" class="ltx_td ltx_align_center">2.54</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<th id="S4.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Multi-task L2S <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_center">2.19</td>
<td id="S4.T2.1.6.5.3" class="ltx_td ltx_align_center">1.85</td>
<td id="S4.T2.1.6.5.4" class="ltx_td ltx_align_center">3.01</td>
<td id="S4.T2.1.6.5.5" class="ltx_td ltx_align_center">2.64</td>
</tr>
<tr id="S4.T2.1.7.6" class="ltx_tr">
<th id="S4.T2.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Lip-to-Text + TTS baseline</th>
<td id="S4.T2.1.7.6.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.6.2.1" class="ltx_text ltx_font_bold">3.61</span></td>
<td id="S4.T2.1.7.6.3" class="ltx_td ltx_align_center">2.87</td>
<td id="S4.T2.1.7.6.4" class="ltx_td ltx_align_center">1.01</td>
<td id="S4.T2.1.7.6.5" class="ltx_td ltx_align_center">2.96</td>
</tr>
<tr id="S4.T2.1.8.7" class="ltx_tr">
<th id="S4.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T2.1.8.7.1.1" class="ltx_text ltx_font_bold">Ours</span></th>
<td id="S4.T2.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b">3.49</td>
<td id="S4.T2.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.8.7.3.1" class="ltx_text ltx_font_bold">3.52</span></td>
<td id="S4.T2.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.8.7.4.1" class="ltx_text ltx_font_bold">3.82</span></td>
<td id="S4.T2.1.8.7.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.8.7.5.1" class="ltx_text ltx_font_bold">3.31</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Applications in Assistive Technology</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Lip-to-Speech synthesis has a host of applications in a world that is becoming increasingly digital. Simple applications such as performing video calls in quiet environments, filling in the audio interruptions due to technical issues, eliminating unwanted background chatter, etc., can be made possible with accurate lip-to-speech. We believe the most significant application of lip-to-speech can be in assistive technologies. It can revolutionize the current assistive systems used to improve the communication ability of people suffering from various disorders affecting their speech. Patients suffering from vocal cord disabilities can mouth words to communicate naturally with the world around them. The synthesized speech can be personalized and also be in sync with the speaker’s lip movements.</p>
</div>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Generating Speech for a Patient suffering from ALS:</h5>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">A recent work <cite class="ltx_cite ltx_citemacro_citep">(Sen et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite> proposed a lip reading technique for a patient suffering from ALS. The patient has feeble vocal cord movements but can mouth words silently. The authors of the paper collected limited amounts of data from the patient and trained a model to recognize words and sentences from his lip movements. As a result of our significant improvement in this task, we can now demonstrate the benefit of using lip-to-speech as a future assistive technology. Through the help of authors of <cite class="ltx_cite ltx_citemacro_citep">(Sen et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>, we evaluate our lip-to-speech system on the patients’ data. Please note that the data was anonymized and was used only for research purposes. We find that the lip-to-text module generates fairly accurate text (WER of <math id="S4.SS3.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\approx 37\%" display="inline"><semantics id="S4.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml"></mi><mo id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml">≈</mo><mrow id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mn id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">37</mn><mo id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.1" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1"><approx id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.2">absent</csymbol><apply id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS3.SSS0.Px1.p1.1.m1.1.1.3.2">37</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px1.p1.1.m1.1c">\approx 37\%</annotation></semantics></math>), and the visual TTS model generates clear speech in sync with the patient’s lip movements. This is the first demonstration of automatic lip-to-speech synthesis for an unseen speaker in an entirely out-of-domain real-world application. Further, we also test our model on the other deaf speakers studied in <cite class="ltx_cite ltx_citemacro_citep">(Sen et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite> and observe accurate performance.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2403.01087/assets/images/als_demo.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="273" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>We demonstrate our model on an ALS patient who cannot voice words but can mouth them. We can generate the speech corresponding to the silent lip movements. Lip-to-Speech can thus be a cheap and non-invasive method to assist someone who has lost their voice.</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Ethical Considerations</h5>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">We acknowledge that our work has the potential to generate synthetic speech for videos, given that we only require a <math id="S4.SS3.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="1-" display="inline"><semantics id="S4.SS3.SSS0.Px2.p1.1.m1.1a"><mrow id="S4.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml">1</mn><mo id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.3" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1">limit-from</csymbol><cn type="integer" id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.2">1</cn><minus id="S4.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS0.Px2.p1.1.m1.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS0.Px2.p1.1.m1.1c">1-</annotation></semantics></math>second voice sample from any target speaker. However, since the video provides context and constrains the output speech, the generated speech will likely follow the original content closely. We recognize the importance of ethical considerations regarding using such models and ensure that our models will only be shared with users who consent to limit their usage to research-oriented and ethically valid tasks.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Ablation Studies</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we perform several ablation studies to understand the effect of different components of our model. All the ablation experiments are performed on the LRS2 <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> test set.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Effect of different pre-trained lip-to-text models</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.2" class="ltx_p">Additional experiments were conducted using other pre-trained lip-to-text models, specifically DeepLR <cite class="ltx_cite ltx_citemacro_citep">(Afouras et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018a</a>)</cite> and AV-HuBERT <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite>. The former had a WER of <math id="S5.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="51.3" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.1.m1.1a"><mn id="S5.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">51.3</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.1.m1.1b"><cn type="float" id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1">51.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.1.m1.1c">51.3</annotation></semantics></math> on the LRS2 test set, while the latter had a WER of <math id="S5.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="46.6" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.2.m2.1a"><mn id="S5.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">46.6</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.2.m2.1b"><cn type="float" id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1">46.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.2.m2.1c">46.6</annotation></semantics></math>.
The input text to our pre-trained Visual TTS module was taken from different Lip-to-Text models. Additionally, we also directly provide the ground-truth text from the LRS2 test set. As shown in Table <a href="#S5.T3" title="Table 3 ‣ Effect of different pre-trained lip-to-text models ‣ 5. Ablation Studies ‣ Towards Accurate Lip-to-Speech Synthesis in-the-Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our approach recovered speech that was somewhat accurate, despite the presence of noisy text transcripts from both models. This can be attributed to two factors: (i) the correction of errors made by the lip-to-text network by our pipeline to some extent (demonstrated in the demo video); and (ii) the reduced difference in scores between homonyms such as ”ship and sheep” or ”berth and birth” in the audio domain.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3. </span>Comparison of using generated text from different lip-to-text network in our pipeline. We also report the WER of the lip reading model (L2T-WER) on the LRS2 test set as a reference.</figcaption>
<table id="S5.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.5.5" class="ltx_tr">
<th id="S5.T3.5.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.5.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></th>
<th id="S5.T3.5.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.5.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">L2T-WER</span></th>
<th id="S5.T3.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">PESQ<math id="S5.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T3.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S5.T3.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">STOI<math id="S5.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.2.2.2.1.m1.1a"><mo stretchy="false" id="S5.T3.2.2.2.1.m1.1.1" xref="S5.T3.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.1.m1.1b"><ci id="S5.T3.2.2.2.1.m1.1.1.cmml" xref="S5.T3.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S5.T3.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">ESTOI<math id="S5.T3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.3.3.3.1.m1.1a"><mo stretchy="false" id="S5.T3.3.3.3.1.m1.1.1" xref="S5.T3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.1.m1.1b"><ci id="S5.T3.3.3.3.1.m1.1.1.cmml" xref="S5.T3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S5.T3.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.4.4.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">LSE-C<math id="S5.T3.4.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.4.4.4.1.m1.1a"><mo stretchy="false" id="S5.T3.4.4.4.1.m1.1.1" xref="S5.T3.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.1.m1.1b"><ci id="S5.T3.4.4.4.1.m1.1.1.cmml" xref="S5.T3.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S5.T3.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">LSE-D<math id="S5.T3.5.5.5.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.5.5.5.1.m1.1a"><mo stretchy="false" id="S5.T3.5.5.5.1.m1.1.1" xref="S5.T3.5.5.5.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.1.m1.1b"><ci id="S5.T3.5.5.5.1.m1.1.1.cmml" xref="S5.T3.5.5.5.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.5.6.1" class="ltx_tr">
<th id="S5.T3.5.6.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T3.5.6.1.1.1" class="ltx_text" style="font-size:90%;">Deep Lip Reading </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T3.5.6.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Afouras et al<span class="ltx_text">.</span><span id="S5.T3.5.6.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib3" title="" class="ltx_ref">2018a</a><span id="S5.T3.5.6.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S5.T3.5.6.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.6.1.2.1" class="ltx_text" style="font-size:90%;">51.3</span></td>
<td id="S5.T3.5.6.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.6.1.3.1" class="ltx_text" style="font-size:90%;">1.17</span></td>
<td id="S5.T3.5.6.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.6.1.4.1" class="ltx_text" style="font-size:90%;">0.40</span></td>
<td id="S5.T3.5.6.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.6.1.5.1" class="ltx_text" style="font-size:90%;">0.22</span></td>
<td id="S5.T3.5.6.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.6.1.6.1" class="ltx_text" style="font-size:90%;">7.847</span></td>
<td id="S5.T3.5.6.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.6.1.7.1" class="ltx_text" style="font-size:90%;">6.904</span></td>
</tr>
<tr id="S5.T3.5.7.2" class="ltx_tr">
<th id="S5.T3.5.7.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:1.0pt;padding-right:1.0pt;">
<span id="S5.T3.5.7.2.1.1" class="ltx_text" style="font-size:90%;">AV-HuBERT </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S5.T3.5.7.2.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Shi et al<span class="ltx_text">.</span><span id="S5.T3.5.7.2.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib35" title="" class="ltx_ref">2022</a><span id="S5.T3.5.7.2.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S5.T3.5.7.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.7.2.2.1" class="ltx_text" style="font-size:90%;">46.1</span></td>
<td id="S5.T3.5.7.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.7.2.3.1" class="ltx_text" style="font-size:90%;">1.27</span></td>
<td id="S5.T3.5.7.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.7.2.4.1" class="ltx_text" style="font-size:90%;">0.53</span></td>
<td id="S5.T3.5.7.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.7.2.5.1" class="ltx_text" style="font-size:90%;">0.40</span></td>
<td id="S5.T3.5.7.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.7.2.6.1" class="ltx_text" style="font-size:90%;">7.960</span></td>
<td id="S5.T3.5.7.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.7.2.7.1" class="ltx_text" style="font-size:90%;">7.003</span></td>
</tr>
<tr id="S5.T3.5.8.3" class="ltx_tr">
<th id="S5.T3.5.8.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.8.3.1.1" class="ltx_text" style="font-size:90%;">VTP (Ours)</span></th>
<td id="S5.T3.5.8.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.8.3.2.1" class="ltx_text" style="font-size:90%;">22.6</span></td>
<td id="S5.T3.5.8.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.8.3.3.1" class="ltx_text" style="font-size:90%;">1.47</span></td>
<td id="S5.T3.5.8.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.8.3.4.1" class="ltx_text" style="font-size:90%;">0.65</span></td>
<td id="S5.T3.5.8.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.8.3.5.1" class="ltx_text" style="font-size:90%;">0.47</span></td>
<td id="S5.T3.5.8.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.8.3.6.1" class="ltx_text" style="font-size:90%;">8.083</span></td>
<td id="S5.T3.5.8.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.8.3.7.1" class="ltx_text" style="font-size:90%;">6.586</span></td>
</tr>
<tr id="S5.T3.5.9.4" class="ltx_tr">
<th id="S5.T3.5.9.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.9.4.1.1" class="ltx_text" style="font-size:90%;">GT text</span></th>
<td id="S5.T3.5.9.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.9.4.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T3.5.9.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.9.4.3.1" class="ltx_text" style="font-size:90%;">1.51</span></td>
<td id="S5.T3.5.9.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.9.4.4.1" class="ltx_text" style="font-size:90%;">0.69</span></td>
<td id="S5.T3.5.9.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.9.4.5.1" class="ltx_text" style="font-size:90%;">0.50</span></td>
<td id="S5.T3.5.9.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.9.4.6.1" class="ltx_text" style="font-size:90%;">8.781</span></td>
<td id="S5.T3.5.9.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:1.0pt;padding-right:1.0pt;"><span id="S5.T3.5.9.4.7.1" class="ltx_text" style="font-size:90%;">6.106</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Effect of different visual representations</h5>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">We train our proposed Visual Text-to-Speech module with RGB face crops instead of the VTP embeddings to generate speech conditioned on text and lip movements. Based on our observations from Table <a href="#S5.T4" title="Table 4 ‣ Effect of different visual representations ‣ 5. Ablation Studies ‣ Towards Accurate Lip-to-Speech Synthesis in-the-Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, VTP embeddings are the most suitable for this task because they excel in localizing and representing the shape of the speaker’s lips.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4. </span>We present the effect of using different visual representations for training the Visual TTS module.</figcaption>
<table id="S5.T4.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.5.5" class="ltx_tr">
<th id="S5.T4.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T4.5.5.6.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S5.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T4.1.1.1.1" class="ltx_text ltx_font_bold">PESQ<math id="S5.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T4.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S5.T4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T4.2.2.2.1" class="ltx_text ltx_font_bold">STOI<math id="S5.T4.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.2.2.2.1.m1.1a"><mo stretchy="false" id="S5.T4.2.2.2.1.m1.1.1" xref="S5.T4.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.1.m1.1b"><ci id="S5.T4.2.2.2.1.m1.1.1.cmml" xref="S5.T4.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S5.T4.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T4.3.3.3.1" class="ltx_text ltx_font_bold">ESTOI<math id="S5.T4.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.3.3.3.1.m1.1a"><mo stretchy="false" id="S5.T4.3.3.3.1.m1.1.1" xref="S5.T4.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.1.m1.1b"><ci id="S5.T4.3.3.3.1.m1.1.1.cmml" xref="S5.T4.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S5.T4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T4.4.4.4.1" class="ltx_text ltx_font_bold">LSE-C<math id="S5.T4.4.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.4.4.4.1.m1.1a"><mo stretchy="false" id="S5.T4.4.4.4.1.m1.1.1" xref="S5.T4.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.4.1.m1.1b"><ci id="S5.T4.4.4.4.1.m1.1.1.cmml" xref="S5.T4.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.4.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S5.T4.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T4.5.5.5.1" class="ltx_text ltx_font_bold">LSE-D<math id="S5.T4.5.5.5.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.5.5.5.1.m1.1a"><mo stretchy="false" id="S5.T4.5.5.5.1.m1.1.1" xref="S5.T4.5.5.5.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.5.1.m1.1b"><ci id="S5.T4.5.5.5.1.m1.1.1.cmml" xref="S5.T4.5.5.5.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.5.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.5.6.1" class="ltx_tr">
<th id="S5.T4.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Face crops</th>
<td id="S5.T4.5.6.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1.17</td>
<td id="S5.T4.5.6.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.40</td>
<td id="S5.T4.5.6.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.22</td>
<td id="S5.T4.5.6.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">7.847</td>
<td id="S5.T4.5.6.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">6.904</td>
</tr>
<tr id="S5.T4.5.7.2" class="ltx_tr">
<th id="S5.T4.5.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">VTP (Ours)</th>
<td id="S5.T4.5.7.2.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">1.47</td>
<td id="S5.T4.5.7.2.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">0.65</td>
<td id="S5.T4.5.7.2.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">0.47</td>
<td id="S5.T4.5.7.2.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">8.083</td>
<td id="S5.T4.5.7.2.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;">6.586</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Limitations</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In our work, we address the problem of lip-to-speech networks not being able to learn a language model directly from speech supervision. We do so by using a pre-trained lip-to-text network. While our model does not require ground-truth text annotations, the lip-to-text model which we build upon has been trained with text supervision. However, recent efforts in self-supervised pre-training have led to a sharp decrease in the number of text annotations required for training accurate lip-to-text <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite>, making it easier to extend such models to lip-to-speech using our approach. Currently, we have only tested our model in English, and it remains to be validated in other languages.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Our research presents an innovative approach to unconstrained multi-speaker lip-to-speech synthesis that outperforms previous methods by incorporating language and visual information from a highly accurate lip-to-text model. We demonstrate significant improvements in lip-to-speech synthesis, generating high-quality outputs that seamlessly synchronize with silent lip video. Our study can potentially open up exciting avenues for future research. We are particularly encouraged by the success of our approach in assistive technology, where we have shown that our method can generate accurate speech from silent lip movements of individuals with speech impairments. Overall, we are optimistic about the possibilities of our approach to improve communication and enhance the quality of life for people with speech impairments, and we look forward to seeing our work drive further progress in this field.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p"><span id="S7.p3.1.1" class="ltx_text ltx_font_bold">Acknowledgement:</span> This work is supported by MeitY, Government of India</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afouras et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2018c)</span>
<span class="ltx_bibblock">
Triantafyllos Afouras,
Joon Son Chung, Andrew Senior,
Oriol Vinyals, and Andrew Zisserman.
2018c.

</span>
<span class="ltx_bibblock">Deep audio-visual speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and
machine intelligence</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afouras et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2018a)</span>
<span class="ltx_bibblock">
Triantafyllos Afouras,
Joon Son Chung, and Andrew Zisserman.
2018a.

</span>
<span class="ltx_bibblock">Deep Lip Reading: a comparison of models and an
online application. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afouras et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2018b)</span>
<span class="ltx_bibblock">
Triantafyllos Afouras,
Joon Son Chung, and Andrew Zisserman.
2018b.

</span>
<span class="ltx_bibblock">LRS3-TED: a large-scale dataset for visual speech
recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.00496</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akbari et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Hassan Akbari, Himani
Arora, Liangliang Cao, and Nima
Mesgarani. 2017.

</span>
<span class="ltx_bibblock">Lip2Audspec: Speech Reconstruction from Silent Lip
Movements Video.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</em> (2017),
2516–2520.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Assael et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Yannis M Assael, Brendan
Shillingford, Shimon Whiteson, and
Nando De Freitas. 2016.

</span>
<span class="ltx_bibblock">Lipnet: End-to-end sentence-level lipreading.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1611.01599</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
William Chan, Navdeep
Jaitly, Quoc Le, and Oriol Vinyals.
2016.

</span>
<span class="ltx_bibblock">Listen, attend and spell: A neural network for
large vocabulary conversational speech recognition. In
<em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">2016 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>. 4960–4964.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICASSP.2016.7472621" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICASSP.2016.7472621</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Joon Son Chung, Andrew
Senior, Oriol Vinyals, and Andrew
Zisserman. 2017.

</span>
<span class="ltx_bibblock">Lip reading sentences in the wild. In
<em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>. IEEE, 3444–3453.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung and Zisserman (2016a)</span>
<span class="ltx_bibblock">
Joon Son Chung and
Andrew Zisserman. 2016a.

</span>
<span class="ltx_bibblock">Lip reading in the wild. In
<em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Asian Conference on Computer Vision</em>. Springer,
87–103.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung and Zisserman (2016b)</span>
<span class="ltx_bibblock">
J. S. Chung and A.
Zisserman. 2016b.

</span>
<span class="ltx_bibblock">Out of time: automated lip sync in the wild. In
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Workshop on Multi-view Lip-reading, ACCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cooke et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2006)</span>
<span class="ltx_bibblock">
Martin Cooke, Jon Barker,
Stuart Cunningham, and Xu Shao.
2006.

</span>
<span class="ltx_bibblock">An audio-visual corpus for speech perception and
automatic speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of
America</em> 120, 5 (2006),
2421–2424.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ephrat et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ariel Ephrat, Tavi
Halperin, and Shmuel Peleg.
2017.

</span>
<span class="ltx_bibblock">Improved Speech Reconstruction from Silent Video.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">2017 IEEE International Conference on
Computer Vision Workshops (ICCVW)</em> (2017),
455–462.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ephrat and Peleg (2017)</span>
<span class="ltx_bibblock">
Ariel Ephrat and Shmuel
Peleg. 2017.

</span>
<span class="ltx_bibblock">Vid2Speech: speech reconstruction from silent
video. In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2017 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag and Al-Onaizan (2017)</span>
<span class="ltx_bibblock">
Markus Freitag and Yaser
Al-Onaizan. 2017.

</span>
<span class="ltx_bibblock">Beam Search Strategies for Neural Machine
Translation. In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">NMT@ACL</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">gil Lee et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Sang gil Lee, Wei Ping,
Boris Ginsburg, Bryan Catanzaro, and
Sung-Hoon Yoon. 2022.

</span>
<span class="ltx_bibblock">BigVGAN: A Universal Neural Vocoder with
Large-Scale Training.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">ArXiv</em> abs/2206.04658
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harte and Gillen (2015)</span>
<span class="ltx_bibblock">
Naomi Harte and Eoin
Gillen. 2015.

</span>
<span class="ltx_bibblock">TCD-TIMIT: An audio-visual corpus of continuous
speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Multimedia</em>
17, 5 (2015),
603–615.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassid et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Michael Hassid,
Michelle Tadmor Ramanovich, Brendan
Shillingford, Miaosen Wang, Ye Jia,
and Tal Remez. 2022.

</span>
<span class="ltx_bibblock">More than Words: In-the-Wild Visually-Driven
Prosody for Text-to-Speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em> (2022),
10577–10587.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hegde et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Sindhu B. Hegde, K R
Prajwal, Rudrabha Mukhopadhyay, Vinay P.
Namboodiri, and C.V. Jawahar.
2022.

</span>
<span class="ltx_bibblock">Lip-to-Speech Synthesis for Arbitrary Speakers in
the Wild. In <em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM
International Conference on Multimedia</em> (Lisboa, Portugal)
<em id="bib.bib18.4.2" class="ltx_emph ltx_font_italic">(MM ’22)</em>. Association for
Computing Machinery, New York, NY, USA,
6250–6258.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3503161.3548081" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3503161.3548081</a>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Chenxu Hu, Qiao Tian,
Tingle Li, Yuping Wang,
Yuxuan Wang, and Hang Zhao.
2021.

</span>
<span class="ltx_bibblock">Neural Dubber: Dubbing for Videos According to
Scripts. In <em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Ye Jia, Yu Zhang,
Ron J. Weiss, Quan Wang,
Jonathan Shen, Fei Ren,
Zhifeng Chen, Patrick Nguyen,
Ruoming Pang, Ignacio Lopez Moreno, and
Yonghui Wu. 2018.

</span>
<span class="ltx_bibblock">Transfer Learning from Speaker Verification to
Multispeaker Text-to-Speech Synthesis. In
<em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd International Conference on
Neural Information Processing Systems</em> <em id="bib.bib20.4.2" class="ltx_emph ltx_font_italic">(NIPS’18)</em>.
Curran Associates Inc., 4485–4495.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Minsu Kim, Joanna Hong,
and Yong Man Ro. 2022.

</span>
<span class="ltx_bibblock">Lip to Speech Synthesis with Visual Context
Attentional GAN. In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Neural Information Processing
Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Minsu Kim, Joanna Hong,
and Yong Man Ro. 2023.

</span>
<span class="ltx_bibblock">Lip-to-Speech Synthesis in the Wild with Multi-task
Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">ArXiv</em> abs/2302.08841
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2015)</span>
<span class="ltx_bibblock">
Diederik P. Kingma and
Jimmy Ba. 2015.

</span>
<span class="ltx_bibblock">Adam: A Method for Stochastic Optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/1412.6980
(2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mira et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Rodrigo Mira, Alexandros
Haliassos, Stavros Petridis, Björn
Schuller, and Maja Pantic.
2022.

</span>
<span class="ltx_bibblock">SVTS: Scalable Video-to-Speech Synthesis. In
<em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">Interspeech</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Momeni et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Liliane Momeni, Hannah
Bull, Prajwal K R, Samuel Albanie,
Gül Varol, and Andrew Zisserman.
2022.

</span>
<span class="ltx_bibblock">Automatic dense annotation of large-vocabulary sign
language videos. In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">ECCV</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ping et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Wei Ping, Kainan Peng,
Andrew Gibiansky, Sercan O. Arik,
Ajay Kannan, Sharan Narang,
Jonathan Raiman, and John Miller.
2018.

</span>
<span class="ltx_bibblock">Deep Voice 3: 2000-Speaker Neural Text-to-Speech.
In <em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://openreview.net/forum?id=HJtEm4p6Z" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=HJtEm4p6Z</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prajwal et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
K R Prajwal, Triantafyllos
Afouras, and Andrew Zisserman.
2022.

</span>
<span class="ltx_bibblock">Sub-Word Level Lip Reading With Visual Attention.
In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>.
5162–5172.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prajwal et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
K R Prajwal, Rudrabha
Mukhopadhyay, Vinay P. Namboodiri, and
C.V. Jawahar. 2020a.

</span>
<span class="ltx_bibblock">Learning Individual Speaking Styles for Accurate
Lip to Speech Synthesis. In <em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">The IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prajwal et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
K R Prajwal, Rudrabha
Mukhopadhyay, Vinay P. Namboodiri, and
C.V. Jawahar. 2020b.

</span>
<span class="ltx_bibblock">A Lip Sync Expert Is All You Need for Speech to Lip
Generation In the Wild. In <em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th
ACM International Conference on Multimedia</em> (Seattle, WA, USA)
<em id="bib.bib29.4.2" class="ltx_emph ltx_font_italic">(MM ’20)</em>. Association for
Computing Machinery, New York, NY, USA,
484–492.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3394171.3413532" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3394171.3413532</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">R et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Prajwal K R, Liliane
Momeni, Triantafyllos Afouras, and
Andrew Zisserman. 2021.

</span>
<span class="ltx_bibblock">Visual Keyword Spotting with Attention. In
<em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">BMVC</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yi Ren, Chenxu Hu,
Xu Tan, Tao Qin, Sheng
Zhao, Zhou Zhao, and Tie-Yan Liu.
2020.

</span>
<span class="ltx_bibblock">Fastspeech 2: Fast and high-quality end-to-end text
to speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.04558</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sen et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Bipasha Sen, Aditya
Agarwal, Rudrabha Mukhopadhyay, Vinay
Namboodiri, and CV Jawahar.
2021.

</span>
<span class="ltx_bibblock">Personalized One-Shot Lipreading for an ALS
Patient.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.01740</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Serdyuk et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Dmitriy Serdyuk, Otavio
Braga, and Olivier Siohan.
2022.

</span>
<span class="ltx_bibblock">Transformer-Based Video Front-Ends for
Audio-Visual Speech Recognition for Single and Muti-Person Video. In
<em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2022</em>.
2833–2837.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.21437/Interspeech.2022-10920" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.21437/Interspeech.2022-10920</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Jonathan Shen, R. Pang,
Ron J. Weiss, M. Schuster,
Navdeep Jaitly, Z. Yang,
Z. Chen, Yu Zhang,
Yuxuan Wang, R. Skerry-Ryan,
R. A. Saurous, Yannis Agiomyrgiannakis,
and Y. Wu. 2018.

</span>
<span class="ltx_bibblock">Natural TTS Synthesis by Conditioning Wavenet on
MEL Spectrogram Predictions.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</em> (2018),
4779–4783.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Bowen Shi, Wei-Ning Hsu,
Kushal Lakhotia, and Abdel rahman
Mohamed. 2022.

</span>
<span class="ltx_bibblock">Learning Audio-Visual Speech Representation by
Masked Multimodal Cluster Prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">ArXiv</em> abs/2201.02184
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam M.
Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin.
2017.

</span>
<span class="ltx_bibblock">Attention is All you Need.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">ArXiv</em> abs/1706.03762
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vougioukas et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Konstantinos Vougioukas,
Pingchuan Ma, Stavros Petridis, and
Maja Pantic. 2019.

</span>
<span class="ltx_bibblock">Video-Driven Speech Reconstruction using Generative
Adversarial Networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.06301</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.01086" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.01087" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.01087">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.01087" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.01088" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 17:45:01 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
