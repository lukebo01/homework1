<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.11858] Convexity based pruning of speech representation models</title><meta property="og:description" content="Speech representation models based on the transformer architecture and trained by self-supervised learning have shown great promise for solving tasks such as speech and speaker recognition, keyword spotting, emotion de‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Convexity based pruning of speech representation models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Convexity based pruning of speech representation models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.11858">

<!--Generated on Thu Sep  5 16:07:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Convexity based pruning of speech representation models</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Speech representation models based on the transformer architecture and trained by self-supervised learning have shown great promise for solving tasks such as speech and speaker recognition, keyword spotting, emotion detection, and more. Typically, it is found that larger models lead to better performance. However, the significant computational effort involved in such large transformer systems is a challenge for embedded and real-world applications. Recent work has shown that there is significant redundancy in the transformer models for NLP and massive layer pruning is feasible (Sajjad et al., 2023). Here, we investigate layer pruning in audio models. We base the pruning decision on a convexity criterion. Convexity of classification regions has recently been proposed as an indicator of subsequent fine-tuning performance in a range of application domains, including NLP and audio. In empirical investigations, we find a massive reduction in the computational effort with no loss of performance or even improvements in certain cases.
</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">‚Äî‚Äâ</span></span>
Convexity, network pruning, self-supervised learning, speech representation learning</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Self-supervised speech representation models have emerged as highly promising models for many speech-related tasks, such as automatic speech recognition or speech translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. There are many different models available with similar performance, differing mainly in pretraining strategies. How these various pretraining strategies influence the representations and how the latent representations develop across the models are still open questions only limited works have explored. In this work, we investigate geometric properties, in particular convexity, of the learned representations and their development across the layers of speech representation models. We also explore how fine-tuning influences the representations and whether knowledge about convex regions in the latent space can help guide the pruning of models to reduce computational complexity and improve downstream performance.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recent work has focused on analyzing the word and phoneme content encoded in latent representations using canonical correlation analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. They show how acoustic, phonetic, and word content develops across layers in several models and relate the word or phoneme content to performance in downstream tasks. They find that using a linear classifier on intermediate frozen layers can outperform classifiers based on the full frozen model, due to more meaningful representations. While our work is inspired by the aforementioned results, it adds a new dimension of analyzing the structure of the latent space directly, by testing geometric properties (in this case convexity). We also fine-tune on intermediate layers directly (i.e. fine-tuning of a pruned model) and show that pruned models can even outperform fine-tuned full models.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Pruning of deep neural networks is a technique that removes redundant neurons not influencing the performance and, hence, reduces the size of a trained network without the need for retraining. There are many types of pruning algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. In this work, we propose a static layer-wise pruning method for transformer models: we prune all layers from a certain point until the end of the network, also called <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">top-layer dropping</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Fan et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> proposed LayerDrop: layer-level dropout during training. They showed that it facilitates robust dropping of layers at test time.
In comparison, our approach allows to use an already existing pretrained model without changing the training procedure. We propose to choose the number of layers for pruning based on the graph convexity score, hence, eliminating the necessity of estimating a suitable number of layers to prune or testing multiple layers.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We use a recently developed workflow for measuring the convexity of regions in latent spaces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Importantly, the authors found evidence that convexity in pretrained models is related to downstream accuracy of subsequent fine-tuned models. Here we explore this result to establish a pruning strategy. We adapt and apply this metric to speech representation models to improve understanding of these models and the influence of fine-tuning to specific tasks.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our main contributions are (i) in-depth analysis of the latent spaces of state-of-the-art speech representation models, leading to a better understanding of latent representations formed by self-supervised training, (ii) the investigation of changes in the latent representations during fine-tuning to different downstream tasks, and (iii) the development of pruning strategies based on geometric properties of latent representations leading to comparable or better downstream performance with drastically reduced computational complexity.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We find that speech representation models develop distinctive patterns of convex regions, which are similar across most models (except for one). We show that fine-tuning increases the convexity of relevant classes while, at the same time, decreasing the convexity of non-relevant features. Finally, we demonstrate how pruning based on the convexity score can lead to comparable and even better-performing models while decreasing model sizes by up to 70%, decreasing training times by 20-50% and inference times by 25-60%.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Graph Convexity</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We use the graph convexity score introduced by Tƒõtkov√° et al.¬†in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. It extends the common definition of convexity for Euclidean spaces to curved manifolds and utilizes sampled data, making it a suitable tool for analyzing high-dimensional latent spaces in a computationally feasible way. Graph convexity is defined as follows:</p>
</div>
<div id="Thmdefinitionx1" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span id="Thmdefinitionx1.1.1.1" class="ltx_text ltx_font_bold">Definition</span></span><span id="Thmdefinitionx1.2.2" class="ltx_text ltx_font_bold"> </span>(Graph Convexity)<span id="Thmdefinitionx1.3.3" class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmdefinitionx1.p1" class="ltx_para">
<p id="Thmdefinitionx1.p1.6" class="ltx_p">Let <math id="Thmdefinitionx1.p1.1.m1.2" class="ltx_Math" alttext="(V,E)" display="inline"><semantics id="Thmdefinitionx1.p1.1.m1.2a"><mrow id="Thmdefinitionx1.p1.1.m1.2.3.2" xref="Thmdefinitionx1.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="Thmdefinitionx1.p1.1.m1.2.3.2.1" xref="Thmdefinitionx1.p1.1.m1.2.3.1.cmml">(</mo><mi id="Thmdefinitionx1.p1.1.m1.1.1" xref="Thmdefinitionx1.p1.1.m1.1.1.cmml">V</mi><mo id="Thmdefinitionx1.p1.1.m1.2.3.2.2" xref="Thmdefinitionx1.p1.1.m1.2.3.1.cmml">,</mo><mi id="Thmdefinitionx1.p1.1.m1.2.2" xref="Thmdefinitionx1.p1.1.m1.2.2.cmml">E</mi><mo stretchy="false" id="Thmdefinitionx1.p1.1.m1.2.3.2.3" xref="Thmdefinitionx1.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx1.p1.1.m1.2b"><interval closure="open" id="Thmdefinitionx1.p1.1.m1.2.3.1.cmml" xref="Thmdefinitionx1.p1.1.m1.2.3.2"><ci id="Thmdefinitionx1.p1.1.m1.1.1.cmml" xref="Thmdefinitionx1.p1.1.m1.1.1">ùëâ</ci><ci id="Thmdefinitionx1.p1.1.m1.2.2.cmml" xref="Thmdefinitionx1.p1.1.m1.2.2">ùê∏</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx1.p1.1.m1.2c">(V,E)</annotation></semantics></math> be a graph and <math id="Thmdefinitionx1.p1.2.m2.1" class="ltx_Math" alttext="A\subseteq V" display="inline"><semantics id="Thmdefinitionx1.p1.2.m2.1a"><mrow id="Thmdefinitionx1.p1.2.m2.1.1" xref="Thmdefinitionx1.p1.2.m2.1.1.cmml"><mi id="Thmdefinitionx1.p1.2.m2.1.1.2" xref="Thmdefinitionx1.p1.2.m2.1.1.2.cmml">A</mi><mo id="Thmdefinitionx1.p1.2.m2.1.1.1" xref="Thmdefinitionx1.p1.2.m2.1.1.1.cmml">‚äÜ</mo><mi id="Thmdefinitionx1.p1.2.m2.1.1.3" xref="Thmdefinitionx1.p1.2.m2.1.1.3.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx1.p1.2.m2.1b"><apply id="Thmdefinitionx1.p1.2.m2.1.1.cmml" xref="Thmdefinitionx1.p1.2.m2.1.1"><subset id="Thmdefinitionx1.p1.2.m2.1.1.1.cmml" xref="Thmdefinitionx1.p1.2.m2.1.1.1"></subset><ci id="Thmdefinitionx1.p1.2.m2.1.1.2.cmml" xref="Thmdefinitionx1.p1.2.m2.1.1.2">ùê¥</ci><ci id="Thmdefinitionx1.p1.2.m2.1.1.3.cmml" xref="Thmdefinitionx1.p1.2.m2.1.1.3">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx1.p1.2.m2.1c">A\subseteq V</annotation></semantics></math>. <math id="Thmdefinitionx1.p1.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="Thmdefinitionx1.p1.3.m3.1a"><mi id="Thmdefinitionx1.p1.3.m3.1.1" xref="Thmdefinitionx1.p1.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="Thmdefinitionx1.p1.3.m3.1b"><ci id="Thmdefinitionx1.p1.3.m3.1.1.cmml" xref="Thmdefinitionx1.p1.3.m3.1.1">ùê¥</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx1.p1.3.m3.1c">A</annotation></semantics></math> is convex if for all pairs <math id="Thmdefinitionx1.p1.4.m4.2" class="ltx_Math" alttext="x,y\in A" display="inline"><semantics id="Thmdefinitionx1.p1.4.m4.2a"><mrow id="Thmdefinitionx1.p1.4.m4.2.3" xref="Thmdefinitionx1.p1.4.m4.2.3.cmml"><mrow id="Thmdefinitionx1.p1.4.m4.2.3.2.2" xref="Thmdefinitionx1.p1.4.m4.2.3.2.1.cmml"><mi id="Thmdefinitionx1.p1.4.m4.1.1" xref="Thmdefinitionx1.p1.4.m4.1.1.cmml">x</mi><mo id="Thmdefinitionx1.p1.4.m4.2.3.2.2.1" xref="Thmdefinitionx1.p1.4.m4.2.3.2.1.cmml">,</mo><mi id="Thmdefinitionx1.p1.4.m4.2.2" xref="Thmdefinitionx1.p1.4.m4.2.2.cmml">y</mi></mrow><mo id="Thmdefinitionx1.p1.4.m4.2.3.1" xref="Thmdefinitionx1.p1.4.m4.2.3.1.cmml">‚àà</mo><mi id="Thmdefinitionx1.p1.4.m4.2.3.3" xref="Thmdefinitionx1.p1.4.m4.2.3.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx1.p1.4.m4.2b"><apply id="Thmdefinitionx1.p1.4.m4.2.3.cmml" xref="Thmdefinitionx1.p1.4.m4.2.3"><in id="Thmdefinitionx1.p1.4.m4.2.3.1.cmml" xref="Thmdefinitionx1.p1.4.m4.2.3.1"></in><list id="Thmdefinitionx1.p1.4.m4.2.3.2.1.cmml" xref="Thmdefinitionx1.p1.4.m4.2.3.2.2"><ci id="Thmdefinitionx1.p1.4.m4.1.1.cmml" xref="Thmdefinitionx1.p1.4.m4.1.1">ùë•</ci><ci id="Thmdefinitionx1.p1.4.m4.2.2.cmml" xref="Thmdefinitionx1.p1.4.m4.2.2">ùë¶</ci></list><ci id="Thmdefinitionx1.p1.4.m4.2.3.3.cmml" xref="Thmdefinitionx1.p1.4.m4.2.3.3">ùê¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx1.p1.4.m4.2c">x,y\in A</annotation></semantics></math>, there exists a shortest path <math id="Thmdefinitionx1.p1.5.m5.2" class="ltx_Math" alttext="P=(x=v_{0},v_{1},v_{2},\dots,v_{n-1},y=v_{n})" display="inline"><semantics id="Thmdefinitionx1.p1.5.m5.2a"><mrow id="Thmdefinitionx1.p1.5.m5.2.2" xref="Thmdefinitionx1.p1.5.m5.2.2.cmml"><mi id="Thmdefinitionx1.p1.5.m5.2.2.3" xref="Thmdefinitionx1.p1.5.m5.2.2.3.cmml">P</mi><mo id="Thmdefinitionx1.p1.5.m5.2.2.2" xref="Thmdefinitionx1.p1.5.m5.2.2.2.cmml">=</mo><mrow id="Thmdefinitionx1.p1.5.m5.2.2.1.1" xref="Thmdefinitionx1.p1.5.m5.2.2.cmml"><mo stretchy="false" id="Thmdefinitionx1.p1.5.m5.2.2.1.1.2" xref="Thmdefinitionx1.p1.5.m5.2.2.cmml">(</mo><mrow id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.3.cmml"><mrow id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.cmml"><mi id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.6" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.6.cmml">x</mi><mo id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.5" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.5.cmml">=</mo><mrow id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.5.cmml"><msub id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1.2" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1.2.cmml">v</mi><mn id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1.3" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.5" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.5.cmml">,</mo><msub id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2.cmml"><mi id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2.2" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2.2.cmml">v</mi><mn id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2.3" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.6" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.5.cmml">,</mo><msub id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3.cmml"><mi id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3.2" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3.2.cmml">v</mi><mn id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3.3" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3.3.cmml">2</mn></msub><mo id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.7" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.5.cmml">,</mo><mi mathvariant="normal" id="Thmdefinitionx1.p1.5.m5.1.1" xref="Thmdefinitionx1.p1.5.m5.1.1.cmml">‚Ä¶</mi><mo id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.8" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.5.cmml">,</mo><msub id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.cmml"><mi id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.2" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.2.cmml">v</mi><mrow id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.cmml"><mi id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.2" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.2.cmml">n</mi><mo id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.1" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.1.cmml">‚àí</mo><mn id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.3" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.3" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.3a.cmml">,</mo><mrow id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.cmml"><mi id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.2" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.2.cmml">y</mi><mo id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.1" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.1.cmml">=</mo><msub id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3.cmml"><mi id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3.2" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3.2.cmml">v</mi><mi id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3.3" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3.3.cmml">n</mi></msub></mrow></mrow><mo stretchy="false" id="Thmdefinitionx1.p1.5.m5.2.2.1.1.3" xref="Thmdefinitionx1.p1.5.m5.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx1.p1.5.m5.2b"><apply id="Thmdefinitionx1.p1.5.m5.2.2.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2"><eq id="Thmdefinitionx1.p1.5.m5.2.2.2.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.2"></eq><ci id="Thmdefinitionx1.p1.5.m5.2.2.3.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.3">ùëÉ</ci><apply id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.3.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2"><csymbol cd="ambiguous" id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.3a.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.3">formulae-sequence</csymbol><apply id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1"><eq id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.5.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.5"></eq><ci id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.6.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.6">ùë•</ci><list id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.5.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4"><apply id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1.2">ùë£</ci><cn type="integer" id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.1.1.1.3">0</cn></apply><apply id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2.1.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2.2.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2.2">ùë£</ci><cn type="integer" id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2.3.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.2.2.2.3">1</cn></apply><apply id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3.1.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3.2.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3.2">ùë£</ci><cn type="integer" id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3.3.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.3.3.3.3">2</cn></apply><ci id="Thmdefinitionx1.p1.5.m5.1.1.cmml" xref="Thmdefinitionx1.p1.5.m5.1.1">‚Ä¶</ci><apply id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4"><csymbol cd="ambiguous" id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.1.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4">subscript</csymbol><ci id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.2.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.2">ùë£</ci><apply id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3"><minus id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.1.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.1"></minus><ci id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.2.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.2">ùëõ</ci><cn type="integer" id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.3.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.1.1.4.4.4.3.3">1</cn></apply></apply></list></apply><apply id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2"><eq id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.1.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.1"></eq><ci id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.2.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.2">ùë¶</ci><apply id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3"><csymbol cd="ambiguous" id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3.1.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3">subscript</csymbol><ci id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3.2.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3.2">ùë£</ci><ci id="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3.3.cmml" xref="Thmdefinitionx1.p1.5.m5.2.2.1.1.1.2.2.3.3">ùëõ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx1.p1.5.m5.2c">P=(x=v_{0},v_{1},v_{2},\dots,v_{n-1},y=v_{n})</annotation></semantics></math> and <math id="Thmdefinitionx1.p1.6.m6.3" class="ltx_Math" alttext="\forall i\in\{0,\dots,n\}:v_{i}\in A" display="inline"><semantics id="Thmdefinitionx1.p1.6.m6.3a"><mrow id="Thmdefinitionx1.p1.6.m6.3.4" xref="Thmdefinitionx1.p1.6.m6.3.4.cmml"><mrow id="Thmdefinitionx1.p1.6.m6.3.4.2" xref="Thmdefinitionx1.p1.6.m6.3.4.2.cmml"><mrow id="Thmdefinitionx1.p1.6.m6.3.4.2.2" xref="Thmdefinitionx1.p1.6.m6.3.4.2.2.cmml"><mo rspace="0.167em" id="Thmdefinitionx1.p1.6.m6.3.4.2.2.1" xref="Thmdefinitionx1.p1.6.m6.3.4.2.2.1.cmml">‚àÄ</mo><mi id="Thmdefinitionx1.p1.6.m6.3.4.2.2.2" xref="Thmdefinitionx1.p1.6.m6.3.4.2.2.2.cmml">i</mi></mrow><mo id="Thmdefinitionx1.p1.6.m6.3.4.2.1" xref="Thmdefinitionx1.p1.6.m6.3.4.2.1.cmml">‚àà</mo><mrow id="Thmdefinitionx1.p1.6.m6.3.4.2.3.2" xref="Thmdefinitionx1.p1.6.m6.3.4.2.3.1.cmml"><mo stretchy="false" id="Thmdefinitionx1.p1.6.m6.3.4.2.3.2.1" xref="Thmdefinitionx1.p1.6.m6.3.4.2.3.1.cmml">{</mo><mn id="Thmdefinitionx1.p1.6.m6.1.1" xref="Thmdefinitionx1.p1.6.m6.1.1.cmml">0</mn><mo id="Thmdefinitionx1.p1.6.m6.3.4.2.3.2.2" xref="Thmdefinitionx1.p1.6.m6.3.4.2.3.1.cmml">,</mo><mi mathvariant="normal" id="Thmdefinitionx1.p1.6.m6.2.2" xref="Thmdefinitionx1.p1.6.m6.2.2.cmml">‚Ä¶</mi><mo id="Thmdefinitionx1.p1.6.m6.3.4.2.3.2.3" xref="Thmdefinitionx1.p1.6.m6.3.4.2.3.1.cmml">,</mo><mi id="Thmdefinitionx1.p1.6.m6.3.3" xref="Thmdefinitionx1.p1.6.m6.3.3.cmml">n</mi><mo rspace="0.278em" stretchy="false" id="Thmdefinitionx1.p1.6.m6.3.4.2.3.2.4" xref="Thmdefinitionx1.p1.6.m6.3.4.2.3.1.cmml">}</mo></mrow></mrow><mo rspace="0.278em" id="Thmdefinitionx1.p1.6.m6.3.4.1" xref="Thmdefinitionx1.p1.6.m6.3.4.1.cmml">:</mo><mrow id="Thmdefinitionx1.p1.6.m6.3.4.3" xref="Thmdefinitionx1.p1.6.m6.3.4.3.cmml"><msub id="Thmdefinitionx1.p1.6.m6.3.4.3.2" xref="Thmdefinitionx1.p1.6.m6.3.4.3.2.cmml"><mi id="Thmdefinitionx1.p1.6.m6.3.4.3.2.2" xref="Thmdefinitionx1.p1.6.m6.3.4.3.2.2.cmml">v</mi><mi id="Thmdefinitionx1.p1.6.m6.3.4.3.2.3" xref="Thmdefinitionx1.p1.6.m6.3.4.3.2.3.cmml">i</mi></msub><mo id="Thmdefinitionx1.p1.6.m6.3.4.3.1" xref="Thmdefinitionx1.p1.6.m6.3.4.3.1.cmml">‚àà</mo><mi id="Thmdefinitionx1.p1.6.m6.3.4.3.3" xref="Thmdefinitionx1.p1.6.m6.3.4.3.3.cmml">A</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinitionx1.p1.6.m6.3b"><apply id="Thmdefinitionx1.p1.6.m6.3.4.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4"><ci id="Thmdefinitionx1.p1.6.m6.3.4.1.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.1">:</ci><apply id="Thmdefinitionx1.p1.6.m6.3.4.2.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.2"><in id="Thmdefinitionx1.p1.6.m6.3.4.2.1.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.2.1"></in><apply id="Thmdefinitionx1.p1.6.m6.3.4.2.2.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.2.2"><csymbol cd="latexml" id="Thmdefinitionx1.p1.6.m6.3.4.2.2.1.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.2.2.1">for-all</csymbol><ci id="Thmdefinitionx1.p1.6.m6.3.4.2.2.2.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.2.2.2">ùëñ</ci></apply><set id="Thmdefinitionx1.p1.6.m6.3.4.2.3.1.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.2.3.2"><cn type="integer" id="Thmdefinitionx1.p1.6.m6.1.1.cmml" xref="Thmdefinitionx1.p1.6.m6.1.1">0</cn><ci id="Thmdefinitionx1.p1.6.m6.2.2.cmml" xref="Thmdefinitionx1.p1.6.m6.2.2">‚Ä¶</ci><ci id="Thmdefinitionx1.p1.6.m6.3.3.cmml" xref="Thmdefinitionx1.p1.6.m6.3.3">ùëõ</ci></set></apply><apply id="Thmdefinitionx1.p1.6.m6.3.4.3.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.3"><in id="Thmdefinitionx1.p1.6.m6.3.4.3.1.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.3.1"></in><apply id="Thmdefinitionx1.p1.6.m6.3.4.3.2.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.3.2"><csymbol cd="ambiguous" id="Thmdefinitionx1.p1.6.m6.3.4.3.2.1.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.3.2">subscript</csymbol><ci id="Thmdefinitionx1.p1.6.m6.3.4.3.2.2.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.3.2.2">ùë£</ci><ci id="Thmdefinitionx1.p1.6.m6.3.4.3.2.3.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.3.2.3">ùëñ</ci></apply><ci id="Thmdefinitionx1.p1.6.m6.3.4.3.3.cmml" xref="Thmdefinitionx1.p1.6.m6.3.4.3.3">ùê¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinitionx1.p1.6.m6.3c">\forall i\in\{0,\dots,n\}:v_{i}\in A</annotation></semantics></math>.</p>
</div>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Converting the problem to data-driven estimators leads to graphs with vertices being the data points and edges defined as Euclidean distances between pairs of data. In curved manifolds, the shortest curve between two points is a geodesic (corresponding to a segment in Euclidean spaces) and that converts to the shortest path in the graph.
Hence, in a simplified way, the graph convexity score is defined as a proportion of the ‚Äúwell-classified‚Äù vertices on the shortest paths between each two points from the same class.
In contrast to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, we calculate the convexity score of the labeled classes and not the decision regions (i.e., we take the target labels as class labels instead of the predicted labels).</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.3" class="ltx_p">We describe the construction in detail: first, we extract representations for all data points after each layer. For each layer separately, we compute Euclidean distances between all representations of data points. We keep only <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mn id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><cn type="integer" id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">10</annotation></semantics></math> nearest neighbors and use this as an incidence matrix for a weighted graph. For a pair of points belonging to the same class, we find the shortest path in the graph and compute the proportion of vertices on the path (without the endpoints) that belong to the same class. We average these scores over all pairs of points over all classes. Therefore, the graph convexity score is a number between <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><mn id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><cn type="integer" id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">0</cn></annotation-xml></semantics></math> and <math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><mn id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><cn type="integer" id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">1</annotation></semantics></math> (where higher is better) computed separately for each model and each layer. Disconnected graphs have a score of 0, while directly connected points (i.e. closest neighbors) have a score of 1.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.2" class="ltx_p">We will refer to the graph convexity score as <span id="S2.SS1.p4.2.1" class="ltx_text ltx_font_italic">convexity</span> for the rest of the paper. The baseline convexity score corresponds to <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="1/c" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><mrow id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml"><mn id="S2.SS1.p4.1.m1.1.1.2" xref="S2.SS1.p4.1.m1.1.1.2.cmml">1</mn><mo id="S2.SS1.p4.1.m1.1.1.1" xref="S2.SS1.p4.1.m1.1.1.1.cmml">/</mo><mi id="S2.SS1.p4.1.m1.1.1.3" xref="S2.SS1.p4.1.m1.1.1.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><apply id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1"><divide id="S2.SS1.p4.1.m1.1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1.1"></divide><cn type="integer" id="S2.SS1.p4.1.m1.1.1.2.cmml" xref="S2.SS1.p4.1.m1.1.1.2">1</cn><ci id="S2.SS1.p4.1.m1.1.1.3.cmml" xref="S2.SS1.p4.1.m1.1.1.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">1/c</annotation></semantics></math>, with <math id="S2.SS1.p4.2.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.SS1.p4.2.m2.1a"><mi id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><ci id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">c</annotation></semantics></math> being the number of balanced classes.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The dataset used for all experiments is the <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">speech commands v0.02</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. All audio files are sampled to 16000Hz and zero-padded to a length of 16000 if necessary. From this dataset, we create three datasets to measure different concepts: a word dataset, a speaker dataset, and a phoneme dataset.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Word dataset.</span> The word dataset is the <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_italic">speech commands</span> dataset, where we use the <span id="S2.SS2.p2.1.3" class="ltx_text ltx_font_italic">label</span> information to define the 35 classes (the <span id="S2.SS2.p2.1.4" class="ltx_text ltx_font_italic">_silence_</span> class is excluded). We use the validation set (9982 files) for the convexity analysis and the test set (4890 files) for testing fine-tuned models.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Speaker dataset.</span> For the speaker dataset, we filter the train set from the <span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_italic">speech commands</span> dataset by the <span id="S2.SS2.p3.1.3" class="ltx_text ltx_font_italic">speaker_id</span> to only include speakers that appear more than 50 times. We then split the dataset into a train and test dataset, resulting in 39126 training and 4348 test files with 388 classes. We use the test set for the convexity analysis and testing of fine-tuned models.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Phoneme dataset.</span> For the convexity of phonemes, we create a phoneme dataset based on the validation set of the <span id="S2.SS2.p4.1.2" class="ltx_text ltx_font_italic">speech commands</span> dataset. Phonemes are defined based on phonetic transcriptions (using X-SAMPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>) of the spoken word, timings for each phoneme utterance are provided by the MAUS web service <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and the corresponding audio parts are extracted. The extracted dataset consists of 33317 audio files covering 35 phonemes. For the convexity analysis, we only include phonemes with more than 1000 occurrences, resulting in 17 phonemes and 25463 audio files.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Models</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">We investigate transformer-based models and consider models with different sizes and pretraining strategies to explore the effect of these factors on the representations. We also compare the representations of pretrained and fine-tuned models.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">We choose four models based on acceptance in the field and performance on the SUPERB benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In particular, wav2vec2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, wavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and ccc-wav2vec2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. We deploy two sizes of each model: ‚Äôbase‚Äô with 12 layers and embedding size 768, and ‚Äôlarge‚Äô with 24 layers and embedding size 1024 (except for ccc-wav2vec2, where no large model is available). Pretrained models are retrieved from <a href="huggingface.co" title="" class="ltx_ref ltx_url ltx_font_typewriter">huggingface.co</a>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.4" class="ltx_p"><span id="S2.SS3.p3.4.1" class="ltx_text ltx_font_bold">Fine-tuning</span> of the models is done with the Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> Trainer module using the training set of <span id="S2.SS3.p3.4.2" class="ltx_text ltx_font_italic">speech commands v0.02</span> for word classification and the created training set for speaker identification (described above). For fine-tuning, a feature projector and a linear classification layer are added to the pretrained models. Models are fine-tuned to perform audio classification on the 35 available words and speaker identification of 388 speakers. Model performance (accuracy) is reported on the corresponding test set, any hyperparameter tuning and layer selection is based on the validation set. Fine-tuning is done for the full models (i.e., all layers) and pruned models (i.e., only up to a certain layer), based on the experiments conducted (see <a href="#S2.SS4" title="2.4 Experiments ‚Ä£ 2 Methods ‚Ä£ Convexity based pruning of speech representation models" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection¬†2.4</span></a>). Hyperparameters are identical for fine-tuning of all comparable models to minimize co-variates for the analysis. All models are trained with a batch size of 32 for a maximum of 18000 steps with early stopping (based on validation accuracy) and a cross-entropy loss. The learning rate is <math id="S2.SS3.p3.1.m1.1" class="ltx_Math" alttext="2\times 10^{-5}" display="inline"><semantics id="S2.SS3.p3.1.m1.1a"><mrow id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml"><mn id="S2.SS3.p3.1.m1.1.1.2" xref="S2.SS3.p3.1.m1.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.p3.1.m1.1.1.1" xref="S2.SS3.p3.1.m1.1.1.1.cmml">√ó</mo><msup id="S2.SS3.p3.1.m1.1.1.3" xref="S2.SS3.p3.1.m1.1.1.3.cmml"><mn id="S2.SS3.p3.1.m1.1.1.3.2" xref="S2.SS3.p3.1.m1.1.1.3.2.cmml">10</mn><mrow id="S2.SS3.p3.1.m1.1.1.3.3" xref="S2.SS3.p3.1.m1.1.1.3.3.cmml"><mo id="S2.SS3.p3.1.m1.1.1.3.3a" xref="S2.SS3.p3.1.m1.1.1.3.3.cmml">‚àí</mo><mn id="S2.SS3.p3.1.m1.1.1.3.3.2" xref="S2.SS3.p3.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><apply id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1"><times id="S2.SS3.p3.1.m1.1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1.1"></times><cn type="integer" id="S2.SS3.p3.1.m1.1.1.2.cmml" xref="S2.SS3.p3.1.m1.1.1.2">2</cn><apply id="S2.SS3.p3.1.m1.1.1.3.cmml" xref="S2.SS3.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p3.1.m1.1.1.3.1.cmml" xref="S2.SS3.p3.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S2.SS3.p3.1.m1.1.1.3.2.cmml" xref="S2.SS3.p3.1.m1.1.1.3.2">10</cn><apply id="S2.SS3.p3.1.m1.1.1.3.3.cmml" xref="S2.SS3.p3.1.m1.1.1.3.3"><minus id="S2.SS3.p3.1.m1.1.1.3.3.1.cmml" xref="S2.SS3.p3.1.m1.1.1.3.3"></minus><cn type="integer" id="S2.SS3.p3.1.m1.1.1.3.3.2.cmml" xref="S2.SS3.p3.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">2\times 10^{-5}</annotation></semantics></math> for full models and <math id="S2.SS3.p3.2.m2.1" class="ltx_Math" alttext="5\times 10^{-5}" display="inline"><semantics id="S2.SS3.p3.2.m2.1a"><mrow id="S2.SS3.p3.2.m2.1.1" xref="S2.SS3.p3.2.m2.1.1.cmml"><mn id="S2.SS3.p3.2.m2.1.1.2" xref="S2.SS3.p3.2.m2.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.p3.2.m2.1.1.1" xref="S2.SS3.p3.2.m2.1.1.1.cmml">√ó</mo><msup id="S2.SS3.p3.2.m2.1.1.3" xref="S2.SS3.p3.2.m2.1.1.3.cmml"><mn id="S2.SS3.p3.2.m2.1.1.3.2" xref="S2.SS3.p3.2.m2.1.1.3.2.cmml">10</mn><mrow id="S2.SS3.p3.2.m2.1.1.3.3" xref="S2.SS3.p3.2.m2.1.1.3.3.cmml"><mo id="S2.SS3.p3.2.m2.1.1.3.3a" xref="S2.SS3.p3.2.m2.1.1.3.3.cmml">‚àí</mo><mn id="S2.SS3.p3.2.m2.1.1.3.3.2" xref="S2.SS3.p3.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.2.m2.1b"><apply id="S2.SS3.p3.2.m2.1.1.cmml" xref="S2.SS3.p3.2.m2.1.1"><times id="S2.SS3.p3.2.m2.1.1.1.cmml" xref="S2.SS3.p3.2.m2.1.1.1"></times><cn type="integer" id="S2.SS3.p3.2.m2.1.1.2.cmml" xref="S2.SS3.p3.2.m2.1.1.2">5</cn><apply id="S2.SS3.p3.2.m2.1.1.3.cmml" xref="S2.SS3.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p3.2.m2.1.1.3.1.cmml" xref="S2.SS3.p3.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S2.SS3.p3.2.m2.1.1.3.2.cmml" xref="S2.SS3.p3.2.m2.1.1.3.2">10</cn><apply id="S2.SS3.p3.2.m2.1.1.3.3.cmml" xref="S2.SS3.p3.2.m2.1.1.3.3"><minus id="S2.SS3.p3.2.m2.1.1.3.3.1.cmml" xref="S2.SS3.p3.2.m2.1.1.3.3"></minus><cn type="integer" id="S2.SS3.p3.2.m2.1.1.3.3.2.cmml" xref="S2.SS3.p3.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.2.m2.1c">5\times 10^{-5}</annotation></semantics></math> for pruned models for the word classification, and <math id="S2.SS3.p3.3.m3.1" class="ltx_Math" alttext="4\times 10^{-5}" display="inline"><semantics id="S2.SS3.p3.3.m3.1a"><mrow id="S2.SS3.p3.3.m3.1.1" xref="S2.SS3.p3.3.m3.1.1.cmml"><mn id="S2.SS3.p3.3.m3.1.1.2" xref="S2.SS3.p3.3.m3.1.1.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.p3.3.m3.1.1.1" xref="S2.SS3.p3.3.m3.1.1.1.cmml">√ó</mo><msup id="S2.SS3.p3.3.m3.1.1.3" xref="S2.SS3.p3.3.m3.1.1.3.cmml"><mn id="S2.SS3.p3.3.m3.1.1.3.2" xref="S2.SS3.p3.3.m3.1.1.3.2.cmml">10</mn><mrow id="S2.SS3.p3.3.m3.1.1.3.3" xref="S2.SS3.p3.3.m3.1.1.3.3.cmml"><mo id="S2.SS3.p3.3.m3.1.1.3.3a" xref="S2.SS3.p3.3.m3.1.1.3.3.cmml">‚àí</mo><mn id="S2.SS3.p3.3.m3.1.1.3.3.2" xref="S2.SS3.p3.3.m3.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.3.m3.1b"><apply id="S2.SS3.p3.3.m3.1.1.cmml" xref="S2.SS3.p3.3.m3.1.1"><times id="S2.SS3.p3.3.m3.1.1.1.cmml" xref="S2.SS3.p3.3.m3.1.1.1"></times><cn type="integer" id="S2.SS3.p3.3.m3.1.1.2.cmml" xref="S2.SS3.p3.3.m3.1.1.2">4</cn><apply id="S2.SS3.p3.3.m3.1.1.3.cmml" xref="S2.SS3.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p3.3.m3.1.1.3.1.cmml" xref="S2.SS3.p3.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="S2.SS3.p3.3.m3.1.1.3.2.cmml" xref="S2.SS3.p3.3.m3.1.1.3.2">10</cn><apply id="S2.SS3.p3.3.m3.1.1.3.3.cmml" xref="S2.SS3.p3.3.m3.1.1.3.3"><minus id="S2.SS3.p3.3.m3.1.1.3.3.1.cmml" xref="S2.SS3.p3.3.m3.1.1.3.3"></minus><cn type="integer" id="S2.SS3.p3.3.m3.1.1.3.3.2.cmml" xref="S2.SS3.p3.3.m3.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.3.m3.1c">4\times 10^{-5}</annotation></semantics></math> for full models and <math id="S2.SS3.p3.4.m4.1" class="ltx_Math" alttext="7\times 10^{-5}" display="inline"><semantics id="S2.SS3.p3.4.m4.1a"><mrow id="S2.SS3.p3.4.m4.1.1" xref="S2.SS3.p3.4.m4.1.1.cmml"><mn id="S2.SS3.p3.4.m4.1.1.2" xref="S2.SS3.p3.4.m4.1.1.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.p3.4.m4.1.1.1" xref="S2.SS3.p3.4.m4.1.1.1.cmml">√ó</mo><msup id="S2.SS3.p3.4.m4.1.1.3" xref="S2.SS3.p3.4.m4.1.1.3.cmml"><mn id="S2.SS3.p3.4.m4.1.1.3.2" xref="S2.SS3.p3.4.m4.1.1.3.2.cmml">10</mn><mrow id="S2.SS3.p3.4.m4.1.1.3.3" xref="S2.SS3.p3.4.m4.1.1.3.3.cmml"><mo id="S2.SS3.p3.4.m4.1.1.3.3a" xref="S2.SS3.p3.4.m4.1.1.3.3.cmml">‚àí</mo><mn id="S2.SS3.p3.4.m4.1.1.3.3.2" xref="S2.SS3.p3.4.m4.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.4.m4.1b"><apply id="S2.SS3.p3.4.m4.1.1.cmml" xref="S2.SS3.p3.4.m4.1.1"><times id="S2.SS3.p3.4.m4.1.1.1.cmml" xref="S2.SS3.p3.4.m4.1.1.1"></times><cn type="integer" id="S2.SS3.p3.4.m4.1.1.2.cmml" xref="S2.SS3.p3.4.m4.1.1.2">7</cn><apply id="S2.SS3.p3.4.m4.1.1.3.cmml" xref="S2.SS3.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p3.4.m4.1.1.3.1.cmml" xref="S2.SS3.p3.4.m4.1.1.3">superscript</csymbol><cn type="integer" id="S2.SS3.p3.4.m4.1.1.3.2.cmml" xref="S2.SS3.p3.4.m4.1.1.3.2">10</cn><apply id="S2.SS3.p3.4.m4.1.1.3.3.cmml" xref="S2.SS3.p3.4.m4.1.1.3.3"><minus id="S2.SS3.p3.4.m4.1.1.3.3.1.cmml" xref="S2.SS3.p3.4.m4.1.1.3.3"></minus><cn type="integer" id="S2.SS3.p3.4.m4.1.1.3.3.2.cmml" xref="S2.SS3.p3.4.m4.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.4.m4.1c">7\times 10^{-5}</annotation></semantics></math> for pruned models for speaker identification. All models were trained successfully with these parameters and converged within the maximum steps.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Experiments</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">We perform three different sets of experiments. First, we investigate to what extent and where pretrained models form convex regions of relevant information. In the second set of experiments, we explore how fine-tuning to a certain downstream task changes the representations. Lastly, we use insights from the first two experiments to develop an informed pruning strategy and improve fine-tuning efficiency.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p"><span id="S2.SS4.p2.1.1" class="ltx_text ltx_font_bold">Convex Regions in Pretrained Models.</span> In the first experiment, we extract the latent representations of the three datasets (words, speakers, and phonemes) for all pretrained models after each transformer layer. We measure the convexity and compare how it develops across the models. Based on these results, we choose layers for pruning (see the last experiment).</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p"><span id="S2.SS4.p3.1.1" class="ltx_text ltx_font_bold">Change of Convexity during Fine-tuning.</span> Models are fine-tuned to two different downstream tasks: word classification and speaker identification (details are described in <a href="#S2.SS3" title="2.3 Models ‚Ä£ 2 Methods ‚Ä£ Convexity based pruning of speech representation models" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection¬†2.3</span></a>). After fine-tuning, we extract the latent representations of the word, phoneme, and speaker dataset and measure the convexity. We analyze how the convexity changes based on the task and compare convexity scores to downstream accuracies.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p"><span id="S2.SS4.p4.1.1" class="ltx_text ltx_font_bold">Model Pruning Based on Convexity.</span> For model pruning, we choose the layer with the <span id="S2.SS4.p4.1.2" class="ltx_text ltx_font_italic">best</span> latent representations in the pretrained model, the best layer is chosen by either the highest convexity score or if the score is not significantly increasing anymore for later layers. We then prune (i.e. delete) all following layers and fine-tune only the remaining layers. For each task, the new final layer is chosen based on the convexity scores of the relevant data (words for the word classification, speaker for the speaker identification), so we train different sizes of models for the different tasks. We compare the model size, downstream accuracy, training time and inference time. Training time is measured only for one training run, but under the same conditions. Inference time is measured 300 times for a random 1s audio input on the GPU. For comparison, we also prune the base models after each second layer to see how pruning at other layers influences the performance.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results and Discussion</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2408.11858/assets/figures/convexity_finetuned.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="357" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.8.1.1" class="ltx_text ltx_font_bold">Fig.¬†1</span>: </span>Convexity of latent representations of words, phonemes, and speakers. Evaluated for pretrained ( <svg id="S3.F1.4.pic1" class="ltx_picture" height="1" overflow="visible" version="1.1" width="17.16"><g transform="translate(0,1) matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 0 M 0 0 L 16.6 0" style="fill:none"></path></g></svg>) and fine-tuned models (word classification:  <svg id="S3.F1.5.pic2" class="ltx_picture" height="1" overflow="visible" version="1.1" width="17.16"><g transform="translate(0,1) matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28)" fill="#000000" stroke="#000000" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" stroke-width="0.4pt"><path d="M 0 0 M 0 0 L 16.6 0" style="fill:none"></path></g></svg>, speaker identification:  <svg id="S3.F1.6.pic3" class="ltx_picture" height="1" overflow="visible" version="1.1" width="17.16"><g transform="translate(0,1) matrix(1 0 0 -1 0 0) translate(0.28,0) translate(0,0.28)" fill="#000000" stroke="#000000" stroke-dasharray="0.4pt,2.0pt" stroke-dashoffset="0.0pt" stroke-width="0.4pt"><path d="M 0 0 M 0 0 L 16.6 0" style="fill:none"></path></g></svg>), for base models (upper row) and large models (lower row). Models fine-tuned for word classification show increased convexity for word and phoneme representations and decreased convexity for speaker representations, while models fine-tuned for speaker identification show increased convexity for speaker representations and reduced convexity for word and phoneme representations.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Convex Regions in Pretrained Models</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><a href="#S3.F1" title="Figure 1 ‚Ä£ 3 Results and Discussion ‚Ä£ Convexity based pruning of speech representation models" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure¬†1</span></a> shows convexity scores for all models, pretrained models are depicted as full lines. The convexity of words increases across the layers (for all models except wav2vec2) while the convexity of speakers generally decreases. The convexity for phonemes stays relatively stable across all layers and is lower than the convexity of words and speakers. The wav2vec2 model has a distinctively different development of the convexity scores compared to all other models. The convexity of words follows an autoencoder-like development, where the convexity increases until layer 8 and then decreases back to levels of early layers. It also has a second peak in speaker convexity in later layers, that other models do not express.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Very similar behavior for word-content encoding was also observed by Pasad et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, where especially the distinct auto-encoder-like behavior for words was also observed for wav2vec2 but not for other models. They argue this behavior is caused by different pre-training objectives; in wav2vec2 the objective is to predict acoustic tokens, while the others predict more abstract semantic cluster IDs.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Change of Convexity during Fine-tuning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">We fine-tuned models for word classification and speaker identification, all results can be seen in the exact accuracies for all models can be seen in <a href="#S3.T1" title="Table 1 ‚Ä£ 3.3 Model Pruning Based on Convexity ‚Ä£ 3 Results and Discussion ‚Ä£ Convexity based pruning of speech representation models" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table¬†1</span></a>. The fine-tuned models for word classification all achieve test accuracy higher than <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="97\%" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">97</mn><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">97</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">97\%</annotation></semantics></math> and very similar convexity scores in the later layers. The fine-tuned models for speaker identification all achieve test accuracies of more than <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="93\%" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mn id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">93</mn><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">93</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">93\%</annotation></semantics></math>, except for HuBERT large which only has 69.64%. This is also reflected in the convexity scores which are lower for HuBERT large (see <a href="#S3.F1" title="Figure 1 ‚Ä£ 3 Results and Discussion ‚Ä£ Convexity based pruning of speech representation models" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure¬†1</span></a>). We generally observe that the higher the convexity score in the last layer is, the higher the performance. A similar relation between accuracy and convexity was also observed by Tƒõtkov√° et al. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The fine-tuned models for word classification and speaker identification show very distinct behaviors in their representation space, as shown in <a href="#S3.F1" title="Figure 1 ‚Ä£ 3 Results and Discussion ‚Ä£ Convexity based pruning of speech representation models" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure¬†1</span></a>. As one would expect, the convexity for the relevant classes increases significantly and is close to 1 for the later layers in all models that have an accuracy higher than <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="95\%" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">95</mn><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">95\%</annotation></semantics></math>. The convexity for the non-related class decreases drastically. The convexity of phonemes increases for the word classification model and decreases for speaker identification, which aligns with the expected development as phoneme information can be used for word classification but is not as useful for speaker identification.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We also observe that the relevant convexity scores converge to a high rate relatively early in the models, for word classification at about half the model and for speaker identification already after 4-5 layers. Based on these observations and the insights from the pretrained models, we hypothesize that later layers in the network may be redundant and not needed for the actual downstream task. We, therefore, conduct the last experiment of pruning the network and only fine-tuning the crucial layers.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model Pruning Based on Convexity</h3>

<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.8.1.1" class="ltx_tr">
<th id="S3.T1.8.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_rr"></th>
<td id="S3.T1.8.1.1.2" class="ltx_td"></td>
<td id="S3.T1.8.1.1.3" class="ltx_td ltx_align_center"><span id="S3.T1.8.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">word</span></td>
<td id="S3.T1.8.1.1.4" class="ltx_td ltx_align_center"><span id="S3.T1.8.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">class.</span></td>
<td id="S3.T1.8.1.1.5" class="ltx_td"></td>
<td id="S3.T1.8.1.1.6" class="ltx_td ltx_border_rr"></td>
<td id="S3.T1.8.1.1.7" class="ltx_td"></td>
<td id="S3.T1.8.1.1.8" class="ltx_td ltx_align_center"><span id="S3.T1.8.1.1.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">speaker</span></td>
<td id="S3.T1.8.1.1.9" class="ltx_td ltx_align_center"><span id="S3.T1.8.1.1.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">ident.</span></td>
<td id="S3.T1.8.1.1.10" class="ltx_td"></td>
<td id="S3.T1.8.1.1.11" class="ltx_td"></td>
</tr>
<tr id="S3.T1.8.2.2" class="ltx_tr">
<th id="S3.T1.8.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_rr ltx_border_tt"></th>
<td id="S3.T1.8.2.2.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.2.2.2.1" class="ltx_text" style="font-size:90%;">Full</span></td>
<td id="S3.T1.8.2.2.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.2.2.3.1" class="ltx_text" style="font-size:90%;">Pruned</span></td>
<td id="S3.T1.8.2.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.2.2.4.1" class="ltx_text" style="font-size:90%;">Acc</span></td>
<td id="S3.T1.8.2.2.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.2.2.5.1" class="ltx_text" style="font-size:90%;">Train time</span></td>
<td id="S3.T1.8.2.2.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><span id="S3.T1.8.2.2.6.1" class="ltx_text" style="font-size:90%;">Inference</span></td>
<td id="S3.T1.8.2.2.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.2.2.7.1" class="ltx_text" style="font-size:90%;">Full</span></td>
<td id="S3.T1.8.2.2.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.2.2.8.1" class="ltx_text" style="font-size:90%;">Pruned</span></td>
<td id="S3.T1.8.2.2.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.2.2.9.1" class="ltx_text" style="font-size:90%;">Acc</span></td>
<td id="S3.T1.8.2.2.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.2.2.10.1" class="ltx_text" style="font-size:90%;">Train time</span></td>
<td id="S3.T1.8.2.2.11" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.2.2.11.1" class="ltx_text" style="font-size:90%;">Inference</span></td>
</tr>
<tr id="S3.T1.8.3.3" class="ltx_tr">
<th id="S3.T1.8.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr"><span id="S3.T1.8.3.3.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<td id="S3.T1.8.3.3.2" class="ltx_td ltx_align_center"><span id="S3.T1.8.3.3.2.1" class="ltx_text" style="font-size:90%;">Model</span></td>
<td id="S3.T1.8.3.3.3" class="ltx_td ltx_align_center"><span id="S3.T1.8.3.3.3.1" class="ltx_text" style="font-size:90%;">Model</span></td>
<td id="S3.T1.8.3.3.4" class="ltx_td ltx_align_center"><span id="S3.T1.8.3.3.4.1" class="ltx_text" style="font-size:90%;">diff. (%)</span></td>
<td id="S3.T1.8.3.3.5" class="ltx_td ltx_align_center"><span id="S3.T1.8.3.3.5.1" class="ltx_text" style="font-size:90%;">diff. (%)</span></td>
<td id="S3.T1.8.3.3.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.T1.8.3.3.6.1" class="ltx_text" style="font-size:90%;">diff. (%)</span></td>
<td id="S3.T1.8.3.3.7" class="ltx_td ltx_align_center"><span id="S3.T1.8.3.3.7.1" class="ltx_text" style="font-size:90%;">Model</span></td>
<td id="S3.T1.8.3.3.8" class="ltx_td ltx_align_center"><span id="S3.T1.8.3.3.8.1" class="ltx_text" style="font-size:90%;">Model</span></td>
<td id="S3.T1.8.3.3.9" class="ltx_td ltx_align_center"><span id="S3.T1.8.3.3.9.1" class="ltx_text" style="font-size:90%;">diff. (%)</span></td>
<td id="S3.T1.8.3.3.10" class="ltx_td ltx_align_center"><span id="S3.T1.8.3.3.10.1" class="ltx_text" style="font-size:90%;">diff. (%)</span></td>
<td id="S3.T1.8.3.3.11" class="ltx_td ltx_align_center"><span id="S3.T1.8.3.3.11.1" class="ltx_text" style="font-size:90%;">diff. (%)</span></td>
</tr>
<tr id="S3.T1.8.4.4" class="ltx_tr">
<th id="S3.T1.8.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_tt"><span id="S3.T1.8.4.4.1.1" class="ltx_text" style="font-size:90%;">wav2vec2</span></th>
<td id="S3.T1.8.4.4.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.4.4.2.1" class="ltx_text" style="font-size:90%;">98.32</span></td>
<td id="S3.T1.8.4.4.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.4.4.3.1" class="ltx_text" style="font-size:90%;">98.43</span></td>
<td id="S3.T1.8.4.4.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.4.4.4.1" class="ltx_text" style="font-size:90%;">+ 0.11</span></td>
<td id="S3.T1.8.4.4.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.4.4.5.1" class="ltx_text" style="font-size:90%;">- 15.88</span></td>
<td id="S3.T1.8.4.4.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><span id="S3.T1.8.4.4.6.1" class="ltx_text" style="font-size:90%;">- 22. 73</span></td>
<td id="S3.T1.8.4.4.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.4.4.7.1" class="ltx_text" style="font-size:90%;">95.17</span></td>
<td id="S3.T1.8.4.4.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.4.4.8.1" class="ltx_text" style="font-size:90%;">97.79</span></td>
<td id="S3.T1.8.4.4.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.4.4.9.1" class="ltx_text" style="font-size:90%;">+ 2.75</span></td>
<td id="S3.T1.8.4.4.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.4.4.10.1" class="ltx_text" style="font-size:90%;">- 38.33</span></td>
<td id="S3.T1.8.4.4.11" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.4.4.11.1" class="ltx_text" style="font-size:90%;">- 53.25</span></td>
</tr>
<tr id="S3.T1.8.5.5" class="ltx_tr">
<th id="S3.T1.8.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr"><span id="S3.T1.8.5.5.1.1" class="ltx_text" style="font-size:90%;">large</span></th>
<td id="S3.T1.8.5.5.2" class="ltx_td ltx_align_center"><span id="S3.T1.8.5.5.2.1" class="ltx_text" style="font-size:90%;">97.21</span></td>
<td id="S3.T1.8.5.5.3" class="ltx_td ltx_align_center"><span id="S3.T1.8.5.5.3.1" class="ltx_text" style="font-size:90%;">96.92</span></td>
<td id="S3.T1.8.5.5.4" class="ltx_td ltx_align_center"><span id="S3.T1.8.5.5.4.1" class="ltx_text" style="font-size:90%;">- 0.20</span></td>
<td id="S3.T1.8.5.5.5" class="ltx_td ltx_align_center"><span id="S3.T1.8.5.5.5.1" class="ltx_text" style="font-size:90%;">- 37.48</span></td>
<td id="S3.T1.8.5.5.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.T1.8.5.5.6.1" class="ltx_text" style="font-size:90%;">- 35.42</span></td>
<td id="S3.T1.8.5.5.7" class="ltx_td ltx_align_center"><span id="S3.T1.8.5.5.7.1" class="ltx_text" style="font-size:90%;">98.52</span></td>
<td id="S3.T1.8.5.5.8" class="ltx_td ltx_align_center"><span id="S3.T1.8.5.5.8.1" class="ltx_text" style="font-size:90%;">98.68</span></td>
<td id="S3.T1.8.5.5.9" class="ltx_td ltx_align_center"><span id="S3.T1.8.5.5.9.1" class="ltx_text" style="font-size:90%;">+ 0.16</span></td>
<td id="S3.T1.8.5.5.10" class="ltx_td ltx_align_center"><span id="S3.T1.8.5.5.10.1" class="ltx_text" style="font-size:90%;">- 59.08</span></td>
<td id="S3.T1.8.5.5.11" class="ltx_td ltx_align_center"><span id="S3.T1.8.5.5.11.1" class="ltx_text" style="font-size:90%;">- 66.64</span></td>
</tr>
<tr id="S3.T1.8.6.6" class="ltx_tr">
<th id="S3.T1.8.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t"><span id="S3.T1.8.6.6.1.1" class="ltx_text" style="font-size:90%;">wavLM</span></th>
<td id="S3.T1.8.6.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.6.6.2.1" class="ltx_text" style="font-size:90%;">97.22</span></td>
<td id="S3.T1.8.6.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.6.6.3.1" class="ltx_text" style="font-size:90%;">96.96</span></td>
<td id="S3.T1.8.6.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.6.6.4.1" class="ltx_text" style="font-size:90%;">- 0.25</span></td>
<td id="S3.T1.8.6.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.6.6.5.1" class="ltx_text" style="font-size:90%;">- 17.33</span></td>
<td id="S3.T1.8.6.6.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.T1.8.6.6.6.1" class="ltx_text" style="font-size:90%;">- 22.32</span></td>
<td id="S3.T1.8.6.6.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.6.6.7.1" class="ltx_text" style="font-size:90%;">93.18</span></td>
<td id="S3.T1.8.6.6.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.6.6.8.1" class="ltx_text" style="font-size:90%;">96.27</span></td>
<td id="S3.T1.8.6.6.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.6.6.9.1" class="ltx_text" style="font-size:90%;">+ 3.38</span></td>
<td id="S3.T1.8.6.6.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.6.6.10.1" class="ltx_text" style="font-size:90%;">- 45.30</span></td>
<td id="S3.T1.8.6.6.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.6.6.11.1" class="ltx_text" style="font-size:90%;">- 55.10</span></td>
</tr>
<tr id="S3.T1.8.7.7" class="ltx_tr">
<th id="S3.T1.8.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr"><span id="S3.T1.8.7.7.1.1" class="ltx_text" style="font-size:90%;">large</span></th>
<td id="S3.T1.8.7.7.2" class="ltx_td ltx_align_center"><span id="S3.T1.8.7.7.2.1" class="ltx_text" style="font-size:90%;">98.86</span></td>
<td id="S3.T1.8.7.7.3" class="ltx_td ltx_align_center"><span id="S3.T1.8.7.7.3.1" class="ltx_text" style="font-size:90%;">98.66</span></td>
<td id="S3.T1.8.7.7.4" class="ltx_td ltx_align_center"><span id="S3.T1.8.7.7.4.1" class="ltx_text" style="font-size:90%;">- 0.20</span></td>
<td id="S3.T1.8.7.7.5" class="ltx_td ltx_align_center"><span id="S3.T1.8.7.7.5.1" class="ltx_text" style="font-size:90%;">- 25.64</span></td>
<td id="S3.T1.8.7.7.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.T1.8.7.7.6.1" class="ltx_text" style="font-size:90%;">- 30.36</span></td>
<td id="S3.T1.8.7.7.7" class="ltx_td ltx_align_center"><span id="S3.T1.8.7.7.7.1" class="ltx_text" style="font-size:90%;">97.14</span></td>
<td id="S3.T1.8.7.7.8" class="ltx_td ltx_align_center"><span id="S3.T1.8.7.7.8.1" class="ltx_text" style="font-size:90%;">97.53</span></td>
<td id="S3.T1.8.7.7.9" class="ltx_td ltx_align_center"><span id="S3.T1.8.7.7.9.1" class="ltx_text" style="font-size:90%;">+ 0.40</span></td>
<td id="S3.T1.8.7.7.10" class="ltx_td ltx_align_center"><span id="S3.T1.8.7.7.10.1" class="ltx_text" style="font-size:90%;">- 66.93</span></td>
<td id="S3.T1.8.7.7.11" class="ltx_td ltx_align_center"><span id="S3.T1.8.7.7.11.1" class="ltx_text" style="font-size:90%;">- 64.73</span></td>
</tr>
<tr id="S3.T1.8.8.8" class="ltx_tr">
<th id="S3.T1.8.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t"><span id="S3.T1.8.8.8.1.1" class="ltx_text" style="font-size:90%;">HuBERT</span></th>
<td id="S3.T1.8.8.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.8.8.2.1" class="ltx_text" style="font-size:90%;">97.43</span></td>
<td id="S3.T1.8.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.8.8.3.1" class="ltx_text" style="font-size:90%;">96.65</span></td>
<td id="S3.T1.8.8.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.8.8.4.1" class="ltx_text" style="font-size:90%;">- 0.80</span></td>
<td id="S3.T1.8.8.8.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.8.8.5.1" class="ltx_text" style="font-size:90%;">- 5.08</span></td>
<td id="S3.T1.8.8.8.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.T1.8.8.8.6.1" class="ltx_text" style="font-size:90%;">- 18.64</span></td>
<td id="S3.T1.8.8.8.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.8.8.7.1" class="ltx_text" style="font-size:90%;">94.36</span></td>
<td id="S3.T1.8.8.8.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.8.8.8.1" class="ltx_text" style="font-size:90%;">92.43</span></td>
<td id="S3.T1.8.8.8.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.8.8.9.1" class="ltx_text" style="font-size:90%;">- 1.80</span></td>
<td id="S3.T1.8.8.8.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.8.8.10.1" class="ltx_text" style="font-size:90%;">- 44.98</span></td>
<td id="S3.T1.8.8.8.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.8.8.11.1" class="ltx_text" style="font-size:90%;">- 52.99</span></td>
</tr>
<tr id="S3.T1.8.9.9" class="ltx_tr">
<th id="S3.T1.8.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr"><span id="S3.T1.8.9.9.1.1" class="ltx_text" style="font-size:90%;">large</span></th>
<td id="S3.T1.8.9.9.2" class="ltx_td ltx_align_center"><span id="S3.T1.8.9.9.2.1" class="ltx_text" style="font-size:90%;">98.39</span></td>
<td id="S3.T1.8.9.9.3" class="ltx_td ltx_align_center"><span id="S3.T1.8.9.9.3.1" class="ltx_text" style="font-size:90%;">98.83</span></td>
<td id="S3.T1.8.9.9.4" class="ltx_td ltx_align_center"><span id="S3.T1.8.9.9.4.1" class="ltx_text" style="font-size:90%;">+ 0.45</span></td>
<td id="S3.T1.8.9.9.5" class="ltx_td ltx_align_center"><span id="S3.T1.8.9.9.5.1" class="ltx_text" style="font-size:90%;">-17.49</span></td>
<td id="S3.T1.8.9.9.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S3.T1.8.9.9.6.1" class="ltx_text" style="font-size:90%;">-30.35</span></td>
<td id="S3.T1.8.9.9.7" class="ltx_td ltx_align_center"><span id="S3.T1.8.9.9.7.1" class="ltx_text" style="font-size:90%;">69.64</span></td>
<td id="S3.T1.8.9.9.8" class="ltx_td ltx_align_center"><span id="S3.T1.8.9.9.8.1" class="ltx_text" style="font-size:90%;">96.36</span></td>
<td id="S3.T1.8.9.9.9" class="ltx_td ltx_align_center"><span id="S3.T1.8.9.9.9.1" class="ltx_text" style="font-size:90%;">+ 38.37*</span></td>
<td id="S3.T1.8.9.9.10" class="ltx_td ltx_align_center"><span id="S3.T1.8.9.9.10.1" class="ltx_text" style="font-size:90%;">- 58.89</span></td>
<td id="S3.T1.8.9.9.11" class="ltx_td ltx_align_center"><span id="S3.T1.8.9.9.11.1" class="ltx_text" style="font-size:90%;">- 65.94</span></td>
</tr>
<tr id="S3.T1.8.10.10" class="ltx_tr">
<th id="S3.T1.8.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t"><span id="S3.T1.8.10.10.1.1" class="ltx_text" style="font-size:90%;">ccc-wav2vec2</span></th>
<td id="S3.T1.8.10.10.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.10.10.2.1" class="ltx_text" style="font-size:90%;">97.61</span></td>
<td id="S3.T1.8.10.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.10.10.3.1" class="ltx_text" style="font-size:90%;">96.65</span></td>
<td id="S3.T1.8.10.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.10.10.4.1" class="ltx_text" style="font-size:90%;">- 0.98</span></td>
<td id="S3.T1.8.10.10.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.10.10.5.1" class="ltx_text" style="font-size:90%;">- 45.09</span></td>
<td id="S3.T1.8.10.10.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.T1.8.10.10.6.1" class="ltx_text" style="font-size:90%;">-21.81</span></td>
<td id="S3.T1.8.10.10.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.10.10.7.1" class="ltx_text" style="font-size:90%;">96.20</span></td>
<td id="S3.T1.8.10.10.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.10.10.8.1" class="ltx_text" style="font-size:90%;">98.55</span></td>
<td id="S3.T1.8.10.10.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.10.10.9.1" class="ltx_text" style="font-size:90%;">+ 2.43</span></td>
<td id="S3.T1.8.10.10.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.10.10.10.1" class="ltx_text" style="font-size:90%;">- 21.38</span></td>
<td id="S3.T1.8.10.10.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.8.10.10.11.1" class="ltx_text" style="font-size:90%;">-52.64</span></td>
</tr>
<tr id="S3.T1.8.11.11" class="ltx_tr">
<th id="S3.T1.8.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_tt"><span id="S3.T1.8.11.11.1.1" class="ltx_text" style="font-size:90%;">Average</span></th>
<td id="S3.T1.8.11.11.2" class="ltx_td ltx_border_tt"></td>
<td id="S3.T1.8.11.11.3" class="ltx_td ltx_border_tt"></td>
<td id="S3.T1.8.11.11.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.11.11.4.1" class="ltx_text" style="font-size:90%;">-0.25</span></td>
<td id="S3.T1.8.11.11.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.11.11.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">-23.42</span></td>
<td id="S3.T1.8.11.11.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><span id="S3.T1.8.11.11.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">-25.9</span></td>
<td id="S3.T1.8.11.11.7" class="ltx_td ltx_border_tt"></td>
<td id="S3.T1.8.11.11.8" class="ltx_td ltx_border_tt"></td>
<td id="S3.T1.8.11.11.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.11.11.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">+1.22</span></td>
<td id="S3.T1.8.11.11.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.11.11.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">-47.84</span></td>
<td id="S3.T1.8.11.11.11" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.8.11.11.11.1" class="ltx_text ltx_font_bold" style="font-size:90%;">-58.75</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.14.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Accuracies (in %, <math id="S3.T1.4.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.T1.4.m1.1b"><mi id="S3.T1.4.m1.1.1" xref="S3.T1.4.m1.1.1.cmml">œÉ</mi><annotation-xml encoding="MathML-Content" id="S3.T1.4.m1.1c"><ci id="S3.T1.4.m1.1.1.cmml" xref="S3.T1.4.m1.1.1">ùúé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.m1.1d">\sigma</annotation></semantics></math>=0.15-0.32%) and differences in train time and inference time (<math id="S3.T1.5.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.T1.5.m2.1b"><mi id="S3.T1.5.m2.1.1" xref="S3.T1.5.m2.1.1.cmml">œÉ</mi><annotation-xml encoding="MathML-Content" id="S3.T1.5.m2.1c"><ci id="S3.T1.5.m2.1.1.cmml" xref="S3.T1.5.m2.1.1">ùúé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.m2.1d">\sigma</annotation></semantics></math>=0.9-1.1%) for word classification and speaker identification for full and pruned fine-tuned models. For words classification, the base models are pruned to 8 layers and the large models are pruned to 15 layers. For speaker identification, the base models are pruned to 2 layers and the large models are pruned to 4 layers. On average, the accuracy for word classification decreased by only 0.25% (<math id="S3.T1.6.m3.1" class="ltx_Math" alttext="\approxeq\sigma" display="inline"><semantics id="S3.T1.6.m3.1b"><mrow id="S3.T1.6.m3.1.1" xref="S3.T1.6.m3.1.1.cmml"><mi id="S3.T1.6.m3.1.1.2" xref="S3.T1.6.m3.1.1.2.cmml"></mi><mo id="S3.T1.6.m3.1.1.1" xref="S3.T1.6.m3.1.1.1.cmml">‚âä</mo><mi id="S3.T1.6.m3.1.1.3" xref="S3.T1.6.m3.1.1.3.cmml">œÉ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.6.m3.1c"><apply id="S3.T1.6.m3.1.1.cmml" xref="S3.T1.6.m3.1.1"><csymbol cd="latexml" id="S3.T1.6.m3.1.1.1.cmml" xref="S3.T1.6.m3.1.1.1">approximately-equals-or-equals</csymbol><csymbol cd="latexml" id="S3.T1.6.m3.1.1.2.cmml" xref="S3.T1.6.m3.1.1.2">absent</csymbol><ci id="S3.T1.6.m3.1.1.3.cmml" xref="S3.T1.6.m3.1.1.3">ùúé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.m3.1d">\approxeq\sigma</annotation></semantics></math>), while the accuracy for speaker identification increased by 1.22% (*¬†excluded). The training time and inference time was significantly reduced in both cases.</figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We perform static layer-wise pruning on the pretrained models based on the convexity scores for relevant data. We simply delete all transformer layers after the layer with the best convexity score or after which layer the score is not significantly improving anymore. Interestingly, it turns out to be the same layer for all models. For word classification, all base models (12 layers) are pruned to 8 layers and large models (24 layers) to 15 layers. This corresponds to a parameter reduction of 22.48% and 31.91% respectively. For speaker identification, the base models are pruned to 2 layers and the large models are pruned to 4 layers. This corresponds to a parameter reduction of 67.38% and 75.78% respectively. The pruned models are then fine-tuned for the downstream tasks.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">For the word classification task, the test set accuracies of the pruned model are very comparable to the full model, the exact accuracies are shown in <a href="#S3.T1" title="Table 1 ‚Ä£ 3.3 Model Pruning Based on Convexity ‚Ä£ 3 Results and Discussion ‚Ä£ Convexity based pruning of speech representation models" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table¬†1</span></a>. Only a slight decrease of 0.25% was observed on average, while the training time could be reduced by on average 20% and inference time by 25%. For the speaker identification task, the pruned models even outperformed the full models on average by <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="1.2\%" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mn id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">1.2</mn><mo id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">1.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">1.2\%</annotation></semantics></math>. The training time was reduced to almost half (by 48% on average) and inference time was reduced by almost 60%. This clearly shows, how a better understanding of the latent space representations can help with making informed decisions on which layers are useful for certain tasks and lead not only to smaller and more efficient models but even to better performance.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2408.11858/assets/figures/pruning.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="343" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.4.1.1" class="ltx_text ltx_font_bold">Fig.¬†2</span>: </span>Accuracies for word classification of the pruned base models (number of layers denoted for each point) vs. the convexity score for words for that layer in the pre-trained model. The best performing pruned model is marked with <math id="S3.F2.2.m1.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="S3.F2.2.m1.1b"><mi mathvariant="normal" id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml">‚òÖ</mi><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.1c"><ci id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1">‚òÖ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.1d">\bigstar</annotation></semantics></math>, which is layer 8 for all models except ccc-wav2vec2.</figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">We also prune the base models at every second layer and train it for the word classification task to investigate if the best layer based on the convexity score is actually the best layer to prune the model. The results in <a href="#S3.F2" title="Figure 2 ‚Ä£ 3.3 Model Pruning Based on Convexity ‚Ä£ 3 Results and Discussion ‚Ä£ Convexity based pruning of speech representation models" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure¬†2</span></a> show a clear relation between the convexity score of a certain layer and accuracy after pruning to that layer. For three models the best performing pruned model is in fact the one pruned to 8 layers, where the convexity score is also the highest or not significantly increasing for later layers anymore. Eventhough layer 10 is the highest convexity layer for wavLM, HuBERT and ccc-wav2vec2, only for ccc-wav2vec2 the model pruned to 10 layers performs slightly better than the one pruned to 8 layers. This shows, that using the layer after which the convexity doesn‚Äôt increase substantially anymore instead of using the actually highest convexity layer can be a good strategy as the smaller models have a better/comparable performance. The models pruned to less layers show a slight decrease in performance, however, the accuracy is still 96-98% for pruning at layers 4, 6 and is still above 92% for pruning at layer 2 and 0. Even removing all transformer layers and only using the feature extractor still leads to a performance of 70-80% for all models. This also shows that most of the model can easily be pruned without much of a performance decrease. We omit this experiment for other model and task combinations to save computational resources. Yet, the results for speaker identification in <a href="#S3.T1" title="Table 1 ‚Ä£ 3.3 Model Pruning Based on Convexity ‚Ä£ 3 Results and Discussion ‚Ä£ Convexity based pruning of speech representation models" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table¬†1</span></a> support the conclusion that pruning at layers with higher convexity scores can actually improve performance, as the accuracy for speaker identification for all except one model significantly increased when using a high convexity layer instead of the full model.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Pasad et al.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> also observed a correlation between the word and phoneme content of the representations of each layer and performance on related tasks. In contrast to this work, they, however, only trained a linear classifier on each layer instead of actually pruning and fine-tuning models. Several other works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> have also found that earlier layers in models have more meaningful representations and that later layers are redundant. Some more complicated pruning strategies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> have also been applied to speech representation models (in particular wav2vec2) with comparable results in size reduction without accuracy decrease. However, these pruning strategies require more optimization and computation. Similar patterns in pruning were observed in NLP task, where performance was maintained within 1% while pruning up to 9 (of 12) layers and earlier layers were found to be the most critical ones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We investigated how convexity of decision regions emerges in self-supervised speech representation models. We find that word and speaker information is mostly encoded in convex regions in specific layers, located in the middle and the beginning of the network respectively. Fine-tuning to word classification and speaker identification increases convexity of the relevant features drastically while decreasing convexity of irrelevant features. Based on the observations of high convexity layers in pre-trained models and converged convexity in early layers of fine-tuned models, we propose that convexity can be used to design a layer pruning workflow, by simply dropping later layers in the transformer stack that do not add further convexity. In empirical investigations, we find massive reduction in the computational effort without loss of performance and interestingly, we found improvements in certain cases. Overall, we find that most transformer layers can be removed with limited or no loss in performance, highlighting the need for more critical view on large transformer models and encouraging the more thorough investigation of latent spaces of audio models to determine the critical and the redundant parts.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Acknowledgements</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work was supported by the Novo Nordisk Foundation grant NNF22OC0076907 ‚ÄùCognitive spaces - Next generation explainability‚Äù, the DIREC Bridge project Deep Learning and Automation of Imaging-Based Quality of Seeds and Grains, Innovation Fund Denmark grant number 9142-00001B and by the Pioneer Centre for AI, DNRF grant number P1.
<span id="S5.p1.1.1" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Abdelrahman Mohamed et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">‚ÄúSelf-supervised speech representation learning: A review,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Journal of Selected Topics in Signal Processing</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, vol. 16, no. 6, pp. 1179‚Äì1210, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Shu-Wen¬†Yang et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">‚ÄúSUPERB: speech processing universal performance benchmark,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">. 2021, pp. 1194‚Äì1198, ISCA.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Ankita Pasad, Ju-Chieh Chou, and Karen Livescu,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">‚ÄúLayer-wise analysis of a self-supervised speech representation model,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2021, pp. 914‚Äì921.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Ankita Pasad, Bowen Shi, and Karen Livescu,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">‚ÄúComparative layer-wise analysis of self-supervised speech models,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2023, pp. 1‚Äì5.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">‚ÄúPruning and quantization for deep neural network acceleration: A survey,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neurocomputing</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, vol. 461, pp. 370‚Äì403, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">‚ÄúOn the effect of dropping layers of pre-trained transformer models,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Speech &amp; Language</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, vol. 77, pp. 101429, 2023.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Angela Fan, Edouard Grave, and Armand Joulin,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">‚ÄúReducing transformer depth on demand with structured dropout,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Lenka Tƒõtkov√°, Thea Br√ºsch, Teresa Scheidt, Fabian Mager, Rasmus Aagaard, Jonathan Foldager, Tommy Alstr√∏m, and Lars¬†Kai Hansen,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">‚ÄúOn convex conceptual regions in deep network representations,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.17154</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
P.¬†Warden,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">‚ÄúSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv e-prints</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, Apr. 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
John¬†C Wells,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">‚ÄúComputer-coding the ipa: a proposed extension of sampa,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Revised draft</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, vol. 4, no. 28, pp. 1995, 1995.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Thomas Kisler, Uwe Reichel, and Florian Schiel,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">‚ÄúMultilingual processing of speech via web services,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Speech &amp; Language</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, vol. 45, pp. 326‚Äì347, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">‚Äúwav2vec 2.0: A framework for self-supervised learning of speech representations,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, vol. 33, pp. 12449‚Äì12460, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Sanyuan Chen et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">‚ÄúWavlm: Large-scale self-supervised pre-training for full stack speech processing,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Journal of Selected Topics in Signal Processing</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, vol. 16, no. 6, pp. 1505‚Äì1518, 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Wei-Ning Hsu et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">‚ÄúHubert: Self-supervised speech representation learning by masked prediction of hidden units,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, vol. 29, pp. 3451‚Äì3460, 2021.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Vasista¬†Sai Lodagala, Sreyan Ghosh, and Srinivasan Umesh,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">‚ÄúCcc-wav2vec 2.0: Clustering aided cross contrastive self-supervised learning of speech representations,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2022 IEEE Spoken Language Technology Workshop (SLT)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2023, pp. 1‚Äì8.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Thomas Wolf et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">‚ÄúTransformers: State-of-the-art natural language processing,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 38‚Äì45.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Ankita Pasad, Chung-Ming Chien, Shane Settle, and Karen Livescu,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">‚ÄúWhat do self-supervised speech models know about words?,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Transactions of the Association for Computational Linguistics</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, vol. 12, pp. 372‚Äì391, 2024.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Andy¬†T Liu, Shang-Wen Li, and Hung-yi Lee,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">‚ÄúTera: Self-supervised learning of transformer encoder representation for speech,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, vol. 29, pp. 2351‚Äì2366, 2021.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Emina Alickovic, Tobias Dorszewski, et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">‚ÄúPredicting eeg responses to attended speech via deep neural networks for speech,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2023 45th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2023, pp. 1‚Äì4.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, and Shinji Watanabe,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">‚ÄúStructured pruning of self-supervised pre-trained models for speech recognition and understanding,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2023, pp. 1‚Äì5.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Shucong Zhang, Erfan Loweimi, Peter Bell, and Steve Renals,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">‚ÄúOn the usefulness of self-attention for automatic speech recognition with transformers,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2021 IEEE Spoken Language Technology Workshop (SLT)</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2021, pp. 89‚Äì96.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.11857" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.11858" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.11858">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.11858" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.11859" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 16:07:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
