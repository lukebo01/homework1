<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.04007] Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition</title><meta property="og:description" content="Speech emotion recognition (SER) classifies human emotions in speech with a computer model.
Recently, performance in SER has steadily increased as deep learning techniques have adapted.
However, unlike many domains tha‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.04007">

<!--Generated on Sun Oct  6 00:07:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Byunggun Kim
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Younghun Kwon
</span><span class="ltx_author_notes">This work was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF), funded by the Ministry of Education, Science, and Technology (NRF2022R1F1A1064459). 
<br class="ltx_break">Byunggun Kim is with the Department of Applied Artificial Intelligence, Hanyang University(ERICA), Ansan, Kyunggi-Do, 425-791, Republic of Korea 
<br class="ltx_break">Younghun Kwon is with the Department of Applied Artificial Intelligence and Department of Applied Physics, Hanyang University (ERICA), Ansan, Kyunggi-Do, 425-791, Republic of Korea.

<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text">Speech emotion recognition (SER) classifies human emotions in speech with a computer model.
Recently, performance in SER has steadily increased as deep learning techniques have adapted.
However, unlike many domains that use speech data, data for training in the SER model is insufficient.
This causes overfitting of training of the neural network, resulting in performance degradation.
In fact, successful emotion recognition requires an effective preprocessing method and a model structure that efficiently uses the number of weight parameters.
In this study, we propose using eight dataset versions with different frequency-time resolutions to search for an effective emotional speech preprocessing method.
We propose a 6-layer convolutional neural network (CNN) model with efficient channel attention (ECA) to pursue an efficient model structure.
In particular, the well-positioned ECA blocks can improve channel feature representation with only a few parameters.
With the interactive emotional dyadic motion capture (IEMOCAP) dataset, increasing the frequency resolution in preprocessing emotional speech can improve emotion recognition performance.
Also, ECA after the deep convolution layer can effectively increase channel feature representation.
Consequently, the best result (79.37UA 79.68WA) can be obtained, exceeding the performance of previous SER models.
Furthermore, to compensate for the lack of emotional speech data, we experiment with multiple preprocessing data methods that augment trainable data preprocessed with all different settings from one sample.
In the experiment, we can achieve the highest result (80.28UA 80.46WA).</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
<span id="id2.id1" class="ltx_text">
speech emotion recognition (SER), convolutional neural network (CNN), efficient channel attention (ECA), log-Mel spectrogram, data augmentation
</span>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Speech emotion recognition (SER) is the technique in which a computer can recognize the inherent emotional features of a human‚Äôs speech signal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
In particular, interest in the human-computer interaction (HCI) systems has arisen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and SER has noticed that some applications, such as psychotherapy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and consultation calls <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, require emotional labor.
However, it is hard to understand how human speech emotions can be represented in terms of the exact values using common standards.
This is because each person‚Äôs method of recognizing emotions differs depending on their personality and culture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
The ambiguity of human emotions is one of the main challenges in developing an accurate SER model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Therefore, many studies have attempted to use the deep learning-based models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, that can be trained directly from the emotional speech data.
Among them, in recent, the convolutional neural network (CNN) based models trained with speech spectral features are proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
A CNN-based model has two advantages.
First, owing to the convolutional layer‚Äôs weight sharing, a relatively small number of trainable parameters can be used.
Second, a deep CNN layer model makes it possible to learn global context features using filters of only a small size <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
For the SER, it is essential to learn the linguistic features and the paralinguistic features of speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
Therefore, the CNN model, which can learn the context of emotional speech utterances, performs better than the other structures.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, to learn the overall context of the input data with a CNN-based model, a sufficient number of deep layers must be stacked, or a larger filter kernel must be used.
Therefore, attention modules have been proposed to cover the CNN layer‚Äôs weakness for the SER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> proposed a multiscale area attention, which applies the transformer-type attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> to the CNN-based model.
This significantly improves the recognition performance by dividing the time-frequency spatial information into granular perspectives.
Guo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> proposed spectral temporal channel attention, which is a modified version of bottleneck attention module (BAM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
Therefore, it used not only focus on spatial features but also attention to channel features.
In addition, it has an independent attention learning structure in all the axes of the input features.
However, channel attention requires more learning parameters than spatial attention because of the two multi-layer perceptron (MLP) layers.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">More trainable parameters are required when examining the attention structure and considering the more diverse aggregated input features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.
However, an increase in trainable parameters causes overfitting problems when trainable samples are leaked, such as in SER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
Therefore, in this study, we search for an efficient attention structure that can improve emotional feature expression with only a few learning parameters while maintaining a deep CNN-based model.
Through experiments, we observe that CNN‚Äôs channel features are essential for emotion classification performance.
Therefore, the efficient channel attention (ECA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> that can learn how to focus on the important channel features is applied to the SER problem for the first time.
With the interactive emotional dyadic motion capture (IEMOCAP) corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, we experiment to look for methods to use the ECA with a CNN-based model that is more suitable.
The ECA can find important channel features by focusing on the relationship between neighboring channels for the emotion classification.
To achieve this, the ECA uses a 1-D convolution layer.
Therefore, it is highly efficient because it requires only a few trainable parameters equal to the kernel size <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We also search for a more appropriate preprocessing method to better represent the emotional features.
Previous studies have preprocessed emotional speech signals using different methods; therefore, it is difficult to compare the results.
Therefore, we prepare the eight different types of preprocessed datasets.
Specifically, we choose the log-Mel spectrogram preprocessing method first.
We preprocess the speech signals with different and overlapping window sizes using short-term Fourier transformation (STFT).
We also evaluate the emotion classification performances of various CNN-based models.
As a result, we can observe that a preprocessing method with a large window size can accurately represent the emotional features.
Fig. <a href="#S1.F1" title="Figure 1 ‚Ä£ I Introduction ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the overall pipeline used in the experiments.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.04007/assets/images/ser_pipeline.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="698" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overall pipeline of speech emotion recognition with CNN-based efficient channel attention architectures.</figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In summary, our contributions to this paper are below:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Several experiments have shown that a CNN‚Äôs channel complexity significantly affects the SER.
Therefore, the ECA was applied to the SER domain for the first time.
The ECA efficiently improved the emotion classification results with only a few training parameters by extracting the relationship between the features of the neighboring channel.
It was shown, in particular, that using ECA is effective in the deep layer of the CNN model, which has many channels.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We conducted experiments using eight datasets preprocessed under various settings to determine the most effective preprocessing method for accurately expressing emotional features.
The precision of our results, a testament to the reliability of our experiment‚Äôs outcomes, was evident.
Training with a log-Mel spectrogram, representing a relatively high-frequency resolution, proved significantly more effective in emotion classification.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">In the field of SER, the need to learn data poses a significant challenge.
To address this, we proposed STFT data augmentation, which uses various preprocessed settings in STFT to supplement the expression of emotional features.
Our proposed STFT data augmentation had a profound impact, resulting in a substantial enhancement in emotion classification performance, demonstrating the efficacy of this approach.
When using a CNN-based model with ECA applied, we achieved the highest performance to date (80.28UA 80.46WA 80.37ACC), inspiring confidence in this method‚Äôs potential.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">This paper‚Äôs overall composition is as follows: 
<br class="ltx_break">Section <a href="#S2" title="II Related works ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> describes the several CNN-based models with attention modules proposed for SER.
Section <a href="#S3" title="III preprocessing method ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> and Section <a href="#S4" title="IV Model architecture ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> introduce our proposed method.
Section <a href="#S5" title="V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> presents the details of the experimental settings and evaluation results when the ECA is applied to our CNN-based model.
Finally, Section <a href="#S6" title="VI Conclusion ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related works</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Many different attention methods have been proposed.
In this section, we present an overview of the development flow of CNN-based models using several attention methods.
We divide contents whether the recurrent neural network (RNN) is used or not in the CNN-based model.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">CNN-RNN Models with Attention Mechanism</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The CNN can effectively learn local spatial features from the data.
It is also possible to learn the global spatial features, such as the context of the data when stacking multiple CNNs.
However, if the model stacks more layers, its complexity increases.
In the SER problem, increasing the model complexity is critical.
Therefore, to train the emotional context from the speech signals while sustaining the model complexity, most of studies have proposed a combination model a CNN and RNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
Although it is possible to learn the temporal features of speech using RNN, there are limitations to learning long sequences.
Therefore, to compensate for the limitations, many models that combine attention mechanisms with RNN have been proposed.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">M. Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> proposed a CNN-LSTM-Attention model to aggregate the hidden states in each time step.
This enables effective learning even for long sequences.
In addition, for more comprehensive context learning, ADRNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, which uses residual connections and dilated convolution layers together, and ASRNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, which compensates for the shortcomings of RNN through a sliding RNN method, have been proposed.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">However, models using CNN-RNN-Attention layers forcefully learn the spatio-temporal features together.
However, models consisting of CNN and RNN models in parallel are proposed to independently learn spatio-temporal features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> proposed a structure that separates LSTM and CNN in parallel and applies independent attention.
Furthermore, Z. Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> proposed AMSnet, which is a parallel model that effectively synthesizes features through a connection attention mechanism.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">CNN-based Models with Attention Mechanism</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Recently, there has been a trend to use only CNN and attention layers without an RNN.
Because RNN requires many more computational and training parameters than others, they focus on developing attention mechanism methods that help learn the context of the spatial features of speech spectrogram.
Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> demonstrated effective emotional feature learning only through self-attention after the CNN layer for the spatial context learning.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Another attention mechanism method was proposed to enhance the feature learning of the CNN layers.
Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> demonstrated the importance of frequency features that use frequential attention, in addition to spatial attention.
Xu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> proposed the ATDA, which applied independent self-attention to all feature axes (temporal, frequential, and channel) to compensate for the weakness of temporal feature learning owing to the lack of an RNN.
Guo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> proposed STC, which is a more efficient attention method for all feature axes of the CNN structure.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">We further explore this trend and attempt to find an attention structure that is efficient and effective in learning emotional features while using a deep-layer CNN structure.
In particular, for efficient attention, we focused on learning the channel features that contained the context information of the input data in the CNN layer.
Therefore, we applied the ECA module to the SER problem and obtained improved emotion classification performance than before.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">preprocessing method</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we explain the preprocessing method used to extract crucial emotional speech signal features.
Speech preprocessing suitable for a specific purpose is necessary to effectively learn a neural network model that may provide better performance.
It is not yet known which speech preprocessing method is the best for emotion recognition in speech.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Therefore, we need to determine which speech preprocessing method is the best way to recognize the emotion of speech.
For this purpose, we selected the log-Mel Spectrogram, which is frequently used for speech recognition.
Then, we check the suitable windowing and overlap times in the log-Mel Spectrogram to obtain the best result for the emotional recognition of speech.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">STFT</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The STFT is a method used to obtain the feature in frequency features by dividing a signal into short time periods.
Even though STFT is used in speech processing, the setting in STFT that is the most suitable for the emotional recognition of speech has yet to be discovered.
Therefore, we want to determine the value of the best setting when a neural network based on a CNN is applied to speech emotion recognition.
The STFT can be described as follows:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="X_{\_}t(f)=\sum_{\_}{\tau=-\infty}^{\infty}x(\tau)w(\tau-ts)e^{2\pi if\tau}" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><msub id="S3.E1.m1.3.3.3.2" xref="S3.E1.m1.3.3.3.2.cmml"><mi id="S3.E1.m1.3.3.3.2.2" xref="S3.E1.m1.3.3.3.2.2.cmml">X</mi><mi mathvariant="normal" id="S3.E1.m1.3.3.3.2.3" xref="S3.E1.m1.3.3.3.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.1" xref="S3.E1.m1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.1a" xref="S3.E1.m1.3.3.3.1.cmml">‚Äã</mo><mrow id="S3.E1.m1.3.3.3.4.2" xref="S3.E1.m1.3.3.3.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.3.4.2.1" xref="S3.E1.m1.3.3.3.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">f</mi><mo stretchy="false" id="S3.E1.m1.3.3.3.4.2.2" xref="S3.E1.m1.3.3.3.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E1.m1.3.3.4" xref="S3.E1.m1.3.3.4.cmml">=</mo><mrow id="S3.E1.m1.3.3.5" xref="S3.E1.m1.3.3.5.cmml"><munder id="S3.E1.m1.3.3.5.1" xref="S3.E1.m1.3.3.5.1.cmml"><mo movablelimits="false" id="S3.E1.m1.3.3.5.1.2" xref="S3.E1.m1.3.3.5.1.2.cmml">‚àë</mo><mi mathvariant="normal" id="S3.E1.m1.3.3.5.1.3" xref="S3.E1.m1.3.3.5.1.3.cmml">_</mi></munder><mi id="S3.E1.m1.3.3.5.2" xref="S3.E1.m1.3.3.5.2.cmml">œÑ</mi></mrow><mo id="S3.E1.m1.3.3.6" xref="S3.E1.m1.3.3.6.cmml">=</mo><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><mo id="S3.E1.m1.3.3.1a" xref="S3.E1.m1.3.3.1.cmml">‚àí</mo><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><msup id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml"><mi mathvariant="normal" id="S3.E1.m1.3.3.1.1.3.2" xref="S3.E1.m1.3.3.1.1.3.2.cmml">‚àû</mi><mi mathvariant="normal" id="S3.E1.m1.3.3.1.1.3.3" xref="S3.E1.m1.3.3.1.1.3.3.cmml">‚àû</mi></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml">‚Äã</mo><mi id="S3.E1.m1.3.3.1.1.4" xref="S3.E1.m1.3.3.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.2a" xref="S3.E1.m1.3.3.1.1.2.cmml">‚Äã</mo><mrow id="S3.E1.m1.3.3.1.1.5.2" xref="S3.E1.m1.3.3.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.1.5.2.1" xref="S3.E1.m1.3.3.1.1.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">œÑ</mi><mo stretchy="false" id="S3.E1.m1.3.3.1.1.5.2.2" xref="S3.E1.m1.3.3.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.2b" xref="S3.E1.m1.3.3.1.1.2.cmml">‚Äã</mo><mi id="S3.E1.m1.3.3.1.1.6" xref="S3.E1.m1.3.3.1.1.6.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.2c" xref="S3.E1.m1.3.3.1.1.2.cmml">‚Äã</mo><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.2.cmml">œÑ</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.cmml">‚àí</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.1.1.1.3.1" xref="S3.E1.m1.3.3.1.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.3.3.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.3.3.cmml">s</mi></mrow></mrow><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.2d" xref="S3.E1.m1.3.3.1.1.2.cmml">‚Äã</mo><msup id="S3.E1.m1.3.3.1.1.7" xref="S3.E1.m1.3.3.1.1.7.cmml"><mi id="S3.E1.m1.3.3.1.1.7.2" xref="S3.E1.m1.3.3.1.1.7.2.cmml">e</mi><mrow id="S3.E1.m1.3.3.1.1.7.3" xref="S3.E1.m1.3.3.1.1.7.3.cmml"><mn id="S3.E1.m1.3.3.1.1.7.3.2" xref="S3.E1.m1.3.3.1.1.7.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.7.3.1" xref="S3.E1.m1.3.3.1.1.7.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.3.3.1.1.7.3.3" xref="S3.E1.m1.3.3.1.1.7.3.3.cmml">œÄ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.7.3.1a" xref="S3.E1.m1.3.3.1.1.7.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.3.3.1.1.7.3.4" xref="S3.E1.m1.3.3.1.1.7.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.7.3.1b" xref="S3.E1.m1.3.3.1.1.7.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.3.3.1.1.7.3.5" xref="S3.E1.m1.3.3.1.1.7.3.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.7.3.1c" xref="S3.E1.m1.3.3.1.1.7.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.3.3.1.1.7.3.6" xref="S3.E1.m1.3.3.1.1.7.3.6.cmml">œÑ</mi></mrow></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><and id="S3.E1.m1.3.3a.cmml" xref="S3.E1.m1.3.3"></and><apply id="S3.E1.m1.3.3b.cmml" xref="S3.E1.m1.3.3"><eq id="S3.E1.m1.3.3.4.cmml" xref="S3.E1.m1.3.3.4"></eq><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><times id="S3.E1.m1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.1"></times><apply id="S3.E1.m1.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.2.1.cmml" xref="S3.E1.m1.3.3.3.2">subscript</csymbol><ci id="S3.E1.m1.3.3.3.2.2.cmml" xref="S3.E1.m1.3.3.3.2.2">ùëã</ci><ci id="S3.E1.m1.3.3.3.2.3.cmml" xref="S3.E1.m1.3.3.3.2.3">_</ci></apply><ci id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3">ùë°</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ùëì</ci></apply><apply id="S3.E1.m1.3.3.5.cmml" xref="S3.E1.m1.3.3.5"><apply id="S3.E1.m1.3.3.5.1.cmml" xref="S3.E1.m1.3.3.5.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.5.1.1.cmml" xref="S3.E1.m1.3.3.5.1">subscript</csymbol><sum id="S3.E1.m1.3.3.5.1.2.cmml" xref="S3.E1.m1.3.3.5.1.2"></sum><ci id="S3.E1.m1.3.3.5.1.3.cmml" xref="S3.E1.m1.3.3.5.1.3">_</ci></apply><ci id="S3.E1.m1.3.3.5.2.cmml" xref="S3.E1.m1.3.3.5.2">ùúè</ci></apply></apply><apply id="S3.E1.m1.3.3c.cmml" xref="S3.E1.m1.3.3"><eq id="S3.E1.m1.3.3.6.cmml" xref="S3.E1.m1.3.3.6"></eq><share href="#S3.E1.m1.3.3.5.cmml" id="S3.E1.m1.3.3d.cmml" xref="S3.E1.m1.3.3"></share><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><minus id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1"></minus><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1"><times id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"></times><apply id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.3">superscript</csymbol><infinity id="S3.E1.m1.3.3.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.3.2"></infinity><infinity id="S3.E1.m1.3.3.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.3.3"></infinity></apply><ci id="S3.E1.m1.3.3.1.1.4.cmml" xref="S3.E1.m1.3.3.1.1.4">ùë•</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ùúè</ci><ci id="S3.E1.m1.3.3.1.1.6.cmml" xref="S3.E1.m1.3.3.1.1.6">ùë§</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><minus id="S3.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1"></minus><ci id="S3.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2">ùúè</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3"><times id="S3.E1.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3.1"></times><ci id="S3.E1.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3.2">ùë°</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3.3">ùë†</ci></apply></apply><apply id="S3.E1.m1.3.3.1.1.7.cmml" xref="S3.E1.m1.3.3.1.1.7"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.7.1.cmml" xref="S3.E1.m1.3.3.1.1.7">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.7.2.cmml" xref="S3.E1.m1.3.3.1.1.7.2">ùëí</ci><apply id="S3.E1.m1.3.3.1.1.7.3.cmml" xref="S3.E1.m1.3.3.1.1.7.3"><times id="S3.E1.m1.3.3.1.1.7.3.1.cmml" xref="S3.E1.m1.3.3.1.1.7.3.1"></times><cn type="integer" id="S3.E1.m1.3.3.1.1.7.3.2.cmml" xref="S3.E1.m1.3.3.1.1.7.3.2">2</cn><ci id="S3.E1.m1.3.3.1.1.7.3.3.cmml" xref="S3.E1.m1.3.3.1.1.7.3.3">ùúã</ci><ci id="S3.E1.m1.3.3.1.1.7.3.4.cmml" xref="S3.E1.m1.3.3.1.1.7.3.4">ùëñ</ci><ci id="S3.E1.m1.3.3.1.1.7.3.5.cmml" xref="S3.E1.m1.3.3.1.1.7.3.5">ùëì</ci><ci id="S3.E1.m1.3.3.1.1.7.3.6.cmml" xref="S3.E1.m1.3.3.1.1.7.3.6">ùúè</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">X_{\_}t(f)=\sum_{\_}{\tau=-\infty}^{\infty}x(\tau)w(\tau-ts)e^{2\pi if\tau}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="s=l-o" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">s</mi><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">l</mi><mo id="S3.E2.m1.1.1.3.1" xref="S3.E2.m1.1.1.3.1.cmml">‚àí</mo><mi id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml">o</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"></eq><ci id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">ùë†</ci><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><minus id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3.1"></minus><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">ùëô</ci><ci id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3">ùëú</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">s=l-o</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.7" class="ltx_p">The output <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="X_{\_}t(f)" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.2" xref="S3.SS1.p4.1.m1.1.2.cmml"><msub id="S3.SS1.p4.1.m1.1.2.2" xref="S3.SS1.p4.1.m1.1.2.2.cmml"><mi id="S3.SS1.p4.1.m1.1.2.2.2" xref="S3.SS1.p4.1.m1.1.2.2.2.cmml">X</mi><mi mathvariant="normal" id="S3.SS1.p4.1.m1.1.2.2.3" xref="S3.SS1.p4.1.m1.1.2.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p4.1.m1.1.2.1" xref="S3.SS1.p4.1.m1.1.2.1.cmml">‚Äã</mo><mi id="S3.SS1.p4.1.m1.1.2.3" xref="S3.SS1.p4.1.m1.1.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.1.m1.1.2.1a" xref="S3.SS1.p4.1.m1.1.2.1.cmml">‚Äã</mo><mrow id="S3.SS1.p4.1.m1.1.2.4.2" xref="S3.SS1.p4.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p4.1.m1.1.2.4.2.1" xref="S3.SS1.p4.1.m1.1.2.cmml">(</mo><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">f</mi><mo stretchy="false" id="S3.SS1.p4.1.m1.1.2.4.2.2" xref="S3.SS1.p4.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.2"><times id="S3.SS1.p4.1.m1.1.2.1.cmml" xref="S3.SS1.p4.1.m1.1.2.1"></times><apply id="S3.SS1.p4.1.m1.1.2.2.cmml" xref="S3.SS1.p4.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.2.2.1.cmml" xref="S3.SS1.p4.1.m1.1.2.2">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.2.2.2.cmml" xref="S3.SS1.p4.1.m1.1.2.2.2">ùëã</ci><ci id="S3.SS1.p4.1.m1.1.2.2.3.cmml" xref="S3.SS1.p4.1.m1.1.2.2.3">_</ci></apply><ci id="S3.SS1.p4.1.m1.1.2.3.cmml" xref="S3.SS1.p4.1.m1.1.2.3">ùë°</ci><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">ùëì</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">X_{\_}t(f)</annotation></semantics></math> is obtained by applying a windowing function <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="(w)" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mrow id="S3.SS1.p4.2.m2.1.2.2"><mo stretchy="false" id="S3.SS1.p4.2.m2.1.2.2.1">(</mo><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">w</mi><mo stretchy="false" id="S3.SS1.p4.2.m2.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">ùë§</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">(w)</annotation></semantics></math> to a signal <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="x(\tau)" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1.2" xref="S3.SS1.p4.3.m3.1.2.cmml"><mi id="S3.SS1.p4.3.m3.1.2.2" xref="S3.SS1.p4.3.m3.1.2.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.3.m3.1.2.1" xref="S3.SS1.p4.3.m3.1.2.1.cmml">‚Äã</mo><mrow id="S3.SS1.p4.3.m3.1.2.3.2" xref="S3.SS1.p4.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS1.p4.3.m3.1.2.3.2.1" xref="S3.SS1.p4.3.m3.1.2.cmml">(</mo><mi id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">œÑ</mi><mo stretchy="false" id="S3.SS1.p4.3.m3.1.2.3.2.2" xref="S3.SS1.p4.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.2.cmml" xref="S3.SS1.p4.3.m3.1.2"><times id="S3.SS1.p4.3.m3.1.2.1.cmml" xref="S3.SS1.p4.3.m3.1.2.1"></times><ci id="S3.SS1.p4.3.m3.1.2.2.cmml" xref="S3.SS1.p4.3.m3.1.2.2">ùë•</ci><ci id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">ùúè</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">x(\tau)</annotation></semantics></math> in an interval of the windowing function.
The output <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="X_{\_}t(f)" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><mrow id="S3.SS1.p4.4.m4.1.2" xref="S3.SS1.p4.4.m4.1.2.cmml"><msub id="S3.SS1.p4.4.m4.1.2.2" xref="S3.SS1.p4.4.m4.1.2.2.cmml"><mi id="S3.SS1.p4.4.m4.1.2.2.2" xref="S3.SS1.p4.4.m4.1.2.2.2.cmml">X</mi><mi mathvariant="normal" id="S3.SS1.p4.4.m4.1.2.2.3" xref="S3.SS1.p4.4.m4.1.2.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p4.4.m4.1.2.1" xref="S3.SS1.p4.4.m4.1.2.1.cmml">‚Äã</mo><mi id="S3.SS1.p4.4.m4.1.2.3" xref="S3.SS1.p4.4.m4.1.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.4.m4.1.2.1a" xref="S3.SS1.p4.4.m4.1.2.1.cmml">‚Äã</mo><mrow id="S3.SS1.p4.4.m4.1.2.4.2" xref="S3.SS1.p4.4.m4.1.2.cmml"><mo stretchy="false" id="S3.SS1.p4.4.m4.1.2.4.2.1" xref="S3.SS1.p4.4.m4.1.2.cmml">(</mo><mi id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml">f</mi><mo stretchy="false" id="S3.SS1.p4.4.m4.1.2.4.2.2" xref="S3.SS1.p4.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.2.cmml" xref="S3.SS1.p4.4.m4.1.2"><times id="S3.SS1.p4.4.m4.1.2.1.cmml" xref="S3.SS1.p4.4.m4.1.2.1"></times><apply id="S3.SS1.p4.4.m4.1.2.2.cmml" xref="S3.SS1.p4.4.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.2.2.1.cmml" xref="S3.SS1.p4.4.m4.1.2.2">subscript</csymbol><ci id="S3.SS1.p4.4.m4.1.2.2.2.cmml" xref="S3.SS1.p4.4.m4.1.2.2.2">ùëã</ci><ci id="S3.SS1.p4.4.m4.1.2.2.3.cmml" xref="S3.SS1.p4.4.m4.1.2.2.3">_</ci></apply><ci id="S3.SS1.p4.4.m4.1.2.3.cmml" xref="S3.SS1.p4.4.m4.1.2.3">ùë°</ci><ci id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">ùëì</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">X_{\_}t(f)</annotation></semantics></math> then becomes a feature of the frequency.
The windowing function moves according to stride <math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="(s)" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><mrow id="S3.SS1.p4.5.m5.1.2.2"><mo stretchy="false" id="S3.SS1.p4.5.m5.1.2.2.1">(</mo><mi id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml">s</mi><mo stretchy="false" id="S3.SS1.p4.5.m5.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><ci id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">ùë†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">(s)</annotation></semantics></math>, which is determined by the windowing length <math id="S3.SS1.p4.6.m6.1" class="ltx_Math" alttext="(l)" display="inline"><semantics id="S3.SS1.p4.6.m6.1a"><mrow id="S3.SS1.p4.6.m6.1.2.2"><mo stretchy="false" id="S3.SS1.p4.6.m6.1.2.2.1">(</mo><mi id="S3.SS1.p4.6.m6.1.1" xref="S3.SS1.p4.6.m6.1.1.cmml">l</mi><mo stretchy="false" id="S3.SS1.p4.6.m6.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.6.m6.1b"><ci id="S3.SS1.p4.6.m6.1.1.cmml" xref="S3.SS1.p4.6.m6.1.1">ùëô</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.6.m6.1c">(l)</annotation></semantics></math> and overlapping length <math id="S3.SS1.p4.7.m7.1" class="ltx_Math" alttext="(o)" display="inline"><semantics id="S3.SS1.p4.7.m7.1a"><mrow id="S3.SS1.p4.7.m7.1.2.2"><mo stretchy="false" id="S3.SS1.p4.7.m7.1.2.2.1">(</mo><mi id="S3.SS1.p4.7.m7.1.1" xref="S3.SS1.p4.7.m7.1.1.cmml">o</mi><mo stretchy="false" id="S3.SS1.p4.7.m7.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.7.m7.1b"><ci id="S3.SS1.p4.7.m7.1.1.cmml" xref="S3.SS1.p4.7.m7.1.1">ùëú</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.7.m7.1c">(o)</annotation></semantics></math>.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2409.04007/assets/images/sad.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="138" height="101" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2409.04007/assets/images/hap.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="138" height="101" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F2.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2409.04007/assets/images/ang.png" id="S3.F2.sf3.g1" class="ltx_graphics ltx_img_landscape" width="138" height="101" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F2.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/2409.04007/assets/images/neu.png" id="S3.F2.sf4.g1" class="ltx_graphics ltx_img_landscape" width="138" height="101" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The log-Mel spectrogram images of each category of emotions preprocessed by the setting of version 8. (a) Sadness (Ses05M impro02 M013). (b) Happiness (Ses05M impro03 M018). (c) Angry (Ses03M impro05a M012) (d) Neutral (Ses02F impro08 F005)</figcaption>
</figure>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Note that the output <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="X_{\_}t(f)" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mrow id="S3.SS1.p5.1.m1.1.2" xref="S3.SS1.p5.1.m1.1.2.cmml"><msub id="S3.SS1.p5.1.m1.1.2.2" xref="S3.SS1.p5.1.m1.1.2.2.cmml"><mi id="S3.SS1.p5.1.m1.1.2.2.2" xref="S3.SS1.p5.1.m1.1.2.2.2.cmml">X</mi><mi mathvariant="normal" id="S3.SS1.p5.1.m1.1.2.2.3" xref="S3.SS1.p5.1.m1.1.2.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p5.1.m1.1.2.1" xref="S3.SS1.p5.1.m1.1.2.1.cmml">‚Äã</mo><mi id="S3.SS1.p5.1.m1.1.2.3" xref="S3.SS1.p5.1.m1.1.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.1.m1.1.2.1a" xref="S3.SS1.p5.1.m1.1.2.1.cmml">‚Äã</mo><mrow id="S3.SS1.p5.1.m1.1.2.4.2" xref="S3.SS1.p5.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p5.1.m1.1.2.4.2.1" xref="S3.SS1.p5.1.m1.1.2.cmml">(</mo><mi id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">f</mi><mo stretchy="false" id="S3.SS1.p5.1.m1.1.2.4.2.2" xref="S3.SS1.p5.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><apply id="S3.SS1.p5.1.m1.1.2.cmml" xref="S3.SS1.p5.1.m1.1.2"><times id="S3.SS1.p5.1.m1.1.2.1.cmml" xref="S3.SS1.p5.1.m1.1.2.1"></times><apply id="S3.SS1.p5.1.m1.1.2.2.cmml" xref="S3.SS1.p5.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p5.1.m1.1.2.2.1.cmml" xref="S3.SS1.p5.1.m1.1.2.2">subscript</csymbol><ci id="S3.SS1.p5.1.m1.1.2.2.2.cmml" xref="S3.SS1.p5.1.m1.1.2.2.2">ùëã</ci><ci id="S3.SS1.p5.1.m1.1.2.2.3.cmml" xref="S3.SS1.p5.1.m1.1.2.2.3">_</ci></apply><ci id="S3.SS1.p5.1.m1.1.2.3.cmml" xref="S3.SS1.p5.1.m1.1.2.3">ùë°</ci><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">ùëì</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">X_{\_}t(f)</annotation></semantics></math> has a resolution limit.
If the windowing length is longer, the frequency resolution increases; however, the resolution in time decreases.
If the windowing length is shorter, the frequency resolution decreases, however, the time resolution increases.
Therefore, we need to determine which features are more important in terms of time or frequency.
For this purpose, we performed our experiment by using eight different settings during preprocessing.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Speech Emotion Discrimination with Log-Mel Spectrogram</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Because the amount of data for emotion recognition in speech is limited, it is advantageous for the size of the features to become small.
The log-Mel spectrogram is effective for emotion recognition in speech because it can reduce the size of the features expressed in frequency.
In addition, the log-Mel spectrogram displays speech characteristics in 2D images.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Fig. <a href="#S3.F2" title="Figure 2 ‚Ä£ III-A STFT ‚Ä£ III preprocessing method ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an example of a log-Mel spectrogram for each emotion class.
Fig. <a href="#S3.F2" title="Figure 2 ‚Ä£ III-A STFT ‚Ä£ III preprocessing method ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (a) and Fig. <a href="#S3.F2" title="Figure 2 ‚Ä£ III-A STFT ‚Ä£ III preprocessing method ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (c) show the characteristics of speech in sadness and angry emotions.
In the case of sadness, the utterance time was short, and the speech was distributed in the low-frequency regions.
Meanwhile, some parts of the high-frequency region were observed in the speech of angry.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">However, recognizing the differences between different emotions is challenging.
In addition, some images are ambiguous when characterizing the corresponding emotions.
Therefore, a deep neural network model based on a CNN must be introduced to recognize the emotions of speech.
In this study, we suggest the structure of a CNN model based on efficient channel attention, which is effective and efficient in emotion recognition of speech.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Model architecture</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Because there is little training data for speech-emotional recognition, we need to use as few parameters as possible to improve the successful recognition of emotions in speech.
Therefore, we consider a neural network model for images based on a CNN.
In addition, to effectively learn speech emotions in context, we set up a model to elevate the learning ability in the channel.
Therefore, we apply efficient channel attention (ECA) to our neural network model-based CNN.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Convolution Layer</span>
</h3>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table I: </span>parameter settings of each layer in baseline model</figcaption>
<table id="S4.T1.8.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.8.8.9.1" class="ltx_tr">
<th id="S4.T1.8.8.9.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Layer</th>
<th id="S4.T1.8.8.9.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">parameter settings</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Conv1</th>
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">kernel size = (3,3), stride=1, number of filters = <math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="16*n" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mrow id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T1.1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.1.m1.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.1.1.1.1.m1.1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.1.cmml">‚àó</mo><mi id="S4.T1.1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1"><times id="S4.T1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1.1"></times><cn type="integer" id="S4.T1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.1.m1.1.1.2">16</cn><ci id="S4.T1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.1.m1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">16*n</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.2.2.2" class="ltx_tr">
<th id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Conv2</th>
<td id="S4.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_border_t">kernel size = (3,3), stride=1, number of filters = <math id="S4.T1.2.2.2.1.m1.1" class="ltx_Math" alttext="32*n" display="inline"><semantics id="S4.T1.2.2.2.1.m1.1a"><mrow id="S4.T1.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.1.m1.1.1.cmml"><mn id="S4.T1.2.2.2.1.m1.1.1.2" xref="S4.T1.2.2.2.1.m1.1.1.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.2.2.2.1.m1.1.1.1" xref="S4.T1.2.2.2.1.m1.1.1.1.cmml">‚àó</mo><mi id="S4.T1.2.2.2.1.m1.1.1.3" xref="S4.T1.2.2.2.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.m1.1b"><apply id="S4.T1.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.m1.1.1"><times id="S4.T1.2.2.2.1.m1.1.1.1.cmml" xref="S4.T1.2.2.2.1.m1.1.1.1"></times><cn type="integer" id="S4.T1.2.2.2.1.m1.1.1.2.cmml" xref="S4.T1.2.2.2.1.m1.1.1.2">32</cn><ci id="S4.T1.2.2.2.1.m1.1.1.3.cmml" xref="S4.T1.2.2.2.1.m1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.m1.1c">32*n</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<th id="S4.T1.3.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Conv3</th>
<td id="S4.T1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t">kernel size = (3,3), stride=1, number of filters = <math id="S4.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="48*n" display="inline"><semantics id="S4.T1.3.3.3.1.m1.1a"><mrow id="S4.T1.3.3.3.1.m1.1.1" xref="S4.T1.3.3.3.1.m1.1.1.cmml"><mn id="S4.T1.3.3.3.1.m1.1.1.2" xref="S4.T1.3.3.3.1.m1.1.1.2.cmml">48</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.3.3.3.1.m1.1.1.1" xref="S4.T1.3.3.3.1.m1.1.1.1.cmml">‚àó</mo><mi id="S4.T1.3.3.3.1.m1.1.1.3" xref="S4.T1.3.3.3.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.1.m1.1b"><apply id="S4.T1.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.1.m1.1.1"><times id="S4.T1.3.3.3.1.m1.1.1.1.cmml" xref="S4.T1.3.3.3.1.m1.1.1.1"></times><cn type="integer" id="S4.T1.3.3.3.1.m1.1.1.2.cmml" xref="S4.T1.3.3.3.1.m1.1.1.2">48</cn><ci id="S4.T1.3.3.3.1.m1.1.1.3.cmml" xref="S4.T1.3.3.3.1.m1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.1.m1.1c">48*n</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.4.4.4" class="ltx_tr">
<th id="S4.T1.4.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Conv4</th>
<td id="S4.T1.4.4.4.1" class="ltx_td ltx_align_center ltx_border_t">kernel size = (3,3), stride=1, number of filters = <math id="S4.T1.4.4.4.1.m1.1" class="ltx_Math" alttext="64*n" display="inline"><semantics id="S4.T1.4.4.4.1.m1.1a"><mrow id="S4.T1.4.4.4.1.m1.1.1" xref="S4.T1.4.4.4.1.m1.1.1.cmml"><mn id="S4.T1.4.4.4.1.m1.1.1.2" xref="S4.T1.4.4.4.1.m1.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.4.4.4.1.m1.1.1.1" xref="S4.T1.4.4.4.1.m1.1.1.1.cmml">‚àó</mo><mi id="S4.T1.4.4.4.1.m1.1.1.3" xref="S4.T1.4.4.4.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.1.m1.1b"><apply id="S4.T1.4.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.4.1.m1.1.1"><times id="S4.T1.4.4.4.1.m1.1.1.1.cmml" xref="S4.T1.4.4.4.1.m1.1.1.1"></times><cn type="integer" id="S4.T1.4.4.4.1.m1.1.1.2.cmml" xref="S4.T1.4.4.4.1.m1.1.1.2">64</cn><ci id="S4.T1.4.4.4.1.m1.1.1.3.cmml" xref="S4.T1.4.4.4.1.m1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.1.m1.1c">64*n</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.5.5.5" class="ltx_tr">
<th id="S4.T1.5.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Conv5</th>
<td id="S4.T1.5.5.5.1" class="ltx_td ltx_align_center ltx_border_t">kernel size = (3,3), stride=1, number of filters = <math id="S4.T1.5.5.5.1.m1.1" class="ltx_Math" alttext="80*n" display="inline"><semantics id="S4.T1.5.5.5.1.m1.1a"><mrow id="S4.T1.5.5.5.1.m1.1.1" xref="S4.T1.5.5.5.1.m1.1.1.cmml"><mn id="S4.T1.5.5.5.1.m1.1.1.2" xref="S4.T1.5.5.5.1.m1.1.1.2.cmml">80</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.5.5.5.1.m1.1.1.1" xref="S4.T1.5.5.5.1.m1.1.1.1.cmml">‚àó</mo><mi id="S4.T1.5.5.5.1.m1.1.1.3" xref="S4.T1.5.5.5.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.1.m1.1b"><apply id="S4.T1.5.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.5.1.m1.1.1"><times id="S4.T1.5.5.5.1.m1.1.1.1.cmml" xref="S4.T1.5.5.5.1.m1.1.1.1"></times><cn type="integer" id="S4.T1.5.5.5.1.m1.1.1.2.cmml" xref="S4.T1.5.5.5.1.m1.1.1.2">80</cn><ci id="S4.T1.5.5.5.1.m1.1.1.3.cmml" xref="S4.T1.5.5.5.1.m1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.1.m1.1c">80*n</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.6.6.6" class="ltx_tr">
<th id="S4.T1.6.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Conv6</th>
<td id="S4.T1.6.6.6.1" class="ltx_td ltx_align_center ltx_border_t">kernel size = (3,3), stride=1, number of filters = <math id="S4.T1.6.6.6.1.m1.1" class="ltx_Math" alttext="96*n" display="inline"><semantics id="S4.T1.6.6.6.1.m1.1a"><mrow id="S4.T1.6.6.6.1.m1.1.1" xref="S4.T1.6.6.6.1.m1.1.1.cmml"><mn id="S4.T1.6.6.6.1.m1.1.1.2" xref="S4.T1.6.6.6.1.m1.1.1.2.cmml">96</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.6.6.6.1.m1.1.1.1" xref="S4.T1.6.6.6.1.m1.1.1.1.cmml">‚àó</mo><mi id="S4.T1.6.6.6.1.m1.1.1.3" xref="S4.T1.6.6.6.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.1.m1.1b"><apply id="S4.T1.6.6.6.1.m1.1.1.cmml" xref="S4.T1.6.6.6.1.m1.1.1"><times id="S4.T1.6.6.6.1.m1.1.1.1.cmml" xref="S4.T1.6.6.6.1.m1.1.1.1"></times><cn type="integer" id="S4.T1.6.6.6.1.m1.1.1.2.cmml" xref="S4.T1.6.6.6.1.m1.1.1.2">96</cn><ci id="S4.T1.6.6.6.1.m1.1.1.3.cmml" xref="S4.T1.6.6.6.1.m1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.1.m1.1c">96*n</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.7.7.7" class="ltx_tr">
<th id="S4.T1.7.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Linear1</th>
<td id="S4.T1.7.7.7.1" class="ltx_td ltx_align_center ltx_border_t">weight parameter shape = <math id="S4.T1.7.7.7.1.m1.2" class="ltx_Math" alttext="(96*n,96*n)" display="inline"><semantics id="S4.T1.7.7.7.1.m1.2a"><mrow id="S4.T1.7.7.7.1.m1.2.2.2" xref="S4.T1.7.7.7.1.m1.2.2.3.cmml"><mo stretchy="false" id="S4.T1.7.7.7.1.m1.2.2.2.3" xref="S4.T1.7.7.7.1.m1.2.2.3.cmml">(</mo><mrow id="S4.T1.7.7.7.1.m1.1.1.1.1" xref="S4.T1.7.7.7.1.m1.1.1.1.1.cmml"><mn id="S4.T1.7.7.7.1.m1.1.1.1.1.2" xref="S4.T1.7.7.7.1.m1.1.1.1.1.2.cmml">96</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.7.7.7.1.m1.1.1.1.1.1" xref="S4.T1.7.7.7.1.m1.1.1.1.1.1.cmml">‚àó</mo><mi id="S4.T1.7.7.7.1.m1.1.1.1.1.3" xref="S4.T1.7.7.7.1.m1.1.1.1.1.3.cmml">n</mi></mrow><mo id="S4.T1.7.7.7.1.m1.2.2.2.4" xref="S4.T1.7.7.7.1.m1.2.2.3.cmml">,</mo><mrow id="S4.T1.7.7.7.1.m1.2.2.2.2" xref="S4.T1.7.7.7.1.m1.2.2.2.2.cmml"><mn id="S4.T1.7.7.7.1.m1.2.2.2.2.2" xref="S4.T1.7.7.7.1.m1.2.2.2.2.2.cmml">96</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.7.7.7.1.m1.2.2.2.2.1" xref="S4.T1.7.7.7.1.m1.2.2.2.2.1.cmml">‚àó</mo><mi id="S4.T1.7.7.7.1.m1.2.2.2.2.3" xref="S4.T1.7.7.7.1.m1.2.2.2.2.3.cmml">n</mi></mrow><mo stretchy="false" id="S4.T1.7.7.7.1.m1.2.2.2.5" xref="S4.T1.7.7.7.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.1.m1.2b"><interval closure="open" id="S4.T1.7.7.7.1.m1.2.2.3.cmml" xref="S4.T1.7.7.7.1.m1.2.2.2"><apply id="S4.T1.7.7.7.1.m1.1.1.1.1.cmml" xref="S4.T1.7.7.7.1.m1.1.1.1.1"><times id="S4.T1.7.7.7.1.m1.1.1.1.1.1.cmml" xref="S4.T1.7.7.7.1.m1.1.1.1.1.1"></times><cn type="integer" id="S4.T1.7.7.7.1.m1.1.1.1.1.2.cmml" xref="S4.T1.7.7.7.1.m1.1.1.1.1.2">96</cn><ci id="S4.T1.7.7.7.1.m1.1.1.1.1.3.cmml" xref="S4.T1.7.7.7.1.m1.1.1.1.1.3">ùëõ</ci></apply><apply id="S4.T1.7.7.7.1.m1.2.2.2.2.cmml" xref="S4.T1.7.7.7.1.m1.2.2.2.2"><times id="S4.T1.7.7.7.1.m1.2.2.2.2.1.cmml" xref="S4.T1.7.7.7.1.m1.2.2.2.2.1"></times><cn type="integer" id="S4.T1.7.7.7.1.m1.2.2.2.2.2.cmml" xref="S4.T1.7.7.7.1.m1.2.2.2.2.2">96</cn><ci id="S4.T1.7.7.7.1.m1.2.2.2.2.3.cmml" xref="S4.T1.7.7.7.1.m1.2.2.2.2.3">ùëõ</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.1.m1.2c">(96*n,96*n)</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.8.8.8" class="ltx_tr">
<th id="S4.T1.8.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Linear2</th>
<td id="S4.T1.8.8.8.1" class="ltx_td ltx_align_center ltx_border_t">weight parameter shape = <math id="S4.T1.8.8.8.1.m1.2" class="ltx_Math" alttext="(4,96*n)" display="inline"><semantics id="S4.T1.8.8.8.1.m1.2a"><mrow id="S4.T1.8.8.8.1.m1.2.2.1" xref="S4.T1.8.8.8.1.m1.2.2.2.cmml"><mo stretchy="false" id="S4.T1.8.8.8.1.m1.2.2.1.2" xref="S4.T1.8.8.8.1.m1.2.2.2.cmml">(</mo><mn id="S4.T1.8.8.8.1.m1.1.1" xref="S4.T1.8.8.8.1.m1.1.1.cmml">4</mn><mo id="S4.T1.8.8.8.1.m1.2.2.1.3" xref="S4.T1.8.8.8.1.m1.2.2.2.cmml">,</mo><mrow id="S4.T1.8.8.8.1.m1.2.2.1.1" xref="S4.T1.8.8.8.1.m1.2.2.1.1.cmml"><mn id="S4.T1.8.8.8.1.m1.2.2.1.1.2" xref="S4.T1.8.8.8.1.m1.2.2.1.1.2.cmml">96</mn><mo lspace="0.222em" rspace="0.222em" id="S4.T1.8.8.8.1.m1.2.2.1.1.1" xref="S4.T1.8.8.8.1.m1.2.2.1.1.1.cmml">‚àó</mo><mi id="S4.T1.8.8.8.1.m1.2.2.1.1.3" xref="S4.T1.8.8.8.1.m1.2.2.1.1.3.cmml">n</mi></mrow><mo stretchy="false" id="S4.T1.8.8.8.1.m1.2.2.1.4" xref="S4.T1.8.8.8.1.m1.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.1.m1.2b"><interval closure="open" id="S4.T1.8.8.8.1.m1.2.2.2.cmml" xref="S4.T1.8.8.8.1.m1.2.2.1"><cn type="integer" id="S4.T1.8.8.8.1.m1.1.1.cmml" xref="S4.T1.8.8.8.1.m1.1.1">4</cn><apply id="S4.T1.8.8.8.1.m1.2.2.1.1.cmml" xref="S4.T1.8.8.8.1.m1.2.2.1.1"><times id="S4.T1.8.8.8.1.m1.2.2.1.1.1.cmml" xref="S4.T1.8.8.8.1.m1.2.2.1.1.1"></times><cn type="integer" id="S4.T1.8.8.8.1.m1.2.2.1.1.2.cmml" xref="S4.T1.8.8.8.1.m1.2.2.1.1.2">96</cn><ci id="S4.T1.8.8.8.1.m1.2.2.1.1.3.cmml" xref="S4.T1.8.8.8.1.m1.2.2.1.1.3">ùëõ</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.1.m1.2c">(4,96*n)</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.8.8.10.1" class="ltx_tr">
<th id="S4.T1.8.8.10.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t">Avgpool</th>
<td id="S4.T1.8.8.10.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">kernel size = (2,2), stride=2</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The Convolution layer learns the input features included in the local region by using various filters.
The 2D Convolution layer learns the spatial information of the 2D images.
When a 2D convolution layer is applied to a spectrogram for recognition for recognizing speech emotions, it can learn the relationship between time and frequency.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.7" class="ltx_Math" alttext="(X^{l}*W_{\_}k^{l})_{\_}{ij}=\sum_{\_}t\sum_{\_}f\sum_{\_}cX^{l}(t,f,c)\cdot W_{\_}k^{l}(i-t,j-f,c)" display="block"><semantics id="S4.E3.m1.7a"><mrow id="S4.E3.m1.7.7" xref="S4.E3.m1.7.7.cmml"><mrow id="S4.E3.m1.5.5.1" xref="S4.E3.m1.5.5.1.cmml"><msub id="S4.E3.m1.5.5.1.1" xref="S4.E3.m1.5.5.1.1.cmml"><mrow id="S4.E3.m1.5.5.1.1.1.1" xref="S4.E3.m1.5.5.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m1.5.5.1.1.1.1.2" xref="S4.E3.m1.5.5.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.5.5.1.1.1.1.1" xref="S4.E3.m1.5.5.1.1.1.1.1.cmml"><mrow id="S4.E3.m1.5.5.1.1.1.1.1.2" xref="S4.E3.m1.5.5.1.1.1.1.1.2.cmml"><msup id="S4.E3.m1.5.5.1.1.1.1.1.2.2" xref="S4.E3.m1.5.5.1.1.1.1.1.2.2.cmml"><mi id="S4.E3.m1.5.5.1.1.1.1.1.2.2.2" xref="S4.E3.m1.5.5.1.1.1.1.1.2.2.2.cmml">X</mi><mi id="S4.E3.m1.5.5.1.1.1.1.1.2.2.3" xref="S4.E3.m1.5.5.1.1.1.1.1.2.2.3.cmml">l</mi></msup><mo lspace="0.222em" rspace="0.222em" id="S4.E3.m1.5.5.1.1.1.1.1.2.1" xref="S4.E3.m1.5.5.1.1.1.1.1.2.1.cmml">‚àó</mo><msub id="S4.E3.m1.5.5.1.1.1.1.1.2.3" xref="S4.E3.m1.5.5.1.1.1.1.1.2.3.cmml"><mi id="S4.E3.m1.5.5.1.1.1.1.1.2.3.2" xref="S4.E3.m1.5.5.1.1.1.1.1.2.3.2.cmml">W</mi><mi mathvariant="normal" id="S4.E3.m1.5.5.1.1.1.1.1.2.3.3" xref="S4.E3.m1.5.5.1.1.1.1.1.2.3.3.cmml">_</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S4.E3.m1.5.5.1.1.1.1.1.1" xref="S4.E3.m1.5.5.1.1.1.1.1.1.cmml">‚Äã</mo><msup id="S4.E3.m1.5.5.1.1.1.1.1.3" xref="S4.E3.m1.5.5.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.5.5.1.1.1.1.1.3.2" xref="S4.E3.m1.5.5.1.1.1.1.1.3.2.cmml">k</mi><mi id="S4.E3.m1.5.5.1.1.1.1.1.3.3" xref="S4.E3.m1.5.5.1.1.1.1.1.3.3.cmml">l</mi></msup></mrow><mo stretchy="false" id="S4.E3.m1.5.5.1.1.1.1.3" xref="S4.E3.m1.5.5.1.1.1.1.1.cmml">)</mo></mrow><mi mathvariant="normal" id="S4.E3.m1.5.5.1.1.3" xref="S4.E3.m1.5.5.1.1.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S4.E3.m1.5.5.1.2" xref="S4.E3.m1.5.5.1.2.cmml">‚Äã</mo><mi id="S4.E3.m1.5.5.1.3" xref="S4.E3.m1.5.5.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.5.5.1.2a" xref="S4.E3.m1.5.5.1.2.cmml">‚Äã</mo><mi id="S4.E3.m1.5.5.1.4" xref="S4.E3.m1.5.5.1.4.cmml">j</mi></mrow><mo rspace="0.111em" id="S4.E3.m1.7.7.4" xref="S4.E3.m1.7.7.4.cmml">=</mo><mrow id="S4.E3.m1.7.7.3" xref="S4.E3.m1.7.7.3.cmml"><munder id="S4.E3.m1.7.7.3.3" xref="S4.E3.m1.7.7.3.3.cmml"><mo movablelimits="false" id="S4.E3.m1.7.7.3.3.2" xref="S4.E3.m1.7.7.3.3.2.cmml">‚àë</mo><mi mathvariant="normal" id="S4.E3.m1.7.7.3.3.3" xref="S4.E3.m1.7.7.3.3.3.cmml">_</mi></munder><mrow id="S4.E3.m1.7.7.3.2" xref="S4.E3.m1.7.7.3.2.cmml"><mi id="S4.E3.m1.7.7.3.2.4" xref="S4.E3.m1.7.7.3.2.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.7.7.3.2.3" xref="S4.E3.m1.7.7.3.2.3.cmml">‚Äã</mo><mrow id="S4.E3.m1.7.7.3.2.2" xref="S4.E3.m1.7.7.3.2.2.cmml"><munder id="S4.E3.m1.7.7.3.2.2.3" xref="S4.E3.m1.7.7.3.2.2.3.cmml"><mo movablelimits="false" id="S4.E3.m1.7.7.3.2.2.3.2" xref="S4.E3.m1.7.7.3.2.2.3.2.cmml">‚àë</mo><mi mathvariant="normal" id="S4.E3.m1.7.7.3.2.2.3.3" xref="S4.E3.m1.7.7.3.2.2.3.3.cmml">_</mi></munder><mrow id="S4.E3.m1.7.7.3.2.2.2" xref="S4.E3.m1.7.7.3.2.2.2.cmml"><mi id="S4.E3.m1.7.7.3.2.2.2.4" xref="S4.E3.m1.7.7.3.2.2.2.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.7.7.3.2.2.2.3" xref="S4.E3.m1.7.7.3.2.2.2.3.cmml">‚Äã</mo><mrow id="S4.E3.m1.7.7.3.2.2.2.2" xref="S4.E3.m1.7.7.3.2.2.2.2.cmml"><munder id="S4.E3.m1.7.7.3.2.2.2.2.3" xref="S4.E3.m1.7.7.3.2.2.2.2.3.cmml"><mo movablelimits="false" id="S4.E3.m1.7.7.3.2.2.2.2.3.2" xref="S4.E3.m1.7.7.3.2.2.2.2.3.2.cmml">‚àë</mo><mi mathvariant="normal" id="S4.E3.m1.7.7.3.2.2.2.2.3.3" xref="S4.E3.m1.7.7.3.2.2.2.2.3.3.cmml">_</mi></munder><mrow id="S4.E3.m1.7.7.3.2.2.2.2.2" xref="S4.E3.m1.7.7.3.2.2.2.2.2.cmml"><mrow id="S4.E3.m1.7.7.3.2.2.2.2.2.4" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.cmml"><mrow id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.cmml"><mi id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.2" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.1" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.1.cmml">‚Äã</mo><msup id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3.cmml"><mi id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3.2" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3.2.cmml">X</mi><mi id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3.3" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3.3.cmml">l</mi></msup><mo lspace="0em" rspace="0em" id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.1a" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.1.cmml">‚Äã</mo><mrow id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.4.2" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.4.1.cmml"><mo stretchy="false" id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.4.2.1" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.4.1.cmml">(</mo><mi id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml">t</mi><mo id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.4.2.2" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.4.1.cmml">,</mo><mi id="S4.E3.m1.2.2" xref="S4.E3.m1.2.2.cmml">f</mi><mo id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.4.2.3" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.4.1.cmml">,</mo><mi id="S4.E3.m1.3.3" xref="S4.E3.m1.3.3.cmml">c</mi><mo rspace="0.055em" stretchy="false" id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.4.2.4" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.4.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S4.E3.m1.7.7.3.2.2.2.2.2.4.1" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.1.cmml">‚ãÖ</mo><msub id="S4.E3.m1.7.7.3.2.2.2.2.2.4.3" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.3.cmml"><mi id="S4.E3.m1.7.7.3.2.2.2.2.2.4.3.2" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.3.2.cmml">W</mi><mi mathvariant="normal" id="S4.E3.m1.7.7.3.2.2.2.2.2.4.3.3" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.3.3.cmml">_</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S4.E3.m1.7.7.3.2.2.2.2.2.3" xref="S4.E3.m1.7.7.3.2.2.2.2.2.3.cmml">‚Äã</mo><msup id="S4.E3.m1.7.7.3.2.2.2.2.2.5" xref="S4.E3.m1.7.7.3.2.2.2.2.2.5.cmml"><mi id="S4.E3.m1.7.7.3.2.2.2.2.2.5.2" xref="S4.E3.m1.7.7.3.2.2.2.2.2.5.2.cmml">k</mi><mi id="S4.E3.m1.7.7.3.2.2.2.2.2.5.3" xref="S4.E3.m1.7.7.3.2.2.2.2.2.5.3.cmml">l</mi></msup><mo lspace="0em" rspace="0em" id="S4.E3.m1.7.7.3.2.2.2.2.2.3a" xref="S4.E3.m1.7.7.3.2.2.2.2.2.3.cmml">‚Äã</mo><mrow id="S4.E3.m1.7.7.3.2.2.2.2.2.2.2" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.3" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.3.cmml">(</mo><mrow id="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1" xref="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.2.cmml">i</mi><mo id="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><mi id="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.3.cmml">t</mi></mrow><mo id="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.4" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.3.cmml">,</mo><mrow id="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.cmml"><mi id="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.2" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.2.cmml">j</mi><mo id="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.1" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.1.cmml">‚àí</mo><mi id="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.3" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.3.cmml">f</mi></mrow><mo id="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.5" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.3.cmml">,</mo><mi id="S4.E3.m1.4.4" xref="S4.E3.m1.4.4.cmml">c</mi><mo stretchy="false" id="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.6" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.7b"><apply id="S4.E3.m1.7.7.cmml" xref="S4.E3.m1.7.7"><eq id="S4.E3.m1.7.7.4.cmml" xref="S4.E3.m1.7.7.4"></eq><apply id="S4.E3.m1.5.5.1.cmml" xref="S4.E3.m1.5.5.1"><times id="S4.E3.m1.5.5.1.2.cmml" xref="S4.E3.m1.5.5.1.2"></times><apply id="S4.E3.m1.5.5.1.1.cmml" xref="S4.E3.m1.5.5.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.1.1.2.cmml" xref="S4.E3.m1.5.5.1.1">subscript</csymbol><apply id="S4.E3.m1.5.5.1.1.1.1.1.cmml" xref="S4.E3.m1.5.5.1.1.1.1"><times id="S4.E3.m1.5.5.1.1.1.1.1.1.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.1"></times><apply id="S4.E3.m1.5.5.1.1.1.1.1.2.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.2"><times id="S4.E3.m1.5.5.1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.2.1"></times><apply id="S4.E3.m1.5.5.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.1.1.1.1.1.2.2.1.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.2.2">superscript</csymbol><ci id="S4.E3.m1.5.5.1.1.1.1.1.2.2.2.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.2.2.2">ùëã</ci><ci id="S4.E3.m1.5.5.1.1.1.1.1.2.2.3.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.2.2.3">ùëô</ci></apply><apply id="S4.E3.m1.5.5.1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.1.1.1.1.1.2.3.1.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.2.3">subscript</csymbol><ci id="S4.E3.m1.5.5.1.1.1.1.1.2.3.2.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.2.3.2">ùëä</ci><ci id="S4.E3.m1.5.5.1.1.1.1.1.2.3.3.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.2.3.3">_</ci></apply></apply><apply id="S4.E3.m1.5.5.1.1.1.1.1.3.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.3">superscript</csymbol><ci id="S4.E3.m1.5.5.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.3.2">ùëò</ci><ci id="S4.E3.m1.5.5.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.5.5.1.1.1.1.1.3.3">ùëô</ci></apply></apply><ci id="S4.E3.m1.5.5.1.1.3.cmml" xref="S4.E3.m1.5.5.1.1.3">_</ci></apply><ci id="S4.E3.m1.5.5.1.3.cmml" xref="S4.E3.m1.5.5.1.3">ùëñ</ci><ci id="S4.E3.m1.5.5.1.4.cmml" xref="S4.E3.m1.5.5.1.4">ùëó</ci></apply><apply id="S4.E3.m1.7.7.3.cmml" xref="S4.E3.m1.7.7.3"><apply id="S4.E3.m1.7.7.3.3.cmml" xref="S4.E3.m1.7.7.3.3"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.3.3.1.cmml" xref="S4.E3.m1.7.7.3.3">subscript</csymbol><sum id="S4.E3.m1.7.7.3.3.2.cmml" xref="S4.E3.m1.7.7.3.3.2"></sum><ci id="S4.E3.m1.7.7.3.3.3.cmml" xref="S4.E3.m1.7.7.3.3.3">_</ci></apply><apply id="S4.E3.m1.7.7.3.2.cmml" xref="S4.E3.m1.7.7.3.2"><times id="S4.E3.m1.7.7.3.2.3.cmml" xref="S4.E3.m1.7.7.3.2.3"></times><ci id="S4.E3.m1.7.7.3.2.4.cmml" xref="S4.E3.m1.7.7.3.2.4">ùë°</ci><apply id="S4.E3.m1.7.7.3.2.2.cmml" xref="S4.E3.m1.7.7.3.2.2"><apply id="S4.E3.m1.7.7.3.2.2.3.cmml" xref="S4.E3.m1.7.7.3.2.2.3"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.3.2.2.3.1.cmml" xref="S4.E3.m1.7.7.3.2.2.3">subscript</csymbol><sum id="S4.E3.m1.7.7.3.2.2.3.2.cmml" xref="S4.E3.m1.7.7.3.2.2.3.2"></sum><ci id="S4.E3.m1.7.7.3.2.2.3.3.cmml" xref="S4.E3.m1.7.7.3.2.2.3.3">_</ci></apply><apply id="S4.E3.m1.7.7.3.2.2.2.cmml" xref="S4.E3.m1.7.7.3.2.2.2"><times id="S4.E3.m1.7.7.3.2.2.2.3.cmml" xref="S4.E3.m1.7.7.3.2.2.2.3"></times><ci id="S4.E3.m1.7.7.3.2.2.2.4.cmml" xref="S4.E3.m1.7.7.3.2.2.2.4">ùëì</ci><apply id="S4.E3.m1.7.7.3.2.2.2.2.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2"><apply id="S4.E3.m1.7.7.3.2.2.2.2.3.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.3.2.2.2.2.3.1.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.3">subscript</csymbol><sum id="S4.E3.m1.7.7.3.2.2.2.2.3.2.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.3.2"></sum><ci id="S4.E3.m1.7.7.3.2.2.2.2.3.3.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.3.3">_</ci></apply><apply id="S4.E3.m1.7.7.3.2.2.2.2.2.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2"><times id="S4.E3.m1.7.7.3.2.2.2.2.2.3.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.3"></times><apply id="S4.E3.m1.7.7.3.2.2.2.2.2.4.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4"><ci id="S4.E3.m1.7.7.3.2.2.2.2.2.4.1.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.1">‚ãÖ</ci><apply id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2"><times id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.1.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.1"></times><ci id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.2.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.2">ùëê</ci><apply id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3.1.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3">superscript</csymbol><ci id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3.2.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3.2">ùëã</ci><ci id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3.3.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.3.3">ùëô</ci></apply><vector id="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.4.1.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.2.4.2"><ci id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1">ùë°</ci><ci id="S4.E3.m1.2.2.cmml" xref="S4.E3.m1.2.2">ùëì</ci><ci id="S4.E3.m1.3.3.cmml" xref="S4.E3.m1.3.3">ùëê</ci></vector></apply><apply id="S4.E3.m1.7.7.3.2.2.2.2.2.4.3.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.3"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.3.2.2.2.2.2.4.3.1.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.3">subscript</csymbol><ci id="S4.E3.m1.7.7.3.2.2.2.2.2.4.3.2.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.3.2">ùëä</ci><ci id="S4.E3.m1.7.7.3.2.2.2.2.2.4.3.3.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.4.3.3">_</ci></apply></apply><apply id="S4.E3.m1.7.7.3.2.2.2.2.2.5.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.5"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.3.2.2.2.2.2.5.1.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.5">superscript</csymbol><ci id="S4.E3.m1.7.7.3.2.2.2.2.2.5.2.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.5.2">ùëò</ci><ci id="S4.E3.m1.7.7.3.2.2.2.2.2.5.3.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.5.3">ùëô</ci></apply><vector id="S4.E3.m1.7.7.3.2.2.2.2.2.2.3.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.2"><apply id="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1"><minus id="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.1"></minus><ci id="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.2">ùëñ</ci><ci id="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.6.6.2.1.1.1.1.1.1.1.1.3">ùë°</ci></apply><apply id="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2"><minus id="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.1.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.1"></minus><ci id="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.2.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.2">ùëó</ci><ci id="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.3.cmml" xref="S4.E3.m1.7.7.3.2.2.2.2.2.2.2.2.3">ùëì</ci></apply><ci id="S4.E3.m1.4.4.cmml" xref="S4.E3.m1.4.4">ùëê</ci></vector></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.7c">(X^{l}*W_{\_}k^{l})_{\_}{ij}=\sum_{\_}t\sum_{\_}f\sum_{\_}cX^{l}(t,f,c)\cdot W_{\_}k^{l}(i-t,j-f,c)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<table id="S4.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E4.m1.4" class="ltx_Math" alttext="\mathrm{2DConv}(X^{l+1})=[(X^{l}*W_{\_}1^{l}),...,(X^{l}*W_{\_}{c^{\prime}}^{l})]" display="block"><semantics id="S4.E4.m1.4a"><mrow id="S4.E4.m1.4.4" xref="S4.E4.m1.4.4.cmml"><mrow id="S4.E4.m1.2.2.1" xref="S4.E4.m1.2.2.1.cmml"><mn id="S4.E4.m1.2.2.1.3" xref="S4.E4.m1.2.2.1.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.1.2" xref="S4.E4.m1.2.2.1.2.cmml">‚Äã</mo><mi mathvariant="normal" id="S4.E4.m1.2.2.1.4" xref="S4.E4.m1.2.2.1.4.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.1.2a" xref="S4.E4.m1.2.2.1.2.cmml">‚Äã</mo><mi mathvariant="normal" id="S4.E4.m1.2.2.1.5" xref="S4.E4.m1.2.2.1.5.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.1.2b" xref="S4.E4.m1.2.2.1.2.cmml">‚Äã</mo><mi mathvariant="normal" id="S4.E4.m1.2.2.1.6" xref="S4.E4.m1.2.2.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.1.2c" xref="S4.E4.m1.2.2.1.2.cmml">‚Äã</mo><mi mathvariant="normal" id="S4.E4.m1.2.2.1.7" xref="S4.E4.m1.2.2.1.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.1.2d" xref="S4.E4.m1.2.2.1.2.cmml">‚Äã</mo><mi mathvariant="normal" id="S4.E4.m1.2.2.1.8" xref="S4.E4.m1.2.2.1.8.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.1.2e" xref="S4.E4.m1.2.2.1.2.cmml">‚Äã</mo><mrow id="S4.E4.m1.2.2.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.E4.m1.2.2.1.1.1.2" xref="S4.E4.m1.2.2.1.1.1.1.cmml">(</mo><msup id="S4.E4.m1.2.2.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.cmml"><mi id="S4.E4.m1.2.2.1.1.1.1.2" xref="S4.E4.m1.2.2.1.1.1.1.2.cmml">X</mi><mrow id="S4.E4.m1.2.2.1.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.1.3.cmml"><mi id="S4.E4.m1.2.2.1.1.1.1.3.2" xref="S4.E4.m1.2.2.1.1.1.1.3.2.cmml">l</mi><mo id="S4.E4.m1.2.2.1.1.1.1.3.1" xref="S4.E4.m1.2.2.1.1.1.1.3.1.cmml">+</mo><mn id="S4.E4.m1.2.2.1.1.1.1.3.3" xref="S4.E4.m1.2.2.1.1.1.1.3.3.cmml">1</mn></mrow></msup><mo stretchy="false" id="S4.E4.m1.2.2.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E4.m1.4.4.4" xref="S4.E4.m1.4.4.4.cmml">=</mo><mrow id="S4.E4.m1.4.4.3.2" xref="S4.E4.m1.4.4.3.3.cmml"><mo stretchy="false" id="S4.E4.m1.4.4.3.2.3" xref="S4.E4.m1.4.4.3.3.cmml">[</mo><mrow id="S4.E4.m1.3.3.2.1.1.1" xref="S4.E4.m1.3.3.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.E4.m1.3.3.2.1.1.1.2" xref="S4.E4.m1.3.3.2.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.3.3.2.1.1.1.1" xref="S4.E4.m1.3.3.2.1.1.1.1.cmml"><mrow id="S4.E4.m1.3.3.2.1.1.1.1.2" xref="S4.E4.m1.3.3.2.1.1.1.1.2.cmml"><msup id="S4.E4.m1.3.3.2.1.1.1.1.2.2" xref="S4.E4.m1.3.3.2.1.1.1.1.2.2.cmml"><mi id="S4.E4.m1.3.3.2.1.1.1.1.2.2.2" xref="S4.E4.m1.3.3.2.1.1.1.1.2.2.2.cmml">X</mi><mi id="S4.E4.m1.3.3.2.1.1.1.1.2.2.3" xref="S4.E4.m1.3.3.2.1.1.1.1.2.2.3.cmml">l</mi></msup><mo lspace="0.222em" rspace="0.222em" id="S4.E4.m1.3.3.2.1.1.1.1.2.1" xref="S4.E4.m1.3.3.2.1.1.1.1.2.1.cmml">‚àó</mo><msub id="S4.E4.m1.3.3.2.1.1.1.1.2.3" xref="S4.E4.m1.3.3.2.1.1.1.1.2.3.cmml"><mi id="S4.E4.m1.3.3.2.1.1.1.1.2.3.2" xref="S4.E4.m1.3.3.2.1.1.1.1.2.3.2.cmml">W</mi><mi mathvariant="normal" id="S4.E4.m1.3.3.2.1.1.1.1.2.3.3" xref="S4.E4.m1.3.3.2.1.1.1.1.2.3.3.cmml">_</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.2.1.1.1.1.1" xref="S4.E4.m1.3.3.2.1.1.1.1.1.cmml">‚Äã</mo><msup id="S4.E4.m1.3.3.2.1.1.1.1.3" xref="S4.E4.m1.3.3.2.1.1.1.1.3.cmml"><mn id="S4.E4.m1.3.3.2.1.1.1.1.3.2" xref="S4.E4.m1.3.3.2.1.1.1.1.3.2.cmml">1</mn><mi id="S4.E4.m1.3.3.2.1.1.1.1.3.3" xref="S4.E4.m1.3.3.2.1.1.1.1.3.3.cmml">l</mi></msup></mrow><mo stretchy="false" id="S4.E4.m1.3.3.2.1.1.1.3" xref="S4.E4.m1.3.3.2.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E4.m1.4.4.3.2.4" xref="S4.E4.m1.4.4.3.3.cmml">,</mo><mi mathvariant="normal" id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml">‚Ä¶</mi><mo id="S4.E4.m1.4.4.3.2.5" xref="S4.E4.m1.4.4.3.3.cmml">,</mo><mrow id="S4.E4.m1.4.4.3.2.2.1" xref="S4.E4.m1.4.4.3.2.2.1.1.cmml"><mo stretchy="false" id="S4.E4.m1.4.4.3.2.2.1.2" xref="S4.E4.m1.4.4.3.2.2.1.1.cmml">(</mo><mrow id="S4.E4.m1.4.4.3.2.2.1.1" xref="S4.E4.m1.4.4.3.2.2.1.1.cmml"><mrow id="S4.E4.m1.4.4.3.2.2.1.1.2" xref="S4.E4.m1.4.4.3.2.2.1.1.2.cmml"><msup id="S4.E4.m1.4.4.3.2.2.1.1.2.2" xref="S4.E4.m1.4.4.3.2.2.1.1.2.2.cmml"><mi id="S4.E4.m1.4.4.3.2.2.1.1.2.2.2" xref="S4.E4.m1.4.4.3.2.2.1.1.2.2.2.cmml">X</mi><mi id="S4.E4.m1.4.4.3.2.2.1.1.2.2.3" xref="S4.E4.m1.4.4.3.2.2.1.1.2.2.3.cmml">l</mi></msup><mo lspace="0.222em" rspace="0.222em" id="S4.E4.m1.4.4.3.2.2.1.1.2.1" xref="S4.E4.m1.4.4.3.2.2.1.1.2.1.cmml">‚àó</mo><msub id="S4.E4.m1.4.4.3.2.2.1.1.2.3" xref="S4.E4.m1.4.4.3.2.2.1.1.2.3.cmml"><mi id="S4.E4.m1.4.4.3.2.2.1.1.2.3.2" xref="S4.E4.m1.4.4.3.2.2.1.1.2.3.2.cmml">W</mi><mi mathvariant="normal" id="S4.E4.m1.4.4.3.2.2.1.1.2.3.3" xref="S4.E4.m1.4.4.3.2.2.1.1.2.3.3.cmml">_</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S4.E4.m1.4.4.3.2.2.1.1.1" xref="S4.E4.m1.4.4.3.2.2.1.1.1.cmml">‚Äã</mo><mmultiscripts id="S4.E4.m1.4.4.3.2.2.1.1.3" xref="S4.E4.m1.4.4.3.2.2.1.1.3.cmml"><mi id="S4.E4.m1.4.4.3.2.2.1.1.3.2.2" xref="S4.E4.m1.4.4.3.2.2.1.1.3.2.2.cmml">c</mi><mrow id="S4.E4.m1.4.4.3.2.2.1.1.3a" xref="S4.E4.m1.4.4.3.2.2.1.1.3.cmml"></mrow><mo id="S4.E4.m1.4.4.3.2.2.1.1.3.2.3" xref="S4.E4.m1.4.4.3.2.2.1.1.3.2.3.cmml">‚Ä≤</mo><mrow id="S4.E4.m1.4.4.3.2.2.1.1.3b" xref="S4.E4.m1.4.4.3.2.2.1.1.3.cmml"></mrow><mi id="S4.E4.m1.4.4.3.2.2.1.1.3.3" xref="S4.E4.m1.4.4.3.2.2.1.1.3.3.cmml">l</mi></mmultiscripts></mrow><mo stretchy="false" id="S4.E4.m1.4.4.3.2.2.1.3" xref="S4.E4.m1.4.4.3.2.2.1.1.cmml">)</mo></mrow><mo stretchy="false" id="S4.E4.m1.4.4.3.2.6" xref="S4.E4.m1.4.4.3.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.4b"><apply id="S4.E4.m1.4.4.cmml" xref="S4.E4.m1.4.4"><eq id="S4.E4.m1.4.4.4.cmml" xref="S4.E4.m1.4.4.4"></eq><apply id="S4.E4.m1.2.2.1.cmml" xref="S4.E4.m1.2.2.1"><times id="S4.E4.m1.2.2.1.2.cmml" xref="S4.E4.m1.2.2.1.2"></times><cn type="integer" id="S4.E4.m1.2.2.1.3.cmml" xref="S4.E4.m1.2.2.1.3">2</cn><ci id="S4.E4.m1.2.2.1.4.cmml" xref="S4.E4.m1.2.2.1.4">D</ci><ci id="S4.E4.m1.2.2.1.5.cmml" xref="S4.E4.m1.2.2.1.5">C</ci><ci id="S4.E4.m1.2.2.1.6.cmml" xref="S4.E4.m1.2.2.1.6">o</ci><ci id="S4.E4.m1.2.2.1.7.cmml" xref="S4.E4.m1.2.2.1.7">n</ci><ci id="S4.E4.m1.2.2.1.8.cmml" xref="S4.E4.m1.2.2.1.8">v</ci><apply id="S4.E4.m1.2.2.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1">superscript</csymbol><ci id="S4.E4.m1.2.2.1.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.2">ùëã</ci><apply id="S4.E4.m1.2.2.1.1.1.1.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.3"><plus id="S4.E4.m1.2.2.1.1.1.1.3.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.3.1"></plus><ci id="S4.E4.m1.2.2.1.1.1.1.3.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.3.2">ùëô</ci><cn type="integer" id="S4.E4.m1.2.2.1.1.1.1.3.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.3.3">1</cn></apply></apply></apply><list id="S4.E4.m1.4.4.3.3.cmml" xref="S4.E4.m1.4.4.3.2"><apply id="S4.E4.m1.3.3.2.1.1.1.1.cmml" xref="S4.E4.m1.3.3.2.1.1.1"><times id="S4.E4.m1.3.3.2.1.1.1.1.1.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.1"></times><apply id="S4.E4.m1.3.3.2.1.1.1.1.2.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.2"><times id="S4.E4.m1.3.3.2.1.1.1.1.2.1.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.2.1"></times><apply id="S4.E4.m1.3.3.2.1.1.1.1.2.2.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E4.m1.3.3.2.1.1.1.1.2.2.1.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.2.2">superscript</csymbol><ci id="S4.E4.m1.3.3.2.1.1.1.1.2.2.2.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.2.2.2">ùëã</ci><ci id="S4.E4.m1.3.3.2.1.1.1.1.2.2.3.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.2.2.3">ùëô</ci></apply><apply id="S4.E4.m1.3.3.2.1.1.1.1.2.3.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E4.m1.3.3.2.1.1.1.1.2.3.1.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.2.3">subscript</csymbol><ci id="S4.E4.m1.3.3.2.1.1.1.1.2.3.2.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.2.3.2">ùëä</ci><ci id="S4.E4.m1.3.3.2.1.1.1.1.2.3.3.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.2.3.3">_</ci></apply></apply><apply id="S4.E4.m1.3.3.2.1.1.1.1.3.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.3.3.2.1.1.1.1.3.1.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.3">superscript</csymbol><cn type="integer" id="S4.E4.m1.3.3.2.1.1.1.1.3.2.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.3.2">1</cn><ci id="S4.E4.m1.3.3.2.1.1.1.1.3.3.cmml" xref="S4.E4.m1.3.3.2.1.1.1.1.3.3">ùëô</ci></apply></apply><ci id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1">‚Ä¶</ci><apply id="S4.E4.m1.4.4.3.2.2.1.1.cmml" xref="S4.E4.m1.4.4.3.2.2.1"><times id="S4.E4.m1.4.4.3.2.2.1.1.1.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.1"></times><apply id="S4.E4.m1.4.4.3.2.2.1.1.2.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.2"><times id="S4.E4.m1.4.4.3.2.2.1.1.2.1.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.2.1"></times><apply id="S4.E4.m1.4.4.3.2.2.1.1.2.2.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S4.E4.m1.4.4.3.2.2.1.1.2.2.1.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.2.2">superscript</csymbol><ci id="S4.E4.m1.4.4.3.2.2.1.1.2.2.2.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.2.2.2">ùëã</ci><ci id="S4.E4.m1.4.4.3.2.2.1.1.2.2.3.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.2.2.3">ùëô</ci></apply><apply id="S4.E4.m1.4.4.3.2.2.1.1.2.3.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S4.E4.m1.4.4.3.2.2.1.1.2.3.1.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.2.3">subscript</csymbol><ci id="S4.E4.m1.4.4.3.2.2.1.1.2.3.2.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.2.3.2">ùëä</ci><ci id="S4.E4.m1.4.4.3.2.2.1.1.2.3.3.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.2.3.3">_</ci></apply></apply><apply id="S4.E4.m1.4.4.3.2.2.1.1.3.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.4.4.3.2.2.1.1.3.1.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.3">superscript</csymbol><apply id="S4.E4.m1.4.4.3.2.2.1.1.3.2.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.4.4.3.2.2.1.1.3.2.1.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.3">superscript</csymbol><ci id="S4.E4.m1.4.4.3.2.2.1.1.3.2.2.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.3.2.2">ùëê</ci><ci id="S4.E4.m1.4.4.3.2.2.1.1.3.2.3.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.3.2.3">‚Ä≤</ci></apply><ci id="S4.E4.m1.4.4.3.2.2.1.1.3.3.cmml" xref="S4.E4.m1.4.4.3.2.2.1.1.3.3">ùëô</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.4c">\mathrm{2DConv}(X^{l+1})=[(X^{l}*W_{\_}1^{l}),...,(X^{l}*W_{\_}{c^{\prime}}^{l})]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.4" class="ltx_p">Equations (<a href="#S4.E3" title="In IV-A Convolution Layer ‚Ä£ IV Model architecture ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) and (<a href="#S4.E4" title="In IV-A Convolution Layer ‚Ä£ IV Model architecture ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) show the method for calculating the convolution when a spectrogram is used as the input.
The input <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="X^{l}\in\mathbb{R}^{T\times F\times C}" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><msup id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2.2" xref="S4.SS1.p4.1.m1.1.1.2.2.cmml">X</mi><mi id="S4.SS1.p4.1.m1.1.1.2.3" xref="S4.SS1.p4.1.m1.1.1.2.3.cmml">l</mi></msup><mo id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml">‚àà</mo><msup id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml"><mi id="S4.SS1.p4.1.m1.1.1.3.2" xref="S4.SS1.p4.1.m1.1.1.3.2.cmml">‚Ñù</mi><mrow id="S4.SS1.p4.1.m1.1.1.3.3" xref="S4.SS1.p4.1.m1.1.1.3.3.cmml"><mi id="S4.SS1.p4.1.m1.1.1.3.3.2" xref="S4.SS1.p4.1.m1.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p4.1.m1.1.1.3.3.1" xref="S4.SS1.p4.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S4.SS1.p4.1.m1.1.1.3.3.3" xref="S4.SS1.p4.1.m1.1.1.3.3.3.cmml">F</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p4.1.m1.1.1.3.3.1a" xref="S4.SS1.p4.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S4.SS1.p4.1.m1.1.1.3.3.4" xref="S4.SS1.p4.1.m1.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><in id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1"></in><apply id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.2.1.cmml" xref="S4.SS1.p4.1.m1.1.1.2">superscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.2.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2.2">ùëã</ci><ci id="S4.SS1.p4.1.m1.1.1.2.3.cmml" xref="S4.SS1.p4.1.m1.1.1.2.3">ùëô</ci></apply><apply id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.2">‚Ñù</ci><apply id="S4.SS1.p4.1.m1.1.1.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3"><times id="S4.SS1.p4.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3.1"></times><ci id="S4.SS1.p4.1.m1.1.1.3.3.2.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3.2">ùëá</ci><ci id="S4.SS1.p4.1.m1.1.1.3.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3.3">ùêπ</ci><ci id="S4.SS1.p4.1.m1.1.1.3.3.4.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3.4">ùê∂</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">X^{l}\in\mathbb{R}^{T\times F\times C}</annotation></semantics></math> is convoluted with weight filter <math id="S4.SS1.p4.2.m2.3" class="ltx_Math" alttext="W^{l}=[W_{\_}1^{l},...,W_{\_}{C^{\prime}}^{l}]\in\mathbb{R}^{K\times K\times C\times C^{\prime}}" display="inline"><semantics id="S4.SS1.p4.2.m2.3a"><mrow id="S4.SS1.p4.2.m2.3.3" xref="S4.SS1.p4.2.m2.3.3.cmml"><msup id="S4.SS1.p4.2.m2.3.3.4" xref="S4.SS1.p4.2.m2.3.3.4.cmml"><mi id="S4.SS1.p4.2.m2.3.3.4.2" xref="S4.SS1.p4.2.m2.3.3.4.2.cmml">W</mi><mi id="S4.SS1.p4.2.m2.3.3.4.3" xref="S4.SS1.p4.2.m2.3.3.4.3.cmml">l</mi></msup><mo id="S4.SS1.p4.2.m2.3.3.5" xref="S4.SS1.p4.2.m2.3.3.5.cmml">=</mo><mrow id="S4.SS1.p4.2.m2.3.3.2.2" xref="S4.SS1.p4.2.m2.3.3.2.3.cmml"><mo stretchy="false" id="S4.SS1.p4.2.m2.3.3.2.2.3" xref="S4.SS1.p4.2.m2.3.3.2.3.cmml">[</mo><mrow id="S4.SS1.p4.2.m2.2.2.1.1.1" xref="S4.SS1.p4.2.m2.2.2.1.1.1.cmml"><msub id="S4.SS1.p4.2.m2.2.2.1.1.1.2" xref="S4.SS1.p4.2.m2.2.2.1.1.1.2.cmml"><mi id="S4.SS1.p4.2.m2.2.2.1.1.1.2.2" xref="S4.SS1.p4.2.m2.2.2.1.1.1.2.2.cmml">W</mi><mi mathvariant="normal" id="S4.SS1.p4.2.m2.2.2.1.1.1.2.3" xref="S4.SS1.p4.2.m2.2.2.1.1.1.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS1.p4.2.m2.2.2.1.1.1.1" xref="S4.SS1.p4.2.m2.2.2.1.1.1.1.cmml">‚Äã</mo><msup id="S4.SS1.p4.2.m2.2.2.1.1.1.3" xref="S4.SS1.p4.2.m2.2.2.1.1.1.3.cmml"><mn id="S4.SS1.p4.2.m2.2.2.1.1.1.3.2" xref="S4.SS1.p4.2.m2.2.2.1.1.1.3.2.cmml">1</mn><mi id="S4.SS1.p4.2.m2.2.2.1.1.1.3.3" xref="S4.SS1.p4.2.m2.2.2.1.1.1.3.3.cmml">l</mi></msup></mrow><mo id="S4.SS1.p4.2.m2.3.3.2.2.4" xref="S4.SS1.p4.2.m2.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml">‚Ä¶</mi><mo id="S4.SS1.p4.2.m2.3.3.2.2.5" xref="S4.SS1.p4.2.m2.3.3.2.3.cmml">,</mo><mrow id="S4.SS1.p4.2.m2.3.3.2.2.2" xref="S4.SS1.p4.2.m2.3.3.2.2.2.cmml"><msub id="S4.SS1.p4.2.m2.3.3.2.2.2.2" xref="S4.SS1.p4.2.m2.3.3.2.2.2.2.cmml"><mi id="S4.SS1.p4.2.m2.3.3.2.2.2.2.2" xref="S4.SS1.p4.2.m2.3.3.2.2.2.2.2.cmml">W</mi><mi mathvariant="normal" id="S4.SS1.p4.2.m2.3.3.2.2.2.2.3" xref="S4.SS1.p4.2.m2.3.3.2.2.2.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS1.p4.2.m2.3.3.2.2.2.1" xref="S4.SS1.p4.2.m2.3.3.2.2.2.1.cmml">‚Äã</mo><mmultiscripts id="S4.SS1.p4.2.m2.3.3.2.2.2.3" xref="S4.SS1.p4.2.m2.3.3.2.2.2.3.cmml"><mi id="S4.SS1.p4.2.m2.3.3.2.2.2.3.2.2" xref="S4.SS1.p4.2.m2.3.3.2.2.2.3.2.2.cmml">C</mi><mrow id="S4.SS1.p4.2.m2.3.3.2.2.2.3a" xref="S4.SS1.p4.2.m2.3.3.2.2.2.3.cmml"></mrow><mo id="S4.SS1.p4.2.m2.3.3.2.2.2.3.2.3" xref="S4.SS1.p4.2.m2.3.3.2.2.2.3.2.3.cmml">‚Ä≤</mo><mrow id="S4.SS1.p4.2.m2.3.3.2.2.2.3b" xref="S4.SS1.p4.2.m2.3.3.2.2.2.3.cmml"></mrow><mi id="S4.SS1.p4.2.m2.3.3.2.2.2.3.3" xref="S4.SS1.p4.2.m2.3.3.2.2.2.3.3.cmml">l</mi></mmultiscripts></mrow><mo stretchy="false" id="S4.SS1.p4.2.m2.3.3.2.2.6" xref="S4.SS1.p4.2.m2.3.3.2.3.cmml">]</mo></mrow><mo id="S4.SS1.p4.2.m2.3.3.6" xref="S4.SS1.p4.2.m2.3.3.6.cmml">‚àà</mo><msup id="S4.SS1.p4.2.m2.3.3.7" xref="S4.SS1.p4.2.m2.3.3.7.cmml"><mi id="S4.SS1.p4.2.m2.3.3.7.2" xref="S4.SS1.p4.2.m2.3.3.7.2.cmml">‚Ñù</mi><mrow id="S4.SS1.p4.2.m2.3.3.7.3" xref="S4.SS1.p4.2.m2.3.3.7.3.cmml"><mi id="S4.SS1.p4.2.m2.3.3.7.3.2" xref="S4.SS1.p4.2.m2.3.3.7.3.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p4.2.m2.3.3.7.3.1" xref="S4.SS1.p4.2.m2.3.3.7.3.1.cmml">√ó</mo><mi id="S4.SS1.p4.2.m2.3.3.7.3.3" xref="S4.SS1.p4.2.m2.3.3.7.3.3.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p4.2.m2.3.3.7.3.1a" xref="S4.SS1.p4.2.m2.3.3.7.3.1.cmml">√ó</mo><mi id="S4.SS1.p4.2.m2.3.3.7.3.4" xref="S4.SS1.p4.2.m2.3.3.7.3.4.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p4.2.m2.3.3.7.3.1b" xref="S4.SS1.p4.2.m2.3.3.7.3.1.cmml">√ó</mo><msup id="S4.SS1.p4.2.m2.3.3.7.3.5" xref="S4.SS1.p4.2.m2.3.3.7.3.5.cmml"><mi id="S4.SS1.p4.2.m2.3.3.7.3.5.2" xref="S4.SS1.p4.2.m2.3.3.7.3.5.2.cmml">C</mi><mo id="S4.SS1.p4.2.m2.3.3.7.3.5.3" xref="S4.SS1.p4.2.m2.3.3.7.3.5.3.cmml">‚Ä≤</mo></msup></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.3b"><apply id="S4.SS1.p4.2.m2.3.3.cmml" xref="S4.SS1.p4.2.m2.3.3"><and id="S4.SS1.p4.2.m2.3.3a.cmml" xref="S4.SS1.p4.2.m2.3.3"></and><apply id="S4.SS1.p4.2.m2.3.3b.cmml" xref="S4.SS1.p4.2.m2.3.3"><eq id="S4.SS1.p4.2.m2.3.3.5.cmml" xref="S4.SS1.p4.2.m2.3.3.5"></eq><apply id="S4.SS1.p4.2.m2.3.3.4.cmml" xref="S4.SS1.p4.2.m2.3.3.4"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.3.3.4.1.cmml" xref="S4.SS1.p4.2.m2.3.3.4">superscript</csymbol><ci id="S4.SS1.p4.2.m2.3.3.4.2.cmml" xref="S4.SS1.p4.2.m2.3.3.4.2">ùëä</ci><ci id="S4.SS1.p4.2.m2.3.3.4.3.cmml" xref="S4.SS1.p4.2.m2.3.3.4.3">ùëô</ci></apply><list id="S4.SS1.p4.2.m2.3.3.2.3.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2"><apply id="S4.SS1.p4.2.m2.2.2.1.1.1.cmml" xref="S4.SS1.p4.2.m2.2.2.1.1.1"><times id="S4.SS1.p4.2.m2.2.2.1.1.1.1.cmml" xref="S4.SS1.p4.2.m2.2.2.1.1.1.1"></times><apply id="S4.SS1.p4.2.m2.2.2.1.1.1.2.cmml" xref="S4.SS1.p4.2.m2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.2.2.1.1.1.2.1.cmml" xref="S4.SS1.p4.2.m2.2.2.1.1.1.2">subscript</csymbol><ci id="S4.SS1.p4.2.m2.2.2.1.1.1.2.2.cmml" xref="S4.SS1.p4.2.m2.2.2.1.1.1.2.2">ùëä</ci><ci id="S4.SS1.p4.2.m2.2.2.1.1.1.2.3.cmml" xref="S4.SS1.p4.2.m2.2.2.1.1.1.2.3">_</ci></apply><apply id="S4.SS1.p4.2.m2.2.2.1.1.1.3.cmml" xref="S4.SS1.p4.2.m2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.2.2.1.1.1.3.1.cmml" xref="S4.SS1.p4.2.m2.2.2.1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p4.2.m2.2.2.1.1.1.3.2.cmml" xref="S4.SS1.p4.2.m2.2.2.1.1.1.3.2">1</cn><ci id="S4.SS1.p4.2.m2.2.2.1.1.1.3.3.cmml" xref="S4.SS1.p4.2.m2.2.2.1.1.1.3.3">ùëô</ci></apply></apply><ci id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">‚Ä¶</ci><apply id="S4.SS1.p4.2.m2.3.3.2.2.2.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2.2"><times id="S4.SS1.p4.2.m2.3.3.2.2.2.1.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2.2.1"></times><apply id="S4.SS1.p4.2.m2.3.3.2.2.2.2.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.3.3.2.2.2.2.1.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2.2.2">subscript</csymbol><ci id="S4.SS1.p4.2.m2.3.3.2.2.2.2.2.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2.2.2.2">ùëä</ci><ci id="S4.SS1.p4.2.m2.3.3.2.2.2.2.3.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2.2.2.3">_</ci></apply><apply id="S4.SS1.p4.2.m2.3.3.2.2.2.3.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2.2.3"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.3.3.2.2.2.3.1.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2.2.3">superscript</csymbol><apply id="S4.SS1.p4.2.m2.3.3.2.2.2.3.2.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2.2.3"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.3.3.2.2.2.3.2.1.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2.2.3">superscript</csymbol><ci id="S4.SS1.p4.2.m2.3.3.2.2.2.3.2.2.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2.2.3.2.2">ùê∂</ci><ci id="S4.SS1.p4.2.m2.3.3.2.2.2.3.2.3.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2.2.3.2.3">‚Ä≤</ci></apply><ci id="S4.SS1.p4.2.m2.3.3.2.2.2.3.3.cmml" xref="S4.SS1.p4.2.m2.3.3.2.2.2.3.3">ùëô</ci></apply></apply></list></apply><apply id="S4.SS1.p4.2.m2.3.3c.cmml" xref="S4.SS1.p4.2.m2.3.3"><in id="S4.SS1.p4.2.m2.3.3.6.cmml" xref="S4.SS1.p4.2.m2.3.3.6"></in><share href="#S4.SS1.p4.2.m2.3.3.2.cmml" id="S4.SS1.p4.2.m2.3.3d.cmml" xref="S4.SS1.p4.2.m2.3.3"></share><apply id="S4.SS1.p4.2.m2.3.3.7.cmml" xref="S4.SS1.p4.2.m2.3.3.7"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.3.3.7.1.cmml" xref="S4.SS1.p4.2.m2.3.3.7">superscript</csymbol><ci id="S4.SS1.p4.2.m2.3.3.7.2.cmml" xref="S4.SS1.p4.2.m2.3.3.7.2">‚Ñù</ci><apply id="S4.SS1.p4.2.m2.3.3.7.3.cmml" xref="S4.SS1.p4.2.m2.3.3.7.3"><times id="S4.SS1.p4.2.m2.3.3.7.3.1.cmml" xref="S4.SS1.p4.2.m2.3.3.7.3.1"></times><ci id="S4.SS1.p4.2.m2.3.3.7.3.2.cmml" xref="S4.SS1.p4.2.m2.3.3.7.3.2">ùêæ</ci><ci id="S4.SS1.p4.2.m2.3.3.7.3.3.cmml" xref="S4.SS1.p4.2.m2.3.3.7.3.3">ùêæ</ci><ci id="S4.SS1.p4.2.m2.3.3.7.3.4.cmml" xref="S4.SS1.p4.2.m2.3.3.7.3.4">ùê∂</ci><apply id="S4.SS1.p4.2.m2.3.3.7.3.5.cmml" xref="S4.SS1.p4.2.m2.3.3.7.3.5"><csymbol cd="ambiguous" id="S4.SS1.p4.2.m2.3.3.7.3.5.1.cmml" xref="S4.SS1.p4.2.m2.3.3.7.3.5">superscript</csymbol><ci id="S4.SS1.p4.2.m2.3.3.7.3.5.2.cmml" xref="S4.SS1.p4.2.m2.3.3.7.3.5.2">ùê∂</ci><ci id="S4.SS1.p4.2.m2.3.3.7.3.5.3.cmml" xref="S4.SS1.p4.2.m2.3.3.7.3.5.3">‚Ä≤</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.3c">W^{l}=[W_{\_}1^{l},...,W_{\_}{C^{\prime}}^{l}]\in\mathbb{R}^{K\times K\times C\times C^{\prime}}</annotation></semantics></math> to transform the <math id="S4.SS1.p4.3.m3.1" class="ltx_Math" alttext="C^{\prime}" display="inline"><semantics id="S4.SS1.p4.3.m3.1a"><msup id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml"><mi id="S4.SS1.p4.3.m3.1.1.2" xref="S4.SS1.p4.3.m3.1.1.2.cmml">C</mi><mo id="S4.SS1.p4.3.m3.1.1.3" xref="S4.SS1.p4.3.m3.1.1.3.cmml">‚Ä≤</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><apply id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.3.m3.1.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1">superscript</csymbol><ci id="S4.SS1.p4.3.m3.1.1.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2">ùê∂</ci><ci id="S4.SS1.p4.3.m3.1.1.3.cmml" xref="S4.SS1.p4.3.m3.1.1.3">‚Ä≤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">C^{\prime}</annotation></semantics></math> size of channel dimension.
In this calculation, the weight filters are used with kernel size <math id="S4.SS1.p4.4.m4.1" class="ltx_Math" alttext="(K\times K)" display="inline"><semantics id="S4.SS1.p4.4.m4.1a"><mrow id="S4.SS1.p4.4.m4.1.1.1" xref="S4.SS1.p4.4.m4.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p4.4.m4.1.1.1.2" xref="S4.SS1.p4.4.m4.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p4.4.m4.1.1.1.1" xref="S4.SS1.p4.4.m4.1.1.1.1.cmml"><mi id="S4.SS1.p4.4.m4.1.1.1.1.2" xref="S4.SS1.p4.4.m4.1.1.1.1.2.cmml">K</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p4.4.m4.1.1.1.1.1" xref="S4.SS1.p4.4.m4.1.1.1.1.1.cmml">√ó</mo><mi id="S4.SS1.p4.4.m4.1.1.1.1.3" xref="S4.SS1.p4.4.m4.1.1.1.1.3.cmml">K</mi></mrow><mo stretchy="false" id="S4.SS1.p4.4.m4.1.1.1.3" xref="S4.SS1.p4.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m4.1b"><apply id="S4.SS1.p4.4.m4.1.1.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1.1"><times id="S4.SS1.p4.4.m4.1.1.1.1.1.cmml" xref="S4.SS1.p4.4.m4.1.1.1.1.1"></times><ci id="S4.SS1.p4.4.m4.1.1.1.1.2.cmml" xref="S4.SS1.p4.4.m4.1.1.1.1.2">ùêæ</ci><ci id="S4.SS1.p4.4.m4.1.1.1.1.3.cmml" xref="S4.SS1.p4.4.m4.1.1.1.1.3">ùêæ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m4.1c">(K\times K)</annotation></semantics></math> of the local spatial information.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">We must set an appropriate number of the weight filters and kernel sizes to better train with this convolution layer.
Therefore, in this study, we explored the trainable parametric efficiency in a CNN-based model for speech emotion recognition.
We mainly focus on some weight filters called ‚Äúchannel size‚Äù.
The channel size in the convolution layer is a hyperparameter related to the information capacity of the neural network.
If we use many weight filters, spatial information can be sensitively extracted from the input data.
However, this leads to a more robust overfitting of the training dataset.
We design a 6-layer CNN architecture to determine an adequate channel size for speech emotion recognition.
This is explained in detail in the following section.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">CNN-Based Architecture</span>
</h3>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2409.04007/assets/images/baseline_model.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="250" height="298" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>CNN-based model architecture. It consists of six convolution blocks and two fully connected layers.</figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We proposed a deep CNN-based model as a baseline for developing an effective SER model.
The structure of the CNN-based model was based on a previously proposed model for SER.
Using this model, we focused on CNN‚Äôs channel features of the CNN, which effectively trained the speech emotion features.
Fig. <a href="#S4.F3" title="Figure 3 ‚Ä£ IV-B CNN-Based Architecture ‚Ä£ IV Model architecture ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the overall architecture of the CNN-based model.
It is composed of six Convolutional blocks and a pooling layer.
And, two fully connected layers are used for emotion classification.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">The detailed model structure is as follows.
First, we chose a convolution block, which is commonly used in image classification models.
The convolution block consists of three layers: convolution layers with <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="(3\times 3)" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p2.1.m1.1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p2.1.m1.1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.1.cmml"><mn id="S4.SS2.p2.1.m1.1.1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p2.1.m1.1.1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.1.1.cmml">√ó</mo><mn id="S4.SS2.p2.1.m1.1.1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.1.1.3.cmml">3</mn></mrow><mo stretchy="false" id="S4.SS2.p2.1.m1.1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"><times id="S4.SS2.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1.1"></times><cn type="integer" id="S4.SS2.p2.1.m1.1.1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1.2">3</cn><cn type="integer" id="S4.SS2.p2.1.m1.1.1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">(3\times 3)</annotation></semantics></math> kernel size, batch normalization, and ReLU activation.
To find the adequate channel size in each convolution layer, we selected the initial channel size in multiples of <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mn id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><cn type="integer" id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">16</annotation></semantics></math>.
We multiplied by a scaling factor to scale up the channel size.
The details of the parameter settings are listed in Table <a href="#S4.T1" title="Table I ‚Ä£ IV-A Convolution Layer ‚Ä£ IV Model architecture ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Next, we used the average pooling layer after all the convolution blocks.
It can efficiently subsample the hidden features without vanishing the features.
Exceptionally, in the sixth pooling layer, we used global average pooling to connect with the next layer.
Finally, two fully connected layers were used in the classification layer.
The weight of each fully connected layer was set to be the same as the output channel size of the last convolution layer.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">The Design of ECA Module</span>
</h3>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2409.04007/assets/images/eca_block.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The design of efficient channel attention module in this study.</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The convolution layer can extract the local spatial features using the number of trainable filters from the input data.
However, when more filters were used in the model, the representation capacity of the filters weakened.
To overcome this situation, we adopt the ECA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> in the CNN-based model, which can effectively improve the representation of the filters.
The ECA is a channel attention architecture proposed by Wang et al.. Learning the relationship between different filters and focusing on the important ones is possible with fewer trainable parameters.
We experimented with the application of an ECA suitable for a CNN-based SER model.
As a result, increasing the filter‚Äôs representation in the convolution layer helps to extract emotional features.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.3" class="ltx_p">The ECA is a type of layer that applies a self-attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
The self-attention mechanism comprises three components: query (<math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mi id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">Q</annotation></semantics></math>), key (<math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mi id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><ci id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">ùêæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">K</annotation></semantics></math>), and value (<math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mi id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><ci id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">ùëâ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">V</annotation></semantics></math>).
These components are used to attend to the crucial information from input features.
Each component‚Äôs role is as follows.
A query is a ‚Äùquestion‚Äù about what is essential in the input features.
The key is ‚Äùhint,‚Äù which uses how similar the query is.
This helps to find the most appropriate information in the input features.
The value is the ‚Äùreal answer‚Äù that exists in pairs with keys and is used as the output features of self-attention.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<table id="S4.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E5.m1.4" class="ltx_Math" alttext="\mathrm{Attention}(Q,K,V)=\mathrm{Softmax}({{QK}\over{\sqrt{n}}})V" display="block"><semantics id="S4.E5.m1.4a"><mrow id="S4.E5.m1.4.5" xref="S4.E5.m1.4.5.cmml"><mrow id="S4.E5.m1.4.5.2" xref="S4.E5.m1.4.5.2.cmml"><mi id="S4.E5.m1.4.5.2.2" xref="S4.E5.m1.4.5.2.2.cmml">Attention</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.4.5.2.1" xref="S4.E5.m1.4.5.2.1.cmml">‚Äã</mo><mrow id="S4.E5.m1.4.5.2.3.2" xref="S4.E5.m1.4.5.2.3.1.cmml"><mo stretchy="false" id="S4.E5.m1.4.5.2.3.2.1" xref="S4.E5.m1.4.5.2.3.1.cmml">(</mo><mi id="S4.E5.m1.1.1" xref="S4.E5.m1.1.1.cmml">Q</mi><mo id="S4.E5.m1.4.5.2.3.2.2" xref="S4.E5.m1.4.5.2.3.1.cmml">,</mo><mi id="S4.E5.m1.2.2" xref="S4.E5.m1.2.2.cmml">K</mi><mo id="S4.E5.m1.4.5.2.3.2.3" xref="S4.E5.m1.4.5.2.3.1.cmml">,</mo><mi id="S4.E5.m1.3.3" xref="S4.E5.m1.3.3.cmml">V</mi><mo stretchy="false" id="S4.E5.m1.4.5.2.3.2.4" xref="S4.E5.m1.4.5.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E5.m1.4.5.1" xref="S4.E5.m1.4.5.1.cmml">=</mo><mrow id="S4.E5.m1.4.5.3" xref="S4.E5.m1.4.5.3.cmml"><mi id="S4.E5.m1.4.5.3.2" xref="S4.E5.m1.4.5.3.2.cmml">Softmax</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.4.5.3.1" xref="S4.E5.m1.4.5.3.1.cmml">‚Äã</mo><mrow id="S4.E5.m1.4.5.3.3.2" xref="S4.E5.m1.4.4.cmml"><mo stretchy="false" id="S4.E5.m1.4.5.3.3.2.1" xref="S4.E5.m1.4.4.cmml">(</mo><mfrac id="S4.E5.m1.4.4" xref="S4.E5.m1.4.4.cmml"><mrow id="S4.E5.m1.4.4.2" xref="S4.E5.m1.4.4.2.cmml"><mi id="S4.E5.m1.4.4.2.2" xref="S4.E5.m1.4.4.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.4.4.2.1" xref="S4.E5.m1.4.4.2.1.cmml">‚Äã</mo><mi id="S4.E5.m1.4.4.2.3" xref="S4.E5.m1.4.4.2.3.cmml">K</mi></mrow><msqrt id="S4.E5.m1.4.4.3" xref="S4.E5.m1.4.4.3.cmml"><mi id="S4.E5.m1.4.4.3.2" xref="S4.E5.m1.4.4.3.2.cmml">n</mi></msqrt></mfrac><mo stretchy="false" id="S4.E5.m1.4.5.3.3.2.2" xref="S4.E5.m1.4.4.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.E5.m1.4.5.3.1a" xref="S4.E5.m1.4.5.3.1.cmml">‚Äã</mo><mi id="S4.E5.m1.4.5.3.4" xref="S4.E5.m1.4.5.3.4.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.4b"><apply id="S4.E5.m1.4.5.cmml" xref="S4.E5.m1.4.5"><eq id="S4.E5.m1.4.5.1.cmml" xref="S4.E5.m1.4.5.1"></eq><apply id="S4.E5.m1.4.5.2.cmml" xref="S4.E5.m1.4.5.2"><times id="S4.E5.m1.4.5.2.1.cmml" xref="S4.E5.m1.4.5.2.1"></times><ci id="S4.E5.m1.4.5.2.2.cmml" xref="S4.E5.m1.4.5.2.2">Attention</ci><vector id="S4.E5.m1.4.5.2.3.1.cmml" xref="S4.E5.m1.4.5.2.3.2"><ci id="S4.E5.m1.1.1.cmml" xref="S4.E5.m1.1.1">ùëÑ</ci><ci id="S4.E5.m1.2.2.cmml" xref="S4.E5.m1.2.2">ùêæ</ci><ci id="S4.E5.m1.3.3.cmml" xref="S4.E5.m1.3.3">ùëâ</ci></vector></apply><apply id="S4.E5.m1.4.5.3.cmml" xref="S4.E5.m1.4.5.3"><times id="S4.E5.m1.4.5.3.1.cmml" xref="S4.E5.m1.4.5.3.1"></times><ci id="S4.E5.m1.4.5.3.2.cmml" xref="S4.E5.m1.4.5.3.2">Softmax</ci><apply id="S4.E5.m1.4.4.cmml" xref="S4.E5.m1.4.5.3.3.2"><divide id="S4.E5.m1.4.4.1.cmml" xref="S4.E5.m1.4.5.3.3.2"></divide><apply id="S4.E5.m1.4.4.2.cmml" xref="S4.E5.m1.4.4.2"><times id="S4.E5.m1.4.4.2.1.cmml" xref="S4.E5.m1.4.4.2.1"></times><ci id="S4.E5.m1.4.4.2.2.cmml" xref="S4.E5.m1.4.4.2.2">ùëÑ</ci><ci id="S4.E5.m1.4.4.2.3.cmml" xref="S4.E5.m1.4.4.2.3">ùêæ</ci></apply><apply id="S4.E5.m1.4.4.3.cmml" xref="S4.E5.m1.4.4.3"><root id="S4.E5.m1.4.4.3a.cmml" xref="S4.E5.m1.4.4.3"></root><ci id="S4.E5.m1.4.4.3.2.cmml" xref="S4.E5.m1.4.4.3.2">ùëõ</ci></apply></apply><ci id="S4.E5.m1.4.5.3.4.cmml" xref="S4.E5.m1.4.5.3.4">ùëâ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.4c">\mathrm{Attention}(Q,K,V)=\mathrm{Softmax}({{QK}\over{\sqrt{n}}})V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">The most common self-attention mechanism in (<a href="#S4.E5" title="In IV-C The Design of ECA Module ‚Ä£ IV Model architecture ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) is the inner-product attention.
In the first step, each query is multiplied with the multiple keys to obtain the relevant score matrix.
In the second step, the score matrix is represented by a probability distribution within each query using the Softmax function.
In the final step, the output is represented by a linear combination of all the values with a probability distribution.
In summary, the output is the weighted value obtained that is most closely related to the query indirectly through the keys.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<table id="S4.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E6.m1.4" class="ltx_Math" alttext="\mathrm{ECA}(Q,V)=\sigma(\mathrm{1DConv(Q)})\otimes V" display="block"><semantics id="S4.E6.m1.4a"><mrow id="S4.E6.m1.4.4" xref="S4.E6.m1.4.4.cmml"><mrow id="S4.E6.m1.4.4.3" xref="S4.E6.m1.4.4.3.cmml"><mi id="S4.E6.m1.4.4.3.2" xref="S4.E6.m1.4.4.3.2.cmml">ECA</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.3.1" xref="S4.E6.m1.4.4.3.1.cmml">‚Äã</mo><mrow id="S4.E6.m1.4.4.3.3.2" xref="S4.E6.m1.4.4.3.3.1.cmml"><mo stretchy="false" id="S4.E6.m1.4.4.3.3.2.1" xref="S4.E6.m1.4.4.3.3.1.cmml">(</mo><mi id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml">Q</mi><mo id="S4.E6.m1.4.4.3.3.2.2" xref="S4.E6.m1.4.4.3.3.1.cmml">,</mo><mi id="S4.E6.m1.2.2" xref="S4.E6.m1.2.2.cmml">V</mi><mo stretchy="false" id="S4.E6.m1.4.4.3.3.2.3" xref="S4.E6.m1.4.4.3.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E6.m1.4.4.2" xref="S4.E6.m1.4.4.2.cmml">=</mo><mrow id="S4.E6.m1.4.4.1" xref="S4.E6.m1.4.4.1.cmml"><mrow id="S4.E6.m1.4.4.1.1" xref="S4.E6.m1.4.4.1.1.cmml"><mi id="S4.E6.m1.4.4.1.1.3" xref="S4.E6.m1.4.4.1.1.3.cmml">œÉ</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.1.1.2" xref="S4.E6.m1.4.4.1.1.2.cmml">‚Äã</mo><mrow id="S4.E6.m1.4.4.1.1.1.1" xref="S4.E6.m1.4.4.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E6.m1.4.4.1.1.1.1.2" xref="S4.E6.m1.4.4.1.1.1.1.1.cmml">(</mo><mrow id="S4.E6.m1.4.4.1.1.1.1.1" xref="S4.E6.m1.4.4.1.1.1.1.1.cmml"><mn id="S4.E6.m1.4.4.1.1.1.1.1.2" xref="S4.E6.m1.4.4.1.1.1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.1.1.1.1.1.1" xref="S4.E6.m1.4.4.1.1.1.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S4.E6.m1.4.4.1.1.1.1.1.3" xref="S4.E6.m1.4.4.1.1.1.1.1.3.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.1.1.1.1.1.1a" xref="S4.E6.m1.4.4.1.1.1.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S4.E6.m1.4.4.1.1.1.1.1.4" xref="S4.E6.m1.4.4.1.1.1.1.1.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.1.1.1.1.1.1b" xref="S4.E6.m1.4.4.1.1.1.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S4.E6.m1.4.4.1.1.1.1.1.5" xref="S4.E6.m1.4.4.1.1.1.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.1.1.1.1.1.1c" xref="S4.E6.m1.4.4.1.1.1.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S4.E6.m1.4.4.1.1.1.1.1.6" xref="S4.E6.m1.4.4.1.1.1.1.1.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.1.1.1.1.1.1d" xref="S4.E6.m1.4.4.1.1.1.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S4.E6.m1.4.4.1.1.1.1.1.7" xref="S4.E6.m1.4.4.1.1.1.1.1.7.cmml">v</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.1.1.1.1.1.1e" xref="S4.E6.m1.4.4.1.1.1.1.1.1.cmml">‚Äã</mo><mrow id="S4.E6.m1.4.4.1.1.1.1.1.8.2" xref="S4.E6.m1.4.4.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E6.m1.4.4.1.1.1.1.1.8.2.1" xref="S4.E6.m1.4.4.1.1.1.1.1.cmml">(</mo><mi mathvariant="normal" id="S4.E6.m1.3.3" xref="S4.E6.m1.3.3.cmml">Q</mi><mo stretchy="false" id="S4.E6.m1.4.4.1.1.1.1.1.8.2.2" xref="S4.E6.m1.4.4.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.055em" stretchy="false" id="S4.E6.m1.4.4.1.1.1.1.3" xref="S4.E6.m1.4.4.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S4.E6.m1.4.4.1.2" xref="S4.E6.m1.4.4.1.2.cmml">‚äó</mo><mi id="S4.E6.m1.4.4.1.3" xref="S4.E6.m1.4.4.1.3.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.4b"><apply id="S4.E6.m1.4.4.cmml" xref="S4.E6.m1.4.4"><eq id="S4.E6.m1.4.4.2.cmml" xref="S4.E6.m1.4.4.2"></eq><apply id="S4.E6.m1.4.4.3.cmml" xref="S4.E6.m1.4.4.3"><times id="S4.E6.m1.4.4.3.1.cmml" xref="S4.E6.m1.4.4.3.1"></times><ci id="S4.E6.m1.4.4.3.2.cmml" xref="S4.E6.m1.4.4.3.2">ECA</ci><interval closure="open" id="S4.E6.m1.4.4.3.3.1.cmml" xref="S4.E6.m1.4.4.3.3.2"><ci id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.1">ùëÑ</ci><ci id="S4.E6.m1.2.2.cmml" xref="S4.E6.m1.2.2">ùëâ</ci></interval></apply><apply id="S4.E6.m1.4.4.1.cmml" xref="S4.E6.m1.4.4.1"><csymbol cd="latexml" id="S4.E6.m1.4.4.1.2.cmml" xref="S4.E6.m1.4.4.1.2">tensor-product</csymbol><apply id="S4.E6.m1.4.4.1.1.cmml" xref="S4.E6.m1.4.4.1.1"><times id="S4.E6.m1.4.4.1.1.2.cmml" xref="S4.E6.m1.4.4.1.1.2"></times><ci id="S4.E6.m1.4.4.1.1.3.cmml" xref="S4.E6.m1.4.4.1.1.3">ùúé</ci><apply id="S4.E6.m1.4.4.1.1.1.1.1.cmml" xref="S4.E6.m1.4.4.1.1.1.1"><times id="S4.E6.m1.4.4.1.1.1.1.1.1.cmml" xref="S4.E6.m1.4.4.1.1.1.1.1.1"></times><cn type="integer" id="S4.E6.m1.4.4.1.1.1.1.1.2.cmml" xref="S4.E6.m1.4.4.1.1.1.1.1.2">1</cn><ci id="S4.E6.m1.4.4.1.1.1.1.1.3.cmml" xref="S4.E6.m1.4.4.1.1.1.1.1.3">D</ci><ci id="S4.E6.m1.4.4.1.1.1.1.1.4.cmml" xref="S4.E6.m1.4.4.1.1.1.1.1.4">C</ci><ci id="S4.E6.m1.4.4.1.1.1.1.1.5.cmml" xref="S4.E6.m1.4.4.1.1.1.1.1.5">o</ci><ci id="S4.E6.m1.4.4.1.1.1.1.1.6.cmml" xref="S4.E6.m1.4.4.1.1.1.1.1.6">n</ci><ci id="S4.E6.m1.4.4.1.1.1.1.1.7.cmml" xref="S4.E6.m1.4.4.1.1.1.1.1.7">v</ci><ci id="S4.E6.m1.3.3.cmml" xref="S4.E6.m1.3.3">Q</ci></apply></apply><ci id="S4.E6.m1.4.4.1.3.cmml" xref="S4.E6.m1.4.4.1.3">ùëâ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.4c">\mathrm{ECA}(Q,V)=\sigma(\mathrm{1DConv(Q)})\otimes V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS3.p6" class="ltx_para">
<table id="S4.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E7.m1.1" class="ltx_Math" alttext="Q=\mathrm{GAP}(X)" display="block"><semantics id="S4.E7.m1.1a"><mrow id="S4.E7.m1.1.2" xref="S4.E7.m1.1.2.cmml"><mi id="S4.E7.m1.1.2.2" xref="S4.E7.m1.1.2.2.cmml">Q</mi><mo id="S4.E7.m1.1.2.1" xref="S4.E7.m1.1.2.1.cmml">=</mo><mrow id="S4.E7.m1.1.2.3" xref="S4.E7.m1.1.2.3.cmml"><mi id="S4.E7.m1.1.2.3.2" xref="S4.E7.m1.1.2.3.2.cmml">GAP</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.2.3.1" xref="S4.E7.m1.1.2.3.1.cmml">‚Äã</mo><mrow id="S4.E7.m1.1.2.3.3.2" xref="S4.E7.m1.1.2.3.cmml"><mo stretchy="false" id="S4.E7.m1.1.2.3.3.2.1" xref="S4.E7.m1.1.2.3.cmml">(</mo><mi id="S4.E7.m1.1.1" xref="S4.E7.m1.1.1.cmml">X</mi><mo stretchy="false" id="S4.E7.m1.1.2.3.3.2.2" xref="S4.E7.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E7.m1.1b"><apply id="S4.E7.m1.1.2.cmml" xref="S4.E7.m1.1.2"><eq id="S4.E7.m1.1.2.1.cmml" xref="S4.E7.m1.1.2.1"></eq><ci id="S4.E7.m1.1.2.2.cmml" xref="S4.E7.m1.1.2.2">ùëÑ</ci><apply id="S4.E7.m1.1.2.3.cmml" xref="S4.E7.m1.1.2.3"><times id="S4.E7.m1.1.2.3.1.cmml" xref="S4.E7.m1.1.2.3.1"></times><ci id="S4.E7.m1.1.2.3.2.cmml" xref="S4.E7.m1.1.2.3.2">GAP</ci><ci id="S4.E7.m1.1.1.cmml" xref="S4.E7.m1.1.1">ùëã</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.1c">Q=\mathrm{GAP}(X)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS3.p7" class="ltx_para">
<p id="S4.SS3.p7.1" class="ltx_p">ECA is simplified by omitting the key from the existing self-attention structure.
Equation (<a href="#S4.E6" title="In IV-C The Design of ECA Module ‚Ä£ IV Model architecture ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) shows the progress of the ECA.
The most significant difference from self-attention in (<a href="#S4.E5" title="In IV-C The Design of ECA Module ‚Ä£ IV Model architecture ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) is that there is no key; however, the importance of the value is judged by the score learned from the query itself.
This simplified attention mechanism is suitable for application in the SER.</p>
</div>
<div id="S4.SS3.p8" class="ltx_para">
<p id="S4.SS3.p8.1" class="ltx_p">The ECA structure is shown in Fig. <a href="#S4.F4" title="Figure 4 ‚Ä£ IV-C The Design of ECA Module ‚Ä£ IV Model architecture ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
The ECA structure can be divided into three steps.
The first involves the preparation of a query.
The query represents the channel features in each input.
We used global average pooling, which can contain temporal-frequency features in each channel without trainable parameters. Please refer (<a href="#S4.E7" title="In IV-C The Design of ECA Module ‚Ä£ IV Model architecture ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<div id="S4.SS3.p9" class="ltx_para">
<p id="S4.SS3.p9.1" class="ltx_p">The next step is to make the score that represents the importance of each channel feature.
We trained the channel feature‚Äôs relation with a 1-D convolution layer to obtain the score.
Specifically, the number of neighboring channel queries that train the relation with the target channel query is determined by the kernel size of the 1-D convolution layer.</p>
</div>
<div id="S4.SS3.p10" class="ltx_para">
<p id="S4.SS3.p10.1" class="ltx_p">Moreover, with the sigmoid function, we score within 0 to 1.
Next, a key difference from the original ECA is the omission of batch normalization.
This change is made because batch normalization tends to extract global features of speech data rather than individual emotional features.</p>
</div>
<div id="S4.SS3.p11" class="ltx_para">
<p id="S4.SS3.p11.1" class="ltx_p">The final step is the re-representation of the input features.
An element-wise product is performed on the input features and the attention channel score.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2409.04007/assets/images/eca_cnn_model.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="299" height="401" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The ECA block position in CNN-based model.</figcaption>
</figure>
<div id="S4.SS3.p12" class="ltx_para">
<p id="S4.SS3.p12.1" class="ltx_p">ECA can efficiently improve the channel feature of the convolution layer, which is essential for classifying speech emotions.
Fig. <a href="#S4.F5" title="Figure 5 ‚Ä£ IV-C The Design of ECA Module ‚Ä£ IV Model architecture ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the ECA block used after the convolution block.
We searched for more appropriate ECA block settings for the CNN-based model.
To achieve this, we compared the different experiments using several kernel sizes and block positions in the model.
Consequently, unlike the original ECA method used in all layers after the convolution block, using some convolution blocks with many channel features can be effective for emotion recognition performance.
This shows that the ECA works well when the complexity of the filter increases.</p>
</div>
<div id="S4.SS3.p13" class="ltx_para">
<p id="S4.SS3.p13.1" class="ltx_p">In particular, the effectiveness of ECA can be increased when augmentation data are used for training.
Typically, sufficient data are required to use an attention-structured neural network effectively.
Therefore, we used eight different datasets as the augmentation data, as listed in Table <a href="#S5.T2" title="Table II ‚Ä£ V-B Data Preprocessing ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.
Consequently, the performance difference between the model combined with the ECA and the model without the ECA is visible.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Weighted Focal Loss</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Most speech emotion datasets have an unbalanced distribution, depending on the emotion labels.
Therefore, when we use cross entropy as a loss function, the model can be focused on a relatively large number of specific emotion labels (neutral and happiness).
In addition, emotion classification is complex, depending on the label.
For example, ‚Äúhappiness‚Äù and ‚Äúangry‚Äù are often misclassified.
Therefore, we used weighted focal loss to deal with those specific situations.
The weighted focal loss can be expressed as follows.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<table id="S4.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E8.m1.5" class="ltx_Math" alttext="\mathrm{WeightedFocal}(p_{\_}i,w_{\_}i)=-\sum_{\_}{i=1}^{n_{\_}{class}}w_{\_}i(1-p_{\_}i)^{\gamma}\log(p_{\_}i)" display="block"><semantics id="S4.E8.m1.5a"><mrow id="S4.E8.m1.5.5" xref="S4.E8.m1.5.5.cmml"><mrow id="S4.E8.m1.3.3.2" xref="S4.E8.m1.3.3.2.cmml"><mi id="S4.E8.m1.3.3.2.4" xref="S4.E8.m1.3.3.2.4.cmml">WeightedFocal</mi><mo lspace="0em" rspace="0em" id="S4.E8.m1.3.3.2.3" xref="S4.E8.m1.3.3.2.3.cmml">‚Äã</mo><mrow id="S4.E8.m1.3.3.2.2.2" xref="S4.E8.m1.3.3.2.2.3.cmml"><mo stretchy="false" id="S4.E8.m1.3.3.2.2.2.3" xref="S4.E8.m1.3.3.2.2.3.cmml">(</mo><mrow id="S4.E8.m1.2.2.1.1.1.1" xref="S4.E8.m1.2.2.1.1.1.1.cmml"><msub id="S4.E8.m1.2.2.1.1.1.1.2" xref="S4.E8.m1.2.2.1.1.1.1.2.cmml"><mi id="S4.E8.m1.2.2.1.1.1.1.2.2" xref="S4.E8.m1.2.2.1.1.1.1.2.2.cmml">p</mi><mi mathvariant="normal" id="S4.E8.m1.2.2.1.1.1.1.2.3" xref="S4.E8.m1.2.2.1.1.1.1.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S4.E8.m1.2.2.1.1.1.1.1" xref="S4.E8.m1.2.2.1.1.1.1.1.cmml">‚Äã</mo><mi id="S4.E8.m1.2.2.1.1.1.1.3" xref="S4.E8.m1.2.2.1.1.1.1.3.cmml">i</mi></mrow><mo id="S4.E8.m1.3.3.2.2.2.4" xref="S4.E8.m1.3.3.2.2.3.cmml">,</mo><mrow id="S4.E8.m1.3.3.2.2.2.2" xref="S4.E8.m1.3.3.2.2.2.2.cmml"><msub id="S4.E8.m1.3.3.2.2.2.2.2" xref="S4.E8.m1.3.3.2.2.2.2.2.cmml"><mi id="S4.E8.m1.3.3.2.2.2.2.2.2" xref="S4.E8.m1.3.3.2.2.2.2.2.2.cmml">w</mi><mi mathvariant="normal" id="S4.E8.m1.3.3.2.2.2.2.2.3" xref="S4.E8.m1.3.3.2.2.2.2.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S4.E8.m1.3.3.2.2.2.2.1" xref="S4.E8.m1.3.3.2.2.2.2.1.cmml">‚Äã</mo><mi id="S4.E8.m1.3.3.2.2.2.2.3" xref="S4.E8.m1.3.3.2.2.2.2.3.cmml">i</mi></mrow><mo stretchy="false" id="S4.E8.m1.3.3.2.2.2.5" xref="S4.E8.m1.3.3.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E8.m1.5.5.6" xref="S4.E8.m1.5.5.6.cmml">=</mo><mrow id="S4.E8.m1.5.5.7" xref="S4.E8.m1.5.5.7.cmml"><mo id="S4.E8.m1.5.5.7a" xref="S4.E8.m1.5.5.7.cmml">‚àí</mo><mrow id="S4.E8.m1.5.5.7.2" xref="S4.E8.m1.5.5.7.2.cmml"><munder id="S4.E8.m1.5.5.7.2.1" xref="S4.E8.m1.5.5.7.2.1.cmml"><mo movablelimits="false" id="S4.E8.m1.5.5.7.2.1.2" xref="S4.E8.m1.5.5.7.2.1.2.cmml">‚àë</mo><mi mathvariant="normal" id="S4.E8.m1.5.5.7.2.1.3" xref="S4.E8.m1.5.5.7.2.1.3.cmml">_</mi></munder><mi id="S4.E8.m1.5.5.7.2.2" xref="S4.E8.m1.5.5.7.2.2.cmml">i</mi></mrow></mrow><mo id="S4.E8.m1.5.5.8" xref="S4.E8.m1.5.5.8.cmml">=</mo><mrow id="S4.E8.m1.5.5.4" xref="S4.E8.m1.5.5.4.cmml"><msup id="S4.E8.m1.5.5.4.4" xref="S4.E8.m1.5.5.4.4.cmml"><mn id="S4.E8.m1.5.5.4.4.2" xref="S4.E8.m1.5.5.4.4.2.cmml">1</mn><mrow id="S4.E8.m1.5.5.4.4.3" xref="S4.E8.m1.5.5.4.4.3.cmml"><msub id="S4.E8.m1.5.5.4.4.3.2" xref="S4.E8.m1.5.5.4.4.3.2.cmml"><mi id="S4.E8.m1.5.5.4.4.3.2.2" xref="S4.E8.m1.5.5.4.4.3.2.2.cmml">n</mi><mi mathvariant="normal" id="S4.E8.m1.5.5.4.4.3.2.3" xref="S4.E8.m1.5.5.4.4.3.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S4.E8.m1.5.5.4.4.3.1" xref="S4.E8.m1.5.5.4.4.3.1.cmml">‚Äã</mo><mi id="S4.E8.m1.5.5.4.4.3.3" xref="S4.E8.m1.5.5.4.4.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E8.m1.5.5.4.4.3.1a" xref="S4.E8.m1.5.5.4.4.3.1.cmml">‚Äã</mo><mi id="S4.E8.m1.5.5.4.4.3.4" xref="S4.E8.m1.5.5.4.4.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E8.m1.5.5.4.4.3.1b" xref="S4.E8.m1.5.5.4.4.3.1.cmml">‚Äã</mo><mi id="S4.E8.m1.5.5.4.4.3.5" xref="S4.E8.m1.5.5.4.4.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E8.m1.5.5.4.4.3.1c" xref="S4.E8.m1.5.5.4.4.3.1.cmml">‚Äã</mo><mi id="S4.E8.m1.5.5.4.4.3.6" xref="S4.E8.m1.5.5.4.4.3.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E8.m1.5.5.4.4.3.1d" xref="S4.E8.m1.5.5.4.4.3.1.cmml">‚Äã</mo><mi id="S4.E8.m1.5.5.4.4.3.7" xref="S4.E8.m1.5.5.4.4.3.7.cmml">s</mi></mrow></msup><mo lspace="0em" rspace="0em" id="S4.E8.m1.5.5.4.3" xref="S4.E8.m1.5.5.4.3.cmml">‚Äã</mo><msub id="S4.E8.m1.5.5.4.5" xref="S4.E8.m1.5.5.4.5.cmml"><mi id="S4.E8.m1.5.5.4.5.2" xref="S4.E8.m1.5.5.4.5.2.cmml">w</mi><mi mathvariant="normal" id="S4.E8.m1.5.5.4.5.3" xref="S4.E8.m1.5.5.4.5.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S4.E8.m1.5.5.4.3a" xref="S4.E8.m1.5.5.4.3.cmml">‚Äã</mo><mi id="S4.E8.m1.5.5.4.6" xref="S4.E8.m1.5.5.4.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E8.m1.5.5.4.3b" xref="S4.E8.m1.5.5.4.3.cmml">‚Äã</mo><msup id="S4.E8.m1.4.4.3.1" xref="S4.E8.m1.4.4.3.1.cmml"><mrow id="S4.E8.m1.4.4.3.1.1.1" xref="S4.E8.m1.4.4.3.1.1.1.1.cmml"><mo stretchy="false" id="S4.E8.m1.4.4.3.1.1.1.2" xref="S4.E8.m1.4.4.3.1.1.1.1.cmml">(</mo><mrow id="S4.E8.m1.4.4.3.1.1.1.1" xref="S4.E8.m1.4.4.3.1.1.1.1.cmml"><mn id="S4.E8.m1.4.4.3.1.1.1.1.2" xref="S4.E8.m1.4.4.3.1.1.1.1.2.cmml">1</mn><mo id="S4.E8.m1.4.4.3.1.1.1.1.1" xref="S4.E8.m1.4.4.3.1.1.1.1.1.cmml">‚àí</mo><mrow id="S4.E8.m1.4.4.3.1.1.1.1.3" xref="S4.E8.m1.4.4.3.1.1.1.1.3.cmml"><msub id="S4.E8.m1.4.4.3.1.1.1.1.3.2" xref="S4.E8.m1.4.4.3.1.1.1.1.3.2.cmml"><mi id="S4.E8.m1.4.4.3.1.1.1.1.3.2.2" xref="S4.E8.m1.4.4.3.1.1.1.1.3.2.2.cmml">p</mi><mi mathvariant="normal" id="S4.E8.m1.4.4.3.1.1.1.1.3.2.3" xref="S4.E8.m1.4.4.3.1.1.1.1.3.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S4.E8.m1.4.4.3.1.1.1.1.3.1" xref="S4.E8.m1.4.4.3.1.1.1.1.3.1.cmml">‚Äã</mo><mi id="S4.E8.m1.4.4.3.1.1.1.1.3.3" xref="S4.E8.m1.4.4.3.1.1.1.1.3.3.cmml">i</mi></mrow></mrow><mo stretchy="false" id="S4.E8.m1.4.4.3.1.1.1.3" xref="S4.E8.m1.4.4.3.1.1.1.1.cmml">)</mo></mrow><mi id="S4.E8.m1.4.4.3.1.3" xref="S4.E8.m1.4.4.3.1.3.cmml">Œ≥</mi></msup><mo lspace="0.167em" rspace="0em" id="S4.E8.m1.5.5.4.3c" xref="S4.E8.m1.5.5.4.3.cmml">‚Äã</mo><mrow id="S4.E8.m1.5.5.4.2.1" xref="S4.E8.m1.5.5.4.2.2.cmml"><mi id="S4.E8.m1.1.1" xref="S4.E8.m1.1.1.cmml">log</mi><mo id="S4.E8.m1.5.5.4.2.1a" xref="S4.E8.m1.5.5.4.2.2.cmml">‚Å°</mo><mrow id="S4.E8.m1.5.5.4.2.1.1" xref="S4.E8.m1.5.5.4.2.2.cmml"><mo stretchy="false" id="S4.E8.m1.5.5.4.2.1.1.2" xref="S4.E8.m1.5.5.4.2.2.cmml">(</mo><mrow id="S4.E8.m1.5.5.4.2.1.1.1" xref="S4.E8.m1.5.5.4.2.1.1.1.cmml"><msub id="S4.E8.m1.5.5.4.2.1.1.1.2" xref="S4.E8.m1.5.5.4.2.1.1.1.2.cmml"><mi id="S4.E8.m1.5.5.4.2.1.1.1.2.2" xref="S4.E8.m1.5.5.4.2.1.1.1.2.2.cmml">p</mi><mi mathvariant="normal" id="S4.E8.m1.5.5.4.2.1.1.1.2.3" xref="S4.E8.m1.5.5.4.2.1.1.1.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S4.E8.m1.5.5.4.2.1.1.1.1" xref="S4.E8.m1.5.5.4.2.1.1.1.1.cmml">‚Äã</mo><mi id="S4.E8.m1.5.5.4.2.1.1.1.3" xref="S4.E8.m1.5.5.4.2.1.1.1.3.cmml">i</mi></mrow><mo stretchy="false" id="S4.E8.m1.5.5.4.2.1.1.3" xref="S4.E8.m1.5.5.4.2.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E8.m1.5b"><apply id="S4.E8.m1.5.5.cmml" xref="S4.E8.m1.5.5"><and id="S4.E8.m1.5.5a.cmml" xref="S4.E8.m1.5.5"></and><apply id="S4.E8.m1.5.5b.cmml" xref="S4.E8.m1.5.5"><eq id="S4.E8.m1.5.5.6.cmml" xref="S4.E8.m1.5.5.6"></eq><apply id="S4.E8.m1.3.3.2.cmml" xref="S4.E8.m1.3.3.2"><times id="S4.E8.m1.3.3.2.3.cmml" xref="S4.E8.m1.3.3.2.3"></times><ci id="S4.E8.m1.3.3.2.4.cmml" xref="S4.E8.m1.3.3.2.4">WeightedFocal</ci><interval closure="open" id="S4.E8.m1.3.3.2.2.3.cmml" xref="S4.E8.m1.3.3.2.2.2"><apply id="S4.E8.m1.2.2.1.1.1.1.cmml" xref="S4.E8.m1.2.2.1.1.1.1"><times id="S4.E8.m1.2.2.1.1.1.1.1.cmml" xref="S4.E8.m1.2.2.1.1.1.1.1"></times><apply id="S4.E8.m1.2.2.1.1.1.1.2.cmml" xref="S4.E8.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E8.m1.2.2.1.1.1.1.2.1.cmml" xref="S4.E8.m1.2.2.1.1.1.1.2">subscript</csymbol><ci id="S4.E8.m1.2.2.1.1.1.1.2.2.cmml" xref="S4.E8.m1.2.2.1.1.1.1.2.2">ùëù</ci><ci id="S4.E8.m1.2.2.1.1.1.1.2.3.cmml" xref="S4.E8.m1.2.2.1.1.1.1.2.3">_</ci></apply><ci id="S4.E8.m1.2.2.1.1.1.1.3.cmml" xref="S4.E8.m1.2.2.1.1.1.1.3">ùëñ</ci></apply><apply id="S4.E8.m1.3.3.2.2.2.2.cmml" xref="S4.E8.m1.3.3.2.2.2.2"><times id="S4.E8.m1.3.3.2.2.2.2.1.cmml" xref="S4.E8.m1.3.3.2.2.2.2.1"></times><apply id="S4.E8.m1.3.3.2.2.2.2.2.cmml" xref="S4.E8.m1.3.3.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E8.m1.3.3.2.2.2.2.2.1.cmml" xref="S4.E8.m1.3.3.2.2.2.2.2">subscript</csymbol><ci id="S4.E8.m1.3.3.2.2.2.2.2.2.cmml" xref="S4.E8.m1.3.3.2.2.2.2.2.2">ùë§</ci><ci id="S4.E8.m1.3.3.2.2.2.2.2.3.cmml" xref="S4.E8.m1.3.3.2.2.2.2.2.3">_</ci></apply><ci id="S4.E8.m1.3.3.2.2.2.2.3.cmml" xref="S4.E8.m1.3.3.2.2.2.2.3">ùëñ</ci></apply></interval></apply><apply id="S4.E8.m1.5.5.7.cmml" xref="S4.E8.m1.5.5.7"><minus id="S4.E8.m1.5.5.7.1.cmml" xref="S4.E8.m1.5.5.7"></minus><apply id="S4.E8.m1.5.5.7.2.cmml" xref="S4.E8.m1.5.5.7.2"><apply id="S4.E8.m1.5.5.7.2.1.cmml" xref="S4.E8.m1.5.5.7.2.1"><csymbol cd="ambiguous" id="S4.E8.m1.5.5.7.2.1.1.cmml" xref="S4.E8.m1.5.5.7.2.1">subscript</csymbol><sum id="S4.E8.m1.5.5.7.2.1.2.cmml" xref="S4.E8.m1.5.5.7.2.1.2"></sum><ci id="S4.E8.m1.5.5.7.2.1.3.cmml" xref="S4.E8.m1.5.5.7.2.1.3">_</ci></apply><ci id="S4.E8.m1.5.5.7.2.2.cmml" xref="S4.E8.m1.5.5.7.2.2">ùëñ</ci></apply></apply></apply><apply id="S4.E8.m1.5.5c.cmml" xref="S4.E8.m1.5.5"><eq id="S4.E8.m1.5.5.8.cmml" xref="S4.E8.m1.5.5.8"></eq><share href="#S4.E8.m1.5.5.7.cmml" id="S4.E8.m1.5.5d.cmml" xref="S4.E8.m1.5.5"></share><apply id="S4.E8.m1.5.5.4.cmml" xref="S4.E8.m1.5.5.4"><times id="S4.E8.m1.5.5.4.3.cmml" xref="S4.E8.m1.5.5.4.3"></times><apply id="S4.E8.m1.5.5.4.4.cmml" xref="S4.E8.m1.5.5.4.4"><csymbol cd="ambiguous" id="S4.E8.m1.5.5.4.4.1.cmml" xref="S4.E8.m1.5.5.4.4">superscript</csymbol><cn type="integer" id="S4.E8.m1.5.5.4.4.2.cmml" xref="S4.E8.m1.5.5.4.4.2">1</cn><apply id="S4.E8.m1.5.5.4.4.3.cmml" xref="S4.E8.m1.5.5.4.4.3"><times id="S4.E8.m1.5.5.4.4.3.1.cmml" xref="S4.E8.m1.5.5.4.4.3.1"></times><apply id="S4.E8.m1.5.5.4.4.3.2.cmml" xref="S4.E8.m1.5.5.4.4.3.2"><csymbol cd="ambiguous" id="S4.E8.m1.5.5.4.4.3.2.1.cmml" xref="S4.E8.m1.5.5.4.4.3.2">subscript</csymbol><ci id="S4.E8.m1.5.5.4.4.3.2.2.cmml" xref="S4.E8.m1.5.5.4.4.3.2.2">ùëõ</ci><ci id="S4.E8.m1.5.5.4.4.3.2.3.cmml" xref="S4.E8.m1.5.5.4.4.3.2.3">_</ci></apply><ci id="S4.E8.m1.5.5.4.4.3.3.cmml" xref="S4.E8.m1.5.5.4.4.3.3">ùëê</ci><ci id="S4.E8.m1.5.5.4.4.3.4.cmml" xref="S4.E8.m1.5.5.4.4.3.4">ùëô</ci><ci id="S4.E8.m1.5.5.4.4.3.5.cmml" xref="S4.E8.m1.5.5.4.4.3.5">ùëé</ci><ci id="S4.E8.m1.5.5.4.4.3.6.cmml" xref="S4.E8.m1.5.5.4.4.3.6">ùë†</ci><ci id="S4.E8.m1.5.5.4.4.3.7.cmml" xref="S4.E8.m1.5.5.4.4.3.7">ùë†</ci></apply></apply><apply id="S4.E8.m1.5.5.4.5.cmml" xref="S4.E8.m1.5.5.4.5"><csymbol cd="ambiguous" id="S4.E8.m1.5.5.4.5.1.cmml" xref="S4.E8.m1.5.5.4.5">subscript</csymbol><ci id="S4.E8.m1.5.5.4.5.2.cmml" xref="S4.E8.m1.5.5.4.5.2">ùë§</ci><ci id="S4.E8.m1.5.5.4.5.3.cmml" xref="S4.E8.m1.5.5.4.5.3">_</ci></apply><ci id="S4.E8.m1.5.5.4.6.cmml" xref="S4.E8.m1.5.5.4.6">ùëñ</ci><apply id="S4.E8.m1.4.4.3.1.cmml" xref="S4.E8.m1.4.4.3.1"><csymbol cd="ambiguous" id="S4.E8.m1.4.4.3.1.2.cmml" xref="S4.E8.m1.4.4.3.1">superscript</csymbol><apply id="S4.E8.m1.4.4.3.1.1.1.1.cmml" xref="S4.E8.m1.4.4.3.1.1.1"><minus id="S4.E8.m1.4.4.3.1.1.1.1.1.cmml" xref="S4.E8.m1.4.4.3.1.1.1.1.1"></minus><cn type="integer" id="S4.E8.m1.4.4.3.1.1.1.1.2.cmml" xref="S4.E8.m1.4.4.3.1.1.1.1.2">1</cn><apply id="S4.E8.m1.4.4.3.1.1.1.1.3.cmml" xref="S4.E8.m1.4.4.3.1.1.1.1.3"><times id="S4.E8.m1.4.4.3.1.1.1.1.3.1.cmml" xref="S4.E8.m1.4.4.3.1.1.1.1.3.1"></times><apply id="S4.E8.m1.4.4.3.1.1.1.1.3.2.cmml" xref="S4.E8.m1.4.4.3.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E8.m1.4.4.3.1.1.1.1.3.2.1.cmml" xref="S4.E8.m1.4.4.3.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E8.m1.4.4.3.1.1.1.1.3.2.2.cmml" xref="S4.E8.m1.4.4.3.1.1.1.1.3.2.2">ùëù</ci><ci id="S4.E8.m1.4.4.3.1.1.1.1.3.2.3.cmml" xref="S4.E8.m1.4.4.3.1.1.1.1.3.2.3">_</ci></apply><ci id="S4.E8.m1.4.4.3.1.1.1.1.3.3.cmml" xref="S4.E8.m1.4.4.3.1.1.1.1.3.3">ùëñ</ci></apply></apply><ci id="S4.E8.m1.4.4.3.1.3.cmml" xref="S4.E8.m1.4.4.3.1.3">ùõæ</ci></apply><apply id="S4.E8.m1.5.5.4.2.2.cmml" xref="S4.E8.m1.5.5.4.2.1"><log id="S4.E8.m1.1.1.cmml" xref="S4.E8.m1.1.1"></log><apply id="S4.E8.m1.5.5.4.2.1.1.1.cmml" xref="S4.E8.m1.5.5.4.2.1.1.1"><times id="S4.E8.m1.5.5.4.2.1.1.1.1.cmml" xref="S4.E8.m1.5.5.4.2.1.1.1.1"></times><apply id="S4.E8.m1.5.5.4.2.1.1.1.2.cmml" xref="S4.E8.m1.5.5.4.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E8.m1.5.5.4.2.1.1.1.2.1.cmml" xref="S4.E8.m1.5.5.4.2.1.1.1.2">subscript</csymbol><ci id="S4.E8.m1.5.5.4.2.1.1.1.2.2.cmml" xref="S4.E8.m1.5.5.4.2.1.1.1.2.2">ùëù</ci><ci id="S4.E8.m1.5.5.4.2.1.1.1.2.3.cmml" xref="S4.E8.m1.5.5.4.2.1.1.1.2.3">_</ci></apply><ci id="S4.E8.m1.5.5.4.2.1.1.1.3.cmml" xref="S4.E8.m1.5.5.4.2.1.1.1.3">ùëñ</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E8.m1.5c">\mathrm{WeightedFocal}(p_{\_}i,w_{\_}i)=-\sum_{\_}{i=1}^{n_{\_}{class}}w_{\_}i(1-p_{\_}i)^{\gamma}\log(p_{\_}i)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.4" class="ltx_p">It has two properties.
First, the focal loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> achieves flexible learning rates in different emotional classes <math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="(n_{\_}{class})" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><mrow id="S4.SS4.p3.1.m1.1.1.1" xref="S4.SS4.p3.1.m1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS4.p3.1.m1.1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.p3.1.m1.1.1.1.1" xref="S4.SS4.p3.1.m1.1.1.1.1.cmml"><msub id="S4.SS4.p3.1.m1.1.1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.1.1.2.cmml"><mi id="S4.SS4.p3.1.m1.1.1.1.1.2.2" xref="S4.SS4.p3.1.m1.1.1.1.1.2.2.cmml">n</mi><mi mathvariant="normal" id="S4.SS4.p3.1.m1.1.1.1.1.2.3" xref="S4.SS4.p3.1.m1.1.1.1.1.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS4.p3.1.m1.1.1.1.1.1" xref="S4.SS4.p3.1.m1.1.1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS4.p3.1.m1.1.1.1.1.3" xref="S4.SS4.p3.1.m1.1.1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p3.1.m1.1.1.1.1.1a" xref="S4.SS4.p3.1.m1.1.1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS4.p3.1.m1.1.1.1.1.4" xref="S4.SS4.p3.1.m1.1.1.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p3.1.m1.1.1.1.1.1b" xref="S4.SS4.p3.1.m1.1.1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS4.p3.1.m1.1.1.1.1.5" xref="S4.SS4.p3.1.m1.1.1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p3.1.m1.1.1.1.1.1c" xref="S4.SS4.p3.1.m1.1.1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS4.p3.1.m1.1.1.1.1.6" xref="S4.SS4.p3.1.m1.1.1.1.1.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p3.1.m1.1.1.1.1.1d" xref="S4.SS4.p3.1.m1.1.1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS4.p3.1.m1.1.1.1.1.7" xref="S4.SS4.p3.1.m1.1.1.1.1.7.cmml">s</mi></mrow><mo stretchy="false" id="S4.SS4.p3.1.m1.1.1.1.3" xref="S4.SS4.p3.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1.1"><times id="S4.SS4.p3.1.m1.1.1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1.1.1.1"></times><apply id="S4.SS4.p3.1.m1.1.1.1.1.2.cmml" xref="S4.SS4.p3.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.p3.1.m1.1.1.1.1.2.1.cmml" xref="S4.SS4.p3.1.m1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS4.p3.1.m1.1.1.1.1.2.2.cmml" xref="S4.SS4.p3.1.m1.1.1.1.1.2.2">ùëõ</ci><ci id="S4.SS4.p3.1.m1.1.1.1.1.2.3.cmml" xref="S4.SS4.p3.1.m1.1.1.1.1.2.3">_</ci></apply><ci id="S4.SS4.p3.1.m1.1.1.1.1.3.cmml" xref="S4.SS4.p3.1.m1.1.1.1.1.3">ùëê</ci><ci id="S4.SS4.p3.1.m1.1.1.1.1.4.cmml" xref="S4.SS4.p3.1.m1.1.1.1.1.4">ùëô</ci><ci id="S4.SS4.p3.1.m1.1.1.1.1.5.cmml" xref="S4.SS4.p3.1.m1.1.1.1.1.5">ùëé</ci><ci id="S4.SS4.p3.1.m1.1.1.1.1.6.cmml" xref="S4.SS4.p3.1.m1.1.1.1.1.6">ùë†</ci><ci id="S4.SS4.p3.1.m1.1.1.1.1.7.cmml" xref="S4.SS4.p3.1.m1.1.1.1.1.7">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">(n_{\_}{class})</annotation></semantics></math>.
The focal loss function is a slightly more generalized function of the cross entropy weighted by the predicted probability of each emotion class <math id="S4.SS4.p3.2.m2.1" class="ltx_Math" alttext="(p_{\_}i)" display="inline"><semantics id="S4.SS4.p3.2.m2.1a"><mrow id="S4.SS4.p3.2.m2.1.1.1" xref="S4.SS4.p3.2.m2.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS4.p3.2.m2.1.1.1.2" xref="S4.SS4.p3.2.m2.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.p3.2.m2.1.1.1.1" xref="S4.SS4.p3.2.m2.1.1.1.1.cmml"><msub id="S4.SS4.p3.2.m2.1.1.1.1.2" xref="S4.SS4.p3.2.m2.1.1.1.1.2.cmml"><mi id="S4.SS4.p3.2.m2.1.1.1.1.2.2" xref="S4.SS4.p3.2.m2.1.1.1.1.2.2.cmml">p</mi><mi mathvariant="normal" id="S4.SS4.p3.2.m2.1.1.1.1.2.3" xref="S4.SS4.p3.2.m2.1.1.1.1.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS4.p3.2.m2.1.1.1.1.1" xref="S4.SS4.p3.2.m2.1.1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS4.p3.2.m2.1.1.1.1.3" xref="S4.SS4.p3.2.m2.1.1.1.1.3.cmml">i</mi></mrow><mo stretchy="false" id="S4.SS4.p3.2.m2.1.1.1.3" xref="S4.SS4.p3.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1.1"><times id="S4.SS4.p3.2.m2.1.1.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1.1.1.1"></times><apply id="S4.SS4.p3.2.m2.1.1.1.1.2.cmml" xref="S4.SS4.p3.2.m2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.p3.2.m2.1.1.1.1.2.1.cmml" xref="S4.SS4.p3.2.m2.1.1.1.1.2">subscript</csymbol><ci id="S4.SS4.p3.2.m2.1.1.1.1.2.2.cmml" xref="S4.SS4.p3.2.m2.1.1.1.1.2.2">ùëù</ci><ci id="S4.SS4.p3.2.m2.1.1.1.1.2.3.cmml" xref="S4.SS4.p3.2.m2.1.1.1.1.2.3">_</ci></apply><ci id="S4.SS4.p3.2.m2.1.1.1.1.3.cmml" xref="S4.SS4.p3.2.m2.1.1.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">(p_{\_}i)</annotation></semantics></math>.
For emotion classes with low probability values, the loss increases.
This causes relatively more learning in backward updates. Conversely, emotion classes with high probability values resulted in relatively less learning.
The hyperparameter gamma (<math id="S4.SS4.p3.3.m3.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S4.SS4.p3.3.m3.1a"><mi id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml">Œ≥</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.1b"><ci id="S4.SS4.p3.3.m3.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1">ùõæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.1c">\gamma</annotation></semantics></math>) controls how dramatically the loss value changes.
In our experiment, we set <math id="S4.SS4.p3.4.m4.1" class="ltx_Math" alttext="\gamma=1" display="inline"><semantics id="S4.SS4.p3.4.m4.1a"><mrow id="S4.SS4.p3.4.m4.1.1" xref="S4.SS4.p3.4.m4.1.1.cmml"><mi id="S4.SS4.p3.4.m4.1.1.2" xref="S4.SS4.p3.4.m4.1.1.2.cmml">Œ≥</mi><mo id="S4.SS4.p3.4.m4.1.1.1" xref="S4.SS4.p3.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS4.p3.4.m4.1.1.3" xref="S4.SS4.p3.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.4.m4.1b"><apply id="S4.SS4.p3.4.m4.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1"><eq id="S4.SS4.p3.4.m4.1.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1.1"></eq><ci id="S4.SS4.p3.4.m4.1.1.2.cmml" xref="S4.SS4.p3.4.m4.1.1.2">ùõæ</ci><cn type="integer" id="S4.SS4.p3.4.m4.1.1.3.cmml" xref="S4.SS4.p3.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.4.m4.1c">\gamma=1</annotation></semantics></math>.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Second, to avoid imbalanced learning owing to the number of emotion classes, the loss function is multiplied by the learning rate weight <math id="S4.SS4.p4.1.m1.1" class="ltx_Math" alttext="(w_{\_}i)" display="inline"><semantics id="S4.SS4.p4.1.m1.1a"><mrow id="S4.SS4.p4.1.m1.1.1.1" xref="S4.SS4.p4.1.m1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS4.p4.1.m1.1.1.1.2" xref="S4.SS4.p4.1.m1.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.p4.1.m1.1.1.1.1" xref="S4.SS4.p4.1.m1.1.1.1.1.cmml"><msub id="S4.SS4.p4.1.m1.1.1.1.1.2" xref="S4.SS4.p4.1.m1.1.1.1.1.2.cmml"><mi id="S4.SS4.p4.1.m1.1.1.1.1.2.2" xref="S4.SS4.p4.1.m1.1.1.1.1.2.2.cmml">w</mi><mi mathvariant="normal" id="S4.SS4.p4.1.m1.1.1.1.1.2.3" xref="S4.SS4.p4.1.m1.1.1.1.1.2.3.cmml">_</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS4.p4.1.m1.1.1.1.1.1" xref="S4.SS4.p4.1.m1.1.1.1.1.1.cmml">‚Äã</mo><mi id="S4.SS4.p4.1.m1.1.1.1.1.3" xref="S4.SS4.p4.1.m1.1.1.1.1.3.cmml">i</mi></mrow><mo stretchy="false" id="S4.SS4.p4.1.m1.1.1.1.3" xref="S4.SS4.p4.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.1.m1.1b"><apply id="S4.SS4.p4.1.m1.1.1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1.1"><times id="S4.SS4.p4.1.m1.1.1.1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1.1.1.1"></times><apply id="S4.SS4.p4.1.m1.1.1.1.1.2.cmml" xref="S4.SS4.p4.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.p4.1.m1.1.1.1.1.2.1.cmml" xref="S4.SS4.p4.1.m1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS4.p4.1.m1.1.1.1.1.2.2.cmml" xref="S4.SS4.p4.1.m1.1.1.1.1.2.2">ùë§</ci><ci id="S4.SS4.p4.1.m1.1.1.1.1.2.3.cmml" xref="S4.SS4.p4.1.m1.1.1.1.1.2.3">_</ci></apply><ci id="S4.SS4.p4.1.m1.1.1.1.1.3.cmml" xref="S4.SS4.p4.1.m1.1.1.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.1.m1.1c">(w_{\_}i)</annotation></semantics></math> based on the number of labels in the training dataset.
The learning rate weight was obtained as the reciprocal ratio of the number of data for each label.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiment Setting and Results</span>
</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Dataset (IEMOCAP)</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The IEMOCAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> is the most popular dataset used in the SER problem.
In this dataset, ten individual actors (five men and five women) recorded their voices, facial movements, and overall behavior for five sessions to understand human emotions in various situations.
The IEMOCAP‚Äôs data are divided into an improvised set, which contains improvised acting, and a script, which acts through dialogue.
Three or more annotators manually classified the emotional speeches, and the final decision was made through a majority vote.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The data samples that we used in the experiment are as follows.
We selected only the Improvised set, considering situations in which human emotions can appear naturally.
Next, we choose five emotional classes corresponding to ‚Äúangry‚Äù, ‚Äúsadness‚Äù, ‚Äúhappiness‚Äù, ‚Äúneutral‚Äù and ‚Äúexcited‚Äù.
These classes are commonly used in various experiments.
Moreover, to compensate for the data sample‚Äôs number of ‚Äúhappiness,‚Äù ‚Äúexcited‚Äù is considered as ‚Äúhappiness‚Äù.
The experiments were conducted using 2943 speech data samples containing four emotional classes (angry:289, sadness:608, happiness:947, neutral:1099).</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Data Preprocessing</span>
</h3>

<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table II: </span>preprocessing setting in each version of datasets</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Dataset Version</th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Window</th>
<th id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Overlap</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<th id="S5.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">1</th>
<td id="S5.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">15 ms</td>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">5 ms</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<th id="S5.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_center">20 ms</td>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_center">10 ms</td>
</tr>
<tr id="S5.T2.1.4.3" class="ltx_tr">
<th id="S5.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">3</th>
<td id="S5.T2.1.4.3.2" class="ltx_td ltx_align_center">25 ms</td>
<td id="S5.T2.1.4.3.3" class="ltx_td ltx_align_center">15 ms</td>
</tr>
<tr id="S5.T2.1.5.4" class="ltx_tr">
<th id="S5.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">4</th>
<td id="S5.T2.1.5.4.2" class="ltx_td ltx_align_center">30 ms</td>
<td id="S5.T2.1.5.4.3" class="ltx_td ltx_align_center">20 ms</td>
</tr>
<tr id="S5.T2.1.6.5" class="ltx_tr">
<th id="S5.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">5</th>
<td id="S5.T2.1.6.5.2" class="ltx_td ltx_align_center">35 ms</td>
<td id="S5.T2.1.6.5.3" class="ltx_td ltx_align_center">25 ms</td>
</tr>
<tr id="S5.T2.1.7.6" class="ltx_tr">
<th id="S5.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">6</th>
<td id="S5.T2.1.7.6.2" class="ltx_td ltx_align_center">40 ms</td>
<td id="S5.T2.1.7.6.3" class="ltx_td ltx_align_center">30 ms</td>
</tr>
<tr id="S5.T2.1.8.7" class="ltx_tr">
<th id="S5.T2.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">7</th>
<td id="S5.T2.1.8.7.2" class="ltx_td ltx_align_center">45 ms</td>
<td id="S5.T2.1.8.7.3" class="ltx_td ltx_align_center">35 ms</td>
</tr>
<tr id="S5.T2.1.9.8" class="ltx_tr">
<th id="S5.T2.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">8</th>
<td id="S5.T2.1.9.8.2" class="ltx_td ltx_align_center ltx_border_b">50 ms</td>
<td id="S5.T2.1.9.8.3" class="ltx_td ltx_align_center ltx_border_b">40 ms</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table III: </span>STFT setting used in previous proposed SER models</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Model Name</th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Method</th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Window</th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Overlap</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<td id="S5.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">ATDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">MFCC</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">48 ms</td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">24 ms</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<td id="S5.T3.1.3.2.1" class="ltx_td ltx_align_left">Area Attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_left">log-Mel</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_left">40 ms</td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_left">10 ms</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<td id="S5.T3.1.4.3.1" class="ltx_td ltx_align_left">MHA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_left">log-Mel</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_left">46 ms</td>
<td id="S5.T3.1.4.3.4" class="ltx_td ltx_align_left">23 ms</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<td id="S5.T3.1.5.4.1" class="ltx_td ltx_align_left">STC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_left">spectrogram</td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_left">16 ms</td>
<td id="S5.T3.1.5.4.4" class="ltx_td ltx_align_left">8 ms</td>
</tr>
<tr id="S5.T3.1.6.5" class="ltx_tr">
<td id="S5.T3.1.6.5.1" class="ltx_td ltx_align_left">HNSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S5.T3.1.6.5.2" class="ltx_td ltx_align_left">log-Mel</td>
<td id="S5.T3.1.6.5.3" class="ltx_td ltx_align_left">25 ms</td>
<td id="S5.T3.1.6.5.4" class="ltx_td ltx_align_left">10 ms</td>
</tr>
<tr id="S5.T3.1.7.6" class="ltx_tr">
<td id="S5.T3.1.7.6.1" class="ltx_td ltx_align_left">TIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S5.T3.1.7.6.2" class="ltx_td ltx_align_left">MFCC</td>
<td id="S5.T3.1.7.6.3" class="ltx_td ltx_align_left">50 ms</td>
<td id="S5.T3.1.7.6.4" class="ltx_td ltx_align_left">38.5 ms</td>
</tr>
<tr id="S5.T3.1.8.7" class="ltx_tr">
<td id="S5.T3.1.8.7.1" class="ltx_td ltx_align_left ltx_border_b">AMSNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S5.T3.1.8.7.2" class="ltx_td ltx_align_left ltx_border_b">spectrogram</td>
<td id="S5.T3.1.8.7.3" class="ltx_td ltx_align_left ltx_border_b">50 ms</td>
<td id="S5.T3.1.8.7.4" class="ltx_td ltx_align_left ltx_border_b">25 ms</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">To efficiently represent the input data, we extracted the log-Mel spectrogram features from the speech signal.
The data preprocessing steps are as follows.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.2" class="ltx_p">First, signal segmentation is performed to equalize the length of the input data.
The IEMOCAP dataset speech samples had an average length of 4.5 s and varied from short (<math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="\sim 0.5s" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml"></mi><mo id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">‚àº</mo><mrow id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml"><mn id="S5.SS2.p2.1.m1.1.1.3.2" xref="S5.SS2.p2.1.m1.1.1.3.2.cmml">0.5</mn><mo lspace="0em" rspace="0em" id="S5.SS2.p2.1.m1.1.1.3.1" xref="S5.SS2.p2.1.m1.1.1.3.1.cmml">‚Äã</mo><mi id="S5.SS2.p2.1.m1.1.1.3.3" xref="S5.SS2.p2.1.m1.1.1.3.3.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">absent</csymbol><apply id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3"><times id="S5.SS2.p2.1.m1.1.1.3.1.cmml" xref="S5.SS2.p2.1.m1.1.1.3.1"></times><cn type="float" id="S5.SS2.p2.1.m1.1.1.3.2.cmml" xref="S5.SS2.p2.1.m1.1.1.3.2">0.5</cn><ci id="S5.SS2.p2.1.m1.1.1.3.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3.3">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\sim 0.5s</annotation></semantics></math>) to long (<math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="\sim 30s" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mrow id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml"><mi id="S5.SS2.p2.2.m2.1.1.2" xref="S5.SS2.p2.2.m2.1.1.2.cmml"></mi><mo id="S5.SS2.p2.2.m2.1.1.1" xref="S5.SS2.p2.2.m2.1.1.1.cmml">‚àº</mo><mrow id="S5.SS2.p2.2.m2.1.1.3" xref="S5.SS2.p2.2.m2.1.1.3.cmml"><mn id="S5.SS2.p2.2.m2.1.1.3.2" xref="S5.SS2.p2.2.m2.1.1.3.2.cmml">30</mn><mo lspace="0em" rspace="0em" id="S5.SS2.p2.2.m2.1.1.3.1" xref="S5.SS2.p2.2.m2.1.1.3.1.cmml">‚Äã</mo><mi id="S5.SS2.p2.2.m2.1.1.3.3" xref="S5.SS2.p2.2.m2.1.1.3.3.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><apply id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.SS2.p2.2.m2.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S5.SS2.p2.2.m2.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.2">absent</csymbol><apply id="S5.SS2.p2.2.m2.1.1.3.cmml" xref="S5.SS2.p2.2.m2.1.1.3"><times id="S5.SS2.p2.2.m2.1.1.3.1.cmml" xref="S5.SS2.p2.2.m2.1.1.3.1"></times><cn type="integer" id="S5.SS2.p2.2.m2.1.1.3.2.cmml" xref="S5.SS2.p2.2.m2.1.1.3.2">30</cn><ci id="S5.SS2.p2.2.m2.1.1.3.3.cmml" xref="S5.SS2.p2.2.m2.1.1.3.3">ùë†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">\sim 30s</annotation></semantics></math>) speech.
Therefore, the lengths of the speech samples were consistent at 6 s, which was slightly longer than the average.
Specifically, if the speech data were shorter than 6 s, zero padding was performed at the beginning and end of the signal with the same length for the signal position in the center.
If the speech was longer than 6 s, both the beginning and ends of the signal were cut to the same size to contain as long an utterance as possible.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">Next, we prepared the different versions of the datasets to search for more effective preprocessing settings with different window sizes and overlaps in the STFT.
Therefore, an interval was set based on previous studies.
As listed in Table <a href="#S5.T3" title="Table III ‚Ä£ V-B Data Preprocessing ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, most previous studies set the window size from 16 ms to 50 ms.
Based on this, we chose eight different window sizes at 5ms intervals within a slightly wider range of 15 ms to 50 ms.
The overlap size was adjusted to obtain the same size of input data.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">In the final step, log-Mel filters were applied to effectively decrease the input data size
Specifically, we use 64-number Mel filters to increase frequency resolution.
The detailed settings for each dataset version of the dataset are listed in Table <a href="#S5.T2" title="Table II ‚Ä£ V-B Data Preprocessing ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.
All preprocessing steps were conducted using MATLAB.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Experimental Setup and Evaluation</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We use 5-fold cross-validation for the entire dataset to ensure general SER performance.
Samples from all the sets were randomly selected.
To evaluate the model performances, we used the unweighted average accuracy (UA) and weighted average accuracy (WA).
These two metrics are commonly used in the SER.
Additionally, to analyze the balanced evaluation, we used the mean of UA and WA (ACC) values.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.4" class="ltx_p">The PyTorch framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, a deep learning framework, was implemented in all the experiments in this study.
The specific hyperparameters of the models were as follows:
All weight parameters were initialized with He initialization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.
For the model‚Äôs optimization, we use the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> with <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><msup id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mn id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">10</mn><mrow id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml"><mo id="S5.SS3.p2.1.m1.1.1.3a" xref="S5.SS3.p2.1.m1.1.1.3.cmml">‚àí</mo><mn id="S5.SS3.p2.1.m1.1.1.3.2" xref="S5.SS3.p2.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">superscript</csymbol><cn type="integer" id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2">10</cn><apply id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3"><minus id="S5.SS3.p2.1.m1.1.1.3.1.cmml" xref="S5.SS3.p2.1.m1.1.1.3"></minus><cn type="integer" id="S5.SS3.p2.1.m1.1.1.3.2.cmml" xref="S5.SS3.p2.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">10^{-4}</annotation></semantics></math> initial learning rates and <math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="10^{-6}" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><msup id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml"><mn id="S5.SS3.p2.2.m2.1.1.2" xref="S5.SS3.p2.2.m2.1.1.2.cmml">10</mn><mrow id="S5.SS3.p2.2.m2.1.1.3" xref="S5.SS3.p2.2.m2.1.1.3.cmml"><mo id="S5.SS3.p2.2.m2.1.1.3a" xref="S5.SS3.p2.2.m2.1.1.3.cmml">‚àí</mo><mn id="S5.SS3.p2.2.m2.1.1.3.2" xref="S5.SS3.p2.2.m2.1.1.3.2.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><apply id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.2.m2.1.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1">superscript</csymbol><cn type="integer" id="S5.SS3.p2.2.m2.1.1.2.cmml" xref="S5.SS3.p2.2.m2.1.1.2">10</cn><apply id="S5.SS3.p2.2.m2.1.1.3.cmml" xref="S5.SS3.p2.2.m2.1.1.3"><minus id="S5.SS3.p2.2.m2.1.1.3.1.cmml" xref="S5.SS3.p2.2.m2.1.1.3"></minus><cn type="integer" id="S5.SS3.p2.2.m2.1.1.3.2.cmml" xref="S5.SS3.p2.2.m2.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">10^{-6}</annotation></semantics></math> decay rates.
The batch size was set to 32, and the focal loss parameter <math id="S5.SS3.p2.3.m3.1" class="ltx_Math" alttext="Œ≥" display="inline"><semantics id="S5.SS3.p2.3.m3.1a"><mi id="S5.SS3.p2.3.m3.1.1" xref="S5.SS3.p2.3.m3.1.1.cmml">Œ≥</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.3.m3.1b"><ci id="S5.SS3.p2.3.m3.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1">ùõæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.3.m3.1c">Œ≥</annotation></semantics></math> was set to <math id="S5.SS3.p2.4.m4.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S5.SS3.p2.4.m4.1a"><mn id="S5.SS3.p2.4.m4.1.1" xref="S5.SS3.p2.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.4.m4.1b"><cn type="integer" id="S5.SS3.p2.4.m4.1.1.cmml" xref="S5.SS3.p2.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.4.m4.1c">1</annotation></semantics></math>.
Finally, the models were trained for 150 epochs.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.4.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.5.2" class="ltx_text ltx_font_italic">Searching the Proper Channel Size for CNN-Based Model Architecture</span>
</h3>

<figure id="S5.F6" class="ltx_figure"><img src="/html/2409.04007/assets/images/cnn_channel_size_result.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="349" height="191" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The comparison of ACC results from models using different channel sizes and versions of the dataset.</figcaption>
</figure>
<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">To extract emotional features from speech data more effectively, we first experimented with the number of channel sizes used in a CNN-based model.
The channel size is the most critical hyperparameter in the convolution layer.
Also, the proper channel size is crucial to efficiently reduce the number of trainable weight parameters.
The optimal number of channels can improve the classification performance of the SER model.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.2" class="ltx_p">For this experiment, we individually trained and evaluated the eight different CNN-based models with variant channel sizes.
The detailed number of channel sizes is set with the parameter <math id="S5.SS4.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.SS4.p2.1.m1.1a"><mi id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><ci id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">n</annotation></semantics></math>, as listed in Table <a href="#S4.T1" title="Table I ‚Ä£ IV-A Convolution Layer ‚Ä£ IV Model architecture ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.
Parameter <math id="S5.SS4.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.SS4.p2.2.m2.1a"><mi id="S5.SS4.p2.2.m2.1.1" xref="S5.SS4.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.2.m2.1b"><ci id="S5.SS4.p2.2.m2.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.2.m2.1c">n</annotation></semantics></math> was selected as an integer ranging from 1 to 8.
In addition, we used eight different datasets to analyze which version of the preprocessing methods best represents the emotional features.
The preprocessing methods are listed in Table <a href="#S5.T2" title="Table II ‚Ä£ V-B Data Preprocessing ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.5" class="ltx_p">Fig. <a href="#S5.F6" title="Figure 6 ‚Ä£ V-D Searching the Proper Channel Size for CNN-Based Model Architecture ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the performance of the entire model for the different versions of datasets.
From the perspective of channel size change, the model performance changes drastically as the channel size increases in range from <math id="S5.SS4.p3.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S5.SS4.p3.1.m1.1a"><mn id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.1b"><cn type="integer" id="S5.SS4.p3.1.m1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.1c">1</annotation></semantics></math> to <math id="S5.SS4.p3.2.m2.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S5.SS4.p3.2.m2.1a"><mn id="S5.SS4.p3.2.m2.1.1" xref="S5.SS4.p3.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.2.m2.1b"><cn type="integer" id="S5.SS4.p3.2.m2.1.1.cmml" xref="S5.SS4.p3.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.2.m2.1c">3</annotation></semantics></math>.
In particular, when comparing the performance of the <math id="S5.SS4.p3.3.m3.1" class="ltx_Math" alttext="n=1" display="inline"><semantics id="S5.SS4.p3.3.m3.1a"><mrow id="S5.SS4.p3.3.m3.1.1" xref="S5.SS4.p3.3.m3.1.1.cmml"><mi id="S5.SS4.p3.3.m3.1.1.2" xref="S5.SS4.p3.3.m3.1.1.2.cmml">n</mi><mo id="S5.SS4.p3.3.m3.1.1.1" xref="S5.SS4.p3.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS4.p3.3.m3.1.1.3" xref="S5.SS4.p3.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.3.m3.1b"><apply id="S5.SS4.p3.3.m3.1.1.cmml" xref="S5.SS4.p3.3.m3.1.1"><eq id="S5.SS4.p3.3.m3.1.1.1.cmml" xref="S5.SS4.p3.3.m3.1.1.1"></eq><ci id="S5.SS4.p3.3.m3.1.1.2.cmml" xref="S5.SS4.p3.3.m3.1.1.2">ùëõ</ci><cn type="integer" id="S5.SS4.p3.3.m3.1.1.3.cmml" xref="S5.SS4.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.3.m3.1c">n=1</annotation></semantics></math> (blue line) and <math id="S5.SS4.p3.4.m4.1" class="ltx_Math" alttext="n=3" display="inline"><semantics id="S5.SS4.p3.4.m4.1a"><mrow id="S5.SS4.p3.4.m4.1.1" xref="S5.SS4.p3.4.m4.1.1.cmml"><mi id="S5.SS4.p3.4.m4.1.1.2" xref="S5.SS4.p3.4.m4.1.1.2.cmml">n</mi><mo id="S5.SS4.p3.4.m4.1.1.1" xref="S5.SS4.p3.4.m4.1.1.1.cmml">=</mo><mn id="S5.SS4.p3.4.m4.1.1.3" xref="S5.SS4.p3.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.4.m4.1b"><apply id="S5.SS4.p3.4.m4.1.1.cmml" xref="S5.SS4.p3.4.m4.1.1"><eq id="S5.SS4.p3.4.m4.1.1.1.cmml" xref="S5.SS4.p3.4.m4.1.1.1"></eq><ci id="S5.SS4.p3.4.m4.1.1.2.cmml" xref="S5.SS4.p3.4.m4.1.1.2">ùëõ</ci><cn type="integer" id="S5.SS4.p3.4.m4.1.1.3.cmml" xref="S5.SS4.p3.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.4.m4.1c">n=3</annotation></semantics></math> (purple line) models, there was a <math id="S5.SS4.p3.5.m5.1" class="ltx_Math" alttext="2\%\sim 3\%" display="inline"><semantics id="S5.SS4.p3.5.m5.1a"><mrow id="S5.SS4.p3.5.m5.1.1" xref="S5.SS4.p3.5.m5.1.1.cmml"><mrow id="S5.SS4.p3.5.m5.1.1.2" xref="S5.SS4.p3.5.m5.1.1.2.cmml"><mn id="S5.SS4.p3.5.m5.1.1.2.2" xref="S5.SS4.p3.5.m5.1.1.2.2.cmml">2</mn><mo id="S5.SS4.p3.5.m5.1.1.2.1" xref="S5.SS4.p3.5.m5.1.1.2.1.cmml">%</mo></mrow><mo id="S5.SS4.p3.5.m5.1.1.1" xref="S5.SS4.p3.5.m5.1.1.1.cmml">‚àº</mo><mrow id="S5.SS4.p3.5.m5.1.1.3" xref="S5.SS4.p3.5.m5.1.1.3.cmml"><mn id="S5.SS4.p3.5.m5.1.1.3.2" xref="S5.SS4.p3.5.m5.1.1.3.2.cmml">3</mn><mo id="S5.SS4.p3.5.m5.1.1.3.1" xref="S5.SS4.p3.5.m5.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.5.m5.1b"><apply id="S5.SS4.p3.5.m5.1.1.cmml" xref="S5.SS4.p3.5.m5.1.1"><csymbol cd="latexml" id="S5.SS4.p3.5.m5.1.1.1.cmml" xref="S5.SS4.p3.5.m5.1.1.1">similar-to</csymbol><apply id="S5.SS4.p3.5.m5.1.1.2.cmml" xref="S5.SS4.p3.5.m5.1.1.2"><csymbol cd="latexml" id="S5.SS4.p3.5.m5.1.1.2.1.cmml" xref="S5.SS4.p3.5.m5.1.1.2.1">percent</csymbol><cn type="integer" id="S5.SS4.p3.5.m5.1.1.2.2.cmml" xref="S5.SS4.p3.5.m5.1.1.2.2">2</cn></apply><apply id="S5.SS4.p3.5.m5.1.1.3.cmml" xref="S5.SS4.p3.5.m5.1.1.3"><csymbol cd="latexml" id="S5.SS4.p3.5.m5.1.1.3.1.cmml" xref="S5.SS4.p3.5.m5.1.1.3.1">percent</csymbol><cn type="integer" id="S5.SS4.p3.5.m5.1.1.3.2.cmml" xref="S5.SS4.p3.5.m5.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.5.m5.1c">2\%\sim 3\%</annotation></semantics></math> gap.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.3" class="ltx_p">Next, the best result was obtained when we used the <math id="S5.SS4.p4.1.m1.1" class="ltx_Math" alttext="n=4" display="inline"><semantics id="S5.SS4.p4.1.m1.1a"><mrow id="S5.SS4.p4.1.m1.1.1" xref="S5.SS4.p4.1.m1.1.1.cmml"><mi id="S5.SS4.p4.1.m1.1.1.2" xref="S5.SS4.p4.1.m1.1.1.2.cmml">n</mi><mo id="S5.SS4.p4.1.m1.1.1.1" xref="S5.SS4.p4.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS4.p4.1.m1.1.1.3" xref="S5.SS4.p4.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.1.m1.1b"><apply id="S5.SS4.p4.1.m1.1.1.cmml" xref="S5.SS4.p4.1.m1.1.1"><eq id="S5.SS4.p4.1.m1.1.1.1.cmml" xref="S5.SS4.p4.1.m1.1.1.1"></eq><ci id="S5.SS4.p4.1.m1.1.1.2.cmml" xref="S5.SS4.p4.1.m1.1.1.2">ùëõ</ci><cn type="integer" id="S5.SS4.p4.1.m1.1.1.3.cmml" xref="S5.SS4.p4.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.1.m1.1c">n=4</annotation></semantics></math> (khaki line) model with the dataset of version 6 (79.16 UA 79.27 WA 79.22 ACC).
In addition, the model with <math id="S5.SS4.p4.2.m2.1" class="ltx_Math" alttext="n=4" display="inline"><semantics id="S5.SS4.p4.2.m2.1a"><mrow id="S5.SS4.p4.2.m2.1.1" xref="S5.SS4.p4.2.m2.1.1.cmml"><mi id="S5.SS4.p4.2.m2.1.1.2" xref="S5.SS4.p4.2.m2.1.1.2.cmml">n</mi><mo id="S5.SS4.p4.2.m2.1.1.1" xref="S5.SS4.p4.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS4.p4.2.m2.1.1.3" xref="S5.SS4.p4.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.2.m2.1b"><apply id="S5.SS4.p4.2.m2.1.1.cmml" xref="S5.SS4.p4.2.m2.1.1"><eq id="S5.SS4.p4.2.m2.1.1.1.cmml" xref="S5.SS4.p4.2.m2.1.1.1"></eq><ci id="S5.SS4.p4.2.m2.1.1.2.cmml" xref="S5.SS4.p4.2.m2.1.1.2">ùëõ</ci><cn type="integer" id="S5.SS4.p4.2.m2.1.1.3.cmml" xref="S5.SS4.p4.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.2.m2.1c">n=4</annotation></semantics></math> (khaki line) shows the best results for datasets of versions 2, 6, and 8.
This shows that an appropriate channel size can lead to a decent performance in the emotion recognition possible.
However, no improvement was observed in the models using a larger channel size <math id="S5.SS4.p4.3.m3.1" class="ltx_Math" alttext="(4\leq n\leq 8)" display="inline"><semantics id="S5.SS4.p4.3.m3.1a"><mrow id="S5.SS4.p4.3.m3.1.1.1" xref="S5.SS4.p4.3.m3.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p4.3.m3.1.1.1.2" xref="S5.SS4.p4.3.m3.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.p4.3.m3.1.1.1.1" xref="S5.SS4.p4.3.m3.1.1.1.1.cmml"><mn id="S5.SS4.p4.3.m3.1.1.1.1.2" xref="S5.SS4.p4.3.m3.1.1.1.1.2.cmml">4</mn><mo id="S5.SS4.p4.3.m3.1.1.1.1.3" xref="S5.SS4.p4.3.m3.1.1.1.1.3.cmml">‚â§</mo><mi id="S5.SS4.p4.3.m3.1.1.1.1.4" xref="S5.SS4.p4.3.m3.1.1.1.1.4.cmml">n</mi><mo id="S5.SS4.p4.3.m3.1.1.1.1.5" xref="S5.SS4.p4.3.m3.1.1.1.1.5.cmml">‚â§</mo><mn id="S5.SS4.p4.3.m3.1.1.1.1.6" xref="S5.SS4.p4.3.m3.1.1.1.1.6.cmml">8</mn></mrow><mo stretchy="false" id="S5.SS4.p4.3.m3.1.1.1.3" xref="S5.SS4.p4.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.3.m3.1b"><apply id="S5.SS4.p4.3.m3.1.1.1.1.cmml" xref="S5.SS4.p4.3.m3.1.1.1"><and id="S5.SS4.p4.3.m3.1.1.1.1a.cmml" xref="S5.SS4.p4.3.m3.1.1.1"></and><apply id="S5.SS4.p4.3.m3.1.1.1.1b.cmml" xref="S5.SS4.p4.3.m3.1.1.1"><leq id="S5.SS4.p4.3.m3.1.1.1.1.3.cmml" xref="S5.SS4.p4.3.m3.1.1.1.1.3"></leq><cn type="integer" id="S5.SS4.p4.3.m3.1.1.1.1.2.cmml" xref="S5.SS4.p4.3.m3.1.1.1.1.2">4</cn><ci id="S5.SS4.p4.3.m3.1.1.1.1.4.cmml" xref="S5.SS4.p4.3.m3.1.1.1.1.4">ùëõ</ci></apply><apply id="S5.SS4.p4.3.m3.1.1.1.1c.cmml" xref="S5.SS4.p4.3.m3.1.1.1"><leq id="S5.SS4.p4.3.m3.1.1.1.1.5.cmml" xref="S5.SS4.p4.3.m3.1.1.1.1.5"></leq><share href="#S5.SS4.p4.3.m3.1.1.1.1.4.cmml" id="S5.SS4.p4.3.m3.1.1.1.1d.cmml" xref="S5.SS4.p4.3.m3.1.1.1"></share><cn type="integer" id="S5.SS4.p4.3.m3.1.1.1.1.6.cmml" xref="S5.SS4.p4.3.m3.1.1.1.1.6">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.3.m3.1c">(4\leq n\leq 8)</annotation></semantics></math>.
Therefore, a larger channel size can be inefficient for training.</p>
</div>
<div id="S5.SS4.p5" class="ltx_para">
<p id="S5.SS4.p5.2" class="ltx_p">In experiments with different versions of datasets, except <math id="S5.SS4.p5.1.m1.1" class="ltx_Math" alttext="n=1" display="inline"><semantics id="S5.SS4.p5.1.m1.1a"><mrow id="S5.SS4.p5.1.m1.1.1" xref="S5.SS4.p5.1.m1.1.1.cmml"><mi id="S5.SS4.p5.1.m1.1.1.2" xref="S5.SS4.p5.1.m1.1.1.2.cmml">n</mi><mo id="S5.SS4.p5.1.m1.1.1.1" xref="S5.SS4.p5.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS4.p5.1.m1.1.1.3" xref="S5.SS4.p5.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p5.1.m1.1b"><apply id="S5.SS4.p5.1.m1.1.1.cmml" xref="S5.SS4.p5.1.m1.1.1"><eq id="S5.SS4.p5.1.m1.1.1.1.cmml" xref="S5.SS4.p5.1.m1.1.1.1"></eq><ci id="S5.SS4.p5.1.m1.1.1.2.cmml" xref="S5.SS4.p5.1.m1.1.1.2">ùëõ</ci><cn type="integer" id="S5.SS4.p5.1.m1.1.1.3.cmml" xref="S5.SS4.p5.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p5.1.m1.1c">n=1</annotation></semantics></math>, the best performance of each model can be observed in the higher versions of the datasets <math id="S5.SS4.p5.2.m2.1" class="ltx_Math" alttext="(5\sim 8)" display="inline"><semantics id="S5.SS4.p5.2.m2.1a"><mrow id="S5.SS4.p5.2.m2.1.1.1" xref="S5.SS4.p5.2.m2.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p5.2.m2.1.1.1.2" xref="S5.SS4.p5.2.m2.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.p5.2.m2.1.1.1.1" xref="S5.SS4.p5.2.m2.1.1.1.1.cmml"><mn id="S5.SS4.p5.2.m2.1.1.1.1.2" xref="S5.SS4.p5.2.m2.1.1.1.1.2.cmml">5</mn><mo id="S5.SS4.p5.2.m2.1.1.1.1.1" xref="S5.SS4.p5.2.m2.1.1.1.1.1.cmml">‚àº</mo><mn id="S5.SS4.p5.2.m2.1.1.1.1.3" xref="S5.SS4.p5.2.m2.1.1.1.1.3.cmml">8</mn></mrow><mo stretchy="false" id="S5.SS4.p5.2.m2.1.1.1.3" xref="S5.SS4.p5.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p5.2.m2.1b"><apply id="S5.SS4.p5.2.m2.1.1.1.1.cmml" xref="S5.SS4.p5.2.m2.1.1.1"><csymbol cd="latexml" id="S5.SS4.p5.2.m2.1.1.1.1.1.cmml" xref="S5.SS4.p5.2.m2.1.1.1.1.1">similar-to</csymbol><cn type="integer" id="S5.SS4.p5.2.m2.1.1.1.1.2.cmml" xref="S5.SS4.p5.2.m2.1.1.1.1.2">5</cn><cn type="integer" id="S5.SS4.p5.2.m2.1.1.1.1.3.cmml" xref="S5.SS4.p5.2.m2.1.1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p5.2.m2.1c">(5\sim 8)</annotation></semantics></math>.
This implies that a larger window size can effectively represent emotional features.</p>
</div>
<div id="S5.SS4.p6" class="ltx_para">
<p id="S5.SS4.p6.1" class="ltx_p">In summary, the channel size is an important factor in the performance of a CNN-based model.
Therefore, in the next experiments, we explored how to improve the training channel features using the ECA.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS5.4.1.1" class="ltx_text">V-E</span> </span><span id="S5.SS5.5.2" class="ltx_text ltx_font_italic">Using Original ECA Method in CNN-Based Models</span>
</h3>

<figure id="S5.F7" class="ltx_figure"><img src="/html/2409.04007/assets/images/original_eca_result.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="349" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The comparison of ACC results of the ECA block used or not in the CNN-based model with <math id="S5.F7.2.m1.1" class="ltx_Math" alttext="n=4" display="inline"><semantics id="S5.F7.2.m1.1b"><mrow id="S5.F7.2.m1.1.1" xref="S5.F7.2.m1.1.1.cmml"><mi id="S5.F7.2.m1.1.1.2" xref="S5.F7.2.m1.1.1.2.cmml">n</mi><mo id="S5.F7.2.m1.1.1.1" xref="S5.F7.2.m1.1.1.1.cmml">=</mo><mn id="S5.F7.2.m1.1.1.3" xref="S5.F7.2.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F7.2.m1.1c"><apply id="S5.F7.2.m1.1.1.cmml" xref="S5.F7.2.m1.1.1"><eq id="S5.F7.2.m1.1.1.1.cmml" xref="S5.F7.2.m1.1.1.1"></eq><ci id="S5.F7.2.m1.1.1.2.cmml" xref="S5.F7.2.m1.1.1.2">ùëõ</ci><cn type="integer" id="S5.F7.2.m1.1.1.3.cmml" xref="S5.F7.2.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F7.2.m1.1d">n=4</annotation></semantics></math>. The proposed ECA block shows more effective results than the original ECA block in the version 7 and 8 datasets.</figcaption>
</figure>
<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.2" class="ltx_p">To efficiently increase the channel feature representation, we use the ECA blocks in a CNN-based model.
We first experimented with the original ECA block.
For the experiments, we selected the CNN-based model (<math id="S5.SS5.p1.1.m1.1" class="ltx_Math" alttext="n=4" display="inline"><semantics id="S5.SS5.p1.1.m1.1a"><mrow id="S5.SS5.p1.1.m1.1.1" xref="S5.SS5.p1.1.m1.1.1.cmml"><mi id="S5.SS5.p1.1.m1.1.1.2" xref="S5.SS5.p1.1.m1.1.1.2.cmml">n</mi><mo id="S5.SS5.p1.1.m1.1.1.1" xref="S5.SS5.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS5.p1.1.m1.1.1.3" xref="S5.SS5.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.1.m1.1b"><apply id="S5.SS5.p1.1.m1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1"><eq id="S5.SS5.p1.1.m1.1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1.1"></eq><ci id="S5.SS5.p1.1.m1.1.1.2.cmml" xref="S5.SS5.p1.1.m1.1.1.2">ùëõ</ci><cn type="integer" id="S5.SS5.p1.1.m1.1.1.3.cmml" xref="S5.SS5.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.1.m1.1c">n=4</annotation></semantics></math>) that achieved the best performance in a channel size search experiment in section <a href="#S5.SS4" title="V-D Searching the Proper Channel Size for CNN-Based Model Architecture ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-D</span></span></a>.
Moreover, the original ECA blocks in this model were applied.
The original ECA is positioned after each convolution layer.
Therefore six ECA blocks were added to the CNN-based model.
Next, the ECA‚Äôs kernel sizes <math id="S5.SS5.p1.2.m2.1" class="ltx_Math" alttext="(k)" display="inline"><semantics id="S5.SS5.p1.2.m2.1a"><mrow id="S5.SS5.p1.2.m2.1.2.2"><mo stretchy="false" id="S5.SS5.p1.2.m2.1.2.2.1">(</mo><mi id="S5.SS5.p1.2.m2.1.1" xref="S5.SS5.p1.2.m2.1.1.cmml">k</mi><mo stretchy="false" id="S5.SS5.p1.2.m2.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.2.m2.1b"><ci id="S5.SS5.p1.2.m2.1.1.cmml" xref="S5.SS5.p1.2.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.2.m2.1c">(k)</annotation></semantics></math> were set based on the channel size of each layer.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.2" class="ltx_p">Fig. <a href="#S5.F7" title="Figure 7 ‚Ä£ V-E Using Original ECA Method in CNN-Based Models ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the performance of each dataset before and after applying ECA.
Compared with the CNN-based model, the original ECA method (red line) showed an overall decrease in emotion recognition performance from <math id="S5.SS5.p2.1.m1.1" class="ltx_Math" alttext="0.1\%" display="inline"><semantics id="S5.SS5.p2.1.m1.1a"><mrow id="S5.SS5.p2.1.m1.1.1" xref="S5.SS5.p2.1.m1.1.1.cmml"><mn id="S5.SS5.p2.1.m1.1.1.2" xref="S5.SS5.p2.1.m1.1.1.2.cmml">0.1</mn><mo id="S5.SS5.p2.1.m1.1.1.1" xref="S5.SS5.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.1.m1.1b"><apply id="S5.SS5.p2.1.m1.1.1.cmml" xref="S5.SS5.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.SS5.p2.1.m1.1.1.1.cmml" xref="S5.SS5.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS5.p2.1.m1.1.1.2.cmml" xref="S5.SS5.p2.1.m1.1.1.2">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.1.m1.1c">0.1\%</annotation></semantics></math> to <math id="S5.SS5.p2.2.m2.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="S5.SS5.p2.2.m2.1a"><mrow id="S5.SS5.p2.2.m2.1.1" xref="S5.SS5.p2.2.m2.1.1.cmml"><mn id="S5.SS5.p2.2.m2.1.1.2" xref="S5.SS5.p2.2.m2.1.1.2.cmml">1</mn><mo id="S5.SS5.p2.2.m2.1.1.1" xref="S5.SS5.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.2.m2.1b"><apply id="S5.SS5.p2.2.m2.1.1.cmml" xref="S5.SS5.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.SS5.p2.2.m2.1.1.1.cmml" xref="S5.SS5.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.SS5.p2.2.m2.1.1.2.cmml" xref="S5.SS5.p2.2.m2.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.2.m2.1c">1\%</annotation></semantics></math>.
This implies that the original ECA method is not suitable for a direct application.
Therefore, we experimented to determine a more appropriate way to use the ECA block.</p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.2" class="ltx_p">The differences between the original ECA and our ECA methods are as follows:
First, we applied the ECA blocks after the 5th and 6th convolution layers, which were relatively deeper than the other layers.
In addition, a larger kernel size <math id="S5.SS5.p3.1.m1.1" class="ltx_Math" alttext="(k=7)" display="inline"><semantics id="S5.SS5.p3.1.m1.1a"><mrow id="S5.SS5.p3.1.m1.1.1.1" xref="S5.SS5.p3.1.m1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS5.p3.1.m1.1.1.1.2" xref="S5.SS5.p3.1.m1.1.1.1.1.cmml">(</mo><mrow id="S5.SS5.p3.1.m1.1.1.1.1" xref="S5.SS5.p3.1.m1.1.1.1.1.cmml"><mi id="S5.SS5.p3.1.m1.1.1.1.1.2" xref="S5.SS5.p3.1.m1.1.1.1.1.2.cmml">k</mi><mo id="S5.SS5.p3.1.m1.1.1.1.1.1" xref="S5.SS5.p3.1.m1.1.1.1.1.1.cmml">=</mo><mn id="S5.SS5.p3.1.m1.1.1.1.1.3" xref="S5.SS5.p3.1.m1.1.1.1.1.3.cmml">7</mn></mrow><mo stretchy="false" id="S5.SS5.p3.1.m1.1.1.1.3" xref="S5.SS5.p3.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p3.1.m1.1b"><apply id="S5.SS5.p3.1.m1.1.1.1.1.cmml" xref="S5.SS5.p3.1.m1.1.1.1"><eq id="S5.SS5.p3.1.m1.1.1.1.1.1.cmml" xref="S5.SS5.p3.1.m1.1.1.1.1.1"></eq><ci id="S5.SS5.p3.1.m1.1.1.1.1.2.cmml" xref="S5.SS5.p3.1.m1.1.1.1.1.2">ùëò</ci><cn type="integer" id="S5.SS5.p3.1.m1.1.1.1.1.3.cmml" xref="S5.SS5.p3.1.m1.1.1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p3.1.m1.1c">(k=7)</annotation></semantics></math> was used compared to the original kernel size <math id="S5.SS5.p3.2.m2.1" class="ltx_Math" alttext="(k=3)" display="inline"><semantics id="S5.SS5.p3.2.m2.1a"><mrow id="S5.SS5.p3.2.m2.1.1.1" xref="S5.SS5.p3.2.m2.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS5.p3.2.m2.1.1.1.2" xref="S5.SS5.p3.2.m2.1.1.1.1.cmml">(</mo><mrow id="S5.SS5.p3.2.m2.1.1.1.1" xref="S5.SS5.p3.2.m2.1.1.1.1.cmml"><mi id="S5.SS5.p3.2.m2.1.1.1.1.2" xref="S5.SS5.p3.2.m2.1.1.1.1.2.cmml">k</mi><mo id="S5.SS5.p3.2.m2.1.1.1.1.1" xref="S5.SS5.p3.2.m2.1.1.1.1.1.cmml">=</mo><mn id="S5.SS5.p3.2.m2.1.1.1.1.3" xref="S5.SS5.p3.2.m2.1.1.1.1.3.cmml">3</mn></mrow><mo stretchy="false" id="S5.SS5.p3.2.m2.1.1.1.3" xref="S5.SS5.p3.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p3.2.m2.1b"><apply id="S5.SS5.p3.2.m2.1.1.1.1.cmml" xref="S5.SS5.p3.2.m2.1.1.1"><eq id="S5.SS5.p3.2.m2.1.1.1.1.1.cmml" xref="S5.SS5.p3.2.m2.1.1.1.1.1"></eq><ci id="S5.SS5.p3.2.m2.1.1.1.1.2.cmml" xref="S5.SS5.p3.2.m2.1.1.1.1.2">ùëò</ci><cn type="integer" id="S5.SS5.p3.2.m2.1.1.1.1.3.cmml" xref="S5.SS5.p3.2.m2.1.1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p3.2.m2.1c">(k=3)</annotation></semantics></math>.
By applying our ECA, the number of trainable parameters increased by only 14.</p>
</div>
<div id="S5.SS5.p4" class="ltx_para">
<p id="S5.SS5.p4.1" class="ltx_p">Consequently, unlike the original ECA method, the proposed ECA method (purple line) obtained a <math id="S5.SS5.p4.1.m1.1" class="ltx_Math" alttext="0.3\%" display="inline"><semantics id="S5.SS5.p4.1.m1.1a"><mrow id="S5.SS5.p4.1.m1.1.1" xref="S5.SS5.p4.1.m1.1.1.cmml"><mn id="S5.SS5.p4.1.m1.1.1.2" xref="S5.SS5.p4.1.m1.1.1.2.cmml">0.3</mn><mo id="S5.SS5.p4.1.m1.1.1.1" xref="S5.SS5.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p4.1.m1.1b"><apply id="S5.SS5.p4.1.m1.1.1.cmml" xref="S5.SS5.p4.1.m1.1.1"><csymbol cd="latexml" id="S5.SS5.p4.1.m1.1.1.1.cmml" xref="S5.SS5.p4.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS5.p4.1.m1.1.1.2.cmml" xref="S5.SS5.p4.1.m1.1.1.2">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p4.1.m1.1c">0.3\%</annotation></semantics></math> higher performance than that without an ECA block.
In particular, the best performance (79.37 UA 79.68 WA 79.53 ACC) was obtained when we trained with dataset version 8.
This result indicates that the ECA method can improve performance using SER properly.
The channel features of deeper layers are particularly important for extracting the speech emotion features.</p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS6.4.1.1" class="ltx_text">V-F</span> </span><span id="S5.SS6.5.2" class="ltx_text ltx_font_italic">Searching the Proper ECA Block Usage with Different Version of Datasets</span>
</h3>

<figure id="S5.F8" class="ltx_figure"><img src="/html/2409.04007/assets/images/eca_search_result.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="349" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The comparison of ACC results with the ECA blocks used in different layers of CNN-based model with different kernel sizes.</figcaption>
</figure>
<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p">To effectively use ECA blocks, we experimented with how the model performance changes when using the ECA blocks in different positions on a CNN-based model with different kernel sizes <math id="S5.SS6.p1.1.m1.1" class="ltx_Math" alttext="(k)" display="inline"><semantics id="S5.SS6.p1.1.m1.1a"><mrow id="S5.SS6.p1.1.m1.1.2.2"><mo stretchy="false" id="S5.SS6.p1.1.m1.1.2.2.1">(</mo><mi id="S5.SS6.p1.1.m1.1.1" xref="S5.SS6.p1.1.m1.1.1.cmml">k</mi><mo stretchy="false" id="S5.SS6.p1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.p1.1.m1.1b"><ci id="S5.SS6.p1.1.m1.1.1.cmml" xref="S5.SS6.p1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p1.1.m1.1c">(k)</annotation></semantics></math>.
A CNN-based model has a structure in which the channel size increases as the layer deepens; therefore, the complexity of the channel features in deep layers is relatively high.
Based on that situation, we want to determine where the ECA block can be more effective in helping to train the channel features.
Specifically, the experiment was conducted by sequentially adding the ECA blocks starting from the sixth convolution layer, which was the deepest layer in the model.</p>
</div>
<div id="S5.SS6.p2" class="ltx_para">
<p id="S5.SS6.p2.1" class="ltx_p">Subsequently, we changed the kernel size of the 1-d convolution layer of the ECA block.
The kernel size is the length of the local region in which the relationships between neighboring channel features are learned.
In our experiments, four different kernel sizes (3, 5, 7, and 9) were used to verify the change in performance according to the kernel size.
Moreover, we excluded the cases in which the kernel size was larger than nine because of poor performance.
However, to determine the best kernel size, we must train <math id="S5.SS6.p2.1.m1.1" class="ltx_Math" alttext="(4+1)^{6}" display="inline"><semantics id="S5.SS6.p2.1.m1.1a"><msup id="S5.SS6.p2.1.m1.1.1" xref="S5.SS6.p2.1.m1.1.1.cmml"><mrow id="S5.SS6.p2.1.m1.1.1.1.1" xref="S5.SS6.p2.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS6.p2.1.m1.1.1.1.1.2" xref="S5.SS6.p2.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS6.p2.1.m1.1.1.1.1.1" xref="S5.SS6.p2.1.m1.1.1.1.1.1.cmml"><mn id="S5.SS6.p2.1.m1.1.1.1.1.1.2" xref="S5.SS6.p2.1.m1.1.1.1.1.1.2.cmml">4</mn><mo id="S5.SS6.p2.1.m1.1.1.1.1.1.1" xref="S5.SS6.p2.1.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S5.SS6.p2.1.m1.1.1.1.1.1.3" xref="S5.SS6.p2.1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S5.SS6.p2.1.m1.1.1.1.1.3" xref="S5.SS6.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S5.SS6.p2.1.m1.1.1.3" xref="S5.SS6.p2.1.m1.1.1.3.cmml">6</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS6.p2.1.m1.1b"><apply id="S5.SS6.p2.1.m1.1.1.cmml" xref="S5.SS6.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS6.p2.1.m1.1.1.2.cmml" xref="S5.SS6.p2.1.m1.1.1">superscript</csymbol><apply id="S5.SS6.p2.1.m1.1.1.1.1.1.cmml" xref="S5.SS6.p2.1.m1.1.1.1.1"><plus id="S5.SS6.p2.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS6.p2.1.m1.1.1.1.1.1.1"></plus><cn type="integer" id="S5.SS6.p2.1.m1.1.1.1.1.1.2.cmml" xref="S5.SS6.p2.1.m1.1.1.1.1.1.2">4</cn><cn type="integer" id="S5.SS6.p2.1.m1.1.1.1.1.1.3.cmml" xref="S5.SS6.p2.1.m1.1.1.1.1.1.3">1</cn></apply><cn type="integer" id="S5.SS6.p2.1.m1.1.1.3.cmml" xref="S5.SS6.p2.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p2.1.m1.1c">(4+1)^{6}</annotation></semantics></math> times to consider all case sizes according to the ECA block.
Therefore, we skipped some cases of the experiments owing to limitations in computational resources.
In all the experiments, we used the dataset of version 8.</p>
</div>
<div id="S5.SS6.p3" class="ltx_para">
<p id="S5.SS6.p3.1" class="ltx_p">Fig. <a href="#S5.F8" title="Figure 8 ‚Ä£ V-F Searching the Proper ECA Block Usage with Different Version of Datasets ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows that the performance decreases when ECA blocks are used after most convolution layers, whereas the performance improves when ECA blocks are used for relatively deep layers (3 to 6 layers).
In particular, when ECA blocks are used on the fifth and sixth floors, the highest performance of the model without the ECA block (orange line, 79.16 UA 79.27 WA 79.22 ACC) was improved by approximately <math id="S5.SS6.p3.1.m1.1" class="ltx_Math" alttext="0.3\%" display="inline"><semantics id="S5.SS6.p3.1.m1.1a"><mrow id="S5.SS6.p3.1.m1.1.1" xref="S5.SS6.p3.1.m1.1.1.cmml"><mn id="S5.SS6.p3.1.m1.1.1.2" xref="S5.SS6.p3.1.m1.1.1.2.cmml">0.3</mn><mo id="S5.SS6.p3.1.m1.1.1.1" xref="S5.SS6.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.p3.1.m1.1b"><apply id="S5.SS6.p3.1.m1.1.1.cmml" xref="S5.SS6.p3.1.m1.1.1"><csymbol cd="latexml" id="S5.SS6.p3.1.m1.1.1.1.cmml" xref="S5.SS6.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS6.p3.1.m1.1.1.2.cmml" xref="S5.SS6.p3.1.m1.1.1.2">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p3.1.m1.1c">0.3\%</annotation></semantics></math> based on the ACC.
In addition, it shows high performance in many cases compared to the best result when the original ECA blocks are used (pink line, 78.71 UA 78.80 WA 78.76 ACC).
These results show that the ECA block positioned in the deep-layer channel features was effective.</p>
</div>
<div id="S5.SS6.p4" class="ltx_para">
<p id="S5.SS6.p4.5" class="ltx_p">Next, if the results are analyzed according to the kernel size, the kernel size that shows the best performance is <math id="S5.SS6.p4.1.m1.1" class="ltx_Math" alttext="k=7" display="inline"><semantics id="S5.SS6.p4.1.m1.1a"><mrow id="S5.SS6.p4.1.m1.1.1" xref="S5.SS6.p4.1.m1.1.1.cmml"><mi id="S5.SS6.p4.1.m1.1.1.2" xref="S5.SS6.p4.1.m1.1.1.2.cmml">k</mi><mo id="S5.SS6.p4.1.m1.1.1.1" xref="S5.SS6.p4.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS6.p4.1.m1.1.1.3" xref="S5.SS6.p4.1.m1.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.p4.1.m1.1b"><apply id="S5.SS6.p4.1.m1.1.1.cmml" xref="S5.SS6.p4.1.m1.1.1"><eq id="S5.SS6.p4.1.m1.1.1.1.cmml" xref="S5.SS6.p4.1.m1.1.1.1"></eq><ci id="S5.SS6.p4.1.m1.1.1.2.cmml" xref="S5.SS6.p4.1.m1.1.1.2">ùëò</ci><cn type="integer" id="S5.SS6.p4.1.m1.1.1.3.cmml" xref="S5.SS6.p4.1.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p4.1.m1.1c">k=7</annotation></semantics></math> (purple line).
Compared with the other kernel sizes (3, 5, and 9), <math id="S5.SS6.p4.2.m2.1" class="ltx_Math" alttext="k=7" display="inline"><semantics id="S5.SS6.p4.2.m2.1a"><mrow id="S5.SS6.p4.2.m2.1.1" xref="S5.SS6.p4.2.m2.1.1.cmml"><mi id="S5.SS6.p4.2.m2.1.1.2" xref="S5.SS6.p4.2.m2.1.1.2.cmml">k</mi><mo id="S5.SS6.p4.2.m2.1.1.1" xref="S5.SS6.p4.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS6.p4.2.m2.1.1.3" xref="S5.SS6.p4.2.m2.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.p4.2.m2.1b"><apply id="S5.SS6.p4.2.m2.1.1.cmml" xref="S5.SS6.p4.2.m2.1.1"><eq id="S5.SS6.p4.2.m2.1.1.1.cmml" xref="S5.SS6.p4.2.m2.1.1.1"></eq><ci id="S5.SS6.p4.2.m2.1.1.2.cmml" xref="S5.SS6.p4.2.m2.1.1.2">ùëò</ci><cn type="integer" id="S5.SS6.p4.2.m2.1.1.3.cmml" xref="S5.SS6.p4.2.m2.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p4.2.m2.1c">k=7</annotation></semantics></math> had an overall high accuracy range (<math id="S5.SS6.p4.3.m3.1" class="ltx_Math" alttext="78.59\sim 79.53" display="inline"><semantics id="S5.SS6.p4.3.m3.1a"><mrow id="S5.SS6.p4.3.m3.1.1" xref="S5.SS6.p4.3.m3.1.1.cmml"><mn id="S5.SS6.p4.3.m3.1.1.2" xref="S5.SS6.p4.3.m3.1.1.2.cmml">78.59</mn><mo id="S5.SS6.p4.3.m3.1.1.1" xref="S5.SS6.p4.3.m3.1.1.1.cmml">‚àº</mo><mn id="S5.SS6.p4.3.m3.1.1.3" xref="S5.SS6.p4.3.m3.1.1.3.cmml">79.53</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.p4.3.m3.1b"><apply id="S5.SS6.p4.3.m3.1.1.cmml" xref="S5.SS6.p4.3.m3.1.1"><csymbol cd="latexml" id="S5.SS6.p4.3.m3.1.1.1.cmml" xref="S5.SS6.p4.3.m3.1.1.1">similar-to</csymbol><cn type="float" id="S5.SS6.p4.3.m3.1.1.2.cmml" xref="S5.SS6.p4.3.m3.1.1.2">78.59</cn><cn type="float" id="S5.SS6.p4.3.m3.1.1.3.cmml" xref="S5.SS6.p4.3.m3.1.1.3">79.53</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p4.3.m3.1c">78.59\sim 79.53</annotation></semantics></math> ACC).
However, <math id="S5.SS6.p4.4.m4.1" class="ltx_Math" alttext="k=7" display="inline"><semantics id="S5.SS6.p4.4.m4.1a"><mrow id="S5.SS6.p4.4.m4.1.1" xref="S5.SS6.p4.4.m4.1.1.cmml"><mi id="S5.SS6.p4.4.m4.1.1.2" xref="S5.SS6.p4.4.m4.1.1.2.cmml">k</mi><mo id="S5.SS6.p4.4.m4.1.1.1" xref="S5.SS6.p4.4.m4.1.1.1.cmml">=</mo><mn id="S5.SS6.p4.4.m4.1.1.3" xref="S5.SS6.p4.4.m4.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.p4.4.m4.1b"><apply id="S5.SS6.p4.4.m4.1.1.cmml" xref="S5.SS6.p4.4.m4.1.1"><eq id="S5.SS6.p4.4.m4.1.1.1.cmml" xref="S5.SS6.p4.4.m4.1.1.1"></eq><ci id="S5.SS6.p4.4.m4.1.1.2.cmml" xref="S5.SS6.p4.4.m4.1.1.2">ùëò</ci><cn type="integer" id="S5.SS6.p4.4.m4.1.1.3.cmml" xref="S5.SS6.p4.4.m4.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p4.4.m4.1c">k=7</annotation></semantics></math> did not always exhibit the best performance in any of the cases.
For example, the four ECA blocks used in model <math id="S5.SS6.p4.5.m5.1" class="ltx_Math" alttext="k=9" display="inline"><semantics id="S5.SS6.p4.5.m5.1a"><mrow id="S5.SS6.p4.5.m5.1.1" xref="S5.SS6.p4.5.m5.1.1.cmml"><mi id="S5.SS6.p4.5.m5.1.1.2" xref="S5.SS6.p4.5.m5.1.1.2.cmml">k</mi><mo id="S5.SS6.p4.5.m5.1.1.1" xref="S5.SS6.p4.5.m5.1.1.1.cmml">=</mo><mn id="S5.SS6.p4.5.m5.1.1.3" xref="S5.SS6.p4.5.m5.1.1.3.cmml">9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS6.p4.5.m5.1b"><apply id="S5.SS6.p4.5.m5.1.1.cmml" xref="S5.SS6.p4.5.m5.1.1"><eq id="S5.SS6.p4.5.m5.1.1.1.cmml" xref="S5.SS6.p4.5.m5.1.1.1"></eq><ci id="S5.SS6.p4.5.m5.1.1.2.cmml" xref="S5.SS6.p4.5.m5.1.1.2">ùëò</ci><cn type="integer" id="S5.SS6.p4.5.m5.1.1.3.cmml" xref="S5.SS6.p4.5.m5.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p4.5.m5.1c">k=9</annotation></semantics></math> (khaki line) performed better than the others (1, 3, and 5).</p>
</div>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2409.04007/assets/images/n4_eca_result.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="349" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>The comparison of ACC results used different versions of datasets with the CNN-based model using the ECA blocks in layers 5 and 6.</figcaption>
</figure>
<div id="S5.SS6.p5" class="ltx_para">
<p id="S5.SS6.p5.1" class="ltx_p">In the next experiment, we examined the effect of the ECA block using eight different emotional speech preprocessing datasets.
For this purpose, a CNN-based model with the ECA blocks on the fifth and sixth convolution layers was used, which were the cases with the highest performance in the previous experiment.
In addition, four kernel sizes (3, 5, 7, and 9) were used.</p>
</div>
<div id="S5.SS6.p6" class="ltx_para">
<p id="S5.SS6.p6.1" class="ltx_p">As shown in Fig. <a href="#S5.F9" title="Figure 9 ‚Ä£ V-F Searching the Proper ECA Block Usage with Different Version of Datasets ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, the model performance tended to increase from dataset versions 1 to 8
In particular, as in the previous experiment, the version 8 dataset showed better results than the other version datasets in most cases.
This indicates that a large-sized window in emotional speech preprocessing is effective.
However, for most dataset versions, the performance was lower than that of CNN-based models alone.
Eventually, it will be essential to train the model using fined emotional features to obtain an effect from the attention layer.</p>
</div>
</section>
<section id="S5.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS7.4.1.1" class="ltx_text">V-G</span> </span><span id="S5.SS7.5.2" class="ltx_text ltx_font_italic">Augmentation Method with Different Version of STFT Datasets</span>
</h3>

<div id="S5.SS7.p1" class="ltx_para">
<p id="S5.SS7.p1.1" class="ltx_p">To overcome the limitations of representing speech emotional features obtained using only one preprocessing method, multiple preprocessing data augmentation experiments were performed.
For this purpose, only the training dataset was added from the eight different versions of the datasets obtained by setting listed in Table <a href="#S4.T1" title="Table I ‚Ä£ IV-A Convolution Layer ‚Ä£ IV Model architecture ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.
Because each of the eight preprocessing methods has a different window size and overlap size, the model can train with richer emotional features.</p>
</div>
<figure id="S5.F10" class="ltx_figure"><img src="/html/2409.04007/assets/images/ascending_augment.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="349" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>The comparison of ACC results used the STFT data augmentations. The number of dataset are selected with dataset versions in ascending order.</figcaption>
</figure>
<figure id="S5.F11" class="ltx_figure"><img src="/html/2409.04007/assets/images/descending_augment.png" id="S5.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="349" height="191" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>The comparison of ACC results used the STFT data augmentations. The number of dataset are selected with dataset versions in descending order.</figcaption>
</figure>
<div id="S5.SS7.p2" class="ltx_para">
<p id="S5.SS7.p2.1" class="ltx_p">We conducted two different experiments depending on the dataset selection methods to determine out the effect of multiple preprocessing data augmentation on SER.
In the first case, we selected version 1 as the test set and collected training data samples from version 2 to version 8 in ascending order.
Second, in contrast to the first, we selected dataset version 8 as the test set and collected the training data samples in descending order from version 7 to version 1.
The models used in the experiment are CNN-based models with ECA blocks and models without an ECA block.</p>
</div>
<div id="S5.SS7.p3" class="ltx_para">
<p id="S5.SS7.p3.1" class="ltx_p">Fig. <a href="#S5.F10" title="Figure 10 ‚Ä£ V-G Augmentation Method with Different Version of STFT Datasets ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows the augmentation experiments in ascending order.
In most cases, the results were higher than those in the cases where the augmentation method was not applied to either model.
Specifically, the best results in the CNN-based model were obtained using all the preprocessing datasets (80.10 UA 80.02 WA 80.06 ACC).
In the case of models with ECA blocks, the best results (79.69 UA 79.51 WA 79.60 ACC) were obtained when versions 1 and 2 were used.
These results show that the multiple preprocessing augmentation method can improve performance when a small amount of data is available.</p>
</div>
<div id="S5.SS7.p4" class="ltx_para">
<p id="S5.SS7.p4.1" class="ltx_p">Fig. <a href="#S5.F11" title="Figure 11 ‚Ä£ V-G Augmentation Method with Different Version of STFT Datasets ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows the augmentation experiments in descending order.
An interesting result shown in Fig. <a href="#S5.F11" title="Figure 11 ‚Ä£ V-G Augmentation Method with Different Version of STFT Datasets ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> is that the CNN model (79.66 UA 80.50 WA 80.08 ACC) and the ECA CNN (80.28 UA 80.46 WA 80.37 ACC) model exhibited the highest performance.
In particular, the CNN-based model with ECA blocks showed 0.8% higher based on ACC than the ascending experimental result.
In addition, the CNN-based model with ECA blocks (red line) is usually 0.2% to 0.82% higher than that of the CNN model (blue line).
In other words, the multiple preprocessing augmentation method can significantly improve the learning of emotional features using ECA blocks.</p>
</div>
<div id="S5.SS7.p5" class="ltx_para">
<p id="S5.SS7.p5.1" class="ltx_p">From these two experiments, we can observe that the multiple preprocessing augmentation method can compensate for the training problem with a few speech-emotional data samples, which is one of the difficulties in SER.
In addition, the ECA blocks can work more effectively using the data augmentation method.
This method achieved the highest performance (80.28 UA 80.46 WA 80.37 ACC).</p>
</div>
</section>
<section id="S5.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS8.4.1.1" class="ltx_text">V-H</span> </span><span id="S5.SS8.5.2" class="ltx_text ltx_font_italic">Comparison with Other Attention Models</span>
</h3>

<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table IV: </span>performance comparison with different SER models</figcaption>
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Model Name</th>
<th id="S5.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Evaluation Results</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.2.1" class="ltx_tr">
<td id="S5.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">STC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</td>
<td id="S5.T4.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">59.1 UA 60.5 WA</td>
</tr>
<tr id="S5.T4.1.3.2" class="ltx_tr">
<td id="S5.T4.1.3.2.1" class="ltx_td ltx_align_left">AMSNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S5.T4.1.3.2.2" class="ltx_td ltx_align_left">70.5 UA 69.2 WA</td>
</tr>
<tr id="S5.T4.1.4.3" class="ltx_tr">
<td id="S5.T4.1.4.3.1" class="ltx_td ltx_align_left">MHA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S5.T4.1.4.3.2" class="ltx_td ltx_align_left">70.1 UA 76.4 WA</td>
</tr>
<tr id="S5.T4.1.5.4" class="ltx_tr">
<td id="S5.T4.1.5.4.1" class="ltx_td ltx_align_left">HNSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S5.T4.1.5.4.2" class="ltx_td ltx_align_left">72.5 UA 70.5 WA</td>
</tr>
<tr id="S5.T4.1.6.5" class="ltx_tr">
<td id="S5.T4.1.6.5.1" class="ltx_td ltx_align_left">ATDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S5.T4.1.6.5.2" class="ltx_td ltx_align_left">75.4 UA 76.2 WA</td>
</tr>
<tr id="S5.T4.1.7.6" class="ltx_tr">
<td id="S5.T4.1.7.6.1" class="ltx_td ltx_align_left">Area Attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S5.T4.1.7.6.2" class="ltx_td ltx_align_left">77.5 UA 79.3 WA</td>
</tr>
<tr id="S5.T4.1.8.7" class="ltx_tr">
<td id="S5.T4.1.8.7.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">Our method</td>
<td id="S5.T4.1.8.7.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_t"><span id="S5.T4.1.8.7.2.1" class="ltx_text ltx_font_bold">80.3 UA 80.5 WA</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS8.p1" class="ltx_para">
<p id="S5.SS8.p1.1" class="ltx_p">Next, we compare with other proposed models‚Äô performance that used attention methods.
For this, we chose our best results models that contained ECA blocks and STFT data augmentation.
Table <a href="#S5.T4" title="Table IV ‚Ä£ V-H Comparison with Other Attention Models ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> lists the results of the UA and WA evaluations.
All models compared with our method use spectrogram data and attention methods used for feature aggregation or extraction in the independent axis of the data.</p>
</div>
<div id="S5.SS8.p2" class="ltx_para">
<p id="S5.SS8.p2.1" class="ltx_p">As listed in Table <a href="#S5.T4" title="Table IV ‚Ä£ V-H Comparison with Other Attention Models ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, the proposed model shows a significantly better performance than the other models.
this is because the combination of deep CNN layers and ECA is an efficient structure for extracting emotional context.
In addition, the insufficient expressions of emotional features can be reinforced using the STFT augmentation method.
Therefore, for the SER, it is important to increase the representation of the emotional feature data and effectively learn the context within it.</p>
</div>
<figure id="S5.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F12.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2409.04007/assets/images/confusion_cnn.png" id="S5.F12.sf1.g1" class="ltx_graphics ltx_img_square" width="138" height="127" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F12.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2409.04007/assets/images/confusion_eca.png" id="S5.F12.sf2.g1" class="ltx_graphics ltx_img_square" width="138" height="127" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>The confusion matrices whether the ECA block is used or not in CNN-based model. (a) CNN-based model (<math id="S5.F12.2.m1.1" class="ltx_Math" alttext="n=4" display="inline"><semantics id="S5.F12.2.m1.1b"><mrow id="S5.F12.2.m1.1.1" xref="S5.F12.2.m1.1.1.cmml"><mi id="S5.F12.2.m1.1.1.2" xref="S5.F12.2.m1.1.1.2.cmml">n</mi><mo id="S5.F12.2.m1.1.1.1" xref="S5.F12.2.m1.1.1.1.cmml">=</mo><mn id="S5.F12.2.m1.1.1.3" xref="S5.F12.2.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F12.2.m1.1c"><apply id="S5.F12.2.m1.1.1.cmml" xref="S5.F12.2.m1.1.1"><eq id="S5.F12.2.m1.1.1.1.cmml" xref="S5.F12.2.m1.1.1.1"></eq><ci id="S5.F12.2.m1.1.1.2.cmml" xref="S5.F12.2.m1.1.1.2">ùëõ</ci><cn type="integer" id="S5.F12.2.m1.1.1.3.cmml" xref="S5.F12.2.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F12.2.m1.1d">n=4</annotation></semantics></math>). (b) ECA block used in layer 5 and 6.</figcaption>
</figure>
<figure id="S5.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F13.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2409.04007/assets/images/confusion_aug_cnn.png" id="S5.F13.sf1.g1" class="ltx_graphics ltx_img_square" width="138" height="127" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F13.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2409.04007/assets/images/confusion_aug_eca.png" id="S5.F13.sf2.g1" class="ltx_graphics ltx_img_square" width="138" height="127" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>The confusion matrices whether the ECA block is used or not in CNN-based model with STFT augmentation method. (a) CNN-based model (<math id="S5.F13.2.m1.1" class="ltx_Math" alttext="n=4" display="inline"><semantics id="S5.F13.2.m1.1b"><mrow id="S5.F13.2.m1.1.1" xref="S5.F13.2.m1.1.1.cmml"><mi id="S5.F13.2.m1.1.1.2" xref="S5.F13.2.m1.1.1.2.cmml">n</mi><mo id="S5.F13.2.m1.1.1.1" xref="S5.F13.2.m1.1.1.1.cmml">=</mo><mn id="S5.F13.2.m1.1.1.3" xref="S5.F13.2.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F13.2.m1.1c"><apply id="S5.F13.2.m1.1.1.cmml" xref="S5.F13.2.m1.1.1"><eq id="S5.F13.2.m1.1.1.1.cmml" xref="S5.F13.2.m1.1.1.1"></eq><ci id="S5.F13.2.m1.1.1.2.cmml" xref="S5.F13.2.m1.1.1.2">ùëõ</ci><cn type="integer" id="S5.F13.2.m1.1.1.3.cmml" xref="S5.F13.2.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F13.2.m1.1d">n=4</annotation></semantics></math>). (b) ECA block used in layer 5 and 6.</figcaption>
</figure>
</section>
<section id="S5.SS9" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS9.4.1.1" class="ltx_text">V-I</span> </span><span id="S5.SS9.5.2" class="ltx_text ltx_font_italic">Analysis of the Ablation Studies</span>
</h3>

<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table V: </span>ablation results</figcaption>
<table id="S5.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Method</th>
<th id="S5.T5.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Evaluation results</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.1.2.1" class="ltx_tr">
<td id="S5.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">CNN-based model</td>
<td id="S5.T5.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">79.16 UA 79.27 WA 79.22 ACC</td>
</tr>
<tr id="S5.T5.1.3.2" class="ltx_tr">
<td id="S5.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_border_t">+ Original ECA block</td>
<td id="S5.T5.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t">78.71 UA 78.80 WA 78.76 ACC</td>
</tr>
<tr id="S5.T5.1.4.3" class="ltx_tr">
<td id="S5.T5.1.4.3.1" class="ltx_td ltx_align_left">+ Our ECA block</td>
<td id="S5.T5.1.4.3.2" class="ltx_td ltx_align_left"><span id="S5.T5.1.4.3.2.1" class="ltx_text ltx_font_bold">79.37 UA 79.68 WA 79.53 ACC</span></td>
</tr>
<tr id="S5.T5.1.5.4" class="ltx_tr">
<td id="S5.T5.1.5.4.1" class="ltx_td ltx_align_left ltx_border_t">+ STFT augmentation</td>
<td id="S5.T5.1.5.4.2" class="ltx_td ltx_align_left ltx_border_t">79.66 UA 80.50 WA 80.08 ACC</td>
</tr>
<tr id="S5.T5.1.6.5" class="ltx_tr">
<td id="S5.T5.1.6.5.1" class="ltx_td ltx_align_left ltx_border_b">+ + Our ECA block</td>
<td id="S5.T5.1.6.5.2" class="ltx_td ltx_align_left ltx_border_b"><span id="S5.T5.1.6.5.2.1" class="ltx_text ltx_font_bold">80.28 UA 80.46 WA 80.37 ACC</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS9.p1" class="ltx_para">
<p id="S5.SS9.p1.1" class="ltx_p">For a detailed analysis of the proposed methods, the results of the ablation models were compared.
Table <a href="#S5.T5" title="Table V ‚Ä£ V-I Analysis of the Ablation Studies ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> lists performance depending on whether or not the ECA block and STFT data augmentation were used.
From the overall results, we can observe that the emotion classification performance is improved when applying our proposed ECA block.
In particular, the effect of the ECA block can significantly improve performance when used together with the STFT data augmentation method.</p>
</div>
<div id="S5.SS9.p2" class="ltx_para">
<p id="S5.SS9.p2.1" class="ltx_p">Subsequently, we compared the classification performance for each emotion according to the ablation models.
Fig. <a href="#S5.F12" title="Figure 12 ‚Ä£ V-H Comparison with Other Attention Models ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> and Fig. <a href="#S5.F13" title="Figure 13 ‚Ä£ V-H Comparison with Other Attention Models ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> show the confusion matrices of the ablation models.
As shown in Fig. <a href="#S5.F12" title="Figure 12 ‚Ä£ V-H Comparison with Other Attention Models ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> and Fig. <a href="#S5.F13" title="Figure 13 ‚Ä£ V-H Comparison with Other Attention Models ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, you can see that in most models, the classification performance of angry and sadness was high; however, the classification performance for angry, happiness, and neutral tended to be low.
Compared with Fig. <a href="#S5.F12" title="Figure 12 ‚Ä£ V-H Comparison with Other Attention Models ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>(a), Fig. <a href="#S5.F13" title="Figure 13 ‚Ä£ V-H Comparison with Other Attention Models ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>(b) shows that the classification performance of all emotions improved when the ECA and STFT augmentation methods were used.</p>
</div>
<div id="S5.SS9.p3" class="ltx_para">
<p id="S5.SS9.p3.1" class="ltx_p">Specifically, in Fig. <a href="#S5.F12" title="Figure 12 ‚Ä£ V-H Comparison with Other Attention Models ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>(a) and (b), the happiness classification accuracy improved by 3.5%, and the neutral classification accuracy decreased by 2% using the ECA.
It means that the ambiguity of classifying happiness and neutral was resolved through the ECA block.
As shown in Fig. <a href="#S5.F13" title="Figure 13 ‚Ä£ V-H Comparison with Other Attention Models ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, the neutral classification accuracy improved by approximately 2% <math id="S5.SS9.p3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS9.p3.1.m1.1a"><mo id="S5.SS9.p3.1.m1.1.1" xref="S5.SS9.p3.1.m1.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S5.SS9.p3.1.m1.1b"><csymbol cd="latexml" id="S5.SS9.p3.1.m1.1.1.cmml" xref="S5.SS9.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS9.p3.1.m1.1c">\sim</annotation></semantics></math> 4%.
In particular, as shown in Fig. <a href="#S5.F13" title="Figure 13 ‚Ä£ V-H Comparison with Other Attention Models ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>(b), both angry and neutral classification accuracies were significantly improved.</p>
</div>
<div id="S5.SS9.p4" class="ltx_para">
<p id="S5.SS9.p4.1" class="ltx_p">The results show that channel feature extraction with the ECA blocks is effective for SER.
Subsequently, to understand how the ECA block works for classifying each emotion, we checked the channel weights for each emotion.
Fig. <a href="#S5.F14" title="Figure 14 ‚Ä£ V-I Analysis of the Ablation Studies ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> shows a plot of the channel weight of the ECA blocks learned in the 5th and 6th layers when using the STFT augmentation method.
To plot the channel weights, the weights from the test set were averaged.</p>
</div>
<div id="S5.SS9.p5" class="ltx_para">
<p id="S5.SS9.p5.1" class="ltx_p">As shown in Fig. <a href="#S5.F14" title="Figure 14 ‚Ä£ V-I Analysis of the Ablation Studies ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>(a), the ECA weights of the fifth layer are not related to any emotion.
However, as shown in Fig. <a href="#S5.F14" title="Figure 14 ‚Ä£ V-I Analysis of the Ablation Studies ‚Ä£ V Experiment Setting and Results ‚Ä£ Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>(b), the ECA weights of the last layer are noticeably different.
In particular, channel weights were distinguishable between angry (blue line) and sadness (orange line).
In addition, neutral (red line) is distinct from angry (blue line) and sadness (orange line).
However, it is slightly different from the neutral (red line) to happiness (green line), because it is difficult to distinguish between them.</p>
</div>
<div id="S5.SS9.p6" class="ltx_para">
<p id="S5.SS9.p6.1" class="ltx_p">In summary, it is difficult to distinguish between all emotions, especially angry, happiness, and neutral emotions.
However, the proposed method can increase the accuracy of all emotions.
Therefore, adopting channel attention in the CNN-based model can effectively extract the emotional context.</p>
</div>
<figure id="S5.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F14.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2409.04007/assets/images/aug_eca_layer5.png" id="S5.F14.sf1.g1" class="ltx_graphics ltx_img_landscape" width="299" height="225" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F14.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2409.04007/assets/images/aug_eca_layer6.png" id="S5.F14.sf2.g1" class="ltx_graphics ltx_img_landscape" width="299" height="225" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>The ECA‚Äôs channel weight plots of each emotion class. (a) The channel weights in layer 5. (b) The channel weights in layer 6.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This study proposes a promising and more effective preprocessing method and an ECA module for SER, offering the potential for significant advancements in the field.
Our experiment, conducted with eight different preprocessing datasets from the IEMOCAP corpus, revealed a significant finding: a spectrogram with a higher frequency resolution is more effective in training emotional features, providing valuable insight for future research in the field.
Our study is the first study to apply an ECA to the SER.
We achieved significantly better results than previous models by applying ECA to our CNN-based model with an effective preprocessing method.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Considering these results, correctly understanding the relationship between the channel features in the CNN structure can be a clue to understanding how to distinguish emotions.
However, the ECA is limited in that it only considers the relationship between neighboring channels.
In future work, we will look for attention structures that are efficient but can learn the relationships between channel features more broadly.
In addition, it is necessary to determine a better preprocessing method for emotion recognition by analyzing the frequency features associated with each emotion.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B.¬†W. Schuller, ‚ÄúSpeech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends,‚Äù <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, vol.¬†61, no.¬†5, pp. 90‚Äì99, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S.¬†Brave and C.¬†Nass, ‚ÄúEmotion in human-computer interaction,‚Äù in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">The human-computer interaction handbook</em>.¬†¬†¬†CRC Press, 2007, pp. 103‚Äì118.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Y.¬†Yang, C.¬†Fairbairn, and J.¬†F. Cohn, ‚ÄúDetecting depression severity from vocal prosody,‚Äù <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on affective computing</em>, vol.¬†4, no.¬†2, pp. 142‚Äì150, 2012.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M.¬†Bojaniƒá, V.¬†Deliƒá, and A.¬†Karpov, ‚ÄúCall redistribution for a call center based on speech emotion recognition,‚Äù <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, vol.¬†10, no.¬†13, p. 4653, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J.¬†A. Russell, ‚ÄúCulture and the categorization of emotions.‚Äù <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Psychological bulletin</em>, vol. 110, no.¬†3, p. 426, 1991.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R.¬†Jahangir, Y.¬†W. Teh, F.¬†Hanif, and G.¬†Mujtaba, ‚ÄúDeep learning approaches for speech emotion recognition: state of the art and research challenges,‚Äù <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Multimedia Tools and Applications</em>, vol.¬†80, no.¬†16, pp. 23‚Äâ745‚Äì23‚Äâ812, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
K.¬†Han, D.¬†Yu, and I.¬†Tashev, ‚ÄúSpeech emotion recognition using deep neural network and extreme learning machine,‚Äù in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Interspeech 2014</em>, 2014.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H.¬†M. Fayek, M.¬†Lech, and L.¬†Cavedon, ‚ÄúEvaluating deep learning architectures for speech emotion recognition,‚Äù <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Neural Networks</em>, vol.¬†92, pp. 60‚Äì68, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S.¬†Mirsamadi, E.¬†Barsoum, and C.¬†Zhang, ‚ÄúAutomatic speech emotion recognition using recurrent neural networks with local attention,‚Äù in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2017 IEEE International conference on acoustics, speech and signal processing (ICASSP)</em>.¬†¬†¬†IEEE, 2017, pp. 2227‚Äì2231.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y.¬†Xie, R.¬†Liang, Z.¬†Liang, C.¬†Huang, C.¬†Zou, and B.¬†Schuller, ‚ÄúSpeech emotion classification using attention-based lstm,‚Äù <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.¬†27, no.¬†11, pp. 1675‚Äì1685, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Q.¬†Cao, M.¬†Hou, B.¬†Chen, Z.¬†Zhang, and G.¬†Lu, ‚ÄúHierarchical network based on the fusion of static and dynamic features for speech emotion recognition,‚Äù in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.¬†¬†¬†IEEE, 2021, pp. 6334‚Äì6338.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
W.¬†Lim, D.¬†Jang, and T.¬†Lee, ‚ÄúSpeech emotion recognition using convolutional and recurrent neural networks,‚Äù in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2016 Asia-Pacific signal and information processing association annual summit and conference (APSIPA)</em>.¬†¬†¬†IEEE, 2016, pp. 1‚Äì4.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A.¬†Satt, S.¬†Rozenberg, R.¬†Hoory <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúEfficient emotion recognition from speech using deep learning on spectrograms.‚Äù in <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">Interspeech</em>, 2017, pp. 1089‚Äì1093.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
X.¬†Ma, Z.¬†Wu, J.¬†Jia, M.¬†Xu, H.¬†Meng, and L.¬†Cai, ‚ÄúEmotion recognition from variable-length speech segments using deep learning on spectrograms.‚Äù in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>, 2018, pp. 3683‚Äì3687.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M.¬†Chen, X.¬†He, J.¬†Yang, and H.¬†Zhang, ‚Äú3-d convolutional recurrent neural networks with attention model for speech emotion recognition,‚Äù <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Letters</em>, vol.¬†25, no.¬†10, pp. 1440‚Äì1444, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
H.¬†Meng, T.¬†Yan, F.¬†Yuan, and H.¬†Wei, ‚ÄúSpeech emotion recognition from 3d log-mel spectrograms with deep learning network,‚Äù <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE access</em>, vol.¬†7, pp. 125‚Äâ868‚Äì125‚Äâ881, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Z.¬†Peng, X.¬†Li, Z.¬†Zhu, M.¬†Unoki, J.¬†Dang, and M.¬†Akagi, ‚ÄúSpeech emotion recognition using 3d convolutions and attention-based sliding recurrent networks with auditory front-ends,‚Äù <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol.¬†8, pp. 16‚Äâ560‚Äì16‚Äâ572, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A.¬†Aftab, A.¬†Morsali, S.¬†Ghaemmaghami, and B.¬†Champagne, ‚ÄúLight-sernet: A lightweight fully convolutional neural network for speech emotion recognition,‚Äù in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.¬†¬†¬†IEEE, 2022, pp. 6912‚Äì6916.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
K.¬†Simonyan and A.¬†Zisserman, ‚ÄúVery deep convolutional networks for large-scale image recognition,‚Äù <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.1556</em>, 2014.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M.¬†A. Islam, S.¬†Jia, and N.¬†D. Bruce, ‚ÄúHow much position information do convolutional neural networks encode?‚Äù <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.08248</em>, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
W.¬†Han, Z.¬†Zhang, Y.¬†Zhang, J.¬†Yu, C.-C. Chiu, J.¬†Qin, A.¬†Gulati, R.¬†Pang, and Y.¬†Wu, ‚ÄúContextnet: Improving convolutional neural networks for automatic speech recognition with global context,‚Äù <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.03191</em>, 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
T.¬†M. Wani, T.¬†S. Gunawan, S.¬†A.¬†A. Qadri, M.¬†Kartiwi, and E.¬†Ambikairajah, ‚ÄúA comprehensive review of speech emotion recognition systems,‚Äù <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE access</em>, vol.¬†9, pp. 47‚Äâ795‚Äì47‚Äâ814, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M.¬†Xu, F.¬†Zhang, and S.¬†U. Khan, ‚ÄúImprove accuracy of speech emotion recognition with attention head fusion,‚Äù in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2020 10th annual computing and communication workshop and conference (CCWC)</em>.¬†¬†¬†IEEE, 2020, pp. 1058‚Äì1064.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S.¬†Li, X.¬†Xing, W.¬†Fan, B.¬†Cai, P.¬†Fordson, and X.¬†Xu, ‚ÄúSpatiotemporal and frequential cascaded attention networks for speech emotion recognition,‚Äù <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, vol. 448, pp. 238‚Äì248, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M.¬†Xu, F.¬†Zhang, X.¬†Cui, and W.¬†Zhang, ‚ÄúSpeech emotion recognition with multiscale area attention and data augmentation,‚Äù in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021-2021 IEEE International conference on acoustics, speech and signal processing (ICASSP)</em>.¬†¬†¬†IEEE, 2021, pp. 6319‚Äì6323.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
L.-Y. Liu, W.-Z. Liu, J.¬†Zhou, H.-Y. Deng, and L.¬†Feng, ‚ÄúAtda: Attentional temporal dynamic activation for speech emotion recognition,‚Äù <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em>, vol. 243, p. 108472, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
L.¬†Guo, S.¬†Ding, L.¬†Wang, and J.¬†Dang, ‚ÄúDstcnet: Deep spectro-temporal-channel attention network for speech emotion recognition,‚Äù <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Z.¬†Chen, J.¬†Li, H.¬†Liu, X.¬†Wang, H.¬†Wang, and Q.¬†Zheng, ‚ÄúLearning multi-scale features for speech emotion recognition with connection attention mechanism,‚Äù <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, vol. 214, p. 118943, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A.¬†Vaswani, N.¬†Shazeer, N.¬†Parmar, J.¬†Uszkoreit, L.¬†Jones, A.¬†N. Gomez, ≈Å.¬†Kaiser, and I.¬†Polosukhin, ‚ÄúAttention is all you need,‚Äù in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 2017, pp. 5998‚Äì6008.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J.¬†Hu, L.¬†Shen, and G.¬†Sun, in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2018, pp. 7132‚Äì7141.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J.¬†Park, S.¬†Woo, J.-Y. Lee, and I.¬†S. Kweon, ‚ÄúBam: Bottleneck attention module,‚Äù <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.06514</em>, 2018.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J.¬†Park, S.¬†Woo, J.-Y. Lee, and I.¬†S. Kweon, ‚ÄúA simple and light-weight attention module for convolutional neural networks,‚Äù <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, vol. 128, no.¬†4, pp. 783‚Äì798, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
X.¬†Hu, L.¬†Chu, J.¬†Pei, W.¬†Liu, and J.¬†Bian, ‚ÄúModel complexity of deep learning: A survey,‚Äù <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Knowledge and Information Systems</em>, vol.¬†63, pp. 2585‚Äì2619, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
M.¬†El¬†Ayadi, M.¬†S. Kamel, and F.¬†Karray, ‚ÄúSurvey on speech emotion recognition: Features, classification schemes, and databases,‚Äù <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Pattern recognition</em>, vol.¬†44, no.¬†3, pp. 572‚Äì587, 2011.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Q.¬†Wang, B.¬†Wu, P.¬†Zhu, P.¬†Li, W.¬†Zuo, and Q.¬†Hu, ‚ÄúEca-net: Efficient channel attention for deep convolutional neural networks,‚Äù in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 11‚Äâ534‚Äì11‚Äâ542.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
C.¬†Busso, M.¬†Bulut, C.-C. Lee, A.¬†Kazemzadeh, E.¬†Mower, S.¬†Kim, J.¬†N. Chang, S.¬†Lee, and S.¬†S. Narayanan, ‚ÄúIemocap: Interactive emotional dyadic motion capture database,‚Äù <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Language resources and evaluation</em>, vol.¬†42, pp. 335‚Äì359, 2008.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
S.¬†Kiranyaz, O.¬†Avci, O.¬†Abdeljaber, T.¬†Ince, M.¬†Gabbouj, and D.¬†J. Inman, ‚Äú1d convolutional neural networks and applications: A survey,‚Äù <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Mechanical systems and signal processing</em>, vol. 151, p. 107398, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
P.¬†Jiang, H.¬†Fu, H.¬†Tao, P.¬†Lei, and L.¬†Zhao, ‚ÄúParallelized convolutional recurrent neural network with spectral features for speech emotion recognition,‚Äù <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">IEEE access</em>, vol.¬†7, pp. 90‚Äâ368‚Äì90‚Äâ377, 2019.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Z.¬†Zhao, Z.¬†Bao, Y.¬†Zhao, Z.¬†Zhang, N.¬†Cummins, Z.¬†Ren, and B.¬†Schuller, ‚ÄúExploring deep spectrum representations via attention-based recurrent and convolutional neural networks for speech emotion recognition,‚Äù <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">IEEE access</em>, vol.¬†7, pp. 97‚Äâ515‚Äì97‚Äâ525, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
T.-Y. Lin, P.¬†Goyal, R.¬†Girshick, K.¬†He, and P.¬†Doll√°r, ‚ÄúFocal loss for dense object detection,‚Äù in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>, 2017, pp. 2980‚Äì2988.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
A.¬†Nediyanchath, P.¬†Paramasivam, and P.¬†Yenigalla, ‚ÄúMulti-head attention for speech emotion recognition with auxiliary learning of gender recognition,‚Äù in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.¬†¬†¬†IEEE, 2020, pp. 7179‚Äì7183.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
J.¬†Ye, X.-C. Wen, Y.¬†Wei, Y.¬†Xu, K.¬†Liu, and H.¬†Shan, ‚ÄúTemporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition,‚Äù in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.¬†¬†¬†IEEE, 2023, pp. 1‚Äì5.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
A.¬†Paszke, S.¬†Gross, F.¬†Massa, A.¬†Lerer, J.¬†Bradbury, G.¬†Chanan, T.¬†Killeen, Z.¬†Lin, N.¬†Gimelshein, L.¬†Antiga <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúPytorch: An imperative style, high-performance deep learning library,‚Äù <em id="bib.bib43.2.2" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.¬†32, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
K.¬†He, X.¬†Zhang, S.¬†Ren, and J.¬†Sun, ‚ÄúDelving deep into rectifiers: Surpassing human-level performance on imagenet classification,‚Äù in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>, 2015, pp. 1026‚Äì1034.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
D.¬†P. Kingma and J.¬†Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>, 2014.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.04006" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.04007" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.04007">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.04007" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.04008" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 00:07:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
