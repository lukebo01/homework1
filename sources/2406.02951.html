<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.02951] AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection</title><meta property="og:description" content="With the rapid growth in deepfake video content, we require improved and generalizable methods to detect them.
Most existing detection methods either use uni-modal cues or rely on supervised training to capture the dis…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.02951">

<!--Generated on Sat Jul  6 00:13:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Trevine Oorloff<sup id="id1.1.id1" class="ltx_sup">1,2</sup>
       Surya Koppisetti<sup id="id2.2.id2" class="ltx_sup">2</sup>
       Nicolò Bonettini<sup id="id3.3.id3" class="ltx_sup">2</sup>
       Divyaraj Solanki<sup id="id4.4.id4" class="ltx_sup">2</sup> 
<br class="ltx_break">       Ben Colman<sup id="id5.5.id5" class="ltx_sup">2</sup>
       Yaser Yacoob<sup id="id6.6.id6" class="ltx_sup">1</sup>
       Ali Shahriyari<sup id="id7.7.id7" class="ltx_sup">2</sup>
       Gaurav Bharaj<sup id="id8.8.id8" class="ltx_sup">2</sup>

<br class="ltx_break"><sup id="id9.9.id9" class="ltx_sup">1</sup>University of Maryland - College Park         <sup id="id10.10.id10" class="ltx_sup">2</sup>Reality Defender Inc.
<br class="ltx_break">
</span><span class="ltx_author_notes">This work was completed during an internship at Reality Defender Inc.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">With the rapid growth in deepfake video content, we require improved and generalizable methods to detect them.
Most existing detection methods either use uni-modal cues or rely on supervised training to capture the dissonance between the audio and visual modalities.
While the former disregards the audio-visual correspondences entirely, the latter predominantly focuses on discerning audio-visual cues <span id="id11.id1.1" class="ltx_text ltx_font_italic">within the training corpus</span>, thereby potentially overlooking correspondences that can help detect unseen deepfakes.
We present Audio-Visual Feature Fusion (AVFF), a two-stage cross-modal learning method that explicitly captures the correspondence between the audio and visual modalities for improved deepfake detection. The first stage pursues representation learning via self-supervision on real videos to capture the intrinsic audio-visual correspondences. To extract rich cross-modal representations, we use contrastive learning and autoencoding objectives, and introduce a novel audio-visual complementary masking and feature fusion strategy. The learned representations are tuned in the second stage, where deepfake classification is pursued via supervised learning on both real and fake videos.
Extensive experiments and analysis suggest that our novel representation learning paradigm is highly discriminative in nature.
We report 98.6% accuracy and 99.1% AUC on the FakeAVCeleb dataset, outperforming the current audio-visual state-of-the-art by 14.9% and 9.9%, respectively.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deepfake generative AI technology enables new opportunities to create rich and quality content in multimedia applications such as virtual reality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>, movie production <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>, and telepresence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>, <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite>. However, its malicious use has become a major societal threat posing a number of problems including frauds<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402?_sm_au_=i5VjqL43tSrQV756QcLJjKQ1j7GJ1" title="" class="ltx_ref ltx_href">Fraudsters Used AI to Mimic CEO’s Voice in Cybercrime Case</a></span></span></span>, defamation<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.bbc.com/news/entertainment-arts-65854112" title="" class="ltx_ref ltx_href">Deepfake porn documentary explores its ‘life-shattering’ impact</a>, <a target="_blank" href="https://www.washingtonpost.com/technology/2023/11/05/ai-deepfake-porn-teens-women-impact/" title="" class="ltx_ref ltx_href">AI Fake Nudes are booming</a></span></span></span> and disinformation<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://www.bbc.com/news/uk-66130785" title="" class="ltx_ref ltx_href">Martin Lewis felt ‘sick’ seeing deepfake scam ad on Facebook</a>, <a target="_blank" href="https://www.nbcnews.com/tech/tech-news/deepfake-scams-arrived-fake-videos-spread-facebook-tiktok-youtube-rcna101415" title="" class="ltx_ref ltx_href">Deepfake scams have arrived</a></span></span></span>. As the generative AI landscape continues to evolve, there is a growing need for robust deepfake detection that helps preserve content integrity.
In this paper, we study video deepfake detection where either or both the visual and audio content are AI-generated.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.02951/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="396" height="268" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">We use audio-visual correspondences for deepfake detection. Transformer-based encoders are used to extract audio and visual feature tokens, which are then masked complementarily. The visible audio tokens are sent through a learnable A2V network to predict the masked visual tokens. These predicted visual tokens are fused with the visible visual tokens to obtain the full visual embeddings. Full audio embeddings are obtained in a similar way using the V2A network.
The audio/visual embeddings are then used for video reconstruction in the MAE sense, and subsequently for deepfake classification.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">We pursue multi-modal learning and draw inspiration from previous works, such as SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, and AudioCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>, where the correspondence between different modalities (audio, text, visual) was leveraged to significantly enhance performance on various downstream tasks. We note that in real video face context, the audio-visual correspondence is deeply intuitive since there is an intrinsic correlation between the mouth articulations (visemes) and the speech units (phonemes) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">72</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, as well as an alignment of emotional nuances embedded in the facial and speech expressions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>. Such inherent audio-visual correspondence, for example, in audio-driven emotion, is challenging to faithfully replicate in deepfake videos. Based on these observations, we propose a video deepfake detection method that learns efficient representations for audio and visual modalities. The proposed method employs a novel complementary masking and cross-modal feature fusion strategy to explicitly capture the audio-visual correspondences.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Previous literature on audio-visual video deepfake detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> use supervised contrastive learning to capture the audio-visual correspondence. Such methods align the audio and visual embeddings closer to each other, if the content in both modalities is real, and push them apart if either or both modalities are generative. Similarly, others pursue a single stage supervised learning method, where models are directly trained on labeled deepfake datasets for deepfake classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>.
While such methods yield promising results, we conjecture that they may not fully exploit the audio-visual correspondence. Also, training solely on a deepfake dataset narrows the model’s focus to discern separable features within the training corpus, potentially overlooking subtle audio-visual correspondences that can help detect unseen deepfake samples (observe in <a href="#S4.T1" title="In 4.1 Implementation ‣ 4 Experiments and Results ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tabs.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S4.T3" title="Table 3 ‣ 4.2 Evaluation and Discussion ‣ 4 Experiments and Results ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the weaker performance of other baselines compared to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> and ours).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To circumvent these issues, we propose a two stage training pipeline comprising of (i) a self-supervised representation learning stage that explicitly enforces audio-visual correspondence using a novel approach, and (ii) a supervised downstream classification stage.
In the representation learning stage, we extract audio-visual representations via self-supervised learning on real face videos, which are available in abundance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>, <a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">73</span></a>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>. Drawing inspirations from CAV-MAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>, we make use of the complementary nature of two learning objectives: contrastive learning and autoencoding. For extracting rich representations, we supplement the contrastive learning objective by a novel audio-visual complementary masking and fusion strategy that sits within the autoencoding objective. In the classification stage, we train a classifier that exploits the lack of cohesion between audio-visual features of deepfake videos to separate them from real videos.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We evaluate our method against existing state-of-the-art approaches on multiple benchmarks. Our results reveal substantial improvements, when compared against the existing audio-visual state-of-the-art, enhancing the performance by 9.9% in AUC and 14.9% in accuracy when evaluated on the FakeAVCeleb dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>.
This underscores the effectiveness of <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">explicitly</span> leveraging audio-visual correspondences through the proposed method. In summary:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a novel self-supervised representation learning method that <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">explicitly</span> captures audio-visual correspondences in real videos. To learn the correspondences, we pursue a dual-objective of contrastive learning and autoencoding, and supplement it with a novel audio-visual complementary masking and fusion strategy.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Qualitative analysis using t-SNE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> shows a clear separation between the real and fake video embeddings at the end of the representation learning stage. This demonstrates the efficacy of the proposed representation learning.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We propose a two-stage deepfake detection method comprising of the aforementioned representation learning stage followed by a deepfake classification stage. Our method yields state-of-the-art performance on deepfake detection when either or both the audio and visual contents are AI generated.
We achieve 98.6% accuracy and 99.1% AUC on FakeAVCeleb, surpassing the existing audio-visual state-of-the-art by 14.9% and 9.9% respectively.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Multi-Modal Representation Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Learning a joint representation from multiple modalities has been shown to be effective for different tasks in the state-of-the-art. SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> proposes a Siamese Network to estimate the lip-sync error between audio and visual modalities. This framework processes each modality through a distinct branch and employs a contrastive loss to promote the similarities in the encoding space. More recently, improvements in the <span title="" class="ltx_glossaryref">Natural Language Processing (NLP)</span> field brought by BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, allowed to use text modality in multi-modal frameworks. Another example is CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, a zero-shot image classification model that leverages separate encoders for images and captions to find a suitable pairing in the latent space. AudioCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> extends this approach to audio, enabling multi-modal classification.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Several self-supervised methods have emerged, inspired by the <span title="" class="ltx_glossaryref">Masked Autoencoder (MAE)</span> framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>.
AV-MAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> is a joint masked autoencoder for audio, visual, and joint audio/visual classification.
The authors explore different encoding policies for dual-modality inputs, demonstrating the ability to decode one masked modality from the other.
CAV-MAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> raises concerns about the ability of a vanilla masked autoencoder to learn a coordinated representation between audio and visuals (<em id="S2.SS1.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS1.p2.1.2" class="ltx_text"></span>, a representation that enforces similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>) and adds a contrastive loss to explicitly leverage the audio-visual pair information.
In this work, we draw inspiration from CAV-MAE, in using a dual contrastive-autoencoding objective for effective representation learning. Our approach diverges from existing MAE literature: (i) in terms of the masking strategy, where we use a complementary masking strategy post-encoding; (ii) in terms of the cross-modal fusion, where for every modality we replace the shared learnable masked tokens of MAEs with tokens predicted from the other modality. We do this to enforce explicit correspondence between audio and visual modalities.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2406.02951/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="426" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.59.29.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.56.28" class="ltx_text ltx_font_bold" style="font-size:90%;">Audio-Visual Representation Learning Stage.<span id="S2.F2.56.28.28" class="ltx_text ltx_font_medium"> A real input sample, <math id="S2.F2.29.1.1.m1.1" class="ltx_Math" alttext="x\in\mathcal{D}_{r}" display="inline"><semantics id="S2.F2.29.1.1.m1.1b"><mrow id="S2.F2.29.1.1.m1.1.1" xref="S2.F2.29.1.1.m1.1.1.cmml"><mi id="S2.F2.29.1.1.m1.1.1.2" xref="S2.F2.29.1.1.m1.1.1.2.cmml">x</mi><mo id="S2.F2.29.1.1.m1.1.1.1" xref="S2.F2.29.1.1.m1.1.1.1.cmml">∈</mo><msub id="S2.F2.29.1.1.m1.1.1.3" xref="S2.F2.29.1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.F2.29.1.1.m1.1.1.3.2" xref="S2.F2.29.1.1.m1.1.1.3.2.cmml">𝒟</mi><mi id="S2.F2.29.1.1.m1.1.1.3.3" xref="S2.F2.29.1.1.m1.1.1.3.3.cmml">r</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.29.1.1.m1.1c"><apply id="S2.F2.29.1.1.m1.1.1.cmml" xref="S2.F2.29.1.1.m1.1.1"><in id="S2.F2.29.1.1.m1.1.1.1.cmml" xref="S2.F2.29.1.1.m1.1.1.1"></in><ci id="S2.F2.29.1.1.m1.1.1.2.cmml" xref="S2.F2.29.1.1.m1.1.1.2">𝑥</ci><apply id="S2.F2.29.1.1.m1.1.1.3.cmml" xref="S2.F2.29.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.F2.29.1.1.m1.1.1.3.1.cmml" xref="S2.F2.29.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.F2.29.1.1.m1.1.1.3.2.cmml" xref="S2.F2.29.1.1.m1.1.1.3.2">𝒟</ci><ci id="S2.F2.29.1.1.m1.1.1.3.3.cmml" xref="S2.F2.29.1.1.m1.1.1.3.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.29.1.1.m1.1d">x\in\mathcal{D}_{r}</annotation></semantics></math>, with corresponding audio and visual tokens (<math id="S2.F2.30.2.2.m2.1" class="ltx_Math" alttext="\bm{x_{a}}" display="inline"><semantics id="S2.F2.30.2.2.m2.1b"><msub id="S2.F2.30.2.2.m2.1.1" xref="S2.F2.30.2.2.m2.1.1.cmml"><mi id="S2.F2.30.2.2.m2.1.1.2" xref="S2.F2.30.2.2.m2.1.1.2.cmml">𝒙</mi><mi id="S2.F2.30.2.2.m2.1.1.3" xref="S2.F2.30.2.2.m2.1.1.3.cmml">𝒂</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.30.2.2.m2.1c"><apply id="S2.F2.30.2.2.m2.1.1.cmml" xref="S2.F2.30.2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.F2.30.2.2.m2.1.1.1.cmml" xref="S2.F2.30.2.2.m2.1.1">subscript</csymbol><ci id="S2.F2.30.2.2.m2.1.1.2.cmml" xref="S2.F2.30.2.2.m2.1.1.2">𝒙</ci><ci id="S2.F2.30.2.2.m2.1.1.3.cmml" xref="S2.F2.30.2.2.m2.1.1.3">𝒂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.30.2.2.m2.1d">\bm{x_{a}}</annotation></semantics></math>, <math id="S2.F2.31.3.3.m3.1" class="ltx_Math" alttext="\bm{x_{v}}" display="inline"><semantics id="S2.F2.31.3.3.m3.1b"><msub id="S2.F2.31.3.3.m3.1.1" xref="S2.F2.31.3.3.m3.1.1.cmml"><mi id="S2.F2.31.3.3.m3.1.1.2" xref="S2.F2.31.3.3.m3.1.1.2.cmml">𝒙</mi><mi id="S2.F2.31.3.3.m3.1.1.3" xref="S2.F2.31.3.3.m3.1.1.3.cmml">𝒗</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.31.3.3.m3.1c"><apply id="S2.F2.31.3.3.m3.1.1.cmml" xref="S2.F2.31.3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.F2.31.3.3.m3.1.1.1.cmml" xref="S2.F2.31.3.3.m3.1.1">subscript</csymbol><ci id="S2.F2.31.3.3.m3.1.1.2.cmml" xref="S2.F2.31.3.3.m3.1.1.2">𝒙</ci><ci id="S2.F2.31.3.3.m3.1.1.3.cmml" xref="S2.F2.31.3.3.m3.1.1.3">𝒗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.31.3.3.m3.1d">\bm{x_{v}}</annotation></semantics></math>), is split along the temporal dimension, creating <math id="S2.F2.32.4.4.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.F2.32.4.4.m4.1b"><mi id="S2.F2.32.4.4.m4.1.1" xref="S2.F2.32.4.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.F2.32.4.4.m4.1c"><ci id="S2.F2.32.4.4.m4.1.1.cmml" xref="S2.F2.32.4.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.32.4.4.m4.1d">K</annotation></semantics></math> slices,
<math id="S2.F2.33.5.5.m5.3" class="ltx_Math" alttext="\{x_{a,t_{i}}\}_{i=1}^{K}" display="inline"><semantics id="S2.F2.33.5.5.m5.3b"><msubsup id="S2.F2.33.5.5.m5.3.3" xref="S2.F2.33.5.5.m5.3.3.cmml"><mrow id="S2.F2.33.5.5.m5.3.3.1.1.1" xref="S2.F2.33.5.5.m5.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.F2.33.5.5.m5.3.3.1.1.1.2" xref="S2.F2.33.5.5.m5.3.3.1.1.2.cmml">{</mo><msub id="S2.F2.33.5.5.m5.3.3.1.1.1.1" xref="S2.F2.33.5.5.m5.3.3.1.1.1.1.cmml"><mi id="S2.F2.33.5.5.m5.3.3.1.1.1.1.2" xref="S2.F2.33.5.5.m5.3.3.1.1.1.1.2.cmml">x</mi><mrow id="S2.F2.33.5.5.m5.2.2.2.2" xref="S2.F2.33.5.5.m5.2.2.2.3.cmml"><mi id="S2.F2.33.5.5.m5.1.1.1.1" xref="S2.F2.33.5.5.m5.1.1.1.1.cmml">a</mi><mo id="S2.F2.33.5.5.m5.2.2.2.2.2" xref="S2.F2.33.5.5.m5.2.2.2.3.cmml">,</mo><msub id="S2.F2.33.5.5.m5.2.2.2.2.1" xref="S2.F2.33.5.5.m5.2.2.2.2.1.cmml"><mi id="S2.F2.33.5.5.m5.2.2.2.2.1.2" xref="S2.F2.33.5.5.m5.2.2.2.2.1.2.cmml">t</mi><mi id="S2.F2.33.5.5.m5.2.2.2.2.1.3" xref="S2.F2.33.5.5.m5.2.2.2.2.1.3.cmml">i</mi></msub></mrow></msub><mo stretchy="false" id="S2.F2.33.5.5.m5.3.3.1.1.1.3" xref="S2.F2.33.5.5.m5.3.3.1.1.2.cmml">}</mo></mrow><mrow id="S2.F2.33.5.5.m5.3.3.1.3" xref="S2.F2.33.5.5.m5.3.3.1.3.cmml"><mi id="S2.F2.33.5.5.m5.3.3.1.3.2" xref="S2.F2.33.5.5.m5.3.3.1.3.2.cmml">i</mi><mo id="S2.F2.33.5.5.m5.3.3.1.3.1" xref="S2.F2.33.5.5.m5.3.3.1.3.1.cmml">=</mo><mn id="S2.F2.33.5.5.m5.3.3.1.3.3" xref="S2.F2.33.5.5.m5.3.3.1.3.3.cmml">1</mn></mrow><mi id="S2.F2.33.5.5.m5.3.3.3" xref="S2.F2.33.5.5.m5.3.3.3.cmml">K</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.F2.33.5.5.m5.3c"><apply id="S2.F2.33.5.5.m5.3.3.cmml" xref="S2.F2.33.5.5.m5.3.3"><csymbol cd="ambiguous" id="S2.F2.33.5.5.m5.3.3.2.cmml" xref="S2.F2.33.5.5.m5.3.3">superscript</csymbol><apply id="S2.F2.33.5.5.m5.3.3.1.cmml" xref="S2.F2.33.5.5.m5.3.3"><csymbol cd="ambiguous" id="S2.F2.33.5.5.m5.3.3.1.2.cmml" xref="S2.F2.33.5.5.m5.3.3">subscript</csymbol><set id="S2.F2.33.5.5.m5.3.3.1.1.2.cmml" xref="S2.F2.33.5.5.m5.3.3.1.1.1"><apply id="S2.F2.33.5.5.m5.3.3.1.1.1.1.cmml" xref="S2.F2.33.5.5.m5.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S2.F2.33.5.5.m5.3.3.1.1.1.1.1.cmml" xref="S2.F2.33.5.5.m5.3.3.1.1.1.1">subscript</csymbol><ci id="S2.F2.33.5.5.m5.3.3.1.1.1.1.2.cmml" xref="S2.F2.33.5.5.m5.3.3.1.1.1.1.2">𝑥</ci><list id="S2.F2.33.5.5.m5.2.2.2.3.cmml" xref="S2.F2.33.5.5.m5.2.2.2.2"><ci id="S2.F2.33.5.5.m5.1.1.1.1.cmml" xref="S2.F2.33.5.5.m5.1.1.1.1">𝑎</ci><apply id="S2.F2.33.5.5.m5.2.2.2.2.1.cmml" xref="S2.F2.33.5.5.m5.2.2.2.2.1"><csymbol cd="ambiguous" id="S2.F2.33.5.5.m5.2.2.2.2.1.1.cmml" xref="S2.F2.33.5.5.m5.2.2.2.2.1">subscript</csymbol><ci id="S2.F2.33.5.5.m5.2.2.2.2.1.2.cmml" xref="S2.F2.33.5.5.m5.2.2.2.2.1.2">𝑡</ci><ci id="S2.F2.33.5.5.m5.2.2.2.2.1.3.cmml" xref="S2.F2.33.5.5.m5.2.2.2.2.1.3">𝑖</ci></apply></list></apply></set><apply id="S2.F2.33.5.5.m5.3.3.1.3.cmml" xref="S2.F2.33.5.5.m5.3.3.1.3"><eq id="S2.F2.33.5.5.m5.3.3.1.3.1.cmml" xref="S2.F2.33.5.5.m5.3.3.1.3.1"></eq><ci id="S2.F2.33.5.5.m5.3.3.1.3.2.cmml" xref="S2.F2.33.5.5.m5.3.3.1.3.2">𝑖</ci><cn type="integer" id="S2.F2.33.5.5.m5.3.3.1.3.3.cmml" xref="S2.F2.33.5.5.m5.3.3.1.3.3">1</cn></apply></apply><ci id="S2.F2.33.5.5.m5.3.3.3.cmml" xref="S2.F2.33.5.5.m5.3.3.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.33.5.5.m5.3d">\{x_{a,t_{i}}\}_{i=1}^{K}</annotation></semantics></math> and <math id="S2.F2.34.6.6.m6.3" class="ltx_Math" alttext="\{x_{v,t_{i}}\}_{i=1}^{K}" display="inline"><semantics id="S2.F2.34.6.6.m6.3b"><msubsup id="S2.F2.34.6.6.m6.3.3" xref="S2.F2.34.6.6.m6.3.3.cmml"><mrow id="S2.F2.34.6.6.m6.3.3.1.1.1" xref="S2.F2.34.6.6.m6.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.F2.34.6.6.m6.3.3.1.1.1.2" xref="S2.F2.34.6.6.m6.3.3.1.1.2.cmml">{</mo><msub id="S2.F2.34.6.6.m6.3.3.1.1.1.1" xref="S2.F2.34.6.6.m6.3.3.1.1.1.1.cmml"><mi id="S2.F2.34.6.6.m6.3.3.1.1.1.1.2" xref="S2.F2.34.6.6.m6.3.3.1.1.1.1.2.cmml">x</mi><mrow id="S2.F2.34.6.6.m6.2.2.2.2" xref="S2.F2.34.6.6.m6.2.2.2.3.cmml"><mi id="S2.F2.34.6.6.m6.1.1.1.1" xref="S2.F2.34.6.6.m6.1.1.1.1.cmml">v</mi><mo id="S2.F2.34.6.6.m6.2.2.2.2.2" xref="S2.F2.34.6.6.m6.2.2.2.3.cmml">,</mo><msub id="S2.F2.34.6.6.m6.2.2.2.2.1" xref="S2.F2.34.6.6.m6.2.2.2.2.1.cmml"><mi id="S2.F2.34.6.6.m6.2.2.2.2.1.2" xref="S2.F2.34.6.6.m6.2.2.2.2.1.2.cmml">t</mi><mi id="S2.F2.34.6.6.m6.2.2.2.2.1.3" xref="S2.F2.34.6.6.m6.2.2.2.2.1.3.cmml">i</mi></msub></mrow></msub><mo stretchy="false" id="S2.F2.34.6.6.m6.3.3.1.1.1.3" xref="S2.F2.34.6.6.m6.3.3.1.1.2.cmml">}</mo></mrow><mrow id="S2.F2.34.6.6.m6.3.3.1.3" xref="S2.F2.34.6.6.m6.3.3.1.3.cmml"><mi id="S2.F2.34.6.6.m6.3.3.1.3.2" xref="S2.F2.34.6.6.m6.3.3.1.3.2.cmml">i</mi><mo id="S2.F2.34.6.6.m6.3.3.1.3.1" xref="S2.F2.34.6.6.m6.3.3.1.3.1.cmml">=</mo><mn id="S2.F2.34.6.6.m6.3.3.1.3.3" xref="S2.F2.34.6.6.m6.3.3.1.3.3.cmml">1</mn></mrow><mi id="S2.F2.34.6.6.m6.3.3.3" xref="S2.F2.34.6.6.m6.3.3.3.cmml">K</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.F2.34.6.6.m6.3c"><apply id="S2.F2.34.6.6.m6.3.3.cmml" xref="S2.F2.34.6.6.m6.3.3"><csymbol cd="ambiguous" id="S2.F2.34.6.6.m6.3.3.2.cmml" xref="S2.F2.34.6.6.m6.3.3">superscript</csymbol><apply id="S2.F2.34.6.6.m6.3.3.1.cmml" xref="S2.F2.34.6.6.m6.3.3"><csymbol cd="ambiguous" id="S2.F2.34.6.6.m6.3.3.1.2.cmml" xref="S2.F2.34.6.6.m6.3.3">subscript</csymbol><set id="S2.F2.34.6.6.m6.3.3.1.1.2.cmml" xref="S2.F2.34.6.6.m6.3.3.1.1.1"><apply id="S2.F2.34.6.6.m6.3.3.1.1.1.1.cmml" xref="S2.F2.34.6.6.m6.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S2.F2.34.6.6.m6.3.3.1.1.1.1.1.cmml" xref="S2.F2.34.6.6.m6.3.3.1.1.1.1">subscript</csymbol><ci id="S2.F2.34.6.6.m6.3.3.1.1.1.1.2.cmml" xref="S2.F2.34.6.6.m6.3.3.1.1.1.1.2">𝑥</ci><list id="S2.F2.34.6.6.m6.2.2.2.3.cmml" xref="S2.F2.34.6.6.m6.2.2.2.2"><ci id="S2.F2.34.6.6.m6.1.1.1.1.cmml" xref="S2.F2.34.6.6.m6.1.1.1.1">𝑣</ci><apply id="S2.F2.34.6.6.m6.2.2.2.2.1.cmml" xref="S2.F2.34.6.6.m6.2.2.2.2.1"><csymbol cd="ambiguous" id="S2.F2.34.6.6.m6.2.2.2.2.1.1.cmml" xref="S2.F2.34.6.6.m6.2.2.2.2.1">subscript</csymbol><ci id="S2.F2.34.6.6.m6.2.2.2.2.1.2.cmml" xref="S2.F2.34.6.6.m6.2.2.2.2.1.2">𝑡</ci><ci id="S2.F2.34.6.6.m6.2.2.2.2.1.3.cmml" xref="S2.F2.34.6.6.m6.2.2.2.2.1.3">𝑖</ci></apply></list></apply></set><apply id="S2.F2.34.6.6.m6.3.3.1.3.cmml" xref="S2.F2.34.6.6.m6.3.3.1.3"><eq id="S2.F2.34.6.6.m6.3.3.1.3.1.cmml" xref="S2.F2.34.6.6.m6.3.3.1.3.1"></eq><ci id="S2.F2.34.6.6.m6.3.3.1.3.2.cmml" xref="S2.F2.34.6.6.m6.3.3.1.3.2">𝑖</ci><cn type="integer" id="S2.F2.34.6.6.m6.3.3.1.3.3.cmml" xref="S2.F2.34.6.6.m6.3.3.1.3.3">1</cn></apply></apply><ci id="S2.F2.34.6.6.m6.3.3.3.cmml" xref="S2.F2.34.6.6.m6.3.3.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.34.6.6.m6.3d">\{x_{v,t_{i}}\}_{i=1}^{K}</annotation></semantics></math> (illustrated with <math id="S2.F2.35.7.7.m7.1" class="ltx_Math" alttext="K=8" display="inline"><semantics id="S2.F2.35.7.7.m7.1b"><mrow id="S2.F2.35.7.7.m7.1.1" xref="S2.F2.35.7.7.m7.1.1.cmml"><mi id="S2.F2.35.7.7.m7.1.1.2" xref="S2.F2.35.7.7.m7.1.1.2.cmml">K</mi><mo id="S2.F2.35.7.7.m7.1.1.1" xref="S2.F2.35.7.7.m7.1.1.1.cmml">=</mo><mn id="S2.F2.35.7.7.m7.1.1.3" xref="S2.F2.35.7.7.m7.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.35.7.7.m7.1c"><apply id="S2.F2.35.7.7.m7.1.1.cmml" xref="S2.F2.35.7.7.m7.1.1"><eq id="S2.F2.35.7.7.m7.1.1.1.cmml" xref="S2.F2.35.7.7.m7.1.1.1"></eq><ci id="S2.F2.35.7.7.m7.1.1.2.cmml" xref="S2.F2.35.7.7.m7.1.1.2">𝐾</ci><cn type="integer" id="S2.F2.35.7.7.m7.1.1.3.cmml" xref="S2.F2.35.7.7.m7.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.35.7.7.m7.1d">K=8</annotation></semantics></math> in the figure).
The temporal slices are then encoded using unimodal transformers, <math id="S2.F2.36.8.8.m8.1" class="ltx_Math" alttext="E_{a}" display="inline"><semantics id="S2.F2.36.8.8.m8.1b"><msub id="S2.F2.36.8.8.m8.1.1" xref="S2.F2.36.8.8.m8.1.1.cmml"><mi id="S2.F2.36.8.8.m8.1.1.2" xref="S2.F2.36.8.8.m8.1.1.2.cmml">E</mi><mi id="S2.F2.36.8.8.m8.1.1.3" xref="S2.F2.36.8.8.m8.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.36.8.8.m8.1c"><apply id="S2.F2.36.8.8.m8.1.1.cmml" xref="S2.F2.36.8.8.m8.1.1"><csymbol cd="ambiguous" id="S2.F2.36.8.8.m8.1.1.1.cmml" xref="S2.F2.36.8.8.m8.1.1">subscript</csymbol><ci id="S2.F2.36.8.8.m8.1.1.2.cmml" xref="S2.F2.36.8.8.m8.1.1.2">𝐸</ci><ci id="S2.F2.36.8.8.m8.1.1.3.cmml" xref="S2.F2.36.8.8.m8.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.36.8.8.m8.1d">E_{a}</annotation></semantics></math> and <math id="S2.F2.37.9.9.m9.1" class="ltx_Math" alttext="E_{v}" display="inline"><semantics id="S2.F2.37.9.9.m9.1b"><msub id="S2.F2.37.9.9.m9.1.1" xref="S2.F2.37.9.9.m9.1.1.cmml"><mi id="S2.F2.37.9.9.m9.1.1.2" xref="S2.F2.37.9.9.m9.1.1.2.cmml">E</mi><mi id="S2.F2.37.9.9.m9.1.1.3" xref="S2.F2.37.9.9.m9.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.37.9.9.m9.1c"><apply id="S2.F2.37.9.9.m9.1.1.cmml" xref="S2.F2.37.9.9.m9.1.1"><csymbol cd="ambiguous" id="S2.F2.37.9.9.m9.1.1.1.cmml" xref="S2.F2.37.9.9.m9.1.1">subscript</csymbol><ci id="S2.F2.37.9.9.m9.1.1.2.cmml" xref="S2.F2.37.9.9.m9.1.1.2">𝐸</ci><ci id="S2.F2.37.9.9.m9.1.1.3.cmml" xref="S2.F2.37.9.9.m9.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.37.9.9.m9.1d">E_{v}</annotation></semantics></math>, to yield feature embeddings <math id="S2.F2.38.10.10.m10.1" class="ltx_Math" alttext="\bm{a}" display="inline"><semantics id="S2.F2.38.10.10.m10.1b"><mi id="S2.F2.38.10.10.m10.1.1" xref="S2.F2.38.10.10.m10.1.1.cmml">𝒂</mi><annotation-xml encoding="MathML-Content" id="S2.F2.38.10.10.m10.1c"><ci id="S2.F2.38.10.10.m10.1.1.cmml" xref="S2.F2.38.10.10.m10.1.1">𝒂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.38.10.10.m10.1d">\bm{a}</annotation></semantics></math> and <math id="S2.F2.39.11.11.m11.1" class="ltx_Math" alttext="\bm{v}" display="inline"><semantics id="S2.F2.39.11.11.m11.1b"><mi id="S2.F2.39.11.11.m11.1.1" xref="S2.F2.39.11.11.m11.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S2.F2.39.11.11.m11.1c"><ci id="S2.F2.39.11.11.m11.1.1.cmml" xref="S2.F2.39.11.11.m11.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.39.11.11.m11.1d">\bm{v}</annotation></semantics></math>. We then complementarily mask <math id="S2.F2.40.12.12.m12.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S2.F2.40.12.12.m12.1b"><mrow id="S2.F2.40.12.12.m12.1.1" xref="S2.F2.40.12.12.m12.1.1.cmml"><mn id="S2.F2.40.12.12.m12.1.1.2" xref="S2.F2.40.12.12.m12.1.1.2.cmml">50</mn><mo id="S2.F2.40.12.12.m12.1.1.1" xref="S2.F2.40.12.12.m12.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.40.12.12.m12.1c"><apply id="S2.F2.40.12.12.m12.1.1.cmml" xref="S2.F2.40.12.12.m12.1.1"><csymbol cd="latexml" id="S2.F2.40.12.12.m12.1.1.1.cmml" xref="S2.F2.40.12.12.m12.1.1.1">percent</csymbol><cn type="integer" id="S2.F2.40.12.12.m12.1.1.2.cmml" xref="S2.F2.40.12.12.m12.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.40.12.12.m12.1d">50\%</annotation></semantics></math> of the temporal slices in (<math id="S2.F2.41.13.13.m13.1" class="ltx_Math" alttext="\bm{a}" display="inline"><semantics id="S2.F2.41.13.13.m13.1b"><mi id="S2.F2.41.13.13.m13.1.1" xref="S2.F2.41.13.13.m13.1.1.cmml">𝒂</mi><annotation-xml encoding="MathML-Content" id="S2.F2.41.13.13.m13.1c"><ci id="S2.F2.41.13.13.m13.1.1.cmml" xref="S2.F2.41.13.13.m13.1.1">𝒂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.41.13.13.m13.1d">\bm{a}</annotation></semantics></math>, <math id="S2.F2.42.14.14.m14.1" class="ltx_Math" alttext="\bm{v}" display="inline"><semantics id="S2.F2.42.14.14.m14.1b"><mi id="S2.F2.42.14.14.m14.1.1" xref="S2.F2.42.14.14.m14.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S2.F2.42.14.14.m14.1c"><ci id="S2.F2.42.14.14.m14.1.1.cmml" xref="S2.F2.42.14.14.m14.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.42.14.14.m14.1d">\bm{v}</annotation></semantics></math>) with binary masks (<math id="S2.F2.43.15.15.m15.1" class="ltx_Math" alttext="\bm{M}_{a}" display="inline"><semantics id="S2.F2.43.15.15.m15.1b"><msub id="S2.F2.43.15.15.m15.1.1" xref="S2.F2.43.15.15.m15.1.1.cmml"><mi id="S2.F2.43.15.15.m15.1.1.2" xref="S2.F2.43.15.15.m15.1.1.2.cmml">𝑴</mi><mi id="S2.F2.43.15.15.m15.1.1.3" xref="S2.F2.43.15.15.m15.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.43.15.15.m15.1c"><apply id="S2.F2.43.15.15.m15.1.1.cmml" xref="S2.F2.43.15.15.m15.1.1"><csymbol cd="ambiguous" id="S2.F2.43.15.15.m15.1.1.1.cmml" xref="S2.F2.43.15.15.m15.1.1">subscript</csymbol><ci id="S2.F2.43.15.15.m15.1.1.2.cmml" xref="S2.F2.43.15.15.m15.1.1.2">𝑴</ci><ci id="S2.F2.43.15.15.m15.1.1.3.cmml" xref="S2.F2.43.15.15.m15.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.43.15.15.m15.1d">\bm{M}_{a}</annotation></semantics></math>, <math id="S2.F2.44.16.16.m16.1" class="ltx_Math" alttext="\bm{M}_{v}" display="inline"><semantics id="S2.F2.44.16.16.m16.1b"><msub id="S2.F2.44.16.16.m16.1.1" xref="S2.F2.44.16.16.m16.1.1.cmml"><mi id="S2.F2.44.16.16.m16.1.1.2" xref="S2.F2.44.16.16.m16.1.1.2.cmml">𝑴</mi><mi id="S2.F2.44.16.16.m16.1.1.3" xref="S2.F2.44.16.16.m16.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.44.16.16.m16.1c"><apply id="S2.F2.44.16.16.m16.1.1.cmml" xref="S2.F2.44.16.16.m16.1.1"><csymbol cd="ambiguous" id="S2.F2.44.16.16.m16.1.1.1.cmml" xref="S2.F2.44.16.16.m16.1.1">subscript</csymbol><ci id="S2.F2.44.16.16.m16.1.1.2.cmml" xref="S2.F2.44.16.16.m16.1.1.2">𝑴</ci><ci id="S2.F2.44.16.16.m16.1.1.3.cmml" xref="S2.F2.44.16.16.m16.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.44.16.16.m16.1d">\bm{M}_{v}</annotation></semantics></math>).
The visible slices of <math id="S2.F2.45.17.17.m17.1" class="ltx_Math" alttext="\bm{a}" display="inline"><semantics id="S2.F2.45.17.17.m17.1b"><mi id="S2.F2.45.17.17.m17.1.1" xref="S2.F2.45.17.17.m17.1.1.cmml">𝒂</mi><annotation-xml encoding="MathML-Content" id="S2.F2.45.17.17.m17.1c"><ci id="S2.F2.45.17.17.m17.1.1.cmml" xref="S2.F2.45.17.17.m17.1.1">𝒂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.45.17.17.m17.1d">\bm{a}</annotation></semantics></math> and <math id="S2.F2.46.18.18.m18.1" class="ltx_Math" alttext="\bm{v}" display="inline"><semantics id="S2.F2.46.18.18.m18.1b"><mi id="S2.F2.46.18.18.m18.1.1" xref="S2.F2.46.18.18.m18.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S2.F2.46.18.18.m18.1c"><ci id="S2.F2.46.18.18.m18.1.1.cmml" xref="S2.F2.46.18.18.m18.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.46.18.18.m18.1d">\bm{v}</annotation></semantics></math> are passed through A2V and V2A networks respectively, to generate cross-modal slices <math id="S2.F2.47.19.19.m19.1" class="ltx_Math" alttext="\bm{a_{v}}" display="inline"><semantics id="S2.F2.47.19.19.m19.1b"><msub id="S2.F2.47.19.19.m19.1.1" xref="S2.F2.47.19.19.m19.1.1.cmml"><mi id="S2.F2.47.19.19.m19.1.1.2" xref="S2.F2.47.19.19.m19.1.1.2.cmml">𝒂</mi><mi id="S2.F2.47.19.19.m19.1.1.3" xref="S2.F2.47.19.19.m19.1.1.3.cmml">𝒗</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.47.19.19.m19.1c"><apply id="S2.F2.47.19.19.m19.1.1.cmml" xref="S2.F2.47.19.19.m19.1.1"><csymbol cd="ambiguous" id="S2.F2.47.19.19.m19.1.1.1.cmml" xref="S2.F2.47.19.19.m19.1.1">subscript</csymbol><ci id="S2.F2.47.19.19.m19.1.1.2.cmml" xref="S2.F2.47.19.19.m19.1.1.2">𝒂</ci><ci id="S2.F2.47.19.19.m19.1.1.3.cmml" xref="S2.F2.47.19.19.m19.1.1.3">𝒗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.47.19.19.m19.1d">\bm{a_{v}}</annotation></semantics></math> and <math id="S2.F2.48.20.20.m20.1" class="ltx_Math" alttext="\bm{v_{a}}" display="inline"><semantics id="S2.F2.48.20.20.m20.1b"><msub id="S2.F2.48.20.20.m20.1.1" xref="S2.F2.48.20.20.m20.1.1.cmml"><mi id="S2.F2.48.20.20.m20.1.1.2" xref="S2.F2.48.20.20.m20.1.1.2.cmml">𝒗</mi><mi id="S2.F2.48.20.20.m20.1.1.3" xref="S2.F2.48.20.20.m20.1.1.3.cmml">𝒂</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.48.20.20.m20.1c"><apply id="S2.F2.48.20.20.m20.1.1.cmml" xref="S2.F2.48.20.20.m20.1.1"><csymbol cd="ambiguous" id="S2.F2.48.20.20.m20.1.1.1.cmml" xref="S2.F2.48.20.20.m20.1.1">subscript</csymbol><ci id="S2.F2.48.20.20.m20.1.1.2.cmml" xref="S2.F2.48.20.20.m20.1.1.2">𝒗</ci><ci id="S2.F2.48.20.20.m20.1.1.3.cmml" xref="S2.F2.48.20.20.m20.1.1.3">𝒂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.48.20.20.m20.1d">\bm{v_{a}}</annotation></semantics></math>. The masked slices of <math id="S2.F2.49.21.21.m21.1" class="ltx_Math" alttext="\bm{a}" display="inline"><semantics id="S2.F2.49.21.21.m21.1b"><mi id="S2.F2.49.21.21.m21.1.1" xref="S2.F2.49.21.21.m21.1.1.cmml">𝒂</mi><annotation-xml encoding="MathML-Content" id="S2.F2.49.21.21.m21.1c"><ci id="S2.F2.49.21.21.m21.1.1.cmml" xref="S2.F2.49.21.21.m21.1.1">𝒂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.49.21.21.m21.1d">\bm{a}</annotation></semantics></math> and <math id="S2.F2.50.22.22.m22.1" class="ltx_Math" alttext="\bm{v}" display="inline"><semantics id="S2.F2.50.22.22.m22.1b"><mi id="S2.F2.50.22.22.m22.1.1" xref="S2.F2.50.22.22.m22.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S2.F2.50.22.22.m22.1c"><ci id="S2.F2.50.22.22.m22.1.1.cmml" xref="S2.F2.50.22.22.m22.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.50.22.22.m22.1d">\bm{v}</annotation></semantics></math> are then replaced with the corresponding slices in <math id="S2.F2.51.23.23.m23.1" class="ltx_Math" alttext="\bm{a_{v}}" display="inline"><semantics id="S2.F2.51.23.23.m23.1b"><msub id="S2.F2.51.23.23.m23.1.1" xref="S2.F2.51.23.23.m23.1.1.cmml"><mi id="S2.F2.51.23.23.m23.1.1.2" xref="S2.F2.51.23.23.m23.1.1.2.cmml">𝒂</mi><mi id="S2.F2.51.23.23.m23.1.1.3" xref="S2.F2.51.23.23.m23.1.1.3.cmml">𝒗</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.51.23.23.m23.1c"><apply id="S2.F2.51.23.23.m23.1.1.cmml" xref="S2.F2.51.23.23.m23.1.1"><csymbol cd="ambiguous" id="S2.F2.51.23.23.m23.1.1.1.cmml" xref="S2.F2.51.23.23.m23.1.1">subscript</csymbol><ci id="S2.F2.51.23.23.m23.1.1.2.cmml" xref="S2.F2.51.23.23.m23.1.1.2">𝒂</ci><ci id="S2.F2.51.23.23.m23.1.1.3.cmml" xref="S2.F2.51.23.23.m23.1.1.3">𝒗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.51.23.23.m23.1d">\bm{a_{v}}</annotation></semantics></math> and <math id="S2.F2.52.24.24.m24.1" class="ltx_Math" alttext="\bm{v_{a}}" display="inline"><semantics id="S2.F2.52.24.24.m24.1b"><msub id="S2.F2.52.24.24.m24.1.1" xref="S2.F2.52.24.24.m24.1.1.cmml"><mi id="S2.F2.52.24.24.m24.1.1.2" xref="S2.F2.52.24.24.m24.1.1.2.cmml">𝒗</mi><mi id="S2.F2.52.24.24.m24.1.1.3" xref="S2.F2.52.24.24.m24.1.1.3.cmml">𝒂</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.52.24.24.m24.1c"><apply id="S2.F2.52.24.24.m24.1.1.cmml" xref="S2.F2.52.24.24.m24.1.1"><csymbol cd="ambiguous" id="S2.F2.52.24.24.m24.1.1.1.cmml" xref="S2.F2.52.24.24.m24.1.1">subscript</csymbol><ci id="S2.F2.52.24.24.m24.1.1.2.cmml" xref="S2.F2.52.24.24.m24.1.1.2">𝒗</ci><ci id="S2.F2.52.24.24.m24.1.1.3.cmml" xref="S2.F2.52.24.24.m24.1.1.3">𝒂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.52.24.24.m24.1d">\bm{v_{a}}</annotation></semantics></math>. The resulting cross-modal fusion representations, <math id="S2.F2.53.25.25.m25.1" class="ltx_Math" alttext="\bm{a^{\prime}}" display="inline"><semantics id="S2.F2.53.25.25.m25.1b"><msup id="S2.F2.53.25.25.m25.1.1" xref="S2.F2.53.25.25.m25.1.1.cmml"><mi id="S2.F2.53.25.25.m25.1.1.2" xref="S2.F2.53.25.25.m25.1.1.2.cmml">𝒂</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S2.F2.53.25.25.m25.1.1.3" xref="S2.F2.53.25.25.m25.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.F2.53.25.25.m25.1c"><apply id="S2.F2.53.25.25.m25.1.1.cmml" xref="S2.F2.53.25.25.m25.1.1"><csymbol cd="ambiguous" id="S2.F2.53.25.25.m25.1.1.1.cmml" xref="S2.F2.53.25.25.m25.1.1">superscript</csymbol><ci id="S2.F2.53.25.25.m25.1.1.2.cmml" xref="S2.F2.53.25.25.m25.1.1.2">𝒂</ci><ci id="S2.F2.53.25.25.m25.1.1.3.cmml" xref="S2.F2.53.25.25.m25.1.1.3">bold-′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.53.25.25.m25.1d">\bm{a^{\prime}}</annotation></semantics></math> and <math id="S2.F2.54.26.26.m26.1" class="ltx_Math" alttext="\bm{v^{\prime}}" display="inline"><semantics id="S2.F2.54.26.26.m26.1b"><msup id="S2.F2.54.26.26.m26.1.1" xref="S2.F2.54.26.26.m26.1.1.cmml"><mi id="S2.F2.54.26.26.m26.1.1.2" xref="S2.F2.54.26.26.m26.1.1.2.cmml">𝒗</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S2.F2.54.26.26.m26.1.1.3" xref="S2.F2.54.26.26.m26.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.F2.54.26.26.m26.1c"><apply id="S2.F2.54.26.26.m26.1.1.cmml" xref="S2.F2.54.26.26.m26.1.1"><csymbol cd="ambiguous" id="S2.F2.54.26.26.m26.1.1.1.cmml" xref="S2.F2.54.26.26.m26.1.1">superscript</csymbol><ci id="S2.F2.54.26.26.m26.1.1.2.cmml" xref="S2.F2.54.26.26.m26.1.1.2">𝒗</ci><ci id="S2.F2.54.26.26.m26.1.1.3.cmml" xref="S2.F2.54.26.26.m26.1.1.3">bold-′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.54.26.26.m26.1d">\bm{v^{\prime}}</annotation></semantics></math>, are input to unimodal decoders to obtain the audio and visual reconstructions, <math id="S2.F2.55.27.27.m27.1" class="ltx_Math" alttext="\bm{\hat{x}_{a}}" display="inline"><semantics id="S2.F2.55.27.27.m27.1b"><msub id="S2.F2.55.27.27.m27.1.1" xref="S2.F2.55.27.27.m27.1.1.cmml"><mover accent="true" id="S2.F2.55.27.27.m27.1.1.2" xref="S2.F2.55.27.27.m27.1.1.2.cmml"><mi id="S2.F2.55.27.27.m27.1.1.2.2" xref="S2.F2.55.27.27.m27.1.1.2.2.cmml">𝒙</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S2.F2.55.27.27.m27.1.1.2.1" xref="S2.F2.55.27.27.m27.1.1.2.1.cmml">^</mo></mover><mi id="S2.F2.55.27.27.m27.1.1.3" xref="S2.F2.55.27.27.m27.1.1.3.cmml">𝒂</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.55.27.27.m27.1c"><apply id="S2.F2.55.27.27.m27.1.1.cmml" xref="S2.F2.55.27.27.m27.1.1"><csymbol cd="ambiguous" id="S2.F2.55.27.27.m27.1.1.1.cmml" xref="S2.F2.55.27.27.m27.1.1">subscript</csymbol><apply id="S2.F2.55.27.27.m27.1.1.2.cmml" xref="S2.F2.55.27.27.m27.1.1.2"><ci id="S2.F2.55.27.27.m27.1.1.2.1.cmml" xref="S2.F2.55.27.27.m27.1.1.2.1">bold-^</ci><ci id="S2.F2.55.27.27.m27.1.1.2.2.cmml" xref="S2.F2.55.27.27.m27.1.1.2.2">𝒙</ci></apply><ci id="S2.F2.55.27.27.m27.1.1.3.cmml" xref="S2.F2.55.27.27.m27.1.1.3">𝒂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.55.27.27.m27.1d">\bm{\hat{x}_{a}}</annotation></semantics></math> and <math id="S2.F2.56.28.28.m28.1" class="ltx_Math" alttext="\bm{\hat{x}_{v}}" display="inline"><semantics id="S2.F2.56.28.28.m28.1b"><msub id="S2.F2.56.28.28.m28.1.1" xref="S2.F2.56.28.28.m28.1.1.cmml"><mover accent="true" id="S2.F2.56.28.28.m28.1.1.2" xref="S2.F2.56.28.28.m28.1.1.2.cmml"><mi id="S2.F2.56.28.28.m28.1.1.2.2" xref="S2.F2.56.28.28.m28.1.1.2.2.cmml">𝒙</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S2.F2.56.28.28.m28.1.1.2.1" xref="S2.F2.56.28.28.m28.1.1.2.1.cmml">^</mo></mover><mi id="S2.F2.56.28.28.m28.1.1.3" xref="S2.F2.56.28.28.m28.1.1.3.cmml">𝒗</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.56.28.28.m28.1c"><apply id="S2.F2.56.28.28.m28.1.1.cmml" xref="S2.F2.56.28.28.m28.1.1"><csymbol cd="ambiguous" id="S2.F2.56.28.28.m28.1.1.1.cmml" xref="S2.F2.56.28.28.m28.1.1">subscript</csymbol><apply id="S2.F2.56.28.28.m28.1.1.2.cmml" xref="S2.F2.56.28.28.m28.1.1.2"><ci id="S2.F2.56.28.28.m28.1.1.2.1.cmml" xref="S2.F2.56.28.28.m28.1.1.2.1">bold-^</ci><ci id="S2.F2.56.28.28.m28.1.1.2.2.cmml" xref="S2.F2.56.28.28.m28.1.1.2.2">𝒙</ci></apply><ci id="S2.F2.56.28.28.m28.1.1.3.cmml" xref="S2.F2.56.28.28.m28.1.1.3">𝒗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.56.28.28.m28.1d">\bm{\hat{x}_{v}}</annotation></semantics></math>. For the learning, we use a dual-objective loss function, which computes the contrastive loss between the audio and visual feature embeddings and the autoencoder loss between the input and the reconstruction of the masked tokens.
</span></span></figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Deepfake Detection</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p"><span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_bold">Visual-only methods.</span>
Multiple recent works have made use of visual-only artifacts for deepfake detection.
The authors of  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> train a <span title="" class="ltx_glossaryref">Convolutional Neural Network (CNN)</span> (XceptionNet) end-to-end, setting one of the first baselines on the dataset they shared with the community.
Some methods target specific face regions for exposing deepfakes. For example, LipForensics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> relies on lip movements that might be difficult to reproduce by generative methods. Others consider inconsistent head pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">66</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> or eye blinking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>.
Another common approach is to consider both spatial and temporal domains. FTCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite> proposes a combination of a <span title="" class="ltx_glossaryref">CNN</span> and a transformer network to exploit short-time and long-time temporal incoherence. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite> extracts spatial features by means of an attention-based network and then fuses those features with a temporal module.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Several papers based on the <span title="" class="ltx_glossaryref">Vision Transformer (ViT)</span> have been published since the advent of the original paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> for image classification. An example is CViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>]</cite>, where learnable features are extracted by means of a <span title="" class="ltx_glossaryref">CNN</span> and subsequently fed to a <span title="" class="ltx_glossaryref">ViT</span> for the classification task. Similar approaches are followed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">74</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>.
Recently, generalization to unseen deepfake methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>]</cite> and the impact of the identity leakage during training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> have also been investigated.
RealForensics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> proposed a hybrid approach that consists of using a multi-modal pre-training pipeline, where audio and visuals exclusively from real samples are used for computing internal representations that help the classifier to discriminate between real and fake video. This is not considered a pure multi-modal approach as the final classification is performed just on visuals, discarding the audio modality.
As modern-day video deepfakes consist of both audio and visual manipulations, uni-modal deepfake detection methods prove to be less effective.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Audio-visual methods.</span>
These methods consider audio and visual signals to target deepfake detection on both modalities.
One of the first papers to address multi-modality is <span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_italic">Emotions Don’t Lie</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>, proposing a Siamese Network where uni-modal features are passed to an emotion recognition network to compare affective cues corresponding to perceived emotion from the two modalities within a video.
<span id="S2.SS2.p3.1.3" class="ltx_text ltx_font_italic">Not made for each other</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> explicitly modeled the dissimilarity between modalities, and proposed the Modality Dissonance Score (MDS) network, where a contrastive loss is computed on single modality embeddings to expose differences on audio-visual pairs.
<span id="S2.SS2.p3.1.4" class="ltx_text ltx_font_italic">Voice-Face matching Detection</span> (VFD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> is another example of using contrastive loss for modeling face and voice homogeneity.
A similar concept is exploited in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, where the focus is on phoneme-viseme mismatch. The idea is that a given dynamic of mouth shape (viseme) should correspond to a given emitted sound (phoneme). The authors only focus on the mouth region, showing how deepfake methods struggle to reproduce certain dynamics.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">More recently, the paradigm for multi-modality shifted towards fusion of single-modality features. AV-DFD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">71</span></a>]</cite> proposes a joint audio-visual deepfake detection framework in which visual and audio features are aligned and tiled to be passed onto a cross-attention mechanism on the temporal dimension. More recent papers study the encoding/decoding potential of <span title="" class="ltx_glossaryref">ViT</span> s and build feature fusion in the embedding space on the decoder side. Examples are AVFakeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> and AVoiD-DF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p">The proposed algorithm, AVFF, consists of two stages: (i) representation learning, and (ii) deepfake classification. Stage 1 aims to acquire an audio-visual representation with cross-modal correspondence via self-supervised learning, and it solely utilizes real face videos. The model learns audio-visual correspondences inherent to real videos via a contrastive learning objective and an effective complementary masking and fusion strategy that sits within an auto-encoding objective (see Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.1 Multi-Modal Representation Learning ‣ 2 Related Works ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Here, the complementary masking and fusion strategy takes uni-modal audio and visual embeddings <math id="S3.p1.1.m1.2" class="ltx_Math" alttext="(\bm{a},\bm{v})" display="inline"><semantics id="S3.p1.1.m1.2a"><mrow id="S3.p1.1.m1.2.3.2" xref="S3.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.p1.1.m1.2.3.2.1" xref="S3.p1.1.m1.2.3.1.cmml">(</mo><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">𝒂</mi><mo id="S3.p1.1.m1.2.3.2.2" xref="S3.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.p1.1.m1.2.2" xref="S3.p1.1.m1.2.2.cmml">𝒗</mi><mo stretchy="false" id="S3.p1.1.m1.2.3.2.3" xref="S3.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.2b"><interval closure="open" id="S3.p1.1.m1.2.3.1.cmml" xref="S3.p1.1.m1.2.3.2"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝒂</ci><ci id="S3.p1.1.m1.2.2.cmml" xref="S3.p1.1.m1.2.2">𝒗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.2c">(\bm{a},\bm{v})</annotation></semantics></math> and systematically masks them to force the learning of advanced embeddings <math id="S3.p1.2.m2.2" class="ltx_Math" alttext="(\bm{a^{\prime}},\bm{v^{\prime}})" display="inline"><semantics id="S3.p1.2.m2.2a"><mrow id="S3.p1.2.m2.2.2.2" xref="S3.p1.2.m2.2.2.3.cmml"><mo stretchy="false" id="S3.p1.2.m2.2.2.2.3" xref="S3.p1.2.m2.2.2.3.cmml">(</mo><msup id="S3.p1.2.m2.1.1.1.1" xref="S3.p1.2.m2.1.1.1.1.cmml"><mi id="S3.p1.2.m2.1.1.1.1.2" xref="S3.p1.2.m2.1.1.1.1.2.cmml">𝒂</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.p1.2.m2.1.1.1.1.3" xref="S3.p1.2.m2.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.p1.2.m2.2.2.2.4" xref="S3.p1.2.m2.2.2.3.cmml">,</mo><msup id="S3.p1.2.m2.2.2.2.2" xref="S3.p1.2.m2.2.2.2.2.cmml"><mi id="S3.p1.2.m2.2.2.2.2.2" xref="S3.p1.2.m2.2.2.2.2.2.cmml">𝒗</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.p1.2.m2.2.2.2.2.3" xref="S3.p1.2.m2.2.2.2.2.3.cmml">′</mo></msup><mo stretchy="false" id="S3.p1.2.m2.2.2.2.5" xref="S3.p1.2.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.2b"><interval closure="open" id="S3.p1.2.m2.2.2.3.cmml" xref="S3.p1.2.m2.2.2.2"><apply id="S3.p1.2.m2.1.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1.1">superscript</csymbol><ci id="S3.p1.2.m2.1.1.1.1.2.cmml" xref="S3.p1.2.m2.1.1.1.1.2">𝒂</ci><ci id="S3.p1.2.m2.1.1.1.1.3.cmml" xref="S3.p1.2.m2.1.1.1.1.3">bold-′</ci></apply><apply id="S3.p1.2.m2.2.2.2.2.cmml" xref="S3.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p1.2.m2.2.2.2.2.1.cmml" xref="S3.p1.2.m2.2.2.2.2">superscript</csymbol><ci id="S3.p1.2.m2.2.2.2.2.2.cmml" xref="S3.p1.2.m2.2.2.2.2.2">𝒗</ci><ci id="S3.p1.2.m2.2.2.2.2.3.cmml" xref="S3.p1.2.m2.2.2.2.2.3">bold-′</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.2c">(\bm{a^{\prime}},\bm{v^{\prime}})</annotation></semantics></math> via reconstruction in the MAE sense. To instill cross-modal dependency, tokens from one modality are used to learn the masked embeddings of the other modality via cross-modal token conversion networks (A2V and V2A in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.1 Multi-Modal Representation Learning ‣ 2 Related Works ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Since we work exclusively with real face videos in this stage, the model learns the dependency between “real” speech audio and the corresponding visual facial features. In Stage 2, a classifier is trained to distinguish between real and fake videos using the learned representations from the first stage. Effectively, the representation learning stage serves as pre-training for the downstream task of video deepfake detection.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preprocessing</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Initially, visual frames and the corresponding audio waveforms are extracted from raw videos at a sampling rate of 5 fps and 16 kHz, respectively. Given our emphasis on audio-visual correspondence, we align the cropped facial regions and eliminate the background in the visual frames using FaceX-Zoo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>.
This step is performed since background variations typically exhibit minimal correspondence with speech audio. Simultaneously, the audio waveform is converted into a log-mel spectrogram with <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">L</annotation></semantics></math> frequency bins.
Henceforth, we refer to the preprocessed visual frames and log-mel spectrograms as the visual and audio input representations, respectively.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Representation Learning Stage</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The primary objective of this stage is to learn a representation that effectively captures audio-visual feature correspondences inherently present in real videos (and different thereof from the correspondences in fake videos). Drawing inspiration from CAV-MAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>, we propose a dual self-supervised learning approach that incorporates contrastive learning and autoencoding objectives. While contrastive learning helps build cross-modal correlations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>, we found in preliminary experiments that relying solely on it does not establish a strong correspondence between the audio and visual modalities. Therefore, we supplement it with an autoencoding objective and embed a complementary masking and cross-modal fusion strategy into the autoencoding framework. This allows us to learn rich cross-modal representations that result in improved deepfake detection (see <a href="#S5.T4" title="In 5 Ablation Study ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>). In <a href="#S2.F2" title="In 2.1 Multi-Modal Representation Learning ‣ 2 Related Works ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we illustrate the overall pipeline for our representation learning stage and discuss its key components next.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.27" class="ltx_p"><span id="S3.SS2.p2.27.1" class="ltx_text ltx_font_bold">Input Tokenization.</span>
Given a dataset <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{r}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">𝒟</mi><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝒟</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathcal{D}_{r}</annotation></semantics></math> of real talking human portrait videos, we denote a video sample <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="x\in\mathcal{D}_{r}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">x</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">∈</mo><msub id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.2.m2.1.1.3.2" xref="S3.SS2.p2.2.m2.1.1.3.2.cmml">𝒟</mi><mi id="S3.SS2.p2.2.m2.1.1.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.cmml">r</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><in id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></in><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑥</ci><apply id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2">𝒟</ci><ci id="S3.SS2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">x\in\mathcal{D}_{r}</annotation></semantics></math> with a time duration <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">T</annotation></semantics></math> as comprising of audio and visual components, <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="{x}_{a}\in\mathbb{R}^{T_{a}\times L}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><msub id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2.2" xref="S3.SS2.p2.4.m4.1.1.2.2.cmml">x</mi><mi id="S3.SS2.p2.4.m4.1.1.2.3" xref="S3.SS2.p2.4.m4.1.1.2.3.cmml">a</mi></msub><mo id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml"><mi id="S3.SS2.p2.4.m4.1.1.3.2" xref="S3.SS2.p2.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.4.m4.1.1.3.3" xref="S3.SS2.p2.4.m4.1.1.3.3.cmml"><msub id="S3.SS2.p2.4.m4.1.1.3.3.2" xref="S3.SS2.p2.4.m4.1.1.3.3.2.cmml"><mi id="S3.SS2.p2.4.m4.1.1.3.3.2.2" xref="S3.SS2.p2.4.m4.1.1.3.3.2.2.cmml">T</mi><mi id="S3.SS2.p2.4.m4.1.1.3.3.2.3" xref="S3.SS2.p2.4.m4.1.1.3.3.2.3.cmml">a</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.4.m4.1.1.3.3.1" xref="S3.SS2.p2.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.4.m4.1.1.3.3.3" xref="S3.SS2.p2.4.m4.1.1.3.3.3.cmml">L</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><in id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></in><apply id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2.2">𝑥</ci><ci id="S3.SS2.p2.4.m4.1.1.2.3.cmml" xref="S3.SS2.p2.4.m4.1.1.2.3">𝑎</ci></apply><apply id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.4.m4.1.1.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3"><times id="S3.SS2.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.1"></times><apply id="S3.SS2.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.3.3.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.2">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.3.3.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.2.2">𝑇</ci><ci id="S3.SS2.p2.4.m4.1.1.3.3.2.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.2.3">𝑎</ci></apply><ci id="S3.SS2.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.3">𝐿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">{x}_{a}\in\mathbb{R}^{T_{a}\times L}</annotation></semantics></math> and <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="{x}_{v}\in\mathbb{R}^{T_{v}\times C\times H\times W}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><msub id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2.2" xref="S3.SS2.p2.5.m5.1.1.2.2.cmml">x</mi><mi id="S3.SS2.p2.5.m5.1.1.2.3" xref="S3.SS2.p2.5.m5.1.1.2.3.cmml">v</mi></msub><mo id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml"><mi id="S3.SS2.p2.5.m5.1.1.3.2" xref="S3.SS2.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.5.m5.1.1.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.cmml"><msub id="S3.SS2.p2.5.m5.1.1.3.3.2" xref="S3.SS2.p2.5.m5.1.1.3.3.2.cmml"><mi id="S3.SS2.p2.5.m5.1.1.3.3.2.2" xref="S3.SS2.p2.5.m5.1.1.3.3.2.2.cmml">T</mi><mi id="S3.SS2.p2.5.m5.1.1.3.3.2.3" xref="S3.SS2.p2.5.m5.1.1.3.3.2.3.cmml">v</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.5.m5.1.1.3.3.1" xref="S3.SS2.p2.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.5.m5.1.1.3.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.3.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.5.m5.1.1.3.3.1a" xref="S3.SS2.p2.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.5.m5.1.1.3.3.4" xref="S3.SS2.p2.5.m5.1.1.3.3.4.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.5.m5.1.1.3.3.1b" xref="S3.SS2.p2.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.5.m5.1.1.3.3.5" xref="S3.SS2.p2.5.m5.1.1.3.3.5.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><in id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></in><apply id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.2.1.cmml" xref="S3.SS2.p2.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2.2">𝑥</ci><ci id="S3.SS2.p2.5.m5.1.1.2.3.cmml" xref="S3.SS2.p2.5.m5.1.1.2.3">𝑣</ci></apply><apply id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS2.p2.5.m5.1.1.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3"><times id="S3.SS2.p2.5.m5.1.1.3.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.1"></times><apply id="S3.SS2.p2.5.m5.1.1.3.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.3.3.2.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.2">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.3.3.2.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.2.2">𝑇</ci><ci id="S3.SS2.p2.5.m5.1.1.3.3.2.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.2.3">𝑣</ci></apply><ci id="S3.SS2.p2.5.m5.1.1.3.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.3">𝐶</ci><ci id="S3.SS2.p2.5.m5.1.1.3.3.4.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.4">𝐻</ci><ci id="S3.SS2.p2.5.m5.1.1.3.3.5.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.5">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">{x}_{v}\in\mathbb{R}^{T_{v}\times C\times H\times W}</annotation></semantics></math>, respectively. In <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="{x}_{a}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><msub id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">x</mi><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">𝑥</ci><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">{x}_{a}</annotation></semantics></math>, the <math id="S3.SS2.p2.7.m7.2" class="ltx_Math" alttext="(T_{a},L)" display="inline"><semantics id="S3.SS2.p2.7.m7.2a"><mrow id="S3.SS2.p2.7.m7.2.2.1" xref="S3.SS2.p2.7.m7.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.p2.7.m7.2.2.1.2" xref="S3.SS2.p2.7.m7.2.2.2.cmml">(</mo><msub id="S3.SS2.p2.7.m7.2.2.1.1" xref="S3.SS2.p2.7.m7.2.2.1.1.cmml"><mi id="S3.SS2.p2.7.m7.2.2.1.1.2" xref="S3.SS2.p2.7.m7.2.2.1.1.2.cmml">T</mi><mi id="S3.SS2.p2.7.m7.2.2.1.1.3" xref="S3.SS2.p2.7.m7.2.2.1.1.3.cmml">a</mi></msub><mo id="S3.SS2.p2.7.m7.2.2.1.3" xref="S3.SS2.p2.7.m7.2.2.2.cmml">,</mo><mi id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml">L</mi><mo stretchy="false" id="S3.SS2.p2.7.m7.2.2.1.4" xref="S3.SS2.p2.7.m7.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.2b"><interval closure="open" id="S3.SS2.p2.7.m7.2.2.2.cmml" xref="S3.SS2.p2.7.m7.2.2.1"><apply id="S3.SS2.p2.7.m7.2.2.1.1.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.2.2.1.1.1.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p2.7.m7.2.2.1.1.2.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.2">𝑇</ci><ci id="S3.SS2.p2.7.m7.2.2.1.1.3.cmml" xref="S3.SS2.p2.7.m7.2.2.1.1.3">𝑎</ci></apply><ci id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">𝐿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.2c">(T_{a},L)</annotation></semantics></math> denote the number of audio frames and mel-frequency bins. In <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="{x}_{v}" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><msub id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml"><mi id="S3.SS2.p2.8.m8.1.1.2" xref="S3.SS2.p2.8.m8.1.1.2.cmml">x</mi><mi id="S3.SS2.p2.8.m8.1.1.3" xref="S3.SS2.p2.8.m8.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><apply id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m8.1.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p2.8.m8.1.1.2.cmml" xref="S3.SS2.p2.8.m8.1.1.2">𝑥</ci><ci id="S3.SS2.p2.8.m8.1.1.3.cmml" xref="S3.SS2.p2.8.m8.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">{x}_{v}</annotation></semantics></math>, the (<math id="S3.SS2.p2.9.m9.1" class="ltx_Math" alttext="T_{v}" display="inline"><semantics id="S3.SS2.p2.9.m9.1a"><msub id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml"><mi id="S3.SS2.p2.9.m9.1.1.2" xref="S3.SS2.p2.9.m9.1.1.2.cmml">T</mi><mi id="S3.SS2.p2.9.m9.1.1.3" xref="S3.SS2.p2.9.m9.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><apply id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m9.1.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p2.9.m9.1.1.2.cmml" xref="S3.SS2.p2.9.m9.1.1.2">𝑇</ci><ci id="S3.SS2.p2.9.m9.1.1.3.cmml" xref="S3.SS2.p2.9.m9.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">T_{v}</annotation></semantics></math>, <math id="S3.SS2.p2.10.m10.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.SS2.p2.10.m10.1a"><mi id="S3.SS2.p2.10.m10.1.1" xref="S3.SS2.p2.10.m10.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m10.1b"><ci id="S3.SS2.p2.10.m10.1.1.cmml" xref="S3.SS2.p2.10.m10.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m10.1c">H</annotation></semantics></math>, <math id="S3.SS2.p2.11.m11.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS2.p2.11.m11.1a"><mi id="S3.SS2.p2.11.m11.1.1" xref="S3.SS2.p2.11.m11.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m11.1b"><ci id="S3.SS2.p2.11.m11.1.1.cmml" xref="S3.SS2.p2.11.m11.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m11.1c">W</annotation></semantics></math>, <math id="S3.SS2.p2.12.m12.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.p2.12.m12.1a"><mi id="S3.SS2.p2.12.m12.1.1" xref="S3.SS2.p2.12.m12.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.12.m12.1b"><ci id="S3.SS2.p2.12.m12.1.1.cmml" xref="S3.SS2.p2.12.m12.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.12.m12.1c">C</annotation></semantics></math>) denote the number of visual frames, height, width, and number of channels. We choose <math id="S3.SS2.p2.13.m13.1" class="ltx_Math" alttext="T_{a}" display="inline"><semantics id="S3.SS2.p2.13.m13.1a"><msub id="S3.SS2.p2.13.m13.1.1" xref="S3.SS2.p2.13.m13.1.1.cmml"><mi id="S3.SS2.p2.13.m13.1.1.2" xref="S3.SS2.p2.13.m13.1.1.2.cmml">T</mi><mi id="S3.SS2.p2.13.m13.1.1.3" xref="S3.SS2.p2.13.m13.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.13.m13.1b"><apply id="S3.SS2.p2.13.m13.1.1.cmml" xref="S3.SS2.p2.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.13.m13.1.1.1.cmml" xref="S3.SS2.p2.13.m13.1.1">subscript</csymbol><ci id="S3.SS2.p2.13.m13.1.1.2.cmml" xref="S3.SS2.p2.13.m13.1.1.2">𝑇</ci><ci id="S3.SS2.p2.13.m13.1.1.3.cmml" xref="S3.SS2.p2.13.m13.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.13.m13.1c">T_{a}</annotation></semantics></math> and <math id="S3.SS2.p2.14.m14.1" class="ltx_Math" alttext="T_{v}" display="inline"><semantics id="S3.SS2.p2.14.m14.1a"><msub id="S3.SS2.p2.14.m14.1.1" xref="S3.SS2.p2.14.m14.1.1.cmml"><mi id="S3.SS2.p2.14.m14.1.1.2" xref="S3.SS2.p2.14.m14.1.1.2.cmml">T</mi><mi id="S3.SS2.p2.14.m14.1.1.3" xref="S3.SS2.p2.14.m14.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.14.m14.1b"><apply id="S3.SS2.p2.14.m14.1.1.cmml" xref="S3.SS2.p2.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.14.m14.1.1.1.cmml" xref="S3.SS2.p2.14.m14.1.1">subscript</csymbol><ci id="S3.SS2.p2.14.m14.1.1.2.cmml" xref="S3.SS2.p2.14.m14.1.1.2">𝑇</ci><ci id="S3.SS2.p2.14.m14.1.1.3.cmml" xref="S3.SS2.p2.14.m14.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.14.m14.1c">T_{v}</annotation></semantics></math> such that <math id="S3.SS2.p2.15.m15.1" class="ltx_Math" alttext="T_{a}\cdot n_{a}=T_{v}\cdot n_{v}=T" display="inline"><semantics id="S3.SS2.p2.15.m15.1a"><mrow id="S3.SS2.p2.15.m15.1.1" xref="S3.SS2.p2.15.m15.1.1.cmml"><mrow id="S3.SS2.p2.15.m15.1.1.2" xref="S3.SS2.p2.15.m15.1.1.2.cmml"><msub id="S3.SS2.p2.15.m15.1.1.2.2" xref="S3.SS2.p2.15.m15.1.1.2.2.cmml"><mi id="S3.SS2.p2.15.m15.1.1.2.2.2" xref="S3.SS2.p2.15.m15.1.1.2.2.2.cmml">T</mi><mi id="S3.SS2.p2.15.m15.1.1.2.2.3" xref="S3.SS2.p2.15.m15.1.1.2.2.3.cmml">a</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.15.m15.1.1.2.1" xref="S3.SS2.p2.15.m15.1.1.2.1.cmml">⋅</mo><msub id="S3.SS2.p2.15.m15.1.1.2.3" xref="S3.SS2.p2.15.m15.1.1.2.3.cmml"><mi id="S3.SS2.p2.15.m15.1.1.2.3.2" xref="S3.SS2.p2.15.m15.1.1.2.3.2.cmml">n</mi><mi id="S3.SS2.p2.15.m15.1.1.2.3.3" xref="S3.SS2.p2.15.m15.1.1.2.3.3.cmml">a</mi></msub></mrow><mo id="S3.SS2.p2.15.m15.1.1.3" xref="S3.SS2.p2.15.m15.1.1.3.cmml">=</mo><mrow id="S3.SS2.p2.15.m15.1.1.4" xref="S3.SS2.p2.15.m15.1.1.4.cmml"><msub id="S3.SS2.p2.15.m15.1.1.4.2" xref="S3.SS2.p2.15.m15.1.1.4.2.cmml"><mi id="S3.SS2.p2.15.m15.1.1.4.2.2" xref="S3.SS2.p2.15.m15.1.1.4.2.2.cmml">T</mi><mi id="S3.SS2.p2.15.m15.1.1.4.2.3" xref="S3.SS2.p2.15.m15.1.1.4.2.3.cmml">v</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.15.m15.1.1.4.1" xref="S3.SS2.p2.15.m15.1.1.4.1.cmml">⋅</mo><msub id="S3.SS2.p2.15.m15.1.1.4.3" xref="S3.SS2.p2.15.m15.1.1.4.3.cmml"><mi id="S3.SS2.p2.15.m15.1.1.4.3.2" xref="S3.SS2.p2.15.m15.1.1.4.3.2.cmml">n</mi><mi id="S3.SS2.p2.15.m15.1.1.4.3.3" xref="S3.SS2.p2.15.m15.1.1.4.3.3.cmml">v</mi></msub></mrow><mo id="S3.SS2.p2.15.m15.1.1.5" xref="S3.SS2.p2.15.m15.1.1.5.cmml">=</mo><mi id="S3.SS2.p2.15.m15.1.1.6" xref="S3.SS2.p2.15.m15.1.1.6.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.15.m15.1b"><apply id="S3.SS2.p2.15.m15.1.1.cmml" xref="S3.SS2.p2.15.m15.1.1"><and id="S3.SS2.p2.15.m15.1.1a.cmml" xref="S3.SS2.p2.15.m15.1.1"></and><apply id="S3.SS2.p2.15.m15.1.1b.cmml" xref="S3.SS2.p2.15.m15.1.1"><eq id="S3.SS2.p2.15.m15.1.1.3.cmml" xref="S3.SS2.p2.15.m15.1.1.3"></eq><apply id="S3.SS2.p2.15.m15.1.1.2.cmml" xref="S3.SS2.p2.15.m15.1.1.2"><ci id="S3.SS2.p2.15.m15.1.1.2.1.cmml" xref="S3.SS2.p2.15.m15.1.1.2.1">⋅</ci><apply id="S3.SS2.p2.15.m15.1.1.2.2.cmml" xref="S3.SS2.p2.15.m15.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.15.m15.1.1.2.2.1.cmml" xref="S3.SS2.p2.15.m15.1.1.2.2">subscript</csymbol><ci id="S3.SS2.p2.15.m15.1.1.2.2.2.cmml" xref="S3.SS2.p2.15.m15.1.1.2.2.2">𝑇</ci><ci id="S3.SS2.p2.15.m15.1.1.2.2.3.cmml" xref="S3.SS2.p2.15.m15.1.1.2.2.3">𝑎</ci></apply><apply id="S3.SS2.p2.15.m15.1.1.2.3.cmml" xref="S3.SS2.p2.15.m15.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p2.15.m15.1.1.2.3.1.cmml" xref="S3.SS2.p2.15.m15.1.1.2.3">subscript</csymbol><ci id="S3.SS2.p2.15.m15.1.1.2.3.2.cmml" xref="S3.SS2.p2.15.m15.1.1.2.3.2">𝑛</ci><ci id="S3.SS2.p2.15.m15.1.1.2.3.3.cmml" xref="S3.SS2.p2.15.m15.1.1.2.3.3">𝑎</ci></apply></apply><apply id="S3.SS2.p2.15.m15.1.1.4.cmml" xref="S3.SS2.p2.15.m15.1.1.4"><ci id="S3.SS2.p2.15.m15.1.1.4.1.cmml" xref="S3.SS2.p2.15.m15.1.1.4.1">⋅</ci><apply id="S3.SS2.p2.15.m15.1.1.4.2.cmml" xref="S3.SS2.p2.15.m15.1.1.4.2"><csymbol cd="ambiguous" id="S3.SS2.p2.15.m15.1.1.4.2.1.cmml" xref="S3.SS2.p2.15.m15.1.1.4.2">subscript</csymbol><ci id="S3.SS2.p2.15.m15.1.1.4.2.2.cmml" xref="S3.SS2.p2.15.m15.1.1.4.2.2">𝑇</ci><ci id="S3.SS2.p2.15.m15.1.1.4.2.3.cmml" xref="S3.SS2.p2.15.m15.1.1.4.2.3">𝑣</ci></apply><apply id="S3.SS2.p2.15.m15.1.1.4.3.cmml" xref="S3.SS2.p2.15.m15.1.1.4.3"><csymbol cd="ambiguous" id="S3.SS2.p2.15.m15.1.1.4.3.1.cmml" xref="S3.SS2.p2.15.m15.1.1.4.3">subscript</csymbol><ci id="S3.SS2.p2.15.m15.1.1.4.3.2.cmml" xref="S3.SS2.p2.15.m15.1.1.4.3.2">𝑛</ci><ci id="S3.SS2.p2.15.m15.1.1.4.3.3.cmml" xref="S3.SS2.p2.15.m15.1.1.4.3.3">𝑣</ci></apply></apply></apply><apply id="S3.SS2.p2.15.m15.1.1c.cmml" xref="S3.SS2.p2.15.m15.1.1"><eq id="S3.SS2.p2.15.m15.1.1.5.cmml" xref="S3.SS2.p2.15.m15.1.1.5"></eq><share href="#S3.SS2.p2.15.m15.1.1.4.cmml" id="S3.SS2.p2.15.m15.1.1d.cmml" xref="S3.SS2.p2.15.m15.1.1"></share><ci id="S3.SS2.p2.15.m15.1.1.6.cmml" xref="S3.SS2.p2.15.m15.1.1.6">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.15.m15.1c">T_{a}\cdot n_{a}=T_{v}\cdot n_{v}=T</annotation></semantics></math>, where <math id="S3.SS2.p2.16.m16.1" class="ltx_Math" alttext="n_{a}" display="inline"><semantics id="S3.SS2.p2.16.m16.1a"><msub id="S3.SS2.p2.16.m16.1.1" xref="S3.SS2.p2.16.m16.1.1.cmml"><mi id="S3.SS2.p2.16.m16.1.1.2" xref="S3.SS2.p2.16.m16.1.1.2.cmml">n</mi><mi id="S3.SS2.p2.16.m16.1.1.3" xref="S3.SS2.p2.16.m16.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.16.m16.1b"><apply id="S3.SS2.p2.16.m16.1.1.cmml" xref="S3.SS2.p2.16.m16.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.16.m16.1.1.1.cmml" xref="S3.SS2.p2.16.m16.1.1">subscript</csymbol><ci id="S3.SS2.p2.16.m16.1.1.2.cmml" xref="S3.SS2.p2.16.m16.1.1.2">𝑛</ci><ci id="S3.SS2.p2.16.m16.1.1.3.cmml" xref="S3.SS2.p2.16.m16.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.16.m16.1c">n_{a}</annotation></semantics></math> and <math id="S3.SS2.p2.17.m17.1" class="ltx_Math" alttext="n_{v}" display="inline"><semantics id="S3.SS2.p2.17.m17.1a"><msub id="S3.SS2.p2.17.m17.1.1" xref="S3.SS2.p2.17.m17.1.1.cmml"><mi id="S3.SS2.p2.17.m17.1.1.2" xref="S3.SS2.p2.17.m17.1.1.2.cmml">n</mi><mi id="S3.SS2.p2.17.m17.1.1.3" xref="S3.SS2.p2.17.m17.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.17.m17.1b"><apply id="S3.SS2.p2.17.m17.1.1.cmml" xref="S3.SS2.p2.17.m17.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.17.m17.1.1.1.cmml" xref="S3.SS2.p2.17.m17.1.1">subscript</csymbol><ci id="S3.SS2.p2.17.m17.1.1.2.cmml" xref="S3.SS2.p2.17.m17.1.1.2">𝑛</ci><ci id="S3.SS2.p2.17.m17.1.1.3.cmml" xref="S3.SS2.p2.17.m17.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.17.m17.1c">n_{v}</annotation></semantics></math> are the sampling frequencies of the audio and visual sequences. We tokenize <math id="S3.SS2.p2.18.m18.1" class="ltx_Math" alttext="{x}_{a}" display="inline"><semantics id="S3.SS2.p2.18.m18.1a"><msub id="S3.SS2.p2.18.m18.1.1" xref="S3.SS2.p2.18.m18.1.1.cmml"><mi id="S3.SS2.p2.18.m18.1.1.2" xref="S3.SS2.p2.18.m18.1.1.2.cmml">x</mi><mi id="S3.SS2.p2.18.m18.1.1.3" xref="S3.SS2.p2.18.m18.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.18.m18.1b"><apply id="S3.SS2.p2.18.m18.1.1.cmml" xref="S3.SS2.p2.18.m18.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.18.m18.1.1.1.cmml" xref="S3.SS2.p2.18.m18.1.1">subscript</csymbol><ci id="S3.SS2.p2.18.m18.1.1.2.cmml" xref="S3.SS2.p2.18.m18.1.1.2">𝑥</ci><ci id="S3.SS2.p2.18.m18.1.1.3.cmml" xref="S3.SS2.p2.18.m18.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.18.m18.1c">{x}_{a}</annotation></semantics></math> using <math id="S3.SS2.p2.19.m19.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S3.SS2.p2.19.m19.1a"><mrow id="S3.SS2.p2.19.m19.1.1" xref="S3.SS2.p2.19.m19.1.1.cmml"><mn id="S3.SS2.p2.19.m19.1.1.2" xref="S3.SS2.p2.19.m19.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.19.m19.1.1.1" xref="S3.SS2.p2.19.m19.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.19.m19.1.1.3" xref="S3.SS2.p2.19.m19.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.19.m19.1b"><apply id="S3.SS2.p2.19.m19.1.1.cmml" xref="S3.SS2.p2.19.m19.1.1"><times id="S3.SS2.p2.19.m19.1.1.1.cmml" xref="S3.SS2.p2.19.m19.1.1.1"></times><cn type="integer" id="S3.SS2.p2.19.m19.1.1.2.cmml" xref="S3.SS2.p2.19.m19.1.1.2">16</cn><cn type="integer" id="S3.SS2.p2.19.m19.1.1.3.cmml" xref="S3.SS2.p2.19.m19.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.19.m19.1c">16\times 16</annotation></semantics></math> non-overlapping 2D patches (similar to Audio-MAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>), and <math id="S3.SS2.p2.20.m20.1" class="ltx_Math" alttext="{x}_{v}" display="inline"><semantics id="S3.SS2.p2.20.m20.1a"><msub id="S3.SS2.p2.20.m20.1.1" xref="S3.SS2.p2.20.m20.1.1.cmml"><mi id="S3.SS2.p2.20.m20.1.1.2" xref="S3.SS2.p2.20.m20.1.1.2.cmml">x</mi><mi id="S3.SS2.p2.20.m20.1.1.3" xref="S3.SS2.p2.20.m20.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.20.m20.1b"><apply id="S3.SS2.p2.20.m20.1.1.cmml" xref="S3.SS2.p2.20.m20.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.20.m20.1.1.1.cmml" xref="S3.SS2.p2.20.m20.1.1">subscript</csymbol><ci id="S3.SS2.p2.20.m20.1.1.2.cmml" xref="S3.SS2.p2.20.m20.1.1.2">𝑥</ci><ci id="S3.SS2.p2.20.m20.1.1.3.cmml" xref="S3.SS2.p2.20.m20.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.20.m20.1c">{x}_{v}</annotation></semantics></math> using <math id="S3.SS2.p2.21.m21.1" class="ltx_Math" alttext="2\times 16\times 16" display="inline"><semantics id="S3.SS2.p2.21.m21.1a"><mrow id="S3.SS2.p2.21.m21.1.1" xref="S3.SS2.p2.21.m21.1.1.cmml"><mn id="S3.SS2.p2.21.m21.1.1.2" xref="S3.SS2.p2.21.m21.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.21.m21.1.1.1" xref="S3.SS2.p2.21.m21.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.21.m21.1.1.3" xref="S3.SS2.p2.21.m21.1.1.3.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.21.m21.1.1.1a" xref="S3.SS2.p2.21.m21.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.21.m21.1.1.4" xref="S3.SS2.p2.21.m21.1.1.4.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.21.m21.1b"><apply id="S3.SS2.p2.21.m21.1.1.cmml" xref="S3.SS2.p2.21.m21.1.1"><times id="S3.SS2.p2.21.m21.1.1.1.cmml" xref="S3.SS2.p2.21.m21.1.1.1"></times><cn type="integer" id="S3.SS2.p2.21.m21.1.1.2.cmml" xref="S3.SS2.p2.21.m21.1.1.2">2</cn><cn type="integer" id="S3.SS2.p2.21.m21.1.1.3.cmml" xref="S3.SS2.p2.21.m21.1.1.3">16</cn><cn type="integer" id="S3.SS2.p2.21.m21.1.1.4.cmml" xref="S3.SS2.p2.21.m21.1.1.4">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.21.m21.1c">2\times 16\times 16</annotation></semantics></math> non-overlapping 3D spatio-temporal patches (similar to MARLIN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>). The resulting representations are denoted as <math id="S3.SS2.p2.22.m22.1" class="ltx_Math" alttext="\bm{x_{a}}" display="inline"><semantics id="S3.SS2.p2.22.m22.1a"><msub id="S3.SS2.p2.22.m22.1.1" xref="S3.SS2.p2.22.m22.1.1.cmml"><mi id="S3.SS2.p2.22.m22.1.1.2" xref="S3.SS2.p2.22.m22.1.1.2.cmml">𝒙</mi><mi id="S3.SS2.p2.22.m22.1.1.3" xref="S3.SS2.p2.22.m22.1.1.3.cmml">𝒂</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.22.m22.1b"><apply id="S3.SS2.p2.22.m22.1.1.cmml" xref="S3.SS2.p2.22.m22.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.22.m22.1.1.1.cmml" xref="S3.SS2.p2.22.m22.1.1">subscript</csymbol><ci id="S3.SS2.p2.22.m22.1.1.2.cmml" xref="S3.SS2.p2.22.m22.1.1.2">𝒙</ci><ci id="S3.SS2.p2.22.m22.1.1.3.cmml" xref="S3.SS2.p2.22.m22.1.1.3">𝒂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.22.m22.1c">\bm{x_{a}}</annotation></semantics></math> and <math id="S3.SS2.p2.23.m23.1" class="ltx_Math" alttext="\bm{x_{v}}" display="inline"><semantics id="S3.SS2.p2.23.m23.1a"><msub id="S3.SS2.p2.23.m23.1.1" xref="S3.SS2.p2.23.m23.1.1.cmml"><mi id="S3.SS2.p2.23.m23.1.1.2" xref="S3.SS2.p2.23.m23.1.1.2.cmml">𝒙</mi><mi id="S3.SS2.p2.23.m23.1.1.3" xref="S3.SS2.p2.23.m23.1.1.3.cmml">𝒗</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.23.m23.1b"><apply id="S3.SS2.p2.23.m23.1.1.cmml" xref="S3.SS2.p2.23.m23.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.23.m23.1.1.1.cmml" xref="S3.SS2.p2.23.m23.1.1">subscript</csymbol><ci id="S3.SS2.p2.23.m23.1.1.2.cmml" xref="S3.SS2.p2.23.m23.1.1.2">𝒙</ci><ci id="S3.SS2.p2.23.m23.1.1.3.cmml" xref="S3.SS2.p2.23.m23.1.1.3">𝒗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.23.m23.1c">\bm{x_{v}}</annotation></semantics></math>. Subsequently, we segment each of the tokenized representations into 8 equal temporal slices, <math id="S3.SS2.p2.24.m24.3" class="ltx_Math" alttext="\bm{x_{a}}=\{x_{a,{t_{i}}}\}_{i=1}^{8}" display="inline"><semantics id="S3.SS2.p2.24.m24.3a"><mrow id="S3.SS2.p2.24.m24.3.3" xref="S3.SS2.p2.24.m24.3.3.cmml"><msub id="S3.SS2.p2.24.m24.3.3.3" xref="S3.SS2.p2.24.m24.3.3.3.cmml"><mi id="S3.SS2.p2.24.m24.3.3.3.2" xref="S3.SS2.p2.24.m24.3.3.3.2.cmml">𝒙</mi><mi id="S3.SS2.p2.24.m24.3.3.3.3" xref="S3.SS2.p2.24.m24.3.3.3.3.cmml">𝒂</mi></msub><mo id="S3.SS2.p2.24.m24.3.3.2" xref="S3.SS2.p2.24.m24.3.3.2.cmml">=</mo><msubsup id="S3.SS2.p2.24.m24.3.3.1" xref="S3.SS2.p2.24.m24.3.3.1.cmml"><mrow id="S3.SS2.p2.24.m24.3.3.1.1.1.1" xref="S3.SS2.p2.24.m24.3.3.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p2.24.m24.3.3.1.1.1.1.2" xref="S3.SS2.p2.24.m24.3.3.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p2.24.m24.3.3.1.1.1.1.1" xref="S3.SS2.p2.24.m24.3.3.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.24.m24.3.3.1.1.1.1.1.2" xref="S3.SS2.p2.24.m24.3.3.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.SS2.p2.24.m24.2.2.2.2" xref="S3.SS2.p2.24.m24.2.2.2.3.cmml"><mi id="S3.SS2.p2.24.m24.1.1.1.1" xref="S3.SS2.p2.24.m24.1.1.1.1.cmml">a</mi><mo id="S3.SS2.p2.24.m24.2.2.2.2.2" xref="S3.SS2.p2.24.m24.2.2.2.3.cmml">,</mo><msub id="S3.SS2.p2.24.m24.2.2.2.2.1" xref="S3.SS2.p2.24.m24.2.2.2.2.1.cmml"><mi id="S3.SS2.p2.24.m24.2.2.2.2.1.2" xref="S3.SS2.p2.24.m24.2.2.2.2.1.2.cmml">t</mi><mi id="S3.SS2.p2.24.m24.2.2.2.2.1.3" xref="S3.SS2.p2.24.m24.2.2.2.2.1.3.cmml">i</mi></msub></mrow></msub><mo stretchy="false" id="S3.SS2.p2.24.m24.3.3.1.1.1.1.3" xref="S3.SS2.p2.24.m24.3.3.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p2.24.m24.3.3.1.1.3" xref="S3.SS2.p2.24.m24.3.3.1.1.3.cmml"><mi id="S3.SS2.p2.24.m24.3.3.1.1.3.2" xref="S3.SS2.p2.24.m24.3.3.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p2.24.m24.3.3.1.1.3.1" xref="S3.SS2.p2.24.m24.3.3.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p2.24.m24.3.3.1.1.3.3" xref="S3.SS2.p2.24.m24.3.3.1.1.3.3.cmml">1</mn></mrow><mn id="S3.SS2.p2.24.m24.3.3.1.3" xref="S3.SS2.p2.24.m24.3.3.1.3.cmml">8</mn></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.24.m24.3b"><apply id="S3.SS2.p2.24.m24.3.3.cmml" xref="S3.SS2.p2.24.m24.3.3"><eq id="S3.SS2.p2.24.m24.3.3.2.cmml" xref="S3.SS2.p2.24.m24.3.3.2"></eq><apply id="S3.SS2.p2.24.m24.3.3.3.cmml" xref="S3.SS2.p2.24.m24.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.24.m24.3.3.3.1.cmml" xref="S3.SS2.p2.24.m24.3.3.3">subscript</csymbol><ci id="S3.SS2.p2.24.m24.3.3.3.2.cmml" xref="S3.SS2.p2.24.m24.3.3.3.2">𝒙</ci><ci id="S3.SS2.p2.24.m24.3.3.3.3.cmml" xref="S3.SS2.p2.24.m24.3.3.3.3">𝒂</ci></apply><apply id="S3.SS2.p2.24.m24.3.3.1.cmml" xref="S3.SS2.p2.24.m24.3.3.1"><csymbol cd="ambiguous" id="S3.SS2.p2.24.m24.3.3.1.2.cmml" xref="S3.SS2.p2.24.m24.3.3.1">superscript</csymbol><apply id="S3.SS2.p2.24.m24.3.3.1.1.cmml" xref="S3.SS2.p2.24.m24.3.3.1"><csymbol cd="ambiguous" id="S3.SS2.p2.24.m24.3.3.1.1.2.cmml" xref="S3.SS2.p2.24.m24.3.3.1">subscript</csymbol><set id="S3.SS2.p2.24.m24.3.3.1.1.1.2.cmml" xref="S3.SS2.p2.24.m24.3.3.1.1.1.1"><apply id="S3.SS2.p2.24.m24.3.3.1.1.1.1.1.cmml" xref="S3.SS2.p2.24.m24.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.24.m24.3.3.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.24.m24.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.24.m24.3.3.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.24.m24.3.3.1.1.1.1.1.2">𝑥</ci><list id="S3.SS2.p2.24.m24.2.2.2.3.cmml" xref="S3.SS2.p2.24.m24.2.2.2.2"><ci id="S3.SS2.p2.24.m24.1.1.1.1.cmml" xref="S3.SS2.p2.24.m24.1.1.1.1">𝑎</ci><apply id="S3.SS2.p2.24.m24.2.2.2.2.1.cmml" xref="S3.SS2.p2.24.m24.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.SS2.p2.24.m24.2.2.2.2.1.1.cmml" xref="S3.SS2.p2.24.m24.2.2.2.2.1">subscript</csymbol><ci id="S3.SS2.p2.24.m24.2.2.2.2.1.2.cmml" xref="S3.SS2.p2.24.m24.2.2.2.2.1.2">𝑡</ci><ci id="S3.SS2.p2.24.m24.2.2.2.2.1.3.cmml" xref="S3.SS2.p2.24.m24.2.2.2.2.1.3">𝑖</ci></apply></list></apply></set><apply id="S3.SS2.p2.24.m24.3.3.1.1.3.cmml" xref="S3.SS2.p2.24.m24.3.3.1.1.3"><eq id="S3.SS2.p2.24.m24.3.3.1.1.3.1.cmml" xref="S3.SS2.p2.24.m24.3.3.1.1.3.1"></eq><ci id="S3.SS2.p2.24.m24.3.3.1.1.3.2.cmml" xref="S3.SS2.p2.24.m24.3.3.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS2.p2.24.m24.3.3.1.1.3.3.cmml" xref="S3.SS2.p2.24.m24.3.3.1.1.3.3">1</cn></apply></apply><cn type="integer" id="S3.SS2.p2.24.m24.3.3.1.3.cmml" xref="S3.SS2.p2.24.m24.3.3.1.3">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.24.m24.3c">\bm{x_{a}}=\{x_{a,{t_{i}}}\}_{i=1}^{8}</annotation></semantics></math> and <math id="S3.SS2.p2.25.m25.3" class="ltx_Math" alttext="\bm{x_{v}}=\{x_{v,{t_{i}}}\}_{i=1}^{8}" display="inline"><semantics id="S3.SS2.p2.25.m25.3a"><mrow id="S3.SS2.p2.25.m25.3.3" xref="S3.SS2.p2.25.m25.3.3.cmml"><msub id="S3.SS2.p2.25.m25.3.3.3" xref="S3.SS2.p2.25.m25.3.3.3.cmml"><mi id="S3.SS2.p2.25.m25.3.3.3.2" xref="S3.SS2.p2.25.m25.3.3.3.2.cmml">𝒙</mi><mi id="S3.SS2.p2.25.m25.3.3.3.3" xref="S3.SS2.p2.25.m25.3.3.3.3.cmml">𝒗</mi></msub><mo id="S3.SS2.p2.25.m25.3.3.2" xref="S3.SS2.p2.25.m25.3.3.2.cmml">=</mo><msubsup id="S3.SS2.p2.25.m25.3.3.1" xref="S3.SS2.p2.25.m25.3.3.1.cmml"><mrow id="S3.SS2.p2.25.m25.3.3.1.1.1.1" xref="S3.SS2.p2.25.m25.3.3.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p2.25.m25.3.3.1.1.1.1.2" xref="S3.SS2.p2.25.m25.3.3.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p2.25.m25.3.3.1.1.1.1.1" xref="S3.SS2.p2.25.m25.3.3.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.25.m25.3.3.1.1.1.1.1.2" xref="S3.SS2.p2.25.m25.3.3.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.SS2.p2.25.m25.2.2.2.2" xref="S3.SS2.p2.25.m25.2.2.2.3.cmml"><mi id="S3.SS2.p2.25.m25.1.1.1.1" xref="S3.SS2.p2.25.m25.1.1.1.1.cmml">v</mi><mo id="S3.SS2.p2.25.m25.2.2.2.2.2" xref="S3.SS2.p2.25.m25.2.2.2.3.cmml">,</mo><msub id="S3.SS2.p2.25.m25.2.2.2.2.1" xref="S3.SS2.p2.25.m25.2.2.2.2.1.cmml"><mi id="S3.SS2.p2.25.m25.2.2.2.2.1.2" xref="S3.SS2.p2.25.m25.2.2.2.2.1.2.cmml">t</mi><mi id="S3.SS2.p2.25.m25.2.2.2.2.1.3" xref="S3.SS2.p2.25.m25.2.2.2.2.1.3.cmml">i</mi></msub></mrow></msub><mo stretchy="false" id="S3.SS2.p2.25.m25.3.3.1.1.1.1.3" xref="S3.SS2.p2.25.m25.3.3.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p2.25.m25.3.3.1.1.3" xref="S3.SS2.p2.25.m25.3.3.1.1.3.cmml"><mi id="S3.SS2.p2.25.m25.3.3.1.1.3.2" xref="S3.SS2.p2.25.m25.3.3.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p2.25.m25.3.3.1.1.3.1" xref="S3.SS2.p2.25.m25.3.3.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p2.25.m25.3.3.1.1.3.3" xref="S3.SS2.p2.25.m25.3.3.1.1.3.3.cmml">1</mn></mrow><mn id="S3.SS2.p2.25.m25.3.3.1.3" xref="S3.SS2.p2.25.m25.3.3.1.3.cmml">8</mn></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.25.m25.3b"><apply id="S3.SS2.p2.25.m25.3.3.cmml" xref="S3.SS2.p2.25.m25.3.3"><eq id="S3.SS2.p2.25.m25.3.3.2.cmml" xref="S3.SS2.p2.25.m25.3.3.2"></eq><apply id="S3.SS2.p2.25.m25.3.3.3.cmml" xref="S3.SS2.p2.25.m25.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.25.m25.3.3.3.1.cmml" xref="S3.SS2.p2.25.m25.3.3.3">subscript</csymbol><ci id="S3.SS2.p2.25.m25.3.3.3.2.cmml" xref="S3.SS2.p2.25.m25.3.3.3.2">𝒙</ci><ci id="S3.SS2.p2.25.m25.3.3.3.3.cmml" xref="S3.SS2.p2.25.m25.3.3.3.3">𝒗</ci></apply><apply id="S3.SS2.p2.25.m25.3.3.1.cmml" xref="S3.SS2.p2.25.m25.3.3.1"><csymbol cd="ambiguous" id="S3.SS2.p2.25.m25.3.3.1.2.cmml" xref="S3.SS2.p2.25.m25.3.3.1">superscript</csymbol><apply id="S3.SS2.p2.25.m25.3.3.1.1.cmml" xref="S3.SS2.p2.25.m25.3.3.1"><csymbol cd="ambiguous" id="S3.SS2.p2.25.m25.3.3.1.1.2.cmml" xref="S3.SS2.p2.25.m25.3.3.1">subscript</csymbol><set id="S3.SS2.p2.25.m25.3.3.1.1.1.2.cmml" xref="S3.SS2.p2.25.m25.3.3.1.1.1.1"><apply id="S3.SS2.p2.25.m25.3.3.1.1.1.1.1.cmml" xref="S3.SS2.p2.25.m25.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.25.m25.3.3.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.25.m25.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.25.m25.3.3.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.25.m25.3.3.1.1.1.1.1.2">𝑥</ci><list id="S3.SS2.p2.25.m25.2.2.2.3.cmml" xref="S3.SS2.p2.25.m25.2.2.2.2"><ci id="S3.SS2.p2.25.m25.1.1.1.1.cmml" xref="S3.SS2.p2.25.m25.1.1.1.1">𝑣</ci><apply id="S3.SS2.p2.25.m25.2.2.2.2.1.cmml" xref="S3.SS2.p2.25.m25.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.SS2.p2.25.m25.2.2.2.2.1.1.cmml" xref="S3.SS2.p2.25.m25.2.2.2.2.1">subscript</csymbol><ci id="S3.SS2.p2.25.m25.2.2.2.2.1.2.cmml" xref="S3.SS2.p2.25.m25.2.2.2.2.1.2">𝑡</ci><ci id="S3.SS2.p2.25.m25.2.2.2.2.1.3.cmml" xref="S3.SS2.p2.25.m25.2.2.2.2.1.3">𝑖</ci></apply></list></apply></set><apply id="S3.SS2.p2.25.m25.3.3.1.1.3.cmml" xref="S3.SS2.p2.25.m25.3.3.1.1.3"><eq id="S3.SS2.p2.25.m25.3.3.1.1.3.1.cmml" xref="S3.SS2.p2.25.m25.3.3.1.1.3.1"></eq><ci id="S3.SS2.p2.25.m25.3.3.1.1.3.2.cmml" xref="S3.SS2.p2.25.m25.3.3.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS2.p2.25.m25.3.3.1.1.3.3.cmml" xref="S3.SS2.p2.25.m25.3.3.1.1.3.3">1</cn></apply></apply><cn type="integer" id="S3.SS2.p2.25.m25.3.3.1.3.cmml" xref="S3.SS2.p2.25.m25.3.3.1.3">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.25.m25.3c">\bm{x_{v}}=\{x_{v,{t_{i}}}\}_{i=1}^{8}</annotation></semantics></math>, where the number of slices was decided empirically. This slicing preserves the temporal correspondence of each slice between modalities, as <math id="S3.SS2.p2.26.m26.2" class="ltx_Math" alttext="x_{a,{t_{i}}}" display="inline"><semantics id="S3.SS2.p2.26.m26.2a"><msub id="S3.SS2.p2.26.m26.2.3" xref="S3.SS2.p2.26.m26.2.3.cmml"><mi id="S3.SS2.p2.26.m26.2.3.2" xref="S3.SS2.p2.26.m26.2.3.2.cmml">x</mi><mrow id="S3.SS2.p2.26.m26.2.2.2.2" xref="S3.SS2.p2.26.m26.2.2.2.3.cmml"><mi id="S3.SS2.p2.26.m26.1.1.1.1" xref="S3.SS2.p2.26.m26.1.1.1.1.cmml">a</mi><mo id="S3.SS2.p2.26.m26.2.2.2.2.2" xref="S3.SS2.p2.26.m26.2.2.2.3.cmml">,</mo><msub id="S3.SS2.p2.26.m26.2.2.2.2.1" xref="S3.SS2.p2.26.m26.2.2.2.2.1.cmml"><mi id="S3.SS2.p2.26.m26.2.2.2.2.1.2" xref="S3.SS2.p2.26.m26.2.2.2.2.1.2.cmml">t</mi><mi id="S3.SS2.p2.26.m26.2.2.2.2.1.3" xref="S3.SS2.p2.26.m26.2.2.2.2.1.3.cmml">i</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.26.m26.2b"><apply id="S3.SS2.p2.26.m26.2.3.cmml" xref="S3.SS2.p2.26.m26.2.3"><csymbol cd="ambiguous" id="S3.SS2.p2.26.m26.2.3.1.cmml" xref="S3.SS2.p2.26.m26.2.3">subscript</csymbol><ci id="S3.SS2.p2.26.m26.2.3.2.cmml" xref="S3.SS2.p2.26.m26.2.3.2">𝑥</ci><list id="S3.SS2.p2.26.m26.2.2.2.3.cmml" xref="S3.SS2.p2.26.m26.2.2.2.2"><ci id="S3.SS2.p2.26.m26.1.1.1.1.cmml" xref="S3.SS2.p2.26.m26.1.1.1.1">𝑎</ci><apply id="S3.SS2.p2.26.m26.2.2.2.2.1.cmml" xref="S3.SS2.p2.26.m26.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.SS2.p2.26.m26.2.2.2.2.1.1.cmml" xref="S3.SS2.p2.26.m26.2.2.2.2.1">subscript</csymbol><ci id="S3.SS2.p2.26.m26.2.2.2.2.1.2.cmml" xref="S3.SS2.p2.26.m26.2.2.2.2.1.2">𝑡</ci><ci id="S3.SS2.p2.26.m26.2.2.2.2.1.3.cmml" xref="S3.SS2.p2.26.m26.2.2.2.2.1.3">𝑖</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.26.m26.2c">x_{a,{t_{i}}}</annotation></semantics></math> and <math id="S3.SS2.p2.27.m27.2" class="ltx_Math" alttext="x_{v,{t_{i}}}" display="inline"><semantics id="S3.SS2.p2.27.m27.2a"><msub id="S3.SS2.p2.27.m27.2.3" xref="S3.SS2.p2.27.m27.2.3.cmml"><mi id="S3.SS2.p2.27.m27.2.3.2" xref="S3.SS2.p2.27.m27.2.3.2.cmml">x</mi><mrow id="S3.SS2.p2.27.m27.2.2.2.2" xref="S3.SS2.p2.27.m27.2.2.2.3.cmml"><mi id="S3.SS2.p2.27.m27.1.1.1.1" xref="S3.SS2.p2.27.m27.1.1.1.1.cmml">v</mi><mo id="S3.SS2.p2.27.m27.2.2.2.2.2" xref="S3.SS2.p2.27.m27.2.2.2.3.cmml">,</mo><msub id="S3.SS2.p2.27.m27.2.2.2.2.1" xref="S3.SS2.p2.27.m27.2.2.2.2.1.cmml"><mi id="S3.SS2.p2.27.m27.2.2.2.2.1.2" xref="S3.SS2.p2.27.m27.2.2.2.2.1.2.cmml">t</mi><mi id="S3.SS2.p2.27.m27.2.2.2.2.1.3" xref="S3.SS2.p2.27.m27.2.2.2.2.1.3.cmml">i</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.27.m27.2b"><apply id="S3.SS2.p2.27.m27.2.3.cmml" xref="S3.SS2.p2.27.m27.2.3"><csymbol cd="ambiguous" id="S3.SS2.p2.27.m27.2.3.1.cmml" xref="S3.SS2.p2.27.m27.2.3">subscript</csymbol><ci id="S3.SS2.p2.27.m27.2.3.2.cmml" xref="S3.SS2.p2.27.m27.2.3.2">𝑥</ci><list id="S3.SS2.p2.27.m27.2.2.2.3.cmml" xref="S3.SS2.p2.27.m27.2.2.2.2"><ci id="S3.SS2.p2.27.m27.1.1.1.1.cmml" xref="S3.SS2.p2.27.m27.1.1.1.1">𝑣</ci><apply id="S3.SS2.p2.27.m27.2.2.2.2.1.cmml" xref="S3.SS2.p2.27.m27.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.SS2.p2.27.m27.2.2.2.2.1.1.cmml" xref="S3.SS2.p2.27.m27.2.2.2.2.1">subscript</csymbol><ci id="S3.SS2.p2.27.m27.2.2.2.2.1.2.cmml" xref="S3.SS2.p2.27.m27.2.2.2.2.1.2">𝑡</ci><ci id="S3.SS2.p2.27.m27.2.2.2.2.1.3.cmml" xref="S3.SS2.p2.27.m27.2.2.2.2.1.3">𝑖</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.27.m27.2c">x_{v,{t_{i}}}</annotation></semantics></math> correspond to the same time interval.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.9" class="ltx_p"><span id="S3.SS2.p3.9.1" class="ltx_text ltx_font_bold">Feature Encoding.</span>
The two uni-modal audio and visual encoders, <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="E_{a}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">E</mi><mi id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝐸</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">E_{a}</annotation></semantics></math> and <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="E_{v}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">E</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">𝐸</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">E_{v}</annotation></semantics></math>, encode the tokenized inputs, <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="\bm{x_{a}}" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><msub id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">𝒙</mi><mi id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">𝒂</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">𝒙</ci><ci id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3">𝒂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">\bm{x_{a}}</annotation></semantics></math> and <math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="\bm{x_{v}}" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><msub id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mi id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">𝒙</mi><mi id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml">𝒗</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">𝒙</ci><ci id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3">𝒗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">\bm{x_{v}}</annotation></semantics></math>, and output uni-modal features <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="\bm{a}" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mi id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">𝒂</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><ci id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">𝒂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">\bm{a}</annotation></semantics></math> and <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="\bm{v}" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mi id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><ci id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">\bm{v}</annotation></semantics></math> respectively: <math id="S3.SS2.p3.7.m7.2" class="ltx_Math" alttext="\bm{p}=\{p_{t_{i}}\}_{i=1}^{8}=E_{p}(\bm{x_{p}}+pos_{p}^{e})" display="inline"><semantics id="S3.SS2.p3.7.m7.2a"><mrow id="S3.SS2.p3.7.m7.2.2" xref="S3.SS2.p3.7.m7.2.2.cmml"><mi id="S3.SS2.p3.7.m7.2.2.4" xref="S3.SS2.p3.7.m7.2.2.4.cmml">𝒑</mi><mo id="S3.SS2.p3.7.m7.2.2.5" xref="S3.SS2.p3.7.m7.2.2.5.cmml">=</mo><msubsup id="S3.SS2.p3.7.m7.1.1.1" xref="S3.SS2.p3.7.m7.1.1.1.cmml"><mrow id="S3.SS2.p3.7.m7.1.1.1.1.1.1" xref="S3.SS2.p3.7.m7.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p3.7.m7.1.1.1.1.1.1.2" xref="S3.SS2.p3.7.m7.1.1.1.1.1.2.cmml">{</mo><msub id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.2" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.2.cmml">p</mi><msub id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3.2" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3.2.cmml">t</mi><mi id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3.3" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></msub><mo stretchy="false" id="S3.SS2.p3.7.m7.1.1.1.1.1.1.3" xref="S3.SS2.p3.7.m7.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS2.p3.7.m7.1.1.1.1.3" xref="S3.SS2.p3.7.m7.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.7.m7.1.1.1.1.3.2" xref="S3.SS2.p3.7.m7.1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p3.7.m7.1.1.1.1.3.1" xref="S3.SS2.p3.7.m7.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p3.7.m7.1.1.1.1.3.3" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3.cmml">1</mn></mrow><mn id="S3.SS2.p3.7.m7.1.1.1.3" xref="S3.SS2.p3.7.m7.1.1.1.3.cmml">8</mn></msubsup><mo id="S3.SS2.p3.7.m7.2.2.6" xref="S3.SS2.p3.7.m7.2.2.6.cmml">=</mo><mrow id="S3.SS2.p3.7.m7.2.2.2" xref="S3.SS2.p3.7.m7.2.2.2.cmml"><msub id="S3.SS2.p3.7.m7.2.2.2.3" xref="S3.SS2.p3.7.m7.2.2.2.3.cmml"><mi id="S3.SS2.p3.7.m7.2.2.2.3.2" xref="S3.SS2.p3.7.m7.2.2.2.3.2.cmml">E</mi><mi id="S3.SS2.p3.7.m7.2.2.2.3.3" xref="S3.SS2.p3.7.m7.2.2.2.3.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p3.7.m7.2.2.2.2" xref="S3.SS2.p3.7.m7.2.2.2.2.cmml">​</mo><mrow id="S3.SS2.p3.7.m7.2.2.2.1.1" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.7.m7.2.2.2.1.1.2" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS2.p3.7.m7.2.2.2.1.1.1" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.cmml"><msub id="S3.SS2.p3.7.m7.2.2.2.1.1.1.2" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.2.cmml"><mi id="S3.SS2.p3.7.m7.2.2.2.1.1.1.2.2" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.2.2.cmml">𝒙</mi><mi id="S3.SS2.p3.7.m7.2.2.2.1.1.1.2.3" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.2.3.cmml">𝒑</mi></msub><mo id="S3.SS2.p3.7.m7.2.2.2.1.1.1.1" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.1.cmml">+</mo><mrow id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.cmml"><mi id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.2" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.1" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.3" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.1a" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.1.cmml">​</mo><msubsup id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.cmml"><mi id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.2.2" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.2.2.cmml">s</mi><mi id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.2.3" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.2.3.cmml">p</mi><mi id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.3" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.3.cmml">e</mi></msubsup></mrow></mrow><mo stretchy="false" id="S3.SS2.p3.7.m7.2.2.2.1.1.3" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.2b"><apply id="S3.SS2.p3.7.m7.2.2.cmml" xref="S3.SS2.p3.7.m7.2.2"><and id="S3.SS2.p3.7.m7.2.2a.cmml" xref="S3.SS2.p3.7.m7.2.2"></and><apply id="S3.SS2.p3.7.m7.2.2b.cmml" xref="S3.SS2.p3.7.m7.2.2"><eq id="S3.SS2.p3.7.m7.2.2.5.cmml" xref="S3.SS2.p3.7.m7.2.2.5"></eq><ci id="S3.SS2.p3.7.m7.2.2.4.cmml" xref="S3.SS2.p3.7.m7.2.2.4">𝒑</ci><apply id="S3.SS2.p3.7.m7.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.1">superscript</csymbol><apply id="S3.SS2.p3.7.m7.1.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.1">subscript</csymbol><set id="S3.SS2.p3.7.m7.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1"><apply id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.2">𝑝</ci><apply id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3.2">𝑡</ci><ci id="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></set><apply id="S3.SS2.p3.7.m7.1.1.1.1.3.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3"><eq id="S3.SS2.p3.7.m7.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.1"></eq><ci id="S3.SS2.p3.7.m7.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS2.p3.7.m7.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.7.m7.1.1.1.1.3.3">1</cn></apply></apply><cn type="integer" id="S3.SS2.p3.7.m7.1.1.1.3.cmml" xref="S3.SS2.p3.7.m7.1.1.1.3">8</cn></apply></apply><apply id="S3.SS2.p3.7.m7.2.2c.cmml" xref="S3.SS2.p3.7.m7.2.2"><eq id="S3.SS2.p3.7.m7.2.2.6.cmml" xref="S3.SS2.p3.7.m7.2.2.6"></eq><share href="#S3.SS2.p3.7.m7.1.1.1.cmml" id="S3.SS2.p3.7.m7.2.2d.cmml" xref="S3.SS2.p3.7.m7.2.2"></share><apply id="S3.SS2.p3.7.m7.2.2.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2"><times id="S3.SS2.p3.7.m7.2.2.2.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2.2"></times><apply id="S3.SS2.p3.7.m7.2.2.2.3.cmml" xref="S3.SS2.p3.7.m7.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.2.2.2.3.1.cmml" xref="S3.SS2.p3.7.m7.2.2.2.3">subscript</csymbol><ci id="S3.SS2.p3.7.m7.2.2.2.3.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2.3.2">𝐸</ci><ci id="S3.SS2.p3.7.m7.2.2.2.3.3.cmml" xref="S3.SS2.p3.7.m7.2.2.2.3.3">𝑝</ci></apply><apply id="S3.SS2.p3.7.m7.2.2.2.1.1.1.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1"><plus id="S3.SS2.p3.7.m7.2.2.2.1.1.1.1.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.1"></plus><apply id="S3.SS2.p3.7.m7.2.2.2.1.1.1.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.2.2.2.1.1.1.2.1.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.2">subscript</csymbol><ci id="S3.SS2.p3.7.m7.2.2.2.1.1.1.2.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.2.2">𝒙</ci><ci id="S3.SS2.p3.7.m7.2.2.2.1.1.1.2.3.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.2.3">𝒑</ci></apply><apply id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3"><times id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.1.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.1"></times><ci id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.2">𝑝</ci><ci id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.3.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.3">𝑜</ci><apply id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.1.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4">superscript</csymbol><apply id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.2.1.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4">subscript</csymbol><ci id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.2.2.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.2.2">𝑠</ci><ci id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.2.3.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.2.3">𝑝</ci></apply><ci id="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.3.cmml" xref="S3.SS2.p3.7.m7.2.2.2.1.1.1.3.4.3">𝑒</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.2c">\bm{p}=\{p_{t_{i}}\}_{i=1}^{8}=E_{p}(\bm{x_{p}}+pos_{p}^{e})</annotation></semantics></math>, where, <math id="S3.SS2.p3.8.m8.2" class="ltx_Math" alttext="p\in\{a,v\}" display="inline"><semantics id="S3.SS2.p3.8.m8.2a"><mrow id="S3.SS2.p3.8.m8.2.3" xref="S3.SS2.p3.8.m8.2.3.cmml"><mi id="S3.SS2.p3.8.m8.2.3.2" xref="S3.SS2.p3.8.m8.2.3.2.cmml">p</mi><mo id="S3.SS2.p3.8.m8.2.3.1" xref="S3.SS2.p3.8.m8.2.3.1.cmml">∈</mo><mrow id="S3.SS2.p3.8.m8.2.3.3.2" xref="S3.SS2.p3.8.m8.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p3.8.m8.2.3.3.2.1" xref="S3.SS2.p3.8.m8.2.3.3.1.cmml">{</mo><mi id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml">a</mi><mo id="S3.SS2.p3.8.m8.2.3.3.2.2" xref="S3.SS2.p3.8.m8.2.3.3.1.cmml">,</mo><mi id="S3.SS2.p3.8.m8.2.2" xref="S3.SS2.p3.8.m8.2.2.cmml">v</mi><mo stretchy="false" id="S3.SS2.p3.8.m8.2.3.3.2.3" xref="S3.SS2.p3.8.m8.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.2b"><apply id="S3.SS2.p3.8.m8.2.3.cmml" xref="S3.SS2.p3.8.m8.2.3"><in id="S3.SS2.p3.8.m8.2.3.1.cmml" xref="S3.SS2.p3.8.m8.2.3.1"></in><ci id="S3.SS2.p3.8.m8.2.3.2.cmml" xref="S3.SS2.p3.8.m8.2.3.2">𝑝</ci><set id="S3.SS2.p3.8.m8.2.3.3.1.cmml" xref="S3.SS2.p3.8.m8.2.3.3.2"><ci id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">𝑎</ci><ci id="S3.SS2.p3.8.m8.2.2.cmml" xref="S3.SS2.p3.8.m8.2.2">𝑣</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.2c">p\in\{a,v\}</annotation></semantics></math> and <math id="S3.SS2.p3.9.m9.1" class="ltx_Math" alttext="pos_{p}^{e}" display="inline"><semantics id="S3.SS2.p3.9.m9.1a"><mrow id="S3.SS2.p3.9.m9.1.1" xref="S3.SS2.p3.9.m9.1.1.cmml"><mi id="S3.SS2.p3.9.m9.1.1.2" xref="S3.SS2.p3.9.m9.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.9.m9.1.1.1" xref="S3.SS2.p3.9.m9.1.1.1.cmml">​</mo><mi id="S3.SS2.p3.9.m9.1.1.3" xref="S3.SS2.p3.9.m9.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.9.m9.1.1.1a" xref="S3.SS2.p3.9.m9.1.1.1.cmml">​</mo><msubsup id="S3.SS2.p3.9.m9.1.1.4" xref="S3.SS2.p3.9.m9.1.1.4.cmml"><mi id="S3.SS2.p3.9.m9.1.1.4.2.2" xref="S3.SS2.p3.9.m9.1.1.4.2.2.cmml">s</mi><mi id="S3.SS2.p3.9.m9.1.1.4.2.3" xref="S3.SS2.p3.9.m9.1.1.4.2.3.cmml">p</mi><mi id="S3.SS2.p3.9.m9.1.1.4.3" xref="S3.SS2.p3.9.m9.1.1.4.3.cmml">e</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m9.1b"><apply id="S3.SS2.p3.9.m9.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1"><times id="S3.SS2.p3.9.m9.1.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1.1"></times><ci id="S3.SS2.p3.9.m9.1.1.2.cmml" xref="S3.SS2.p3.9.m9.1.1.2">𝑝</ci><ci id="S3.SS2.p3.9.m9.1.1.3.cmml" xref="S3.SS2.p3.9.m9.1.1.3">𝑜</ci><apply id="S3.SS2.p3.9.m9.1.1.4.cmml" xref="S3.SS2.p3.9.m9.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.1.1.4.1.cmml" xref="S3.SS2.p3.9.m9.1.1.4">superscript</csymbol><apply id="S3.SS2.p3.9.m9.1.1.4.2.cmml" xref="S3.SS2.p3.9.m9.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.1.1.4.2.1.cmml" xref="S3.SS2.p3.9.m9.1.1.4">subscript</csymbol><ci id="S3.SS2.p3.9.m9.1.1.4.2.2.cmml" xref="S3.SS2.p3.9.m9.1.1.4.2.2">𝑠</ci><ci id="S3.SS2.p3.9.m9.1.1.4.2.3.cmml" xref="S3.SS2.p3.9.m9.1.1.4.2.3">𝑝</ci></apply><ci id="S3.SS2.p3.9.m9.1.1.4.3.cmml" xref="S3.SS2.p3.9.m9.1.1.4.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.1c">pos_{p}^{e}</annotation></semantics></math> is the learnable positional embedding.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.13" class="ltx_p"><span id="S3.SS2.p4.13.1" class="ltx_text ltx_font_bold">Complementary Masking.</span>
Within the uni-modal feature embeddings, <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\bm{a}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">𝒂</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝒂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\bm{a}</annotation></semantics></math> and <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="\bm{v}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\bm{v}</annotation></semantics></math>, we mask <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mrow id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mn id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">50</mn><mo id="S3.SS2.p4.3.m3.1.1.1" xref="S3.SS2.p4.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><csymbol cd="latexml" id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">50\%</annotation></semantics></math> of the temporal slices using binary masks, <math id="S3.SS2.p4.4.m4.4" class="ltx_Math" alttext="(\bm{M}_{a},\bm{M}_{v})\in\{\mathbf{0},\mathbf{1}\}" display="inline"><semantics id="S3.SS2.p4.4.m4.4a"><mrow id="S3.SS2.p4.4.m4.4.4" xref="S3.SS2.p4.4.m4.4.4.cmml"><mrow id="S3.SS2.p4.4.m4.4.4.2.2" xref="S3.SS2.p4.4.m4.4.4.2.3.cmml"><mo stretchy="false" id="S3.SS2.p4.4.m4.4.4.2.2.3" xref="S3.SS2.p4.4.m4.4.4.2.3.cmml">(</mo><msub id="S3.SS2.p4.4.m4.3.3.1.1.1" xref="S3.SS2.p4.4.m4.3.3.1.1.1.cmml"><mi id="S3.SS2.p4.4.m4.3.3.1.1.1.2" xref="S3.SS2.p4.4.m4.3.3.1.1.1.2.cmml">𝑴</mi><mi id="S3.SS2.p4.4.m4.3.3.1.1.1.3" xref="S3.SS2.p4.4.m4.3.3.1.1.1.3.cmml">a</mi></msub><mo id="S3.SS2.p4.4.m4.4.4.2.2.4" xref="S3.SS2.p4.4.m4.4.4.2.3.cmml">,</mo><msub id="S3.SS2.p4.4.m4.4.4.2.2.2" xref="S3.SS2.p4.4.m4.4.4.2.2.2.cmml"><mi id="S3.SS2.p4.4.m4.4.4.2.2.2.2" xref="S3.SS2.p4.4.m4.4.4.2.2.2.2.cmml">𝑴</mi><mi id="S3.SS2.p4.4.m4.4.4.2.2.2.3" xref="S3.SS2.p4.4.m4.4.4.2.2.2.3.cmml">v</mi></msub><mo stretchy="false" id="S3.SS2.p4.4.m4.4.4.2.2.5" xref="S3.SS2.p4.4.m4.4.4.2.3.cmml">)</mo></mrow><mo id="S3.SS2.p4.4.m4.4.4.3" xref="S3.SS2.p4.4.m4.4.4.3.cmml">∈</mo><mrow id="S3.SS2.p4.4.m4.4.4.4.2" xref="S3.SS2.p4.4.m4.4.4.4.1.cmml"><mo stretchy="false" id="S3.SS2.p4.4.m4.4.4.4.2.1" xref="S3.SS2.p4.4.m4.4.4.4.1.cmml">{</mo><mn id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml">𝟎</mn><mo id="S3.SS2.p4.4.m4.4.4.4.2.2" xref="S3.SS2.p4.4.m4.4.4.4.1.cmml">,</mo><mn id="S3.SS2.p4.4.m4.2.2" xref="S3.SS2.p4.4.m4.2.2.cmml">𝟏</mn><mo stretchy="false" id="S3.SS2.p4.4.m4.4.4.4.2.3" xref="S3.SS2.p4.4.m4.4.4.4.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.4b"><apply id="S3.SS2.p4.4.m4.4.4.cmml" xref="S3.SS2.p4.4.m4.4.4"><in id="S3.SS2.p4.4.m4.4.4.3.cmml" xref="S3.SS2.p4.4.m4.4.4.3"></in><interval closure="open" id="S3.SS2.p4.4.m4.4.4.2.3.cmml" xref="S3.SS2.p4.4.m4.4.4.2.2"><apply id="S3.SS2.p4.4.m4.3.3.1.1.1.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.3.3.1.1.1.1.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1">subscript</csymbol><ci id="S3.SS2.p4.4.m4.3.3.1.1.1.2.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.2">𝑴</ci><ci id="S3.SS2.p4.4.m4.3.3.1.1.1.3.cmml" xref="S3.SS2.p4.4.m4.3.3.1.1.1.3">𝑎</ci></apply><apply id="S3.SS2.p4.4.m4.4.4.2.2.2.cmml" xref="S3.SS2.p4.4.m4.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m4.4.4.2.2.2.1.cmml" xref="S3.SS2.p4.4.m4.4.4.2.2.2">subscript</csymbol><ci id="S3.SS2.p4.4.m4.4.4.2.2.2.2.cmml" xref="S3.SS2.p4.4.m4.4.4.2.2.2.2">𝑴</ci><ci id="S3.SS2.p4.4.m4.4.4.2.2.2.3.cmml" xref="S3.SS2.p4.4.m4.4.4.2.2.2.3">𝑣</ci></apply></interval><set id="S3.SS2.p4.4.m4.4.4.4.1.cmml" xref="S3.SS2.p4.4.m4.4.4.4.2"><cn type="integer" id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">0</cn><cn type="integer" id="S3.SS2.p4.4.m4.2.2.cmml" xref="S3.SS2.p4.4.m4.2.2">1</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.4c">(\bm{M}_{a},\bm{M}_{v})\in\{\mathbf{0},\mathbf{1}\}</annotation></semantics></math>, such that <math id="S3.SS2.p4.5.m5.1" class="ltx_Math" alttext="\bm{M}_{a}" display="inline"><semantics id="S3.SS2.p4.5.m5.1a"><msub id="S3.SS2.p4.5.m5.1.1" xref="S3.SS2.p4.5.m5.1.1.cmml"><mi id="S3.SS2.p4.5.m5.1.1.2" xref="S3.SS2.p4.5.m5.1.1.2.cmml">𝑴</mi><mi id="S3.SS2.p4.5.m5.1.1.3" xref="S3.SS2.p4.5.m5.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><apply id="S3.SS2.p4.5.m5.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.5.m5.1.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p4.5.m5.1.1.2.cmml" xref="S3.SS2.p4.5.m5.1.1.2">𝑴</ci><ci id="S3.SS2.p4.5.m5.1.1.3.cmml" xref="S3.SS2.p4.5.m5.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">\bm{M}_{a}</annotation></semantics></math> and <math id="S3.SS2.p4.6.m6.1" class="ltx_Math" alttext="\bm{M}_{v}" display="inline"><semantics id="S3.SS2.p4.6.m6.1a"><msub id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml"><mi id="S3.SS2.p4.6.m6.1.1.2" xref="S3.SS2.p4.6.m6.1.1.2.cmml">𝑴</mi><mi id="S3.SS2.p4.6.m6.1.1.3" xref="S3.SS2.p4.6.m6.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.1b"><apply id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.6.m6.1.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p4.6.m6.1.1.2.cmml" xref="S3.SS2.p4.6.m6.1.1.2">𝑴</ci><ci id="S3.SS2.p4.6.m6.1.1.3.cmml" xref="S3.SS2.p4.6.m6.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.1c">\bm{M}_{v}</annotation></semantics></math> are complementary to each other, <em id="S3.SS2.p4.13.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.p4.13.3" class="ltx_text"></span>, <math id="S3.SS2.p4.7.m7.1" class="ltx_Math" alttext="\bm{M}_{a}=\mathbf{1}" display="inline"><semantics id="S3.SS2.p4.7.m7.1a"><mrow id="S3.SS2.p4.7.m7.1.1" xref="S3.SS2.p4.7.m7.1.1.cmml"><msub id="S3.SS2.p4.7.m7.1.1.2" xref="S3.SS2.p4.7.m7.1.1.2.cmml"><mi id="S3.SS2.p4.7.m7.1.1.2.2" xref="S3.SS2.p4.7.m7.1.1.2.2.cmml">𝑴</mi><mi id="S3.SS2.p4.7.m7.1.1.2.3" xref="S3.SS2.p4.7.m7.1.1.2.3.cmml">a</mi></msub><mo id="S3.SS2.p4.7.m7.1.1.1" xref="S3.SS2.p4.7.m7.1.1.1.cmml">=</mo><mn id="S3.SS2.p4.7.m7.1.1.3" xref="S3.SS2.p4.7.m7.1.1.3.cmml">𝟏</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m7.1b"><apply id="S3.SS2.p4.7.m7.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1"><eq id="S3.SS2.p4.7.m7.1.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1.1"></eq><apply id="S3.SS2.p4.7.m7.1.1.2.cmml" xref="S3.SS2.p4.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p4.7.m7.1.1.2.1.cmml" xref="S3.SS2.p4.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS2.p4.7.m7.1.1.2.2.cmml" xref="S3.SS2.p4.7.m7.1.1.2.2">𝑴</ci><ci id="S3.SS2.p4.7.m7.1.1.2.3.cmml" xref="S3.SS2.p4.7.m7.1.1.2.3">𝑎</ci></apply><cn type="integer" id="S3.SS2.p4.7.m7.1.1.3.cmml" xref="S3.SS2.p4.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m7.1c">\bm{M}_{a}=\mathbf{1}</annotation></semantics></math> for slices where <math id="S3.SS2.p4.8.m8.1" class="ltx_Math" alttext="\bm{M}_{v}=\mathbf{0}" display="inline"><semantics id="S3.SS2.p4.8.m8.1a"><mrow id="S3.SS2.p4.8.m8.1.1" xref="S3.SS2.p4.8.m8.1.1.cmml"><msub id="S3.SS2.p4.8.m8.1.1.2" xref="S3.SS2.p4.8.m8.1.1.2.cmml"><mi id="S3.SS2.p4.8.m8.1.1.2.2" xref="S3.SS2.p4.8.m8.1.1.2.2.cmml">𝑴</mi><mi id="S3.SS2.p4.8.m8.1.1.2.3" xref="S3.SS2.p4.8.m8.1.1.2.3.cmml">v</mi></msub><mo id="S3.SS2.p4.8.m8.1.1.1" xref="S3.SS2.p4.8.m8.1.1.1.cmml">=</mo><mn id="S3.SS2.p4.8.m8.1.1.3" xref="S3.SS2.p4.8.m8.1.1.3.cmml">𝟎</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.8.m8.1b"><apply id="S3.SS2.p4.8.m8.1.1.cmml" xref="S3.SS2.p4.8.m8.1.1"><eq id="S3.SS2.p4.8.m8.1.1.1.cmml" xref="S3.SS2.p4.8.m8.1.1.1"></eq><apply id="S3.SS2.p4.8.m8.1.1.2.cmml" xref="S3.SS2.p4.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p4.8.m8.1.1.2.1.cmml" xref="S3.SS2.p4.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS2.p4.8.m8.1.1.2.2.cmml" xref="S3.SS2.p4.8.m8.1.1.2.2">𝑴</ci><ci id="S3.SS2.p4.8.m8.1.1.2.3.cmml" xref="S3.SS2.p4.8.m8.1.1.2.3">𝑣</ci></apply><cn type="integer" id="S3.SS2.p4.8.m8.1.1.3.cmml" xref="S3.SS2.p4.8.m8.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.8.m8.1c">\bm{M}_{v}=\mathbf{0}</annotation></semantics></math> and vice-versa.
In other words, for every masked slice in the audio feature, the corresponding visual slice is visible and vice versa. Let us denote the visible temporal slices as <math id="S3.SS2.p4.9.m9.1" class="ltx_Math" alttext="\bm{p_{vis}}=\bm{M}_{p}\odot\bm{p}" display="inline"><semantics id="S3.SS2.p4.9.m9.1a"><mrow id="S3.SS2.p4.9.m9.1.1" xref="S3.SS2.p4.9.m9.1.1.cmml"><msub id="S3.SS2.p4.9.m9.1.1.2" xref="S3.SS2.p4.9.m9.1.1.2.cmml"><mi id="S3.SS2.p4.9.m9.1.1.2.2" xref="S3.SS2.p4.9.m9.1.1.2.2.cmml">𝒑</mi><mrow id="S3.SS2.p4.9.m9.1.1.2.3" xref="S3.SS2.p4.9.m9.1.1.2.3.cmml"><mi id="S3.SS2.p4.9.m9.1.1.2.3.2" xref="S3.SS2.p4.9.m9.1.1.2.3.2.cmml">𝒗</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.9.m9.1.1.2.3.1" xref="S3.SS2.p4.9.m9.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p4.9.m9.1.1.2.3.3" xref="S3.SS2.p4.9.m9.1.1.2.3.3.cmml">𝒊</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.9.m9.1.1.2.3.1a" xref="S3.SS2.p4.9.m9.1.1.2.3.1.cmml">​</mo><mi id="S3.SS2.p4.9.m9.1.1.2.3.4" xref="S3.SS2.p4.9.m9.1.1.2.3.4.cmml">𝒔</mi></mrow></msub><mo id="S3.SS2.p4.9.m9.1.1.1" xref="S3.SS2.p4.9.m9.1.1.1.cmml">=</mo><mrow id="S3.SS2.p4.9.m9.1.1.3" xref="S3.SS2.p4.9.m9.1.1.3.cmml"><msub id="S3.SS2.p4.9.m9.1.1.3.2" xref="S3.SS2.p4.9.m9.1.1.3.2.cmml"><mi id="S3.SS2.p4.9.m9.1.1.3.2.2" xref="S3.SS2.p4.9.m9.1.1.3.2.2.cmml">𝑴</mi><mi id="S3.SS2.p4.9.m9.1.1.3.2.3" xref="S3.SS2.p4.9.m9.1.1.3.2.3.cmml">p</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p4.9.m9.1.1.3.1" xref="S3.SS2.p4.9.m9.1.1.3.1.cmml">⊙</mo><mi id="S3.SS2.p4.9.m9.1.1.3.3" xref="S3.SS2.p4.9.m9.1.1.3.3.cmml">𝒑</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.9.m9.1b"><apply id="S3.SS2.p4.9.m9.1.1.cmml" xref="S3.SS2.p4.9.m9.1.1"><eq id="S3.SS2.p4.9.m9.1.1.1.cmml" xref="S3.SS2.p4.9.m9.1.1.1"></eq><apply id="S3.SS2.p4.9.m9.1.1.2.cmml" xref="S3.SS2.p4.9.m9.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p4.9.m9.1.1.2.1.cmml" xref="S3.SS2.p4.9.m9.1.1.2">subscript</csymbol><ci id="S3.SS2.p4.9.m9.1.1.2.2.cmml" xref="S3.SS2.p4.9.m9.1.1.2.2">𝒑</ci><apply id="S3.SS2.p4.9.m9.1.1.2.3.cmml" xref="S3.SS2.p4.9.m9.1.1.2.3"><times id="S3.SS2.p4.9.m9.1.1.2.3.1.cmml" xref="S3.SS2.p4.9.m9.1.1.2.3.1"></times><ci id="S3.SS2.p4.9.m9.1.1.2.3.2.cmml" xref="S3.SS2.p4.9.m9.1.1.2.3.2">𝒗</ci><ci id="S3.SS2.p4.9.m9.1.1.2.3.3.cmml" xref="S3.SS2.p4.9.m9.1.1.2.3.3">𝒊</ci><ci id="S3.SS2.p4.9.m9.1.1.2.3.4.cmml" xref="S3.SS2.p4.9.m9.1.1.2.3.4">𝒔</ci></apply></apply><apply id="S3.SS2.p4.9.m9.1.1.3.cmml" xref="S3.SS2.p4.9.m9.1.1.3"><csymbol cd="latexml" id="S3.SS2.p4.9.m9.1.1.3.1.cmml" xref="S3.SS2.p4.9.m9.1.1.3.1">direct-product</csymbol><apply id="S3.SS2.p4.9.m9.1.1.3.2.cmml" xref="S3.SS2.p4.9.m9.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p4.9.m9.1.1.3.2.1.cmml" xref="S3.SS2.p4.9.m9.1.1.3.2">subscript</csymbol><ci id="S3.SS2.p4.9.m9.1.1.3.2.2.cmml" xref="S3.SS2.p4.9.m9.1.1.3.2.2">𝑴</ci><ci id="S3.SS2.p4.9.m9.1.1.3.2.3.cmml" xref="S3.SS2.p4.9.m9.1.1.3.2.3">𝑝</ci></apply><ci id="S3.SS2.p4.9.m9.1.1.3.3.cmml" xref="S3.SS2.p4.9.m9.1.1.3.3">𝒑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.9.m9.1c">\bm{p_{vis}}=\bm{M}_{p}\odot\bm{p}</annotation></semantics></math> and the masked temporal slices as <math id="S3.SS2.p4.10.m10.1" class="ltx_Math" alttext="\bm{p_{msk}}=(\neg\bm{M}_{p})\odot\bm{p}" display="inline"><semantics id="S3.SS2.p4.10.m10.1a"><mrow id="S3.SS2.p4.10.m10.1.1" xref="S3.SS2.p4.10.m10.1.1.cmml"><msub id="S3.SS2.p4.10.m10.1.1.3" xref="S3.SS2.p4.10.m10.1.1.3.cmml"><mi id="S3.SS2.p4.10.m10.1.1.3.2" xref="S3.SS2.p4.10.m10.1.1.3.2.cmml">𝒑</mi><mrow id="S3.SS2.p4.10.m10.1.1.3.3" xref="S3.SS2.p4.10.m10.1.1.3.3.cmml"><mi id="S3.SS2.p4.10.m10.1.1.3.3.2" xref="S3.SS2.p4.10.m10.1.1.3.3.2.cmml">𝒎</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.10.m10.1.1.3.3.1" xref="S3.SS2.p4.10.m10.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.p4.10.m10.1.1.3.3.3" xref="S3.SS2.p4.10.m10.1.1.3.3.3.cmml">𝒔</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.10.m10.1.1.3.3.1a" xref="S3.SS2.p4.10.m10.1.1.3.3.1.cmml">​</mo><mi id="S3.SS2.p4.10.m10.1.1.3.3.4" xref="S3.SS2.p4.10.m10.1.1.3.3.4.cmml">𝒌</mi></mrow></msub><mo id="S3.SS2.p4.10.m10.1.1.2" xref="S3.SS2.p4.10.m10.1.1.2.cmml">=</mo><mrow id="S3.SS2.p4.10.m10.1.1.1" xref="S3.SS2.p4.10.m10.1.1.1.cmml"><mrow id="S3.SS2.p4.10.m10.1.1.1.1.1" xref="S3.SS2.p4.10.m10.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p4.10.m10.1.1.1.1.1.2" xref="S3.SS2.p4.10.m10.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p4.10.m10.1.1.1.1.1.1" xref="S3.SS2.p4.10.m10.1.1.1.1.1.1.cmml"><mo rspace="0.167em" id="S3.SS2.p4.10.m10.1.1.1.1.1.1.1" xref="S3.SS2.p4.10.m10.1.1.1.1.1.1.1.cmml">¬</mo><msub id="S3.SS2.p4.10.m10.1.1.1.1.1.1.2" xref="S3.SS2.p4.10.m10.1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p4.10.m10.1.1.1.1.1.1.2.2" xref="S3.SS2.p4.10.m10.1.1.1.1.1.1.2.2.cmml">𝑴</mi><mi id="S3.SS2.p4.10.m10.1.1.1.1.1.1.2.3" xref="S3.SS2.p4.10.m10.1.1.1.1.1.1.2.3.cmml">p</mi></msub></mrow><mo rspace="0.055em" stretchy="false" id="S3.SS2.p4.10.m10.1.1.1.1.1.3" xref="S3.SS2.p4.10.m10.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.SS2.p4.10.m10.1.1.1.2" xref="S3.SS2.p4.10.m10.1.1.1.2.cmml">⊙</mo><mi id="S3.SS2.p4.10.m10.1.1.1.3" xref="S3.SS2.p4.10.m10.1.1.1.3.cmml">𝒑</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.10.m10.1b"><apply id="S3.SS2.p4.10.m10.1.1.cmml" xref="S3.SS2.p4.10.m10.1.1"><eq id="S3.SS2.p4.10.m10.1.1.2.cmml" xref="S3.SS2.p4.10.m10.1.1.2"></eq><apply id="S3.SS2.p4.10.m10.1.1.3.cmml" xref="S3.SS2.p4.10.m10.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p4.10.m10.1.1.3.1.cmml" xref="S3.SS2.p4.10.m10.1.1.3">subscript</csymbol><ci id="S3.SS2.p4.10.m10.1.1.3.2.cmml" xref="S3.SS2.p4.10.m10.1.1.3.2">𝒑</ci><apply id="S3.SS2.p4.10.m10.1.1.3.3.cmml" xref="S3.SS2.p4.10.m10.1.1.3.3"><times id="S3.SS2.p4.10.m10.1.1.3.3.1.cmml" xref="S3.SS2.p4.10.m10.1.1.3.3.1"></times><ci id="S3.SS2.p4.10.m10.1.1.3.3.2.cmml" xref="S3.SS2.p4.10.m10.1.1.3.3.2">𝒎</ci><ci id="S3.SS2.p4.10.m10.1.1.3.3.3.cmml" xref="S3.SS2.p4.10.m10.1.1.3.3.3">𝒔</ci><ci id="S3.SS2.p4.10.m10.1.1.3.3.4.cmml" xref="S3.SS2.p4.10.m10.1.1.3.3.4">𝒌</ci></apply></apply><apply id="S3.SS2.p4.10.m10.1.1.1.cmml" xref="S3.SS2.p4.10.m10.1.1.1"><csymbol cd="latexml" id="S3.SS2.p4.10.m10.1.1.1.2.cmml" xref="S3.SS2.p4.10.m10.1.1.1.2">direct-product</csymbol><apply id="S3.SS2.p4.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.10.m10.1.1.1.1.1"><not id="S3.SS2.p4.10.m10.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.10.m10.1.1.1.1.1.1.1"></not><apply id="S3.SS2.p4.10.m10.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p4.10.m10.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p4.10.m10.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p4.10.m10.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.p4.10.m10.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p4.10.m10.1.1.1.1.1.1.2.2">𝑴</ci><ci id="S3.SS2.p4.10.m10.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p4.10.m10.1.1.1.1.1.1.2.3">𝑝</ci></apply></apply><ci id="S3.SS2.p4.10.m10.1.1.1.3.cmml" xref="S3.SS2.p4.10.m10.1.1.1.3">𝒑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.10.m10.1c">\bm{p_{msk}}=(\neg\bm{M}_{p})\odot\bm{p}</annotation></semantics></math>, where <math id="S3.SS2.p4.11.m11.2" class="ltx_Math" alttext="p\in\{a,v\}" display="inline"><semantics id="S3.SS2.p4.11.m11.2a"><mrow id="S3.SS2.p4.11.m11.2.3" xref="S3.SS2.p4.11.m11.2.3.cmml"><mi id="S3.SS2.p4.11.m11.2.3.2" xref="S3.SS2.p4.11.m11.2.3.2.cmml">p</mi><mo id="S3.SS2.p4.11.m11.2.3.1" xref="S3.SS2.p4.11.m11.2.3.1.cmml">∈</mo><mrow id="S3.SS2.p4.11.m11.2.3.3.2" xref="S3.SS2.p4.11.m11.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p4.11.m11.2.3.3.2.1" xref="S3.SS2.p4.11.m11.2.3.3.1.cmml">{</mo><mi id="S3.SS2.p4.11.m11.1.1" xref="S3.SS2.p4.11.m11.1.1.cmml">a</mi><mo id="S3.SS2.p4.11.m11.2.3.3.2.2" xref="S3.SS2.p4.11.m11.2.3.3.1.cmml">,</mo><mi id="S3.SS2.p4.11.m11.2.2" xref="S3.SS2.p4.11.m11.2.2.cmml">v</mi><mo stretchy="false" id="S3.SS2.p4.11.m11.2.3.3.2.3" xref="S3.SS2.p4.11.m11.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.11.m11.2b"><apply id="S3.SS2.p4.11.m11.2.3.cmml" xref="S3.SS2.p4.11.m11.2.3"><in id="S3.SS2.p4.11.m11.2.3.1.cmml" xref="S3.SS2.p4.11.m11.2.3.1"></in><ci id="S3.SS2.p4.11.m11.2.3.2.cmml" xref="S3.SS2.p4.11.m11.2.3.2">𝑝</ci><set id="S3.SS2.p4.11.m11.2.3.3.1.cmml" xref="S3.SS2.p4.11.m11.2.3.3.2"><ci id="S3.SS2.p4.11.m11.1.1.cmml" xref="S3.SS2.p4.11.m11.1.1">𝑎</ci><ci id="S3.SS2.p4.11.m11.2.2.cmml" xref="S3.SS2.p4.11.m11.2.2">𝑣</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.11.m11.2c">p\in\{a,v\}</annotation></semantics></math>, <math id="S3.SS2.p4.12.m12.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S3.SS2.p4.12.m12.1a"><mo id="S3.SS2.p4.12.m12.1.1" xref="S3.SS2.p4.12.m12.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.12.m12.1b"><csymbol cd="latexml" id="S3.SS2.p4.12.m12.1.1.cmml" xref="S3.SS2.p4.12.m12.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.12.m12.1c">\odot</annotation></semantics></math> denotes the Hadamard product and <math id="S3.SS2.p4.13.m13.1" class="ltx_Math" alttext="\neg" display="inline"><semantics id="S3.SS2.p4.13.m13.1a"><mo id="S3.SS2.p4.13.m13.1.1" xref="S3.SS2.p4.13.m13.1.1.cmml">¬</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.13.m13.1b"><not id="S3.SS2.p4.13.m13.1.1.cmml" xref="S3.SS2.p4.13.m13.1.1"></not></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.13.m13.1c">\neg</annotation></semantics></math> is the NOT operator.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.13" class="ltx_p"><span id="S3.SS2.p5.13.1" class="ltx_text ltx_font_bold">Cross-Modal Fusion.</span>
Next, the <span id="S3.SS2.p5.13.2" class="ltx_text ltx_font_italic">visible</span> temporal slices <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="\bm{a_{vis}}" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><msub id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">𝒂</mi><mrow id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.3.2" xref="S3.SS2.p5.1.m1.1.1.3.2.cmml">𝒗</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.3.1" xref="S3.SS2.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.1.m1.1.1.3.3" xref="S3.SS2.p5.1.m1.1.1.3.3.cmml">𝒊</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.1.m1.1.1.3.1a" xref="S3.SS2.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.1.m1.1.1.3.4" xref="S3.SS2.p5.1.m1.1.1.3.4.cmml">𝒔</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">𝒂</ci><apply id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3"><times id="S3.SS2.p5.1.m1.1.1.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.3.1"></times><ci id="S3.SS2.p5.1.m1.1.1.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.3.2">𝒗</ci><ci id="S3.SS2.p5.1.m1.1.1.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3.3">𝒊</ci><ci id="S3.SS2.p5.1.m1.1.1.3.4.cmml" xref="S3.SS2.p5.1.m1.1.1.3.4">𝒔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\bm{a_{vis}}</annotation></semantics></math> and <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="\bm{v_{vis}}" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><msub id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">𝒗</mi><mrow id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml"><mi id="S3.SS2.p5.2.m2.1.1.3.2" xref="S3.SS2.p5.2.m2.1.1.3.2.cmml">𝒗</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.2.m2.1.1.3.1" xref="S3.SS2.p5.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.2.m2.1.1.3.3" xref="S3.SS2.p5.2.m2.1.1.3.3.cmml">𝒊</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.2.m2.1.1.3.1a" xref="S3.SS2.p5.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.2.m2.1.1.3.4" xref="S3.SS2.p5.2.m2.1.1.3.4.cmml">𝒔</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">𝒗</ci><apply id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3"><times id="S3.SS2.p5.2.m2.1.1.3.1.cmml" xref="S3.SS2.p5.2.m2.1.1.3.1"></times><ci id="S3.SS2.p5.2.m2.1.1.3.2.cmml" xref="S3.SS2.p5.2.m2.1.1.3.2">𝒗</ci><ci id="S3.SS2.p5.2.m2.1.1.3.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3.3">𝒊</ci><ci id="S3.SS2.p5.2.m2.1.1.3.4.cmml" xref="S3.SS2.p5.2.m2.1.1.3.4">𝒔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">\bm{v_{vis}}</annotation></semantics></math> are sent through learnable audio-to-visual (A2V) and visual-to-audio (V2A) networks to create their cross-modal temporal counterparts, <math id="S3.SS2.p5.3.m3.1" class="ltx_Math" alttext="\bm{{v}_{a}}=\mathrm{A2V}(\bm{a_{vis}})" display="inline"><semantics id="S3.SS2.p5.3.m3.1a"><mrow id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml"><msub id="S3.SS2.p5.3.m3.1.1.3" xref="S3.SS2.p5.3.m3.1.1.3.cmml"><mi id="S3.SS2.p5.3.m3.1.1.3.2" xref="S3.SS2.p5.3.m3.1.1.3.2.cmml">𝒗</mi><mi id="S3.SS2.p5.3.m3.1.1.3.3" xref="S3.SS2.p5.3.m3.1.1.3.3.cmml">𝒂</mi></msub><mo id="S3.SS2.p5.3.m3.1.1.2" xref="S3.SS2.p5.3.m3.1.1.2.cmml">=</mo><mrow id="S3.SS2.p5.3.m3.1.1.1" xref="S3.SS2.p5.3.m3.1.1.1.cmml"><mi id="S3.SS2.p5.3.m3.1.1.1.3" xref="S3.SS2.p5.3.m3.1.1.1.3.cmml">A2V</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.3.m3.1.1.1.2" xref="S3.SS2.p5.3.m3.1.1.1.2.cmml">​</mo><mrow id="S3.SS2.p5.3.m3.1.1.1.1.1" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p5.3.m3.1.1.1.1.1.2" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p5.3.m3.1.1.1.1.1.1" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p5.3.m3.1.1.1.1.1.1.2" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.2.cmml">𝒂</mi><mrow id="S3.SS2.p5.3.m3.1.1.1.1.1.1.3" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.2" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.2.cmml">𝒗</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.1" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.3" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.3.cmml">𝒊</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.1a" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.4" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.4.cmml">𝒔</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p5.3.m3.1.1.1.1.1.3" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><apply id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1"><eq id="S3.SS2.p5.3.m3.1.1.2.cmml" xref="S3.SS2.p5.3.m3.1.1.2"></eq><apply id="S3.SS2.p5.3.m3.1.1.3.cmml" xref="S3.SS2.p5.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.1.1.3.1.cmml" xref="S3.SS2.p5.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.3.m3.1.1.3.2.cmml" xref="S3.SS2.p5.3.m3.1.1.3.2">𝒗</ci><ci id="S3.SS2.p5.3.m3.1.1.3.3.cmml" xref="S3.SS2.p5.3.m3.1.1.3.3">𝒂</ci></apply><apply id="S3.SS2.p5.3.m3.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1.1"><times id="S3.SS2.p5.3.m3.1.1.1.2.cmml" xref="S3.SS2.p5.3.m3.1.1.1.2"></times><ci id="S3.SS2.p5.3.m3.1.1.1.3.cmml" xref="S3.SS2.p5.3.m3.1.1.1.3">A2V</ci><apply id="S3.SS2.p5.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p5.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.2">𝒂</ci><apply id="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.3"><times id="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.1"></times><ci id="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.2">𝒗</ci><ci id="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.3">𝒊</ci><ci id="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.4.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.1.1.3.4">𝒔</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">\bm{{v}_{a}}=\mathrm{A2V}(\bm{a_{vis}})</annotation></semantics></math> and <math id="S3.SS2.p5.4.m4.1" class="ltx_Math" alttext="\bm{{a}_{v}}=\mathrm{V2A}(\bm{v_{vis}})" display="inline"><semantics id="S3.SS2.p5.4.m4.1a"><mrow id="S3.SS2.p5.4.m4.1.1" xref="S3.SS2.p5.4.m4.1.1.cmml"><msub id="S3.SS2.p5.4.m4.1.1.3" xref="S3.SS2.p5.4.m4.1.1.3.cmml"><mi id="S3.SS2.p5.4.m4.1.1.3.2" xref="S3.SS2.p5.4.m4.1.1.3.2.cmml">𝒂</mi><mi id="S3.SS2.p5.4.m4.1.1.3.3" xref="S3.SS2.p5.4.m4.1.1.3.3.cmml">𝒗</mi></msub><mo id="S3.SS2.p5.4.m4.1.1.2" xref="S3.SS2.p5.4.m4.1.1.2.cmml">=</mo><mrow id="S3.SS2.p5.4.m4.1.1.1" xref="S3.SS2.p5.4.m4.1.1.1.cmml"><mi id="S3.SS2.p5.4.m4.1.1.1.3" xref="S3.SS2.p5.4.m4.1.1.1.3.cmml">V2A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.4.m4.1.1.1.2" xref="S3.SS2.p5.4.m4.1.1.1.2.cmml">​</mo><mrow id="S3.SS2.p5.4.m4.1.1.1.1.1" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p5.4.m4.1.1.1.1.1.2" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p5.4.m4.1.1.1.1.1.1" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p5.4.m4.1.1.1.1.1.1.2" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.2.cmml">𝒗</mi><mrow id="S3.SS2.p5.4.m4.1.1.1.1.1.1.3" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.2" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.2.cmml">𝒗</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.1" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.3" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.3.cmml">𝒊</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.1a" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.4" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.4.cmml">𝒔</mi></mrow></msub><mo stretchy="false" id="S3.SS2.p5.4.m4.1.1.1.1.1.3" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><apply id="S3.SS2.p5.4.m4.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1"><eq id="S3.SS2.p5.4.m4.1.1.2.cmml" xref="S3.SS2.p5.4.m4.1.1.2"></eq><apply id="S3.SS2.p5.4.m4.1.1.3.cmml" xref="S3.SS2.p5.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m4.1.1.3.1.cmml" xref="S3.SS2.p5.4.m4.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.4.m4.1.1.3.2.cmml" xref="S3.SS2.p5.4.m4.1.1.3.2">𝒂</ci><ci id="S3.SS2.p5.4.m4.1.1.3.3.cmml" xref="S3.SS2.p5.4.m4.1.1.3.3">𝒗</ci></apply><apply id="S3.SS2.p5.4.m4.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1.1"><times id="S3.SS2.p5.4.m4.1.1.1.2.cmml" xref="S3.SS2.p5.4.m4.1.1.1.2"></times><ci id="S3.SS2.p5.4.m4.1.1.1.3.cmml" xref="S3.SS2.p5.4.m4.1.1.1.3">V2A</ci><apply id="S3.SS2.p5.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p5.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.2">𝒗</ci><apply id="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.3"><times id="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.1"></times><ci id="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.2">𝒗</ci><ci id="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.3">𝒊</ci><ci id="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.4.cmml" xref="S3.SS2.p5.4.m4.1.1.1.1.1.1.3.4">𝒔</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">\bm{{a}_{v}}=\mathrm{V2A}(\bm{v_{vis}})</annotation></semantics></math>, respectively. Here, <math id="S3.SS2.p5.5.m5.1" class="ltx_Math" alttext="\bm{{v}_{a}}" display="inline"><semantics id="S3.SS2.p5.5.m5.1a"><msub id="S3.SS2.p5.5.m5.1.1" xref="S3.SS2.p5.5.m5.1.1.cmml"><mi id="S3.SS2.p5.5.m5.1.1.2" xref="S3.SS2.p5.5.m5.1.1.2.cmml">𝒗</mi><mi id="S3.SS2.p5.5.m5.1.1.3" xref="S3.SS2.p5.5.m5.1.1.3.cmml">𝒂</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m5.1b"><apply id="S3.SS2.p5.5.m5.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.1.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p5.5.m5.1.1.2.cmml" xref="S3.SS2.p5.5.m5.1.1.2">𝒗</ci><ci id="S3.SS2.p5.5.m5.1.1.3.cmml" xref="S3.SS2.p5.5.m5.1.1.3">𝒂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m5.1c">\bm{{v}_{a}}</annotation></semantics></math> contains <math id="S3.SS2.p5.6.m6.3" class="ltx_Math" alttext="\{{v}_{t_{i},a}=\mathrm{A2V}(a_{t_{i}}),\;\;\forall t_{i}\;\text{where}\;a_{t_{i}}\in\bm{a_{vis}}\}" display="inline"><semantics id="S3.SS2.p5.6.m6.3a"><mrow id="S3.SS2.p5.6.m6.3.3.1" xref="S3.SS2.p5.6.m6.3.3.2.cmml"><mo stretchy="false" id="S3.SS2.p5.6.m6.3.3.1.2" xref="S3.SS2.p5.6.m6.3.3.2.cmml">{</mo><mrow id="S3.SS2.p5.6.m6.3.3.1.1.2" xref="S3.SS2.p5.6.m6.3.3.1.1.3.cmml"><mrow id="S3.SS2.p5.6.m6.3.3.1.1.1.1" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.cmml"><msub id="S3.SS2.p5.6.m6.3.3.1.1.1.1.3" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.3.cmml"><mi id="S3.SS2.p5.6.m6.3.3.1.1.1.1.3.2" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.3.2.cmml">v</mi><mrow id="S3.SS2.p5.6.m6.2.2.2.2" xref="S3.SS2.p5.6.m6.2.2.2.3.cmml"><msub id="S3.SS2.p5.6.m6.2.2.2.2.1" xref="S3.SS2.p5.6.m6.2.2.2.2.1.cmml"><mi id="S3.SS2.p5.6.m6.2.2.2.2.1.2" xref="S3.SS2.p5.6.m6.2.2.2.2.1.2.cmml">t</mi><mi id="S3.SS2.p5.6.m6.2.2.2.2.1.3" xref="S3.SS2.p5.6.m6.2.2.2.2.1.3.cmml">i</mi></msub><mo id="S3.SS2.p5.6.m6.2.2.2.2.2" xref="S3.SS2.p5.6.m6.2.2.2.3.cmml">,</mo><mi id="S3.SS2.p5.6.m6.1.1.1.1" xref="S3.SS2.p5.6.m6.1.1.1.1.cmml">a</mi></mrow></msub><mo id="S3.SS2.p5.6.m6.3.3.1.1.1.1.2" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.2.cmml">=</mo><mrow id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.cmml"><mi id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.3" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.3.cmml">A2V</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.2" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.2" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.2" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.2.cmml">a</mi><msub id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3.2" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3.2.cmml">t</mi><mi id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3.3" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></msub><mo stretchy="false" id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.3" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.727em" id="S3.SS2.p5.6.m6.3.3.1.1.2.3" xref="S3.SS2.p5.6.m6.3.3.1.1.3a.cmml">,</mo><mrow id="S3.SS2.p5.6.m6.3.3.1.1.2.2" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.cmml"><mrow id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.cmml"><mo rspace="0.167em" id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.1" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.1.cmml">∀</mo><mrow id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.cmml"><msub id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2.cmml"><mi id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2.2" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2.2.cmml">t</mi><mi id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2.3" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.1" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.1.cmml">​</mo><mtext id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.3" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.3a.cmml">where</mtext><mo lspace="0.280em" rspace="0em" id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.1a" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.1.cmml">​</mo><msub id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.cmml"><mi id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.2" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.2.cmml">a</mi><msub id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3.cmml"><mi id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3.2" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3.2.cmml">t</mi><mi id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3.3" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3.3.cmml">i</mi></msub></msub></mrow></mrow><mo id="S3.SS2.p5.6.m6.3.3.1.1.2.2.1" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.1.cmml">∈</mo><msub id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.cmml"><mi id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.2" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.2.cmml">𝒂</mi><mrow id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.cmml"><mi id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.2" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.2.cmml">𝒗</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.1" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.1.cmml">​</mo><mi id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.3" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.3.cmml">𝒊</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.1a" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.1.cmml">​</mo><mi id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.4" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.4.cmml">𝒔</mi></mrow></msub></mrow></mrow><mo stretchy="false" id="S3.SS2.p5.6.m6.3.3.1.3" xref="S3.SS2.p5.6.m6.3.3.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.6.m6.3b"><set id="S3.SS2.p5.6.m6.3.3.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1"><apply id="S3.SS2.p5.6.m6.3.3.1.1.3.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.3.3.1.1.3a.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.3">formulae-sequence</csymbol><apply id="S3.SS2.p5.6.m6.3.3.1.1.1.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1"><eq id="S3.SS2.p5.6.m6.3.3.1.1.1.1.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.2"></eq><apply id="S3.SS2.p5.6.m6.3.3.1.1.1.1.3.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.3.3.1.1.1.1.3.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.6.m6.3.3.1.1.1.1.3.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.3.2">𝑣</ci><list id="S3.SS2.p5.6.m6.2.2.2.3.cmml" xref="S3.SS2.p5.6.m6.2.2.2.2"><apply id="S3.SS2.p5.6.m6.2.2.2.2.1.cmml" xref="S3.SS2.p5.6.m6.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.2.2.2.2.1.1.cmml" xref="S3.SS2.p5.6.m6.2.2.2.2.1">subscript</csymbol><ci id="S3.SS2.p5.6.m6.2.2.2.2.1.2.cmml" xref="S3.SS2.p5.6.m6.2.2.2.2.1.2">𝑡</ci><ci id="S3.SS2.p5.6.m6.2.2.2.2.1.3.cmml" xref="S3.SS2.p5.6.m6.2.2.2.2.1.3">𝑖</ci></apply><ci id="S3.SS2.p5.6.m6.1.1.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1.1.1">𝑎</ci></list></apply><apply id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1"><times id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.2"></times><ci id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.3">A2V</ci><apply id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.2">𝑎</ci><apply id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3.2">𝑡</ci><ci id="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply><apply id="S3.SS2.p5.6.m6.3.3.1.1.2.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2"><in id="S3.SS2.p5.6.m6.3.3.1.1.2.2.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.1"></in><apply id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2"><csymbol cd="latexml" id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.1">for-all</csymbol><apply id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2"><times id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.1"></times><apply id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2.2">𝑡</ci><ci id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2.3.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.2.3">𝑖</ci></apply><ci id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.3a.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.3"><mtext id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.3.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.3">where</mtext></ci><apply id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4">subscript</csymbol><ci id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.2">𝑎</ci><apply id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3">subscript</csymbol><ci id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3.2">𝑡</ci><ci id="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3.3.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.2.2.4.3.3">𝑖</ci></apply></apply></apply></apply><apply id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3">subscript</csymbol><ci id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.2">𝒂</ci><apply id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3"><times id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.1.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.1"></times><ci id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.2.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.2">𝒗</ci><ci id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.3.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.3">𝒊</ci><ci id="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.4.cmml" xref="S3.SS2.p5.6.m6.3.3.1.1.2.2.3.3.4">𝒔</ci></apply></apply></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.6.m6.3c">\{{v}_{t_{i},a}=\mathrm{A2V}(a_{t_{i}}),\;\;\forall t_{i}\;\text{where}\;a_{t_{i}}\in\bm{a_{vis}}\}</annotation></semantics></math>, and similarly <math id="S3.SS2.p5.7.m7.1" class="ltx_Math" alttext="\bm{{a}_{v}}" display="inline"><semantics id="S3.SS2.p5.7.m7.1a"><msub id="S3.SS2.p5.7.m7.1.1" xref="S3.SS2.p5.7.m7.1.1.cmml"><mi id="S3.SS2.p5.7.m7.1.1.2" xref="S3.SS2.p5.7.m7.1.1.2.cmml">𝒂</mi><mi id="S3.SS2.p5.7.m7.1.1.3" xref="S3.SS2.p5.7.m7.1.1.3.cmml">𝒗</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.7.m7.1b"><apply id="S3.SS2.p5.7.m7.1.1.cmml" xref="S3.SS2.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.7.m7.1.1.1.cmml" xref="S3.SS2.p5.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p5.7.m7.1.1.2.cmml" xref="S3.SS2.p5.7.m7.1.1.2">𝒂</ci><ci id="S3.SS2.p5.7.m7.1.1.3.cmml" xref="S3.SS2.p5.7.m7.1.1.3">𝒗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.7.m7.1c">\bm{{a}_{v}}</annotation></semantics></math>. Each of the A2V/V2A networks is composed of a single-layer MLP to match the number of tokens of the other modality followed by a single transformer block.
The audio embedding <math id="S3.SS2.p5.8.m8.1" class="ltx_Math" alttext="\bm{a}^{\prime}" display="inline"><semantics id="S3.SS2.p5.8.m8.1a"><msup id="S3.SS2.p5.8.m8.1.1" xref="S3.SS2.p5.8.m8.1.1.cmml"><mi id="S3.SS2.p5.8.m8.1.1.2" xref="S3.SS2.p5.8.m8.1.1.2.cmml">𝒂</mi><mo id="S3.SS2.p5.8.m8.1.1.3" xref="S3.SS2.p5.8.m8.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.8.m8.1b"><apply id="S3.SS2.p5.8.m8.1.1.cmml" xref="S3.SS2.p5.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.8.m8.1.1.1.cmml" xref="S3.SS2.p5.8.m8.1.1">superscript</csymbol><ci id="S3.SS2.p5.8.m8.1.1.2.cmml" xref="S3.SS2.p5.8.m8.1.1.2">𝒂</ci><ci id="S3.SS2.p5.8.m8.1.1.3.cmml" xref="S3.SS2.p5.8.m8.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.8.m8.1c">\bm{a}^{\prime}</annotation></semantics></math> is then created using cross-modal fusion, wherein, we take the original feature <math id="S3.SS2.p5.9.m9.1" class="ltx_Math" alttext="\bm{a}" display="inline"><semantics id="S3.SS2.p5.9.m9.1a"><mi id="S3.SS2.p5.9.m9.1.1" xref="S3.SS2.p5.9.m9.1.1.cmml">𝒂</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.9.m9.1b"><ci id="S3.SS2.p5.9.m9.1.1.cmml" xref="S3.SS2.p5.9.m9.1.1">𝒂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.9.m9.1c">\bm{a}</annotation></semantics></math> and replace each masked slice with the corresponding slice of the same temporal index in the cross-modal vector <math id="S3.SS2.p5.10.m10.1" class="ltx_Math" alttext="\bm{{a}_{v}}" display="inline"><semantics id="S3.SS2.p5.10.m10.1a"><msub id="S3.SS2.p5.10.m10.1.1" xref="S3.SS2.p5.10.m10.1.1.cmml"><mi id="S3.SS2.p5.10.m10.1.1.2" xref="S3.SS2.p5.10.m10.1.1.2.cmml">𝒂</mi><mi id="S3.SS2.p5.10.m10.1.1.3" xref="S3.SS2.p5.10.m10.1.1.3.cmml">𝒗</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.10.m10.1b"><apply id="S3.SS2.p5.10.m10.1.1.cmml" xref="S3.SS2.p5.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.10.m10.1.1.1.cmml" xref="S3.SS2.p5.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.p5.10.m10.1.1.2.cmml" xref="S3.SS2.p5.10.m10.1.1.2">𝒂</ci><ci id="S3.SS2.p5.10.m10.1.1.3.cmml" xref="S3.SS2.p5.10.m10.1.1.3">𝒗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.10.m10.1c">\bm{{a}_{v}}</annotation></semantics></math> given by the V2A network (see <a href="#S2.F2" title="In 2.1 Multi-Modal Representation Learning ‣ 2 Related Works ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>). The visual embedding <math id="S3.SS2.p5.11.m11.1" class="ltx_Math" alttext="\bm{v}^{\prime}" display="inline"><semantics id="S3.SS2.p5.11.m11.1a"><msup id="S3.SS2.p5.11.m11.1.1" xref="S3.SS2.p5.11.m11.1.1.cmml"><mi id="S3.SS2.p5.11.m11.1.1.2" xref="S3.SS2.p5.11.m11.1.1.2.cmml">𝒗</mi><mo id="S3.SS2.p5.11.m11.1.1.3" xref="S3.SS2.p5.11.m11.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.11.m11.1b"><apply id="S3.SS2.p5.11.m11.1.1.cmml" xref="S3.SS2.p5.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.11.m11.1.1.1.cmml" xref="S3.SS2.p5.11.m11.1.1">superscript</csymbol><ci id="S3.SS2.p5.11.m11.1.1.2.cmml" xref="S3.SS2.p5.11.m11.1.1.2">𝒗</ci><ci id="S3.SS2.p5.11.m11.1.1.3.cmml" xref="S3.SS2.p5.11.m11.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.11.m11.1c">\bm{v}^{\prime}</annotation></semantics></math> is similarly obtained from the original feature <math id="S3.SS2.p5.12.m12.1" class="ltx_Math" alttext="\bm{v}" display="inline"><semantics id="S3.SS2.p5.12.m12.1a"><mi id="S3.SS2.p5.12.m12.1.1" xref="S3.SS2.p5.12.m12.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.12.m12.1b"><ci id="S3.SS2.p5.12.m12.1.1.cmml" xref="S3.SS2.p5.12.m12.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.12.m12.1c">\bm{v}</annotation></semantics></math> and the cross-modal feature <math id="S3.SS2.p5.13.m13.1" class="ltx_Math" alttext="\bm{v}_{a}" display="inline"><semantics id="S3.SS2.p5.13.m13.1a"><msub id="S3.SS2.p5.13.m13.1.1" xref="S3.SS2.p5.13.m13.1.1.cmml"><mi id="S3.SS2.p5.13.m13.1.1.2" xref="S3.SS2.p5.13.m13.1.1.2.cmml">𝒗</mi><mi id="S3.SS2.p5.13.m13.1.1.3" xref="S3.SS2.p5.13.m13.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.13.m13.1b"><apply id="S3.SS2.p5.13.m13.1.1.cmml" xref="S3.SS2.p5.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.13.m13.1.1.1.cmml" xref="S3.SS2.p5.13.m13.1.1">subscript</csymbol><ci id="S3.SS2.p5.13.m13.1.1.2.cmml" xref="S3.SS2.p5.13.m13.1.1.2">𝒗</ci><ci id="S3.SS2.p5.13.m13.1.1.3.cmml" xref="S3.SS2.p5.13.m13.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.13.m13.1c">\bm{v}_{a}</annotation></semantics></math>.
Effectively, this process replaces the masked temporal slices of each modality with cross-modal slices generated from the corresponding temporal slices in the other modality. This is feasible due to our complementary masking strategy, as the masked slices of one modality are the visible slices of the other modality.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.10" class="ltx_p"><span id="S3.SS2.p6.10.1" class="ltx_text ltx_font_bold">Decoding.</span>
The uni-modal audio and visual decoders, <math id="S3.SS2.p6.1.m1.1" class="ltx_Math" alttext="G_{a}" display="inline"><semantics id="S3.SS2.p6.1.m1.1a"><msub id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml"><mi id="S3.SS2.p6.1.m1.1.1.2" xref="S3.SS2.p6.1.m1.1.1.2.cmml">G</mi><mi id="S3.SS2.p6.1.m1.1.1.3" xref="S3.SS2.p6.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><apply id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.1.2">𝐺</ci><ci id="S3.SS2.p6.1.m1.1.1.3.cmml" xref="S3.SS2.p6.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">G_{a}</annotation></semantics></math> and <math id="S3.SS2.p6.2.m2.1" class="ltx_Math" alttext="G_{v}" display="inline"><semantics id="S3.SS2.p6.2.m2.1a"><msub id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml"><mi id="S3.SS2.p6.2.m2.1.1.2" xref="S3.SS2.p6.2.m2.1.1.2.cmml">G</mi><mi id="S3.SS2.p6.2.m2.1.1.3" xref="S3.SS2.p6.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><apply id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.2.m2.1.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p6.2.m2.1.1.2.cmml" xref="S3.SS2.p6.2.m2.1.1.2">𝐺</ci><ci id="S3.SS2.p6.2.m2.1.1.3.cmml" xref="S3.SS2.p6.2.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">G_{v}</annotation></semantics></math>, take <math id="S3.SS2.p6.3.m3.1" class="ltx_Math" alttext="\bm{a^{\prime}}" display="inline"><semantics id="S3.SS2.p6.3.m3.1a"><msup id="S3.SS2.p6.3.m3.1.1" xref="S3.SS2.p6.3.m3.1.1.cmml"><mi id="S3.SS2.p6.3.m3.1.1.2" xref="S3.SS2.p6.3.m3.1.1.2.cmml">𝒂</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS2.p6.3.m3.1.1.3" xref="S3.SS2.p6.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.1b"><apply id="S3.SS2.p6.3.m3.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.3.m3.1.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1">superscript</csymbol><ci id="S3.SS2.p6.3.m3.1.1.2.cmml" xref="S3.SS2.p6.3.m3.1.1.2">𝒂</ci><ci id="S3.SS2.p6.3.m3.1.1.3.cmml" xref="S3.SS2.p6.3.m3.1.1.3">bold-′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.1c">\bm{a^{\prime}}</annotation></semantics></math> and <math id="S3.SS2.p6.4.m4.1" class="ltx_Math" alttext="\bm{v^{\prime}}" display="inline"><semantics id="S3.SS2.p6.4.m4.1a"><msup id="S3.SS2.p6.4.m4.1.1" xref="S3.SS2.p6.4.m4.1.1.cmml"><mi id="S3.SS2.p6.4.m4.1.1.2" xref="S3.SS2.p6.4.m4.1.1.2.cmml">𝒗</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS2.p6.4.m4.1.1.3" xref="S3.SS2.p6.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.4.m4.1b"><apply id="S3.SS2.p6.4.m4.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.4.m4.1.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.p6.4.m4.1.1.2.cmml" xref="S3.SS2.p6.4.m4.1.1.2">𝒗</ci><ci id="S3.SS2.p6.4.m4.1.1.3.cmml" xref="S3.SS2.p6.4.m4.1.1.3">bold-′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.4.m4.1c">\bm{v^{\prime}}</annotation></semantics></math> as input to generate the audio and visual reconstructions, <math id="S3.SS2.p6.5.m5.1" class="ltx_Math" alttext="\bm{\hat{x}_{a}}=G_{a}(\bm{a^{\prime}}+pos^{g}_{a})" display="inline"><semantics id="S3.SS2.p6.5.m5.1a"><mrow id="S3.SS2.p6.5.m5.1.1" xref="S3.SS2.p6.5.m5.1.1.cmml"><msub id="S3.SS2.p6.5.m5.1.1.3" xref="S3.SS2.p6.5.m5.1.1.3.cmml"><mover accent="true" id="S3.SS2.p6.5.m5.1.1.3.2" xref="S3.SS2.p6.5.m5.1.1.3.2.cmml"><mi id="S3.SS2.p6.5.m5.1.1.3.2.2" xref="S3.SS2.p6.5.m5.1.1.3.2.2.cmml">𝒙</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS2.p6.5.m5.1.1.3.2.1" xref="S3.SS2.p6.5.m5.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.SS2.p6.5.m5.1.1.3.3" xref="S3.SS2.p6.5.m5.1.1.3.3.cmml">𝒂</mi></msub><mo id="S3.SS2.p6.5.m5.1.1.2" xref="S3.SS2.p6.5.m5.1.1.2.cmml">=</mo><mrow id="S3.SS2.p6.5.m5.1.1.1" xref="S3.SS2.p6.5.m5.1.1.1.cmml"><msub id="S3.SS2.p6.5.m5.1.1.1.3" xref="S3.SS2.p6.5.m5.1.1.1.3.cmml"><mi id="S3.SS2.p6.5.m5.1.1.1.3.2" xref="S3.SS2.p6.5.m5.1.1.1.3.2.cmml">G</mi><mi id="S3.SS2.p6.5.m5.1.1.1.3.3" xref="S3.SS2.p6.5.m5.1.1.1.3.3.cmml">a</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p6.5.m5.1.1.1.2" xref="S3.SS2.p6.5.m5.1.1.1.2.cmml">​</mo><mrow id="S3.SS2.p6.5.m5.1.1.1.1.1" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p6.5.m5.1.1.1.1.1.2" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p6.5.m5.1.1.1.1.1.1" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.cmml"><msup id="S3.SS2.p6.5.m5.1.1.1.1.1.1.2" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p6.5.m5.1.1.1.1.1.1.2.2" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.2.2.cmml">𝒂</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS2.p6.5.m5.1.1.1.1.1.1.2.3" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.2.3.cmml">′</mo></msup><mo id="S3.SS2.p6.5.m5.1.1.1.1.1.1.1" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.2" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.1" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.3" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.1a" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.1.cmml">​</mo><msubsup id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.cmml"><mi id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.2.2" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.2.2.cmml">s</mi><mi id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.3" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.3.cmml">a</mi><mi id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.2.3" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.2.3.cmml">g</mi></msubsup></mrow></mrow><mo stretchy="false" id="S3.SS2.p6.5.m5.1.1.1.1.1.3" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.5.m5.1b"><apply id="S3.SS2.p6.5.m5.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1"><eq id="S3.SS2.p6.5.m5.1.1.2.cmml" xref="S3.SS2.p6.5.m5.1.1.2"></eq><apply id="S3.SS2.p6.5.m5.1.1.3.cmml" xref="S3.SS2.p6.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p6.5.m5.1.1.3.1.cmml" xref="S3.SS2.p6.5.m5.1.1.3">subscript</csymbol><apply id="S3.SS2.p6.5.m5.1.1.3.2.cmml" xref="S3.SS2.p6.5.m5.1.1.3.2"><ci id="S3.SS2.p6.5.m5.1.1.3.2.1.cmml" xref="S3.SS2.p6.5.m5.1.1.3.2.1">bold-^</ci><ci id="S3.SS2.p6.5.m5.1.1.3.2.2.cmml" xref="S3.SS2.p6.5.m5.1.1.3.2.2">𝒙</ci></apply><ci id="S3.SS2.p6.5.m5.1.1.3.3.cmml" xref="S3.SS2.p6.5.m5.1.1.3.3">𝒂</ci></apply><apply id="S3.SS2.p6.5.m5.1.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1.1"><times id="S3.SS2.p6.5.m5.1.1.1.2.cmml" xref="S3.SS2.p6.5.m5.1.1.1.2"></times><apply id="S3.SS2.p6.5.m5.1.1.1.3.cmml" xref="S3.SS2.p6.5.m5.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p6.5.m5.1.1.1.3.1.cmml" xref="S3.SS2.p6.5.m5.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p6.5.m5.1.1.1.3.2.cmml" xref="S3.SS2.p6.5.m5.1.1.1.3.2">𝐺</ci><ci id="S3.SS2.p6.5.m5.1.1.1.3.3.cmml" xref="S3.SS2.p6.5.m5.1.1.1.3.3">𝑎</ci></apply><apply id="S3.SS2.p6.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1"><plus id="S3.SS2.p6.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.1"></plus><apply id="S3.SS2.p6.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p6.5.m5.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS2.p6.5.m5.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.2.2">𝒂</ci><ci id="S3.SS2.p6.5.m5.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.2.3">bold-′</ci></apply><apply id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3"><times id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.1"></times><ci id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.2">𝑝</ci><ci id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.3">𝑜</ci><apply id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.1.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4">subscript</csymbol><apply id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.2.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.2.1.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4">superscript</csymbol><ci id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.2.2.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.2.2">𝑠</ci><ci id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.2.3.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.2.3">𝑔</ci></apply><ci id="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.3.cmml" xref="S3.SS2.p6.5.m5.1.1.1.1.1.1.3.4.3">𝑎</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.5.m5.1c">\bm{\hat{x}_{a}}=G_{a}(\bm{a^{\prime}}+pos^{g}_{a})</annotation></semantics></math> and <math id="S3.SS2.p6.6.m6.1" class="ltx_Math" alttext="\bm{\hat{x}_{v}}=G_{v}(\bm{v^{\prime}}+pos^{g}_{v})" display="inline"><semantics id="S3.SS2.p6.6.m6.1a"><mrow id="S3.SS2.p6.6.m6.1.1" xref="S3.SS2.p6.6.m6.1.1.cmml"><msub id="S3.SS2.p6.6.m6.1.1.3" xref="S3.SS2.p6.6.m6.1.1.3.cmml"><mover accent="true" id="S3.SS2.p6.6.m6.1.1.3.2" xref="S3.SS2.p6.6.m6.1.1.3.2.cmml"><mi id="S3.SS2.p6.6.m6.1.1.3.2.2" xref="S3.SS2.p6.6.m6.1.1.3.2.2.cmml">𝒙</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS2.p6.6.m6.1.1.3.2.1" xref="S3.SS2.p6.6.m6.1.1.3.2.1.cmml">^</mo></mover><mi id="S3.SS2.p6.6.m6.1.1.3.3" xref="S3.SS2.p6.6.m6.1.1.3.3.cmml">𝒗</mi></msub><mo id="S3.SS2.p6.6.m6.1.1.2" xref="S3.SS2.p6.6.m6.1.1.2.cmml">=</mo><mrow id="S3.SS2.p6.6.m6.1.1.1" xref="S3.SS2.p6.6.m6.1.1.1.cmml"><msub id="S3.SS2.p6.6.m6.1.1.1.3" xref="S3.SS2.p6.6.m6.1.1.1.3.cmml"><mi id="S3.SS2.p6.6.m6.1.1.1.3.2" xref="S3.SS2.p6.6.m6.1.1.1.3.2.cmml">G</mi><mi id="S3.SS2.p6.6.m6.1.1.1.3.3" xref="S3.SS2.p6.6.m6.1.1.1.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p6.6.m6.1.1.1.2" xref="S3.SS2.p6.6.m6.1.1.1.2.cmml">​</mo><mrow id="S3.SS2.p6.6.m6.1.1.1.1.1" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p6.6.m6.1.1.1.1.1.2" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p6.6.m6.1.1.1.1.1.1" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.cmml"><msup id="S3.SS2.p6.6.m6.1.1.1.1.1.1.2" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p6.6.m6.1.1.1.1.1.1.2.2" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.2.2.cmml">𝒗</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS2.p6.6.m6.1.1.1.1.1.1.2.3" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.2.3.cmml">′</mo></msup><mo id="S3.SS2.p6.6.m6.1.1.1.1.1.1.1" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.2" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.1" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.3" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.1a" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.1.cmml">​</mo><msubsup id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.cmml"><mi id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.2.2" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.2.2.cmml">s</mi><mi id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.3" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.3.cmml">v</mi><mi id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.2.3" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.2.3.cmml">g</mi></msubsup></mrow></mrow><mo stretchy="false" id="S3.SS2.p6.6.m6.1.1.1.1.1.3" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.6.m6.1b"><apply id="S3.SS2.p6.6.m6.1.1.cmml" xref="S3.SS2.p6.6.m6.1.1"><eq id="S3.SS2.p6.6.m6.1.1.2.cmml" xref="S3.SS2.p6.6.m6.1.1.2"></eq><apply id="S3.SS2.p6.6.m6.1.1.3.cmml" xref="S3.SS2.p6.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p6.6.m6.1.1.3.1.cmml" xref="S3.SS2.p6.6.m6.1.1.3">subscript</csymbol><apply id="S3.SS2.p6.6.m6.1.1.3.2.cmml" xref="S3.SS2.p6.6.m6.1.1.3.2"><ci id="S3.SS2.p6.6.m6.1.1.3.2.1.cmml" xref="S3.SS2.p6.6.m6.1.1.3.2.1">bold-^</ci><ci id="S3.SS2.p6.6.m6.1.1.3.2.2.cmml" xref="S3.SS2.p6.6.m6.1.1.3.2.2">𝒙</ci></apply><ci id="S3.SS2.p6.6.m6.1.1.3.3.cmml" xref="S3.SS2.p6.6.m6.1.1.3.3">𝒗</ci></apply><apply id="S3.SS2.p6.6.m6.1.1.1.cmml" xref="S3.SS2.p6.6.m6.1.1.1"><times id="S3.SS2.p6.6.m6.1.1.1.2.cmml" xref="S3.SS2.p6.6.m6.1.1.1.2"></times><apply id="S3.SS2.p6.6.m6.1.1.1.3.cmml" xref="S3.SS2.p6.6.m6.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p6.6.m6.1.1.1.3.1.cmml" xref="S3.SS2.p6.6.m6.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p6.6.m6.1.1.1.3.2.cmml" xref="S3.SS2.p6.6.m6.1.1.1.3.2">𝐺</ci><ci id="S3.SS2.p6.6.m6.1.1.1.3.3.cmml" xref="S3.SS2.p6.6.m6.1.1.1.3.3">𝑣</ci></apply><apply id="S3.SS2.p6.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1"><plus id="S3.SS2.p6.6.m6.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.1"></plus><apply id="S3.SS2.p6.6.m6.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p6.6.m6.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS2.p6.6.m6.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.2.2">𝒗</ci><ci id="S3.SS2.p6.6.m6.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.2.3">bold-′</ci></apply><apply id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3"><times id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.1"></times><ci id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.2">𝑝</ci><ci id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.3">𝑜</ci><apply id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.1.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4">subscript</csymbol><apply id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.2.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.2.1.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4">superscript</csymbol><ci id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.2.2.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.2.2">𝑠</ci><ci id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.2.3.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.2.3">𝑔</ci></apply><ci id="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.3.cmml" xref="S3.SS2.p6.6.m6.1.1.1.1.1.1.3.4.3">𝑣</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.6.m6.1c">\bm{\hat{x}_{v}}=G_{v}(\bm{v^{\prime}}+pos^{g}_{v})</annotation></semantics></math>, where <math id="S3.SS2.p6.7.m7.1" class="ltx_Math" alttext="pos^{g}_{a}" display="inline"><semantics id="S3.SS2.p6.7.m7.1a"><mrow id="S3.SS2.p6.7.m7.1.1" xref="S3.SS2.p6.7.m7.1.1.cmml"><mi id="S3.SS2.p6.7.m7.1.1.2" xref="S3.SS2.p6.7.m7.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.7.m7.1.1.1" xref="S3.SS2.p6.7.m7.1.1.1.cmml">​</mo><mi id="S3.SS2.p6.7.m7.1.1.3" xref="S3.SS2.p6.7.m7.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.7.m7.1.1.1a" xref="S3.SS2.p6.7.m7.1.1.1.cmml">​</mo><msubsup id="S3.SS2.p6.7.m7.1.1.4" xref="S3.SS2.p6.7.m7.1.1.4.cmml"><mi id="S3.SS2.p6.7.m7.1.1.4.2.2" xref="S3.SS2.p6.7.m7.1.1.4.2.2.cmml">s</mi><mi id="S3.SS2.p6.7.m7.1.1.4.3" xref="S3.SS2.p6.7.m7.1.1.4.3.cmml">a</mi><mi id="S3.SS2.p6.7.m7.1.1.4.2.3" xref="S3.SS2.p6.7.m7.1.1.4.2.3.cmml">g</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.7.m7.1b"><apply id="S3.SS2.p6.7.m7.1.1.cmml" xref="S3.SS2.p6.7.m7.1.1"><times id="S3.SS2.p6.7.m7.1.1.1.cmml" xref="S3.SS2.p6.7.m7.1.1.1"></times><ci id="S3.SS2.p6.7.m7.1.1.2.cmml" xref="S3.SS2.p6.7.m7.1.1.2">𝑝</ci><ci id="S3.SS2.p6.7.m7.1.1.3.cmml" xref="S3.SS2.p6.7.m7.1.1.3">𝑜</ci><apply id="S3.SS2.p6.7.m7.1.1.4.cmml" xref="S3.SS2.p6.7.m7.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p6.7.m7.1.1.4.1.cmml" xref="S3.SS2.p6.7.m7.1.1.4">subscript</csymbol><apply id="S3.SS2.p6.7.m7.1.1.4.2.cmml" xref="S3.SS2.p6.7.m7.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p6.7.m7.1.1.4.2.1.cmml" xref="S3.SS2.p6.7.m7.1.1.4">superscript</csymbol><ci id="S3.SS2.p6.7.m7.1.1.4.2.2.cmml" xref="S3.SS2.p6.7.m7.1.1.4.2.2">𝑠</ci><ci id="S3.SS2.p6.7.m7.1.1.4.2.3.cmml" xref="S3.SS2.p6.7.m7.1.1.4.2.3">𝑔</ci></apply><ci id="S3.SS2.p6.7.m7.1.1.4.3.cmml" xref="S3.SS2.p6.7.m7.1.1.4.3">𝑎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.7.m7.1c">pos^{g}_{a}</annotation></semantics></math> and <math id="S3.SS2.p6.8.m8.1" class="ltx_Math" alttext="pos^{g}_{v}" display="inline"><semantics id="S3.SS2.p6.8.m8.1a"><mrow id="S3.SS2.p6.8.m8.1.1" xref="S3.SS2.p6.8.m8.1.1.cmml"><mi id="S3.SS2.p6.8.m8.1.1.2" xref="S3.SS2.p6.8.m8.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.8.m8.1.1.1" xref="S3.SS2.p6.8.m8.1.1.1.cmml">​</mo><mi id="S3.SS2.p6.8.m8.1.1.3" xref="S3.SS2.p6.8.m8.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p6.8.m8.1.1.1a" xref="S3.SS2.p6.8.m8.1.1.1.cmml">​</mo><msubsup id="S3.SS2.p6.8.m8.1.1.4" xref="S3.SS2.p6.8.m8.1.1.4.cmml"><mi id="S3.SS2.p6.8.m8.1.1.4.2.2" xref="S3.SS2.p6.8.m8.1.1.4.2.2.cmml">s</mi><mi id="S3.SS2.p6.8.m8.1.1.4.3" xref="S3.SS2.p6.8.m8.1.1.4.3.cmml">v</mi><mi id="S3.SS2.p6.8.m8.1.1.4.2.3" xref="S3.SS2.p6.8.m8.1.1.4.2.3.cmml">g</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.8.m8.1b"><apply id="S3.SS2.p6.8.m8.1.1.cmml" xref="S3.SS2.p6.8.m8.1.1"><times id="S3.SS2.p6.8.m8.1.1.1.cmml" xref="S3.SS2.p6.8.m8.1.1.1"></times><ci id="S3.SS2.p6.8.m8.1.1.2.cmml" xref="S3.SS2.p6.8.m8.1.1.2">𝑝</ci><ci id="S3.SS2.p6.8.m8.1.1.3.cmml" xref="S3.SS2.p6.8.m8.1.1.3">𝑜</ci><apply id="S3.SS2.p6.8.m8.1.1.4.cmml" xref="S3.SS2.p6.8.m8.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p6.8.m8.1.1.4.1.cmml" xref="S3.SS2.p6.8.m8.1.1.4">subscript</csymbol><apply id="S3.SS2.p6.8.m8.1.1.4.2.cmml" xref="S3.SS2.p6.8.m8.1.1.4"><csymbol cd="ambiguous" id="S3.SS2.p6.8.m8.1.1.4.2.1.cmml" xref="S3.SS2.p6.8.m8.1.1.4">superscript</csymbol><ci id="S3.SS2.p6.8.m8.1.1.4.2.2.cmml" xref="S3.SS2.p6.8.m8.1.1.4.2.2">𝑠</ci><ci id="S3.SS2.p6.8.m8.1.1.4.2.3.cmml" xref="S3.SS2.p6.8.m8.1.1.4.2.3">𝑔</ci></apply><ci id="S3.SS2.p6.8.m8.1.1.4.3.cmml" xref="S3.SS2.p6.8.m8.1.1.4.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.8.m8.1c">pos^{g}_{v}</annotation></semantics></math> are learnable positional embeddings for each modality. The decoders use a transformer-based architecture and are entrusted with the task of effectively utilizing the mix of uni-modal slices and cross-modal slices present in <math id="S3.SS2.p6.9.m9.1" class="ltx_Math" alttext="\bm{a^{\prime}}" display="inline"><semantics id="S3.SS2.p6.9.m9.1a"><msup id="S3.SS2.p6.9.m9.1.1" xref="S3.SS2.p6.9.m9.1.1.cmml"><mi id="S3.SS2.p6.9.m9.1.1.2" xref="S3.SS2.p6.9.m9.1.1.2.cmml">𝒂</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS2.p6.9.m9.1.1.3" xref="S3.SS2.p6.9.m9.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.9.m9.1b"><apply id="S3.SS2.p6.9.m9.1.1.cmml" xref="S3.SS2.p6.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.9.m9.1.1.1.cmml" xref="S3.SS2.p6.9.m9.1.1">superscript</csymbol><ci id="S3.SS2.p6.9.m9.1.1.2.cmml" xref="S3.SS2.p6.9.m9.1.1.2">𝒂</ci><ci id="S3.SS2.p6.9.m9.1.1.3.cmml" xref="S3.SS2.p6.9.m9.1.1.3">bold-′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.9.m9.1c">\bm{a^{\prime}}</annotation></semantics></math> and <math id="S3.SS2.p6.10.m10.1" class="ltx_Math" alttext="\bm{v^{\prime}}" display="inline"><semantics id="S3.SS2.p6.10.m10.1a"><msup id="S3.SS2.p6.10.m10.1.1" xref="S3.SS2.p6.10.m10.1.1.cmml"><mi id="S3.SS2.p6.10.m10.1.1.2" xref="S3.SS2.p6.10.m10.1.1.2.cmml">𝒗</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS2.p6.10.m10.1.1.3" xref="S3.SS2.p6.10.m10.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.10.m10.1b"><apply id="S3.SS2.p6.10.m10.1.1.cmml" xref="S3.SS2.p6.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.10.m10.1.1.1.cmml" xref="S3.SS2.p6.10.m10.1.1">superscript</csymbol><ci id="S3.SS2.p6.10.m10.1.1.2.cmml" xref="S3.SS2.p6.10.m10.1.1.2">𝒗</ci><ci id="S3.SS2.p6.10.m10.1.1.3.cmml" xref="S3.SS2.p6.10.m10.1.1.3">bold-′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.10.m10.1c">\bm{v^{\prime}}</annotation></semantics></math> to generate the reconstructions for the two modalities.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para ltx_noindent">
<p id="S3.SS2.p7.6" class="ltx_p"><span id="S3.SS2.p7.6.1" class="ltx_text ltx_font_bold">Loss Functions.</span>
We use a dual objective loss, comprising an audio-visual contrastive loss and an autoencoding loss. The bi-directional audio-visual contrastive loss is defined as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.6" class="ltx_Math" alttext="\footnotesize\mathcal{L}_{c}=-\mkern-24.0mu\sum_{\begin{subarray}{c}p,q\in\{a,v\},\\
p\neq q\end{subarray}}\mkern-12.0mu\frac{1}{2N}\sum_{i=1}^{N}\log\left[\frac{\exp(\|\bm{\bar{p}}^{(i)}\|^{T}\|\bm{\bar{q}}^{(i)}\|/\tau)}{\sum_{j=1}^{N}\exp(\|\bm{\bar{p}}^{(i)}\|^{T}\|\bm{\bar{q}}^{(j)}\|/\tau)}\right]" display="block"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.7" xref="S3.E1.m1.6.7.cmml"><msub id="S3.E1.m1.6.7.2" xref="S3.E1.m1.6.7.2.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="80%" id="S3.E1.m1.6.7.2.2" xref="S3.E1.m1.6.7.2.2.cmml">ℒ</mi><mi mathsize="80%" id="S3.E1.m1.6.7.2.3" xref="S3.E1.m1.6.7.2.3.cmml">c</mi></msub><mo mathsize="80%" id="S3.E1.m1.6.7.1" xref="S3.E1.m1.6.7.1.cmml">=</mo><mrow id="S3.E1.m1.6.7.3" xref="S3.E1.m1.6.7.3.cmml"><mo mathsize="80%" id="S3.E1.m1.6.7.3a" xref="S3.E1.m1.6.7.3.cmml">−</mo><mrow id="S3.E1.m1.6.7.3.2" xref="S3.E1.m1.6.7.3.2.cmml"><munder id="S3.E1.m1.6.7.3.2.1" xref="S3.E1.m1.6.7.3.2.1.cmml"><mo maxsize="80%" minsize="80%" movablelimits="false" stretchy="true" id="S3.E1.m1.6.7.3.2.1.2" xref="S3.E1.m1.6.7.3.2.1.2.cmml">∑</mo><mtable rowspacing="0pt" id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.2.cmml"><mtr id="S3.E1.m1.1.1.1.1.1.1a" xref="S3.E1.m1.1.1.1.2.cmml"><mtd id="S3.E1.m1.1.1.1.1.1.1b" xref="S3.E1.m1.1.1.1.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.2.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.3.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.3.3.3.cmml">p</mi><mo id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.2.2.1" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.2.1.cmml">,</mo><mi id="S3.E1.m1.1.1.1.1.1.1.4.4.4.4.4" xref="S3.E1.m1.1.1.1.1.1.1.4.4.4.4.4.cmml">q</mi></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.1" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.1.cmml">∈</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.3.2.1" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.3.1.cmml">{</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">a</mi><mo id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.3.2.2" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.3.1.cmml">,</mo><mi id="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.2.cmml">v</mi><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.3.2.3" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.3.1.cmml">}</mo></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.2" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1.1.1c" xref="S3.E1.m1.1.1.1.2.cmml"><mtd id="S3.E1.m1.1.1.1.1.1.1d" xref="S3.E1.m1.1.1.1.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.6.1.1" xref="S3.E1.m1.1.1.1.1.1.1.6.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.6.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.6.1.1.2.cmml">p</mi><mo id="S3.E1.m1.1.1.1.1.1.1.6.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.6.1.1.1.cmml">≠</mo><mi id="S3.E1.m1.1.1.1.1.1.1.6.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.6.1.1.3.cmml">q</mi></mrow></mtd></mtr></mtable></munder><mrow id="S3.E1.m1.6.7.3.2.2" xref="S3.E1.m1.6.7.3.2.2.cmml"><mfrac id="S3.E1.m1.6.7.3.2.2.2" xref="S3.E1.m1.6.7.3.2.2.2.cmml"><mn mathsize="80%" id="S3.E1.m1.6.7.3.2.2.2.2" xref="S3.E1.m1.6.7.3.2.2.2.2.cmml">1</mn><mrow id="S3.E1.m1.6.7.3.2.2.2.3" xref="S3.E1.m1.6.7.3.2.2.2.3.cmml"><mn mathsize="80%" id="S3.E1.m1.6.7.3.2.2.2.3.2" xref="S3.E1.m1.6.7.3.2.2.2.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.7.3.2.2.2.3.1" xref="S3.E1.m1.6.7.3.2.2.2.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E1.m1.6.7.3.2.2.2.3.3" xref="S3.E1.m1.6.7.3.2.2.2.3.3.cmml">N</mi></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.7.3.2.2.1" xref="S3.E1.m1.6.7.3.2.2.1.cmml">​</mo><mrow id="S3.E1.m1.6.7.3.2.2.3" xref="S3.E1.m1.6.7.3.2.2.3.cmml"><munderover id="S3.E1.m1.6.7.3.2.2.3.1" xref="S3.E1.m1.6.7.3.2.2.3.1.cmml"><mo maxsize="80%" minsize="80%" movablelimits="false" stretchy="true" id="S3.E1.m1.6.7.3.2.2.3.1.2.2" xref="S3.E1.m1.6.7.3.2.2.3.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.6.7.3.2.2.3.1.2.3" xref="S3.E1.m1.6.7.3.2.2.3.1.2.3.cmml"><mi mathsize="80%" id="S3.E1.m1.6.7.3.2.2.3.1.2.3.2" xref="S3.E1.m1.6.7.3.2.2.3.1.2.3.2.cmml">i</mi><mo mathsize="80%" id="S3.E1.m1.6.7.3.2.2.3.1.2.3.1" xref="S3.E1.m1.6.7.3.2.2.3.1.2.3.1.cmml">=</mo><mn mathsize="80%" id="S3.E1.m1.6.7.3.2.2.3.1.2.3.3" xref="S3.E1.m1.6.7.3.2.2.3.1.2.3.3.cmml">1</mn></mrow><mi mathsize="80%" id="S3.E1.m1.6.7.3.2.2.3.1.3" xref="S3.E1.m1.6.7.3.2.2.3.1.3.cmml">N</mi></munderover><mrow id="S3.E1.m1.6.7.3.2.2.3.2.2" xref="S3.E1.m1.6.7.3.2.2.3.2.1.cmml"><mi mathsize="80%" id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml">log</mi><mo id="S3.E1.m1.6.7.3.2.2.3.2.2a" xref="S3.E1.m1.6.7.3.2.2.3.2.1.cmml">⁡</mo><mrow id="S3.E1.m1.6.7.3.2.2.3.2.2.1" xref="S3.E1.m1.6.7.3.2.2.3.2.1.cmml"><mo id="S3.E1.m1.6.7.3.2.2.3.2.2.1.1" xref="S3.E1.m1.6.7.3.2.2.3.2.1.cmml">[</mo><mfrac id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml"><mrow id="S3.E1.m1.4.4.3.4" xref="S3.E1.m1.4.4.3.3.cmml"><mi mathsize="80%" id="S3.E1.m1.4.4.3.2.2.2" xref="S3.E1.m1.4.4.3.3.1.cmml">exp</mi><mo id="S3.E1.m1.4.4.3.4a" xref="S3.E1.m1.4.4.3.3.1.cmml">⁡</mo><mrow id="S3.E1.m1.4.4.3.4.1" xref="S3.E1.m1.4.4.3.3.cmml"><mo id="S3.E1.m1.4.4.3.4.1.1" xref="S3.E1.m1.4.4.3.3.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.1.1.4" xref="S3.E1.m1.2.2.1.1.1.1.1.4.cmml"><msup id="S3.E1.m1.2.2.1.1.1.1.1.3.1" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.2.1.cmml">‖</mo><msup id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.2.cmml"><mi mathsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.2.2" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.2.2.cmml">𝒑</mi><mo class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.2.1" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.2.1.cmml">¯</mo></mover><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.cmml">(</mo><mi mathsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">i</mi><mo maxsize="80%" minsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.cmml">)</mo></mrow></msup><mo maxsize="80%" minsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.2.1.cmml">‖</mo></mrow><mi mathsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.3.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.1.1.4.3" xref="S3.E1.m1.2.2.1.1.1.1.1.4.3.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.4.2.1" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.2.cmml"><mo maxsize="80%" minsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.2.1.cmml">‖</mo><msup id="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.cmml"><mover accent="true" id="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.2.cmml"><mi mathsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.2.2" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.2.2.cmml">𝒒</mi><mo class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" id="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.2.1" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.2.1.cmml">¯</mo></mover><mrow id="S3.E1.m1.2.2.1.1.1.1.1.2.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.2.1.3.1" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.cmml">(</mo><mi mathsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.2.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.2.1.1.cmml">i</mi><mo maxsize="80%" minsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.2.1.3.2" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.cmml">)</mo></mrow></msup><mo maxsize="80%" minsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.2.1.cmml">‖</mo></mrow></mrow><mo maxsize="80%" minsize="80%" stretchy="true" symmetric="true" id="S3.E1.m1.2.2.1.1.1.1.1.5" xref="S3.E1.m1.2.2.1.1.1.1.1.5.cmml">/</mo><mi mathsize="80%" id="S3.E1.m1.2.2.1.1.1.1.1.6" xref="S3.E1.m1.2.2.1.1.1.1.1.6.cmml">τ</mi></mrow><mo id="S3.E1.m1.4.4.3.4.1.2" xref="S3.E1.m1.4.4.3.3.1.cmml">)</mo></mrow></mrow><mrow id="S3.E1.m1.5.5.4" xref="S3.E1.m1.5.5.4.cmml"><msubsup id="S3.E1.m1.5.5.4.3" xref="S3.E1.m1.5.5.4.3.cmml"><mo maxsize="80%" minsize="80%" stretchy="true" id="S3.E1.m1.5.5.4.3.2.2" xref="S3.E1.m1.5.5.4.3.2.2.cmml">∑</mo><mrow id="S3.E1.m1.5.5.4.3.2.3" xref="S3.E1.m1.5.5.4.3.2.3.cmml"><mi mathsize="80%" id="S3.E1.m1.5.5.4.3.2.3.2" xref="S3.E1.m1.5.5.4.3.2.3.2.cmml">j</mi><mo mathsize="80%" id="S3.E1.m1.5.5.4.3.2.3.1" xref="S3.E1.m1.5.5.4.3.2.3.1.cmml">=</mo><mn mathsize="80%" id="S3.E1.m1.5.5.4.3.2.3.3" xref="S3.E1.m1.5.5.4.3.2.3.3.cmml">1</mn></mrow><mi mathsize="80%" id="S3.E1.m1.5.5.4.3.3" xref="S3.E1.m1.5.5.4.3.3.cmml">N</mi></msubsup><mrow id="S3.E1.m1.5.5.4.2.4" xref="S3.E1.m1.5.5.4.2.3.cmml"><mi mathsize="80%" id="S3.E1.m1.5.5.4.2.2.2" xref="S3.E1.m1.5.5.4.2.3.1.cmml">exp</mi><mo id="S3.E1.m1.5.5.4.2.4a" xref="S3.E1.m1.5.5.4.2.3.1.cmml">⁡</mo><mrow id="S3.E1.m1.5.5.4.2.4.1" xref="S3.E1.m1.5.5.4.2.3.cmml"><mo id="S3.E1.m1.5.5.4.2.4.1.1" xref="S3.E1.m1.5.5.4.2.3.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.2.1.1.1.1" xref="S3.E1.m1.3.3.2.1.1.1.1.cmml"><mrow id="S3.E1.m1.3.3.2.1.1.1.1.4" xref="S3.E1.m1.3.3.2.1.1.1.1.4.cmml"><msup id="S3.E1.m1.3.3.2.1.1.1.1.3.1" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.cmml"><mrow id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.2" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.2.1.cmml">‖</mo><msup id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.2" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.2.cmml"><mi mathsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.2.2" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.2.2.cmml">𝒑</mi><mo class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.2.1" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.2.1.cmml">¯</mo></mover><mrow id="S3.E1.m1.3.3.2.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.1.1.3.1" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.cmml">(</mo><mi mathsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.2.1.1.1.1.1.1.1.cmml">i</mi><mo maxsize="80%" minsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.cmml">)</mo></mrow></msup><mo maxsize="80%" minsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.3" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.2.1.cmml">‖</mo></mrow><mi mathsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.3.1.3" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.2.1.1.1.1.4.3" xref="S3.E1.m1.3.3.2.1.1.1.1.4.3.cmml">​</mo><mrow id="S3.E1.m1.3.3.2.1.1.1.1.4.2.1" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.2.cmml"><mo maxsize="80%" minsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.2" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.2.1.cmml">‖</mo><msup id="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.cmml"><mover accent="true" id="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.2" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.2.cmml"><mi mathsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.2.2" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.2.2.cmml">𝒒</mi><mo class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" id="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.2.1" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.2.1.cmml">¯</mo></mover><mrow id="S3.E1.m1.3.3.2.1.1.1.1.2.1.3" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.2.1.3.1" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.cmml">(</mo><mi mathsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.2.1.1" xref="S3.E1.m1.3.3.2.1.1.1.1.2.1.1.cmml">j</mi><mo maxsize="80%" minsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.2.1.3.2" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.cmml">)</mo></mrow></msup><mo maxsize="80%" minsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.3" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.2.1.cmml">‖</mo></mrow></mrow><mo maxsize="80%" minsize="80%" stretchy="true" symmetric="true" id="S3.E1.m1.3.3.2.1.1.1.1.5" xref="S3.E1.m1.3.3.2.1.1.1.1.5.cmml">/</mo><mi mathsize="80%" id="S3.E1.m1.3.3.2.1.1.1.1.6" xref="S3.E1.m1.3.3.2.1.1.1.1.6.cmml">τ</mi></mrow><mo id="S3.E1.m1.5.5.4.2.4.1.2" xref="S3.E1.m1.5.5.4.2.3.1.cmml">)</mo></mrow></mrow></mrow></mfrac><mo id="S3.E1.m1.6.7.3.2.2.3.2.2.1.2" xref="S3.E1.m1.6.7.3.2.2.3.2.1.cmml">]</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><apply id="S3.E1.m1.6.7.cmml" xref="S3.E1.m1.6.7"><eq id="S3.E1.m1.6.7.1.cmml" xref="S3.E1.m1.6.7.1"></eq><apply id="S3.E1.m1.6.7.2.cmml" xref="S3.E1.m1.6.7.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.7.2.1.cmml" xref="S3.E1.m1.6.7.2">subscript</csymbol><ci id="S3.E1.m1.6.7.2.2.cmml" xref="S3.E1.m1.6.7.2.2">ℒ</ci><ci id="S3.E1.m1.6.7.2.3.cmml" xref="S3.E1.m1.6.7.2.3">𝑐</ci></apply><apply id="S3.E1.m1.6.7.3.cmml" xref="S3.E1.m1.6.7.3"><minus id="S3.E1.m1.6.7.3.1.cmml" xref="S3.E1.m1.6.7.3"></minus><apply id="S3.E1.m1.6.7.3.2.cmml" xref="S3.E1.m1.6.7.3.2"><apply id="S3.E1.m1.6.7.3.2.1.cmml" xref="S3.E1.m1.6.7.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.7.3.2.1.1.cmml" xref="S3.E1.m1.6.7.3.2.1">subscript</csymbol><sum id="S3.E1.m1.6.7.3.2.1.2.cmml" xref="S3.E1.m1.6.7.3.2.1.2"></sum><list id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><matrix id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><matrixrow id="S3.E1.m1.1.1.1.1.1.1a.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5"><in id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.1"></in><list id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.2.2"><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3.3.3.3">𝑝</ci><ci id="S3.E1.m1.1.1.1.1.1.1.4.4.4.4.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4.4.4.4.4">𝑞</ci></list><set id="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.5.5.5.5.5.1.3.2"><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1">𝑎</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.2">𝑣</ci></set></apply></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1.1.1b.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.1.1.6.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.6.1.1"><neq id="S3.E1.m1.1.1.1.1.1.1.6.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.6.1.1.1"></neq><ci id="S3.E1.m1.1.1.1.1.1.1.6.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.6.1.1.2">𝑝</ci><ci id="S3.E1.m1.1.1.1.1.1.1.6.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.6.1.1.3">𝑞</ci></apply></matrixrow></matrix></list></apply><apply id="S3.E1.m1.6.7.3.2.2.cmml" xref="S3.E1.m1.6.7.3.2.2"><times id="S3.E1.m1.6.7.3.2.2.1.cmml" xref="S3.E1.m1.6.7.3.2.2.1"></times><apply id="S3.E1.m1.6.7.3.2.2.2.cmml" xref="S3.E1.m1.6.7.3.2.2.2"><divide id="S3.E1.m1.6.7.3.2.2.2.1.cmml" xref="S3.E1.m1.6.7.3.2.2.2"></divide><cn type="integer" id="S3.E1.m1.6.7.3.2.2.2.2.cmml" xref="S3.E1.m1.6.7.3.2.2.2.2">1</cn><apply id="S3.E1.m1.6.7.3.2.2.2.3.cmml" xref="S3.E1.m1.6.7.3.2.2.2.3"><times id="S3.E1.m1.6.7.3.2.2.2.3.1.cmml" xref="S3.E1.m1.6.7.3.2.2.2.3.1"></times><cn type="integer" id="S3.E1.m1.6.7.3.2.2.2.3.2.cmml" xref="S3.E1.m1.6.7.3.2.2.2.3.2">2</cn><ci id="S3.E1.m1.6.7.3.2.2.2.3.3.cmml" xref="S3.E1.m1.6.7.3.2.2.2.3.3">𝑁</ci></apply></apply><apply id="S3.E1.m1.6.7.3.2.2.3.cmml" xref="S3.E1.m1.6.7.3.2.2.3"><apply id="S3.E1.m1.6.7.3.2.2.3.1.cmml" xref="S3.E1.m1.6.7.3.2.2.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.7.3.2.2.3.1.1.cmml" xref="S3.E1.m1.6.7.3.2.2.3.1">superscript</csymbol><apply id="S3.E1.m1.6.7.3.2.2.3.1.2.cmml" xref="S3.E1.m1.6.7.3.2.2.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.7.3.2.2.3.1.2.1.cmml" xref="S3.E1.m1.6.7.3.2.2.3.1">subscript</csymbol><sum id="S3.E1.m1.6.7.3.2.2.3.1.2.2.cmml" xref="S3.E1.m1.6.7.3.2.2.3.1.2.2"></sum><apply id="S3.E1.m1.6.7.3.2.2.3.1.2.3.cmml" xref="S3.E1.m1.6.7.3.2.2.3.1.2.3"><eq id="S3.E1.m1.6.7.3.2.2.3.1.2.3.1.cmml" xref="S3.E1.m1.6.7.3.2.2.3.1.2.3.1"></eq><ci id="S3.E1.m1.6.7.3.2.2.3.1.2.3.2.cmml" xref="S3.E1.m1.6.7.3.2.2.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.6.7.3.2.2.3.1.2.3.3.cmml" xref="S3.E1.m1.6.7.3.2.2.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.6.7.3.2.2.3.1.3.cmml" xref="S3.E1.m1.6.7.3.2.2.3.1.3">𝑁</ci></apply><apply id="S3.E1.m1.6.7.3.2.2.3.2.1.cmml" xref="S3.E1.m1.6.7.3.2.2.3.2.2"><log id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6"></log><apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5"><divide id="S3.E1.m1.5.5.5.cmml" xref="S3.E1.m1.5.5"></divide><apply id="S3.E1.m1.4.4.3.3.cmml" xref="S3.E1.m1.4.4.3.4"><exp id="S3.E1.m1.4.4.3.3.1.cmml" xref="S3.E1.m1.4.4.3.2.2.2"></exp><apply id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><divide id="S3.E1.m1.2.2.1.1.1.1.1.5.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.5"></divide><apply id="S3.E1.m1.2.2.1.1.1.1.1.4.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.4"><times id="S3.E1.m1.2.2.1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.4.3"></times><apply id="S3.E1.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.3.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.2"><ci id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.2.1">bold-¯</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.1.1.1.2.2">𝒑</ci></apply><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1">𝑖</ci></apply></apply><ci id="S3.E1.m1.2.2.1.1.1.1.1.3.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.3.1.3">𝑇</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.4.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.1.1.1.1.1.4.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.2">norm</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.2"><ci id="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.2.1">bold-¯</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.4.2.1.1.2.2">𝒒</ci></apply><ci id="S3.E1.m1.2.2.1.1.1.1.1.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2.1.1">𝑖</ci></apply></apply></apply><ci id="S3.E1.m1.2.2.1.1.1.1.1.6.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.6">𝜏</ci></apply></apply><apply id="S3.E1.m1.5.5.4.cmml" xref="S3.E1.m1.5.5.4"><apply id="S3.E1.m1.5.5.4.3.cmml" xref="S3.E1.m1.5.5.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.4.3.1.cmml" xref="S3.E1.m1.5.5.4.3">superscript</csymbol><apply id="S3.E1.m1.5.5.4.3.2.cmml" xref="S3.E1.m1.5.5.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.4.3.2.1.cmml" xref="S3.E1.m1.5.5.4.3">subscript</csymbol><sum id="S3.E1.m1.5.5.4.3.2.2.cmml" xref="S3.E1.m1.5.5.4.3.2.2"></sum><apply id="S3.E1.m1.5.5.4.3.2.3.cmml" xref="S3.E1.m1.5.5.4.3.2.3"><eq id="S3.E1.m1.5.5.4.3.2.3.1.cmml" xref="S3.E1.m1.5.5.4.3.2.3.1"></eq><ci id="S3.E1.m1.5.5.4.3.2.3.2.cmml" xref="S3.E1.m1.5.5.4.3.2.3.2">𝑗</ci><cn type="integer" id="S3.E1.m1.5.5.4.3.2.3.3.cmml" xref="S3.E1.m1.5.5.4.3.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.5.5.4.3.3.cmml" xref="S3.E1.m1.5.5.4.3.3">𝑁</ci></apply><apply id="S3.E1.m1.5.5.4.2.3.cmml" xref="S3.E1.m1.5.5.4.2.4"><exp id="S3.E1.m1.5.5.4.2.3.1.cmml" xref="S3.E1.m1.5.5.4.2.2.2"></exp><apply id="S3.E1.m1.3.3.2.1.1.1.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1"><divide id="S3.E1.m1.3.3.2.1.1.1.1.5.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.5"></divide><apply id="S3.E1.m1.3.3.2.1.1.1.1.4.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.4"><times id="S3.E1.m1.3.3.2.1.1.1.1.4.3.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.4.3"></times><apply id="S3.E1.m1.3.3.2.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.2.1.1.1.1.3.1.2.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1">superscript</csymbol><apply id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.2.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.2.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.2"><ci id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.2.1">bold-¯</ci><ci id="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.1.1.1.2.2">𝒑</ci></apply><ci id="S3.E1.m1.3.3.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.1.1.1">𝑖</ci></apply></apply><ci id="S3.E1.m1.3.3.2.1.1.1.1.3.1.3.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.3.1.3">𝑇</ci></apply><apply id="S3.E1.m1.3.3.2.1.1.1.1.4.2.2.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.2.1.1.1.1.4.2.2.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.2">norm</csymbol><apply id="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1">superscript</csymbol><apply id="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.2.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.2"><ci id="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.2.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.2.1">bold-¯</ci><ci id="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.2.2.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.4.2.1.1.2.2">𝒒</ci></apply><ci id="S3.E1.m1.3.3.2.1.1.1.1.2.1.1.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.2.1.1">𝑗</ci></apply></apply></apply><ci id="S3.E1.m1.3.3.2.1.1.1.1.6.cmml" xref="S3.E1.m1.3.3.2.1.1.1.1.6">𝜏</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">\footnotesize\mathcal{L}_{c}=-\mkern-24.0mu\sum_{\begin{subarray}{c}p,q\in\{a,v\},\\
p\neq q\end{subarray}}\mkern-12.0mu\frac{1}{2N}\sum_{i=1}^{N}\log\left[\frac{\exp(\|\bm{\bar{p}}^{(i)}\|^{T}\|\bm{\bar{q}}^{(i)}\|/\tau)}{\sum_{j=1}^{N}\exp(\|\bm{\bar{p}}^{(i)}\|^{T}\|\bm{\bar{q}}^{(j)}\|/\tau)}\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p7.5" class="ltx_p">where
<math id="S3.SS2.p7.1.m1.1" class="ltx_Math" alttext="\bm{\bar{p}}^{(i)}" display="inline"><semantics id="S3.SS2.p7.1.m1.1a"><msup id="S3.SS2.p7.1.m1.1.2" xref="S3.SS2.p7.1.m1.1.2.cmml"><mover accent="true" id="S3.SS2.p7.1.m1.1.2.2" xref="S3.SS2.p7.1.m1.1.2.2.cmml"><mi id="S3.SS2.p7.1.m1.1.2.2.2" xref="S3.SS2.p7.1.m1.1.2.2.2.cmml">𝒑</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS2.p7.1.m1.1.2.2.1" xref="S3.SS2.p7.1.m1.1.2.2.1.cmml">¯</mo></mover><mrow id="S3.SS2.p7.1.m1.1.1.1.3" xref="S3.SS2.p7.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p7.1.m1.1.1.1.3.1" xref="S3.SS2.p7.1.m1.1.2.cmml">(</mo><mi id="S3.SS2.p7.1.m1.1.1.1.1" xref="S3.SS2.p7.1.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S3.SS2.p7.1.m1.1.1.1.3.2" xref="S3.SS2.p7.1.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.1b"><apply id="S3.SS2.p7.1.m1.1.2.cmml" xref="S3.SS2.p7.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.1.2.1.cmml" xref="S3.SS2.p7.1.m1.1.2">superscript</csymbol><apply id="S3.SS2.p7.1.m1.1.2.2.cmml" xref="S3.SS2.p7.1.m1.1.2.2"><ci id="S3.SS2.p7.1.m1.1.2.2.1.cmml" xref="S3.SS2.p7.1.m1.1.2.2.1">bold-¯</ci><ci id="S3.SS2.p7.1.m1.1.2.2.2.cmml" xref="S3.SS2.p7.1.m1.1.2.2.2">𝒑</ci></apply><ci id="S3.SS2.p7.1.m1.1.1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1.1.1">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.1c">\bm{\bar{p}}^{(i)}</annotation></semantics></math>
is the mean latent vector across the patch dimension of the uni-modal embeddings of the <math id="S3.SS2.p7.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p7.2.m2.1a"><mi id="S3.SS2.p7.2.m2.1.1" xref="S3.SS2.p7.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.2.m2.1b"><ci id="S3.SS2.p7.2.m2.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.2.m2.1c">i</annotation></semantics></math>-th data sample, <math id="S3.SS2.p7.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p7.3.m3.1a"><mi id="S3.SS2.p7.3.m3.1.1" xref="S3.SS2.p7.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.3.m3.1b"><ci id="S3.SS2.p7.3.m3.1.1.cmml" xref="S3.SS2.p7.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.3.m3.1c">N</annotation></semantics></math> is the number of samples, <math id="S3.SS2.p7.4.m4.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS2.p7.4.m4.1a"><mi id="S3.SS2.p7.4.m4.1.1" xref="S3.SS2.p7.4.m4.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.4.m4.1b"><ci id="S3.SS2.p7.4.m4.1.1.cmml" xref="S3.SS2.p7.4.m4.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.4.m4.1c">\tau</annotation></semantics></math> is the temperature parameter, and <math id="S3.SS2.p7.5.m5.2" class="ltx_Math" alttext="i,j" display="inline"><semantics id="S3.SS2.p7.5.m5.2a"><mrow id="S3.SS2.p7.5.m5.2.3.2" xref="S3.SS2.p7.5.m5.2.3.1.cmml"><mi id="S3.SS2.p7.5.m5.1.1" xref="S3.SS2.p7.5.m5.1.1.cmml">i</mi><mo id="S3.SS2.p7.5.m5.2.3.2.1" xref="S3.SS2.p7.5.m5.2.3.1.cmml">,</mo><mi id="S3.SS2.p7.5.m5.2.2" xref="S3.SS2.p7.5.m5.2.2.cmml">j</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.5.m5.2b"><list id="S3.SS2.p7.5.m5.2.3.1.cmml" xref="S3.SS2.p7.5.m5.2.3.2"><ci id="S3.SS2.p7.5.m5.1.1.cmml" xref="S3.SS2.p7.5.m5.1.1">𝑖</ci><ci id="S3.SS2.p7.5.m5.2.2.cmml" xref="S3.SS2.p7.5.m5.2.2">𝑗</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.5.m5.2c">i,j</annotation></semantics></math> are sample indices.
The audio-visual contrastive loss enforces similarity constraints between the audio and visual embeddings of a given sample.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.6" class="ltx_p">The autoencoder loss, <math id="S3.SS2.p8.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{ae}" display="inline"><semantics id="S3.SS2.p8.1.m1.1a"><msub id="S3.SS2.p8.1.m1.1.1" xref="S3.SS2.p8.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p8.1.m1.1.1.2" xref="S3.SS2.p8.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S3.SS2.p8.1.m1.1.1.3" xref="S3.SS2.p8.1.m1.1.1.3.cmml"><mi id="S3.SS2.p8.1.m1.1.1.3.2" xref="S3.SS2.p8.1.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p8.1.m1.1.1.3.1" xref="S3.SS2.p8.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p8.1.m1.1.1.3.3" xref="S3.SS2.p8.1.m1.1.1.3.3.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.1b"><apply id="S3.SS2.p8.1.m1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.1.m1.1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p8.1.m1.1.1.2.cmml" xref="S3.SS2.p8.1.m1.1.1.2">ℒ</ci><apply id="S3.SS2.p8.1.m1.1.1.3.cmml" xref="S3.SS2.p8.1.m1.1.1.3"><times id="S3.SS2.p8.1.m1.1.1.3.1.cmml" xref="S3.SS2.p8.1.m1.1.1.3.1"></times><ci id="S3.SS2.p8.1.m1.1.1.3.2.cmml" xref="S3.SS2.p8.1.m1.1.1.3.2">𝑎</ci><ci id="S3.SS2.p8.1.m1.1.1.3.3.cmml" xref="S3.SS2.p8.1.m1.1.1.3.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.1c">\mathcal{L}_{ae}</annotation></semantics></math>, is composed of reconstruction and adversarial losses, similar to MARLIN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. The reconstruction MSE loss, <math id="S3.SS2.p8.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{rec}" display="inline"><semantics id="S3.SS2.p8.2.m2.1a"><msub id="S3.SS2.p8.2.m2.1.1" xref="S3.SS2.p8.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p8.2.m2.1.1.2" xref="S3.SS2.p8.2.m2.1.1.2.cmml">ℒ</mi><mrow id="S3.SS2.p8.2.m2.1.1.3" xref="S3.SS2.p8.2.m2.1.1.3.cmml"><mi id="S3.SS2.p8.2.m2.1.1.3.2" xref="S3.SS2.p8.2.m2.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p8.2.m2.1.1.3.1" xref="S3.SS2.p8.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p8.2.m2.1.1.3.3" xref="S3.SS2.p8.2.m2.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p8.2.m2.1.1.3.1a" xref="S3.SS2.p8.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p8.2.m2.1.1.3.4" xref="S3.SS2.p8.2.m2.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.2.m2.1b"><apply id="S3.SS2.p8.2.m2.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.2.m2.1.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p8.2.m2.1.1.2.cmml" xref="S3.SS2.p8.2.m2.1.1.2">ℒ</ci><apply id="S3.SS2.p8.2.m2.1.1.3.cmml" xref="S3.SS2.p8.2.m2.1.1.3"><times id="S3.SS2.p8.2.m2.1.1.3.1.cmml" xref="S3.SS2.p8.2.m2.1.1.3.1"></times><ci id="S3.SS2.p8.2.m2.1.1.3.2.cmml" xref="S3.SS2.p8.2.m2.1.1.3.2">𝑟</ci><ci id="S3.SS2.p8.2.m2.1.1.3.3.cmml" xref="S3.SS2.p8.2.m2.1.1.3.3">𝑒</ci><ci id="S3.SS2.p8.2.m2.1.1.3.4.cmml" xref="S3.SS2.p8.2.m2.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.2.m2.1c">\mathcal{L}_{rec}</annotation></semantics></math>, is computed between the inputs <math id="S3.SS2.p8.3.m3.1" class="ltx_math_unparsed" alttext="(\bm{x_{a}}" display="inline"><semantics id="S3.SS2.p8.3.m3.1a"><mrow id="S3.SS2.p8.3.m3.1b"><mo stretchy="false" id="S3.SS2.p8.3.m3.1.1">(</mo><msub id="S3.SS2.p8.3.m3.1.2"><mi id="S3.SS2.p8.3.m3.1.2.2">𝒙</mi><mi id="S3.SS2.p8.3.m3.1.2.3">𝒂</mi></msub></mrow><annotation encoding="application/x-tex" id="S3.SS2.p8.3.m3.1c">(\bm{x_{a}}</annotation></semantics></math>, <math id="S3.SS2.p8.4.m4.1" class="ltx_math_unparsed" alttext="\bm{x_{v}})" display="inline"><semantics id="S3.SS2.p8.4.m4.1a"><mrow id="S3.SS2.p8.4.m4.1b"><msub id="S3.SS2.p8.4.m4.1.1"><mi id="S3.SS2.p8.4.m4.1.1.2">𝒙</mi><mi id="S3.SS2.p8.4.m4.1.1.3">𝒗</mi></msub><mo stretchy="false" id="S3.SS2.p8.4.m4.1.2">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.p8.4.m4.1c">\bm{x_{v}})</annotation></semantics></math>, and their reconstructions <math id="S3.SS2.p8.5.m5.1" class="ltx_math_unparsed" alttext="(\bm{\hat{x}_{a}}" display="inline"><semantics id="S3.SS2.p8.5.m5.1a"><mrow id="S3.SS2.p8.5.m5.1b"><mo stretchy="false" id="S3.SS2.p8.5.m5.1.1">(</mo><msub id="S3.SS2.p8.5.m5.1.2"><mover accent="true" id="S3.SS2.p8.5.m5.1.2.2"><mi id="S3.SS2.p8.5.m5.1.2.2.2">𝒙</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS2.p8.5.m5.1.2.2.1">^</mo></mover><mi id="S3.SS2.p8.5.m5.1.2.3">𝒂</mi></msub></mrow><annotation encoding="application/x-tex" id="S3.SS2.p8.5.m5.1c">(\bm{\hat{x}_{a}}</annotation></semantics></math>, <math id="S3.SS2.p8.6.m6.1" class="ltx_math_unparsed" alttext="\bm{\hat{x}_{v}})" display="inline"><semantics id="S3.SS2.p8.6.m6.1a"><mrow id="S3.SS2.p8.6.m6.1b"><msub id="S3.SS2.p8.6.m6.1.1"><mover accent="true" id="S3.SS2.p8.6.m6.1.1.2"><mi id="S3.SS2.p8.6.m6.1.1.2.2">𝒙</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S3.SS2.p8.6.m6.1.1.2.1">^</mo></mover><mi id="S3.SS2.p8.6.m6.1.1.3">𝒗</mi></msub><mo stretchy="false" id="S3.SS2.p8.6.m6.1.2">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.p8.6.m6.1c">\bm{\hat{x}_{v}})</annotation></semantics></math>, and is computed only over the masked tokens following the approach in MAEs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.5" class="ltx_Math" alttext="\footnotesize\mathcal{L}_{rec}=\sum_{p\in\{a,v\}}\frac{1}{N}\sum_{i=1}^{N}\|{\bm{x_{p_{msk}}}^{(i)}}-\bm{\hat{x}_{p_{msk}}}^{(i)}\|" display="block"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml"><msub id="S3.E2.m1.5.5.3" xref="S3.E2.m1.5.5.3.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="80%" id="S3.E2.m1.5.5.3.2" xref="S3.E2.m1.5.5.3.2.cmml">ℒ</mi><mrow id="S3.E2.m1.5.5.3.3" xref="S3.E2.m1.5.5.3.3.cmml"><mi mathsize="80%" id="S3.E2.m1.5.5.3.3.2" xref="S3.E2.m1.5.5.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.3.3.1" xref="S3.E2.m1.5.5.3.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E2.m1.5.5.3.3.3" xref="S3.E2.m1.5.5.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.3.3.1a" xref="S3.E2.m1.5.5.3.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E2.m1.5.5.3.3.4" xref="S3.E2.m1.5.5.3.3.4.cmml">c</mi></mrow></msub><mo mathsize="80%" rspace="0.111em" id="S3.E2.m1.5.5.2" xref="S3.E2.m1.5.5.2.cmml">=</mo><mrow id="S3.E2.m1.5.5.1" xref="S3.E2.m1.5.5.1.cmml"><munder id="S3.E2.m1.5.5.1.2" xref="S3.E2.m1.5.5.1.2.cmml"><mo maxsize="80%" minsize="80%" movablelimits="false" stretchy="true" id="S3.E2.m1.5.5.1.2.2" xref="S3.E2.m1.5.5.1.2.2.cmml">∑</mo><mrow id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><mi mathsize="80%" id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.4.cmml">p</mi><mo mathsize="80%" id="S3.E2.m1.2.2.2.3" xref="S3.E2.m1.2.2.2.3.cmml">∈</mo><mrow id="S3.E2.m1.2.2.2.5.2" xref="S3.E2.m1.2.2.2.5.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E2.m1.2.2.2.5.2.1" xref="S3.E2.m1.2.2.2.5.1.cmml">{</mo><mi mathsize="80%" id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">a</mi><mo mathsize="80%" id="S3.E2.m1.2.2.2.5.2.2" xref="S3.E2.m1.2.2.2.5.1.cmml">,</mo><mi mathsize="80%" id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">v</mi><mo maxsize="80%" minsize="80%" id="S3.E2.m1.2.2.2.5.2.3" xref="S3.E2.m1.2.2.2.5.1.cmml">}</mo></mrow></mrow></munder><mrow id="S3.E2.m1.5.5.1.1" xref="S3.E2.m1.5.5.1.1.cmml"><mfrac id="S3.E2.m1.5.5.1.1.3" xref="S3.E2.m1.5.5.1.1.3.cmml"><mn mathsize="80%" id="S3.E2.m1.5.5.1.1.3.2" xref="S3.E2.m1.5.5.1.1.3.2.cmml">1</mn><mi mathsize="80%" id="S3.E2.m1.5.5.1.1.3.3" xref="S3.E2.m1.5.5.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.2" xref="S3.E2.m1.5.5.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.5.5.1.1.1" xref="S3.E2.m1.5.5.1.1.1.cmml"><munderover id="S3.E2.m1.5.5.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.2.cmml"><mo maxsize="80%" minsize="80%" movablelimits="false" rspace="0em" stretchy="true" id="S3.E2.m1.5.5.1.1.1.2.2.2" xref="S3.E2.m1.5.5.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.5.5.1.1.1.2.2.3" xref="S3.E2.m1.5.5.1.1.1.2.2.3.cmml"><mi mathsize="80%" id="S3.E2.m1.5.5.1.1.1.2.2.3.2" xref="S3.E2.m1.5.5.1.1.1.2.2.3.2.cmml">i</mi><mo mathsize="80%" id="S3.E2.m1.5.5.1.1.1.2.2.3.1" xref="S3.E2.m1.5.5.1.1.1.2.2.3.1.cmml">=</mo><mn mathsize="80%" id="S3.E2.m1.5.5.1.1.1.2.2.3.3" xref="S3.E2.m1.5.5.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi mathsize="80%" id="S3.E2.m1.5.5.1.1.1.2.3" xref="S3.E2.m1.5.5.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E2.m1.5.5.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="S3.E2.m1.5.5.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.cmml"><mmultiscripts id="S3.E2.m1.5.5.1.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.cmml"><mi mathsize="80%" id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.2.cmml">𝒙</mi><msub id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.cmml"><mi mathsize="80%" id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.2.cmml">𝒑</mi><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.cmml"><mi mathsize="80%" id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.2.cmml">𝒎</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.3.cmml">𝒔</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.1a" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.4" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.4.cmml">𝒌</mi></mrow></msub><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1.2a" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.cmml"></mrow><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1.2b" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.cmml"></mrow><mrow id="S3.E2.m1.3.3.1.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="S3.E2.m1.3.3.1.3.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.cmml">(</mo><mi mathsize="80%" id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml">i</mi><mo maxsize="80%" minsize="80%" id="S3.E2.m1.3.3.1.3.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.cmml">)</mo></mrow></mmultiscripts><mo mathsize="80%" id="S3.E2.m1.5.5.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.cmml">−</mo><mmultiscripts id="S3.E2.m1.5.5.1.1.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.2.cmml"><mi mathsize="80%" id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.2.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.2.2.cmml">𝒙</mi><mo class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.2.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.2.1.cmml">^</mo></mover><msub id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.cmml"><mi mathsize="80%" id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.2.cmml">𝒑</mi><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.cmml"><mi mathsize="80%" id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.2.cmml">𝒎</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.3.cmml">𝒔</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.1a" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.4" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.4.cmml">𝒌</mi></mrow></msub><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1.3a" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.cmml"></mrow><mrow id="S3.E2.m1.5.5.1.1.1.1.1.1.3b" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.cmml"></mrow><mrow id="S3.E2.m1.4.4.1.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.cmml"><mo maxsize="80%" minsize="80%" id="S3.E2.m1.4.4.1.3.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.cmml">(</mo><mi mathsize="80%" id="S3.E2.m1.4.4.1.1" xref="S3.E2.m1.4.4.1.1.cmml">i</mi><mo maxsize="80%" minsize="80%" id="S3.E2.m1.4.4.1.3.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.cmml">)</mo></mrow></mmultiscripts></mrow><mo maxsize="80%" minsize="80%" id="S3.E2.m1.5.5.1.1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.1.1.2.1.cmml">‖</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5"><eq id="S3.E2.m1.5.5.2.cmml" xref="S3.E2.m1.5.5.2"></eq><apply id="S3.E2.m1.5.5.3.cmml" xref="S3.E2.m1.5.5.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.3.1.cmml" xref="S3.E2.m1.5.5.3">subscript</csymbol><ci id="S3.E2.m1.5.5.3.2.cmml" xref="S3.E2.m1.5.5.3.2">ℒ</ci><apply id="S3.E2.m1.5.5.3.3.cmml" xref="S3.E2.m1.5.5.3.3"><times id="S3.E2.m1.5.5.3.3.1.cmml" xref="S3.E2.m1.5.5.3.3.1"></times><ci id="S3.E2.m1.5.5.3.3.2.cmml" xref="S3.E2.m1.5.5.3.3.2">𝑟</ci><ci id="S3.E2.m1.5.5.3.3.3.cmml" xref="S3.E2.m1.5.5.3.3.3">𝑒</ci><ci id="S3.E2.m1.5.5.3.3.4.cmml" xref="S3.E2.m1.5.5.3.3.4">𝑐</ci></apply></apply><apply id="S3.E2.m1.5.5.1.cmml" xref="S3.E2.m1.5.5.1"><apply id="S3.E2.m1.5.5.1.2.cmml" xref="S3.E2.m1.5.5.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.2.1.cmml" xref="S3.E2.m1.5.5.1.2">subscript</csymbol><sum id="S3.E2.m1.5.5.1.2.2.cmml" xref="S3.E2.m1.5.5.1.2.2"></sum><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><in id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.3"></in><ci id="S3.E2.m1.2.2.2.4.cmml" xref="S3.E2.m1.2.2.2.4">𝑝</ci><set id="S3.E2.m1.2.2.2.5.1.cmml" xref="S3.E2.m1.2.2.2.5.2"><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑎</ci><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">𝑣</ci></set></apply></apply><apply id="S3.E2.m1.5.5.1.1.cmml" xref="S3.E2.m1.5.5.1.1"><times id="S3.E2.m1.5.5.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.2"></times><apply id="S3.E2.m1.5.5.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.3"><divide id="S3.E2.m1.5.5.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.3"></divide><cn type="integer" id="S3.E2.m1.5.5.1.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.3.2">1</cn><ci id="S3.E2.m1.5.5.1.1.3.3.cmml" xref="S3.E2.m1.5.5.1.1.3.3">𝑁</ci></apply><apply id="S3.E2.m1.5.5.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1"><apply id="S3.E2.m1.5.5.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.5.5.1.1.1.2.2.cmml" xref="S3.E2.m1.5.5.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.5.5.1.1.1.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.5.5.1.1.1.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.1.2.2.3"><eq id="S3.E2.m1.5.5.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.5.5.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E2.m1.5.5.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.5.5.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.5.5.1.1.1.2.3.cmml" xref="S3.E2.m1.5.5.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E2.m1.5.5.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.5.5.1.1.1.1.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.2">norm</csymbol><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1"><minus id="S3.E2.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.2">𝒙</ci><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.2">𝒑</ci><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3"><times id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.1"></times><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.2">𝒎</ci><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.3">𝒔</ci><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.4.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.2.2.3.3.4">𝒌</ci></apply></apply></apply><ci id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1">𝑖</ci></apply><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.2"><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.2.1">bold-^</ci><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.2.2">𝒙</ci></apply><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.2">𝒑</ci><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3"><times id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.1"></times><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.2">𝒎</ci><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.3.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.3">𝒔</ci><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.4.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.3.2.3.3.4">𝒌</ci></apply></apply></apply><ci id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1.1">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">\footnotesize\mathcal{L}_{rec}=\sum_{p\in\{a,v\}}\frac{1}{N}\sum_{i=1}^{N}\|{\bm{x_{p_{msk}}}^{(i)}}-\bm{\hat{x}_{p_{msk}}}^{(i)}\|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p8.7" class="ltx_p">For the adversarial loss, <math id="S3.SS2.p8.7.m1.1" class="ltx_Math" alttext="\mathcal{L}_{adv}" display="inline"><semantics id="S3.SS2.p8.7.m1.1a"><msub id="S3.SS2.p8.7.m1.1.1" xref="S3.SS2.p8.7.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p8.7.m1.1.1.2" xref="S3.SS2.p8.7.m1.1.1.2.cmml">ℒ</mi><mrow id="S3.SS2.p8.7.m1.1.1.3" xref="S3.SS2.p8.7.m1.1.1.3.cmml"><mi id="S3.SS2.p8.7.m1.1.1.3.2" xref="S3.SS2.p8.7.m1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p8.7.m1.1.1.3.1" xref="S3.SS2.p8.7.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p8.7.m1.1.1.3.3" xref="S3.SS2.p8.7.m1.1.1.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p8.7.m1.1.1.3.1a" xref="S3.SS2.p8.7.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p8.7.m1.1.1.3.4" xref="S3.SS2.p8.7.m1.1.1.3.4.cmml">v</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.7.m1.1b"><apply id="S3.SS2.p8.7.m1.1.1.cmml" xref="S3.SS2.p8.7.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.7.m1.1.1.1.cmml" xref="S3.SS2.p8.7.m1.1.1">subscript</csymbol><ci id="S3.SS2.p8.7.m1.1.1.2.cmml" xref="S3.SS2.p8.7.m1.1.1.2">ℒ</ci><apply id="S3.SS2.p8.7.m1.1.1.3.cmml" xref="S3.SS2.p8.7.m1.1.1.3"><times id="S3.SS2.p8.7.m1.1.1.3.1.cmml" xref="S3.SS2.p8.7.m1.1.1.3.1"></times><ci id="S3.SS2.p8.7.m1.1.1.3.2.cmml" xref="S3.SS2.p8.7.m1.1.1.3.2">𝑎</ci><ci id="S3.SS2.p8.7.m1.1.1.3.3.cmml" xref="S3.SS2.p8.7.m1.1.1.3.3">𝑑</ci><ci id="S3.SS2.p8.7.m1.1.1.3.4.cmml" xref="S3.SS2.p8.7.m1.1.1.3.4">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.7.m1.1c">\mathcal{L}_{adv}</annotation></semantics></math>, we use the Wasserstein GAN loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> to supplement the reconstruction loss by enhancing the features captured in the reconstructions of each modality. Similar to the reconstruction loss, the adversarial loss is computed only on the masked tokens:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.5" class="ltx_Math" alttext="\footnotesize\mathcal{L}_{adv}^{(G)}=-\mkern-12.0mu\sum_{p\in\{a,v\}}\frac{1}{N}\sum_{i=1}^{N}D_{p}(\bm{\hat{x}_{p_{msk}}}^{(i)})" display="block"><semantics id="S3.E3.m1.5a"><mrow id="S3.E3.m1.5.5" xref="S3.E3.m1.5.5.cmml"><msubsup id="S3.E3.m1.5.5.3" xref="S3.E3.m1.5.5.3.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="80%" id="S3.E3.m1.5.5.3.2.2" xref="S3.E3.m1.5.5.3.2.2.cmml">ℒ</mi><mrow id="S3.E3.m1.5.5.3.2.3" xref="S3.E3.m1.5.5.3.2.3.cmml"><mi mathsize="80%" id="S3.E3.m1.5.5.3.2.3.2" xref="S3.E3.m1.5.5.3.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.3.2.3.1" xref="S3.E3.m1.5.5.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E3.m1.5.5.3.2.3.3" xref="S3.E3.m1.5.5.3.2.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.3.2.3.1a" xref="S3.E3.m1.5.5.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E3.m1.5.5.3.2.3.4" xref="S3.E3.m1.5.5.3.2.3.4.cmml">v</mi></mrow><mrow id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.5.5.3.cmml"><mo maxsize="80%" minsize="80%" id="S3.E3.m1.1.1.1.3.1" xref="S3.E3.m1.5.5.3.cmml">(</mo><mi mathsize="80%" id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">G</mi><mo maxsize="80%" minsize="80%" id="S3.E3.m1.1.1.1.3.2" xref="S3.E3.m1.5.5.3.cmml">)</mo></mrow></msubsup><mo mathsize="80%" id="S3.E3.m1.5.5.2" xref="S3.E3.m1.5.5.2.cmml">=</mo><mrow id="S3.E3.m1.5.5.1" xref="S3.E3.m1.5.5.1.cmml"><mo mathsize="80%" id="S3.E3.m1.5.5.1a" xref="S3.E3.m1.5.5.1.cmml">−</mo><mrow id="S3.E3.m1.5.5.1.1" xref="S3.E3.m1.5.5.1.1.cmml"><munder id="S3.E3.m1.5.5.1.1.2" xref="S3.E3.m1.5.5.1.1.2.cmml"><mo maxsize="80%" minsize="80%" movablelimits="false" stretchy="true" id="S3.E3.m1.5.5.1.1.2.2" xref="S3.E3.m1.5.5.1.1.2.2.cmml">∑</mo><mrow id="S3.E3.m1.3.3.2" xref="S3.E3.m1.3.3.2.cmml"><mi mathsize="80%" id="S3.E3.m1.3.3.2.4" xref="S3.E3.m1.3.3.2.4.cmml">p</mi><mo mathsize="80%" id="S3.E3.m1.3.3.2.3" xref="S3.E3.m1.3.3.2.3.cmml">∈</mo><mrow id="S3.E3.m1.3.3.2.5.2" xref="S3.E3.m1.3.3.2.5.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E3.m1.3.3.2.5.2.1" xref="S3.E3.m1.3.3.2.5.1.cmml">{</mo><mi mathsize="80%" id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml">a</mi><mo mathsize="80%" id="S3.E3.m1.3.3.2.5.2.2" xref="S3.E3.m1.3.3.2.5.1.cmml">,</mo><mi mathsize="80%" id="S3.E3.m1.3.3.2.2" xref="S3.E3.m1.3.3.2.2.cmml">v</mi><mo maxsize="80%" minsize="80%" id="S3.E3.m1.3.3.2.5.2.3" xref="S3.E3.m1.3.3.2.5.1.cmml">}</mo></mrow></mrow></munder><mrow id="S3.E3.m1.5.5.1.1.1" xref="S3.E3.m1.5.5.1.1.1.cmml"><mfrac id="S3.E3.m1.5.5.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.3.cmml"><mn mathsize="80%" id="S3.E3.m1.5.5.1.1.1.3.2" xref="S3.E3.m1.5.5.1.1.1.3.2.cmml">1</mn><mi mathsize="80%" id="S3.E3.m1.5.5.1.1.1.3.3" xref="S3.E3.m1.5.5.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.5.5.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.cmml"><munderover id="S3.E3.m1.5.5.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.2.cmml"><mo maxsize="80%" minsize="80%" movablelimits="false" stretchy="true" id="S3.E3.m1.5.5.1.1.1.1.2.2.2" xref="S3.E3.m1.5.5.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E3.m1.5.5.1.1.1.1.2.2.3" xref="S3.E3.m1.5.5.1.1.1.1.2.2.3.cmml"><mi mathsize="80%" id="S3.E3.m1.5.5.1.1.1.1.2.2.3.2" xref="S3.E3.m1.5.5.1.1.1.1.2.2.3.2.cmml">i</mi><mo mathsize="80%" id="S3.E3.m1.5.5.1.1.1.1.2.2.3.1" xref="S3.E3.m1.5.5.1.1.1.1.2.2.3.1.cmml">=</mo><mn mathsize="80%" id="S3.E3.m1.5.5.1.1.1.1.2.2.3.3" xref="S3.E3.m1.5.5.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi mathsize="80%" id="S3.E3.m1.5.5.1.1.1.1.2.3" xref="S3.E3.m1.5.5.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E3.m1.5.5.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.cmml"><msub id="S3.E3.m1.5.5.1.1.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.1.1.3.cmml"><mi mathsize="80%" id="S3.E3.m1.5.5.1.1.1.1.1.3.2" xref="S3.E3.m1.5.5.1.1.1.1.1.3.2.cmml">D</mi><mi mathsize="80%" id="S3.E3.m1.5.5.1.1.1.1.1.3.3" xref="S3.E3.m1.5.5.1.1.1.1.1.3.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.1.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.5.5.1.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml">(</mo><mmultiscripts id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.2.cmml"><mi mathsize="80%" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.2.2.cmml">𝒙</mi><mo class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.2.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><msub id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.cmml"><mi mathsize="80%" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.2.cmml">𝒑</mi><mrow id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.cmml"><mi mathsize="80%" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.2.cmml">𝒎</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.3" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.3.cmml">𝒔</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.1a" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.4" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.4.cmml">𝒌</mi></mrow></msub><mrow id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1a" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml"></mrow><mrow id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1b" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml"></mrow><mrow id="S3.E3.m1.4.4.1.3" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E3.m1.4.4.1.3.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml">(</mo><mi mathsize="80%" id="S3.E3.m1.4.4.1.1" xref="S3.E3.m1.4.4.1.1.cmml">i</mi><mo maxsize="80%" minsize="80%" id="S3.E3.m1.4.4.1.3.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mmultiscripts><mo maxsize="80%" minsize="80%" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.5b"><apply id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5"><eq id="S3.E3.m1.5.5.2.cmml" xref="S3.E3.m1.5.5.2"></eq><apply id="S3.E3.m1.5.5.3.cmml" xref="S3.E3.m1.5.5.3"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.3.1.cmml" xref="S3.E3.m1.5.5.3">superscript</csymbol><apply id="S3.E3.m1.5.5.3.2.cmml" xref="S3.E3.m1.5.5.3"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.3.2.1.cmml" xref="S3.E3.m1.5.5.3">subscript</csymbol><ci id="S3.E3.m1.5.5.3.2.2.cmml" xref="S3.E3.m1.5.5.3.2.2">ℒ</ci><apply id="S3.E3.m1.5.5.3.2.3.cmml" xref="S3.E3.m1.5.5.3.2.3"><times id="S3.E3.m1.5.5.3.2.3.1.cmml" xref="S3.E3.m1.5.5.3.2.3.1"></times><ci id="S3.E3.m1.5.5.3.2.3.2.cmml" xref="S3.E3.m1.5.5.3.2.3.2">𝑎</ci><ci id="S3.E3.m1.5.5.3.2.3.3.cmml" xref="S3.E3.m1.5.5.3.2.3.3">𝑑</ci><ci id="S3.E3.m1.5.5.3.2.3.4.cmml" xref="S3.E3.m1.5.5.3.2.3.4">𝑣</ci></apply></apply><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">𝐺</ci></apply><apply id="S3.E3.m1.5.5.1.cmml" xref="S3.E3.m1.5.5.1"><minus id="S3.E3.m1.5.5.1.2.cmml" xref="S3.E3.m1.5.5.1"></minus><apply id="S3.E3.m1.5.5.1.1.cmml" xref="S3.E3.m1.5.5.1.1"><apply id="S3.E3.m1.5.5.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.2.1.cmml" xref="S3.E3.m1.5.5.1.1.2">subscript</csymbol><sum id="S3.E3.m1.5.5.1.1.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2"></sum><apply id="S3.E3.m1.3.3.2.cmml" xref="S3.E3.m1.3.3.2"><in id="S3.E3.m1.3.3.2.3.cmml" xref="S3.E3.m1.3.3.2.3"></in><ci id="S3.E3.m1.3.3.2.4.cmml" xref="S3.E3.m1.3.3.2.4">𝑝</ci><set id="S3.E3.m1.3.3.2.5.1.cmml" xref="S3.E3.m1.3.3.2.5.2"><ci id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1.1">𝑎</ci><ci id="S3.E3.m1.3.3.2.2.cmml" xref="S3.E3.m1.3.3.2.2">𝑣</ci></set></apply></apply><apply id="S3.E3.m1.5.5.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1"><times id="S3.E3.m1.5.5.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.2"></times><apply id="S3.E3.m1.5.5.1.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.1.3"><divide id="S3.E3.m1.5.5.1.1.1.3.1.cmml" xref="S3.E3.m1.5.5.1.1.1.3"></divide><cn type="integer" id="S3.E3.m1.5.5.1.1.1.3.2.cmml" xref="S3.E3.m1.5.5.1.1.1.3.2">1</cn><ci id="S3.E3.m1.5.5.1.1.1.3.3.cmml" xref="S3.E3.m1.5.5.1.1.1.3.3">𝑁</ci></apply><apply id="S3.E3.m1.5.5.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1"><apply id="S3.E3.m1.5.5.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.2.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2">superscript</csymbol><apply id="S3.E3.m1.5.5.1.1.1.1.2.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2">subscript</csymbol><sum id="S3.E3.m1.5.5.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2.2.2"></sum><apply id="S3.E3.m1.5.5.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2.2.3"><eq id="S3.E3.m1.5.5.1.1.1.1.2.2.3.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2.2.3.1"></eq><ci id="S3.E3.m1.5.5.1.1.1.1.2.2.3.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E3.m1.5.5.1.1.1.1.2.2.3.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.5.5.1.1.1.1.2.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E3.m1.5.5.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1"><times id="S3.E3.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.2"></times><apply id="S3.E3.m1.5.5.1.1.1.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.5.5.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.3.2">𝐷</ci><ci id="S3.E3.m1.5.5.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.3.3">𝑝</ci></apply><apply id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.2"><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.2.1">bold-^</ci><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.2.2">𝒙</ci></apply><apply id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.2">𝒑</ci><apply id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3"><times id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.1"></times><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.2">𝒎</ci><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.3">𝒔</ci><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.4.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.2.3.3.4">𝒌</ci></apply></apply></apply><ci id="S3.E3.m1.4.4.1.1.cmml" xref="S3.E3.m1.4.4.1.1">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.5c">\footnotesize\mathcal{L}_{adv}^{(G)}=-\mkern-12.0mu\sum_{p\in\{a,v\}}\frac{1}{N}\sum_{i=1}^{N}D_{p}(\bm{\hat{x}_{p_{msk}}}^{(i)})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.6" class="ltx_Math" alttext="\footnotesize\mathcal{L}_{adv}^{(D)}=\sum_{p\in\{a,v\}}\frac{1}{N}\sum_{i=1}^{N}(D_{p}(\bm{\hat{x}_{p_{msk}}}^{(i)})-D_{p}(\bm{x_{p_{msk}}}^{(i)}))" display="block"><semantics id="S3.E4.m1.6a"><mrow id="S3.E4.m1.6.6" xref="S3.E4.m1.6.6.cmml"><msubsup id="S3.E4.m1.6.6.3" xref="S3.E4.m1.6.6.3.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="80%" id="S3.E4.m1.6.6.3.2.2" xref="S3.E4.m1.6.6.3.2.2.cmml">ℒ</mi><mrow id="S3.E4.m1.6.6.3.2.3" xref="S3.E4.m1.6.6.3.2.3.cmml"><mi mathsize="80%" id="S3.E4.m1.6.6.3.2.3.2" xref="S3.E4.m1.6.6.3.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.3.2.3.1" xref="S3.E4.m1.6.6.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E4.m1.6.6.3.2.3.3" xref="S3.E4.m1.6.6.3.2.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.3.2.3.1a" xref="S3.E4.m1.6.6.3.2.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E4.m1.6.6.3.2.3.4" xref="S3.E4.m1.6.6.3.2.3.4.cmml">v</mi></mrow><mrow id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.6.6.3.cmml"><mo maxsize="80%" minsize="80%" id="S3.E4.m1.1.1.1.3.1" xref="S3.E4.m1.6.6.3.cmml">(</mo><mi mathsize="80%" id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml">D</mi><mo maxsize="80%" minsize="80%" id="S3.E4.m1.1.1.1.3.2" xref="S3.E4.m1.6.6.3.cmml">)</mo></mrow></msubsup><mo mathsize="80%" rspace="0.111em" id="S3.E4.m1.6.6.2" xref="S3.E4.m1.6.6.2.cmml">=</mo><mrow id="S3.E4.m1.6.6.1" xref="S3.E4.m1.6.6.1.cmml"><munder id="S3.E4.m1.6.6.1.2" xref="S3.E4.m1.6.6.1.2.cmml"><mo maxsize="80%" minsize="80%" movablelimits="false" stretchy="true" id="S3.E4.m1.6.6.1.2.2" xref="S3.E4.m1.6.6.1.2.2.cmml">∑</mo><mrow id="S3.E4.m1.3.3.2" xref="S3.E4.m1.3.3.2.cmml"><mi mathsize="80%" id="S3.E4.m1.3.3.2.4" xref="S3.E4.m1.3.3.2.4.cmml">p</mi><mo mathsize="80%" id="S3.E4.m1.3.3.2.3" xref="S3.E4.m1.3.3.2.3.cmml">∈</mo><mrow id="S3.E4.m1.3.3.2.5.2" xref="S3.E4.m1.3.3.2.5.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E4.m1.3.3.2.5.2.1" xref="S3.E4.m1.3.3.2.5.1.cmml">{</mo><mi mathsize="80%" id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml">a</mi><mo mathsize="80%" id="S3.E4.m1.3.3.2.5.2.2" xref="S3.E4.m1.3.3.2.5.1.cmml">,</mo><mi mathsize="80%" id="S3.E4.m1.3.3.2.2" xref="S3.E4.m1.3.3.2.2.cmml">v</mi><mo maxsize="80%" minsize="80%" id="S3.E4.m1.3.3.2.5.2.3" xref="S3.E4.m1.3.3.2.5.1.cmml">}</mo></mrow></mrow></munder><mrow id="S3.E4.m1.6.6.1.1" xref="S3.E4.m1.6.6.1.1.cmml"><mfrac id="S3.E4.m1.6.6.1.1.3" xref="S3.E4.m1.6.6.1.1.3.cmml"><mn mathsize="80%" id="S3.E4.m1.6.6.1.1.3.2" xref="S3.E4.m1.6.6.1.1.3.2.cmml">1</mn><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.3.3" xref="S3.E4.m1.6.6.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.2" xref="S3.E4.m1.6.6.1.1.2.cmml">​</mo><mrow id="S3.E4.m1.6.6.1.1.1" xref="S3.E4.m1.6.6.1.1.1.cmml"><munderover id="S3.E4.m1.6.6.1.1.1.2" xref="S3.E4.m1.6.6.1.1.1.2.cmml"><mo maxsize="80%" minsize="80%" movablelimits="false" rspace="0em" stretchy="true" id="S3.E4.m1.6.6.1.1.1.2.2.2" xref="S3.E4.m1.6.6.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E4.m1.6.6.1.1.1.2.2.3" xref="S3.E4.m1.6.6.1.1.1.2.2.3.cmml"><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.2.2.3.2" xref="S3.E4.m1.6.6.1.1.1.2.2.3.2.cmml">i</mi><mo mathsize="80%" id="S3.E4.m1.6.6.1.1.1.2.2.3.1" xref="S3.E4.m1.6.6.1.1.1.2.2.3.1.cmml">=</mo><mn mathsize="80%" id="S3.E4.m1.6.6.1.1.1.2.2.3.3" xref="S3.E4.m1.6.6.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.2.3" xref="S3.E4.m1.6.6.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E4.m1.6.6.1.1.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.6.6.1.1.1.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.6.6.1.1.1.1.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.6.6.1.1.1.1.1.1.1.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.3.cmml"><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.3.2.cmml">D</mi><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.3.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mmultiscripts id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">𝒙</mi><mo class="ltx_mathvariant_bold" mathsize="80%" mathvariant="bold" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><msub id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">𝒑</mi><mrow id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml"><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.2.cmml">𝒎</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.3.cmml">𝒔</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.1a" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.4" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.4.cmml">𝒌</mi></mrow></msub><mrow id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1a" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml"></mrow><mrow id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1b" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml"></mrow><mrow id="S3.E4.m1.4.4.1.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E4.m1.4.4.1.3.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi mathsize="80%" id="S3.E4.m1.4.4.1.1" xref="S3.E4.m1.4.4.1.1.cmml">i</mi><mo maxsize="80%" minsize="80%" id="S3.E4.m1.4.4.1.3.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mmultiscripts><mo maxsize="80%" minsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S3.E4.m1.6.6.1.1.1.1.1.1.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.cmml"><msub id="S3.E4.m1.6.6.1.1.1.1.1.1.2.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.3.cmml"><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.3.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.3.2.cmml">D</mi><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.3.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.3.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.2.cmml">​</mo><mrow id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.cmml">(</mo><mmultiscripts id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.cmml"><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.2.cmml">𝒙</mi><msub id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.cmml"><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.2.cmml">𝒑</mi><mrow id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.cmml"><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.2.cmml">𝒎</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.3.cmml">𝒔</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.1a" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.1.cmml">​</mo><mi mathsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.4" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.4.cmml">𝒌</mi></mrow></msub><mrow id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1a" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.cmml"></mrow><mrow id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1b" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.cmml"></mrow><mrow id="S3.E4.m1.5.5.1.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.E4.m1.5.5.1.3.1" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.cmml">(</mo><mi mathsize="80%" id="S3.E4.m1.5.5.1.1" xref="S3.E4.m1.5.5.1.1.cmml">i</mi><mo maxsize="80%" minsize="80%" id="S3.E4.m1.5.5.1.3.2" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mmultiscripts><mo maxsize="80%" minsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo maxsize="80%" minsize="80%" id="S3.E4.m1.6.6.1.1.1.1.1.3" xref="S3.E4.m1.6.6.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.6b"><apply id="S3.E4.m1.6.6.cmml" xref="S3.E4.m1.6.6"><eq id="S3.E4.m1.6.6.2.cmml" xref="S3.E4.m1.6.6.2"></eq><apply id="S3.E4.m1.6.6.3.cmml" xref="S3.E4.m1.6.6.3"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.3.1.cmml" xref="S3.E4.m1.6.6.3">superscript</csymbol><apply id="S3.E4.m1.6.6.3.2.cmml" xref="S3.E4.m1.6.6.3"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.3.2.1.cmml" xref="S3.E4.m1.6.6.3">subscript</csymbol><ci id="S3.E4.m1.6.6.3.2.2.cmml" xref="S3.E4.m1.6.6.3.2.2">ℒ</ci><apply id="S3.E4.m1.6.6.3.2.3.cmml" xref="S3.E4.m1.6.6.3.2.3"><times id="S3.E4.m1.6.6.3.2.3.1.cmml" xref="S3.E4.m1.6.6.3.2.3.1"></times><ci id="S3.E4.m1.6.6.3.2.3.2.cmml" xref="S3.E4.m1.6.6.3.2.3.2">𝑎</ci><ci id="S3.E4.m1.6.6.3.2.3.3.cmml" xref="S3.E4.m1.6.6.3.2.3.3">𝑑</ci><ci id="S3.E4.m1.6.6.3.2.3.4.cmml" xref="S3.E4.m1.6.6.3.2.3.4">𝑣</ci></apply></apply><ci id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1">𝐷</ci></apply><apply id="S3.E4.m1.6.6.1.cmml" xref="S3.E4.m1.6.6.1"><apply id="S3.E4.m1.6.6.1.2.cmml" xref="S3.E4.m1.6.6.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.2.1.cmml" xref="S3.E4.m1.6.6.1.2">subscript</csymbol><sum id="S3.E4.m1.6.6.1.2.2.cmml" xref="S3.E4.m1.6.6.1.2.2"></sum><apply id="S3.E4.m1.3.3.2.cmml" xref="S3.E4.m1.3.3.2"><in id="S3.E4.m1.3.3.2.3.cmml" xref="S3.E4.m1.3.3.2.3"></in><ci id="S3.E4.m1.3.3.2.4.cmml" xref="S3.E4.m1.3.3.2.4">𝑝</ci><set id="S3.E4.m1.3.3.2.5.1.cmml" xref="S3.E4.m1.3.3.2.5.2"><ci id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1.1">𝑎</ci><ci id="S3.E4.m1.3.3.2.2.cmml" xref="S3.E4.m1.3.3.2.2">𝑣</ci></set></apply></apply><apply id="S3.E4.m1.6.6.1.1.cmml" xref="S3.E4.m1.6.6.1.1"><times id="S3.E4.m1.6.6.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.2"></times><apply id="S3.E4.m1.6.6.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.3"><divide id="S3.E4.m1.6.6.1.1.3.1.cmml" xref="S3.E4.m1.6.6.1.1.3"></divide><cn type="integer" id="S3.E4.m1.6.6.1.1.3.2.cmml" xref="S3.E4.m1.6.6.1.1.3.2">1</cn><ci id="S3.E4.m1.6.6.1.1.3.3.cmml" xref="S3.E4.m1.6.6.1.1.3.3">𝑁</ci></apply><apply id="S3.E4.m1.6.6.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1"><apply id="S3.E4.m1.6.6.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.2.1.cmml" xref="S3.E4.m1.6.6.1.1.1.2">superscript</csymbol><apply id="S3.E4.m1.6.6.1.1.1.2.2.cmml" xref="S3.E4.m1.6.6.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.2.2.1.cmml" xref="S3.E4.m1.6.6.1.1.1.2">subscript</csymbol><sum id="S3.E4.m1.6.6.1.1.1.2.2.2.cmml" xref="S3.E4.m1.6.6.1.1.1.2.2.2"></sum><apply id="S3.E4.m1.6.6.1.1.1.2.2.3.cmml" xref="S3.E4.m1.6.6.1.1.1.2.2.3"><eq id="S3.E4.m1.6.6.1.1.1.2.2.3.1.cmml" xref="S3.E4.m1.6.6.1.1.1.2.2.3.1"></eq><ci id="S3.E4.m1.6.6.1.1.1.2.2.3.2.cmml" xref="S3.E4.m1.6.6.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E4.m1.6.6.1.1.1.2.2.3.3.cmml" xref="S3.E4.m1.6.6.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E4.m1.6.6.1.1.1.2.3.cmml" xref="S3.E4.m1.6.6.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1"><minus id="S3.E4.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.3"></minus><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1"><times id="S3.E4.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.2"></times><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.3.2">𝐷</ci><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.3.3">𝑝</ci></apply><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2"><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.1">bold-^</ci><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2">𝒙</ci></apply><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.2">𝒑</ci><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3"><times id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.1"></times><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.2">𝒎</ci><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.3">𝒔</ci><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.4.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.3.3.4">𝒌</ci></apply></apply></apply><ci id="S3.E4.m1.4.4.1.1.cmml" xref="S3.E4.m1.4.4.1.1">𝑖</ci></apply></apply><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2"><times id="S3.E4.m1.6.6.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.2"></times><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.3.2">𝐷</ci><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.3.3">𝑝</ci></apply><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1">superscript</csymbol><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.2">𝒙</ci><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3">subscript</csymbol><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.2">𝒑</ci><apply id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3"><times id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.1.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.1"></times><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.2.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.2">𝒎</ci><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.3.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.3">𝒔</ci><ci id="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.4.cmml" xref="S3.E4.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.3.3.4">𝒌</ci></apply></apply></apply><ci id="S3.E4.m1.5.5.1.1.cmml" xref="S3.E4.m1.5.5.1.1">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.6c">\footnotesize\mathcal{L}_{adv}^{(D)}=\sum_{p\in\{a,v\}}\frac{1}{N}\sum_{i=1}^{N}(D_{p}(\bm{\hat{x}_{p_{msk}}}^{(i)})-D_{p}(\bm{x_{p_{msk}}}^{(i)}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p8.10" class="ltx_p">Here, <math id="S3.SS2.p8.8.m1.1" class="ltx_Math" alttext="D_{p}" display="inline"><semantics id="S3.SS2.p8.8.m1.1a"><msub id="S3.SS2.p8.8.m1.1.1" xref="S3.SS2.p8.8.m1.1.1.cmml"><mi id="S3.SS2.p8.8.m1.1.1.2" xref="S3.SS2.p8.8.m1.1.1.2.cmml">D</mi><mi id="S3.SS2.p8.8.m1.1.1.3" xref="S3.SS2.p8.8.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.8.m1.1b"><apply id="S3.SS2.p8.8.m1.1.1.cmml" xref="S3.SS2.p8.8.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.8.m1.1.1.1.cmml" xref="S3.SS2.p8.8.m1.1.1">subscript</csymbol><ci id="S3.SS2.p8.8.m1.1.1.2.cmml" xref="S3.SS2.p8.8.m1.1.1.2">𝐷</ci><ci id="S3.SS2.p8.8.m1.1.1.3.cmml" xref="S3.SS2.p8.8.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.8.m1.1c">D_{p}</annotation></semantics></math> denotes the discriminator of each modality, and the <math id="S3.SS2.p8.9.m2.1" class="ltx_Math" alttext="\mathcal{L}_{adv}^{(D)}" display="inline"><semantics id="S3.SS2.p8.9.m2.1a"><msubsup id="S3.SS2.p8.9.m2.1.2" xref="S3.SS2.p8.9.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p8.9.m2.1.2.2.2" xref="S3.SS2.p8.9.m2.1.2.2.2.cmml">ℒ</mi><mrow id="S3.SS2.p8.9.m2.1.2.2.3" xref="S3.SS2.p8.9.m2.1.2.2.3.cmml"><mi id="S3.SS2.p8.9.m2.1.2.2.3.2" xref="S3.SS2.p8.9.m2.1.2.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p8.9.m2.1.2.2.3.1" xref="S3.SS2.p8.9.m2.1.2.2.3.1.cmml">​</mo><mi id="S3.SS2.p8.9.m2.1.2.2.3.3" xref="S3.SS2.p8.9.m2.1.2.2.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p8.9.m2.1.2.2.3.1a" xref="S3.SS2.p8.9.m2.1.2.2.3.1.cmml">​</mo><mi id="S3.SS2.p8.9.m2.1.2.2.3.4" xref="S3.SS2.p8.9.m2.1.2.2.3.4.cmml">v</mi></mrow><mrow id="S3.SS2.p8.9.m2.1.1.1.3" xref="S3.SS2.p8.9.m2.1.2.cmml"><mo stretchy="false" id="S3.SS2.p8.9.m2.1.1.1.3.1" xref="S3.SS2.p8.9.m2.1.2.cmml">(</mo><mi id="S3.SS2.p8.9.m2.1.1.1.1" xref="S3.SS2.p8.9.m2.1.1.1.1.cmml">D</mi><mo stretchy="false" id="S3.SS2.p8.9.m2.1.1.1.3.2" xref="S3.SS2.p8.9.m2.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.9.m2.1b"><apply id="S3.SS2.p8.9.m2.1.2.cmml" xref="S3.SS2.p8.9.m2.1.2"><csymbol cd="ambiguous" id="S3.SS2.p8.9.m2.1.2.1.cmml" xref="S3.SS2.p8.9.m2.1.2">superscript</csymbol><apply id="S3.SS2.p8.9.m2.1.2.2.cmml" xref="S3.SS2.p8.9.m2.1.2"><csymbol cd="ambiguous" id="S3.SS2.p8.9.m2.1.2.2.1.cmml" xref="S3.SS2.p8.9.m2.1.2">subscript</csymbol><ci id="S3.SS2.p8.9.m2.1.2.2.2.cmml" xref="S3.SS2.p8.9.m2.1.2.2.2">ℒ</ci><apply id="S3.SS2.p8.9.m2.1.2.2.3.cmml" xref="S3.SS2.p8.9.m2.1.2.2.3"><times id="S3.SS2.p8.9.m2.1.2.2.3.1.cmml" xref="S3.SS2.p8.9.m2.1.2.2.3.1"></times><ci id="S3.SS2.p8.9.m2.1.2.2.3.2.cmml" xref="S3.SS2.p8.9.m2.1.2.2.3.2">𝑎</ci><ci id="S3.SS2.p8.9.m2.1.2.2.3.3.cmml" xref="S3.SS2.p8.9.m2.1.2.2.3.3">𝑑</ci><ci id="S3.SS2.p8.9.m2.1.2.2.3.4.cmml" xref="S3.SS2.p8.9.m2.1.2.2.3.4">𝑣</ci></apply></apply><ci id="S3.SS2.p8.9.m2.1.1.1.1.cmml" xref="S3.SS2.p8.9.m2.1.1.1.1">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.9.m2.1c">\mathcal{L}_{adv}^{(D)}</annotation></semantics></math> and <math id="S3.SS2.p8.10.m3.1" class="ltx_Math" alttext="\mathcal{L}_{adv}^{(G)}" display="inline"><semantics id="S3.SS2.p8.10.m3.1a"><msubsup id="S3.SS2.p8.10.m3.1.2" xref="S3.SS2.p8.10.m3.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p8.10.m3.1.2.2.2" xref="S3.SS2.p8.10.m3.1.2.2.2.cmml">ℒ</mi><mrow id="S3.SS2.p8.10.m3.1.2.2.3" xref="S3.SS2.p8.10.m3.1.2.2.3.cmml"><mi id="S3.SS2.p8.10.m3.1.2.2.3.2" xref="S3.SS2.p8.10.m3.1.2.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p8.10.m3.1.2.2.3.1" xref="S3.SS2.p8.10.m3.1.2.2.3.1.cmml">​</mo><mi id="S3.SS2.p8.10.m3.1.2.2.3.3" xref="S3.SS2.p8.10.m3.1.2.2.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p8.10.m3.1.2.2.3.1a" xref="S3.SS2.p8.10.m3.1.2.2.3.1.cmml">​</mo><mi id="S3.SS2.p8.10.m3.1.2.2.3.4" xref="S3.SS2.p8.10.m3.1.2.2.3.4.cmml">v</mi></mrow><mrow id="S3.SS2.p8.10.m3.1.1.1.3" xref="S3.SS2.p8.10.m3.1.2.cmml"><mo stretchy="false" id="S3.SS2.p8.10.m3.1.1.1.3.1" xref="S3.SS2.p8.10.m3.1.2.cmml">(</mo><mi id="S3.SS2.p8.10.m3.1.1.1.1" xref="S3.SS2.p8.10.m3.1.1.1.1.cmml">G</mi><mo stretchy="false" id="S3.SS2.p8.10.m3.1.1.1.3.2" xref="S3.SS2.p8.10.m3.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.10.m3.1b"><apply id="S3.SS2.p8.10.m3.1.2.cmml" xref="S3.SS2.p8.10.m3.1.2"><csymbol cd="ambiguous" id="S3.SS2.p8.10.m3.1.2.1.cmml" xref="S3.SS2.p8.10.m3.1.2">superscript</csymbol><apply id="S3.SS2.p8.10.m3.1.2.2.cmml" xref="S3.SS2.p8.10.m3.1.2"><csymbol cd="ambiguous" id="S3.SS2.p8.10.m3.1.2.2.1.cmml" xref="S3.SS2.p8.10.m3.1.2">subscript</csymbol><ci id="S3.SS2.p8.10.m3.1.2.2.2.cmml" xref="S3.SS2.p8.10.m3.1.2.2.2">ℒ</ci><apply id="S3.SS2.p8.10.m3.1.2.2.3.cmml" xref="S3.SS2.p8.10.m3.1.2.2.3"><times id="S3.SS2.p8.10.m3.1.2.2.3.1.cmml" xref="S3.SS2.p8.10.m3.1.2.2.3.1"></times><ci id="S3.SS2.p8.10.m3.1.2.2.3.2.cmml" xref="S3.SS2.p8.10.m3.1.2.2.3.2">𝑎</ci><ci id="S3.SS2.p8.10.m3.1.2.2.3.3.cmml" xref="S3.SS2.p8.10.m3.1.2.2.3.3">𝑑</ci><ci id="S3.SS2.p8.10.m3.1.2.2.3.4.cmml" xref="S3.SS2.p8.10.m3.1.2.2.3.4">𝑣</ci></apply></apply><ci id="S3.SS2.p8.10.m3.1.1.1.1.cmml" xref="S3.SS2.p8.10.m3.1.1.1.1">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.10.m3.1c">\mathcal{L}_{adv}^{(G)}</annotation></semantics></math> represents the adversarial loss during the generator and the discriminator training steps respectively.</p>
</div>
<div id="S3.SS2.p9" class="ltx_para">
<p id="S3.SS2.p9.1" class="ltx_p">The overall training loss for the generative training step is as follows, where the <math id="S3.SS2.p9.1.m1.1" class="ltx_Math" alttext="\lambda_{*}" display="inline"><semantics id="S3.SS2.p9.1.m1.1a"><msub id="S3.SS2.p9.1.m1.1.1" xref="S3.SS2.p9.1.m1.1.1.cmml"><mi id="S3.SS2.p9.1.m1.1.1.2" xref="S3.SS2.p9.1.m1.1.1.2.cmml">λ</mi><mo id="S3.SS2.p9.1.m1.1.1.3" xref="S3.SS2.p9.1.m1.1.1.3.cmml">∗</mo></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.1.m1.1b"><apply id="S3.SS2.p9.1.m1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.1.m1.1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p9.1.m1.1.1.2.cmml" xref="S3.SS2.p9.1.m1.1.1.2">𝜆</ci><times id="S3.SS2.p9.1.m1.1.1.3.cmml" xref="S3.SS2.p9.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.1.m1.1c">\lambda_{*}</annotation></semantics></math> parameters represent the corresponding loss weights:</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\displaystyle\footnotesize\mathcal{L}^{(G)}" display="inline"><semantics id="S3.E5.m1.1a"><msup id="S3.E5.m1.1.2" xref="S3.E5.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="80%" id="S3.E5.m1.1.2.2" xref="S3.E5.m1.1.2.2.cmml">ℒ</mi><mrow id="S3.E5.m1.1.1.1.3" xref="S3.E5.m1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="S3.E5.m1.1.1.1.3.1" xref="S3.E5.m1.1.2.cmml">(</mo><mi mathsize="80%" id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml">G</mi><mo maxsize="80%" minsize="80%" id="S3.E5.m1.1.1.1.3.2" xref="S3.E5.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.2.cmml" xref="S3.E5.m1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.2.1.cmml" xref="S3.E5.m1.1.2">superscript</csymbol><ci id="S3.E5.m1.1.2.2.cmml" xref="S3.E5.m1.1.2.2">ℒ</ci><ci id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle\footnotesize\mathcal{L}^{(G)}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E5.m2.1" class="ltx_Math" alttext="\displaystyle=\lambda_{c}\mathcal{L}_{c}+\lambda_{rec}\mathcal{L}_{rec}+\lambda_{adv}\mathcal{L}_{adv}^{(G)}" display="inline"><semantics id="S3.E5.m2.1a"><mrow id="S3.E5.m2.1.2" xref="S3.E5.m2.1.2.cmml"><mi id="S3.E5.m2.1.2.2" xref="S3.E5.m2.1.2.2.cmml"></mi><mo id="S3.E5.m2.1.2.1" xref="S3.E5.m2.1.2.1.cmml">=</mo><mrow id="S3.E5.m2.1.2.3" xref="S3.E5.m2.1.2.3.cmml"><mrow id="S3.E5.m2.1.2.3.2" xref="S3.E5.m2.1.2.3.2.cmml"><msub id="S3.E5.m2.1.2.3.2.2" xref="S3.E5.m2.1.2.3.2.2.cmml"><mi id="S3.E5.m2.1.2.3.2.2.2" xref="S3.E5.m2.1.2.3.2.2.2.cmml">λ</mi><mi id="S3.E5.m2.1.2.3.2.2.3" xref="S3.E5.m2.1.2.3.2.2.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.2.3.2.1" xref="S3.E5.m2.1.2.3.2.1.cmml">​</mo><msub id="S3.E5.m2.1.2.3.2.3" xref="S3.E5.m2.1.2.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m2.1.2.3.2.3.2" xref="S3.E5.m2.1.2.3.2.3.2.cmml">ℒ</mi><mi id="S3.E5.m2.1.2.3.2.3.3" xref="S3.E5.m2.1.2.3.2.3.3.cmml">c</mi></msub></mrow><mo id="S3.E5.m2.1.2.3.1" xref="S3.E5.m2.1.2.3.1.cmml">+</mo><mrow id="S3.E5.m2.1.2.3.3" xref="S3.E5.m2.1.2.3.3.cmml"><msub id="S3.E5.m2.1.2.3.3.2" xref="S3.E5.m2.1.2.3.3.2.cmml"><mi id="S3.E5.m2.1.2.3.3.2.2" xref="S3.E5.m2.1.2.3.3.2.2.cmml">λ</mi><mrow id="S3.E5.m2.1.2.3.3.2.3" xref="S3.E5.m2.1.2.3.3.2.3.cmml"><mi id="S3.E5.m2.1.2.3.3.2.3.2" xref="S3.E5.m2.1.2.3.3.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.2.3.3.2.3.1" xref="S3.E5.m2.1.2.3.3.2.3.1.cmml">​</mo><mi id="S3.E5.m2.1.2.3.3.2.3.3" xref="S3.E5.m2.1.2.3.3.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.2.3.3.2.3.1a" xref="S3.E5.m2.1.2.3.3.2.3.1.cmml">​</mo><mi id="S3.E5.m2.1.2.3.3.2.3.4" xref="S3.E5.m2.1.2.3.3.2.3.4.cmml">c</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.2.3.3.1" xref="S3.E5.m2.1.2.3.3.1.cmml">​</mo><msub id="S3.E5.m2.1.2.3.3.3" xref="S3.E5.m2.1.2.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m2.1.2.3.3.3.2" xref="S3.E5.m2.1.2.3.3.3.2.cmml">ℒ</mi><mrow id="S3.E5.m2.1.2.3.3.3.3" xref="S3.E5.m2.1.2.3.3.3.3.cmml"><mi id="S3.E5.m2.1.2.3.3.3.3.2" xref="S3.E5.m2.1.2.3.3.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.2.3.3.3.3.1" xref="S3.E5.m2.1.2.3.3.3.3.1.cmml">​</mo><mi id="S3.E5.m2.1.2.3.3.3.3.3" xref="S3.E5.m2.1.2.3.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.2.3.3.3.3.1a" xref="S3.E5.m2.1.2.3.3.3.3.1.cmml">​</mo><mi id="S3.E5.m2.1.2.3.3.3.3.4" xref="S3.E5.m2.1.2.3.3.3.3.4.cmml">c</mi></mrow></msub></mrow><mo id="S3.E5.m2.1.2.3.1a" xref="S3.E5.m2.1.2.3.1.cmml">+</mo><mrow id="S3.E5.m2.1.2.3.4" xref="S3.E5.m2.1.2.3.4.cmml"><msub id="S3.E5.m2.1.2.3.4.2" xref="S3.E5.m2.1.2.3.4.2.cmml"><mi id="S3.E5.m2.1.2.3.4.2.2" xref="S3.E5.m2.1.2.3.4.2.2.cmml">λ</mi><mrow id="S3.E5.m2.1.2.3.4.2.3" xref="S3.E5.m2.1.2.3.4.2.3.cmml"><mi id="S3.E5.m2.1.2.3.4.2.3.2" xref="S3.E5.m2.1.2.3.4.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.2.3.4.2.3.1" xref="S3.E5.m2.1.2.3.4.2.3.1.cmml">​</mo><mi id="S3.E5.m2.1.2.3.4.2.3.3" xref="S3.E5.m2.1.2.3.4.2.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.2.3.4.2.3.1a" xref="S3.E5.m2.1.2.3.4.2.3.1.cmml">​</mo><mi id="S3.E5.m2.1.2.3.4.2.3.4" xref="S3.E5.m2.1.2.3.4.2.3.4.cmml">v</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.2.3.4.1" xref="S3.E5.m2.1.2.3.4.1.cmml">​</mo><msubsup id="S3.E5.m2.1.2.3.4.3" xref="S3.E5.m2.1.2.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m2.1.2.3.4.3.2.2" xref="S3.E5.m2.1.2.3.4.3.2.2.cmml">ℒ</mi><mrow id="S3.E5.m2.1.2.3.4.3.2.3" xref="S3.E5.m2.1.2.3.4.3.2.3.cmml"><mi id="S3.E5.m2.1.2.3.4.3.2.3.2" xref="S3.E5.m2.1.2.3.4.3.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.2.3.4.3.2.3.1" xref="S3.E5.m2.1.2.3.4.3.2.3.1.cmml">​</mo><mi id="S3.E5.m2.1.2.3.4.3.2.3.3" xref="S3.E5.m2.1.2.3.4.3.2.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.2.3.4.3.2.3.1a" xref="S3.E5.m2.1.2.3.4.3.2.3.1.cmml">​</mo><mi id="S3.E5.m2.1.2.3.4.3.2.3.4" xref="S3.E5.m2.1.2.3.4.3.2.3.4.cmml">v</mi></mrow><mrow id="S3.E5.m2.1.1.1.3" xref="S3.E5.m2.1.2.3.4.3.cmml"><mo stretchy="false" id="S3.E5.m2.1.1.1.3.1" xref="S3.E5.m2.1.2.3.4.3.cmml">(</mo><mi id="S3.E5.m2.1.1.1.1" xref="S3.E5.m2.1.1.1.1.cmml">G</mi><mo stretchy="false" id="S3.E5.m2.1.1.1.3.2" xref="S3.E5.m2.1.2.3.4.3.cmml">)</mo></mrow></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m2.1b"><apply id="S3.E5.m2.1.2.cmml" xref="S3.E5.m2.1.2"><eq id="S3.E5.m2.1.2.1.cmml" xref="S3.E5.m2.1.2.1"></eq><csymbol cd="latexml" id="S3.E5.m2.1.2.2.cmml" xref="S3.E5.m2.1.2.2">absent</csymbol><apply id="S3.E5.m2.1.2.3.cmml" xref="S3.E5.m2.1.2.3"><plus id="S3.E5.m2.1.2.3.1.cmml" xref="S3.E5.m2.1.2.3.1"></plus><apply id="S3.E5.m2.1.2.3.2.cmml" xref="S3.E5.m2.1.2.3.2"><times id="S3.E5.m2.1.2.3.2.1.cmml" xref="S3.E5.m2.1.2.3.2.1"></times><apply id="S3.E5.m2.1.2.3.2.2.cmml" xref="S3.E5.m2.1.2.3.2.2"><csymbol cd="ambiguous" id="S3.E5.m2.1.2.3.2.2.1.cmml" xref="S3.E5.m2.1.2.3.2.2">subscript</csymbol><ci id="S3.E5.m2.1.2.3.2.2.2.cmml" xref="S3.E5.m2.1.2.3.2.2.2">𝜆</ci><ci id="S3.E5.m2.1.2.3.2.2.3.cmml" xref="S3.E5.m2.1.2.3.2.2.3">𝑐</ci></apply><apply id="S3.E5.m2.1.2.3.2.3.cmml" xref="S3.E5.m2.1.2.3.2.3"><csymbol cd="ambiguous" id="S3.E5.m2.1.2.3.2.3.1.cmml" xref="S3.E5.m2.1.2.3.2.3">subscript</csymbol><ci id="S3.E5.m2.1.2.3.2.3.2.cmml" xref="S3.E5.m2.1.2.3.2.3.2">ℒ</ci><ci id="S3.E5.m2.1.2.3.2.3.3.cmml" xref="S3.E5.m2.1.2.3.2.3.3">𝑐</ci></apply></apply><apply id="S3.E5.m2.1.2.3.3.cmml" xref="S3.E5.m2.1.2.3.3"><times id="S3.E5.m2.1.2.3.3.1.cmml" xref="S3.E5.m2.1.2.3.3.1"></times><apply id="S3.E5.m2.1.2.3.3.2.cmml" xref="S3.E5.m2.1.2.3.3.2"><csymbol cd="ambiguous" id="S3.E5.m2.1.2.3.3.2.1.cmml" xref="S3.E5.m2.1.2.3.3.2">subscript</csymbol><ci id="S3.E5.m2.1.2.3.3.2.2.cmml" xref="S3.E5.m2.1.2.3.3.2.2">𝜆</ci><apply id="S3.E5.m2.1.2.3.3.2.3.cmml" xref="S3.E5.m2.1.2.3.3.2.3"><times id="S3.E5.m2.1.2.3.3.2.3.1.cmml" xref="S3.E5.m2.1.2.3.3.2.3.1"></times><ci id="S3.E5.m2.1.2.3.3.2.3.2.cmml" xref="S3.E5.m2.1.2.3.3.2.3.2">𝑟</ci><ci id="S3.E5.m2.1.2.3.3.2.3.3.cmml" xref="S3.E5.m2.1.2.3.3.2.3.3">𝑒</ci><ci id="S3.E5.m2.1.2.3.3.2.3.4.cmml" xref="S3.E5.m2.1.2.3.3.2.3.4">𝑐</ci></apply></apply><apply id="S3.E5.m2.1.2.3.3.3.cmml" xref="S3.E5.m2.1.2.3.3.3"><csymbol cd="ambiguous" id="S3.E5.m2.1.2.3.3.3.1.cmml" xref="S3.E5.m2.1.2.3.3.3">subscript</csymbol><ci id="S3.E5.m2.1.2.3.3.3.2.cmml" xref="S3.E5.m2.1.2.3.3.3.2">ℒ</ci><apply id="S3.E5.m2.1.2.3.3.3.3.cmml" xref="S3.E5.m2.1.2.3.3.3.3"><times id="S3.E5.m2.1.2.3.3.3.3.1.cmml" xref="S3.E5.m2.1.2.3.3.3.3.1"></times><ci id="S3.E5.m2.1.2.3.3.3.3.2.cmml" xref="S3.E5.m2.1.2.3.3.3.3.2">𝑟</ci><ci id="S3.E5.m2.1.2.3.3.3.3.3.cmml" xref="S3.E5.m2.1.2.3.3.3.3.3">𝑒</ci><ci id="S3.E5.m2.1.2.3.3.3.3.4.cmml" xref="S3.E5.m2.1.2.3.3.3.3.4">𝑐</ci></apply></apply></apply><apply id="S3.E5.m2.1.2.3.4.cmml" xref="S3.E5.m2.1.2.3.4"><times id="S3.E5.m2.1.2.3.4.1.cmml" xref="S3.E5.m2.1.2.3.4.1"></times><apply id="S3.E5.m2.1.2.3.4.2.cmml" xref="S3.E5.m2.1.2.3.4.2"><csymbol cd="ambiguous" id="S3.E5.m2.1.2.3.4.2.1.cmml" xref="S3.E5.m2.1.2.3.4.2">subscript</csymbol><ci id="S3.E5.m2.1.2.3.4.2.2.cmml" xref="S3.E5.m2.1.2.3.4.2.2">𝜆</ci><apply id="S3.E5.m2.1.2.3.4.2.3.cmml" xref="S3.E5.m2.1.2.3.4.2.3"><times id="S3.E5.m2.1.2.3.4.2.3.1.cmml" xref="S3.E5.m2.1.2.3.4.2.3.1"></times><ci id="S3.E5.m2.1.2.3.4.2.3.2.cmml" xref="S3.E5.m2.1.2.3.4.2.3.2">𝑎</ci><ci id="S3.E5.m2.1.2.3.4.2.3.3.cmml" xref="S3.E5.m2.1.2.3.4.2.3.3">𝑑</ci><ci id="S3.E5.m2.1.2.3.4.2.3.4.cmml" xref="S3.E5.m2.1.2.3.4.2.3.4">𝑣</ci></apply></apply><apply id="S3.E5.m2.1.2.3.4.3.cmml" xref="S3.E5.m2.1.2.3.4.3"><csymbol cd="ambiguous" id="S3.E5.m2.1.2.3.4.3.1.cmml" xref="S3.E5.m2.1.2.3.4.3">superscript</csymbol><apply id="S3.E5.m2.1.2.3.4.3.2.cmml" xref="S3.E5.m2.1.2.3.4.3"><csymbol cd="ambiguous" id="S3.E5.m2.1.2.3.4.3.2.1.cmml" xref="S3.E5.m2.1.2.3.4.3">subscript</csymbol><ci id="S3.E5.m2.1.2.3.4.3.2.2.cmml" xref="S3.E5.m2.1.2.3.4.3.2.2">ℒ</ci><apply id="S3.E5.m2.1.2.3.4.3.2.3.cmml" xref="S3.E5.m2.1.2.3.4.3.2.3"><times id="S3.E5.m2.1.2.3.4.3.2.3.1.cmml" xref="S3.E5.m2.1.2.3.4.3.2.3.1"></times><ci id="S3.E5.m2.1.2.3.4.3.2.3.2.cmml" xref="S3.E5.m2.1.2.3.4.3.2.3.2">𝑎</ci><ci id="S3.E5.m2.1.2.3.4.3.2.3.3.cmml" xref="S3.E5.m2.1.2.3.4.3.2.3.3">𝑑</ci><ci id="S3.E5.m2.1.2.3.4.3.2.3.4.cmml" xref="S3.E5.m2.1.2.3.4.3.2.3.4">𝑣</ci></apply></apply><ci id="S3.E5.m2.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1">𝐺</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m2.1c">\displaystyle=\lambda_{c}\mathcal{L}_{c}+\lambda_{rec}\mathcal{L}_{rec}+\lambda_{adv}\mathcal{L}_{adv}^{(G)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p9.2" class="ltx_p">Computing the autoencoding loss objective on the masked temporal slices strictly enforces the decoder to learn from the other modality, as the input embeddings for the decoder at masked indices are obtained from the other modality. This novel strategy explicitly enforces audio-visual correspondence supplementing the contrastive loss objective.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2406.02951/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.17.8.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.14.7" class="ltx_text ltx_font_bold" style="font-size:90%;">Deepfake Classification Stage.<span id="S3.F3.14.7.7" class="ltx_text ltx_font_medium"> Given a sample <math id="S3.F3.8.1.1.m1.1" class="ltx_Math" alttext="x\in\mathcal{D}_{df}" display="inline"><semantics id="S3.F3.8.1.1.m1.1b"><mrow id="S3.F3.8.1.1.m1.1.1" xref="S3.F3.8.1.1.m1.1.1.cmml"><mi id="S3.F3.8.1.1.m1.1.1.2" xref="S3.F3.8.1.1.m1.1.1.2.cmml">x</mi><mo id="S3.F3.8.1.1.m1.1.1.1" xref="S3.F3.8.1.1.m1.1.1.1.cmml">∈</mo><msub id="S3.F3.8.1.1.m1.1.1.3" xref="S3.F3.8.1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.F3.8.1.1.m1.1.1.3.2" xref="S3.F3.8.1.1.m1.1.1.3.2.cmml">𝒟</mi><mrow id="S3.F3.8.1.1.m1.1.1.3.3" xref="S3.F3.8.1.1.m1.1.1.3.3.cmml"><mi id="S3.F3.8.1.1.m1.1.1.3.3.2" xref="S3.F3.8.1.1.m1.1.1.3.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.F3.8.1.1.m1.1.1.3.3.1" xref="S3.F3.8.1.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.F3.8.1.1.m1.1.1.3.3.3" xref="S3.F3.8.1.1.m1.1.1.3.3.3.cmml">f</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.8.1.1.m1.1c"><apply id="S3.F3.8.1.1.m1.1.1.cmml" xref="S3.F3.8.1.1.m1.1.1"><in id="S3.F3.8.1.1.m1.1.1.1.cmml" xref="S3.F3.8.1.1.m1.1.1.1"></in><ci id="S3.F3.8.1.1.m1.1.1.2.cmml" xref="S3.F3.8.1.1.m1.1.1.2">𝑥</ci><apply id="S3.F3.8.1.1.m1.1.1.3.cmml" xref="S3.F3.8.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.F3.8.1.1.m1.1.1.3.1.cmml" xref="S3.F3.8.1.1.m1.1.1.3">subscript</csymbol><ci id="S3.F3.8.1.1.m1.1.1.3.2.cmml" xref="S3.F3.8.1.1.m1.1.1.3.2">𝒟</ci><apply id="S3.F3.8.1.1.m1.1.1.3.3.cmml" xref="S3.F3.8.1.1.m1.1.1.3.3"><times id="S3.F3.8.1.1.m1.1.1.3.3.1.cmml" xref="S3.F3.8.1.1.m1.1.1.3.3.1"></times><ci id="S3.F3.8.1.1.m1.1.1.3.3.2.cmml" xref="S3.F3.8.1.1.m1.1.1.3.3.2">𝑑</ci><ci id="S3.F3.8.1.1.m1.1.1.3.3.3.cmml" xref="S3.F3.8.1.1.m1.1.1.3.3.3">𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.8.1.1.m1.1d">x\in\mathcal{D}_{df}</annotation></semantics></math>, comprising of audio and visual inputs <math id="S3.F3.9.2.2.m2.1" class="ltx_Math" alttext="\bm{x_{a}}" display="inline"><semantics id="S3.F3.9.2.2.m2.1b"><msub id="S3.F3.9.2.2.m2.1.1" xref="S3.F3.9.2.2.m2.1.1.cmml"><mi id="S3.F3.9.2.2.m2.1.1.2" xref="S3.F3.9.2.2.m2.1.1.2.cmml">𝒙</mi><mi id="S3.F3.9.2.2.m2.1.1.3" xref="S3.F3.9.2.2.m2.1.1.3.cmml">𝒂</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F3.9.2.2.m2.1c"><apply id="S3.F3.9.2.2.m2.1.1.cmml" xref="S3.F3.9.2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F3.9.2.2.m2.1.1.1.cmml" xref="S3.F3.9.2.2.m2.1.1">subscript</csymbol><ci id="S3.F3.9.2.2.m2.1.1.2.cmml" xref="S3.F3.9.2.2.m2.1.1.2">𝒙</ci><ci id="S3.F3.9.2.2.m2.1.1.3.cmml" xref="S3.F3.9.2.2.m2.1.1.3">𝒂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.9.2.2.m2.1d">\bm{x_{a}}</annotation></semantics></math> and <math id="S3.F3.10.3.3.m3.1" class="ltx_Math" alttext="\bm{x_{v}}" display="inline"><semantics id="S3.F3.10.3.3.m3.1b"><msub id="S3.F3.10.3.3.m3.1.1" xref="S3.F3.10.3.3.m3.1.1.cmml"><mi id="S3.F3.10.3.3.m3.1.1.2" xref="S3.F3.10.3.3.m3.1.1.2.cmml">𝒙</mi><mi id="S3.F3.10.3.3.m3.1.1.3" xref="S3.F3.10.3.3.m3.1.1.3.cmml">𝒗</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F3.10.3.3.m3.1c"><apply id="S3.F3.10.3.3.m3.1.1.cmml" xref="S3.F3.10.3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.F3.10.3.3.m3.1.1.1.cmml" xref="S3.F3.10.3.3.m3.1.1">subscript</csymbol><ci id="S3.F3.10.3.3.m3.1.1.2.cmml" xref="S3.F3.10.3.3.m3.1.1.2">𝒙</ci><ci id="S3.F3.10.3.3.m3.1.1.3.cmml" xref="S3.F3.10.3.3.m3.1.1.3">𝒗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.10.3.3.m3.1d">\bm{x_{v}}</annotation></semantics></math>, we obtain the unimodal features (<math id="S3.F3.11.4.4.m4.2" class="ltx_Math" alttext="\bm{a},\bm{v}" display="inline"><semantics id="S3.F3.11.4.4.m4.2b"><mrow id="S3.F3.11.4.4.m4.2.3.2" xref="S3.F3.11.4.4.m4.2.3.1.cmml"><mi id="S3.F3.11.4.4.m4.1.1" xref="S3.F3.11.4.4.m4.1.1.cmml">𝒂</mi><mo id="S3.F3.11.4.4.m4.2.3.2.1" xref="S3.F3.11.4.4.m4.2.3.1.cmml">,</mo><mi id="S3.F3.11.4.4.m4.2.2" xref="S3.F3.11.4.4.m4.2.2.cmml">𝒗</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.11.4.4.m4.2c"><list id="S3.F3.11.4.4.m4.2.3.1.cmml" xref="S3.F3.11.4.4.m4.2.3.2"><ci id="S3.F3.11.4.4.m4.1.1.cmml" xref="S3.F3.11.4.4.m4.1.1">𝒂</ci><ci id="S3.F3.11.4.4.m4.2.2.cmml" xref="S3.F3.11.4.4.m4.2.2">𝒗</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.11.4.4.m4.2d">\bm{a},\bm{v}</annotation></semantics></math>) and the cross-modal embeddings (<math id="S3.F3.12.5.5.m5.2" class="ltx_Math" alttext="\bm{a_{v}},\bm{v_{a}}" display="inline"><semantics id="S3.F3.12.5.5.m5.2b"><mrow id="S3.F3.12.5.5.m5.2.2.2" xref="S3.F3.12.5.5.m5.2.2.3.cmml"><msub id="S3.F3.12.5.5.m5.1.1.1.1" xref="S3.F3.12.5.5.m5.1.1.1.1.cmml"><mi id="S3.F3.12.5.5.m5.1.1.1.1.2" xref="S3.F3.12.5.5.m5.1.1.1.1.2.cmml">𝒂</mi><mi id="S3.F3.12.5.5.m5.1.1.1.1.3" xref="S3.F3.12.5.5.m5.1.1.1.1.3.cmml">𝒗</mi></msub><mo id="S3.F3.12.5.5.m5.2.2.2.3" xref="S3.F3.12.5.5.m5.2.2.3.cmml">,</mo><msub id="S3.F3.12.5.5.m5.2.2.2.2" xref="S3.F3.12.5.5.m5.2.2.2.2.cmml"><mi id="S3.F3.12.5.5.m5.2.2.2.2.2" xref="S3.F3.12.5.5.m5.2.2.2.2.2.cmml">𝒗</mi><mi id="S3.F3.12.5.5.m5.2.2.2.2.3" xref="S3.F3.12.5.5.m5.2.2.2.2.3.cmml">𝒂</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.12.5.5.m5.2c"><list id="S3.F3.12.5.5.m5.2.2.3.cmml" xref="S3.F3.12.5.5.m5.2.2.2"><apply id="S3.F3.12.5.5.m5.1.1.1.1.cmml" xref="S3.F3.12.5.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.F3.12.5.5.m5.1.1.1.1.1.cmml" xref="S3.F3.12.5.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.F3.12.5.5.m5.1.1.1.1.2.cmml" xref="S3.F3.12.5.5.m5.1.1.1.1.2">𝒂</ci><ci id="S3.F3.12.5.5.m5.1.1.1.1.3.cmml" xref="S3.F3.12.5.5.m5.1.1.1.1.3">𝒗</ci></apply><apply id="S3.F3.12.5.5.m5.2.2.2.2.cmml" xref="S3.F3.12.5.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S3.F3.12.5.5.m5.2.2.2.2.1.cmml" xref="S3.F3.12.5.5.m5.2.2.2.2">subscript</csymbol><ci id="S3.F3.12.5.5.m5.2.2.2.2.2.cmml" xref="S3.F3.12.5.5.m5.2.2.2.2.2">𝒗</ci><ci id="S3.F3.12.5.5.m5.2.2.2.2.3.cmml" xref="S3.F3.12.5.5.m5.2.2.2.2.3">𝒂</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.12.5.5.m5.2d">\bm{a_{v}},\bm{v_{a}}</annotation></semantics></math>). For each modality, the unimodal and cross-modal embeddings are concatenated to obtain (<math id="S3.F3.13.6.6.m6.2" class="ltx_Math" alttext="\bm{f_{a}},\bm{f_{v}}" display="inline"><semantics id="S3.F3.13.6.6.m6.2b"><mrow id="S3.F3.13.6.6.m6.2.2.2" xref="S3.F3.13.6.6.m6.2.2.3.cmml"><msub id="S3.F3.13.6.6.m6.1.1.1.1" xref="S3.F3.13.6.6.m6.1.1.1.1.cmml"><mi id="S3.F3.13.6.6.m6.1.1.1.1.2" xref="S3.F3.13.6.6.m6.1.1.1.1.2.cmml">𝒇</mi><mi id="S3.F3.13.6.6.m6.1.1.1.1.3" xref="S3.F3.13.6.6.m6.1.1.1.1.3.cmml">𝒂</mi></msub><mo id="S3.F3.13.6.6.m6.2.2.2.3" xref="S3.F3.13.6.6.m6.2.2.3.cmml">,</mo><msub id="S3.F3.13.6.6.m6.2.2.2.2" xref="S3.F3.13.6.6.m6.2.2.2.2.cmml"><mi id="S3.F3.13.6.6.m6.2.2.2.2.2" xref="S3.F3.13.6.6.m6.2.2.2.2.2.cmml">𝒇</mi><mi id="S3.F3.13.6.6.m6.2.2.2.2.3" xref="S3.F3.13.6.6.m6.2.2.2.2.3.cmml">𝒗</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.13.6.6.m6.2c"><list id="S3.F3.13.6.6.m6.2.2.3.cmml" xref="S3.F3.13.6.6.m6.2.2.2"><apply id="S3.F3.13.6.6.m6.1.1.1.1.cmml" xref="S3.F3.13.6.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S3.F3.13.6.6.m6.1.1.1.1.1.cmml" xref="S3.F3.13.6.6.m6.1.1.1.1">subscript</csymbol><ci id="S3.F3.13.6.6.m6.1.1.1.1.2.cmml" xref="S3.F3.13.6.6.m6.1.1.1.1.2">𝒇</ci><ci id="S3.F3.13.6.6.m6.1.1.1.1.3.cmml" xref="S3.F3.13.6.6.m6.1.1.1.1.3">𝒂</ci></apply><apply id="S3.F3.13.6.6.m6.2.2.2.2.cmml" xref="S3.F3.13.6.6.m6.2.2.2.2"><csymbol cd="ambiguous" id="S3.F3.13.6.6.m6.2.2.2.2.1.cmml" xref="S3.F3.13.6.6.m6.2.2.2.2">subscript</csymbol><ci id="S3.F3.13.6.6.m6.2.2.2.2.2.cmml" xref="S3.F3.13.6.6.m6.2.2.2.2.2">𝒇</ci><ci id="S3.F3.13.6.6.m6.2.2.2.2.3.cmml" xref="S3.F3.13.6.6.m6.2.2.2.2.3">𝒗</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.13.6.6.m6.2d">\bm{f_{a}},\bm{f_{v}}</annotation></semantics></math>). A classifier network is then trained to take (<math id="S3.F3.14.7.7.m7.2" class="ltx_Math" alttext="\bm{f_{a}},\bm{f_{v}}" display="inline"><semantics id="S3.F3.14.7.7.m7.2b"><mrow id="S3.F3.14.7.7.m7.2.2.2" xref="S3.F3.14.7.7.m7.2.2.3.cmml"><msub id="S3.F3.14.7.7.m7.1.1.1.1" xref="S3.F3.14.7.7.m7.1.1.1.1.cmml"><mi id="S3.F3.14.7.7.m7.1.1.1.1.2" xref="S3.F3.14.7.7.m7.1.1.1.1.2.cmml">𝒇</mi><mi id="S3.F3.14.7.7.m7.1.1.1.1.3" xref="S3.F3.14.7.7.m7.1.1.1.1.3.cmml">𝒂</mi></msub><mo id="S3.F3.14.7.7.m7.2.2.2.3" xref="S3.F3.14.7.7.m7.2.2.3.cmml">,</mo><msub id="S3.F3.14.7.7.m7.2.2.2.2" xref="S3.F3.14.7.7.m7.2.2.2.2.cmml"><mi id="S3.F3.14.7.7.m7.2.2.2.2.2" xref="S3.F3.14.7.7.m7.2.2.2.2.2.cmml">𝒇</mi><mi id="S3.F3.14.7.7.m7.2.2.2.2.3" xref="S3.F3.14.7.7.m7.2.2.2.2.3.cmml">𝒗</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.14.7.7.m7.2c"><list id="S3.F3.14.7.7.m7.2.2.3.cmml" xref="S3.F3.14.7.7.m7.2.2.2"><apply id="S3.F3.14.7.7.m7.1.1.1.1.cmml" xref="S3.F3.14.7.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="S3.F3.14.7.7.m7.1.1.1.1.1.cmml" xref="S3.F3.14.7.7.m7.1.1.1.1">subscript</csymbol><ci id="S3.F3.14.7.7.m7.1.1.1.1.2.cmml" xref="S3.F3.14.7.7.m7.1.1.1.1.2">𝒇</ci><ci id="S3.F3.14.7.7.m7.1.1.1.1.3.cmml" xref="S3.F3.14.7.7.m7.1.1.1.1.3">𝒂</ci></apply><apply id="S3.F3.14.7.7.m7.2.2.2.2.cmml" xref="S3.F3.14.7.7.m7.2.2.2.2"><csymbol cd="ambiguous" id="S3.F3.14.7.7.m7.2.2.2.2.1.cmml" xref="S3.F3.14.7.7.m7.2.2.2.2">subscript</csymbol><ci id="S3.F3.14.7.7.m7.2.2.2.2.2.cmml" xref="S3.F3.14.7.7.m7.2.2.2.2.2">𝒇</ci><ci id="S3.F3.14.7.7.m7.2.2.2.2.3.cmml" xref="S3.F3.14.7.7.m7.2.2.2.2.3">𝒗</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.14.7.7.m7.2d">\bm{f_{a}},\bm{f_{v}}</annotation></semantics></math>) as input and predict if the input is real or fake.</span></span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Deepfake Classification Stage</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The goal of this stage is to detect video deepfakes, where either or both audio and visual modalities have been faked.
For this, we use the encoders and the cross-modal networks trained in the representation learning phase.
We train a classifier to tell real videos and deepfakes apart using a supervised learning approach. The classification pipeline is depicted in <a href="#S3.F3" title="In 3.2 Representation Learning Stage ‣ 3 Method ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>.
Since the learned representations have a high audio-visual correspondence for real videos, we expect the classifier to exploit the lack of audio-visual cohesion of synthesized samples in distinguishing between real and fake.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.4" class="ltx_p"><span id="S3.SS3.p2.4.1" class="ltx_text ltx_font_bold">Input Tokenization.</span>
The process followed in input tokenization is identical to Stage 1 except for the dataset used. In this stage we draw samples from a labeled deepfake dataset (<math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{df}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">𝒟</mi><mrow id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.3.1" xref="S3.SS3.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p2.1.m1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝒟</ci><apply id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><times id="S3.SS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3.1"></times><ci id="S3.SS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2">𝑑</ci><ci id="S3.SS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\mathcal{D}_{df}</annotation></semantics></math>) consisting of both real and fake videos, <em id="S3.SS3.p2.4.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS3.p2.4.3" class="ltx_text"></span>, <math id="S3.SS3.p2.2.m2.2" class="ltx_Math" alttext="(x,y)\in\mathcal{D}_{df}" display="inline"><semantics id="S3.SS3.p2.2.m2.2a"><mrow id="S3.SS3.p2.2.m2.2.3" xref="S3.SS3.p2.2.m2.2.3.cmml"><mrow id="S3.SS3.p2.2.m2.2.3.2.2" xref="S3.SS3.p2.2.m2.2.3.2.1.cmml"><mo stretchy="false" id="S3.SS3.p2.2.m2.2.3.2.2.1" xref="S3.SS3.p2.2.m2.2.3.2.1.cmml">(</mo><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">x</mi><mo id="S3.SS3.p2.2.m2.2.3.2.2.2" xref="S3.SS3.p2.2.m2.2.3.2.1.cmml">,</mo><mi id="S3.SS3.p2.2.m2.2.2" xref="S3.SS3.p2.2.m2.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS3.p2.2.m2.2.3.2.2.3" xref="S3.SS3.p2.2.m2.2.3.2.1.cmml">)</mo></mrow><mo id="S3.SS3.p2.2.m2.2.3.1" xref="S3.SS3.p2.2.m2.2.3.1.cmml">∈</mo><msub id="S3.SS3.p2.2.m2.2.3.3" xref="S3.SS3.p2.2.m2.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.2.m2.2.3.3.2" xref="S3.SS3.p2.2.m2.2.3.3.2.cmml">𝒟</mi><mrow id="S3.SS3.p2.2.m2.2.3.3.3" xref="S3.SS3.p2.2.m2.2.3.3.3.cmml"><mi id="S3.SS3.p2.2.m2.2.3.3.3.2" xref="S3.SS3.p2.2.m2.2.3.3.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.2.m2.2.3.3.3.1" xref="S3.SS3.p2.2.m2.2.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p2.2.m2.2.3.3.3.3" xref="S3.SS3.p2.2.m2.2.3.3.3.3.cmml">f</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.2b"><apply id="S3.SS3.p2.2.m2.2.3.cmml" xref="S3.SS3.p2.2.m2.2.3"><in id="S3.SS3.p2.2.m2.2.3.1.cmml" xref="S3.SS3.p2.2.m2.2.3.1"></in><interval closure="open" id="S3.SS3.p2.2.m2.2.3.2.1.cmml" xref="S3.SS3.p2.2.m2.2.3.2.2"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝑥</ci><ci id="S3.SS3.p2.2.m2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2">𝑦</ci></interval><apply id="S3.SS3.p2.2.m2.2.3.3.cmml" xref="S3.SS3.p2.2.m2.2.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.2.3.3.1.cmml" xref="S3.SS3.p2.2.m2.2.3.3">subscript</csymbol><ci id="S3.SS3.p2.2.m2.2.3.3.2.cmml" xref="S3.SS3.p2.2.m2.2.3.3.2">𝒟</ci><apply id="S3.SS3.p2.2.m2.2.3.3.3.cmml" xref="S3.SS3.p2.2.m2.2.3.3.3"><times id="S3.SS3.p2.2.m2.2.3.3.3.1.cmml" xref="S3.SS3.p2.2.m2.2.3.3.3.1"></times><ci id="S3.SS3.p2.2.m2.2.3.3.3.2.cmml" xref="S3.SS3.p2.2.m2.2.3.3.3.2">𝑑</ci><ci id="S3.SS3.p2.2.m2.2.3.3.3.3.cmml" xref="S3.SS3.p2.2.m2.2.3.3.3.3">𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.2c">(x,y)\in\mathcal{D}_{df}</annotation></semantics></math>, where <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">x</annotation></semantics></math> is the video sample and <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mi id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">y</annotation></semantics></math> is the label (real/fake).</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.6" class="ltx_p"><span id="S3.SS3.p3.6.1" class="ltx_text ltx_font_bold">Feature Extraction.</span>
The tokenized inputs (<math id="S3.SS3.p3.1.m1.2" class="ltx_Math" alttext="\bm{x_{a}},\bm{x_{v}}" display="inline"><semantics id="S3.SS3.p3.1.m1.2a"><mrow id="S3.SS3.p3.1.m1.2.2.2" xref="S3.SS3.p3.1.m1.2.2.3.cmml"><msub id="S3.SS3.p3.1.m1.1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.1.1.2.cmml">𝒙</mi><mi id="S3.SS3.p3.1.m1.1.1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.1.1.3.cmml">𝒂</mi></msub><mo id="S3.SS3.p3.1.m1.2.2.2.3" xref="S3.SS3.p3.1.m1.2.2.3.cmml">,</mo><msub id="S3.SS3.p3.1.m1.2.2.2.2" xref="S3.SS3.p3.1.m1.2.2.2.2.cmml"><mi id="S3.SS3.p3.1.m1.2.2.2.2.2" xref="S3.SS3.p3.1.m1.2.2.2.2.2.cmml">𝒙</mi><mi id="S3.SS3.p3.1.m1.2.2.2.2.3" xref="S3.SS3.p3.1.m1.2.2.2.2.3.cmml">𝒗</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.2b"><list id="S3.SS3.p3.1.m1.2.2.3.cmml" xref="S3.SS3.p3.1.m1.2.2.2"><apply id="S3.SS3.p3.1.m1.1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1.2">𝒙</ci><ci id="S3.SS3.p3.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.1.1.3">𝒂</ci></apply><apply id="S3.SS3.p3.1.m1.2.2.2.2.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.2.2.2.2.1.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p3.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2.2">𝒙</ci><ci id="S3.SS3.p3.1.m1.2.2.2.2.3.cmml" xref="S3.SS3.p3.1.m1.2.2.2.2.3">𝒗</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.2c">\bm{x_{a}},\bm{x_{v}}</annotation></semantics></math>), are sent through the backbone of Stage 1 to obtain (i) the feature embeddings (<math id="S3.SS3.p3.2.m2.2" class="ltx_Math" alttext="\bm{a},\bm{v}" display="inline"><semantics id="S3.SS3.p3.2.m2.2a"><mrow id="S3.SS3.p3.2.m2.2.3.2" xref="S3.SS3.p3.2.m2.2.3.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">𝒂</mi><mo id="S3.SS3.p3.2.m2.2.3.2.1" xref="S3.SS3.p3.2.m2.2.3.1.cmml">,</mo><mi id="S3.SS3.p3.2.m2.2.2" xref="S3.SS3.p3.2.m2.2.2.cmml">𝒗</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.2b"><list id="S3.SS3.p3.2.m2.2.3.1.cmml" xref="S3.SS3.p3.2.m2.2.3.2"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝒂</ci><ci id="S3.SS3.p3.2.m2.2.2.cmml" xref="S3.SS3.p3.2.m2.2.2">𝒗</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.2c">\bm{a},\bm{v}</annotation></semantics></math>), which are the outputs of the uni-modal encoders, and (ii) the cross-modal embeddings (<math id="S3.SS3.p3.3.m3.2" class="ltx_Math" alttext="\bm{a_{v}},\bm{v_{a}}" display="inline"><semantics id="S3.SS3.p3.3.m3.2a"><mrow id="S3.SS3.p3.3.m3.2.2.2" xref="S3.SS3.p3.3.m3.2.2.3.cmml"><msub id="S3.SS3.p3.3.m3.1.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.1.1.2" xref="S3.SS3.p3.3.m3.1.1.1.1.2.cmml">𝒂</mi><mi id="S3.SS3.p3.3.m3.1.1.1.1.3" xref="S3.SS3.p3.3.m3.1.1.1.1.3.cmml">𝒗</mi></msub><mo id="S3.SS3.p3.3.m3.2.2.2.3" xref="S3.SS3.p3.3.m3.2.2.3.cmml">,</mo><msub id="S3.SS3.p3.3.m3.2.2.2.2" xref="S3.SS3.p3.3.m3.2.2.2.2.cmml"><mi id="S3.SS3.p3.3.m3.2.2.2.2.2" xref="S3.SS3.p3.3.m3.2.2.2.2.2.cmml">𝒗</mi><mi id="S3.SS3.p3.3.m3.2.2.2.2.3" xref="S3.SS3.p3.3.m3.2.2.2.2.3.cmml">𝒂</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.2b"><list id="S3.SS3.p3.3.m3.2.2.3.cmml" xref="S3.SS3.p3.3.m3.2.2.2"><apply id="S3.SS3.p3.3.m3.1.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1.2">𝒂</ci><ci id="S3.SS3.p3.3.m3.1.1.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1.3">𝒗</ci></apply><apply id="S3.SS3.p3.3.m3.2.2.2.2.cmml" xref="S3.SS3.p3.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.2.2.2.2.1.cmml" xref="S3.SS3.p3.3.m3.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p3.3.m3.2.2.2.2.2.cmml" xref="S3.SS3.p3.3.m3.2.2.2.2.2">𝒗</ci><ci id="S3.SS3.p3.3.m3.2.2.2.2.3.cmml" xref="S3.SS3.p3.3.m3.2.2.2.2.3">𝒂</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.2c">\bm{a_{v}},\bm{v_{a}}</annotation></semantics></math>), which are the outputs of the A2V/V2A cross-modal networks. Here, the cross-modal embeddings are computed for all temporal slices (note: we do not use masking in this stage). We concatenate the two embeddings of each modality creating <math id="S3.SS3.p3.4.m4.2" class="ltx_Math" alttext="(\bm{f_{a}},\bm{f_{v}})" display="inline"><semantics id="S3.SS3.p3.4.m4.2a"><mrow id="S3.SS3.p3.4.m4.2.2.2" xref="S3.SS3.p3.4.m4.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p3.4.m4.2.2.2.3" xref="S3.SS3.p3.4.m4.2.2.3.cmml">(</mo><msub id="S3.SS3.p3.4.m4.1.1.1.1" xref="S3.SS3.p3.4.m4.1.1.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.1.1.2" xref="S3.SS3.p3.4.m4.1.1.1.1.2.cmml">𝒇</mi><mi id="S3.SS3.p3.4.m4.1.1.1.1.3" xref="S3.SS3.p3.4.m4.1.1.1.1.3.cmml">𝒂</mi></msub><mo id="S3.SS3.p3.4.m4.2.2.2.4" xref="S3.SS3.p3.4.m4.2.2.3.cmml">,</mo><msub id="S3.SS3.p3.4.m4.2.2.2.2" xref="S3.SS3.p3.4.m4.2.2.2.2.cmml"><mi id="S3.SS3.p3.4.m4.2.2.2.2.2" xref="S3.SS3.p3.4.m4.2.2.2.2.2.cmml">𝒇</mi><mi id="S3.SS3.p3.4.m4.2.2.2.2.3" xref="S3.SS3.p3.4.m4.2.2.2.2.3.cmml">𝒗</mi></msub><mo stretchy="false" id="S3.SS3.p3.4.m4.2.2.2.5" xref="S3.SS3.p3.4.m4.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.2b"><interval closure="open" id="S3.SS3.p3.4.m4.2.2.3.cmml" xref="S3.SS3.p3.4.m4.2.2.2"><apply id="S3.SS3.p3.4.m4.1.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p3.4.m4.1.1.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.2">𝒇</ci><ci id="S3.SS3.p3.4.m4.1.1.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.3">𝒂</ci></apply><apply id="S3.SS3.p3.4.m4.2.2.2.2.cmml" xref="S3.SS3.p3.4.m4.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.2.2.2.2.1.cmml" xref="S3.SS3.p3.4.m4.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p3.4.m4.2.2.2.2.2.cmml" xref="S3.SS3.p3.4.m4.2.2.2.2.2">𝒇</ci><ci id="S3.SS3.p3.4.m4.2.2.2.2.3.cmml" xref="S3.SS3.p3.4.m4.2.2.2.2.3">𝒗</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.2c">(\bm{f_{a}},\bm{f_{v}})</annotation></semantics></math>, where
<math id="S3.SS3.p3.5.m5.4" class="ltx_Math" alttext="\bm{f_{p}}=\bm{p}\,\oplus\,\bm{p_{q}},\;\forall p,q\in\{a,v\},p\neq q" display="inline"><semantics id="S3.SS3.p3.5.m5.4a"><mrow id="S3.SS3.p3.5.m5.4.4.2" xref="S3.SS3.p3.5.m5.4.4.3.cmml"><mrow id="S3.SS3.p3.5.m5.3.3.1.1" xref="S3.SS3.p3.5.m5.3.3.1.1.cmml"><msub id="S3.SS3.p3.5.m5.3.3.1.1.4" xref="S3.SS3.p3.5.m5.3.3.1.1.4.cmml"><mi id="S3.SS3.p3.5.m5.3.3.1.1.4.2" xref="S3.SS3.p3.5.m5.3.3.1.1.4.2.cmml">𝒇</mi><mi id="S3.SS3.p3.5.m5.3.3.1.1.4.3" xref="S3.SS3.p3.5.m5.3.3.1.1.4.3.cmml">𝒑</mi></msub><mo id="S3.SS3.p3.5.m5.3.3.1.1.3" xref="S3.SS3.p3.5.m5.3.3.1.1.3.cmml">=</mo><mrow id="S3.SS3.p3.5.m5.3.3.1.1.2.2" xref="S3.SS3.p3.5.m5.3.3.1.1.2.3.cmml"><mrow id="S3.SS3.p3.5.m5.3.3.1.1.1.1.1" xref="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.cmml"><mi id="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.2" xref="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.2.cmml">𝒑</mi><mo lspace="0.392em" rspace="0.392em" id="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.1" xref="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.1.cmml">⊕</mo><msub id="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3" xref="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3.cmml"><mi id="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3.2" xref="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3.2.cmml">𝒑</mi><mi id="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3.3" xref="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3.3.cmml">𝒒</mi></msub></mrow><mo rspace="0.447em" id="S3.SS3.p3.5.m5.3.3.1.1.2.2.3" xref="S3.SS3.p3.5.m5.3.3.1.1.2.3.cmml">,</mo><mrow id="S3.SS3.p3.5.m5.3.3.1.1.2.2.2" xref="S3.SS3.p3.5.m5.3.3.1.1.2.2.2.cmml"><mo rspace="0.167em" id="S3.SS3.p3.5.m5.3.3.1.1.2.2.2.1" xref="S3.SS3.p3.5.m5.3.3.1.1.2.2.2.1.cmml">∀</mo><mi id="S3.SS3.p3.5.m5.3.3.1.1.2.2.2.2" xref="S3.SS3.p3.5.m5.3.3.1.1.2.2.2.2.cmml">p</mi></mrow></mrow></mrow><mo id="S3.SS3.p3.5.m5.4.4.2.3" xref="S3.SS3.p3.5.m5.4.4.3a.cmml">,</mo><mrow id="S3.SS3.p3.5.m5.4.4.2.2.2" xref="S3.SS3.p3.5.m5.4.4.2.2.3.cmml"><mrow id="S3.SS3.p3.5.m5.4.4.2.2.1.1" xref="S3.SS3.p3.5.m5.4.4.2.2.1.1.cmml"><mi id="S3.SS3.p3.5.m5.4.4.2.2.1.1.2" xref="S3.SS3.p3.5.m5.4.4.2.2.1.1.2.cmml">q</mi><mo id="S3.SS3.p3.5.m5.4.4.2.2.1.1.1" xref="S3.SS3.p3.5.m5.4.4.2.2.1.1.1.cmml">∈</mo><mrow id="S3.SS3.p3.5.m5.4.4.2.2.1.1.3.2" xref="S3.SS3.p3.5.m5.4.4.2.2.1.1.3.1.cmml"><mo stretchy="false" id="S3.SS3.p3.5.m5.4.4.2.2.1.1.3.2.1" xref="S3.SS3.p3.5.m5.4.4.2.2.1.1.3.1.cmml">{</mo><mi id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml">a</mi><mo id="S3.SS3.p3.5.m5.4.4.2.2.1.1.3.2.2" xref="S3.SS3.p3.5.m5.4.4.2.2.1.1.3.1.cmml">,</mo><mi id="S3.SS3.p3.5.m5.2.2" xref="S3.SS3.p3.5.m5.2.2.cmml">v</mi><mo stretchy="false" id="S3.SS3.p3.5.m5.4.4.2.2.1.1.3.2.3" xref="S3.SS3.p3.5.m5.4.4.2.2.1.1.3.1.cmml">}</mo></mrow></mrow><mo id="S3.SS3.p3.5.m5.4.4.2.2.2.3" xref="S3.SS3.p3.5.m5.4.4.2.2.3a.cmml">,</mo><mrow id="S3.SS3.p3.5.m5.4.4.2.2.2.2" xref="S3.SS3.p3.5.m5.4.4.2.2.2.2.cmml"><mi id="S3.SS3.p3.5.m5.4.4.2.2.2.2.2" xref="S3.SS3.p3.5.m5.4.4.2.2.2.2.2.cmml">p</mi><mo id="S3.SS3.p3.5.m5.4.4.2.2.2.2.1" xref="S3.SS3.p3.5.m5.4.4.2.2.2.2.1.cmml">≠</mo><mi id="S3.SS3.p3.5.m5.4.4.2.2.2.2.3" xref="S3.SS3.p3.5.m5.4.4.2.2.2.2.3.cmml">q</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.4b"><apply id="S3.SS3.p3.5.m5.4.4.3.cmml" xref="S3.SS3.p3.5.m5.4.4.2"><csymbol cd="ambiguous" id="S3.SS3.p3.5.m5.4.4.3a.cmml" xref="S3.SS3.p3.5.m5.4.4.2.3">formulae-sequence</csymbol><apply id="S3.SS3.p3.5.m5.3.3.1.1.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1"><eq id="S3.SS3.p3.5.m5.3.3.1.1.3.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.3"></eq><apply id="S3.SS3.p3.5.m5.3.3.1.1.4.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.4"><csymbol cd="ambiguous" id="S3.SS3.p3.5.m5.3.3.1.1.4.1.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.4">subscript</csymbol><ci id="S3.SS3.p3.5.m5.3.3.1.1.4.2.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.4.2">𝒇</ci><ci id="S3.SS3.p3.5.m5.3.3.1.1.4.3.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.4.3">𝒑</ci></apply><list id="S3.SS3.p3.5.m5.3.3.1.1.2.3.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.2.2"><apply id="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.1.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.1">direct-sum</csymbol><ci id="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.2.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.2">𝒑</ci><apply id="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3.1.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3.2.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3.2">𝒑</ci><ci id="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3.3.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.1.1.1.3.3">𝒒</ci></apply></apply><apply id="S3.SS3.p3.5.m5.3.3.1.1.2.2.2.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.2.2.2"><csymbol cd="latexml" id="S3.SS3.p3.5.m5.3.3.1.1.2.2.2.1.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.2.2.2.1">for-all</csymbol><ci id="S3.SS3.p3.5.m5.3.3.1.1.2.2.2.2.cmml" xref="S3.SS3.p3.5.m5.3.3.1.1.2.2.2.2">𝑝</ci></apply></list></apply><apply id="S3.SS3.p3.5.m5.4.4.2.2.3.cmml" xref="S3.SS3.p3.5.m5.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.5.m5.4.4.2.2.3a.cmml" xref="S3.SS3.p3.5.m5.4.4.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS3.p3.5.m5.4.4.2.2.1.1.cmml" xref="S3.SS3.p3.5.m5.4.4.2.2.1.1"><in id="S3.SS3.p3.5.m5.4.4.2.2.1.1.1.cmml" xref="S3.SS3.p3.5.m5.4.4.2.2.1.1.1"></in><ci id="S3.SS3.p3.5.m5.4.4.2.2.1.1.2.cmml" xref="S3.SS3.p3.5.m5.4.4.2.2.1.1.2">𝑞</ci><set id="S3.SS3.p3.5.m5.4.4.2.2.1.1.3.1.cmml" xref="S3.SS3.p3.5.m5.4.4.2.2.1.1.3.2"><ci id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">𝑎</ci><ci id="S3.SS3.p3.5.m5.2.2.cmml" xref="S3.SS3.p3.5.m5.2.2">𝑣</ci></set></apply><apply id="S3.SS3.p3.5.m5.4.4.2.2.2.2.cmml" xref="S3.SS3.p3.5.m5.4.4.2.2.2.2"><neq id="S3.SS3.p3.5.m5.4.4.2.2.2.2.1.cmml" xref="S3.SS3.p3.5.m5.4.4.2.2.2.2.1"></neq><ci id="S3.SS3.p3.5.m5.4.4.2.2.2.2.2.cmml" xref="S3.SS3.p3.5.m5.4.4.2.2.2.2.2">𝑝</ci><ci id="S3.SS3.p3.5.m5.4.4.2.2.2.2.3.cmml" xref="S3.SS3.p3.5.m5.4.4.2.2.2.2.3">𝑞</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.4c">\bm{f_{p}}=\bm{p}\,\oplus\,\bm{p_{q}},\;\forall p,q\in\{a,v\},p\neq q</annotation></semantics></math>,
and <math id="S3.SS3.p3.6.m6.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S3.SS3.p3.6.m6.1a"><mo id="S3.SS3.p3.6.m6.1.1" xref="S3.SS3.p3.6.m6.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m6.1b"><csymbol cd="latexml" id="S3.SS3.p3.6.m6.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m6.1c">\oplus</annotation></semantics></math> is the concatenation operator along the feature dimension.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.7" class="ltx_p"><span id="S3.SS3.p4.7.1" class="ltx_text ltx_font_bold">Classifier Network.</span>
The classifier network, <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mi id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">Q</annotation></semantics></math>, takes as input the combined embeddings of each modality <math id="S3.SS3.p4.2.m2.2" class="ltx_Math" alttext="(\bm{f_{a}},\bm{f_{v}})" display="inline"><semantics id="S3.SS3.p4.2.m2.2a"><mrow id="S3.SS3.p4.2.m2.2.2.2" xref="S3.SS3.p4.2.m2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p4.2.m2.2.2.2.3" xref="S3.SS3.p4.2.m2.2.2.3.cmml">(</mo><msub id="S3.SS3.p4.2.m2.1.1.1.1" xref="S3.SS3.p4.2.m2.1.1.1.1.cmml"><mi id="S3.SS3.p4.2.m2.1.1.1.1.2" xref="S3.SS3.p4.2.m2.1.1.1.1.2.cmml">𝒇</mi><mi id="S3.SS3.p4.2.m2.1.1.1.1.3" xref="S3.SS3.p4.2.m2.1.1.1.1.3.cmml">𝒂</mi></msub><mo id="S3.SS3.p4.2.m2.2.2.2.4" xref="S3.SS3.p4.2.m2.2.2.3.cmml">,</mo><msub id="S3.SS3.p4.2.m2.2.2.2.2" xref="S3.SS3.p4.2.m2.2.2.2.2.cmml"><mi id="S3.SS3.p4.2.m2.2.2.2.2.2" xref="S3.SS3.p4.2.m2.2.2.2.2.2.cmml">𝒇</mi><mi id="S3.SS3.p4.2.m2.2.2.2.2.3" xref="S3.SS3.p4.2.m2.2.2.2.2.3.cmml">𝒗</mi></msub><mo stretchy="false" id="S3.SS3.p4.2.m2.2.2.2.5" xref="S3.SS3.p4.2.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.2b"><interval closure="open" id="S3.SS3.p4.2.m2.2.2.3.cmml" xref="S3.SS3.p4.2.m2.2.2.2"><apply id="S3.SS3.p4.2.m2.1.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p4.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.p4.2.m2.1.1.1.1.2">𝒇</ci><ci id="S3.SS3.p4.2.m2.1.1.1.1.3.cmml" xref="S3.SS3.p4.2.m2.1.1.1.1.3">𝒂</ci></apply><apply id="S3.SS3.p4.2.m2.2.2.2.2.cmml" xref="S3.SS3.p4.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.2.2.2.2.1.cmml" xref="S3.SS3.p4.2.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p4.2.m2.2.2.2.2.2.cmml" xref="S3.SS3.p4.2.m2.2.2.2.2.2">𝒇</ci><ci id="S3.SS3.p4.2.m2.2.2.2.2.3.cmml" xref="S3.SS3.p4.2.m2.2.2.2.2.3">𝒗</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.2c">(\bm{f_{a}},\bm{f_{v}})</annotation></semantics></math>, and predicts if a given sample is real or fake. The classifier network consists of two uni-modal patch reduction networks: <math id="S3.SS3.p4.3.m3.1" class="ltx_math_unparsed" alttext="(\Psi_{a},\Psi_{v}" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><mrow id="S3.SS3.p4.3.m3.1b"><mo stretchy="false" id="S3.SS3.p4.3.m3.1.1">(</mo><msub id="S3.SS3.p4.3.m3.1.2"><mi mathvariant="normal" id="S3.SS3.p4.3.m3.1.2.2">Ψ</mi><mi id="S3.SS3.p4.3.m3.1.2.3">a</mi></msub><mo id="S3.SS3.p4.3.m3.1.3">,</mo><msub id="S3.SS3.p4.3.m3.1.4"><mi mathvariant="normal" id="S3.SS3.p4.3.m3.1.4.2">Ψ</mi><mi id="S3.SS3.p4.3.m3.1.4.3">v</mi></msub></mrow><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">(\Psi_{a},\Psi_{v}</annotation></semantics></math>), followed by a classifier head, <math id="S3.SS3.p4.4.m4.1" class="ltx_Math" alttext="\Gamma" display="inline"><semantics id="S3.SS3.p4.4.m4.1a"><mi mathvariant="normal" id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml">Γ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><ci id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1">Γ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">\Gamma</annotation></semantics></math>. Each embedding <math id="S3.SS3.p4.5.m5.2" class="ltx_Math" alttext="(\bm{f_{a}},\bm{f_{v}})" display="inline"><semantics id="S3.SS3.p4.5.m5.2a"><mrow id="S3.SS3.p4.5.m5.2.2.2" xref="S3.SS3.p4.5.m5.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p4.5.m5.2.2.2.3" xref="S3.SS3.p4.5.m5.2.2.3.cmml">(</mo><msub id="S3.SS3.p4.5.m5.1.1.1.1" xref="S3.SS3.p4.5.m5.1.1.1.1.cmml"><mi id="S3.SS3.p4.5.m5.1.1.1.1.2" xref="S3.SS3.p4.5.m5.1.1.1.1.2.cmml">𝒇</mi><mi id="S3.SS3.p4.5.m5.1.1.1.1.3" xref="S3.SS3.p4.5.m5.1.1.1.1.3.cmml">𝒂</mi></msub><mo id="S3.SS3.p4.5.m5.2.2.2.4" xref="S3.SS3.p4.5.m5.2.2.3.cmml">,</mo><msub id="S3.SS3.p4.5.m5.2.2.2.2" xref="S3.SS3.p4.5.m5.2.2.2.2.cmml"><mi id="S3.SS3.p4.5.m5.2.2.2.2.2" xref="S3.SS3.p4.5.m5.2.2.2.2.2.cmml">𝒇</mi><mi id="S3.SS3.p4.5.m5.2.2.2.2.3" xref="S3.SS3.p4.5.m5.2.2.2.2.3.cmml">𝒗</mi></msub><mo stretchy="false" id="S3.SS3.p4.5.m5.2.2.2.5" xref="S3.SS3.p4.5.m5.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m5.2b"><interval closure="open" id="S3.SS3.p4.5.m5.2.2.3.cmml" xref="S3.SS3.p4.5.m5.2.2.2"><apply id="S3.SS3.p4.5.m5.1.1.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.5.m5.1.1.1.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p4.5.m5.1.1.1.1.2.cmml" xref="S3.SS3.p4.5.m5.1.1.1.1.2">𝒇</ci><ci id="S3.SS3.p4.5.m5.1.1.1.1.3.cmml" xref="S3.SS3.p4.5.m5.1.1.1.1.3">𝒂</ci></apply><apply id="S3.SS3.p4.5.m5.2.2.2.2.cmml" xref="S3.SS3.p4.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p4.5.m5.2.2.2.2.1.cmml" xref="S3.SS3.p4.5.m5.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p4.5.m5.2.2.2.2.2.cmml" xref="S3.SS3.p4.5.m5.2.2.2.2.2">𝒇</ci><ci id="S3.SS3.p4.5.m5.2.2.2.2.3.cmml" xref="S3.SS3.p4.5.m5.2.2.2.2.3">𝒗</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m5.2c">(\bm{f_{a}},\bm{f_{v}})</annotation></semantics></math>, is first distilled in the patch dimension using the corresponding uni-modal patch reduction networks. Then the output embeddings are concatenated along the feature dimension and fed into the classifier head which outputs the logits, <math id="S3.SS3.p4.6.m6.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS3.p4.6.m6.1a"><mi id="S3.SS3.p4.6.m6.1.1" xref="S3.SS3.p4.6.m6.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.6.m6.1b"><ci id="S3.SS3.p4.6.m6.1.1.cmml" xref="S3.SS3.p4.6.m6.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.6.m6.1c">l</annotation></semantics></math>, used to classify if a given sample is real or fake. Formally, <math id="S3.SS3.p4.7.m7.3" class="ltx_Math" alttext="l=Q(\bm{f_{a}},\bm{f_{v}})=\Gamma(\Psi_{a}(\bm{f_{a}})\oplus\Psi_{v}(\bm{f_{v}}))" display="inline"><semantics id="S3.SS3.p4.7.m7.3a"><mrow id="S3.SS3.p4.7.m7.3.3" xref="S3.SS3.p4.7.m7.3.3.cmml"><mi id="S3.SS3.p4.7.m7.3.3.5" xref="S3.SS3.p4.7.m7.3.3.5.cmml">l</mi><mo id="S3.SS3.p4.7.m7.3.3.6" xref="S3.SS3.p4.7.m7.3.3.6.cmml">=</mo><mrow id="S3.SS3.p4.7.m7.2.2.2" xref="S3.SS3.p4.7.m7.2.2.2.cmml"><mi id="S3.SS3.p4.7.m7.2.2.2.4" xref="S3.SS3.p4.7.m7.2.2.2.4.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.7.m7.2.2.2.3" xref="S3.SS3.p4.7.m7.2.2.2.3.cmml">​</mo><mrow id="S3.SS3.p4.7.m7.2.2.2.2.2" xref="S3.SS3.p4.7.m7.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p4.7.m7.2.2.2.2.2.3" xref="S3.SS3.p4.7.m7.2.2.2.2.3.cmml">(</mo><msub id="S3.SS3.p4.7.m7.1.1.1.1.1.1" xref="S3.SS3.p4.7.m7.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p4.7.m7.1.1.1.1.1.1.2" xref="S3.SS3.p4.7.m7.1.1.1.1.1.1.2.cmml">𝒇</mi><mi id="S3.SS3.p4.7.m7.1.1.1.1.1.1.3" xref="S3.SS3.p4.7.m7.1.1.1.1.1.1.3.cmml">𝒂</mi></msub><mo id="S3.SS3.p4.7.m7.2.2.2.2.2.4" xref="S3.SS3.p4.7.m7.2.2.2.2.3.cmml">,</mo><msub id="S3.SS3.p4.7.m7.2.2.2.2.2.2" xref="S3.SS3.p4.7.m7.2.2.2.2.2.2.cmml"><mi id="S3.SS3.p4.7.m7.2.2.2.2.2.2.2" xref="S3.SS3.p4.7.m7.2.2.2.2.2.2.2.cmml">𝒇</mi><mi id="S3.SS3.p4.7.m7.2.2.2.2.2.2.3" xref="S3.SS3.p4.7.m7.2.2.2.2.2.2.3.cmml">𝒗</mi></msub><mo stretchy="false" id="S3.SS3.p4.7.m7.2.2.2.2.2.5" xref="S3.SS3.p4.7.m7.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS3.p4.7.m7.3.3.7" xref="S3.SS3.p4.7.m7.3.3.7.cmml">=</mo><mrow id="S3.SS3.p4.7.m7.3.3.3" xref="S3.SS3.p4.7.m7.3.3.3.cmml"><mi mathvariant="normal" id="S3.SS3.p4.7.m7.3.3.3.3" xref="S3.SS3.p4.7.m7.3.3.3.3.cmml">Γ</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.7.m7.3.3.3.2" xref="S3.SS3.p4.7.m7.3.3.3.2.cmml">​</mo><mrow id="S3.SS3.p4.7.m7.3.3.3.1.1" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p4.7.m7.3.3.3.1.1.2" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.cmml">(</mo><mrow id="S3.SS3.p4.7.m7.3.3.3.1.1.1" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.cmml"><mrow id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.cmml"><msub id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3.cmml"><mi mathvariant="normal" id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3.2" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3.2.cmml">Ψ</mi><mi id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3.3" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3.3.cmml">a</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.2" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.2" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.2" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.2.cmml">𝒇</mi><mi id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.3" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.3.cmml">𝒂</mi></msub><mo stretchy="false" id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.3" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS3.p4.7.m7.3.3.3.1.1.1.3" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.3.cmml">⊕</mo><mrow id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.cmml"><msub id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3.cmml"><mi mathvariant="normal" id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3.2" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3.2.cmml">Ψ</mi><mi id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3.3" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.2" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.2.cmml">​</mo><mrow id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.2" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.cmml">(</mo><msub id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.cmml"><mi id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.2" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.2.cmml">𝒇</mi><mi id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.3" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.3.cmml">𝒗</mi></msub><mo stretchy="false" id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.3" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.SS3.p4.7.m7.3.3.3.1.1.3" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.7.m7.3b"><apply id="S3.SS3.p4.7.m7.3.3.cmml" xref="S3.SS3.p4.7.m7.3.3"><and id="S3.SS3.p4.7.m7.3.3a.cmml" xref="S3.SS3.p4.7.m7.3.3"></and><apply id="S3.SS3.p4.7.m7.3.3b.cmml" xref="S3.SS3.p4.7.m7.3.3"><eq id="S3.SS3.p4.7.m7.3.3.6.cmml" xref="S3.SS3.p4.7.m7.3.3.6"></eq><ci id="S3.SS3.p4.7.m7.3.3.5.cmml" xref="S3.SS3.p4.7.m7.3.3.5">𝑙</ci><apply id="S3.SS3.p4.7.m7.2.2.2.cmml" xref="S3.SS3.p4.7.m7.2.2.2"><times id="S3.SS3.p4.7.m7.2.2.2.3.cmml" xref="S3.SS3.p4.7.m7.2.2.2.3"></times><ci id="S3.SS3.p4.7.m7.2.2.2.4.cmml" xref="S3.SS3.p4.7.m7.2.2.2.4">𝑄</ci><interval closure="open" id="S3.SS3.p4.7.m7.2.2.2.2.3.cmml" xref="S3.SS3.p4.7.m7.2.2.2.2.2"><apply id="S3.SS3.p4.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS3.p4.7.m7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p4.7.m7.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p4.7.m7.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p4.7.m7.1.1.1.1.1.1.2">𝒇</ci><ci id="S3.SS3.p4.7.m7.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p4.7.m7.1.1.1.1.1.1.3">𝒂</ci></apply><apply id="S3.SS3.p4.7.m7.2.2.2.2.2.2.cmml" xref="S3.SS3.p4.7.m7.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p4.7.m7.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p4.7.m7.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p4.7.m7.2.2.2.2.2.2.2">𝒇</ci><ci id="S3.SS3.p4.7.m7.2.2.2.2.2.2.3.cmml" xref="S3.SS3.p4.7.m7.2.2.2.2.2.2.3">𝒗</ci></apply></interval></apply></apply><apply id="S3.SS3.p4.7.m7.3.3c.cmml" xref="S3.SS3.p4.7.m7.3.3"><eq id="S3.SS3.p4.7.m7.3.3.7.cmml" xref="S3.SS3.p4.7.m7.3.3.7"></eq><share href="#S3.SS3.p4.7.m7.2.2.2.cmml" id="S3.SS3.p4.7.m7.3.3d.cmml" xref="S3.SS3.p4.7.m7.3.3"></share><apply id="S3.SS3.p4.7.m7.3.3.3.cmml" xref="S3.SS3.p4.7.m7.3.3.3"><times id="S3.SS3.p4.7.m7.3.3.3.2.cmml" xref="S3.SS3.p4.7.m7.3.3.3.2"></times><ci id="S3.SS3.p4.7.m7.3.3.3.3.cmml" xref="S3.SS3.p4.7.m7.3.3.3.3">Γ</ci><apply id="S3.SS3.p4.7.m7.3.3.3.1.1.1.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1"><csymbol cd="latexml" id="S3.SS3.p4.7.m7.3.3.3.1.1.1.3.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.3">direct-sum</csymbol><apply id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1"><times id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.2.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.2"></times><apply id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3.1.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3">subscript</csymbol><ci id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3.2.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3.2">Ψ</ci><ci id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3.3.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.3.3">𝑎</ci></apply><apply id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.2">𝒇</ci><ci id="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.1.1.1.1.3">𝒂</ci></apply></apply><apply id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2"><times id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.2.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.2"></times><apply id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3.1.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3">subscript</csymbol><ci id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3.2.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3.2">Ψ</ci><ci id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3.3.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.3.3">𝑣</ci></apply><apply id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.1.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1">subscript</csymbol><ci id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.2.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.2">𝒇</ci><ci id="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.3.cmml" xref="S3.SS3.p4.7.m7.3.3.3.1.1.1.2.1.1.1.3">𝒗</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.7.m7.3c">l=Q(\bm{f_{a}},\bm{f_{v}})=\Gamma(\Psi_{a}(\bm{f_{a}})\oplus\Psi_{v}(\bm{f_{v}}))</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.3" class="ltx_p"><span id="S3.SS3.p5.3.1" class="ltx_text ltx_font_bold">Loss Function.</span>
We use the standard cross-entropy loss, denoted by <math id="S3.SS3.p5.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{CE}" display="inline"><semantics id="S3.SS3.p5.1.m1.1a"><msub id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p5.1.m1.1.1.2" xref="S3.SS3.p5.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S3.SS3.p5.1.m1.1.1.3" xref="S3.SS3.p5.1.m1.1.1.3.cmml"><mi id="S3.SS3.p5.1.m1.1.1.3.2" xref="S3.SS3.p5.1.m1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.1.3.1" xref="S3.SS3.p5.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.1.1.3.3" xref="S3.SS3.p5.1.m1.1.1.3.3.cmml">E</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><apply id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p5.1.m1.1.1.2.cmml" xref="S3.SS3.p5.1.m1.1.1.2">ℒ</ci><apply id="S3.SS3.p5.1.m1.1.1.3.cmml" xref="S3.SS3.p5.1.m1.1.1.3"><times id="S3.SS3.p5.1.m1.1.1.3.1.cmml" xref="S3.SS3.p5.1.m1.1.1.3.1"></times><ci id="S3.SS3.p5.1.m1.1.1.3.2.cmml" xref="S3.SS3.p5.1.m1.1.1.3.2">𝐶</ci><ci id="S3.SS3.p5.1.m1.1.1.3.3.cmml" xref="S3.SS3.p5.1.m1.1.1.3.3">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">\mathcal{L}_{CE}</annotation></semantics></math> as the learning objective, computed using the input labels, <math id="S3.SS3.p5.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS3.p5.2.m2.1a"><mi id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><ci id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">y</annotation></semantics></math>, and the output logits, <math id="S3.SS3.p5.3.m3.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS3.p5.3.m3.1a"><mi id="S3.SS3.p5.3.m3.1.1" xref="S3.SS3.p5.3.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.1b"><ci id="S3.SS3.p5.3.m3.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.1c">l</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para ltx_noindent">
<p id="S3.SS3.p6.2" class="ltx_p"><span id="S3.SS3.p6.2.1" class="ltx_text ltx_font_bold">Deepfake Classifier Inference Stage.</span>
During inference, we first split the video into blocks of time <math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><mi id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><ci id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">T</annotation></semantics></math> (the sample length during training) with a step size of <math id="S3.SS3.p6.2.m2.1" class="ltx_Math" alttext="T/8" display="inline"><semantics id="S3.SS3.p6.2.m2.1a"><mrow id="S3.SS3.p6.2.m2.1.1" xref="S3.SS3.p6.2.m2.1.1.cmml"><mi id="S3.SS3.p6.2.m2.1.1.2" xref="S3.SS3.p6.2.m2.1.1.2.cmml">T</mi><mo id="S3.SS3.p6.2.m2.1.1.1" xref="S3.SS3.p6.2.m2.1.1.1.cmml">/</mo><mn id="S3.SS3.p6.2.m2.1.1.3" xref="S3.SS3.p6.2.m2.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m2.1b"><apply id="S3.SS3.p6.2.m2.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1"><divide id="S3.SS3.p6.2.m2.1.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1.1"></divide><ci id="S3.SS3.p6.2.m2.1.1.2.cmml" xref="S3.SS3.p6.2.m2.1.1.2">𝑇</ci><cn type="integer" id="S3.SS3.p6.2.m2.1.1.3.cmml" xref="S3.SS3.p6.2.m2.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m2.1c">T/8</annotation></semantics></math>, which is the duration of a temporal slice. The output logits are computed for each of the blocks and the classification decision (real or fake) is made based on the mean of the output logits.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We train Stage 1 (representation learning), using the LRS3 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, which exclusively contains real videos. In Stage 2 (deepfake classification), we train a classifier that follows a supervised learning approach using the FakeAVCeleb <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> dataset. FakeAVCeleb comprises of both real and fake videos, where either one or both audio-visual modalities have been synthesized using different combinations of several generative deepfake algorithms (visual: FaceSwap<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>, FSGAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>, and Wav2Lip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>; audio: SV2TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>). Please refer to the supplementary for additional details on datasets, architecture, and implementation.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<p id="S4.T1.2" class="ltx_p ltx_align_center"><span id="S4.T1.2.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S4.T1.2.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:367.4pt;height:216pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T1.2.1.1.1" class="ltx_p"><span id="S4.T1.2.1.1.1.1" class="ltx_text">
<span id="S4.T1.2.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T1.2.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T1.2.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt">Method</span>
<span id="S4.T1.2.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Modality</span>
<span id="S4.T1.2.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">ACC</span>
<span id="S4.T1.2.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">AUC</span></span>
<span id="S4.T1.2.1.1.1.1.1.2.2" class="ltx_tr">
<span id="S4.T1.2.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">Xception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite></span>
<span id="S4.T1.2.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">V</span>
<span id="S4.T1.2.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">67.9</span>
<span id="S4.T1.2.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">70.5</span></span>
<span id="S4.T1.2.1.1.1.1.1.3.3" class="ltx_tr">
<span id="S4.T1.2.1.1.1.1.1.3.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">LipForensics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite></span>
<span id="S4.T1.2.1.1.1.1.1.3.3.2" class="ltx_td ltx_align_center">V</span>
<span id="S4.T1.2.1.1.1.1.1.3.3.3" class="ltx_td ltx_align_center">80.1</span>
<span id="S4.T1.2.1.1.1.1.1.3.3.4" class="ltx_td ltx_align_center">82.4</span></span>
<span id="S4.T1.2.1.1.1.1.1.4.4" class="ltx_tr">
<span id="S4.T1.2.1.1.1.1.1.4.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">FTCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite></span>
<span id="S4.T1.2.1.1.1.1.1.4.4.2" class="ltx_td ltx_align_center">V</span>
<span id="S4.T1.2.1.1.1.1.1.4.4.3" class="ltx_td ltx_align_center">64.9</span>
<span id="S4.T1.2.1.1.1.1.1.4.4.4" class="ltx_td ltx_align_center">84.0</span></span>
<span id="S4.T1.2.1.1.1.1.1.5.5" class="ltx_tr">
<span id="S4.T1.2.1.1.1.1.1.5.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">CViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>]</cite></span>
<span id="S4.T1.2.1.1.1.1.1.5.5.2" class="ltx_td ltx_align_center">V</span>
<span id="S4.T1.2.1.1.1.1.1.5.5.3" class="ltx_td ltx_align_center">69.7</span>
<span id="S4.T1.2.1.1.1.1.1.5.5.4" class="ltx_td ltx_align_center">71.8</span></span>
<span id="S4.T1.2.1.1.1.1.1.6.6" class="ltx_tr">
<span id="S4.T1.2.1.1.1.1.1.6.6.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">RealForensics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite></span>
<span id="S4.T1.2.1.1.1.1.1.6.6.2" class="ltx_td ltx_align_center">V</span>
<span id="S4.T1.2.1.1.1.1.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.1.1.1.1.1.6.6.3.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">89.9</span></span>
<span id="S4.T1.2.1.1.1.1.1.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.1.1.1.1.1.6.6.4.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">94.6</span></span></span>
<span id="S4.T1.2.1.1.1.1.1.7.7" class="ltx_tr">
<span id="S4.T1.2.1.1.1.1.1.7.7.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">Emotions Don’t Lie <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite></span>
<span id="S4.T1.2.1.1.1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">AV</span>
<span id="S4.T1.2.1.1.1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">78.1</span>
<span id="S4.T1.2.1.1.1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t">79.8</span></span>
<span id="S4.T1.2.1.1.1.1.1.8.8" class="ltx_tr">
<span id="S4.T1.2.1.1.1.1.1.8.8.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">MDS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite></span>
<span id="S4.T1.2.1.1.1.1.1.8.8.2" class="ltx_td ltx_align_center">AV</span>
<span id="S4.T1.2.1.1.1.1.1.8.8.3" class="ltx_td ltx_align_center">82.8</span>
<span id="S4.T1.2.1.1.1.1.1.8.8.4" class="ltx_td ltx_align_center">86.5</span></span>
<span id="S4.T1.2.1.1.1.1.1.9.9" class="ltx_tr">
<span id="S4.T1.2.1.1.1.1.1.9.9.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">AVFakeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite></span>
<span id="S4.T1.2.1.1.1.1.1.9.9.2" class="ltx_td ltx_align_center">AV</span>
<span id="S4.T1.2.1.1.1.1.1.9.9.3" class="ltx_td ltx_align_center">78.4</span>
<span id="S4.T1.2.1.1.1.1.1.9.9.4" class="ltx_td ltx_align_center">83.4</span></span>
<span id="S4.T1.2.1.1.1.1.1.10.10" class="ltx_tr">
<span id="S4.T1.2.1.1.1.1.1.10.10.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">VFD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite></span>
<span id="S4.T1.2.1.1.1.1.1.10.10.2" class="ltx_td ltx_align_center">AV</span>
<span id="S4.T1.2.1.1.1.1.1.10.10.3" class="ltx_td ltx_align_center">81.5</span>
<span id="S4.T1.2.1.1.1.1.1.10.10.4" class="ltx_td ltx_align_center">86.1</span></span>
<span id="S4.T1.2.1.1.1.1.1.11.11" class="ltx_tr">
<span id="S4.T1.2.1.1.1.1.1.11.11.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">AVoiD-DF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite></span>
<span id="S4.T1.2.1.1.1.1.1.11.11.2" class="ltx_td ltx_align_center">AV</span>
<span id="S4.T1.2.1.1.1.1.1.11.11.3" class="ltx_td ltx_align_center"><span id="S4.T1.2.1.1.1.1.1.11.11.3.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">83.7</span></span>
<span id="S4.T1.2.1.1.1.1.1.11.11.4" class="ltx_td ltx_align_center"><span id="S4.T1.2.1.1.1.1.1.11.11.4.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">89.2</span></span></span>
<span id="S4.T1.2.1.1.1.1.1.12.12" class="ltx_tr">
<span id="S4.T1.2.1.1.1.1.1.12.12.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S4.T1.2.1.1.1.1.1.12.12.1.1" class="ltx_text ltx_font_bold">AVFF (Ours)</span></span>
<span id="S4.T1.2.1.1.1.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">AV</span>
<span id="S4.T1.2.1.1.1.1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.2.1.1.1.1.1.12.12.3.1" class="ltx_text ltx_font_bold">98.6</span></span>
<span id="S4.T1.2.1.1.1.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.2.1.1.1.1.1.12.12.4.1" class="ltx_text ltx_font_bold">99.1</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.4.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Intra-Dataset Performance.<span id="S4.T1.5.2.1" class="ltx_text ltx_font_medium"> We evaluate our method against baselines using a 70%-30% train-test split on the FakeAVCeleb dataset, where we achieve state-of-the-art performance by significant margins. Best result is in bold, and second best per modality is underlined.
</span></span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation and Discussion</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We evaluate the performance of our model against existing state-of-the-art algorithms, on multiple criteria: intra-dataset performance, cross-manipulation generalization, and cross-dataset generalization following  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite>.
We compare our results against state-of-the-art audio-visual approaches and uni-modal (visual) approaches for completeness. We report accuracy (ACC), average precision (AP), and area under the ROC curve (AUC) averaged across multiple runs with different random seeds. For audio-visual algorithms, we label a video as fake if either or both audio and visual modalities have been manipulated. To maintain fairness, for uni-modal algorithms we consider a video as fake only if the visual modality has been manipulated.
Please refer to the supplementary for additional results on robustness to unseen audio and visual perturbations.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Intra-Dataset Performance.</span>
Following the methodology outlined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite>, our training utilizes 70% of all FakeAVCeleb samples, while the remaining 30% constitutes the unseen test set. As denoted in <a href="#S4.T1" title="In 4.1 Implementation ‣ 4 Experiments and Results ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, our approach demonstrates substantial improvements over the existing state-of-the-art, both in audio-visual (AVoiD-DF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite>) and uni-modal (RealForensics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>) deepfake detection. Compared to AVoiD-DF, our method achieves an increase in accuracy of 14.9% (+9.9% in AUC), and compared to RealForensics the accuracy increases by 8.7% (+4.5% AUC).
Overall, the superior performance of audio-visual methods leveraging cross-modal correspondence is evident, outperforming uni-modal approaches that rely on uni-modal artifacts (<em id="S4.SS2.p2.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS2.p2.1.3" class="ltx_text"></span> visual anomalies) introduced by deepfake algorithms. RealForensics, while competitive, discards the audio modality during detection, limiting its applicability exclusively to visual deepfakes. This hinders its practicality as contemporary deepfakes often involve manipulations in both audio and visual modalities.
The enhanced results of both RealForensics and our proposed method highlight the positive impact of employing a pre-training stage for effective representation learning. This observation aligns with findings in other multi-modal representation learning research across diverse downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<p id="S4.T2.2" class="ltx_p ltx_align_center"><span id="S4.T2.2.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S4.T2.2.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:678.8pt;height:181pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T2.2.1.1.1" class="ltx_p"><span id="S4.T2.2.1.1.1.1" class="ltx_text">
<span id="S4.T2.2.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T2.2.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T2.2.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S4.T2.2.1.1.1.1.1.1.1.1.1" class="ltx_text">Method</span></span>
<span id="S4.T2.2.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S4.T2.2.1.1.1.1.1.1.1.2.1" class="ltx_text">Modality</span></span>
<span id="S4.T2.2.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2">RVFA</span>
<span id="S4.T2.2.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2">FVRA-WL</span>
<span id="S4.T2.2.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2">FVFA-FS</span>
<span id="S4.T2.2.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2">FVFA-GAN</span>
<span id="S4.T2.2.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2">FVFA-WL</span>
<span id="S4.T2.2.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2">AVG-FV</span></span>
<span id="S4.T2.2.1.1.1.1.1.2.2" class="ltx_tr">
<span id="S4.T2.2.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">AP</span>
<span id="S4.T2.2.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">AUC</span>
<span id="S4.T2.2.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">AP</span>
<span id="S4.T2.2.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">AUC</span>
<span id="S4.T2.2.1.1.1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">AP</span>
<span id="S4.T2.2.1.1.1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">AUC</span>
<span id="S4.T2.2.1.1.1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">AP</span>
<span id="S4.T2.2.1.1.1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t">AUC</span>
<span id="S4.T2.2.1.1.1.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">AP</span>
<span id="S4.T2.2.1.1.1.1.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t">AUC</span>
<span id="S4.T2.2.1.1.1.1.1.2.2.11" class="ltx_td ltx_align_center ltx_border_t">AP</span>
<span id="S4.T2.2.1.1.1.1.1.2.2.12" class="ltx_td ltx_align_center ltx_border_t">AUC</span></span>
<span id="S4.T2.2.1.1.1.1.1.3.3" class="ltx_tr">
<span id="S4.T2.2.1.1.1.1.1.3.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">Xception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite></span>
<span id="S4.T2.2.1.1.1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">V</span>
<span id="S4.T2.2.1.1.1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S4.T2.2.1.1.1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S4.T2.2.1.1.1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">88.2</span>
<span id="S4.T2.2.1.1.1.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">88.3</span>
<span id="S4.T2.2.1.1.1.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">92.3</span>
<span id="S4.T2.2.1.1.1.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">93.5</span>
<span id="S4.T2.2.1.1.1.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">67.6</span>
<span id="S4.T2.2.1.1.1.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t">68.5</span>
<span id="S4.T2.2.1.1.1.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t">91.0</span>
<span id="S4.T2.2.1.1.1.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_t">91.0</span>
<span id="S4.T2.2.1.1.1.1.1.3.3.13" class="ltx_td ltx_align_center ltx_border_t">84.8</span>
<span id="S4.T2.2.1.1.1.1.1.3.3.14" class="ltx_td ltx_align_center ltx_border_t">85.3</span></span>
<span id="S4.T2.2.1.1.1.1.1.4.4" class="ltx_tr">
<span id="S4.T2.2.1.1.1.1.1.4.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">LipForensics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite></span>
<span id="S4.T2.2.1.1.1.1.1.4.4.2" class="ltx_td ltx_align_center">V</span>
<span id="S4.T2.2.1.1.1.1.1.4.4.3" class="ltx_td ltx_align_center">-</span>
<span id="S4.T2.2.1.1.1.1.1.4.4.4" class="ltx_td ltx_align_center">-</span>
<span id="S4.T2.2.1.1.1.1.1.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.1.1.1.1.4.4.5.1" class="ltx_text ltx_font_bold">97.8</span></span>
<span id="S4.T2.2.1.1.1.1.1.4.4.6" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.1.1.1.1.4.4.6.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">97.7</span></span>
<span id="S4.T2.2.1.1.1.1.1.4.4.7" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.1.1.1.1.4.4.7.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">99.9</span></span>
<span id="S4.T2.2.1.1.1.1.1.4.4.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.1.1.1.1.4.4.8.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">99.9</span></span>
<span id="S4.T2.2.1.1.1.1.1.4.4.9" class="ltx_td ltx_align_center">61.5</span>
<span id="S4.T2.2.1.1.1.1.1.4.4.10" class="ltx_td ltx_align_center">68.1</span>
<span id="S4.T2.2.1.1.1.1.1.4.4.11" class="ltx_td ltx_align_center">98.6</span>
<span id="S4.T2.2.1.1.1.1.1.4.4.12" class="ltx_td ltx_align_center">98.7</span>
<span id="S4.T2.2.1.1.1.1.1.4.4.13" class="ltx_td ltx_align_center">89.4</span>
<span id="S4.T2.2.1.1.1.1.1.4.4.14" class="ltx_td ltx_align_center">91.1</span></span>
<span id="S4.T2.2.1.1.1.1.1.5.5" class="ltx_tr">
<span id="S4.T2.2.1.1.1.1.1.5.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">FTCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite></span>
<span id="S4.T2.2.1.1.1.1.1.5.5.2" class="ltx_td ltx_align_center">V</span>
<span id="S4.T2.2.1.1.1.1.1.5.5.3" class="ltx_td ltx_align_center">-</span>
<span id="S4.T2.2.1.1.1.1.1.5.5.4" class="ltx_td ltx_align_center">-</span>
<span id="S4.T2.2.1.1.1.1.1.5.5.5" class="ltx_td ltx_align_center">96.2</span>
<span id="S4.T2.2.1.1.1.1.1.5.5.6" class="ltx_td ltx_align_center">97.4</span>
<span id="S4.T2.2.1.1.1.1.1.5.5.7" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.1.1.1.1.5.5.7.1" class="ltx_text ltx_font_bold">100.</span></span>
<span id="S4.T2.2.1.1.1.1.1.5.5.8" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.1.1.1.1.5.5.8.1" class="ltx_text ltx_font_bold">100.</span></span>
<span id="S4.T2.2.1.1.1.1.1.5.5.9" class="ltx_td ltx_align_center">77.4</span>
<span id="S4.T2.2.1.1.1.1.1.5.5.10" class="ltx_td ltx_align_center">78.3</span>
<span id="S4.T2.2.1.1.1.1.1.5.5.11" class="ltx_td ltx_align_center">95.6</span>
<span id="S4.T2.2.1.1.1.1.1.5.5.12" class="ltx_td ltx_align_center">96.5</span>
<span id="S4.T2.2.1.1.1.1.1.5.5.13" class="ltx_td ltx_align_center">92.3</span>
<span id="S4.T2.2.1.1.1.1.1.5.5.14" class="ltx_td ltx_align_center">93.1</span></span>
<span id="S4.T2.2.1.1.1.1.1.6.6" class="ltx_tr">
<span id="S4.T2.2.1.1.1.1.1.6.6.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">RealForensics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite></span>
<span id="S4.T2.2.1.1.1.1.1.6.6.2" class="ltx_td ltx_align_center">V</span>
<span id="S4.T2.2.1.1.1.1.1.6.6.3" class="ltx_td ltx_align_center">-</span>
<span id="S4.T2.2.1.1.1.1.1.6.6.4" class="ltx_td ltx_align_center">-</span>
<span id="S4.T2.2.1.1.1.1.1.6.6.5" class="ltx_td ltx_align_center">88.8</span>
<span id="S4.T2.2.1.1.1.1.1.6.6.6" class="ltx_td ltx_align_center">93.0</span>
<span id="S4.T2.2.1.1.1.1.1.6.6.7" class="ltx_td ltx_align_center">99.3</span>
<span id="S4.T2.2.1.1.1.1.1.6.6.8" class="ltx_td ltx_align_center">99.1</span>
<span id="S4.T2.2.1.1.1.1.1.6.6.9" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.1.1.1.1.6.6.9.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">99.8</span></span>
<span id="S4.T2.2.1.1.1.1.1.6.6.10" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.1.1.1.1.6.6.10.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">99.8</span></span>
<span id="S4.T2.2.1.1.1.1.1.6.6.11" class="ltx_td ltx_align_center">93.4</span>
<span id="S4.T2.2.1.1.1.1.1.6.6.12" class="ltx_td ltx_align_center">96.7</span>
<span id="S4.T2.2.1.1.1.1.1.6.6.13" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.1.1.1.1.6.6.13.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">95.3</span></span>
<span id="S4.T2.2.1.1.1.1.1.6.6.14" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.1.1.1.1.6.6.14.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">97.1</span></span></span>
<span id="S4.T2.2.1.1.1.1.1.7.7" class="ltx_tr">
<span id="S4.T2.2.1.1.1.1.1.7.7.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">AV-DFD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">71</span></a>]</cite></span>
<span id="S4.T2.2.1.1.1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">AV</span>
<span id="S4.T2.2.1.1.1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.7.7.3.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">74.9</span></span>
<span id="S4.T2.2.1.1.1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t">73.3</span>
<span id="S4.T2.2.1.1.1.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.7.7.5.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">97.0</span></span>
<span id="S4.T2.2.1.1.1.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t">97.4</span>
<span id="S4.T2.2.1.1.1.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_t">99.6</span>
<span id="S4.T2.2.1.1.1.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_t">99.7</span>
<span id="S4.T2.2.1.1.1.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_t">58.4</span>
<span id="S4.T2.2.1.1.1.1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_t">55.4</span>
<span id="S4.T2.2.1.1.1.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.7.7.11.1" class="ltx_text ltx_font_bold">100.</span></span>
<span id="S4.T2.2.1.1.1.1.1.7.7.12" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.7.7.12.1" class="ltx_text ltx_font_bold">100.</span></span>
<span id="S4.T2.2.1.1.1.1.1.7.7.13" class="ltx_td ltx_align_center ltx_border_t">88.8</span>
<span id="S4.T2.2.1.1.1.1.1.7.7.14" class="ltx_td ltx_align_center ltx_border_t">88.1</span></span>
<span id="S4.T2.2.1.1.1.1.1.8.8" class="ltx_tr">
<span id="S4.T2.2.1.1.1.1.1.8.8.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">AVAD (LRS2) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite></span>
<span id="S4.T2.2.1.1.1.1.1.8.8.2" class="ltx_td ltx_align_center">AV</span>
<span id="S4.T2.2.1.1.1.1.1.8.8.3" class="ltx_td ltx_align_center">62.4</span>
<span id="S4.T2.2.1.1.1.1.1.8.8.4" class="ltx_td ltx_align_center">71.6</span>
<span id="S4.T2.2.1.1.1.1.1.8.8.5" class="ltx_td ltx_align_center">93.6</span>
<span id="S4.T2.2.1.1.1.1.1.8.8.6" class="ltx_td ltx_align_center">93.7</span>
<span id="S4.T2.2.1.1.1.1.1.8.8.7" class="ltx_td ltx_align_center">95.3</span>
<span id="S4.T2.2.1.1.1.1.1.8.8.8" class="ltx_td ltx_align_center">95.8</span>
<span id="S4.T2.2.1.1.1.1.1.8.8.9" class="ltx_td ltx_align_center">94.1</span>
<span id="S4.T2.2.1.1.1.1.1.8.8.10" class="ltx_td ltx_align_center">94.3</span>
<span id="S4.T2.2.1.1.1.1.1.8.8.11" class="ltx_td ltx_align_center">93.8</span>
<span id="S4.T2.2.1.1.1.1.1.8.8.12" class="ltx_td ltx_align_center">94.1</span>
<span id="S4.T2.2.1.1.1.1.1.8.8.13" class="ltx_td ltx_align_center">94.2</span>
<span id="S4.T2.2.1.1.1.1.1.8.8.14" class="ltx_td ltx_align_center">94.5</span></span>
<span id="S4.T2.2.1.1.1.1.1.9.9" class="ltx_tr">
<span id="S4.T2.2.1.1.1.1.1.9.9.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">AVAD (LRS3) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite></span>
<span id="S4.T2.2.1.1.1.1.1.9.9.2" class="ltx_td ltx_align_center">AV</span>
<span id="S4.T2.2.1.1.1.1.1.9.9.3" class="ltx_td ltx_align_center">70.7</span>
<span id="S4.T2.2.1.1.1.1.1.9.9.4" class="ltx_td ltx_align_center"><span id="S4.T2.2.1.1.1.1.1.9.9.4.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">80.5</span></span>
<span id="S4.T2.2.1.1.1.1.1.9.9.5" class="ltx_td ltx_align_center">91.1</span>
<span id="S4.T2.2.1.1.1.1.1.9.9.6" class="ltx_td ltx_align_center">93.0</span>
<span id="S4.T2.2.1.1.1.1.1.9.9.7" class="ltx_td ltx_align_center">91.0</span>
<span id="S4.T2.2.1.1.1.1.1.9.9.8" class="ltx_td ltx_align_center">92.3</span>
<span id="S4.T2.2.1.1.1.1.1.9.9.9" class="ltx_td ltx_align_center">91.6</span>
<span id="S4.T2.2.1.1.1.1.1.9.9.10" class="ltx_td ltx_align_center">92.7</span>
<span id="S4.T2.2.1.1.1.1.1.9.9.11" class="ltx_td ltx_align_center">91.4</span>
<span id="S4.T2.2.1.1.1.1.1.9.9.12" class="ltx_td ltx_align_center">93.1</span>
<span id="S4.T2.2.1.1.1.1.1.9.9.13" class="ltx_td ltx_align_center">91.3</span>
<span id="S4.T2.2.1.1.1.1.1.9.9.14" class="ltx_td ltx_align_center">92.8</span></span>
<span id="S4.T2.2.1.1.1.1.1.10.10" class="ltx_tr">
<span id="S4.T2.2.1.1.1.1.1.10.10.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.10.10.1.1" class="ltx_text ltx_font_bold">AVFF (Ours)</span></span>
<span id="S4.T2.2.1.1.1.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">AV</span>
<span id="S4.T2.2.1.1.1.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.10.10.3.1" class="ltx_text ltx_font_bold">93.3</span></span>
<span id="S4.T2.2.1.1.1.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.10.10.4.1" class="ltx_text ltx_font_bold">92.4</span></span>
<span id="S4.T2.2.1.1.1.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">94.8</span>
<span id="S4.T2.2.1.1.1.1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.10.10.6.1" class="ltx_text ltx_font_bold">98.2</span></span>
<span id="S4.T2.2.1.1.1.1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.10.10.7.1" class="ltx_text ltx_font_bold">100.</span></span>
<span id="S4.T2.2.1.1.1.1.1.10.10.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.10.10.8.1" class="ltx_text ltx_font_bold">100.</span></span>
<span id="S4.T2.2.1.1.1.1.1.10.10.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.10.10.9.1" class="ltx_text ltx_font_bold">99.9</span></span>
<span id="S4.T2.2.1.1.1.1.1.10.10.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.10.10.10.1" class="ltx_text ltx_font_bold">100.</span></span>
<span id="S4.T2.2.1.1.1.1.1.10.10.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.10.10.11.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">99.4</span></span>
<span id="S4.T2.2.1.1.1.1.1.10.10.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.10.10.12.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">99.8</span></span>
<span id="S4.T2.2.1.1.1.1.1.10.10.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.10.10.13.1" class="ltx_text ltx_font_bold">98.5</span></span>
<span id="S4.T2.2.1.1.1.1.1.10.10.14" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.2.1.1.1.1.1.10.10.14.1" class="ltx_text ltx_font_bold">99.5</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.9.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.10.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Cross-Manipulation Generalization on FakeAVCeleb.<span id="S4.T2.10.2.1" class="ltx_text ltx_font_medium">
We evaluate the model’s performance by leaving out one category for testing while training on the rest. We consider the following 5 categories in FakeAVCeleb: (i) </span>RVFA<span id="S4.T2.10.2.2" class="ltx_text ltx_font_medium">: Real Visual - Fake Audio (SV2TTS), (ii) </span>FVRA-WL<span id="S4.T2.10.2.3" class="ltx_text ltx_font_medium">: Fake Visual - Real Audio (Wav2Lip), (iii) </span>FVFA-FS<span id="S4.T2.10.2.4" class="ltx_text ltx_font_medium">: Fake Visual - Fake Audio (FaceSwap + Wav2Lip + SV2TTS), (iv) </span>FVFA-GAN<span id="S4.T2.10.2.5" class="ltx_text ltx_font_medium">: Fake Visual - Fake Audio (FaceSwapGAN + Wav2Lip + SV2TTS), and (v) </span>FVFA-WL<span id="S4.T2.10.2.6" class="ltx_text ltx_font_medium">: Fake Visual - Fake Audio (Wav2Lip + SV2TTS). The column titles correspond to the category of the test set. AVG-FV corresponds to the average metrics of the categories containing fake visuals.
Best result is in bold, and second best is underlined.
Our method yields consistently high performance across all manipulation methods while yielding state-of-the-art performance for several categories.
</span></span></figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Cross-Manipulation Generalization.</span>
In this experiment, we aim to assess the model’s performance on samples generated using previously unseen manipulation methods.
The scalability of deepfake detection algorithms to unseen manipulation methods is crucial for adapting to evolving threats, thus ensuring wide applicability across diverse scenarios.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, we partition the FakeAVCeleb dataset into five categories: RVFA, FVRA-WL, FVFA-FS, FVFA-GAN, and FVFA-WL, based on the algorithms used to generate the deepfakes. Descriptions of these categories are included in the caption of <a href="#S4.T2" title="In 4.2 Evaluation and Discussion ‣ 4 Experiments and Results ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, for ease of reference. Using these categories, we evaluate the model leaving one category out for testing while training on the remaining categories. Results are reported in <a href="#S4.T2" title="In 4.2 Evaluation and Discussion ‣ 4 Experiments and Results ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.
Our method achieves the best performance in almost all cases (and at par with the rest) and notably, yields consistently enhanced performance (AUC &gt; 92+%, AP &gt; 93+%) across all categories, while other baselines (Xception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite>, LipForensics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>, FTCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite>, AV-DFD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">71</span></a>]</cite>) fall short in categories FVFA-GAN and RVFA.
While the performance of AVAD (an unsupervised method) is intriguing, we can notice how other supervised baselines perform better in most cases.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<p id="S4.T3.2" class="ltx_p ltx_align_center"><span id="S4.T3.2.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S4.T3.2.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:363.5pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T3.2.1.1.1" class="ltx_p"><span id="S4.T3.2.1.1.1.1" class="ltx_text">
<span id="S4.T3.2.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T3.2.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T3.2.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt">Method</span>
<span id="S4.T3.2.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Modality</span>
<span id="S4.T3.2.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">AP</span>
<span id="S4.T3.2.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">AUC</span></span>
<span id="S4.T3.2.1.1.1.1.1.2.2" class="ltx_tr">
<span id="S4.T3.2.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">Xception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite></span>
<span id="S4.T3.2.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">V</span>
<span id="S4.T3.2.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">76.9</span>
<span id="S4.T3.2.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">77.7</span></span>
<span id="S4.T3.2.1.1.1.1.1.3.3" class="ltx_tr">
<span id="S4.T3.2.1.1.1.1.1.3.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">LipForensics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite></span>
<span id="S4.T3.2.1.1.1.1.1.3.3.2" class="ltx_td ltx_align_center">V</span>
<span id="S4.T3.2.1.1.1.1.1.3.3.3" class="ltx_td ltx_align_center">89.5</span>
<span id="S4.T3.2.1.1.1.1.1.3.3.4" class="ltx_td ltx_align_center">86.6</span></span>
<span id="S4.T3.2.1.1.1.1.1.4.4" class="ltx_tr">
<span id="S4.T3.2.1.1.1.1.1.4.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">FTCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite></span>
<span id="S4.T3.2.1.1.1.1.1.4.4.2" class="ltx_td ltx_align_center">V</span>
<span id="S4.T3.2.1.1.1.1.1.4.4.3" class="ltx_td ltx_align_center">66.8</span>
<span id="S4.T3.2.1.1.1.1.1.4.4.4" class="ltx_td ltx_align_center">68.1</span></span>
<span id="S4.T3.2.1.1.1.1.1.5.5" class="ltx_tr">
<span id="S4.T3.2.1.1.1.1.1.5.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">RealForensics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite></span>
<span id="S4.T3.2.1.1.1.1.1.5.5.2" class="ltx_td ltx_align_center">V</span>
<span id="S4.T3.2.1.1.1.1.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.1.1.1.1.5.5.3.1" class="ltx_text ltx_font_bold">95.7</span></span>
<span id="S4.T3.2.1.1.1.1.1.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.1.1.1.1.5.5.4.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">93.6</span></span></span>
<span id="S4.T3.2.1.1.1.1.1.6.6" class="ltx_tr">
<span id="S4.T3.2.1.1.1.1.1.6.6.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">AV-DFD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">71</span></a>]</cite></span>
<span id="S4.T3.2.1.1.1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">AV</span>
<span id="S4.T3.2.1.1.1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t">79.6</span>
<span id="S4.T3.2.1.1.1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">82.1</span></span>
<span id="S4.T3.2.1.1.1.1.1.7.7" class="ltx_tr">
<span id="S4.T3.2.1.1.1.1.1.7.7.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">AVAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite></span>
<span id="S4.T3.2.1.1.1.1.1.7.7.2" class="ltx_td ltx_align_center">AV</span>
<span id="S4.T3.2.1.1.1.1.1.7.7.3" class="ltx_td ltx_align_center">87.6</span>
<span id="S4.T3.2.1.1.1.1.1.7.7.4" class="ltx_td ltx_align_center">86.9</span></span>
<span id="S4.T3.2.1.1.1.1.1.8.8" class="ltx_tr">
<span id="S4.T3.2.1.1.1.1.1.8.8.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S4.T3.2.1.1.1.1.1.8.8.1.1" class="ltx_text ltx_font_bold">AVFF (Ours)</span></span>
<span id="S4.T3.2.1.1.1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">AV</span>
<span id="S4.T3.2.1.1.1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.2.1.1.1.1.1.8.8.3.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">93.1</span></span>
<span id="S4.T3.2.1.1.1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.2.1.1.1.1.1.8.8.4.1" class="ltx_text ltx_font_bold">95.5</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.4.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Cross-Dataset Generalization on KoDF.<span id="S4.T3.5.2.1" class="ltx_text ltx_font_medium"> We evaluate our model’s performance against baselines by testing the model trained on the FakeAVCeleb dataset, on a subset of the KoDF dataset. Best result is in bold, and second best is underlined.
</span></span></figcaption>
</figure>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Cross-Dataset Generalization.</span> We also evaluate the adaptability of the model to a different data distribution, by testing on a subset of the KoDF dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>, following the evaluation protocol established by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>. We report the results in <a href="#S4.T3" title="In 4.2 Evaluation and Discussion ‣ 4 Experiments and Results ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>, where we perform at par with RealForensics. Overall, we observe a performance trend similar to cross-generalization to unseen generative methods. Please refer to the supplementary for additional cross-dataset generalization results on DF-TIMIT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> and DFDC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> datasets.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">Analysis on the Learned Representation.</span>
During the training of the downstream task, we observe notable AUC scores on the test set, reaching as high as 90% within the initial 1-3 epochs. Intrigued by this observation, we explore the representations learned at the end of Stage 1 (<a href="#S3.SS2" title="3.2 Representation Learning Stage ‣ 3 Method ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>). We visualize the t-SNE plots of embeddings for random samples from each category of the FakeAVCeleb dataset in <a href="#S4.F4" title="In 4.2 Evaluation and Discussion ‣ 4 Experiments and Results ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>.
We do not expose Stage 1 to any deepfake samples during training, and still observe clear discrimination between real and fake samples.
This explains the high AUC scores achieved even at the very early stages of the downstream training. A consequence of our representation learning stage is that we observe a clear disentanglement between real and deepfake videos within the FakeAVCeleb dataset. Further, distinct clusters are evident for each deepfake category which indicates that our representations are capable of capturing subtle cues that differentiate different deepfake algorithms despite not encountering any of them during the training stage.
<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Stage 1 is exclusively trained on the LRS3 dataset containing only real videos, while samples for the t-SNE are drawn from FakeAVCeleb.</span></span></span>
A further analysis of the t-SNE visualizations reveals that the samples belonging to adjacent clusters are related in terms of the deepfake algorithms used to generate them. For instance, FVRA-WL and FVFA-WL, which are adjacent, both employ Wav2Lip to synthesize the deepfakes (refer to the encircled regions in <a href="#S4.F4" title="In 4.2 Evaluation and Discussion ‣ 4 Experiments and Results ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>).
These findings underscore the efficacy of our novel audio-visual representation learning paradigm. Please refer to the supplementary for the evaluation of the classification performance on the learned representation which further reinforces the above analysis.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2406.02951/assets/Figures/tsne_plot_v2.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="321" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">The t-SNE Visualization of the Embeddings at the end of the Representation Learning Stage<span id="S4.F4.4.2.1" class="ltx_text ltx_font_medium">.
A clear distinction is seen between the representations of real and fake videos, as well as between different deepfake categories. Further analysis indicates that samples of adjacent clusters are generated using the same deepfake algorithm, which we encircle manually to highlight the clusters. </span></span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Ablation Study</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold">Autoencoding Objective.</span>
In this experiment, we train the model using only the contrastive loss objective, discarding the autoencoding objective. This deactivates complementary masking, cross-modality fusion, and decoding modules. The feature embeddings at the output of the encoders <math id="S5.p1.1.m1.2" class="ltx_Math" alttext="(\bm{a},\bm{v})" display="inline"><semantics id="S5.p1.1.m1.2a"><mrow id="S5.p1.1.m1.2.3.2" xref="S5.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S5.p1.1.m1.2.3.2.1" xref="S5.p1.1.m1.2.3.1.cmml">(</mo><mi id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">𝒂</mi><mo id="S5.p1.1.m1.2.3.2.2" xref="S5.p1.1.m1.2.3.1.cmml">,</mo><mi id="S5.p1.1.m1.2.2" xref="S5.p1.1.m1.2.2.cmml">𝒗</mi><mo stretchy="false" id="S5.p1.1.m1.2.3.2.3" xref="S5.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.2b"><interval closure="open" id="S5.p1.1.m1.2.3.1.cmml" xref="S5.p1.1.m1.2.3.2"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">𝒂</ci><ci id="S5.p1.1.m1.2.2.cmml" xref="S5.p1.1.m1.2.2">𝒗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.2c">(\bm{a},\bm{v})</annotation></semantics></math>, are used for the downstream training. Results (see row (i) in <a href="#S5.T4" title="In 5 Ablation Study ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>) indicate a performance reduction, highlighting the importance of the autoencoding objective.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Cross-Modal Fusion.</span>
In this ablation, we discard the A2V/V2A networks, which predict the masked tokens of the other modality, and instead use shared learnable masked tokens similar to MAE approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. We can see that the performance of the model diminishes (especially AP) (see row (ii) in <a href="#S5.T4" title="In 5 Ablation Study ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>). This highlights the importance of the cross-modal fusion module, as it supplements the representation of a given modality with information extracted from the other modality, helping building the correspondence between the two modalities.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Complementary Masking.</span>
Replacing complementary masking with random masking in Stage 1, results in a notable drop in AP and AUC scores, affecting the model’s ability to learn correspondences (see row (iii) in <a href="#S5.T4" title="In 5 Ablation Study ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>). We attribute this performance drop to the inability of the model to learn correspondences between audio and visual modalities due to the randomness, which indicates the importance of complementary masking in the proposed method.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.3" class="ltx_p"><span id="S5.p4.3.1" class="ltx_text ltx_font_bold">Concatenation of Different Embeddings.</span>
In the deepfake classification stage, we concatenate the feature embeddings <math id="S5.p4.1.m1.2" class="ltx_Math" alttext="(\bm{a},\bm{v})" display="inline"><semantics id="S5.p4.1.m1.2a"><mrow id="S5.p4.1.m1.2.3.2" xref="S5.p4.1.m1.2.3.1.cmml"><mo stretchy="false" id="S5.p4.1.m1.2.3.2.1" xref="S5.p4.1.m1.2.3.1.cmml">(</mo><mi id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">𝒂</mi><mo id="S5.p4.1.m1.2.3.2.2" xref="S5.p4.1.m1.2.3.1.cmml">,</mo><mi id="S5.p4.1.m1.2.2" xref="S5.p4.1.m1.2.2.cmml">𝒗</mi><mo stretchy="false" id="S5.p4.1.m1.2.3.2.3" xref="S5.p4.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.2b"><interval closure="open" id="S5.p4.1.m1.2.3.1.cmml" xref="S5.p4.1.m1.2.3.2"><ci id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1">𝒂</ci><ci id="S5.p4.1.m1.2.2.cmml" xref="S5.p4.1.m1.2.2">𝒗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.2c">(\bm{a},\bm{v})</annotation></semantics></math>, with the cross-modal embeddings <math id="S5.p4.2.m2.2" class="ltx_Math" alttext="(\bm{a_{v}},\bm{v_{a}})" display="inline"><semantics id="S5.p4.2.m2.2a"><mrow id="S5.p4.2.m2.2.2.2" xref="S5.p4.2.m2.2.2.3.cmml"><mo stretchy="false" id="S5.p4.2.m2.2.2.2.3" xref="S5.p4.2.m2.2.2.3.cmml">(</mo><msub id="S5.p4.2.m2.1.1.1.1" xref="S5.p4.2.m2.1.1.1.1.cmml"><mi id="S5.p4.2.m2.1.1.1.1.2" xref="S5.p4.2.m2.1.1.1.1.2.cmml">𝒂</mi><mi id="S5.p4.2.m2.1.1.1.1.3" xref="S5.p4.2.m2.1.1.1.1.3.cmml">𝒗</mi></msub><mo id="S5.p4.2.m2.2.2.2.4" xref="S5.p4.2.m2.2.2.3.cmml">,</mo><msub id="S5.p4.2.m2.2.2.2.2" xref="S5.p4.2.m2.2.2.2.2.cmml"><mi id="S5.p4.2.m2.2.2.2.2.2" xref="S5.p4.2.m2.2.2.2.2.2.cmml">𝒗</mi><mi id="S5.p4.2.m2.2.2.2.2.3" xref="S5.p4.2.m2.2.2.2.2.3.cmml">𝒂</mi></msub><mo stretchy="false" id="S5.p4.2.m2.2.2.2.5" xref="S5.p4.2.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.2b"><interval closure="open" id="S5.p4.2.m2.2.2.3.cmml" xref="S5.p4.2.m2.2.2.2"><apply id="S5.p4.2.m2.1.1.1.1.cmml" xref="S5.p4.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S5.p4.2.m2.1.1.1.1.1.cmml" xref="S5.p4.2.m2.1.1.1.1">subscript</csymbol><ci id="S5.p4.2.m2.1.1.1.1.2.cmml" xref="S5.p4.2.m2.1.1.1.1.2">𝒂</ci><ci id="S5.p4.2.m2.1.1.1.1.3.cmml" xref="S5.p4.2.m2.1.1.1.1.3">𝒗</ci></apply><apply id="S5.p4.2.m2.2.2.2.2.cmml" xref="S5.p4.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S5.p4.2.m2.2.2.2.2.1.cmml" xref="S5.p4.2.m2.2.2.2.2">subscript</csymbol><ci id="S5.p4.2.m2.2.2.2.2.2.cmml" xref="S5.p4.2.m2.2.2.2.2.2">𝒗</ci><ci id="S5.p4.2.m2.2.2.2.2.3.cmml" xref="S5.p4.2.m2.2.2.2.2.3">𝒂</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.2c">(\bm{a_{v}},\bm{v_{a}})</annotation></semantics></math>, creating the concatenated embeddings <math id="S5.p4.3.m3.2" class="ltx_Math" alttext="(\bm{f_{a}},\bm{f_{v}})" display="inline"><semantics id="S5.p4.3.m3.2a"><mrow id="S5.p4.3.m3.2.2.2" xref="S5.p4.3.m3.2.2.3.cmml"><mo stretchy="false" id="S5.p4.3.m3.2.2.2.3" xref="S5.p4.3.m3.2.2.3.cmml">(</mo><msub id="S5.p4.3.m3.1.1.1.1" xref="S5.p4.3.m3.1.1.1.1.cmml"><mi id="S5.p4.3.m3.1.1.1.1.2" xref="S5.p4.3.m3.1.1.1.1.2.cmml">𝒇</mi><mi id="S5.p4.3.m3.1.1.1.1.3" xref="S5.p4.3.m3.1.1.1.1.3.cmml">𝒂</mi></msub><mo id="S5.p4.3.m3.2.2.2.4" xref="S5.p4.3.m3.2.2.3.cmml">,</mo><msub id="S5.p4.3.m3.2.2.2.2" xref="S5.p4.3.m3.2.2.2.2.cmml"><mi id="S5.p4.3.m3.2.2.2.2.2" xref="S5.p4.3.m3.2.2.2.2.2.cmml">𝒇</mi><mi id="S5.p4.3.m3.2.2.2.2.3" xref="S5.p4.3.m3.2.2.2.2.3.cmml">𝒗</mi></msub><mo stretchy="false" id="S5.p4.3.m3.2.2.2.5" xref="S5.p4.3.m3.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.3.m3.2b"><interval closure="open" id="S5.p4.3.m3.2.2.3.cmml" xref="S5.p4.3.m3.2.2.2"><apply id="S5.p4.3.m3.1.1.1.1.cmml" xref="S5.p4.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S5.p4.3.m3.1.1.1.1.1.cmml" xref="S5.p4.3.m3.1.1.1.1">subscript</csymbol><ci id="S5.p4.3.m3.1.1.1.1.2.cmml" xref="S5.p4.3.m3.1.1.1.1.2">𝒇</ci><ci id="S5.p4.3.m3.1.1.1.1.3.cmml" xref="S5.p4.3.m3.1.1.1.1.3">𝒂</ci></apply><apply id="S5.p4.3.m3.2.2.2.2.cmml" xref="S5.p4.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S5.p4.3.m3.2.2.2.2.1.cmml" xref="S5.p4.3.m3.2.2.2.2">subscript</csymbol><ci id="S5.p4.3.m3.2.2.2.2.2.cmml" xref="S5.p4.3.m3.2.2.2.2.2">𝒇</ci><ci id="S5.p4.3.m3.2.2.2.2.3.cmml" xref="S5.p4.3.m3.2.2.2.2.3">𝒗</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.3.m3.2c">(\bm{f_{a}},\bm{f_{v}})</annotation></semantics></math>. In this experiment, we evaluate the performance using each of the embeddings in isolation (see rows (iv) and (v) in <a href="#S5.T4" title="In 5 Ablation Study ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>). While the use of each embedding generates promising results, the synergy of the two embeddings enhances the performance.</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Uni-Modal Patch Reduction.</span>
Replacing the uni-modal patch reduction networks (<math id="S5.p5.1.m1.2" class="ltx_Math" alttext="\Psi_{a},\Psi_{v}" display="inline"><semantics id="S5.p5.1.m1.2a"><mrow id="S5.p5.1.m1.2.2.2" xref="S5.p5.1.m1.2.2.3.cmml"><msub id="S5.p5.1.m1.1.1.1.1" xref="S5.p5.1.m1.1.1.1.1.cmml"><mi mathvariant="normal" id="S5.p5.1.m1.1.1.1.1.2" xref="S5.p5.1.m1.1.1.1.1.2.cmml">Ψ</mi><mi id="S5.p5.1.m1.1.1.1.1.3" xref="S5.p5.1.m1.1.1.1.1.3.cmml">a</mi></msub><mo id="S5.p5.1.m1.2.2.2.3" xref="S5.p5.1.m1.2.2.3.cmml">,</mo><msub id="S5.p5.1.m1.2.2.2.2" xref="S5.p5.1.m1.2.2.2.2.cmml"><mi mathvariant="normal" id="S5.p5.1.m1.2.2.2.2.2" xref="S5.p5.1.m1.2.2.2.2.2.cmml">Ψ</mi><mi id="S5.p5.1.m1.2.2.2.2.3" xref="S5.p5.1.m1.2.2.2.2.3.cmml">v</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.p5.1.m1.2b"><list id="S5.p5.1.m1.2.2.3.cmml" xref="S5.p5.1.m1.2.2.2"><apply id="S5.p5.1.m1.1.1.1.1.cmml" xref="S5.p5.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S5.p5.1.m1.1.1.1.1.1.cmml" xref="S5.p5.1.m1.1.1.1.1">subscript</csymbol><ci id="S5.p5.1.m1.1.1.1.1.2.cmml" xref="S5.p5.1.m1.1.1.1.1.2">Ψ</ci><ci id="S5.p5.1.m1.1.1.1.1.3.cmml" xref="S5.p5.1.m1.1.1.1.1.3">𝑎</ci></apply><apply id="S5.p5.1.m1.2.2.2.2.cmml" xref="S5.p5.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S5.p5.1.m1.2.2.2.2.1.cmml" xref="S5.p5.1.m1.2.2.2.2">subscript</csymbol><ci id="S5.p5.1.m1.2.2.2.2.2.cmml" xref="S5.p5.1.m1.2.2.2.2.2">Ψ</ci><ci id="S5.p5.1.m1.2.2.2.2.3.cmml" xref="S5.p5.1.m1.2.2.2.2.3">𝑣</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.1.m1.2c">\Psi_{a},\Psi_{v}</annotation></semantics></math>) with Mean Pooling slightly dents the performance (see row (vi) in <a href="#S5.T4" title="In 5 Ablation Study ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>). This suggests that a simple linear averaging is sub-optimal in that some subtle cues may no longer be preserved when compared to using an MLP which effectively performs weighted non-linear averaging.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<p id="S5.T4.2" class="ltx_p ltx_align_center"><span id="S5.T4.2.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S5.T4.2.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:258.2pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S5.T4.2.1.1.1" class="ltx_p"><span id="S5.T4.2.1.1.1.1" class="ltx_text">
<span id="S5.T4.2.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S5.T4.2.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T4.2.1.1.1.1.1.1.1.1" class="ltx_td ltx_border_tt"></span>
<span id="S5.T4.2.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Method</span>
<span id="S5.T4.2.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AP</span>
<span id="S5.T4.2.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AUC</span></span>
<span id="S5.T4.2.1.1.1.1.1.2.2" class="ltx_tr">
<span id="S5.T4.2.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">(i)</span>
<span id="S5.T4.2.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">Only contrastive loss</span>
<span id="S5.T4.2.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">84.2</span>
<span id="S5.T4.2.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">90.3</span></span>
<span id="S5.T4.2.1.1.1.1.1.3.3" class="ltx_tr">
<span id="S5.T4.2.1.1.1.1.1.3.3.1" class="ltx_td ltx_align_left">(ii)</span>
<span id="S5.T4.2.1.1.1.1.1.3.3.2" class="ltx_td ltx_align_left">Ours w/o cross-modality fusion</span>
<span id="S5.T4.2.1.1.1.1.1.3.3.3" class="ltx_td ltx_align_center">87.2</span>
<span id="S5.T4.2.1.1.1.1.1.3.3.4" class="ltx_td ltx_align_center">93.1</span></span>
<span id="S5.T4.2.1.1.1.1.1.4.4" class="ltx_tr">
<span id="S5.T4.2.1.1.1.1.1.4.4.1" class="ltx_td ltx_align_left">(iii)</span>
<span id="S5.T4.2.1.1.1.1.1.4.4.2" class="ltx_td ltx_align_left">Ours w/o complementary masking</span>
<span id="S5.T4.2.1.1.1.1.1.4.4.3" class="ltx_td ltx_align_center">78.9</span>
<span id="S5.T4.2.1.1.1.1.1.4.4.4" class="ltx_td ltx_align_center">90.7</span></span>
<span id="S5.T4.2.1.1.1.1.1.5.5" class="ltx_tr">
<span id="S5.T4.2.1.1.1.1.1.5.5.1" class="ltx_td ltx_align_left">(iv)</span>
<span id="S5.T4.2.1.1.1.1.1.5.5.2" class="ltx_td ltx_align_left">Only feature embeddings</span>
<span id="S5.T4.2.1.1.1.1.1.5.5.3" class="ltx_td ltx_align_center">89.7</span>
<span id="S5.T4.2.1.1.1.1.1.5.5.4" class="ltx_td ltx_align_center">97.6</span></span>
<span id="S5.T4.2.1.1.1.1.1.6.6" class="ltx_tr">
<span id="S5.T4.2.1.1.1.1.1.6.6.1" class="ltx_td ltx_align_left">(v)</span>
<span id="S5.T4.2.1.1.1.1.1.6.6.2" class="ltx_td ltx_align_left">Only cross-modal embeddings</span>
<span id="S5.T4.2.1.1.1.1.1.6.6.3" class="ltx_td ltx_align_center">94.6</span>
<span id="S5.T4.2.1.1.1.1.1.6.6.4" class="ltx_td ltx_align_center">98.0</span></span>
<span id="S5.T4.2.1.1.1.1.1.7.7" class="ltx_tr">
<span id="S5.T4.2.1.1.1.1.1.7.7.1" class="ltx_td ltx_align_left">(vi)</span>
<span id="S5.T4.2.1.1.1.1.1.7.7.2" class="ltx_td ltx_align_left">Mean Pooling patch dimension</span>
<span id="S5.T4.2.1.1.1.1.1.7.7.3" class="ltx_td ltx_align_center"><span id="S5.T4.2.1.1.1.1.1.7.7.3.1" class="ltx_text ltx_framed ltx_framed_underline">96.5</span></span>
<span id="S5.T4.2.1.1.1.1.1.7.7.4" class="ltx_td ltx_align_center"><span id="S5.T4.2.1.1.1.1.1.7.7.4.1" class="ltx_text ltx_framed ltx_framed_underline">98.1</span></span></span>
<span id="S5.T4.2.1.1.1.1.1.8.8" class="ltx_tr">
<span id="S5.T4.2.1.1.1.1.1.8.8.1" class="ltx_td ltx_border_bb ltx_border_t"></span>
<span id="S5.T4.2.1.1.1.1.1.8.8.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S5.T4.2.1.1.1.1.1.8.8.2.1" class="ltx_text ltx_font_bold">AVFF (Ours)</span></span>
<span id="S5.T4.2.1.1.1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T4.2.1.1.1.1.1.8.8.3.1" class="ltx_text ltx_font_bold">96.7</span></span>
<span id="S5.T4.2.1.1.1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T4.2.1.1.1.1.1.8.8.4.1" class="ltx_text ltx_font_bold">99.1</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.4.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Evaluation on Ablations<span id="S5.T4.5.2.1" class="ltx_text ltx_font_medium">. Best result is in bold, and second best is underlined. The proposed AVFF pipeline performs the best among the considered ablations. </span></span></figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion, Limitations, and Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we propose AVFF, a novel two-staged learning framework for audio-visual deepfake detection. AVFF is composed of a self-supervised representation learning stage that captures the audio-visual correspondence using contrastive learning and a novel complementary masking and cross-modal fusion module within the autoencoding objective, followed by a supervised deepfake classification stage. Our method shows significant improvements in both in-distribution performance as well as generalization on unseen manipulations over both visual-only and audio-visual state-of-the-art algorithms.
Our results not only validate the effectiveness of the proposed approach but also emphasize the potential as a defensive tool against the escalating threat posed by deepfake videos.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Limitations.</span>
Our work is limited to cases where there is a coherence between the audio and visual modalities, as we rely on it to distinguish fake from real videos. Hence, asynchronous videos with audio lags or mismatched audio, videos with multiple speakers, and visuals with occlusions (<em id="S6.p2.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S6.p2.1.3" class="ltx_text"></span>, masks, hands) would be challenging. Our model also requires the input to contain both audio and visual modalities, <em id="S6.p2.1.4" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S6.p2.1.5" class="ltx_text"></span>, videos with only one modality are not supported.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">Future Works.</span>
A few possible future research avenues include: (i) supplementing our representation learning method with a strategy to preserve uni-modal cues to mitigate the aforementioned limitations, (ii) adaptation for other audio-visual downstream tasks such as emotion recognition, and (iii) generalization for non-humanoid audio-driven videos especially with diffusion-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>.</p>
</div>
<div id="S6.p4" class="ltx_para ltx_noindent">
<p id="S6.p4.1" class="ltx_p"><span id="S6.p4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Acknowledgements.</span><span id="S6.p4.1.2" class="ltx_text" style="font-size:90%;"> Yaser Yacoob is supported in part by the DARPA SemaFor Program under HR001120C0124.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="S6.p5" class="ltx_para ltx_noindent">
<span id="S6.p5.1" class="ltx_ERROR undefined">\thetitle</span>
<br class="ltx_break ltx_centering">
<p id="S6.p5.2" class="ltx_p ltx_align_center"><span id="S6.p5.2.1" class="ltx_text" style="font-size:144%;">Supplementary Material 
<br class="ltx_break"></span></p>
</div>
</section>
<section id="S1a" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">A </span>Overview</h2>

<div id="S1a.p1" class="ltx_para">
<p id="S1a.p1.1" class="ltx_p"><span id="S1a.p1.1.1" class="ltx_text" style="font-size:144%;">This document is structured as follows:</span></p>
<ul id="S1.I1a" class="ltx_itemize">
<li id="S1.I1.i1a" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1a.p1" class="ltx_para">
<p id="S1.I1.i1a.p1.1" class="ltx_p"><a href="#S2a" title="B Implementation Details ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">B</span></a><span id="S1.I1.i1a.p1.1.1" class="ltx_text" style="font-size:144%;">: Implementation details</span></p>
</div>
</li>
<li id="S1.I1.i2a" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2a.p1" class="ltx_para">
<p id="S1.I1.i2a.p1.1" class="ltx_p"><a href="#S3a" title="C Dataset Details ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">C</span></a><span id="S1.I1.i2a.p1.1.1" class="ltx_text" style="font-size:144%;">: Dataset details</span></p>
</div>
</li>
<li id="S1.I1.i3a" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3a.p1" class="ltx_para">
<p id="S1.I1.i3a.p1.1" class="ltx_p"><a href="#S4a" title="D Additional Results ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">D</span></a><span id="S1.I1.i3a.p1.1.1" class="ltx_text" style="font-size:144%;">: Additional results</span></p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><a href="#S5a" title="E Visual Examples of Challenging Scenarios ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">E</span></a><span id="S1.I1.i4.p1.1.1" class="ltx_text" style="font-size:144%;">: Visual examples of challenging scenarios</span></p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2a" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">B </span>Implementation Details</h2>

<section id="S2.SS1a" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Representation Learning Stage</h3>

<div id="S2.SS1a.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1a.p1.3" class="ltx_p"><span id="S2.SS1a.p1.3.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Inputs.</span><span id="S2.SS1a.p1.3.2" class="ltx_text" style="font-size:144%;">
We draw samples from the LRS3 dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1a.p1.3.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="S2.SS1a.p1.3.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S2.SS1a.p1.3.5" class="ltx_text" style="font-size:144%;">, which exclusively contains real videos. We preprocess all videos as explained in Sec. 3.1 in the main paper.
The audio stream is converted to a Mel-spectrogram of 128 Mel-frequency bins, with a 16 ms Hamming window every 4 ms.
We randomly sample video clips of </span><math id="S2.SS1a.p1.1.m1.1" class="ltx_Math" alttext="T=3.2" display="inline"><semantics id="S2.SS1a.p1.1.m1.1a"><mrow id="S2.SS1a.p1.1.m1.1.1" xref="S2.SS1a.p1.1.m1.1.1.cmml"><mi mathsize="144%" id="S2.SS1a.p1.1.m1.1.1.2" xref="S2.SS1a.p1.1.m1.1.1.2.cmml">T</mi><mo mathsize="144%" id="S2.SS1a.p1.1.m1.1.1.1" xref="S2.SS1a.p1.1.m1.1.1.1.cmml">=</mo><mn mathsize="144%" id="S2.SS1a.p1.1.m1.1.1.3" xref="S2.SS1a.p1.1.m1.1.1.3.cmml">3.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1a.p1.1.m1.1b"><apply id="S2.SS1a.p1.1.m1.1.1.cmml" xref="S2.SS1a.p1.1.m1.1.1"><eq id="S2.SS1a.p1.1.m1.1.1.1.cmml" xref="S2.SS1a.p1.1.m1.1.1.1"></eq><ci id="S2.SS1a.p1.1.m1.1.1.2.cmml" xref="S2.SS1a.p1.1.m1.1.1.2">𝑇</ci><cn type="float" id="S2.SS1a.p1.1.m1.1.1.3.cmml" xref="S2.SS1a.p1.1.m1.1.1.3">3.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1a.p1.1.m1.1c">T=3.2</annotation></semantics></math><span id="S2.SS1a.p1.3.6" class="ltx_text" style="font-size:144%;">s in duration, sampling 16 visual frames and 768 audio frames (Mel) with clipping/padding where necessary. The 16 visual frames are uniformly sampled such that they are at the first and third quartile of a temporal slice (2 frames/slice </span><math id="S2.SS1a.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.SS1a.p1.2.m2.1a"><mo mathsize="144%" id="S2.SS1a.p1.2.m2.1.1" xref="S2.SS1a.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS1a.p1.2.m2.1b"><times id="S2.SS1a.p1.2.m2.1.1.cmml" xref="S2.SS1a.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1a.p1.2.m2.1c">\times</annotation></semantics></math><span id="S2.SS1a.p1.3.7" class="ltx_text" style="font-size:144%;"> 8 slices). The visual frames are resized to </span><math id="S2.SS1a.p1.3.m3.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S2.SS1a.p1.3.m3.1a"><mrow id="S2.SS1a.p1.3.m3.1.1" xref="S2.SS1a.p1.3.m3.1.1.cmml"><mn mathsize="144%" id="S2.SS1a.p1.3.m3.1.1.2" xref="S2.SS1a.p1.3.m3.1.1.2.cmml">224</mn><mo lspace="0.222em" mathsize="144%" rspace="0.222em" id="S2.SS1a.p1.3.m3.1.1.1" xref="S2.SS1a.p1.3.m3.1.1.1.cmml">×</mo><mn mathsize="144%" id="S2.SS1a.p1.3.m3.1.1.3" xref="S2.SS1a.p1.3.m3.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1a.p1.3.m3.1b"><apply id="S2.SS1a.p1.3.m3.1.1.cmml" xref="S2.SS1a.p1.3.m3.1.1"><times id="S2.SS1a.p1.3.m3.1.1.1.cmml" xref="S2.SS1a.p1.3.m3.1.1.1"></times><cn type="integer" id="S2.SS1a.p1.3.m3.1.1.2.cmml" xref="S2.SS1a.p1.3.m3.1.1.2">224</cn><cn type="integer" id="S2.SS1a.p1.3.m3.1.1.3.cmml" xref="S2.SS1a.p1.3.m3.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1a.p1.3.m3.1c">224\times 224</annotation></semantics></math><span id="S2.SS1a.p1.3.8" class="ltx_text" style="font-size:144%;"> spatially and are augmented using random grayscaling and horizontal flipping, each with a probability of 0.5. We make sure that in a given batch, for each sample we draw another sample from the same video but at a different time interval to make sure the model is exposed to the notion of temporal shifts when computing the contrastive loss. Both audio and visual modalities are normalized.</span></p>
</div>
<div id="S2.SS1a.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1a.p2.1" class="ltx_p"><span id="S2.SS1a.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Architecture.</span><span id="S2.SS1a.p2.1.2" class="ltx_text" style="font-size:144%;">
We adopt the encoder and decoder architectures of each modality from the VideoMAE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1a.p2.1.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a><span id="S2.SS1a.p2.1.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S2.SS1a.p2.1.5" class="ltx_text" style="font-size:144%;"> based on ViT-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1a.p2.1.6.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a><span id="S2.SS1a.p2.1.7.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S2.SS1a.p2.1.8" class="ltx_text" style="font-size:144%;">.
Each of the A2V/V2A networks is composed of a linear layer to match the number of tokens of the other modality followed by a single transformer block.</span></p>
</div>
<div id="S2.SS1a.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1a.p3.3" class="ltx_p"><span id="S2.SS1a.p3.3.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Optimization.</span><span id="S2.SS1a.p3.3.2" class="ltx_text" style="font-size:144%;">
We initialize the audio encoder and decoder using the checkpoint of AudioMAE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1a.p3.3.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a><span id="S2.SS1a.p3.3.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S2.SS1a.p3.3.5" class="ltx_text" style="font-size:144%;"> pretrained on AudioSet-2M </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1a.p3.3.6.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a><span id="S2.SS1a.p3.3.7.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S2.SS1a.p3.3.8" class="ltx_text" style="font-size:144%;"> and the visual encoder and decoder using the checkpoint of MARLIN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1a.p3.3.9.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a><span id="S2.SS1a.p3.3.10.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S2.SS1a.p3.3.11" class="ltx_text" style="font-size:144%;"> pretrained on the YouTubeFace </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1a.p3.3.12.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a><span id="S2.SS1a.p3.3.13.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S2.SS1a.p3.3.14" class="ltx_text" style="font-size:144%;"> dataset.
Subsequently, we train the representation learning framework end-to-end, using the AdamW optimizer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1a.p3.3.15.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a><span id="S2.SS1a.p3.3.16.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S2.SS1a.p3.3.17" class="ltx_text" style="font-size:144%;"> with a learning rate of 1.5e-4 with a cosine decay </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1a.p3.3.18.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a><span id="S2.SS1a.p3.3.19.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S2.SS1a.p3.3.20" class="ltx_text" style="font-size:144%;">.
The weights of the losses are as follows: </span><math id="S2.SS1a.p3.1.m1.1" class="ltx_Math" alttext="\lambda_{c}=0.01" display="inline"><semantics id="S2.SS1a.p3.1.m1.1a"><mrow id="S2.SS1a.p3.1.m1.1.1" xref="S2.SS1a.p3.1.m1.1.1.cmml"><msub id="S2.SS1a.p3.1.m1.1.1.2" xref="S2.SS1a.p3.1.m1.1.1.2.cmml"><mi mathsize="144%" id="S2.SS1a.p3.1.m1.1.1.2.2" xref="S2.SS1a.p3.1.m1.1.1.2.2.cmml">λ</mi><mi mathsize="144%" id="S2.SS1a.p3.1.m1.1.1.2.3" xref="S2.SS1a.p3.1.m1.1.1.2.3.cmml">c</mi></msub><mo mathsize="144%" id="S2.SS1a.p3.1.m1.1.1.1" xref="S2.SS1a.p3.1.m1.1.1.1.cmml">=</mo><mn mathsize="144%" id="S2.SS1a.p3.1.m1.1.1.3" xref="S2.SS1a.p3.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1a.p3.1.m1.1b"><apply id="S2.SS1a.p3.1.m1.1.1.cmml" xref="S2.SS1a.p3.1.m1.1.1"><eq id="S2.SS1a.p3.1.m1.1.1.1.cmml" xref="S2.SS1a.p3.1.m1.1.1.1"></eq><apply id="S2.SS1a.p3.1.m1.1.1.2.cmml" xref="S2.SS1a.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1a.p3.1.m1.1.1.2.1.cmml" xref="S2.SS1a.p3.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS1a.p3.1.m1.1.1.2.2.cmml" xref="S2.SS1a.p3.1.m1.1.1.2.2">𝜆</ci><ci id="S2.SS1a.p3.1.m1.1.1.2.3.cmml" xref="S2.SS1a.p3.1.m1.1.1.2.3">𝑐</ci></apply><cn type="float" id="S2.SS1a.p3.1.m1.1.1.3.cmml" xref="S2.SS1a.p3.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1a.p3.1.m1.1c">\lambda_{c}=0.01</annotation></semantics></math><span id="S2.SS1a.p3.3.21" class="ltx_text" style="font-size:144%;">, </span><math id="S2.SS1a.p3.2.m2.1" class="ltx_Math" alttext="\lambda_{rec}=1.0" display="inline"><semantics id="S2.SS1a.p3.2.m2.1a"><mrow id="S2.SS1a.p3.2.m2.1.1" xref="S2.SS1a.p3.2.m2.1.1.cmml"><msub id="S2.SS1a.p3.2.m2.1.1.2" xref="S2.SS1a.p3.2.m2.1.1.2.cmml"><mi mathsize="144%" id="S2.SS1a.p3.2.m2.1.1.2.2" xref="S2.SS1a.p3.2.m2.1.1.2.2.cmml">λ</mi><mrow id="S2.SS1a.p3.2.m2.1.1.2.3" xref="S2.SS1a.p3.2.m2.1.1.2.3.cmml"><mi mathsize="144%" id="S2.SS1a.p3.2.m2.1.1.2.3.2" xref="S2.SS1a.p3.2.m2.1.1.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS1a.p3.2.m2.1.1.2.3.1" xref="S2.SS1a.p3.2.m2.1.1.2.3.1.cmml">​</mo><mi mathsize="144%" id="S2.SS1a.p3.2.m2.1.1.2.3.3" xref="S2.SS1a.p3.2.m2.1.1.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1a.p3.2.m2.1.1.2.3.1a" xref="S2.SS1a.p3.2.m2.1.1.2.3.1.cmml">​</mo><mi mathsize="144%" id="S2.SS1a.p3.2.m2.1.1.2.3.4" xref="S2.SS1a.p3.2.m2.1.1.2.3.4.cmml">c</mi></mrow></msub><mo mathsize="144%" id="S2.SS1a.p3.2.m2.1.1.1" xref="S2.SS1a.p3.2.m2.1.1.1.cmml">=</mo><mn mathsize="144%" id="S2.SS1a.p3.2.m2.1.1.3" xref="S2.SS1a.p3.2.m2.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1a.p3.2.m2.1b"><apply id="S2.SS1a.p3.2.m2.1.1.cmml" xref="S2.SS1a.p3.2.m2.1.1"><eq id="S2.SS1a.p3.2.m2.1.1.1.cmml" xref="S2.SS1a.p3.2.m2.1.1.1"></eq><apply id="S2.SS1a.p3.2.m2.1.1.2.cmml" xref="S2.SS1a.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS1a.p3.2.m2.1.1.2.1.cmml" xref="S2.SS1a.p3.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS1a.p3.2.m2.1.1.2.2.cmml" xref="S2.SS1a.p3.2.m2.1.1.2.2">𝜆</ci><apply id="S2.SS1a.p3.2.m2.1.1.2.3.cmml" xref="S2.SS1a.p3.2.m2.1.1.2.3"><times id="S2.SS1a.p3.2.m2.1.1.2.3.1.cmml" xref="S2.SS1a.p3.2.m2.1.1.2.3.1"></times><ci id="S2.SS1a.p3.2.m2.1.1.2.3.2.cmml" xref="S2.SS1a.p3.2.m2.1.1.2.3.2">𝑟</ci><ci id="S2.SS1a.p3.2.m2.1.1.2.3.3.cmml" xref="S2.SS1a.p3.2.m2.1.1.2.3.3">𝑒</ci><ci id="S2.SS1a.p3.2.m2.1.1.2.3.4.cmml" xref="S2.SS1a.p3.2.m2.1.1.2.3.4">𝑐</ci></apply></apply><cn type="float" id="S2.SS1a.p3.2.m2.1.1.3.cmml" xref="S2.SS1a.p3.2.m2.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1a.p3.2.m2.1c">\lambda_{rec}=1.0</annotation></semantics></math><span id="S2.SS1a.p3.3.22" class="ltx_text" style="font-size:144%;">, and </span><math id="S2.SS1a.p3.3.m3.1" class="ltx_Math" alttext="\lambda_{adv}=0.1" display="inline"><semantics id="S2.SS1a.p3.3.m3.1a"><mrow id="S2.SS1a.p3.3.m3.1.1" xref="S2.SS1a.p3.3.m3.1.1.cmml"><msub id="S2.SS1a.p3.3.m3.1.1.2" xref="S2.SS1a.p3.3.m3.1.1.2.cmml"><mi mathsize="144%" id="S2.SS1a.p3.3.m3.1.1.2.2" xref="S2.SS1a.p3.3.m3.1.1.2.2.cmml">λ</mi><mrow id="S2.SS1a.p3.3.m3.1.1.2.3" xref="S2.SS1a.p3.3.m3.1.1.2.3.cmml"><mi mathsize="144%" id="S2.SS1a.p3.3.m3.1.1.2.3.2" xref="S2.SS1a.p3.3.m3.1.1.2.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS1a.p3.3.m3.1.1.2.3.1" xref="S2.SS1a.p3.3.m3.1.1.2.3.1.cmml">​</mo><mi mathsize="144%" id="S2.SS1a.p3.3.m3.1.1.2.3.3" xref="S2.SS1a.p3.3.m3.1.1.2.3.3.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.SS1a.p3.3.m3.1.1.2.3.1a" xref="S2.SS1a.p3.3.m3.1.1.2.3.1.cmml">​</mo><mi mathsize="144%" id="S2.SS1a.p3.3.m3.1.1.2.3.4" xref="S2.SS1a.p3.3.m3.1.1.2.3.4.cmml">v</mi></mrow></msub><mo mathsize="144%" id="S2.SS1a.p3.3.m3.1.1.1" xref="S2.SS1a.p3.3.m3.1.1.1.cmml">=</mo><mn mathsize="144%" id="S2.SS1a.p3.3.m3.1.1.3" xref="S2.SS1a.p3.3.m3.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1a.p3.3.m3.1b"><apply id="S2.SS1a.p3.3.m3.1.1.cmml" xref="S2.SS1a.p3.3.m3.1.1"><eq id="S2.SS1a.p3.3.m3.1.1.1.cmml" xref="S2.SS1a.p3.3.m3.1.1.1"></eq><apply id="S2.SS1a.p3.3.m3.1.1.2.cmml" xref="S2.SS1a.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1a.p3.3.m3.1.1.2.1.cmml" xref="S2.SS1a.p3.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS1a.p3.3.m3.1.1.2.2.cmml" xref="S2.SS1a.p3.3.m3.1.1.2.2">𝜆</ci><apply id="S2.SS1a.p3.3.m3.1.1.2.3.cmml" xref="S2.SS1a.p3.3.m3.1.1.2.3"><times id="S2.SS1a.p3.3.m3.1.1.2.3.1.cmml" xref="S2.SS1a.p3.3.m3.1.1.2.3.1"></times><ci id="S2.SS1a.p3.3.m3.1.1.2.3.2.cmml" xref="S2.SS1a.p3.3.m3.1.1.2.3.2">𝑎</ci><ci id="S2.SS1a.p3.3.m3.1.1.2.3.3.cmml" xref="S2.SS1a.p3.3.m3.1.1.2.3.3">𝑑</ci><ci id="S2.SS1a.p3.3.m3.1.1.2.3.4.cmml" xref="S2.SS1a.p3.3.m3.1.1.2.3.4">𝑣</ci></apply></apply><cn type="float" id="S2.SS1a.p3.3.m3.1.1.3.cmml" xref="S2.SS1a.p3.3.m3.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1a.p3.3.m3.1c">\lambda_{adv}=0.1</annotation></semantics></math><span id="S2.SS1a.p3.3.23" class="ltx_text" style="font-size:144%;">, which were chosen empirically and based on previous research </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1a.p3.3.24.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a><span id="S2.SS1a.p3.3.25.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S2.SS1a.p3.3.26" class="ltx_text" style="font-size:144%;">.
We train for 500 epochs with a linear warmup for 40 epochs using a batch size of 32 and a gradient accumulation interval of 2. The training was performed on 4 RTX A6000 GPUs for approximately 60 hours.</span></p>
</div>
</section>
<section id="S2.SS2a" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Deepfake Classification Stage</h3>

<div id="S2.SS2a.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2a.p1.1" class="ltx_p"><span id="S2.SS2a.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Inputs.</span><span id="S2.SS2a.p1.1.2" class="ltx_text" style="font-size:144%;">
We draw samples from FakeAVCeleb </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS2a.p1.1.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a><span id="S2.SS2a.p1.1.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S2.SS2a.p1.1.5" class="ltx_text" style="font-size:144%;">, which consists of deepfake videos where either or both audio and visual modalities have been manipulated.
The preprocessing and sampling strategy is similar to that of Stage 1, except we do not draw an additional sample from the same video clip as we do not use a contrastive learning objective at this stage. We employ weighted sampling to mitigate the issue of class imbalance between real and fake samples.</span></p>
</div>
<div id="S2.SS2a.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2a.p2.1" class="ltx_p"><span id="S2.SS2a.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Architecture.</span><span id="S2.SS2a.p2.1.2" class="ltx_text" style="font-size:144%;">
Each of the uni-modal patch reduction networks is a 3-layer MLP, while the classifier head is a 4-layer MLP. We do not make any changes to the representation learning architecture.</span></p>
</div>
<div id="S2.SS2a.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2a.p3.1" class="ltx_p"><span id="S2.SS2a.p3.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Optimization.</span><span id="S2.SS2a.p3.1.2" class="ltx_text" style="font-size:144%;">
We initialize the representation learning framework using the pretrained checkpoint obtained from Stage 1. Subsequently, we train the pipeline end-to-end, using the AdamW optimizer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS2a.p3.1.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a><span id="S2.SS2a.p3.1.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S2.SS2a.p3.1.5" class="ltx_text" style="font-size:144%;"> with a cosine annealing with warm restarts scheduler </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS2a.p3.1.6.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a><span id="S2.SS2a.p3.1.7.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S2.SS2a.p3.1.8" class="ltx_text" style="font-size:144%;"> with a maximum learning rate of 1.0e-4 for 50 epochs with a batch size of 32. The training was performed on 4 RTX A6000 GPUs for approximately 10 hours.</span></p>
</div>
<figure id="S2.T5" class="ltx_table">
<p id="S2.T5.2" class="ltx_p ltx_align_center"><span id="S2.T5.2.1" class="ltx_text ltx_inline-block" style="font-size:144%;width:433.6pt;">
<span id="S2.T5.2.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:578.7pt;height:127pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S2.T5.2.1.1.1" class="ltx_p"><span id="S2.T5.2.1.1.1.1" class="ltx_text">
<span id="S2.T5.2.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S2.T5.2.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S2.T5.2.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S2.T5.2.1.1.1.1.1.1.1.1.1" class="ltx_text">Method</span></span>
<span id="S2.T5.2.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S2.T5.2.1.1.1.1.1.1.1.2.1" class="ltx_text">Modality</span></span>
<span id="S2.T5.2.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2">DF-TIMIT</span>
<span id="S2.T5.2.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2">DFDC</span></span>
<span id="S2.T5.2.1.1.1.1.1.2.2" class="ltx_tr">
<span id="S2.T5.2.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP</span>
<span id="S2.T5.2.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AUC</span>
<span id="S2.T5.2.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP</span>
<span id="S2.T5.2.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AUC</span></span>
</span>
<span class="ltx_tbody">
<span id="S2.T5.2.1.1.1.1.1.3.1" class="ltx_tr">
<span id="S2.T5.2.1.1.1.1.1.3.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">Xception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite></span>
<span id="S2.T5.2.1.1.1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">V</span>
<span id="S2.T5.2.1.1.1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">86.0</span>
<span id="S2.T5.2.1.1.1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">90.5</span>
<span id="S2.T5.2.1.1.1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">68.0</span>
<span id="S2.T5.2.1.1.1.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">67.6</span></span>
<span id="S2.T5.2.1.1.1.1.1.4.2" class="ltx_tr">
<span id="S2.T5.2.1.1.1.1.1.4.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">LipForensics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite></span>
<span id="S2.T5.2.1.1.1.1.1.4.2.2" class="ltx_td ltx_align_center">V</span>
<span id="S2.T5.2.1.1.1.1.1.4.2.3" class="ltx_td ltx_align_center">96.7</span>
<span id="S2.T5.2.1.1.1.1.1.4.2.4" class="ltx_td ltx_align_center">98.4</span>
<span id="S2.T5.2.1.1.1.1.1.4.2.5" class="ltx_td ltx_align_center">76.8</span>
<span id="S2.T5.2.1.1.1.1.1.4.2.6" class="ltx_td ltx_align_center">77.4</span></span>
<span id="S2.T5.2.1.1.1.1.1.5.3" class="ltx_tr">
<span id="S2.T5.2.1.1.1.1.1.5.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">FTCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite></span>
<span id="S2.T5.2.1.1.1.1.1.5.3.2" class="ltx_td ltx_align_center">V</span>
<span id="S2.T5.2.1.1.1.1.1.5.3.3" class="ltx_td ltx_align_center"><span id="S2.T5.2.1.1.1.1.1.5.3.3.1" class="ltx_text ltx_font_bold">100.</span></span>
<span id="S2.T5.2.1.1.1.1.1.5.3.4" class="ltx_td ltx_align_center">99.8</span>
<span id="S2.T5.2.1.1.1.1.1.5.3.5" class="ltx_td ltx_align_center">70.5</span>
<span id="S2.T5.2.1.1.1.1.1.5.3.6" class="ltx_td ltx_align_center">71.1</span></span>
<span id="S2.T5.2.1.1.1.1.1.6.4" class="ltx_tr">
<span id="S2.T5.2.1.1.1.1.1.6.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">RealForensics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite></span>
<span id="S2.T5.2.1.1.1.1.1.6.4.2" class="ltx_td ltx_align_center">V</span>
<span id="S2.T5.2.1.1.1.1.1.6.4.3" class="ltx_td ltx_align_center">99.2</span>
<span id="S2.T5.2.1.1.1.1.1.6.4.4" class="ltx_td ltx_align_center">99.5</span>
<span id="S2.T5.2.1.1.1.1.1.6.4.5" class="ltx_td ltx_align_center"><span id="S2.T5.2.1.1.1.1.1.6.4.5.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">82.9</span></span>
<span id="S2.T5.2.1.1.1.1.1.6.4.6" class="ltx_td ltx_align_center"><span id="S2.T5.2.1.1.1.1.1.6.4.6.1" class="ltx_text ltx_ulem_uline" style="color:#000000;">83.7</span></span></span>
<span id="S2.T5.2.1.1.1.1.1.7.5" class="ltx_tr">
<span id="S2.T5.2.1.1.1.1.1.7.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S2.T5.2.1.1.1.1.1.7.5.1.1" class="ltx_text ltx_font_bold">AVFF (Ours)</span></span>
<span id="S2.T5.2.1.1.1.1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">AV</span>
<span id="S2.T5.2.1.1.1.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S2.T5.2.1.1.1.1.1.7.5.3.1" class="ltx_text ltx_font_bold">100.</span></span>
<span id="S2.T5.2.1.1.1.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S2.T5.2.1.1.1.1.1.7.5.4.1" class="ltx_text ltx_font_bold">100.</span></span>
<span id="S2.T5.2.1.1.1.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S2.T5.2.1.1.1.1.1.7.5.5.1" class="ltx_text ltx_font_bold">87.0</span></span>
<span id="S2.T5.2.1.1.1.1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S2.T5.2.1.1.1.1.1.7.5.6.1" class="ltx_text ltx_font_bold">86.2</span></span></span>
</span>
</span></span></span>
</span></span></span><span id="S2.T5.2.2" class="ltx_text" style="font-size:144%;"></span></p>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="S2.T5.6.1.1" class="ltx_text" style="font-size:63%;">Table 5</span>: </span><span id="S2.T5.7.2" class="ltx_text ltx_font_bold" style="font-size:63%;">Cross-Dataset Generalization.<span id="S2.T5.7.2.1" class="ltx_text ltx_font_medium"> We evaluate our model’s performance against baselines by testing the model trained on the FakeAVCeleb dataset, on the DF-TIMIT dataset and a subset of the DFDC dataset. Best result is in bold, and second best is underlined.
</span></span></figcaption>
</figure>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2406.02951/assets/Figures/perturbations_plot.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="330" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.8.1.1" class="ltx_text" style="font-size:63%;">Figure 5</span>: </span><span id="S2.F5.9.2" class="ltx_text ltx_font_bold" style="font-size:63%;">Robustness to Unseen Visual Perturbations<span id="S2.F5.9.2.1" class="ltx_text ltx_font_medium">. We illustrate AUC scores (%) as a function of different levels of intensities for various visual perturbations evaluated on the test set of FakeAVCeleb. Our model is more robust than RealForensics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, which is the current state-of-the-art in robustness to unseen visual perturbations. </span></span></figcaption>
</figure>
</section>
</section>
<section id="S3a" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">C </span>Dataset Details</h2>

<div id="S3a.p1" class="ltx_para ltx_noindent">
<p id="S3a.p1.1" class="ltx_p"><span id="S3a.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">LRS3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>. </span><span id="S3a.p1.1.2" class="ltx_text" style="font-size:144%;"> This dataset introduced by Afouras </span><em id="S3a.p1.1.3" class="ltx_emph ltx_font_italic" style="font-size:144%;">et al</em><span id="S3a.p1.1.4" class="ltx_text" style="font-size:144%;">.</span><span id="S3a.p1.1.5" class="ltx_text"></span><span id="S3a.p1.1.6" class="ltx_text" style="font-size:144%;"> exclusively comprises of real videos. It consists of 5594 videos spanning over 400 hours of TED and TED-X talks in English. The videos in the dataset are processed such that each frame contains faces and the audio and visual streams are in sync.</span></p>
</div>
<div id="S3a.p2" class="ltx_para ltx_noindent">
<p id="S3a.p2.1" class="ltx_p"><span id="S3a.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">FakeAVCeleb <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>.</span><span id="S3a.p2.1.2" class="ltx_text" style="font-size:144%;">
The FakeAVCeleb dataset is a deepfake detection dataset, which consists of 20,000 video clips in total. It comprises of 500 real videos sampled from the VoxCeleb2 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p2.1.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a><span id="S3a.p2.1.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p2.1.5" class="ltx_text" style="font-size:144%;"> and 19500 deepfake samples generated using different manipulation methods applied on the set of real videos. The dataset consists of the following manipulations where the deepfake algorithms used in each category are indicated within brackets.</span></p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text" style="font-size:144%;">RVFA: Real Visuals - Fake Audio (SV2TTS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.I1.i1.p1.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a><span id="S3.I1.i1.p1.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3.I1.i1.p1.1.4" class="ltx_text" style="font-size:144%;">)</span></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text" style="font-size:144%;">FVRA-FS: Fake Visuals - Real Audio (FaceSwap </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.I1.i2.p1.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a><span id="S3.I1.i2.p1.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3.I1.i2.p1.1.4" class="ltx_text" style="font-size:144%;">)</span></p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text" style="font-size:144%;">FVFA-FS: Fake Visuals - Fake Audio (SV2TTS + FaceSwap)</span></p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text" style="font-size:144%;">FVFA-GAN: Fake Visuals - Fake Audio (SV2TTS + FaceSwapGAN</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.I1.i4.p1.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a><span id="S3.I1.i4.p1.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3.I1.i4.p1.1.4" class="ltx_text" style="font-size:144%;">)</span></p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text" style="font-size:144%;">FVRA-GAN: Fake Visuals - Real Audio (FaceSwapGAN)</span></p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p"><span id="S3.I1.i6.p1.1.1" class="ltx_text" style="font-size:144%;">FVRA-WL: Fake Visuals - Real Audio (Wav2Lip </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.I1.i6.p1.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a><span id="S3.I1.i6.p1.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3.I1.i6.p1.1.4" class="ltx_text" style="font-size:144%;">)</span></p>
</div>
</li>
<li id="S3.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i7.p1" class="ltx_para">
<p id="S3.I1.i7.p1.1" class="ltx_p"><span id="S3.I1.i7.p1.1.1" class="ltx_text" style="font-size:144%;">FVFA-WL: Fake Visuals - Fake Audio (SV2TTS + Wav2Lip)</span></p>
</div>
</li>
</ul>
</div>
<div id="S3a.p3" class="ltx_para ltx_noindent">
<p id="S3a.p3.1" class="ltx_p"><span id="S3a.p3.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">KoDF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>.</span><span id="S3a.p3.1.2" class="ltx_text" style="font-size:144%;">
This dataset is a large-scale dataset comprising real and synthetic videos of 400+ subjects speaking Korean. KoDF consists of 62K+ real videos and 175K+ fake videos synthesized using the following six algorithms: FaceSwap </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p3.1.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a><span id="S3a.p3.1.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p3.1.5" class="ltx_text" style="font-size:144%;">, DeepFaceLab </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p3.1.6.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a><span id="S3a.p3.1.7.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p3.1.8" class="ltx_text" style="font-size:144%;">, FaceSwapGAN</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p3.1.9.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a><span id="S3a.p3.1.10.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p3.1.11" class="ltx_text" style="font-size:144%;">, FOMM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p3.1.12.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a><span id="S3a.p3.1.13.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p3.1.14" class="ltx_text" style="font-size:144%;">, ATFHP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p3.1.15.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a><span id="S3a.p3.1.16.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p3.1.17" class="ltx_text" style="font-size:144%;">, and Wav2Lip </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p3.1.18.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a><span id="S3a.p3.1.19.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p3.1.20" class="ltx_text" style="font-size:144%;">. We use a subset of this dataset following </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p3.1.21.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a><span id="S3a.p3.1.22.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p3.1.23" class="ltx_text" style="font-size:144%;"> to evaluate the cross-dataset generalization performance of our model (Tab. 3 in the main paper).</span></p>
</div>
<div id="S3a.p4" class="ltx_para ltx_noindent">
<p id="S3a.p4.1" class="ltx_p"><span id="S3a.p4.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">DF-TIMIT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>.</span><span id="S3a.p4.1.2" class="ltx_text" style="font-size:144%;">
The Deepfake TIMIT dataset comprises deepfake videos manipulated using FaceSwapGAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p4.1.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a><span id="S3a.p4.1.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p4.1.5" class="ltx_text" style="font-size:144%;">. The real videos used for manipulation have been sourced by sampling similar-looking identities from the VidTIMIT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p4.1.6.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a><span id="S3a.p4.1.7.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p4.1.8" class="ltx_text" style="font-size:144%;"> dataset. We use their higher-quality (HQ) version, which consists of 320 videos, in evaluating cross-dataset generalization performance.</span></p>
</div>
<div id="S3a.p5" class="ltx_para ltx_noindent">
<p id="S3a.p5.1" class="ltx_p"><span id="S3a.p5.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">DFDC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>. </span><span id="S3a.p5.1.2" class="ltx_text" style="font-size:144%;">
The DeepFake Detection Challenge (DFDC) dataset is another deepfake dataset that consists of samples with fake audio besides FakeAVCeleb. It consists of over 100K video clips in total generated using deepfake algorithms such as MM/NN Face Swap </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p5.1.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a><span id="S3a.p5.1.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p5.1.5" class="ltx_text" style="font-size:144%;">, NTH </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p5.1.6.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">68</span></a><span id="S3a.p5.1.7.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p5.1.8" class="ltx_text" style="font-size:144%;">, FaceSwapGAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p5.1.9.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a><span id="S3a.p5.1.10.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p5.1.11" class="ltx_text" style="font-size:144%;">, StyleGAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p5.1.12.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a><span id="S3a.p5.1.13.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p5.1.14" class="ltx_text" style="font-size:144%;">, and TTS Skins </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p5.1.15.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a><span id="S3a.p5.1.16.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p5.1.17" class="ltx_text" style="font-size:144%;">. We use a subset of this dataset consisting of 3215 videos, as used in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3a.p5.1.18.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a><span id="S3a.p5.1.19.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S3a.p5.1.20" class="ltx_text" style="font-size:144%;"> to evaluate the model’s cross-dataset generalization performance.</span></p>
</div>
</section>
<section id="S4a" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">D </span>Additional Results</h2>

<div id="S4a.p1" class="ltx_para">
<p id="S4a.p1.1" class="ltx_p"><span id="S4a.p1.1.1" class="ltx_text" style="font-size:144%;">In extending our analysis beyond the results outlined in Sec. 4 of the main paper, we conducted additional experiments to provide a more comprehensive evaluation of our model’s performance. This supplementary investigation aims to enhance our understanding and confidence in the efficacy of the proposed approach.</span></p>
</div>
<section id="S4.SS1a" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Cross-Dataset Generalization</h3>

<div id="S4.SS1a.p1" class="ltx_para">
<p id="S4.SS1a.p1.1" class="ltx_p"><span id="S4.SS1a.p1.1.1" class="ltx_text" style="font-size:144%;">In addition to the cross-dataset generalization evaluation reported on the KoDF dataset (Tab. 3 of the main paper), we further evaluate cross-dataset generalization on the DF-TIMIT dataset and a subset of the DFDC dataset following </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1a.p1.1.2.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a><span id="S4.SS1a.p1.1.3.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S4.SS1a.p1.1.4" class="ltx_text" style="font-size:144%;">.
We are limited to comparing against baselines with open-source codes. We were unable to obtain the models nor results for the other baselines from the authors.
As illustrated in </span><a href="#S2.T5" title="In B.2 Deepfake Classification Stage ‣ B Implementation Details ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a><span id="S4.SS1a.p1.1.5" class="ltx_text" style="font-size:144%;">, we achieve the best cross-dataset generalization performance, when evaluated on both DF-TIMIT and DFDC.</span></p>
</div>
</section>
<section id="S4.SS2a" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Robustness to Unseen Perturbations</h3>

<div id="S4.SS2a.p1" class="ltx_para">
<p id="S4.SS2a.p1.1" class="ltx_p"><span id="S4.SS2a.p1.1.1" class="ltx_text" style="font-size:144%;">In real-world scenarios, videos undergo post-processing (</span><em id="S4.SS2a.p1.1.2" class="ltx_emph ltx_font_italic" style="font-size:144%;">e.g</em><span id="S4.SS2a.p1.1.3" class="ltx_text" style="font-size:144%;">.</span><span id="S4.SS2a.p1.1.4" class="ltx_text"></span><span id="S4.SS2a.p1.1.5" class="ltx_text" style="font-size:144%;"> when sharing through social media platforms), which perturbs both audio and visual modalities. Hence, it is crucial for a model to be robust to unseen perturbations. To this end, we evaluate the performance of our model (trained without augmentations) on several unseen perturbations applied to each modality.</span></p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2406.02951/assets/Figures/perturbations_audio_plot.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="490" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.5.1.1" class="ltx_text" style="font-size:63%;">Figure 6</span>: </span><span id="S4.F6.6.2" class="ltx_text ltx_font_bold" style="font-size:63%;">Robustness to Unseen Audio Perturbations<span id="S4.F6.6.2.1" class="ltx_text ltx_font_medium">. We illustrate the variation of AUC and AP scores (%) as a function of different levels of intensities for various audio perturbations evaluated on the test set of FakeAVCeleb. Overall our model depicts impressive robustness to audio perturbations.</span></span></figcaption>
</figure>
<div id="S4.SS2a.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2a.p2.1" class="ltx_p"><span id="S4.SS2a.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Visual Perturbations.</span><span id="S4.SS2a.p2.1.2" class="ltx_text" style="font-size:144%;">
Following </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2a.p2.1.3.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a><span id="S4.SS2a.p2.1.4.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S4.SS2a.p2.1.5" class="ltx_text" style="font-size:144%;">, we evaluate the performance on the following perturbations: saturation, contrast, block-wise distortion, Gaussian noise, Gaussian blur, JPEG compression, and video compression on five different levels of intensities. The implementations for the perturbations and the levels of intensities were sourced from the official repository of DeeperForensics-1.0 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2a.p2.1.6.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="S4.SS2a.p2.1.7.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S4.SS2a.p2.1.8" class="ltx_text" style="font-size:144%;">.
We compare our model’s performance against RealForensics </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2a.p2.1.9.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a><span id="S4.SS2a.p2.1.10.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S4.SS2a.p2.1.11" class="ltx_text" style="font-size:144%;">, which has the current state-of-the-art performance in robustness to unseen visual perturbations </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2a.p2.1.12.1" class="ltx_text" style="font-size:144%;">[</span><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a><span id="S4.SS2a.p2.1.13.2" class="ltx_text" style="font-size:144%;">]</span></cite><span id="S4.SS2a.p2.1.14" class="ltx_text" style="font-size:144%;">.
As depicted in </span><a href="#S2.F5" title="In B.2 Deepfake Classification Stage ‣ B Implementation Details ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a><span id="S4.SS2a.p2.1.15" class="ltx_text" style="font-size:144%;">, our model demonstrates enhanced robustness against unseen visual perturbations compared to RealForensics in most scenarios. Particularly, noteworthy improvements are observed in cases of block-wise distortion, Gaussian noise, and video compression.</span></p>
</div>
<div id="S4.SS2a.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2a.p3.1" class="ltx_p"><span id="S4.SS2a.p3.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Audio Perturbations.</span><span id="S4.SS2a.p3.1.2" class="ltx_text" style="font-size:144%;">
In this experiment, we subject the audio stream to a range of perturbations: Gaussian Noise, pitch shift, changes in reverberance, and audio compression. The performance of our model under these perturbations is illustrated across five intensity levels in </span><a href="#S4.F6" title="In D.2 Robustness to Unseen Perturbations ‣ D Additional Results ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a><span id="S4.SS2a.p3.1.3" class="ltx_text" style="font-size:144%;">.
To generate the perturbed audio samples, we employ the following Python libraries: </span><span id="S4.SS2a.p3.1.4" class="ltx_text ltx_font_italic" style="font-size:144%;">torchaudio</span><span id="S4.SS2a.p3.1.5" class="ltx_text" style="font-size:144%;"> (Gaussian noise, pitch shift), </span><span id="S4.SS2a.p3.1.6" class="ltx_text ltx_font_italic" style="font-size:144%;">pysndfx</span><span id="S4.SS2a.p3.1.7" class="ltx_text" style="font-size:144%;"> (reverberance), and </span><span id="S4.SS2a.p3.1.8" class="ltx_text ltx_font_italic" style="font-size:144%;">pydub</span><span id="S4.SS2a.p3.1.9" class="ltx_text" style="font-size:144%;"> (audio compression).
The parameters used to generate samples at each intensity level are tabulated in </span><a href="#S4.T6" title="In D.2 Robustness to Unseen Perturbations ‣ D Additional Results ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a><span id="S4.SS2a.p3.1.10" class="ltx_text" style="font-size:144%;">.
As seen in </span><a href="#S4.F6" title="In D.2 Robustness to Unseen Perturbations ‣ D Additional Results ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a><span id="S4.SS2a.p3.1.11" class="ltx_text" style="font-size:144%;">, overall our model is robust to various audio perturbations.
A slight decrease in performance is seen in cases of Gaussian noise and pitch shift, with the increase in intensity.
Notably, the model showcases high robustness to changes in reverberance, with minimal fluctuations across all intensity levels.
However, a noticeable reduction in average precision is observed for high-intensity levels of audio compression, potentially due to artifacts introduced by the reduced bitrate in extreme compression scenarios.</span></p>
</div>
<figure id="S4.T6" class="ltx_table">
<p id="S4.T6.5" class="ltx_p ltx_align_center"><span id="S4.T6.5.5" class="ltx_text ltx_inline-block" style="font-size:144%;width:433.6pt;">
<span id="S4.T6.5.5.5.5" class="ltx_inline-block ltx_transformed_outer" style="width:400.6pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T6.5.5.5.5.5" class="ltx_p"><span id="S4.T6.5.5.5.5.5.5" class="ltx_text">
<span id="S4.T6.5.5.5.5.5.5.5" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T6.5.5.5.5.5.5.5.6.1" class="ltx_tr">
<span id="S4.T6.5.5.5.5.5.5.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S4.T6.5.5.5.5.5.5.5.6.1.1.1" class="ltx_text">Perturbation</span></span>
<span id="S4.T6.5.5.5.5.5.5.5.6.1.2" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_5">Intensity Level</span></span>
<span id="S4.T6.5.5.5.5.5.5.5.7.2" class="ltx_tr">
<span id="S4.T6.5.5.5.5.5.5.5.7.2.1" class="ltx_td ltx_align_right ltx_border_t">1</span>
<span id="S4.T6.5.5.5.5.5.5.5.7.2.2" class="ltx_td ltx_align_right ltx_border_t">2</span>
<span id="S4.T6.5.5.5.5.5.5.5.7.2.3" class="ltx_td ltx_align_right ltx_border_t">3</span>
<span id="S4.T6.5.5.5.5.5.5.5.7.2.4" class="ltx_td ltx_align_right ltx_border_t">4</span>
<span id="S4.T6.5.5.5.5.5.5.5.7.2.5" class="ltx_td ltx_align_right ltx_border_t">5</span></span>
<span id="S4.T6.5.5.5.5.5.5.5.8.3" class="ltx_tr">
<span id="S4.T6.5.5.5.5.5.5.5.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Gaussian Noise (SNR)</span>
<span id="S4.T6.5.5.5.5.5.5.5.8.3.2" class="ltx_td ltx_align_right ltx_border_t">40</span>
<span id="S4.T6.5.5.5.5.5.5.5.8.3.3" class="ltx_td ltx_align_right ltx_border_t">30</span>
<span id="S4.T6.5.5.5.5.5.5.5.8.3.4" class="ltx_td ltx_align_right ltx_border_t">20</span>
<span id="S4.T6.5.5.5.5.5.5.5.8.3.5" class="ltx_td ltx_align_right ltx_border_t">15</span>
<span id="S4.T6.5.5.5.5.5.5.5.8.3.6" class="ltx_td ltx_align_right ltx_border_t">10</span></span>
<span id="S4.T6.5.5.5.5.5.5.5.5" class="ltx_tr">
<span id="S4.T6.5.5.5.5.5.5.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_row">Pitch Shift (steps)</span>
<span id="S4.T6.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_right"><math id="S4.T6.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T6.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S4.T6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.1.1.1.1.1.m1.1c">\pm</annotation></semantics></math>2</span>
<span id="S4.T6.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_right"><math id="S4.T6.2.2.2.2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T6.2.2.2.2.2.2.2.2.2.m1.1a"><mo id="S4.T6.2.2.2.2.2.2.2.2.2.m1.1.1" xref="S4.T6.2.2.2.2.2.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.2.2.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T6.2.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S4.T6.2.2.2.2.2.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.2.2.2.2.2.2.m1.1c">\pm</annotation></semantics></math>4</span>
<span id="S4.T6.3.3.3.3.3.3.3.3.3" class="ltx_td ltx_align_right"><math id="S4.T6.3.3.3.3.3.3.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T6.3.3.3.3.3.3.3.3.3.m1.1a"><mo id="S4.T6.3.3.3.3.3.3.3.3.3.m1.1.1" xref="S4.T6.3.3.3.3.3.3.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.3.3.3.3.3.3.m1.1b"><csymbol cd="latexml" id="S4.T6.3.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="S4.T6.3.3.3.3.3.3.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.3.3.3.3.3.3.m1.1c">\pm</annotation></semantics></math>6</span>
<span id="S4.T6.4.4.4.4.4.4.4.4.4" class="ltx_td ltx_align_right"><math id="S4.T6.4.4.4.4.4.4.4.4.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T6.4.4.4.4.4.4.4.4.4.m1.1a"><mo id="S4.T6.4.4.4.4.4.4.4.4.4.m1.1.1" xref="S4.T6.4.4.4.4.4.4.4.4.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T6.4.4.4.4.4.4.4.4.4.m1.1b"><csymbol cd="latexml" id="S4.T6.4.4.4.4.4.4.4.4.4.m1.1.1.cmml" xref="S4.T6.4.4.4.4.4.4.4.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.4.4.4.4.4.4.4.4.m1.1c">\pm</annotation></semantics></math>8</span>
<span id="S4.T6.5.5.5.5.5.5.5.5.5" class="ltx_td ltx_align_right"><math id="S4.T6.5.5.5.5.5.5.5.5.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T6.5.5.5.5.5.5.5.5.5.m1.1a"><mo id="S4.T6.5.5.5.5.5.5.5.5.5.m1.1.1" xref="S4.T6.5.5.5.5.5.5.5.5.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T6.5.5.5.5.5.5.5.5.5.m1.1b"><csymbol cd="latexml" id="S4.T6.5.5.5.5.5.5.5.5.5.m1.1.1.cmml" xref="S4.T6.5.5.5.5.5.5.5.5.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.5.5.5.5.5.5.5.5.5.m1.1c">\pm</annotation></semantics></math>10</span></span>
<span id="S4.T6.5.5.5.5.5.5.5.9.4" class="ltx_tr">
<span id="S4.T6.5.5.5.5.5.5.5.9.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Reverberance</span>
<span id="S4.T6.5.5.5.5.5.5.5.9.4.2" class="ltx_td ltx_align_right">20</span>
<span id="S4.T6.5.5.5.5.5.5.5.9.4.3" class="ltx_td ltx_align_right">40</span>
<span id="S4.T6.5.5.5.5.5.5.5.9.4.4" class="ltx_td ltx_align_right">60</span>
<span id="S4.T6.5.5.5.5.5.5.5.9.4.5" class="ltx_td ltx_align_right">80</span>
<span id="S4.T6.5.5.5.5.5.5.5.9.4.6" class="ltx_td ltx_align_right">100</span></span>
<span id="S4.T6.5.5.5.5.5.5.5.10.5" class="ltx_tr">
<span id="S4.T6.5.5.5.5.5.5.5.10.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Audio Compression (bitrate)</span>
<span id="S4.T6.5.5.5.5.5.5.5.10.5.2" class="ltx_td ltx_align_right ltx_border_bb">320k</span>
<span id="S4.T6.5.5.5.5.5.5.5.10.5.3" class="ltx_td ltx_align_right ltx_border_bb">256k</span>
<span id="S4.T6.5.5.5.5.5.5.5.10.5.4" class="ltx_td ltx_align_right ltx_border_bb">192k</span>
<span id="S4.T6.5.5.5.5.5.5.5.10.5.5" class="ltx_td ltx_align_right ltx_border_bb">128k</span>
<span id="S4.T6.5.5.5.5.5.5.5.10.5.6" class="ltx_td ltx_align_right ltx_border_bb">64k</span></span>
</span>
</span></span></span>
</span></span></span><span id="S4.T6.5.6" class="ltx_text" style="font-size:144%;"></span></p>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.9.1.1" class="ltx_text" style="font-size:63%;">Table 6</span>: </span><span id="S4.T6.10.2" class="ltx_text" style="font-size:63%;">Parameters used to generate samples with audio perturbations at different levels of intensities.</span></figcaption>
</figure>
<figure id="S4.T7" class="ltx_table">
<p id="S4.T7.2" class="ltx_p ltx_align_center"><span id="S4.T7.2.1" class="ltx_text ltx_inline-block" style="font-size:144%;width:433.6pt;">
<span id="S4.T7.2.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:348.8pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T7.2.1.1.1" class="ltx_p"><span id="S4.T7.2.1.1.1.1" class="ltx_text">
<span id="S4.T7.2.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T7.2.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T7.2.1.1.1.1.1.1.1.1" class="ltx_td ltx_border_tt"></span>
<span id="S4.T7.2.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Method</span>
<span id="S4.T7.2.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ACC</span>
<span id="S4.T7.2.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AUC</span></span>
<span id="S4.T7.2.1.1.1.1.1.2.2" class="ltx_tr">
<span id="S4.T7.2.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">(i)</span>
<span id="S4.T7.2.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">Ours with Frozen Stage 1 + MLP</span>
<span id="S4.T7.2.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">94.8</span>
<span id="S4.T7.2.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">85.3</span></span>
<span id="S4.T7.2.1.1.1.1.1.3.3" class="ltx_tr">
<span id="S4.T7.2.1.1.1.1.1.3.3.1" class="ltx_td ltx_align_left">(ii)</span>
<span id="S4.T7.2.1.1.1.1.1.3.3.2" class="ltx_td ltx_align_left">Ours with Frozen Stage 1 + SVM</span>
<span id="S4.T7.2.1.1.1.1.1.3.3.3" class="ltx_td ltx_align_center">96.3</span>
<span id="S4.T7.2.1.1.1.1.1.3.3.4" class="ltx_td ltx_align_center">88.9</span></span>
<span id="S4.T7.2.1.1.1.1.1.4.4" class="ltx_tr">
<span id="S4.T7.2.1.1.1.1.1.4.4.1" class="ltx_td ltx_border_bb ltx_border_t"></span>
<span id="S4.T7.2.1.1.1.1.1.4.4.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S4.T7.2.1.1.1.1.1.4.4.2.1" class="ltx_text ltx_font_bold">AVFF (Ours)</span></span>
<span id="S4.T7.2.1.1.1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T7.2.1.1.1.1.1.4.4.3.1" class="ltx_text ltx_font_bold">98.6</span></span>
<span id="S4.T7.2.1.1.1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T7.2.1.1.1.1.1.4.4.4.1" class="ltx_text ltx_font_bold">99.1</span></span></span>
</span>
</span></span></span>
</span></span></span><span id="S4.T7.2.2" class="ltx_text" style="font-size:144%;"></span></p>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.6.1.1" class="ltx_text" style="font-size:63%;">Table 7</span>: </span><span id="S4.T7.7.2" class="ltx_text ltx_font_bold" style="font-size:63%;">Classification Performance on the Learned Representation.<span id="S4.T7.7.2.1" class="ltx_text ltx_font_medium"> We evaluate the classification performance of the learned representation by freezing the encoders and A2V/V2A networks and training only the downstream networks. We employ (i) an MLP similar to the proposed method, and (ii) a kernel SVM (RBF), as the classifier. Both classifiers yield reasonably high metrics, indicating the effectiveness of the learned representation at the end of Stage 1 in distinguishing between real and fake videos.
</span></span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">D.3 </span>Classification Performance on the Learned Representation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text" style="font-size:144%;">We further evaluate the learned representation at the end of Stage 1, by performing the downstream deepfake classification task with the weights of the encoders and the A2V/V2A networks frozen.
We train two classifiers for the downstream task: (i) the classifier network described in the main paper, and (ii) kernel SVM using an RBF kernel (gamma=0.1, C=1.0).
The results are reported in </span><a href="#S4.T7" title="In D.2 Robustness to Unseen Perturbations ‣ D Additional Results ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a><span id="S4.SS3.p1.1.2" class="ltx_text" style="font-size:144%;">. Both classifiers yield reasonably high accuracy and AUC values. This is indicative of the highly discriminative nature of the learned representation at the end of Stage 1.
This reinforces the analysis on the learned representation at the end of Stage 1, as discussed in Sec. 4.2 of the main paper.</span></p>
</div>
</section>
</section>
<section id="S5a" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:144%;">
<span class="ltx_tag ltx_tag_section">E </span>Visual Examples of Challenging Scenarios</h2>

<div id="S5a.p1" class="ltx_para">
<p id="S5a.p1.1" class="ltx_p"><span id="S5a.p1.1.1" class="ltx_text" style="font-size:144%;">As discussed in Sec. 6 of the main paper, since we rely on audio-visual correspondence to distinguish between real and fake videos, scenarios where such correspondence cannot be established would be challenging.
In </span><a href="#S5.F7" title="In E Visual Examples of Challenging Scenarios ‣ AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection" class="ltx_ref" style="font-size:144%;"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a><span id="S5a.p1.1.2" class="ltx_text" style="font-size:144%;">, we depict a few such visual examples.</span></p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2406.02951/assets/x4.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="151" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.9.1.1" class="ltx_text" style="font-size:63%;">Figure 7</span>: </span><span id="S5.F7.10.2" class="ltx_text ltx_font_bold" style="font-size:63%;">Visual Examples of a Few Challenging Scenarios<span id="S5.F7.10.2.1" class="ltx_text ltx_font_medium">. Images from left to right depict examples of extreme poses (<em id="S5.F7.10.2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.F7.10.2.1.2" class="ltx_text"></span> near profile), occlusions with masks, and occlusions with hands across the face, which makes it challenging for our model to establish correspondence between the audio and visual modalities.</span></span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Afouras et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Lrs3-ted: a large-scale dataset for visual speech recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib1.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1809.00496</em><span id="bib.bib1.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Agarwal et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Shruti Agarwal, Hany Farid, Ohad Fried, and Maneesh Agrawala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Detecting deep-fake videos from phoneme-viseme mismatches.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib2.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</em><span id="bib.bib2.11.3" class="ltx_text" style="font-size:90%;">, pages 660–661, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Arjovsky et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Martin Arjovsky, Soumith Chintala, and Léon Bottou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Wasserstein generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International conference on machine learning</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, pages 214–223. PMLR, 2017.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Bai et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Weiming Bai, Yufan Liu, Zhipeng Zhang, Bing Li, and Weiming Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Aunet: Learning relations between action units for face forgery detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib4.11.3" class="ltx_text" style="font-size:90%;">, pages 24709–24719, 2023.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Baltrušaitis et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Multimodal machine learning: A survey and taxonomy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span id="bib.bib5.10.2" class="ltx_text" style="font-size:90%;">, 41(2):423–443, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Cai et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Zhixi Cai, Shreya Ghosh, Kalin Stefanov, Abhinav Dhall, Jianfei Cai, Hamid Rezatofighi, Reza Haffari, and Munawar Hayat.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Marlin: Masked autoencoder for facial video representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, pages 1493–1504, 2023.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Cheng et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Harry Cheng, Yangyang Guo, Tianyi Wang, Qi Li, Xiaojun Chang, and Liqiang Nie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Voice-face homogeneity tells deepfake.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib7.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.02195</em><span id="bib.bib7.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Chugh et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Komal Chugh, Parul Gupta, Abhinav Dhall, and Ramanathan Subramanian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Not made for each other-audio-visual dissonance-based deepfake detection and localization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib8.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 28th ACM international conference on multimedia</em><span id="bib.bib8.11.3" class="ltx_text" style="font-size:90%;">, pages 439–447, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Chung et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
J Chung, A Nagrani, and A Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Voxceleb2: Deep speaker recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Interspeech 2018</em><span id="bib.bib9.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.4.4.1" class="ltx_text" style="font-size:90%;">Chung and Zisserman [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="font-size:90%;">
J. S. Chung and A. Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">Out of time: automated lip sync in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib10.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Workshop on Multi-view Lip-reading, ACCV</em><span id="bib.bib10.10.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Devlin et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Bert: Pre-training of deep bidirectional transformers for language understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib11.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">North American Chapter of the Association for Computational Linguistics</em><span id="bib.bib11.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Dolhansky et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">The deepfake detection challenge (dfdc) dataset.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib12.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2006.07397</em><span id="bib.bib12.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Dong et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Shichao Dong, Jin Wang, Renhe Ji, Jiajun Liang, Haoqiang Fan, and Zheng Ge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Implicit identity leakage: The stumbling block to improving deepfake detection generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib13.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib13.11.3" class="ltx_text" style="font-size:90%;">, pages 3994–4004, 2023.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Dong et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Ting Zhang, Weiming Zhang, Nenghai Yu, Dong Chen, Fang Wen, and Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Protecting celebrities from deepfake with identity consistency transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib14.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib14.11.3" class="ltx_text" style="font-size:90%;">, pages 9458–9468, 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Dosovitskiy et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span id="bib.bib15.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Feng et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Chao Feng, Ziyang Chen, and Andrew Owens.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Self-supervised video forensics by audio-visual anomaly detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib16.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib16.11.3" class="ltx_text" style="font-size:90%;">, pages 10491–10503, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Gemmeke et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Audio set: An ontology and human-labeled dataset for audio events.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">, pages 776–780. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Georgescu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Mariana-Iuliana Georgescu, Eduardo Fonseca, Radu Tudor Ionescu, Mario Lucic, Cordelia Schmid, and Anurag Arnab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Audiovisual masked autoencoders.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, pages 16144–16154, 2023.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Gong et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Yuan Gong, Andrew Rouditchenko, Alexander H. Liu, David Harwath, Leonid Karlinsky, Hilde Kuehne, and James R. Glass.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Contrastive audio-visual masked autoencoder.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib19.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">. OpenReview.net, 2023.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Guzhov et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Andrey Guzhov, Federico Raue, Jörn Hees, and Andreas Dengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Audioclip: Extending clip to image, text and audio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib20.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em><span id="bib.bib20.11.3" class="ltx_text" style="font-size:90%;">, pages 976–980. IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Haliassos et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Alexandros Haliassos, Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Lips don’t lie: A generalisable and robust approach to face forgery detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib21.11.3" class="ltx_text" style="font-size:90%;">, pages 5039–5049, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Haliassos et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Alexandros Haliassos, Rodrigo Mira, Stavros Petridis, and Maja Pantic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Leveraging real talking faces via self-supervision for robust forgery detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib22.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib22.11.3" class="ltx_text" style="font-size:90%;">, pages 14950–14962, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Masked autoencoders are scalable vision learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib23.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib23.11.3" class="ltx_text" style="font-size:90%;">, pages 15979–15988, 2022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Huang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Baojin Huang, Zhongyuan Wang, Jifan Yang, Jiaxin Ai, Qin Zou, Qian Wang, and Dengpan Ye.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Implicit identity driven deepfake face swapping detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib24.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib24.11.3" class="ltx_text" style="font-size:90%;">, pages 4490–4499, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.4.4.1" class="ltx_text" style="font-size:90%;">Huang and De La Torre [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">
Dong Huang and Fernando De La Torre.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">Facial action transfer with personalized bilinear regression.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib25.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part II 12</em><span id="bib.bib25.10.3" class="ltx_text" style="font-size:90%;">, pages 144–158. Springer, 2012.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Huang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, and Christoph Feichtenhofer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Masked autoencoders that listen.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib26.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib26.10.2" class="ltx_text" style="font-size:90%;">, 35:28708–28720, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Ilyas et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Hafsa Ilyas, Ali Javed, and Khalid Mahmood Malik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Avfakenet: A unified end-to-end dense swin transformer deep learning model for audio–visual deepfakes detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib27.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Applied Soft Computing</em><span id="bib.bib27.10.2" class="ltx_text" style="font-size:90%;">, 136:110124, 2023.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Jeong et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo, Wonmin Byeon, Sangpil Kim, and Jinkyu Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">The power of sound (tpos): Audio reactive video generation with stable diffusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib28.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib28.11.3" class="ltx_text" style="font-size:90%;">, pages 7822–7832, 2023.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Jia et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Transfer learning from speaker verification to multispeaker text-to-speech synthesis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib29.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib29.10.2" class="ltx_text" style="font-size:90%;">, 31, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Jiang et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and Chen Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Deeperforensics-1.0: A large-scale dataset for real-world face forgery detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, pages 2889–2898, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Jung et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Tackhyun Jung, Sangwon Kim, and Keecheon Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Deepvision: Deepfakes detection using human eye blinking pattern.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib31.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Access</em><span id="bib.bib31.10.2" class="ltx_text" style="font-size:90%;">, 8:83144–83154, 2020.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Kaddar et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Bachir Kaddar, Sid Ahmed Fezza, Wassim Hamidouche, Zahid Akhtar, and Abdenour Hadid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Hcit: Deepfake video detection using a hybrid model of cnn features and vision transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib32.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 International Conference on Visual Communications and Image Processing (VCIP)</em><span id="bib.bib32.11.3" class="ltx_text" style="font-size:90%;">, pages 1–5, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Karras et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
Tero Karras, Samuli Laine, and Timo Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">A style-based generator architecture for generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib33.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib33.11.3" class="ltx_text" style="font-size:90%;">, pages 4401–4410, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Khalid et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Hasam Khalid, Shahroz Tariq, Minha Kim, and Simon S Woo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Fakeavceleb: A novel audio-video multimodal deepfake dataset.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib34.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2108.05080</em><span id="bib.bib34.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.4.4.1" class="ltx_text" style="font-size:90%;">Korshunov and Marcel [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.6.1" class="ltx_text" style="font-size:90%;">
Pavel Korshunov and Sébastien Marcel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">Deepfakes: a new threat to face recognition? assessment and detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib35.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.08685</em><span id="bib.bib35.9.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Korshunova et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Iryna Korshunova, Wenzhe Shi, Joni Dambre, and Lucas Theis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">Fast face-swap using convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib36.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</em><span id="bib.bib36.11.3" class="ltx_text" style="font-size:90%;">, pages 3677–3685, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Kwon et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, and Gyeongsu Chae.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Kodf: A large-scale korean deepfake detection dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib37.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib37.11.3" class="ltx_text" style="font-size:90%;">, pages 10744–10753, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Yuezun Li, Ming-Ching Chang, and Siwei Lyu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">In ictu oculi: Exposing ai created fake videos by detecting eye blinking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib38.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 IEEE International Workshop on Information Forensics and Security (WIFS)</em><span id="bib.bib38.11.3" class="ltx_text" style="font-size:90%;">, pages 1–7, 2018.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.4.4.1" class="ltx_text" style="font-size:90%;">Loshchilov and Hutter [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.6.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">Sgdr: Stochastic gradient descent with warm restarts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib39.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span id="bib.bib39.10.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.4.4.1" class="ltx_text" style="font-size:90%;">Loshchilov and Hutter [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.6.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">Decoupled weight decay regularization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib40.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span id="bib.bib40.10.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.4.4.1" class="ltx_text" style="font-size:90%;">Lutz and Bassett [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.6.1" class="ltx_text" style="font-size:90%;">
Kevin Lutz and Robert Bassett.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">Deepfake detection with inconsistent head poses: Reproducibility and analysis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib41.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib41.9.2" class="ltx_text" style="font-size:90%;">, abs/2108.12715, 2021.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Ma et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De la Torre, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Pixel codec avatars.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib42.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib42.11.3" class="ltx_text" style="font-size:90%;">, pages 64–73, 2021.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Mittal et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh Manocha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Emotions don’t lie: An audio-visual deepfake detection method using affective cues.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib43.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 28th ACM international conference on multimedia</em><span id="bib.bib43.11.3" class="ltx_text" style="font-size:90%;">, pages 2823–2832, 2020a.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Mittal et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh Manocha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib44.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI conference on artificial intelligence</em><span id="bib.bib44.11.3" class="ltx_text" style="font-size:90%;">, pages 1359–1367, 2020b.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Morgado et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Pedro Morgado, Ishan Misra, and Nuno Vasconcelos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">Robust audio-visual instance discrimination.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib45.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib45.11.3" class="ltx_text" style="font-size:90%;">, pages 12934–12945, 2021.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Nirkin et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
Yuval Nirkin, Yosi Keller, and Tal Hassner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Fsgan: Subject agnostic face swapping and reenactment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib46.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</em><span id="bib.bib46.11.3" class="ltx_text" style="font-size:90%;">, pages 7184–7193, 2019.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Ojha et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Utkarsh Ojha, Yuheng Li, and Yong Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Towards universal fake image detectors that generalize across generative models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib47.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib47.11.3" class="ltx_text" style="font-size:90%;">, pages 24480–24489, 2023.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Perov et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Umé, Mr Dpfks, Carl Shift Facenheim, Luis RP, Jian Jiang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Deepfacelab: Integrated, flexible and extensible face-swapping framework.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib48.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2005.05535</em><span id="bib.bib48.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Polyak et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Adam Polyak, Lior Wolf, and Yaniv Taigman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">Tts skins: Speaker conversion via asr.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib49.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1904.08983</em><span id="bib.bib49.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Prajwal et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Namboodiri, and CV Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">A lip sync expert is all you need for speech to lip generation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib50.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 28th ACM international conference on multimedia</em><span id="bib.bib50.11.3" class="ltx_text" style="font-size:90%;">, pages 484–492, 2020.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Radford et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib51.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International conference on machine learning</em><span id="bib.bib51.11.3" class="ltx_text" style="font-size:90%;">, pages 8748–8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Rossler et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">Faceforensics++: Learning to detect manipulated facial images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib52.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</em><span id="bib.bib52.11.3" class="ltx_text" style="font-size:90%;">, pages 1–11, 2019.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Rouditchenko et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
Andrew Rouditchenko, Angie Boggust, David Harwath, Brian Chen, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio Feris, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">Avlnet: Learning audio-visual language representations from instructional videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib53.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Annual Conference of the International Speech Communication Association</em><span id="bib.bib53.11.3" class="ltx_text" style="font-size:90%;">. International Speech Communication Association, 2021.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">Ruan et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib54.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib54.11.3" class="ltx_text" style="font-size:90%;">, pages 10219–10228, 2023.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.4.4.1" class="ltx_text" style="font-size:90%;">Sanderson and Lovell [2009]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.6.1" class="ltx_text" style="font-size:90%;">
Conrad Sanderson and Brian C Lovell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">Multi-region probabilistic histograms for robust and scalable identity inference.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib55.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in biometrics: Third international conference, ICB 2009, alghero, italy, june 2-5, 2009. Proceedings 3</em><span id="bib.bib55.10.3" class="ltx_text" style="font-size:90%;">, pages 199–208. Springer, 2009.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.5.5.1" class="ltx_text" style="font-size:90%;">Siarohin et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">
Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.8.1" class="ltx_text" style="font-size:90%;">First order motion model for image animation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib56.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib56.10.2" class="ltx_text" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text" style="font-size:90%;">Tong et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">
Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text" style="font-size:90%;">Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib57.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib57.10.2" class="ltx_text" style="font-size:90%;">, 35:10078–10093, 2022.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.4.4.1" class="ltx_text" style="font-size:90%;">van der Maaten and Hinton [2008]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.6.1" class="ltx_text" style="font-size:90%;">
Laurens van der Maaten and Geoffrey E. Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text" style="font-size:90%;">Visualizing data using t-sne.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib58.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</em><span id="bib.bib58.9.2" class="ltx_text" style="font-size:90%;">, 9:2579–2605, 2008.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2021a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text" style="font-size:90%;">
Jun Wang, Yinglu Liu, Yibo Hu, Hailin Shi, and Tao Mei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text" style="font-size:90%;">Facex-zoo: A pytorch toolbox for face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib59.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 29th ACM International Conference on Multimedia</em><span id="bib.bib59.11.3" class="ltx_text" style="font-size:90%;">, pages 3779–3782, 2021a.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2021b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.7.1" class="ltx_text" style="font-size:90%;">
Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.8.1" class="ltx_text" style="font-size:90%;">One-shot free-view neural talking-head synthesis for video conferencing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib60.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib60.11.3" class="ltx_text" style="font-size:90%;">, pages 10039–10049, 2021b.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.7.1" class="ltx_text" style="font-size:90%;">
Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, and Houqiang Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.8.1" class="ltx_text" style="font-size:90%;">Altfreezing for more general video face forgery detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib61.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib61.11.3" class="ltx_text" style="font-size:90%;">, pages 4129–4138, 2023.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.4.4.1" class="ltx_text" style="font-size:90%;">Wodajo and Atnafu [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.6.1" class="ltx_text" style="font-size:90%;">
Deressa Wodajo and Solomon Atnafu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.7.1" class="ltx_text" style="font-size:90%;">Deepfake video detection using convolutional vision transformer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib62.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2102.11126</em><span id="bib.bib62.9.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.5.5.1" class="ltx_text" style="font-size:90%;">Wolf et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.7.1" class="ltx_text" style="font-size:90%;">
Lior Wolf, Tal Hassner, and Itay Maoz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.8.1" class="ltx_text" style="font-size:90%;">Face recognition in unconstrained videos with matched background similarity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib63.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR 2011</em><span id="bib.bib63.11.3" class="ltx_text" style="font-size:90%;">, pages 529–534. IEEE, 2011.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.5.5.1" class="ltx_text" style="font-size:90%;">Wu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.7.1" class="ltx_text" style="font-size:90%;">
Haojie Wu, Pan Hui, and Pengyuan Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.8.1" class="ltx_text" style="font-size:90%;">Deepfake in the metaverse: An outlook survey.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib64.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.07011</em><span id="bib.bib64.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.7.1" class="ltx_text" style="font-size:90%;">
Wenyuan Yang, Xiaoyu Zhou, Zhikai Chen, Bofei Guo, Zhongjie Ba, Zhihua Xia, Xiaochun Cao, and Kui Ren.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.8.1" class="ltx_text" style="font-size:90%;">Avoid-df: Audio-visual joint learning for detecting deepfake.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib65.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Forensics and Security</em><span id="bib.bib65.10.2" class="ltx_text" style="font-size:90%;">, 18:2015–2029, 2023.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.7.1" class="ltx_text" style="font-size:90%;">
Xin Yang, Yuezun Li, and Siwei Lyu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.8.1" class="ltx_text" style="font-size:90%;">Exposing deep fakes using inconsistent head poses.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib66.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em><span id="bib.bib66.11.3" class="ltx_text" style="font-size:90%;">, pages 8261–8265, 2019.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.5.5.1" class="ltx_text" style="font-size:90%;">Yi et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.7.1" class="ltx_text" style="font-size:90%;">
Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, and Yong-Jin Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.8.1" class="ltx_text" style="font-size:90%;">Audio-driven talking face video generation with learning-based personalized head pose.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib67.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2002.10137</em><span id="bib.bib67.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.5.5.1" class="ltx_text" style="font-size:90%;">Zakharov et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.7.1" class="ltx_text" style="font-size:90%;">
Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor Lempitsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.8.1" class="ltx_text" style="font-size:90%;">Few-shot adversarial learning of realistic neural talking head models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib68.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</em><span id="bib.bib68.11.3" class="ltx_text" style="font-size:90%;">, pages 9459–9468, 2019.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.5.5.1" class="ltx_text" style="font-size:90%;">Zhao et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.7.1" class="ltx_text" style="font-size:90%;">
Xiaohui Zhao, Yang Yu, Rongrong Ni, and Yao Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.8.1" class="ltx_text" style="font-size:90%;">Exploring complementarity of global and local spatiotemporal information for fake face video detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib69.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em><span id="bib.bib69.11.3" class="ltx_text" style="font-size:90%;">, pages 2884–2888, 2022.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.5.5.1" class="ltx_text" style="font-size:90%;">Zheng et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.7.1" class="ltx_text" style="font-size:90%;">
Yinglin Zheng, Jianmin Bao, Dong Chen, Ming Zeng, and Fang Wen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.8.1" class="ltx_text" style="font-size:90%;">Exploring temporal coherence for more general video face forgery detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib70.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</em><span id="bib.bib70.11.3" class="ltx_text" style="font-size:90%;">, pages 15044–15054, 2021.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.4.4.1" class="ltx_text" style="font-size:90%;">Zhou and Lim [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.6.1" class="ltx_text" style="font-size:90%;">
Yipin Zhou and Ser-Nam Lim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.7.1" class="ltx_text" style="font-size:90%;">Joint audio-visual deepfake detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib71.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib71.10.3" class="ltx_text" style="font-size:90%;">, pages 14800–14809, 2021.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.7.1" class="ltx_text" style="font-size:90%;">
Yang Zhou, Zhan Xu, Chris Landreth, Evangelos Kalogerakis, Subhransu Maji, and Karan Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.8.1" class="ltx_text" style="font-size:90%;">Visemenet: Audio-driven animator-centric speech animation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib72.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Transactions on Graphics (TOG)</em><span id="bib.bib72.10.2" class="ltx_text" style="font-size:90%;">, 37(4):1–10, 2018.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib73.5.5.1" class="ltx_text" style="font-size:90%;">Zhu et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib73.7.1" class="ltx_text" style="font-size:90%;">
Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.8.1" class="ltx_text" style="font-size:90%;">Celebv-hq: A large-scale video facial attributes dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib73.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European conference on computer vision</em><span id="bib.bib73.11.3" class="ltx_text" style="font-size:90%;">, pages 650–667. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib74.5.5.1" class="ltx_text" style="font-size:90%;">Zhuang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib74.7.1" class="ltx_text" style="font-size:90%;">
Wanyi Zhuang, Qi Chu, Zhentao Tan, Qiankun Liu, Haojie Yuan, Changtao Miao, Zixiang Luo, and Nenghai Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.8.1" class="ltx_text" style="font-size:90%;">Uia-vit: Unsupervised inconsistency-aware method based on vision transformer for face forgery detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib74.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part V</em><span id="bib.bib74.11.3" class="ltx_text" style="font-size:90%;">, page 391–407, Berlin, Heidelberg, 2022. Springer-Verlag.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.02950" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.02951" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.02951">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.02951" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.02952" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Jul  6 00:13:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
