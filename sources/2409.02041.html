<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.02041] The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge</title><meta property="og:description" content="This technical report outlines our submission system for the CHiME-8 NOTSOFAR-1 Challenge. The primary difficulty of this challenge is the dataset recorded across various conference rooms, which captures real-world com…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.02041">

<!--Generated on Sat Oct  5 21:38:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document" style="font-size:90%;">The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text" style="font-size:90%;">This technical report outlines our submission system for the CHiME-8 NOTSOFAR-1 Challenge. The primary difficulty of this challenge is the dataset recorded across various conference rooms, which captures real-world complexities such as high overlap rates, background noises, a variable number of speakers, and natural conversation styles. To address these issues, we optimized the system in several aspects: For front-end speech signal processing, we introduced a data-driven joint training method for diarization and separation (JDS) to enhance audio quality. Additionally, we also integrated traditional guided source separation (GSS) for multi-channel track to provide complementary information for the JDS. For back-end speech recognition, we enhanced Whisper with WavLM, ConvNeXt, and Transformer innovations, applying multi-task training and Noise KLD augmentation, to significantly advance ASR robustness and accuracy.
Our system attained a Time-Constrained minimum Permutation Word Error Rate (tcpWER) of 14.265% and 22.989% on the CHiME-8 NOTSOFAR-1 Dev-set-2 multi-channel and single-channel tracks, respectively.</span></p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Index Terms</span><span id="p1.1.2" class="ltx_text" style="font-size:90%;">: CHiME challenge, speaker diarization, speech separation, speech recognition, joint training</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>System Description</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text" style="font-size:90%;">Our overall system follows the process illustrated in Fig. </span><a href="#S1.F1" title="Figure 1 ‣ 1 System Description ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S1.p1.1.2" class="ltx_text" style="font-size:90%;">. First, the diarization system is used to predict the speaker’s time distribution, which is then utilized to perform speech separation. Then, the separated speech is sent to the speech recognition system. In the following sections, we will describe the single-channel and multi-channel systems in detail.</span></p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.02041/assets/overall.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="55" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> Overall framework of the system.</figcaption>
</figure>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Multi-channel System</h3>

<section id="S1.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">1.1.1 </span>Diarization</h4>

<figure id="S1.F2" class="ltx_figure"><img src="/html/2409.02041/assets/x1.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="257" height="111" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The diarization pipeline for multi-channel system.</figcaption>
</figure>
<div id="S1.SS1.SSS1.p1" class="ltx_para">
<p id="S1.SS1.SSS1.p1.1" class="ltx_p"><span id="S1.SS1.SSS1.p1.1.1" class="ltx_text" style="font-size:90%;">Fig. </span><a href="#S1.F2" title="Figure 2 ‣ 1.1.1 Diarization ‣ 1.1 Multi-channel System ‣ 1 System Description ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S1.SS1.SSS1.p1.1.2" class="ltx_text" style="font-size:90%;"> illustrates the diarization component of the multi-channel system. For the original multi-channel data, we first perform weighted prediction error (WPE) algorithm, followed by overlap segment detection. For the detected overlapping segments, we employ the multi-channel 3-second continuous speech separation (CSS) method to effectively isolate each speaker’s speech. For non-overlapping segments, we enhance the multi-channel speech using the MVDR beamformer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS1.p1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S1.SS1.SSS1.p1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS1.SSS1.p1.1.5" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S1.SS1.SSS1.p2" class="ltx_para">
<p id="S1.SS1.SSS1.p2.1" class="ltx_p"><span id="S1.SS1.SSS1.p2.1.1" class="ltx_text" style="font-size:90%;">We conduct the clustering-based speaker diarization (CSD) method on these pre-processed speech, resulting in preliminary speaker diarization priors, referred to as ‘Sys-2 RTTM’ in Fig. </span><a href="#S1.F2" title="Figure 2 ‣ 1.1.1 Diarization ‣ 1.1 Multi-channel System ‣ 1 System Description ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S1.SS1.SSS1.p2.1.2" class="ltx_text" style="font-size:90%;">. For CSD system, we use the spectral clustering algorithm. We leverage the ResNet-221 model for speaker embedding extraction, which is trained on the VoxCeleb </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS1.p2.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S1.SS1.SSS1.p2.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS1.SSS1.p2.1.5" class="ltx_text" style="font-size:90%;"> and LibriSpeech datasets. To obtain different diarization priors, we further apply various processing techniques to the speech used for clustering. Firstly, we use the results obtained from clustering as initial priors, and feeding them into the neural network-based speaker diarization (NSD) system to achieve more precise speaker boundary information. The NSD employed in our system is the memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS1.p2.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.SS1.SSS1.p2.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS1.SSS1.p2.1.8" class="ltx_text" style="font-size:90%;">, which combines the advantages of memory-aware multi-speaker embedding and sequence-to-sequence architecture. Then, following our previous methods in CHiME-7 DASR Challenge </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS1.p2.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S1.SS1.SSS1.p2.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS1.SSS1.p2.1.11" class="ltx_text" style="font-size:90%;">, we conduct cACGMM rectification on the original audios, adopting a window length of 120 seconds and a window shift of 60 seconds. This rectification utilizes the previous NSD decoding result as the initialization mask. By implementing a threshold on the spectrum mask of the cACGMM, we obtain a refined secondary initialization of diarization results for the NSD system. After the official GSS initialized with the second NSD decoding results, we perform the re-clustering to obtain better diarization priors (Sys-1 RTTM). Additionally, the decoding results from the first NSD can be directly utilized to initialize the GSS, thereby generating separated audios. For these separated audios, we conducted re-clustering with the fixed number of speakers (maintaining the global number of speakers within a session) and the non-fix number of speakers (the original version), resulting in two initial diarization priors, namely ‘Sys-3 RTTM’ and ‘Sys-4 RTTM’, respectively.</span></p>
</div>
</section>
<section id="S1.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">1.1.2 </span>Separation</h4>

<div id="S1.SS1.SSS2.p1" class="ltx_para">
<p id="S1.SS1.SSS2.p1.1" class="ltx_p"><span id="S1.SS1.SSS2.p1.1.1" class="ltx_text" style="font-size:90%;">After obtaining the RTTMs from the diarization system, we acquire information about the speaker distribution. Utilizing this information, we proceed with various versions of speech separation as depicted in Fig. </span><a href="#S1.F3" title="Figure 3 ‣ 1.1.2 Separation ‣ 1.1 Multi-channel System ‣ 1 System Description ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S1.SS1.SSS2.p1.1.2" class="ltx_text" style="font-size:90%;">. For the first system (V1), we utilize the NSD to optimize the time boundaries. The optimized results are then used to initialize the GSS algorithm, resulting in the separated audios. For the second system (V2), we utilize the time masks estimated from the NSD as the inputs for JDS system. This guides the JDS system in estimating time-frequency (T-F) soft masks. These T-F masks are then employed to initialize the GSS in the T-F domain, thus providing the GSS with initialization information in both time and frequency dimensions. For the third system (V3), we directly utilize the T-F masks predicted by the JDS system to guide the MVDR beamforming, while still employing the time boundaries provided by the NSD to get the separated speech segments. Fig. </span><a href="#S1.F4" title="Figure 4 ‣ 1.1.2 Separation ‣ 1.1 Multi-channel System ‣ 1 System Description ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S1.SS1.SSS2.p1.1.3" class="ltx_text" style="font-size:90%;"> shows the overall framework of multi-channel joint training method for diarization and separation (JDS). The JDS system comprises two main components: the speaker diarization module and the speech separation module. Based on the original end-to-end speaker diarization systems, the JDS system serially integrates the separation module. This helps the speech separation system accurately identify the number of speakers and the corresponding identities. This information also facilitates the speech separation model in more effectively distinguishing between different speakers. Consequently, the separation module in JDS system primarily maps the time information of various speakers to time-frequency information, which significantly simplifies the speech separation process.</span></p>
</div>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2409.02041/assets/x2.png" id="S1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="231" height="46" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The separation pipeline for multi-channel system.</figcaption>
</figure>
<figure id="S1.F4" class="ltx_figure"><img src="/html/2409.02041/assets/x3.png" id="S1.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="438" height="361" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Overall framework of multi-channel JDS method.</figcaption>
</figure>
</section>
<section id="S1.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">1.1.3 </span>Speech Recognition</h4>

<figure id="S1.F5" class="ltx_figure"><img src="/html/2409.02041/assets/EnhancedWhisper.png" id="S1.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="430" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The architecture of Enhanced Whisper.</figcaption>
</figure>
<div id="S1.SS1.SSS3.p1" class="ltx_para">
<p id="S1.SS1.SSS3.p1.1" class="ltx_p"><span id="S1.SS1.SSS3.p1.1.1" class="ltx_text" style="font-size:90%;">For automatic speech recognition tasks, we leverage Whisper </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS3.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S1.SS1.SSS3.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS1.SSS3.p1.1.4" class="ltx_text" style="font-size:90%;">, a state-of-the-art open-source model renowned for its high accuracy. Whisper follows an encoder-decoder architecture based on the Transformer framework. The input to the model is represented as a log Mel-spectrogram. Both the encoder and decoder components feature absolute positional encoding and are composed of several transformer layers. Notably, the encoder contains two layers of 1D convolution preceding the absolute positional encoding stage, which aids in extracting local features from the input audio data.</span></p>
</div>
<div id="S1.SS1.SSS3.p2" class="ltx_para">
<p id="S1.SS1.SSS3.p2.1" class="ltx_p"><span id="S1.SS1.SSS3.p2.1.1" class="ltx_text" style="font-size:90%;">We introduce Enhanced Whisper, a variant that introduces a series of enhancements to the base Whisper model. An overview of the modified architecture is illustrated in Fig.  </span><a href="#S1.F5" title="Figure 5 ‣ 1.1.3 Speech Recognition ‣ 1.1 Multi-channel System ‣ 1 System Description ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S1.SS1.SSS3.p2.1.2" class="ltx_text" style="font-size:90%;">. To refine input feature representation, we drew inspiration from the CHiME-7 DASR Challenge </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS3.p2.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S1.SS1.SSS3.p2.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS1.SSS3.p2.1.5" class="ltx_text" style="font-size:90%;">, leveraging features extracted from self-supervised pre-trained models, particularly WavLM </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS3.p2.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S1.SS1.SSS3.p2.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS1.SSS3.p2.1.8" class="ltx_text" style="font-size:90%;">. Our experiments involved systematically integrating these WavLM-derived features at various stages within the Whisper encoder, including the initial, intermediate, and final layers. We observed that injecting these features at the intermediate layer of the encoder resulted in a slight yet noticeable improvement in performance. The outputs from WavLM and the intermediate layer of the Whisper encoder are integrated via a concatenation operation, followed by a linear transformation to ensure compatibility with the original feature dimensions of the model.
Concerning downsampling convolutions, the baseline Whisper model utilizes two layers of 1D convolution. Inspired by recent advancements like NextFormer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS3.p2.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S1.SS1.SSS3.p2.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS1.SSS3.p2.1.11" class="ltx_text" style="font-size:90%;">, we augmented the original Whisper model with a ConvNeXt structure, running in parallel to the standard 1D convolutions. The ConvNeXt output is added to the original Conv1D output after a linear transformation and then input into the transformer.</span></p>
</div>
<div id="S1.SS1.SSS3.p3" class="ltx_para">
<p id="S1.SS1.SSS3.p3.1" class="ltx_p"><span id="S1.SS1.SSS3.p3.1.1" class="ltx_text" style="font-size:90%;">Regarding positional encoding, Whisper initially relies on absolute positional encoding. However, empirical evidence suggests that absolute positional encoding exhibits limitations in robustness compared to relative positional encoding </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS3.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S1.SS1.SSS3.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS1.SSS3.p3.1.4" class="ltx_text" style="font-size:90%;">. Motivated by these findings, we adopted bias relative positional encoding </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS3.p3.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S1.SS1.SSS3.p3.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS1.SSS3.p3.1.7" class="ltx_text" style="font-size:90%;"> within our enhanced model, aiming to improve its resilience and performance consistency across varying input lengths.</span></p>
</div>
<div id="S1.SS1.SSS3.p4" class="ltx_para">
<p id="S1.SS1.SSS3.p4.1" class="ltx_p"><span id="S1.SS1.SSS3.p4.1.1" class="ltx_text" style="font-size:90%;">In terms of the Transformer block, we took cues from relevant research </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS3.p4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S1.SS1.SSS3.p4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS1.SSS3.p4.1.4" class="ltx_text" style="font-size:90%;"> to integrate a sigmoid gating mechanism. Specifically, the input is projected through a weight matrix (W), followed by a sigmoid activation function. The output of this operation is then scaled by a factor of 2 before being element-wise multiplied with the original output, effectively controlling the flow of information within the Transformer block.
Additionally, we explored the insertion of a depthwise convolution module, akin to those featured in Conformer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS3.p4.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S1.SS1.SSS3.p4.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS1.SSS3.p4.1.7" class="ltx_text" style="font-size:90%;"> models, following the Multi-Head Attention (MHA) layers. This architecture enhances the model’s ability for localized modeling.
Furthermore, we augmented the final layer of the encoder with a Mixture of Experts (MoE) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS1.SSS3.p4.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S1.SS1.SSS3.p4.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS1.SSS3.p4.1.10" class="ltx_text" style="font-size:90%;"> component, aimed at enhancing the model’s representational capacity.</span></p>
</div>
</section>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Single-channel System</h3>

<figure id="S1.F6" class="ltx_figure"><img src="/html/2409.02041/assets/x4.png" id="S1.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="191" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The framework of single-channel system.</figcaption>
</figure>
<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p"><span id="S1.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">The framework of the single-channel system is illustrated in Fig. </span><a href="#S1.F6" title="Figure 6 ‣ 1.2 Single-channel System ‣ 1 System Description ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="S1.SS2.p1.1.2" class="ltx_text" style="font-size:90%;">. Like the multi-channel system, the single-channel system also begins with speaker diarization followed by speech separation and ASR. However, unlike the multi-channel system, each module in the single-channel system (including the overlap detection, CSS, CSD, NSD, and T-F mask estimation) receives only single-channel audios or features as inputs. To get the separated audios for each speaker, the amplitude spectral features of the original mixed audio are multiplied by the T-F masks, and an inverse STFT transformation is performed. Furthermore, re-clustering the separated audios can enhance the precision of the speaker diarization priors, as illustrated at the bottom of the Fig. </span><a href="#S1.F6" title="Figure 6 ‣ 1.2 Single-channel System ‣ 1 System Description ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="S1.SS2.p1.1.3" class="ltx_text" style="font-size:90%;">. For ASR, we use the same model for decoding as in the multi-channel system.</span></p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Datasets</h3>

<section id="S1.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">1.3.1 </span>Diarization and Separation</h4>

<div id="S1.SS3.SSS1.p1" class="ltx_para">
<p id="S1.SS3.SSS1.p1.1" class="ltx_p"><span id="S1.SS3.SSS1.p1.1.1" class="ltx_text" style="font-size:90%;">For the speaker diarization system, the training data comprises the officially simulated training dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS3.SSS1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S1.SS3.SSS1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS3.SSS1.p1.1.4" class="ltx_text" style="font-size:90%;">, Train-set-1 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS3.SSS1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S1.SS3.SSS1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS3.SSS1.p1.1.7" class="ltx_text" style="font-size:90%;">, Train-set-2 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS3.SSS1.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S1.SS3.SSS1.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS3.SSS1.p1.1.10" class="ltx_text" style="font-size:90%;"> and Dev-set-1 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS3.SSS1.p1.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S1.SS3.SSS1.p1.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS3.SSS1.p1.1.13" class="ltx_text" style="font-size:90%;">. We also employed LibriSpeech, MUSAN noise </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS3.SSS1.p1.1.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S1.SS3.SSS1.p1.1.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS3.SSS1.p1.1.16" class="ltx_text" style="font-size:90%;"> and the noises in officially simulated training dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS3.SSS1.p1.1.17.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S1.SS3.SSS1.p1.1.18.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS3.SSS1.p1.1.19" class="ltx_text" style="font-size:90%;"> to simulate the diarization training data</span><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/jsalt2020-asrdiar/jsalt2020_simulate</span></span></span><span id="S1.SS3.SSS1.p1.1.20" class="ltx_text" style="font-size:90%;">. Additionally, we also use the near-field recordings from Train-set-1, Train-set-2 and Dev-set-1 as clean data to simulate multi-channel speaker diarization training data. For speech separation, we use the officially simulated training dataset and also use the near-field recordings from Train-set-1, Train-set-2 and Dev-set-1 to simulate the separation training data.</span></p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>The training sets of speech recognition.</figcaption>
<div id="S1.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:227.6pt;height:202.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.0pt,24.9pt) scale(0.802650644041375,0.802650644041375) ;">
<table id="S1.T1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.3.1.1" class="ltx_tr">
<td id="S1.T1.3.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S1.T1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Duration (h)</span></td>
<td id="S1.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S1.T1.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Corpus</span></td>
<td id="S1.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S1.T1.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Sample Scale</span></td>
</tr>
<tr id="S1.T1.3.1.2" class="ltx_tr">
<td id="S1.T1.3.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.3.1.2.1.1" class="ltx_text" style="font-size:90%;">14</span></td>
<td id="S1.T1.3.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.3.1.2.2.1" class="ltx_text" style="font-size:90%;">Train-set-1 MC GSS</span></td>
<td id="S1.T1.3.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.3.1.2.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
<tr id="S1.T1.3.1.3" class="ltx_tr">
<td id="S1.T1.3.1.3.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.3.1.1" class="ltx_text" style="font-size:90%;">16</span></td>
<td id="S1.T1.3.1.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.3.2.1" class="ltx_text" style="font-size:90%;">Train-set-2 MC GSS</span></td>
<td id="S1.T1.3.1.3.3" class="ltx_td ltx_align_center"><span id="S1.T1.3.1.3.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
<tr id="S1.T1.3.1.4" class="ltx_tr">
<td id="S1.T1.3.1.4.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.4.1.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S1.T1.3.1.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.4.2.1" class="ltx_text" style="font-size:90%;">Dev-set-1 MC GSS</span></td>
<td id="S1.T1.3.1.4.3" class="ltx_td ltx_align_center"><span id="S1.T1.3.1.4.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
<tr id="S1.T1.3.1.5" class="ltx_tr">
<td id="S1.T1.3.1.5.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.5.1.1" class="ltx_text" style="font-size:90%;">14</span></td>
<td id="S1.T1.3.1.5.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.5.2.1" class="ltx_text" style="font-size:90%;">Train-set-1 MC GSS with timestamp</span></td>
<td id="S1.T1.3.1.5.3" class="ltx_td ltx_align_center"><span id="S1.T1.3.1.5.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
<tr id="S1.T1.3.1.6" class="ltx_tr">
<td id="S1.T1.3.1.6.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.6.1.1" class="ltx_text" style="font-size:90%;">16</span></td>
<td id="S1.T1.3.1.6.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.6.2.1" class="ltx_text" style="font-size:90%;">Train-set-2 MC GSS with timestamp</span></td>
<td id="S1.T1.3.1.6.3" class="ltx_td ltx_align_center"><span id="S1.T1.3.1.6.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
<tr id="S1.T1.3.1.7" class="ltx_tr">
<td id="S1.T1.3.1.7.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.7.1.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S1.T1.3.1.7.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.7.2.1" class="ltx_text" style="font-size:90%;">Dev-set-1 MC GSS with timestamp</span></td>
<td id="S1.T1.3.1.7.3" class="ltx_td ltx_align_center"><span id="S1.T1.3.1.7.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
<tr id="S1.T1.3.1.8" class="ltx_tr">
<td id="S1.T1.3.1.8.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.8.1.1" class="ltx_text" style="font-size:90%;">14</span></td>
<td id="S1.T1.3.1.8.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.8.2.1" class="ltx_text" style="font-size:90%;">Train-set-1 MC NN</span></td>
<td id="S1.T1.3.1.8.3" class="ltx_td ltx_align_center"><span id="S1.T1.3.1.8.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
<tr id="S1.T1.3.1.9" class="ltx_tr">
<td id="S1.T1.3.1.9.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.9.1.1" class="ltx_text" style="font-size:90%;">16</span></td>
<td id="S1.T1.3.1.9.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.9.2.1" class="ltx_text" style="font-size:90%;">Train-set-2 MC NN</span></td>
<td id="S1.T1.3.1.9.3" class="ltx_td ltx_align_center"><span id="S1.T1.3.1.9.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
<tr id="S1.T1.3.1.10" class="ltx_tr">
<td id="S1.T1.3.1.10.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.10.1.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S1.T1.3.1.10.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.10.2.1" class="ltx_text" style="font-size:90%;">Dev-set-1 MC NN</span></td>
<td id="S1.T1.3.1.10.3" class="ltx_td ltx_align_center"><span id="S1.T1.3.1.10.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
<tr id="S1.T1.3.1.11" class="ltx_tr">
<td id="S1.T1.3.1.11.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.11.1.1" class="ltx_text" style="font-size:90%;">14</span></td>
<td id="S1.T1.3.1.11.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.11.2.1" class="ltx_text" style="font-size:90%;">Train-set-1 MC ch0 NN</span></td>
<td id="S1.T1.3.1.11.3" class="ltx_td ltx_align_center"><span id="S1.T1.3.1.11.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
<tr id="S1.T1.3.1.12" class="ltx_tr">
<td id="S1.T1.3.1.12.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.12.1.1" class="ltx_text" style="font-size:90%;">16</span></td>
<td id="S1.T1.3.1.12.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.12.2.1" class="ltx_text" style="font-size:90%;">Train-set-2 MC ch0 NN</span></td>
<td id="S1.T1.3.1.12.3" class="ltx_td ltx_align_center"><span id="S1.T1.3.1.12.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
<tr id="S1.T1.3.1.13" class="ltx_tr">
<td id="S1.T1.3.1.13.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.13.1.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="S1.T1.3.1.13.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.3.1.13.2.1" class="ltx_text" style="font-size:90%;">Dev-set-1 MC ch0 NN</span></td>
<td id="S1.T1.3.1.13.3" class="ltx_td ltx_align_center"><span id="S1.T1.3.1.13.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
<tr id="S1.T1.3.1.14" class="ltx_tr">
<td id="S1.T1.3.1.14.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S1.T1.3.1.14.1.1" class="ltx_text" style="font-size:90%;">960</span></td>
<td id="S1.T1.3.1.14.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S1.T1.3.1.14.2.1" class="ltx_text" style="font-size:90%;">LibriSpeech</span></td>
<td id="S1.T1.3.1.14.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.3.1.14.3.1" class="ltx_text" style="font-size:90%;">1</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S1.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">1.3.2 </span>Speech Recognition</h4>

<div id="S1.SS3.SSS2.p1" class="ltx_para">
<p id="S1.SS3.SSS2.p1.1" class="ltx_p"><span id="S1.SS3.SSS2.p1.1.1" class="ltx_text" style="font-size:90%;">The ASR systems were trained using official NOTSOFAR-1 training data and the open-source LibriSpeech dataset with data augmentation methods. The data augmentation methods included speed perturbation and MUSAN noise </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS3.SSS2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S1.SS3.SSS2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS3.SSS2.p1.1.4" class="ltx_text" style="font-size:90%;"> addition. The specific composition of the training data is shown in Table </span><a href="#S1.T1" title="Table 1 ‣ 1.3.1 Diarization and Separation ‣ 1.3 Datasets ‣ 1 System Description ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S1.SS3.SSS2.p1.1.5" class="ltx_text" style="font-size:90%;">. We utilized multi-channel (MC) data processed by both Guided Source Separation (GSS) and Neural Networks (NN), and introduced a word-level timestamp prediction task into the GSS data. We found that this multitask training approach led to a slight improvement in recognition accuracy. We also adopted the practice from Whisper of providing the transcribed text from the preceding utterance as previous-text conditioning, which has noticeably improved the recognition rate. Contrary to using official single-channel (SC) data, we selected NN-processed MC channel 0 (ch0) data as our single-channel training input, observing superior performance with this choice. Drawing inspiration from the principles of RDrop </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.SS3.SSS2.p1.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S1.SS3.SSS2.p1.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.SS3.SSS2.p1.1.8" class="ltx_text" style="font-size:90%;">, we developed a novel data augmentation technique called Noise KLD. This approach entails separately feeding both the original and augmented data samples into the model. Consistency between the model’s predictions for the original and augmented data is ensured by applying Kullback-Leibler divergence (KLD) loss as a regularizer. Through extensive experimentation, we discovered that this method outperforms conventional data augmentation strategies in terms of boosting model performance and generalization.</span></p>
</div>
</section>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Results</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="font-size:90%;">For diarization, the training requires approximately 88 hours, and testing all sentences in Dev-set-2 takes about 1 hour. For the JDS system, training takes approximately 4 days, while testing all sentences in Dev-set-2 requires about 1 hour. For ASR, training requires about 20 hours, and testing all sentences in Dev-set-2 consumes about 6 hours. Typically, we conduct our training on A100 GPUs and perform testing on V100 or A40 GPUs.</span></p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Multi-channel System</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.5" class="ltx_p"><span id="S2.SS1.p1.5.1" class="ltx_text" style="font-size:90%;">Table </span><a href="#S2.T2" title="Table 2 ‣ 2.1 Multi-channel System ‣ 2 Results ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S2.SS1.p1.5.2" class="ltx_text" style="font-size:90%;"> presents the tcpWER (</span><math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mo mathsize="90%" id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><csymbol cd="latexml" id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\%</annotation></semantics></math><span id="S2.SS1.p1.5.3" class="ltx_text" style="font-size:90%;">) of our multi-channel system on Dev-set-2, where ‘Sys-</span><math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi mathsize="90%" id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">N</annotation></semantics></math><span id="S2.SS1.p1.5.4" class="ltx_text" style="font-size:90%;"> RTTM’ corresponds to the system depicted in Fig. </span><a href="#S1.F2" title="Figure 2 ‣ 1.1.1 Diarization ‣ 1.1 Multi-channel System ‣ 1 System Description ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S2.SS1.p1.5.5" class="ltx_text" style="font-size:90%;">, and ‘V</span><math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mo mathsize="90%" id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><times id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">*</annotation></semantics></math><span id="S2.SS1.p1.5.6" class="ltx_text" style="font-size:90%;">’ corresponds to the system shown in Fig. </span><a href="#S1.F3" title="Figure 3 ‣ 1.1.2 Separation ‣ 1.1 Multi-channel System ‣ 1 System Description ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S2.SS1.p1.5.7" class="ltx_text" style="font-size:90%;">. For each system, we fused the posterior probabilities from three different Whisper models (enhanced large-v2, enhanced large-v3, and enhanced large-v3 trained with more data simulated from Librispeech). The last row ‘Fusion’ indicates the average of posterior probabilities across 9 (3 </span><math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mo mathsize="90%" id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><times id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\times</annotation></semantics></math><span id="S2.SS1.p1.5.8" class="ltx_text" style="font-size:90%;"> 3) systems using the same speaker diarization priors. Finally, in the multi-channel track, we submit the fusion results of each ‘Sys-</span><math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mi mathsize="90%" id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">N</annotation></semantics></math><span id="S2.SS1.p1.5.9" class="ltx_text" style="font-size:90%;"> RTTM’ (last column).</span></p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>TcpWER (<math id="S2.T2.2.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S2.T2.2.m1.1b"><mo id="S2.T2.2.m1.1.1" xref="S2.T2.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S2.T2.2.m1.1c"><csymbol cd="latexml" id="S2.T2.2.m1.1.1.cmml" xref="S2.T2.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.m1.1d">\%</annotation></semantics></math>) comparisons on the multi-channel track on Dev-set-2. </figcaption>
<div id="S2.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:227.6pt;height:71.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.0pt,10.0pt) scale(0.780335209441167,0.780335209441167) ;">
<table id="S2.T2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.3.1.1" class="ltx_tr">
<td id="S2.T2.3.1.1.1" class="ltx_td ltx_nopad ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.15pt;padding-bottom:1.15pt;"><svg version="1.1" height="19.44" width="38.74" overflow="visible"><g transform="translate(0,19.44) scale(1,-1)"><path d="M 0,19.44 38.74,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,10.93) scale(1, -1)"><foreignObject width="19.37" height="10.93" overflow="visible">
<span id="S2.T2.3.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S2.T2.3.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S2.T2.3.1.1.1.pic1.1.1.1.1" class="ltx_p"><span id="S2.T2.3.1.1.1.pic1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Sep</span></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(19.54,10.93)"><g transform="translate(0,8.51) scale(1, -1)"><foreignObject width="19.2" height="8.51" overflow="visible">
<span id="S2.T2.3.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S2.T2.3.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S2.T2.3.1.1.1.pic1.2.1.1.1" class="ltx_p"><span id="S2.T2.3.1.1.1.pic1.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Dia</span></span>
</span>
</span></foreignObject></g></g></g></svg></td>
<td id="S2.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.1.2.1" class="ltx_text" style="font-size:90%;">Sys-1 RTTM</span></td>
<td id="S2.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.1.3.1" class="ltx_text" style="font-size:90%;">Sys-2 RTTM</span></td>
<td id="S2.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.1.4.1" class="ltx_text" style="font-size:90%;">Sys-3 RTTM</span></td>
<td id="S2.T2.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.1.5.1" class="ltx_text" style="font-size:90%;">Sys-4 RTTM</span></td>
</tr>
<tr id="S2.T2.3.1.2" class="ltx_tr">
<td id="S2.T2.3.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.2.1.1" class="ltx_text" style="font-size:90%;">V1</span></td>
<td id="S2.T2.3.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.2.2.1" class="ltx_text" style="font-size:90%;">14.953</span></td>
<td id="S2.T2.3.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.2.3.1" class="ltx_text" style="font-size:90%;">14.649</span></td>
<td id="S2.T2.3.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.2.4.1" class="ltx_text" style="font-size:90%;">15.116</span></td>
<td id="S2.T2.3.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.2.5.1" class="ltx_text" style="font-size:90%;">14.571</span></td>
</tr>
<tr id="S2.T2.3.1.3" class="ltx_tr">
<td id="S2.T2.3.1.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.3.1.1" class="ltx_text" style="font-size:90%;">V2</span></td>
<td id="S2.T2.3.1.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.3.2.1" class="ltx_text" style="font-size:90%;">14.911</span></td>
<td id="S2.T2.3.1.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.3.3.1" class="ltx_text" style="font-size:90%;">14.595</span></td>
<td id="S2.T2.3.1.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.3.4.1" class="ltx_text" style="font-size:90%;">15.086</span></td>
<td id="S2.T2.3.1.3.5" class="ltx_td ltx_align_center" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.3.5.1" class="ltx_text" style="font-size:90%;">14.547</span></td>
</tr>
<tr id="S2.T2.3.1.4" class="ltx_tr">
<td id="S2.T2.3.1.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.4.1.1" class="ltx_text" style="font-size:90%;">V3</span></td>
<td id="S2.T2.3.1.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.4.2.1" class="ltx_text" style="font-size:90%;">15.577</span></td>
<td id="S2.T2.3.1.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.4.3.1" class="ltx_text" style="font-size:90%;">15.160</span></td>
<td id="S2.T2.3.1.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.4.4.1" class="ltx_text" style="font-size:90%;">15.703</span></td>
<td id="S2.T2.3.1.4.5" class="ltx_td ltx_align_center" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.4.5.1" class="ltx_text" style="font-size:90%;">15.018</span></td>
</tr>
<tr id="S2.T2.3.1.5" class="ltx_tr">
<td id="S2.T2.3.1.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.5.1.1" class="ltx_text" style="font-size:90%;">Fusion</span></td>
<td id="S2.T2.3.1.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.5.2.1" class="ltx_text" style="font-size:90%;">14.681</span></td>
<td id="S2.T2.3.1.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.5.3.1" class="ltx_text" style="font-size:90%;">14.286</span></td>
<td id="S2.T2.3.1.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.5.4.1" class="ltx_text" style="font-size:90%;">14.847</span></td>
<td id="S2.T2.3.1.5.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T2.3.1.5.5.1" class="ltx_text" style="font-size:90%;">14.265</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Single-channel System</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p"><span id="S2.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">Table </span><a href="#S2.T3" title="Table 3 ‣ 2.2 Single-channel System ‣ 2 Results ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S2.SS2.p1.1.2" class="ltx_text" style="font-size:90%;"> presents the tcpWER (</span><math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mo mathsize="90%" id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\%</annotation></semantics></math><span id="S2.SS2.p1.1.3" class="ltx_text" style="font-size:90%;">) of our single-channel system on Dev-set-2. The diarization priors are derived from NSD and re-clustering, as illustrated in Fig. </span><a href="#S1.F6" title="Figure 6 ‣ 1.2 Single-channel System ‣ 1 System Description ‣ The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="S2.SS2.p1.1.4" class="ltx_text" style="font-size:90%;">. These priors are then input into the JDS system, from which separated audio is obtained via multiplying T-F masks and amplitude spectrum. Similarly, for each subsystem, we have fused posterior probabilities from three different Whisper models (enhanced large-v2, enhanced large-v3, and enhanced large-v3 trained with more data simulated from Librispeech).</span></p>
</div>
<figure id="S2.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>TcpWER (<math id="S2.T3.2.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S2.T3.2.m1.1b"><mo id="S2.T3.2.m1.1.1" xref="S2.T3.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S2.T3.2.m1.1c"><csymbol cd="latexml" id="S2.T3.2.m1.1.1.cmml" xref="S2.T3.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T3.2.m1.1d">\%</annotation></semantics></math>) comparisons on the single-channel track on Dev-set-2. </figcaption>
<div id="S2.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:142.3pt;height:37.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.8pt,-0.2pt) scale(1.01113206264174,1.01113206264174) ;">
<table id="S2.T3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T3.3.1.1" class="ltx_tr">
<td id="S2.T3.3.1.1.1" class="ltx_td ltx_nopad ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.15pt;padding-bottom:1.15pt;"><svg version="1.1" height="19.44" width="38.74" overflow="visible"><g transform="translate(0,19.44) scale(1,-1)"><path d="M 0,19.44 38.74,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,10.93) scale(1, -1)"><foreignObject width="19.37" height="10.93" overflow="visible">
<span id="S2.T3.3.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S2.T3.3.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S2.T3.3.1.1.1.pic1.1.1.1.1" class="ltx_p"><span id="S2.T3.3.1.1.1.pic1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Sep</span></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(19.54,10.93)"><g transform="translate(0,8.51) scale(1, -1)"><foreignObject width="19.2" height="8.51" overflow="visible">
<span id="S2.T3.3.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S2.T3.3.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S2.T3.3.1.1.1.pic1.2.1.1.1" class="ltx_p"><span id="S2.T3.3.1.1.1.pic1.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Dia</span></span>
</span>
</span></foreignObject></g></g></g></svg></td>
<td id="S2.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T3.3.1.1.2.1" class="ltx_text" style="font-size:90%;">NSD</span></td>
<td id="S2.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T3.3.1.1.3.1" class="ltx_text" style="font-size:90%;">Re-clustering</span></td>
</tr>
<tr id="S2.T3.3.1.2" class="ltx_tr">
<td id="S2.T3.3.1.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T3.3.1.2.1.1" class="ltx_text" style="font-size:90%;">JDS</span></td>
<td id="S2.T3.3.1.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T3.3.1.2.2.1" class="ltx_text" style="font-size:90%;">24.611</span></td>
<td id="S2.T3.3.1.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:1.15pt;padding-bottom:1.15pt;"><span id="S2.T3.3.1.2.3.1" class="ltx_text" style="font-size:90%;">22.989</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
T. Higuchi, N. Ito, S. Araki, T. Yoshioka, M. Delcroix, and T. Nakatani,
“Online MVDR beamformer based on complex Gaussian mixture model with
spatial prior for noise robust ASR,” </span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE/ACM Transactions on Audio,
Speech, and Language Processing</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:90%;">, vol. 25, no. 4, pp. 780–793, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, “Voxceleb: Large-scale
speaker verification in the wild,” </span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Speech &amp; Language</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:90%;">,
vol. 60, p. 101027, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
M.-K. He, J. Du, Q.-F. Liu, and C.-H. Lee, “ANSD-MA-MSE: Adaptive Neural
Speaker Diarization Using Memory-Aware Multi-Speaker Embedding,”
</span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:90%;">,
vol. 31, pp. 1561–1573, 2023.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
G. Yang, M. He, S. Niu, R. Wang, Y. Yue, S. Qian, S. Wu, J. Du, and C.-H. Lee,
“Neural speaker diarization using memory-aware multi-speaker embedding with
sequence-to-sequence architecture,” in </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2024-2024 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2024, pp.
11 626–11 630.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
R. Wang, M. He, J. Du, H. Zhou, S. Niu, H. Chen, Y. Yue, G. Yang, S. Wu,
L. Sun, Y. Tu, H. Tang, S. Qian, T. Gao, M. Wang, G. Wan, J. Pan, J. Gao, and
C.-H. Lee, “The ustc-nercslip systems for the chime-7 dasr challenge,”
</span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:90%;">, vol. abs/2308.14638, 2023. [Online]. Available:
</span><a target="_blank" href="https://api.semanticscholar.org/CorpusID:261244449" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://api.semanticscholar.org/CorpusID:261244449</a><span id="bib.bib5.4.4" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever,
“Robust speech recognition via large-scale weak supervision,” </span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:90%;">,
vol. abs/2212.04356, 2022. [Online]. Available:
</span><a target="_blank" href="https://api.semanticscholar.org/CorpusID:252923993" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://api.semanticscholar.org/CorpusID:252923993</a><span id="bib.bib6.4.4" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,
T. Yoshioka, X. Xiao, J. Wu, L. Zhou, S. Ren, Y. Qian, Y. Qian, M. Zeng, and
F. Wei, “Wavlm: Large-scale self-supervised pre-training for full stack
speech processing,” </span><em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Journal of Selected Topics in Signal
Processing</em><span id="bib.bib7.3.3" class="ltx_text" style="font-size:90%;">, vol. 16, pp. 1505–1518, 2021. [Online]. Available:
</span><a target="_blank" href="https://api.semanticscholar.org/CorpusID:239885872" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://api.semanticscholar.org/CorpusID:239885872</a><span id="bib.bib7.4.4" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Y. Jiang, J. Yu, W. Yang, B. Zhang, and Y. Wang, “Nextformer: A convnext
augmented conformer for end-to-end speech recognition,” 2022. [Online].
Available: </span><a target="_blank" href="https://api.semanticscholar.org/CorpusID:250113612" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://api.semanticscholar.org/CorpusID:250113612</a><span id="bib.bib8.2.2" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
P. Zhou, R. Fan, W. Chen, and J. Jia, “Improving generalization of transformer
for speech recognition with parallel schedule sampling and relative
positional embedding,” </span><em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib9.3.3" class="ltx_text" style="font-size:90%;">, vol. abs/1911.00203, 2019. [Online].
Available: </span><a target="_blank" href="https://api.semanticscholar.org/CorpusID:207870654" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://api.semanticscholar.org/CorpusID:207870654</a><span id="bib.bib9.4.4" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Z. Dai, H. Liu, Q. V. Le, and M. Tan, “Coatnet: Marrying convolution and
attention for all data sizes,” </span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:90%;">, vol. abs/2106.04803, 2021.
[Online]. Available: </span><a target="_blank" href="https://api.semanticscholar.org/CorpusID:235376986" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://api.semanticscholar.org/CorpusID:235376986</a><span id="bib.bib10.4.4" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei,
“Retentive network: A successor to transformer for large language models,”
</span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:90%;">, vol. abs/2307.08621, 2023. [Online]. Available:
</span><a target="_blank" href="https://api.semanticscholar.org/CorpusID:259937453" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://api.semanticscholar.org/CorpusID:259937453</a><span id="bib.bib11.4.4" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
B. Peng, E. Alcaide, Q. G. Anthony, A. Albalak, S. Arcadinho, S. Biderman,
H. Cao, X. Cheng, M. Chung, M. Grella, G. Kranthikiran, X. He, H. Hou,
P. Kazienko, J. Kocoń, J. Kong, B. Koptyra, H. Lau, K. S. I. Mantri, F. Mom,
A. Saito, X. Tang, B. Wang, J. S. Wind, S. Wozniak, R. Zhang, Z. Zhang,
Q. Zhao, P. Zhou, J. Zhu, and R. Zhu, “Rwkv: Reinventing rnns for the
transformer era,” in </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Empirical Methods in Natural
Language Processing</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:90%;">, 2023. [Online]. Available:
</span><a target="_blank" href="https://api.semanticscholar.org/CorpusID:258832459" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://api.semanticscholar.org/CorpusID:258832459</a><span id="bib.bib12.4.4" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang,
Z. Zhang, Y. Wu, and R. Pang, “Conformer: Convolution-augmented transformer
for speech recognition,” </span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:90%;">, vol. abs/2005.08100, 2020. [Online].
Available: </span><a target="_blank" href="https://api.semanticscholar.org/CorpusID:218674528" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://api.semanticscholar.org/CorpusID:218674528</a><span id="bib.bib13.4.4" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Z. You, S. Feng, D. Su, and D. Yu, “Speechmoe: Scaling to large acoustic
models with dynamic routing mixture of experts,” in </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Interspeech</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:90%;">,
2021. [Online]. Available:
</span><a target="_blank" href="https://api.semanticscholar.org/CorpusID:234094030" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://api.semanticscholar.org/CorpusID:234094030</a><span id="bib.bib14.4.4" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
“CHiME-8 Simulated Training Dataset,”
</span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;"><a target="_blank" href="https://www.chimechallenge.org/current/task2/datasimulated-training-dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.chimechallenge.org/current/task2/datasimulated-training-dataset</a></em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:90%;">,
2024.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
“CHiME-8 Meetings Recordings Dataset,”
</span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;"><a target="_blank" href="https://www.chimechallenge.org/current/task2/datasimulated-training-dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.chimechallenge.org/current/task2/datasimulated-training-dataset</a></em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:90%;">,
2024.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
D. Snyder, G. Chen, and D. Povey, “MUSAN: A Music, Speech, and Noise
Corpus,” 2015, arXiv:1510.08484v1.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
W. Ji, S. Zan, G. Zhou, and X. Wang, “Research on an improved conformer
end-to-end speech recognition model with r-drop structure,” </span><em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib18.3.3" class="ltx_text" style="font-size:90%;">,
vol. abs/2306.08329, 2023. [Online]. Available:
</span><a target="_blank" href="https://api.semanticscholar.org/CorpusID:259165369" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://api.semanticscholar.org/CorpusID:259165369</a><span id="bib.bib18.4.4" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.02040" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.02041" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.02041">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.02041" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.02042" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 21:38:02 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
