<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.16000] CARNATIC RAGA IDENTIFICATION SYSTEM USING RIGOROUS TIME-DELAY NEURAL NETWORK</title><meta property="og:description" content="Large scale machine learning-based Raga identification continues to be a nontrivial issue in the computational aspects behind Carnatic music. Each raga consists of many unique and intrinsic melodic patterns that can be…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CARNATIC RAGA IDENTIFICATION SYSTEM USING RIGOROUS TIME-DELAY NEURAL NETWORK">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CARNATIC RAGA IDENTIFICATION SYSTEM USING RIGOROUS TIME-DELAY NEURAL NETWORK">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.16000">

<!--Generated on Wed Jun  5 18:13:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document" style="font-size:90%;">CARNATIC RAGA IDENTIFICATION SYSTEM USING RIGOROUS TIME-DELAY NEURAL NETWORK</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text" style="font-size:90%;">Large scale machine learning-based Raga identification continues to be a nontrivial issue in the computational aspects behind Carnatic music. Each raga consists of many unique and intrinsic melodic patterns that can be used to easily identify them from others. These ragas can also then be used to cluster songs within the same raga, as well as identify songs in other closely-related ragas. In this case, the input sound is analyzed using a combination of steps including using a Discrete Fourier Transformation and using Triangular Filtering to create custom bins of possible notes, extracting features from the presence of particular notes – or lack thereof.</span></p>
<p id="id2.id2" class="ltx_p"><span id="id2.id2.1" class="ltx_text" style="font-size:90%;">Using a combination of Neural Networks including 1D Convolutional Neural Networks (conventionally known as Time-Delay Neural Networks) and Long Short-term Memory (LSTM), which are a form of Recurrent Neural Networks, the backbone of the classification strategy to build the model can be created. In addition, to help with variations in shruti, a long-time attention-based mechanism will be implemented to determine the relative changes in frequency rather than the absolute differences. This will provide a much more meaningful data point when training audio clips in different shrutis. To evaluate the accuracy of the classifier, a dataset of 676 recordings is used. The songs are distributed across the list of ragas. The goal of this program is to be able to effectively and efficiently label a much wider range of audio clips in more shrutis, ragas, and with more background noise.</span></p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span><span id="p1.1.2" class="ltx_text" style="font-size:90%;">
Triangular Filtering, TDNN, LSTM, Attention Based Mechanism, Carnatic Music, Raga Recognition, Music Information Retrieval</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text" style="font-size:90%;">Carnatic music is a classical music tradition originating from South India, with roots dating back over 2,000 years. It is characterized by intricate melodies, rhythmic patterns, and improvisation, often accompanied by instruments like the veena, flute, violin, mridangam, and tanpura. Carnatic music is usually classified into three subparts: </span><span id="S1.p1.1.2" class="ltx_text ltx_font_italic" style="font-size:90%;">shruti</span><span id="S1.p1.1.3" class="ltx_text" style="font-size:90%;">, </span><span id="S1.p1.1.4" class="ltx_text ltx_font_italic" style="font-size:90%;">laya</span><span id="S1.p1.1.5" class="ltx_text" style="font-size:90%;">, and </span><span id="S1.p1.1.6" class="ltx_text ltx_font_italic" style="font-size:90%;">raga</span><span id="S1.p1.1.7" class="ltx_text" style="font-size:90%;">. Shruti refers to the scale of the piece, and in the modern era, the 12 shrutis used per octave are congruent to the 12 notes used in the Western scales </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S1.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.10" class="ltx_text" style="font-size:90%;">. However, traditionally an octave can instead be split into 22 shrutis, allowing for finer gradation of the pitch </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S1.p1.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.13" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="font-size:90%;">Laya refers to the repetitive rhythmic timing of a piece. In Carnatic music, laya is established using talam, which is the cycle of equidistant beats for a given song. One measure of talam, referred to as an avarthanam, represents the division of beats into clear patterns. The use of talam helps to indicate the pace of the song </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S1.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.4" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.12" class="ltx_p"><span id="S1.p3.12.1" class="ltx_text" style="font-size:90%;">The final pillar of Carnatic music is the raga, a more complex version of the western melody. These ragas are built on a framework of seven primary notes (S, R, G, M, P, D, N), which correspond roughly to the Western Do, Re, Mi, Fa, Sol, La, Ti. These seven syllables, called swaras, can be split further into 16 notes separated by one half step, displayed in Table </span><a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ CARNATIC RAGA IDENTIFICATION SYSTEM USING RIGOROUS TIME-DELAY NEURAL NETWORK" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S1.p3.12.2" class="ltx_text" style="font-size:90%;"> with their frequencies in C shruti. The table also shows the ratio of each note to the standard S note. This ratio stays constant regardless of the scale of the song. Note that in Carnatic music, certain pitches will overlap with each other. For example, R</span><sub id="S1.p3.12.3" class="ltx_sub"><span id="S1.p3.12.3.1" class="ltx_text" style="font-size:90%;">2</span></sub><span id="S1.p3.12.4" class="ltx_text" style="font-size:90%;"> </span><math id="S1.p3.2.m2.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S1.p3.2.m2.1a"><mo mathsize="90%" id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><eq id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">=</annotation></semantics></math><span id="S1.p3.12.5" class="ltx_text" style="font-size:90%;"> G</span><sub id="S1.p3.12.6" class="ltx_sub"><span id="S1.p3.12.6.1" class="ltx_text" style="font-size:90%;">1</span></sub><span id="S1.p3.12.7" class="ltx_text" style="font-size:90%;">, R</span><sub id="S1.p3.12.8" class="ltx_sub"><span id="S1.p3.12.8.1" class="ltx_text" style="font-size:90%;">3</span></sub><span id="S1.p3.12.9" class="ltx_text" style="font-size:90%;"> </span><math id="S1.p3.5.m5.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S1.p3.5.m5.1a"><mo mathsize="90%" id="S1.p3.5.m5.1.1" xref="S1.p3.5.m5.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S1.p3.5.m5.1b"><eq id="S1.p3.5.m5.1.1.cmml" xref="S1.p3.5.m5.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.5.m5.1c">=</annotation></semantics></math><span id="S1.p3.12.10" class="ltx_text" style="font-size:90%;"> G</span><sub id="S1.p3.12.11" class="ltx_sub"><span id="S1.p3.12.11.1" class="ltx_text" style="font-size:90%;">2</span></sub><span id="S1.p3.12.12" class="ltx_text" style="font-size:90%;">, D</span><sub id="S1.p3.12.13" class="ltx_sub"><span id="S1.p3.12.13.1" class="ltx_text" style="font-size:90%;">2</span></sub><span id="S1.p3.12.14" class="ltx_text" style="font-size:90%;"> </span><math id="S1.p3.8.m8.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S1.p3.8.m8.1a"><mo mathsize="90%" id="S1.p3.8.m8.1.1" xref="S1.p3.8.m8.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S1.p3.8.m8.1b"><eq id="S1.p3.8.m8.1.1.cmml" xref="S1.p3.8.m8.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.8.m8.1c">=</annotation></semantics></math><span id="S1.p3.12.15" class="ltx_text" style="font-size:90%;"> N</span><sub id="S1.p3.12.16" class="ltx_sub"><span id="S1.p3.12.16.1" class="ltx_text" style="font-size:90%;">1</span></sub><span id="S1.p3.12.17" class="ltx_text" style="font-size:90%;">, and D</span><sub id="S1.p3.12.18" class="ltx_sub"><span id="S1.p3.12.18.1" class="ltx_text" style="font-size:90%;">3</span></sub><span id="S1.p3.12.19" class="ltx_text" style="font-size:90%;"> </span><math id="S1.p3.11.m11.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S1.p3.11.m11.1a"><mo mathsize="90%" id="S1.p3.11.m11.1.1" xref="S1.p3.11.m11.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S1.p3.11.m11.1b"><eq id="S1.p3.11.m11.1.1.cmml" xref="S1.p3.11.m11.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.11.m11.1c">=</annotation></semantics></math><span id="S1.p3.12.20" class="ltx_text" style="font-size:90%;"> N</span><sub id="S1.p3.12.21" class="ltx_sub"><span id="S1.p3.12.21.1" class="ltx_text" style="font-size:90%;">2</span></sub><span id="S1.p3.12.22" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.39" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.1.1.2.1" class="ltx_text" style="font-size:90%;">Note</span></th>
<th id="S1.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S1.T1.1.1.1.1" class="ltx_text" style="font-size:90%;">Frequency (</span><math id="S1.T1.1.1.1.m1.1" class="ltx_Math" alttext="Hz" display="inline"><semantics id="S1.T1.1.1.1.m1.1a"><mrow id="S1.T1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.m1.1.1.cmml"><mi mathsize="90%" id="S1.T1.1.1.1.m1.1.1.2" xref="S1.T1.1.1.1.m1.1.1.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S1.T1.1.1.1.m1.1.1.1" xref="S1.T1.1.1.1.m1.1.1.1.cmml">​</mo><mi mathsize="90%" id="S1.T1.1.1.1.m1.1.1.3" xref="S1.T1.1.1.1.m1.1.1.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.m1.1b"><apply id="S1.T1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.m1.1.1"><times id="S1.T1.1.1.1.m1.1.1.1.cmml" xref="S1.T1.1.1.1.m1.1.1.1"></times><ci id="S1.T1.1.1.1.m1.1.1.2.cmml" xref="S1.T1.1.1.1.m1.1.1.2">𝐻</ci><ci id="S1.T1.1.1.1.m1.1.1.3.cmml" xref="S1.T1.1.1.1.m1.1.1.3">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.m1.1c">Hz</annotation></semantics></math><span id="S1.T1.1.1.1.2" class="ltx_text" style="font-size:90%;">)</span>
</th>
<th id="S1.T1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S1.T1.1.1.3.1" class="ltx_text" style="font-size:90%;">Ratio to S</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.3.3" class="ltx_tr">
<th id="S1.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.3.3.3.1" class="ltx_text" style="font-size:90%;">S</span></th>
<td id="S1.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.2.2.1.m1.1" class="ltx_Math" alttext="130.81" display="inline"><semantics id="S1.T1.2.2.1.m1.1a"><mn mathsize="90%" id="S1.T1.2.2.1.m1.1.1" xref="S1.T1.2.2.1.m1.1.1.cmml">130.81</mn><annotation-xml encoding="MathML-Content" id="S1.T1.2.2.1.m1.1b"><cn type="float" id="S1.T1.2.2.1.m1.1.1.cmml" xref="S1.T1.2.2.1.m1.1.1">130.81</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.2.2.1.m1.1c">130.81</annotation></semantics></math></td>
<td id="S1.T1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.3.3.2.m1.1" class="ltx_Math" alttext="1.0000" display="inline"><semantics id="S1.T1.3.3.2.m1.1a"><mn mathsize="90%" id="S1.T1.3.3.2.m1.1.1" xref="S1.T1.3.3.2.m1.1.1.cmml">1.0000</mn><annotation-xml encoding="MathML-Content" id="S1.T1.3.3.2.m1.1b"><cn type="float" id="S1.T1.3.3.2.m1.1.1.cmml" xref="S1.T1.3.3.2.m1.1.1">1.0000</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.3.3.2.m1.1c">1.0000</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.6.6" class="ltx_tr">
<th id="S1.T1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.4.4.1.1" class="ltx_text" style="font-size:90%;">R</span><sub id="S1.T1.4.4.1.2" class="ltx_sub"><span id="S1.T1.4.4.1.2.1" class="ltx_text" style="font-size:90%;">1</span></sub>
</th>
<td id="S1.T1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.5.5.2.m1.1" class="ltx_Math" alttext="138.59" display="inline"><semantics id="S1.T1.5.5.2.m1.1a"><mn mathsize="90%" id="S1.T1.5.5.2.m1.1.1" xref="S1.T1.5.5.2.m1.1.1.cmml">138.59</mn><annotation-xml encoding="MathML-Content" id="S1.T1.5.5.2.m1.1b"><cn type="float" id="S1.T1.5.5.2.m1.1.1.cmml" xref="S1.T1.5.5.2.m1.1.1">138.59</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.5.5.2.m1.1c">138.59</annotation></semantics></math></td>
<td id="S1.T1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.6.6.3.m1.1" class="ltx_Math" alttext="1.0595" display="inline"><semantics id="S1.T1.6.6.3.m1.1a"><mn mathsize="90%" id="S1.T1.6.6.3.m1.1.1" xref="S1.T1.6.6.3.m1.1.1.cmml">1.0595</mn><annotation-xml encoding="MathML-Content" id="S1.T1.6.6.3.m1.1b"><cn type="float" id="S1.T1.6.6.3.m1.1.1.cmml" xref="S1.T1.6.6.3.m1.1.1">1.0595</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.6.6.3.m1.1c">1.0595</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.10.10" class="ltx_tr">
<th id="S1.T1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.8.8.2.1" class="ltx_text" style="font-size:90%;">R</span><sub id="S1.T1.8.8.2.2" class="ltx_sub"><span id="S1.T1.8.8.2.2.1" class="ltx_text" style="font-size:90%;">2</span></sub><span id="S1.T1.8.8.2.3" class="ltx_text" style="font-size:90%;">, G</span><sub id="S1.T1.8.8.2.4" class="ltx_sub"><span id="S1.T1.8.8.2.4.1" class="ltx_text" style="font-size:90%;">1</span></sub>
</th>
<td id="S1.T1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.9.9.3.m1.1" class="ltx_Math" alttext="146.83" display="inline"><semantics id="S1.T1.9.9.3.m1.1a"><mn mathsize="90%" id="S1.T1.9.9.3.m1.1.1" xref="S1.T1.9.9.3.m1.1.1.cmml">146.83</mn><annotation-xml encoding="MathML-Content" id="S1.T1.9.9.3.m1.1b"><cn type="float" id="S1.T1.9.9.3.m1.1.1.cmml" xref="S1.T1.9.9.3.m1.1.1">146.83</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.9.9.3.m1.1c">146.83</annotation></semantics></math></td>
<td id="S1.T1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.10.10.4.m1.1" class="ltx_Math" alttext="1.1225" display="inline"><semantics id="S1.T1.10.10.4.m1.1a"><mn mathsize="90%" id="S1.T1.10.10.4.m1.1.1" xref="S1.T1.10.10.4.m1.1.1.cmml">1.1225</mn><annotation-xml encoding="MathML-Content" id="S1.T1.10.10.4.m1.1b"><cn type="float" id="S1.T1.10.10.4.m1.1.1.cmml" xref="S1.T1.10.10.4.m1.1.1">1.1225</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.10.10.4.m1.1c">1.1225</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.14.14" class="ltx_tr">
<th id="S1.T1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.12.12.2.1" class="ltx_text" style="font-size:90%;">R</span><sub id="S1.T1.12.12.2.2" class="ltx_sub"><span id="S1.T1.12.12.2.2.1" class="ltx_text" style="font-size:90%;">3</span></sub><span id="S1.T1.12.12.2.3" class="ltx_text" style="font-size:90%;">, G</span><sub id="S1.T1.12.12.2.4" class="ltx_sub"><span id="S1.T1.12.12.2.4.1" class="ltx_text" style="font-size:90%;">2</span></sub>
</th>
<td id="S1.T1.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.13.13.3.m1.1" class="ltx_Math" alttext="155.56" display="inline"><semantics id="S1.T1.13.13.3.m1.1a"><mn mathsize="90%" id="S1.T1.13.13.3.m1.1.1" xref="S1.T1.13.13.3.m1.1.1.cmml">155.56</mn><annotation-xml encoding="MathML-Content" id="S1.T1.13.13.3.m1.1b"><cn type="float" id="S1.T1.13.13.3.m1.1.1.cmml" xref="S1.T1.13.13.3.m1.1.1">155.56</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.13.13.3.m1.1c">155.56</annotation></semantics></math></td>
<td id="S1.T1.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.14.14.4.m1.1" class="ltx_Math" alttext="1.1892" display="inline"><semantics id="S1.T1.14.14.4.m1.1a"><mn mathsize="90%" id="S1.T1.14.14.4.m1.1.1" xref="S1.T1.14.14.4.m1.1.1.cmml">1.1892</mn><annotation-xml encoding="MathML-Content" id="S1.T1.14.14.4.m1.1b"><cn type="float" id="S1.T1.14.14.4.m1.1.1.cmml" xref="S1.T1.14.14.4.m1.1.1">1.1892</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.14.14.4.m1.1c">1.1892</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.17.17" class="ltx_tr">
<th id="S1.T1.15.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.15.15.1.1" class="ltx_text" style="font-size:90%;">G</span><sub id="S1.T1.15.15.1.2" class="ltx_sub"><span id="S1.T1.15.15.1.2.1" class="ltx_text" style="font-size:90%;">3</span></sub>
</th>
<td id="S1.T1.16.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.16.16.2.m1.1" class="ltx_Math" alttext="164.81" display="inline"><semantics id="S1.T1.16.16.2.m1.1a"><mn mathsize="90%" id="S1.T1.16.16.2.m1.1.1" xref="S1.T1.16.16.2.m1.1.1.cmml">164.81</mn><annotation-xml encoding="MathML-Content" id="S1.T1.16.16.2.m1.1b"><cn type="float" id="S1.T1.16.16.2.m1.1.1.cmml" xref="S1.T1.16.16.2.m1.1.1">164.81</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.16.16.2.m1.1c">164.81</annotation></semantics></math></td>
<td id="S1.T1.17.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.17.17.3.m1.1" class="ltx_Math" alttext="1.2599" display="inline"><semantics id="S1.T1.17.17.3.m1.1a"><mn mathsize="90%" id="S1.T1.17.17.3.m1.1.1" xref="S1.T1.17.17.3.m1.1.1.cmml">1.2599</mn><annotation-xml encoding="MathML-Content" id="S1.T1.17.17.3.m1.1b"><cn type="float" id="S1.T1.17.17.3.m1.1.1.cmml" xref="S1.T1.17.17.3.m1.1.1">1.2599</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.17.17.3.m1.1c">1.2599</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.20.20" class="ltx_tr">
<th id="S1.T1.18.18.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.18.18.1.1" class="ltx_text" style="font-size:90%;">M</span><sub id="S1.T1.18.18.1.2" class="ltx_sub"><span id="S1.T1.18.18.1.2.1" class="ltx_text" style="font-size:90%;">1</span></sub>
</th>
<td id="S1.T1.19.19.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.19.19.2.m1.1" class="ltx_Math" alttext="174.61" display="inline"><semantics id="S1.T1.19.19.2.m1.1a"><mn mathsize="90%" id="S1.T1.19.19.2.m1.1.1" xref="S1.T1.19.19.2.m1.1.1.cmml">174.61</mn><annotation-xml encoding="MathML-Content" id="S1.T1.19.19.2.m1.1b"><cn type="float" id="S1.T1.19.19.2.m1.1.1.cmml" xref="S1.T1.19.19.2.m1.1.1">174.61</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.19.19.2.m1.1c">174.61</annotation></semantics></math></td>
<td id="S1.T1.20.20.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.20.20.3.m1.1" class="ltx_Math" alttext="1.3348" display="inline"><semantics id="S1.T1.20.20.3.m1.1a"><mn mathsize="90%" id="S1.T1.20.20.3.m1.1.1" xref="S1.T1.20.20.3.m1.1.1.cmml">1.3348</mn><annotation-xml encoding="MathML-Content" id="S1.T1.20.20.3.m1.1b"><cn type="float" id="S1.T1.20.20.3.m1.1.1.cmml" xref="S1.T1.20.20.3.m1.1.1">1.3348</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.20.20.3.m1.1c">1.3348</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.23.23" class="ltx_tr">
<th id="S1.T1.21.21.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.21.21.1.1" class="ltx_text" style="font-size:90%;">M</span><sub id="S1.T1.21.21.1.2" class="ltx_sub"><span id="S1.T1.21.21.1.2.1" class="ltx_text" style="font-size:90%;">2</span></sub>
</th>
<td id="S1.T1.22.22.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.22.22.2.m1.1" class="ltx_Math" alttext="185.00" display="inline"><semantics id="S1.T1.22.22.2.m1.1a"><mn mathsize="90%" id="S1.T1.22.22.2.m1.1.1" xref="S1.T1.22.22.2.m1.1.1.cmml">185.00</mn><annotation-xml encoding="MathML-Content" id="S1.T1.22.22.2.m1.1b"><cn type="float" id="S1.T1.22.22.2.m1.1.1.cmml" xref="S1.T1.22.22.2.m1.1.1">185.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.22.22.2.m1.1c">185.00</annotation></semantics></math></td>
<td id="S1.T1.23.23.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.23.23.3.m1.1" class="ltx_Math" alttext="1.4143" display="inline"><semantics id="S1.T1.23.23.3.m1.1a"><mn mathsize="90%" id="S1.T1.23.23.3.m1.1.1" xref="S1.T1.23.23.3.m1.1.1.cmml">1.4143</mn><annotation-xml encoding="MathML-Content" id="S1.T1.23.23.3.m1.1b"><cn type="float" id="S1.T1.23.23.3.m1.1.1.cmml" xref="S1.T1.23.23.3.m1.1.1">1.4143</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.23.23.3.m1.1c">1.4143</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.25.25" class="ltx_tr">
<th id="S1.T1.25.25.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S1.T1.25.25.3.1" class="ltx_text" style="font-size:90%;">P</span></th>
<td id="S1.T1.24.24.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.24.24.1.m1.1" class="ltx_Math" alttext="196.00" display="inline"><semantics id="S1.T1.24.24.1.m1.1a"><mn mathsize="90%" id="S1.T1.24.24.1.m1.1.1" xref="S1.T1.24.24.1.m1.1.1.cmml">196.00</mn><annotation-xml encoding="MathML-Content" id="S1.T1.24.24.1.m1.1b"><cn type="float" id="S1.T1.24.24.1.m1.1.1.cmml" xref="S1.T1.24.24.1.m1.1.1">196.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.24.24.1.m1.1c">196.00</annotation></semantics></math></td>
<td id="S1.T1.25.25.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.25.25.2.m1.1" class="ltx_Math" alttext="1.4984" display="inline"><semantics id="S1.T1.25.25.2.m1.1a"><mn mathsize="90%" id="S1.T1.25.25.2.m1.1.1" xref="S1.T1.25.25.2.m1.1.1.cmml">1.4984</mn><annotation-xml encoding="MathML-Content" id="S1.T1.25.25.2.m1.1b"><cn type="float" id="S1.T1.25.25.2.m1.1.1.cmml" xref="S1.T1.25.25.2.m1.1.1">1.4984</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.25.25.2.m1.1c">1.4984</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.28.28" class="ltx_tr">
<th id="S1.T1.26.26.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.26.26.1.1" class="ltx_text" style="font-size:90%;">D</span><sub id="S1.T1.26.26.1.2" class="ltx_sub"><span id="S1.T1.26.26.1.2.1" class="ltx_text" style="font-size:90%;">1</span></sub>
</th>
<td id="S1.T1.27.27.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.27.27.2.m1.1" class="ltx_Math" alttext="207.65" display="inline"><semantics id="S1.T1.27.27.2.m1.1a"><mn mathsize="90%" id="S1.T1.27.27.2.m1.1.1" xref="S1.T1.27.27.2.m1.1.1.cmml">207.65</mn><annotation-xml encoding="MathML-Content" id="S1.T1.27.27.2.m1.1b"><cn type="float" id="S1.T1.27.27.2.m1.1.1.cmml" xref="S1.T1.27.27.2.m1.1.1">207.65</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.27.27.2.m1.1c">207.65</annotation></semantics></math></td>
<td id="S1.T1.28.28.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.28.28.3.m1.1" class="ltx_Math" alttext="1.587" display="inline"><semantics id="S1.T1.28.28.3.m1.1a"><mn mathsize="90%" id="S1.T1.28.28.3.m1.1.1" xref="S1.T1.28.28.3.m1.1.1.cmml">1.587</mn><annotation-xml encoding="MathML-Content" id="S1.T1.28.28.3.m1.1b"><cn type="float" id="S1.T1.28.28.3.m1.1.1.cmml" xref="S1.T1.28.28.3.m1.1.1">1.587</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.28.28.3.m1.1c">1.587</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.32.32" class="ltx_tr">
<th id="S1.T1.30.30.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.30.30.2.1" class="ltx_text" style="font-size:90%;">D</span><sub id="S1.T1.30.30.2.2" class="ltx_sub"><span id="S1.T1.30.30.2.2.1" class="ltx_text" style="font-size:90%;">2</span></sub><span id="S1.T1.30.30.2.3" class="ltx_text" style="font-size:90%;">, N</span><sub id="S1.T1.30.30.2.4" class="ltx_sub"><span id="S1.T1.30.30.2.4.1" class="ltx_text" style="font-size:90%;">1</span></sub>
</th>
<td id="S1.T1.31.31.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.31.31.3.m1.1" class="ltx_Math" alttext="220.00" display="inline"><semantics id="S1.T1.31.31.3.m1.1a"><mn mathsize="90%" id="S1.T1.31.31.3.m1.1.1" xref="S1.T1.31.31.3.m1.1.1.cmml">220.00</mn><annotation-xml encoding="MathML-Content" id="S1.T1.31.31.3.m1.1b"><cn type="float" id="S1.T1.31.31.3.m1.1.1.cmml" xref="S1.T1.31.31.3.m1.1.1">220.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.31.31.3.m1.1c">220.00</annotation></semantics></math></td>
<td id="S1.T1.32.32.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.32.32.4.m1.1" class="ltx_Math" alttext="1.6818" display="inline"><semantics id="S1.T1.32.32.4.m1.1a"><mn mathsize="90%" id="S1.T1.32.32.4.m1.1.1" xref="S1.T1.32.32.4.m1.1.1.cmml">1.6818</mn><annotation-xml encoding="MathML-Content" id="S1.T1.32.32.4.m1.1b"><cn type="float" id="S1.T1.32.32.4.m1.1.1.cmml" xref="S1.T1.32.32.4.m1.1.1">1.6818</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.32.32.4.m1.1c">1.6818</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.36.36" class="ltx_tr">
<th id="S1.T1.34.34.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.34.34.2.1" class="ltx_text" style="font-size:90%;">D</span><sub id="S1.T1.34.34.2.2" class="ltx_sub"><span id="S1.T1.34.34.2.2.1" class="ltx_text" style="font-size:90%;">3</span></sub><span id="S1.T1.34.34.2.3" class="ltx_text" style="font-size:90%;">, N</span><sub id="S1.T1.34.34.2.4" class="ltx_sub"><span id="S1.T1.34.34.2.4.1" class="ltx_text" style="font-size:90%;">2</span></sub>
</th>
<td id="S1.T1.35.35.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.35.35.3.m1.1" class="ltx_Math" alttext="233.08" display="inline"><semantics id="S1.T1.35.35.3.m1.1a"><mn mathsize="90%" id="S1.T1.35.35.3.m1.1.1" xref="S1.T1.35.35.3.m1.1.1.cmml">233.08</mn><annotation-xml encoding="MathML-Content" id="S1.T1.35.35.3.m1.1b"><cn type="float" id="S1.T1.35.35.3.m1.1.1.cmml" xref="S1.T1.35.35.3.m1.1.1">233.08</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.35.35.3.m1.1c">233.08</annotation></semantics></math></td>
<td id="S1.T1.36.36.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S1.T1.36.36.4.m1.1" class="ltx_Math" alttext="1.7818" display="inline"><semantics id="S1.T1.36.36.4.m1.1a"><mn mathsize="90%" id="S1.T1.36.36.4.m1.1.1" xref="S1.T1.36.36.4.m1.1.1.cmml">1.7818</mn><annotation-xml encoding="MathML-Content" id="S1.T1.36.36.4.m1.1b"><cn type="float" id="S1.T1.36.36.4.m1.1.1.cmml" xref="S1.T1.36.36.4.m1.1.1">1.7818</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.36.36.4.m1.1c">1.7818</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.39.39" class="ltx_tr">
<th id="S1.T1.37.37.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.37.37.1.1" class="ltx_text" style="font-size:90%;">N</span><sub id="S1.T1.37.37.1.2" class="ltx_sub"><span id="S1.T1.37.37.1.2.1" class="ltx_text" style="font-size:90%;">3</span></sub>
</th>
<td id="S1.T1.38.38.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S1.T1.38.38.2.m1.1" class="ltx_Math" alttext="246.94" display="inline"><semantics id="S1.T1.38.38.2.m1.1a"><mn mathsize="90%" id="S1.T1.38.38.2.m1.1.1" xref="S1.T1.38.38.2.m1.1.1.cmml">246.94</mn><annotation-xml encoding="MathML-Content" id="S1.T1.38.38.2.m1.1b"><cn type="float" id="S1.T1.38.38.2.m1.1.1.cmml" xref="S1.T1.38.38.2.m1.1.1">246.94</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.38.38.2.m1.1c">246.94</annotation></semantics></math></td>
<td id="S1.T1.39.39.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S1.T1.39.39.3.m1.1" class="ltx_Math" alttext="1.8878" display="inline"><semantics id="S1.T1.39.39.3.m1.1a"><mn mathsize="90%" id="S1.T1.39.39.3.m1.1.1" xref="S1.T1.39.39.3.m1.1.1.cmml">1.8878</mn><annotation-xml encoding="MathML-Content" id="S1.T1.39.39.3.m1.1b"><cn type="float" id="S1.T1.39.39.3.m1.1.1.cmml" xref="S1.T1.39.39.3.m1.1.1">1.8878</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.39.39.3.m1.1c">1.8878</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Frequency of Notes in C Shruti and ratio between notes and S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.4" class="ltx_p"><span id="S1.p4.4.1" class="ltx_text" style="font-size:90%;">For each raga in Carnatic music, there is a corresponding ascending and descending melodic scale, refered to as an </span><span id="S1.p4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">arohanam</span><span id="S1.p4.4.3" class="ltx_text" style="font-size:90%;"> and </span><span id="S1.p4.4.4" class="ltx_text ltx_font_italic" style="font-size:90%;">avarohanam</span><span id="S1.p4.4.5" class="ltx_text" style="font-size:90%;">, respectively </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p4.4.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.p4.4.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p4.4.8" class="ltx_text" style="font-size:90%;">. Within each raga’s arohanam and avarohanam, the frequency of each swara must be greater than any swara that precedes it in the natural order described above (e.g. there can be no raga that has a combination of G</span><sub id="S1.p4.4.9" class="ltx_sub"><span id="S1.p4.4.9.1" class="ltx_text" style="font-size:90%;">1</span></sub><span id="S1.p4.4.10" class="ltx_text" style="font-size:90%;"> and R</span><sub id="S1.p4.4.11" class="ltx_sub"><span id="S1.p4.4.11.1" class="ltx_text" style="font-size:90%;">3</span></sub><span id="S1.p4.4.12" class="ltx_text" style="font-size:90%;">, or D</span><sub id="S1.p4.4.13" class="ltx_sub"><span id="S1.p4.4.13.1" class="ltx_text" style="font-size:90%;">2</span></sub><span id="S1.p4.4.14" class="ltx_text" style="font-size:90%;"> and N</span><sub id="S1.p4.4.15" class="ltx_sub"><span id="S1.p4.4.15.1" class="ltx_text" style="font-size:90%;">1</span></sub><span id="S1.p4.4.16" class="ltx_text" style="font-size:90%;">). However, there can be ragas with an arohanam or avarohanam that do not follow the natural swara order, but still follow the frequency order.</span></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.10" class="ltx_p"><span id="S1.p5.10.1" class="ltx_text" style="font-size:90%;">There are 72 ragas in Carnatic that are given the special classification of a Melakarta, or parent raga. Melakartas are defined as ragas that contain all seven swaras in the arohanam and avarohanam, with each swara sung at the same frequency in both the arohanam and avarohanam. The number seventy-two is thus obtained from all possible combinations of the variable swaras (R G M D N). Derived from these foundational melakartas are the Janya ragas (child ragas), which are characterized by missing or extra swaras in the arohanam and/or avarohanam
(e.g. S R</span><sub id="S1.p5.10.2" class="ltx_sub"><span id="S1.p5.10.2.1" class="ltx_text" style="font-size:90%;">2</span></sub><span id="S1.p5.10.3" class="ltx_text" style="font-size:90%;"> G</span><sub id="S1.p5.10.4" class="ltx_sub"><span id="S1.p5.10.4.1" class="ltx_text" style="font-size:90%;">3</span></sub><span id="S1.p5.10.5" class="ltx_text" style="font-size:90%;"> P D</span><sub id="S1.p5.10.6" class="ltx_sub"><span id="S1.p5.10.6.1" class="ltx_text" style="font-size:90%;">2</span></sub><span id="S1.p5.10.7" class="ltx_text" style="font-size:90%;"> </span><math id="S1.p5.4.m4.1" class="ltx_Math" alttext="\bar{\text{S}}" display="inline"><semantics id="S1.p5.4.m4.1a"><mover accent="true" id="S1.p5.4.m4.1.1" xref="S1.p5.4.m4.1.1.cmml"><mtext mathsize="90%" id="S1.p5.4.m4.1.1.2" xref="S1.p5.4.m4.1.1.2a.cmml">S</mtext><mo mathsize="90%" id="S1.p5.4.m4.1.1.1" xref="S1.p5.4.m4.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S1.p5.4.m4.1b"><apply id="S1.p5.4.m4.1.1.cmml" xref="S1.p5.4.m4.1.1"><ci id="S1.p5.4.m4.1.1.1.cmml" xref="S1.p5.4.m4.1.1.1">¯</ci><ci id="S1.p5.4.m4.1.1.2a.cmml" xref="S1.p5.4.m4.1.1.2"><mtext mathsize="90%" id="S1.p5.4.m4.1.1.2.cmml" xref="S1.p5.4.m4.1.1.2">S</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.4.m4.1c">\bar{\text{S}}</annotation></semantics></math><span id="S1.p5.10.8" class="ltx_text" style="font-size:90%;">) or the use of twisted progressions (e.g. S G</span><sub id="S1.p5.10.9" class="ltx_sub"><span id="S1.p5.10.9.1" class="ltx_text" style="font-size:90%;">3</span></sub><span id="S1.p5.10.10" class="ltx_text" style="font-size:90%;"> R</span><sub id="S1.p5.10.11" class="ltx_sub"><span id="S1.p5.10.11.1" class="ltx_text" style="font-size:90%;">2</span></sub><span id="S1.p5.10.12" class="ltx_text" style="font-size:90%;"> M</span><sub id="S1.p5.10.13" class="ltx_sub"><span id="S1.p5.10.13.1" class="ltx_text" style="font-size:90%;">1</span></sub><span id="S1.p5.10.14" class="ltx_text" style="font-size:90%;"> P N</span><sub id="S1.p5.10.15" class="ltx_sub"><span id="S1.p5.10.15.1" class="ltx_text" style="font-size:90%;">3</span></sub><span id="S1.p5.10.16" class="ltx_text" style="font-size:90%;"> </span><math id="S1.p5.9.m9.1" class="ltx_Math" alttext="\bar{\text{S}}" display="inline"><semantics id="S1.p5.9.m9.1a"><mover accent="true" id="S1.p5.9.m9.1.1" xref="S1.p5.9.m9.1.1.cmml"><mtext mathsize="90%" id="S1.p5.9.m9.1.1.2" xref="S1.p5.9.m9.1.1.2a.cmml">S</mtext><mo mathsize="90%" id="S1.p5.9.m9.1.1.1" xref="S1.p5.9.m9.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S1.p5.9.m9.1b"><apply id="S1.p5.9.m9.1.1.cmml" xref="S1.p5.9.m9.1.1"><ci id="S1.p5.9.m9.1.1.1.cmml" xref="S1.p5.9.m9.1.1.1">¯</ci><ci id="S1.p5.9.m9.1.1.2a.cmml" xref="S1.p5.9.m9.1.1.2"><mtext mathsize="90%" id="S1.p5.9.m9.1.1.2.cmml" xref="S1.p5.9.m9.1.1.2">S</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.9.m9.1c">\bar{\text{S}}</annotation></semantics></math><span id="S1.p5.10.17" class="ltx_text" style="font-size:90%;">) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p5.10.18.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S1.p5.10.19.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p5.10.20" class="ltx_text" style="font-size:90%;">. Janya ragas can consist of as few as 4 notes (not including the ending </span><math id="S1.p5.10.m10.1" class="ltx_Math" alttext="\bar{\text{S}}" display="inline"><semantics id="S1.p5.10.m10.1a"><mover accent="true" id="S1.p5.10.m10.1.1" xref="S1.p5.10.m10.1.1.cmml"><mtext mathsize="90%" id="S1.p5.10.m10.1.1.2" xref="S1.p5.10.m10.1.1.2a.cmml">S</mtext><mo mathsize="90%" id="S1.p5.10.m10.1.1.1" xref="S1.p5.10.m10.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S1.p5.10.m10.1b"><apply id="S1.p5.10.m10.1.1.cmml" xref="S1.p5.10.m10.1.1"><ci id="S1.p5.10.m10.1.1.1.cmml" xref="S1.p5.10.m10.1.1.1">¯</ci><ci id="S1.p5.10.m10.1.1.2a.cmml" xref="S1.p5.10.m10.1.1.2"><mtext mathsize="90%" id="S1.p5.10.m10.1.1.2.cmml" xref="S1.p5.10.m10.1.1.2">S</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.10.m10.1c">\bar{\text{S}}</annotation></semantics></math><span id="S1.p5.10.21" class="ltx_text" style="font-size:90%;"> in the next octave to close the scale), and are estimated to number in the tens of thousands.</span></p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text" style="font-size:90%;">Carnatic music also includes a technique that separates it from many other forms of music, known as </span><span id="S1.p6.1.2" class="ltx_text ltx_font_italic" style="font-size:90%;">gamaka</span><span id="S1.p6.1.3" class="ltx_text" style="font-size:90%;">. Gamaka is the modulation of a swara sung to provide additional beauty and variation. Seven types of gamaka are commonly used:</span></p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text" style="font-size:90%;">Jantai - the use of additional emphasis on a repeated note </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.I1.i1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.I1.i1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.I1.i1.p1.1.4" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text" style="font-size:90%;">Jaru - the sliding from a slightly higher or lower frequency through the true frequency of the note </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.I1.i2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.I1.i2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.I1.i2.p1.1.4" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text" style="font-size:90%;">Kampita - the rapid oscillation on a single note by touching the swara directly above and below it </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.I1.i3.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.I1.i3.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.I1.i3.p1.1.4" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><span id="S1.I1.i4.p1.1.1" class="ltx_text" style="font-size:90%;">Khandippu - the descent from an initial swara to the final nonadjacent swara with brief intonations on swaras in between </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.I1.i4.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.I1.i4.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.I1.i4.p1.1.4" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p"><span id="S1.I1.i5.p1.1.1" class="ltx_text" style="font-size:90%;">Odukkal - similar to jaru, shifting to the next swara in the raga before returning to the original swara </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.I1.i5.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.I1.i5.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.I1.i5.p1.1.4" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</li>
<li id="S1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i6.p1" class="ltx_para">
<p id="S1.I1.i6.p1.1" class="ltx_p"><span id="S1.I1.i6.p1.1.1" class="ltx_text" style="font-size:90%;">Orikai - shifting to the next swara in the raga before descending to the swara below the original </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.I1.i6.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.I1.i6.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.I1.i6.p1.1.4" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</li>
<li id="S1.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i7.p1" class="ltx_para">
<p id="S1.I1.i7.p1.1" class="ltx_p"><span id="S1.I1.i7.p1.1.1" class="ltx_text" style="font-size:90%;">Sphuritam - starting a swara a higher frequency than natural and rapidly descending </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.I1.i7.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.I1.i7.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.I1.i7.p1.1.4" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</li>
</ul>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text" style="font-size:90%;">Gamakas add an intricate level of detail to Carnatic music. However, they also provide an additional challenge to machine algorithms that traditionally rely on frequency identification.</span></p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p"><span id="S1.p8.1.1" class="ltx_text" style="font-size:90%;">In this project, we will take a novel approach to raga recognition, using LSTMs and CNNs. Rather than trying to identify pre-determined phrases, we use a combination of neural networks with several layers to allow the machine to determine what patterns and phrases are relevant on its own. Further, this project will greatly expand the dataset to include other ragas and songs in a greater variety of samples, and will explicitly use relative frequency changes rather than absolute frequency changes to attempt to increase overall accuracy. The goal is to stay consistent with existing accuracies even as we increase the potential for variance by increasing the list of ragas to predict. The accuracy will be measured using the Loss Function, as has been done in previous research, to ensure direct comparisons can be made </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S1.p8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p8.1.4" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>State of the Art</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="font-size:90%;">Most existing research in machine learning methods for raga recognition is based on either Pitch Class (PC) identification, or frequency sequence. Some of the best performing PC id methods across both Carnatic and Hindustani (North Indian Classical) music </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.4" class="ltx_text" style="font-size:90%;"> have reached accuracy rates of </span><math id="S2.p1.1.m1.1" class="ltx_Math" alttext="91\%" display="inline"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mn mathsize="90%" id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">91</mn><mo mathsize="90%" id="S2.p1.1.m1.1.1.1" xref="S2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="latexml" id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">91</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">91\%</annotation></semantics></math><span id="S2.p1.1.5" class="ltx_text" style="font-size:90%;">. However, PC id methods have been found to lose the intricacies of Carnatic music through their use of first order functions, which can often smooth over gamakas </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S2.p1.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.8" class="ltx_text" style="font-size:90%;">. This will cause issues when more complex ragas are incorporated into the test set. In more current research, there have been attempts to overcome these deficiencies, such as through Hidden Markov Models </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S2.p1.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.11" class="ltx_text" style="font-size:90%;">, or using the arohanam and avarohanam as support metrics </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.12.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S2.p1.1.13.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.14" class="ltx_text" style="font-size:90%;">, among others.</span></p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text" style="font-size:90%;">In this project, we will be using the arohanam and avarohanam – which, while critical, do not contain much expandable information – in conjunction with actual audio clips to maximize the amount of possible reference points. We will also reduce the reliance on pre-processing that has become common in Carnatic raga identification research, to avoid the possible loss of complex features.</span></p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text" style="font-size:90%;">The general system architecture for this project is highlighted in Figure </span><a href="#S3.F1" title="Figure 1 ‣ 3.1 Signal Separation ‣ 3 Method ‣ CARNATIC RAGA IDENTIFICATION SYSTEM USING RIGOROUS TIME-DELAY NEURAL NETWORK" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S3.p1.1.2" class="ltx_text" style="font-size:90%;">. However, specifics about the reasoning behind use and output for individual components are available below. At a high level, the methodology for this research project underwent substantial revisions to enhance data preprocessing and model training efficiency. Initially, the signal separation was performed using the Mad-TwinNet system </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S3.p1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p1.1.5" class="ltx_text" style="font-size:90%;">, followed by Mel-frequency cepstral coefficient (MFCC) feature extraction and classification using a Time-Delay Neural Network (TDNN) and Long Short-Term Memory (LSTM) network. However, to improve scalability and streamline the preprocessing pipeline, the revised final methodology below incorporates a more robust and efficient approach.</span></p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Signal Separation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">The datasets used include a combination of concert recordings and studio recordings. All of these recordings contain a vocalist as the main artist, but some also have background instruments, such as a Mridangam, Violin, or Tambura, in the background. After significant consideration, this background noise was included in the program, to train the program to anticipate the possibility of background noise. Instead, the first ten percent and last ten percent of each clip were trimmed to cut out any spoken introduction to the piece that could add artifacts to the training set.</span></p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2405.16000/assets/SpeechRecFlowchart.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="334" height="222" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>System Architecture</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Preprocessing / Feature Extraction</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">The preprocessing pipeline for the Carnatic music dataset is designed to prepare the audio data for feature extraction and model training. One key aspect of this pipeline involves customizing the feature extraction process to capture the nuanced melodic structures and tonal intricacies characteristic of Carnatic music. Rather than relying on standard Mel-frequency cepstral coefficients (MFCCs), the methodology adopts a unique approach, crafting custom bins with triangular filter banks. Each bin represents one of the classical Western notes, spanning a total of 56 bins from B1 to E6. This represents a range inclusive of the standard reach of both male and female voices </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S3.SS2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.p1.1.4" class="ltx_text" style="font-size:90%;">. This customization aims to align the feature representation with the distinctive tonal palette of Carnatic music and maximize the capture of the nonstandard melodic patterns inherent in Carnatic ragas.</span></p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text" style="font-size:90%;">In addition to customizing the MFCC computation, the preprocessing pipeline incorporates meticulous filtering of frequency components to focus on the essential spectral characteristics relevant to Carnatic music. By significantly attenuating frequencies below or above the specified bins corresponding to classical Western notes, the pipeline ensures that extra extracted artifacts are not included in the feature set. The preprocessing pipeline also standardizes the audio data format and ensures uniformity in segment duration to facilitate consistent processing and analysis. Each audio segment, extracted from the original recordings, undergoes conversion to the WAV format and adjustment of the sample rate to 22050 Hz. Furthermore, the duration of each segment is standardized to the target duration of 30 seconds, with silent padding applied when necessary. These preprocessing steps allow for robust feature extraction and model training, enabling machine learning algorithms to leverage the enriched dataset to accurately classify ragas.</span></p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Time-Delay Neural Network</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text" style="font-size:90%;">A Time-Delay Neural Network (TDNN) is utilized as the initial neural network in order to tailor the classifier on the Carnatic music samples. A TDNN was chosen due to its adeptness at capturing temporal dependencies within sequential data, making them well-suited for processing time-series data such as audio signals. Given that Carnatic music — with its near continuous flow of gamakas — exhibits several intricate temporal and microtonal patterns that could make it easily confused with a parallel raga in another shruti, the convolutional layers will be utilized to extract local and translation-invariant features from the spectrogram. These layers will capture patterns in the frequency domain that are indicative of the raga regardless of pitch. Further, TDNNs offer a streamlined architecture that facilitates efficient learning from sequential data while maintaining computational tractability. This characteristic is particularly advantageous for our task, as it allows for the exploration of complex temporal relationships within the music samples without excessive computational overhead. Levering the TDNNs here will allow for the establishment of a strong baseline for comparison with more complex models, such as the Long Short-Term Memory networks used later. As such, there are an </span><math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi mathsize="90%" id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">n</annotation></semantics></math><span id="S3.SS3.p1.1.2" class="ltx_text" style="font-size:90%;"> number of convolutional layers, along with additional features that enhance the performance and stability of the neural network models.</span></p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text" style="font-size:90%;">These additional layers include batch normalization (BaN) and Rectified Linear Units (ReLU), which further enhance the performance and stability of the neural network models. Batch Normalization is employed to address internal covariate shift within the network during training </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S3.SS3.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS3.p2.1.4" class="ltx_text" style="font-size:90%;">. By normalizing the activations of each layer across mini-batches, Batch Normalization reduces internal covariate shift, thereby stabilizing and accelerating the training process </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p2.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S3.SS3.p2.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS3.p2.1.7" class="ltx_text" style="font-size:90%;">. The ReLU activation function is then chosen for its ability to mitigate the vanishing gradient problem commonly encountered in deep neural networks </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p2.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S3.SS3.p2.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS3.p2.1.10" class="ltx_text" style="font-size:90%;">. By allowing only positive values to pass through, ReLU accelerates convergence during training by promoting sparse activation, thereby facilitating faster learning and more efficient model training </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p2.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S3.SS3.p2.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS3.p2.1.13" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text" style="font-size:90%;">Finally, max pooling will be applied to reduce the dimensionality of the output function set, and keep only the most salient features </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS3.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S3.SS3.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS3.p3.1.4" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Long-Short Term Memory</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p"><span id="S3.SS4.p1.1.1" class="ltx_text" style="font-size:90%;">A Long Short-Term Memory (LSTM) network is utilized in the next step of the classification process. These variations of recurrent neural networks (RNNs) are augmented with specific dense layers and an attention mechanism to approach the task of classifying the ragas from the many samples recorded in different shrutis in the original music dataset. An LSTM is selected for its adeptness at effectively modeling sequential dependencies, crucial for capturing the intricate temporal patterns and melodic structures inherent in music.</span></p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p"><span id="S3.SS4.p2.1.1" class="ltx_text" style="font-size:90%;">In addition, a flattening layer is introduced after the LSTM module. This flattening layer reshapes the output from the LSTM layer into a one-dimensional array, facilitating the transition from the LSTM’s sequential output to the subsequent fully connected layers. The flattened representation retains the learned temporal dependencies while preparing the data for further processing by the dense layers.</span></p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p"><span id="S3.SS4.p3.1.1" class="ltx_text" style="font-size:90%;">Furthermore, multiple dense layers with dropout regularization are integrated into the architecture. Dropout layers are inserted after each dense layer to mitigate overfitting by randomly dropping a fraction of the neurons during training </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS4.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S3.SS4.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS4.p3.1.4" class="ltx_text" style="font-size:90%;">. This regularization technique helps prevent the model from relying too heavily on specific features or relationships in the data, thereby improving its generalization performance. The dense layers, combined with dropout regularization, contribute to the extraction of abstract features from the sequential input data, enhancing the network’s ability to classify ragas accurately.</span></p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p"><span id="S3.SS4.p4.1.1" class="ltx_text" style="font-size:90%;">Finally, a softmax layer is employed as the final output layer in the neural network architecture for raga identification in Carnatic music samples. This layer transforms the network’s raw outputs into probabilities across different raga classes. By providing probabilistic outputs, the softmax layer enables efficient training via well-defined loss functions and multi-class classification tasks, while also enhancing the transparency and reliability of the raga identification system.</span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text" style="font-size:90%;">In this project, all data is collected manually through Youtube. Samples were chosen through the following method. First, for each of the 172 ragas, the online database Karnatik.com was perused </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S4.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.p1.1.4" class="ltx_text" style="font-size:90%;">, and between 1 and 3 songs were selected, depending on the popularity of the raga. Next, each song was searched on YouTube, and two ideal recordings were selected for each song, one in a male shruti and the other in a female shruti. For the purposes of this project, and ideal recording meets all of the following:</span></p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text" style="font-size:90%;">Good Sound Quality</span></p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text" style="font-size:90%;">More than 4 Minutes</span></p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text" style="font-size:90%;">Less than 20 Minutes</span></p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text" style="font-size:90%;">No </span><span id="S4.I1.i4.p1.1.2" class="ltx_text ltx_font_italic" style="font-size:90%;">thani avarthanam</span><span id="S4.I1.i4.p1.1.3" class="ltx_text" style="font-size:90%;">, a prolonged percussion solo </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.I1.i4.p1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S4.I1.i4.p1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.I1.i4.p1.1.6" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</li>
</ul>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text" style="font-size:90%;">The recordings were chosen as the first in each gender to be listed by YouTube that meet these ideal requirements, regardless of the artist or venue.</span></p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text" style="font-size:90%;">In the event that no suitable songs were found under the ideal conditions, relaxations were made to aid in finding a suitable song. These relaxations occurred in the numerical order listed below until a suitable song was found.</span></p>
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.p1.1.1" class="ltx_text" style="font-size:90%;">Song between 3 and 4 minutes</span></p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.p1.1.1" class="ltx_text" style="font-size:90%;">Song between 20 and 40 minutes</span></p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p"><span id="S4.I2.i3.p1.1.1" class="ltx_text" style="font-size:90%;">Ideal song by the other gender</span></p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p"><span id="S4.I2.i4.p1.1.1" class="ltx_text" style="font-size:90%;">Song between 40 and 60 minutes</span></p>
</div>
</li>
<li id="S4.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S4.I2.i5.p1" class="ltx_para">
<p id="S4.I2.i5.p1.1" class="ltx_p"><span id="S4.I2.i5.p1.1.1" class="ltx_text" style="font-size:90%;">Song by the other gender between 3 and 60 minutes</span></p>
</div>
</li>
<li id="S4.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S4.I2.i6.p1" class="ltx_para">
<p id="S4.I2.i6.p1.1" class="ltx_p"><span id="S4.I2.i6.p1.1.1" class="ltx_text" style="font-size:90%;">Different song by the same gender in the same raga that hasn’t already been sampled and meets the ideal requirements</span></p>
</div>
</li>
</ol>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text" style="font-size:90%;">Under these conditions, a total of 676 samples were chosen across 172 ragas. 62 ragas had 6 samples, 42 ragas had 4 samples, and the remaining 68 ragas had 2 samples. These samples, split into 30-second clips, provided a total of 10,999 clips to use for training and testing. This was split, with 80% of the data used for training, and 20 % used for testing, in line with existing methodology </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S4.p4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.p4.1.4" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Reference</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Dataset</span></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Classifier</span></th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_right ltx_border_t">
<span id="S4.T2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Pillai &amp; Mahajan (2008) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S4.T2.1.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T2.1.2.1.2.1" class="ltx_text" style="font-size:90%;">All 72 melakartas</span></td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T2.1.2.1.3.1" class="ltx_text" style="font-size:90%;">Acoustic Model</span></td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T2.1.2.1.4.1" class="ltx_text" style="font-size:90%;">Vocal, 80.56%</span></td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_right">
<span id="S4.T2.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Shetty &amp; Achary (2009) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.3.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S4.T2.1.3.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_right"><span id="S4.T2.1.3.2.2.1" class="ltx_text" style="font-size:90%;">Arohana-avarohana clips from 20 ragas</span></td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_right"><span id="S4.T2.1.3.2.3.1" class="ltx_text" style="font-size:90%;">ANN</span></td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_right"><span id="S4.T2.1.3.2.4.1" class="ltx_text" style="font-size:90%;">95%</span></td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_right">
<span id="S4.T2.1.4.3.1.1" class="ltx_text" style="font-size:90%;">Ranjani et al. (2011) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.4.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S4.T2.1.4.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_right"><span id="S4.T2.1.4.3.2.1" class="ltx_text" style="font-size:90%;">5 Melakarta ragas of Carnatic Music</span></td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_right"><span id="S4.T2.1.4.3.3.1" class="ltx_text" style="font-size:90%;">GMM</span></td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_right"><span id="S4.T2.1.4.3.4.1" class="ltx_text" style="font-size:90%;">62.10%</span></td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<td id="S4.T2.1.5.4.1" class="ltx_td ltx_align_right">
<span id="S4.T2.1.5.4.1.1" class="ltx_text" style="font-size:90%;">Koduri et al. (2011) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.5.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S4.T2.1.5.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_right"><span id="S4.T2.1.5.4.2.1" class="ltx_text" style="font-size:90%;">176 clips from 10 ragas</span></td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_right"><span id="S4.T2.1.5.4.3.1" class="ltx_text" style="font-size:90%;">K-NN</span></td>
<td id="S4.T2.1.5.4.4" class="ltx_td ltx_align_right"><span id="S4.T2.1.5.4.4.1" class="ltx_text" style="font-size:90%;">76.50%</span></td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<td id="S4.T2.1.6.5.1" class="ltx_td ltx_align_right">
<span id="S4.T2.1.6.5.1.1" class="ltx_text" style="font-size:90%;">Manjabhat et al. (2017) (1) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.6.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S4.T2.1.6.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_right"><span id="S4.T2.1.6.5.2.1" class="ltx_text" style="font-size:90%;">213 clips of 14 ragas</span></td>
<td id="S4.T2.1.6.5.3" class="ltx_td ltx_align_right"><span id="S4.T2.1.6.5.3.1" class="ltx_text" style="font-size:90%;">FFNN</span></td>
<td id="S4.T2.1.6.5.4" class="ltx_td ltx_align_right"><span id="S4.T2.1.6.5.4.1" class="ltx_text" style="font-size:90%;">90.14%</span></td>
</tr>
<tr id="S4.T2.1.7.6" class="ltx_tr">
<td id="S4.T2.1.7.6.1" class="ltx_td ltx_align_right">
<span id="S4.T2.1.7.6.1.1" class="ltx_text" style="font-size:90%;">Manjabhat et al. (2017) (2) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.7.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S4.T2.1.7.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.7.6.2" class="ltx_td ltx_align_right"><span id="S4.T2.1.7.6.2.1" class="ltx_text" style="font-size:90%;">538 clips from 17 ragas</span></td>
<td id="S4.T2.1.7.6.3" class="ltx_td ltx_align_right"><span id="S4.T2.1.7.6.3.1" class="ltx_text" style="font-size:90%;">FFNN</span></td>
<td id="S4.T2.1.7.6.4" class="ltx_td ltx_align_right"><span id="S4.T2.1.7.6.4.1" class="ltx_text" style="font-size:90%;">95%</span></td>
</tr>
<tr id="S4.T2.1.8.7" class="ltx_tr">
<td id="S4.T2.1.8.7.1" class="ltx_td ltx_align_right">
<span id="S4.T2.1.8.7.1.1" class="ltx_text" style="font-size:90%;">Chinthapenta (2019) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.8.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S4.T2.1.8.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T2.1.8.7.2" class="ltx_td ltx_align_right"><span id="S4.T2.1.8.7.2.1" class="ltx_text" style="font-size:90%;">7 ragas, 20 samples of each raga</span></td>
<td id="S4.T2.1.8.7.3" class="ltx_td ltx_align_right"><span id="S4.T2.1.8.7.3.1" class="ltx_text" style="font-size:90%;">LSTM</span></td>
<td id="S4.T2.1.8.7.4" class="ltx_td ltx_align_right"><span id="S4.T2.1.8.7.4.1" class="ltx_text" style="font-size:90%;">30%</span></td>
</tr>
<tr id="S4.T2.1.9.8" class="ltx_tr">
<td id="S4.T2.1.9.8.1" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T2.1.9.8.1.1" class="ltx_text" style="font-size:90%;">This Method</span></td>
<td id="S4.T2.1.9.8.2" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T2.1.9.8.2.1" class="ltx_text" style="font-size:90%;">172 ragas, 676 songs</span></td>
<td id="S4.T2.1.9.8.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T2.1.9.8.3.1" class="ltx_text" style="font-size:90%;">LSTM/TDNN</span></td>
<td id="S4.T2.1.9.8.4" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T2.1.9.8.4.1" class="ltx_text" style="font-size:90%;">95.31%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison to Previous Work</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Experiment</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:90%;">The experimental setup for this research paper involves training a deep learning model with a Sequential architecture using Tensorflow, as shown in Figure </span><a href="#S5.F2" title="Figure 2 ‣ 5 Experiment ‣ CARNATIC RAGA IDENTIFICATION SYSTEM USING RIGOROUS TIME-DELAY NEURAL NETWORK" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S5.p1.1.2" class="ltx_text" style="font-size:90%;">. The model’s architecture comprises of several layers stacked sequentially to form a hierarchical representation of the input data. Specifically, it includes a Convolutional 1D layer with 64 filters and a kernel size of 3, followed by a MaxPooling1D layer to reduce spatial dimensions. BatchNormalization is applied to normalize the activations of the previous layer, enhancing the stability and speed of training. The model further incorporates a Long Short-Term Memory (LSTM) layer with 512 units, which allows it to capture temporal dependencies in the data effectively. Subsequently, a Flatten layer is employed to transform the output into a one-dimensional vector before passing it through two Dense layers with 512 and 256 units, respectively. Dropout layers with a dropout rate of 0.5 are inserted to mitigate overfitting during training. Finally, a Dense layer with 172 units, corresponding to the number of classes in the classification task, produces the model’s output probabilities.</span></p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2405.16000/assets/ModelParams.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="334" height="210" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Model Architecture and Weights</figcaption>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text" style="font-size:90%;">The model also has a total of 8,436,142 trainable parameters, spread across the various layers. Additionally, 128 parameters stem from BatchNormalization and two parameters from the optimizer, totaling 8,436,142 parameters. The dataset used for training, validation, and testing is not explicitly described in terms of size or quality. However, it is standard practice to split the dataset into training and test sets, with approximately 80% allocated for training and 20% for validation and testing </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S5.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.p2.1.4" class="ltx_text" style="font-size:90%;">. The quality of the data is influenced by preprocessing techniques, data augmentation strategies, and the representativeness of the dataset with respect to the problem domain.</span></p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text" style="font-size:90%;">Training the model involves compiling it with a categorical cross-entropy loss function and the Adam optimizer. To prevent overfitting, early stopping with patience of 100 epochs is employed. The model undergoes training for 300 epochs using a batch size of 256 samples. The backend used for deep learning computations is built using TensorFlow on the Keras layers. This model is specifically designed for a classification task with 172 distinct classes, each representing a unique raga.</span></p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text" style="font-size:90%;">Performance metrics of this model were meticulously assessed, leveraging accuracy and loss computations as benchmarks against previous trials.</span></p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.2" class="ltx_p"><span id="S6.p2.2.1" class="ltx_text" style="font-size:90%;">As depicted in Figures </span><a href="#S6.F3" title="Figure 3 ‣ 6 Results ‣ CARNATIC RAGA IDENTIFICATION SYSTEM USING RIGOROUS TIME-DELAY NEURAL NETWORK" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S6.p2.2.2" class="ltx_text" style="font-size:90%;"> and </span><a href="#S6.F4" title="Figure 4 ‣ 6 Results ‣ CARNATIC RAGA IDENTIFICATION SYSTEM USING RIGOROUS TIME-DELAY NEURAL NETWORK" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S6.p2.2.3" class="ltx_text" style="font-size:90%;">, the model’s performance on a singular run is shown, revealing some insights. Notably, the implementation of EarlyStopping culminated in the program ending after 132 epochs of the allowed 300. At the time of halt, the recorded final validation loss stood at 0.3544, higher than the training loss of 0.0153, but relatively low given the high data sample and complexity of the task. The EarlyStopping monitor was successful in observing the delta validation loss, stopping exactly one hundred epochs after flat-lining at the </span><math id="S6.p2.1.m1.1" class="ltx_Math" alttext="30^{th}" display="inline"><semantics id="S6.p2.1.m1.1a"><msup id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml"><mn mathsize="90%" id="S6.p2.1.m1.1.1.2" xref="S6.p2.1.m1.1.1.2.cmml">30</mn><mrow id="S6.p2.1.m1.1.1.3" xref="S6.p2.1.m1.1.1.3.cmml"><mi mathsize="90%" id="S6.p2.1.m1.1.1.3.2" xref="S6.p2.1.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S6.p2.1.m1.1.1.3.1" xref="S6.p2.1.m1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S6.p2.1.m1.1.1.3.3" xref="S6.p2.1.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><apply id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S6.p2.1.m1.1.1.1.cmml" xref="S6.p2.1.m1.1.1">superscript</csymbol><cn type="integer" id="S6.p2.1.m1.1.1.2.cmml" xref="S6.p2.1.m1.1.1.2">30</cn><apply id="S6.p2.1.m1.1.1.3.cmml" xref="S6.p2.1.m1.1.1.3"><times id="S6.p2.1.m1.1.1.3.1.cmml" xref="S6.p2.1.m1.1.1.3.1"></times><ci id="S6.p2.1.m1.1.1.3.2.cmml" xref="S6.p2.1.m1.1.1.3.2">𝑡</ci><ci id="S6.p2.1.m1.1.1.3.3.cmml" xref="S6.p2.1.m1.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">30^{th}</annotation></semantics></math><span id="S6.p2.2.4" class="ltx_text" style="font-size:90%;"> epoch, clearly in line with its set patience, and preventing an unnecessary continuation of the program through to the </span><math id="S6.p2.2.m2.1" class="ltx_Math" alttext="300^{th}" display="inline"><semantics id="S6.p2.2.m2.1a"><msup id="S6.p2.2.m2.1.1" xref="S6.p2.2.m2.1.1.cmml"><mn mathsize="90%" id="S6.p2.2.m2.1.1.2" xref="S6.p2.2.m2.1.1.2.cmml">300</mn><mrow id="S6.p2.2.m2.1.1.3" xref="S6.p2.2.m2.1.1.3.cmml"><mi mathsize="90%" id="S6.p2.2.m2.1.1.3.2" xref="S6.p2.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S6.p2.2.m2.1.1.3.1" xref="S6.p2.2.m2.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S6.p2.2.m2.1.1.3.3" xref="S6.p2.2.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S6.p2.2.m2.1b"><apply id="S6.p2.2.m2.1.1.cmml" xref="S6.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S6.p2.2.m2.1.1.1.cmml" xref="S6.p2.2.m2.1.1">superscript</csymbol><cn type="integer" id="S6.p2.2.m2.1.1.2.cmml" xref="S6.p2.2.m2.1.1.2">300</cn><apply id="S6.p2.2.m2.1.1.3.cmml" xref="S6.p2.2.m2.1.1.3"><times id="S6.p2.2.m2.1.1.3.1.cmml" xref="S6.p2.2.m2.1.1.3.1"></times><ci id="S6.p2.2.m2.1.1.3.2.cmml" xref="S6.p2.2.m2.1.1.3.2">𝑡</ci><ci id="S6.p2.2.m2.1.1.3.3.cmml" xref="S6.p2.2.m2.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.2.m2.1c">300^{th}</annotation></semantics></math><span id="S6.p2.2.5" class="ltx_text" style="font-size:90%;"> epoch.</span></p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text" style="font-size:90%;">The analysis also delved into the accuracy landscape. The validation accuracy of the model ended at 95.31%, with a maximum of 96.12%. Despite trailing marginally behind the training accuracy of 99.57%, the discrepancy did not suggest any severe risk of overfitting. Although direct comparisons can’t be made given the difference in this custom dataset, generalizations can be made. Building off a previous Table published </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S6.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S6.p3.1.4" class="ltx_text" style="font-size:90%;">, we can see that our model is on par or better than previous experiments, even with a much more complex dataset with more ragas included, as can be seen in Table </span><a href="#S4.T2" title="Table 2 ‣ 4 Dataset ‣ CARNATIC RAGA IDENTIFICATION SYSTEM USING RIGOROUS TIME-DELAY NEURAL NETWORK" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S6.p3.1.5" class="ltx_text" style="font-size:90%;"></span></p>
</div>
<figure id="S6.F3" class="ltx_figure"><img src="/html/2405.16000/assets/LastFigLoss.png" id="S6.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="334" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Model Training and Validation Loss</figcaption>
</figure>
<figure id="S6.F4" class="ltx_figure"><img src="/html/2405.16000/assets/LastFigAccuracy.png" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="334" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Model Training and Validation Accuracy</figcaption>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusions</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p"><span id="S7.p1.1.1" class="ltx_text" style="font-size:90%;">This study focuses on developing an effective system for identifying Carnatic ragas using advanced machine-learning techniques. By combining Time-Delay Neural Networks (TDNNs) and Long Short-Term Memory (LSTM) networks, we aim to accurately classify ragas based on their distinctive melodic patterns and rhythmic structures.</span></p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text" style="font-size:90%;">For this model, a carefully curated dataset of 676 recordings spanning 172 ragas, incorporating both concert and studio performances was utilized. Custom triangular filter banks for feature extraction and attention mechanisms to account for variations in shruti were employed to capture the nuanced characteristics of Carnatic music.</span></p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p"><span id="S7.p3.1.1" class="ltx_text" style="font-size:90%;">Experimentation with the model yielded promising results, with a validation accuracy of 95.31%. The implementation of EarlyStopping ensured efficient training and prevented overfitting. The research represents a significant advancement in computational musicology, providing a valuable tool for exploring and preserving the rich heritage of Carnatic music. The results achieve equal or improve on many other past forays in the field, while increasing the number of samples by more than 2 orders of magnitude, and increasing the ragas well beyond the number used in any prior experiments, which are usually measured on a select few popular ragas, or were restricted to the 72 Melakartas.</span></p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p"><span id="S7.p4.1.1" class="ltx_text" style="font-size:90%;">Moving forward, we plan to continue refining the methodology and expanding the dataset to encompass a broader range of ragas, shrutis, and styles. Ultimately, the goal is to continue pushing the boundaries of computational music analysis through the lens of Carnatic music and improve accuracy while increasing complexity.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
P. K. Srimani and Y. G. Parimala, “Artificial Neural Network approach to
develop unique Classification and Raga identification tools for Pattern
Recognition in Carnatic Music,” </span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AIP Conference Proceedings</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:90%;">, vol.
1414, no. 1, pp. 227–231, 12 2011.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
R. Sridhar and T. Geetha, “Swara indentification for south indian classical
music,” in </span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Information Technology, International Conference on</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:90%;">.   Los Alamitos, CA, USA: IEEE Computer
Society, 12 2006, pp. 143–144.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
R. Sridhar and T. V. Geetha, “Raga identification
of carnatic music for music information retrieval,”
</span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal of Recent Trends in
Engineering</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:90%;">, vol. 1, no. 1, pp. 571–574, 05 2009, copyright - Copyright
Academy Publisher May 2009; Last updated - 2011-05-15.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
T. Krishna and V. Ishwar, “Carnatic music: Svara, gamaka, motif and raga
identity,” in </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Serra X, Rao P, Murthy H, Bozkurt B, editors.
Proceedings of the 2nd CompMusic Workshop; 2012 Jul 12-13; Istanbul, Turkey.
Barcelona: Universitat Pompeu Fabra; 2012.</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:90%;">   Universitat Pompeu Fabra, 2012.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
P. Sriram. A carnatic music primer. [Online]. Available:
https://sanskritdocuments.org/english/carnatic.pdf
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
S. Gulati, J. Serrà Julià, K. K. Ganguli, S. Sentürk, and X. Serra,
“Time-delayed melody surfaces for rāga recognition,” in </span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Devaney
J, Mandel MI, Turnbull D, Tzanetakis G, editors. ISMIR 2016. Proceedings of
the 17th International Society for Music Information Retrieval Conference;
2016 Aug 7-11; New York City (NY).[Canada]: ISMIR; 2016. p. 751-7.</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:90%;">   International Society for Music Information
Retrieval (ISMIR), 2016.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
P. Chordia and S. Şentürk, “Joint recognition of raag and tonic in north
indian music,” </span><em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Music Journal</em><span id="bib.bib7.3.3" class="ltx_text" style="font-size:90%;">, vol. 37, no. 3, pp. 82–98,
2013.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
P. R. Gopala Krishna Koduri, Sankalp Gulati and X. Serra, “Rāga recognition
based on pitch distribution methods,” </span><em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of New Music Research</em><span id="bib.bib8.3.3" class="ltx_text" style="font-size:90%;">,
vol. 41, no. 4, pp. 337–350, 2012.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
A. Krishna, P. Rajkumar, S. Pulliyakode, and M. John, “Identification of
carnatic raagas using hidden markov models,” 01 2011.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
K. Drossos, S. Mimilakis, D. Serdyuk, G. Schuller, T. Virtanen, and Y. Bengio,
“Mad twinnet: Masker-denoiser architecture with twin networks for monaural
sound source separation,” 02 2018.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
M. Venkataraman, P. Boominathan, and A. Nallamuthu, “Frequency range measures
in carnatic singers,” </span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Voice</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:90%;">, vol. 36, no. 5, pp.
732.e1–732.e8, 2022.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
training by reducing internal covariate shift,” 2015.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
V. Nair and G. E. Hinton, “Rectified linear units improve restricted boltzmann
machines,” in </span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 27th international conference on
machine learning (ICML-10)</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:90%;">, 2010, pp. 807–814.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
H. Ide and T. Kurita, “Improvement of learning for cnn with relu activation by
sparse regularization,” in </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 International Joint Conference on
Neural Networks (IJCNN)</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:90%;">, 2017, pp. 2684–2691.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Y.-L. Boureau, J. Ponce, and Y. LeCun, “A theoretical analysis of feature
pooling in visual recognition,” in </span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 27th
international conference on machine learning (ICML-10)</em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:90%;">, 2010, pp. 111–118.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
“Dropout: a simple way to prevent neural networks from overfitting,”
</span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The journal of machine learning research</em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:90%;">, vol. 15, no. 1, pp.
1929–1958, 2014.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Ragas. [Online]. Available: https://www.karnatik.com/ragasa.shtml
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
B. Thoshkahna, M. Müller, V. Kulkarni, and N. Jiang, “Novel audio features
for capturing tempo salience in music recordings,” in </span><em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2015 IEEE
international conference on acoustics, speech and signal processing
(ICASSP)</em><span id="bib.bib18.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2015, pp. 181–185.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
A. Gholamy, V. Kreinovich, and O. Kosheleva, “Why 70/30 or 80/20 relation
between training and testing sets: A pedagogical explanation,” 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
R. T. Pillai and S. P. Mahajan, “Automatic carnatic raga identification using
octave mapping and note quantization,” in </span><em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 International
Conference on Communication and Signal Processing (ICCSP)</em><span id="bib.bib20.3.3" class="ltx_text" style="font-size:90%;">, 2017, pp.
0645–0649.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
S. Shetty and K. Achary, “Raga mining of indian music by extracting
arohana-avarohana pattern,” </span><em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal of Recent Trends in
Engineering</em><span id="bib.bib21.3.3" class="ltx_text" style="font-size:90%;">, vol. 1, no. 1, p. 362, 2009.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
H. Ranjani, S. Arthi, and T. Sreenivas, “Carnatic music analysis: Shadja,
swara identification and raga verification in alapana using stochastic
models,” in </span><em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2011 IEEE Workshop on Applications of Signal Processing to
Audio and Acoustics (WASPAA)</em><span id="bib.bib22.3.3" class="ltx_text" style="font-size:90%;">.   IEEE,
2011, pp. 29–32.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
G. K. Koduri, S. Gulati, and P. Rao, “A survey of raaga recognition techniques
and improvements to the state-of-the-art,” </span><em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Sound and Music Computing</em><span id="bib.bib23.3.3" class="ltx_text" style="font-size:90%;">,
vol. 38, pp. 39–41, 2011.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
K. S. R. S. Samsekai Manjabhat, Shashidhar G. Koolagudi and P. B. Ramteke,
“Raga and tonic identification in carnatic music,” </span><em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of New
Music Research</em><span id="bib.bib24.3.3" class="ltx_text" style="font-size:90%;">, vol. 46, no. 3, pp. 229–245, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
P. Chinthapenta, “Machine learning for raga classification in indian classical
music,” Undergraduate Research Scholars Program, 2019. [Online]. Available:
https://hdl.handle.net/1969.1/194515
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.15999" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.16000" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.16000">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.16000" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.16001" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 18:13:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
