<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.16547] Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)</title><meta property="og:description" content="This paper is concerned with automatic continuous speech recognition using trainable systems. The aim of this work is to build acoustic models for spoken Swedish. This is done employing hidden Markov models and using t…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.16547">

<!--Generated on Sun May  5 20:58:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Developing Acoustic Models for Automatic Speech Recognition in Swedish<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (<a target="_blank" href="http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html</a>)</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Giampiero Salvi
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text" style="font-size:90%;">Kungliga Tekniska Högskolan (KTH)
<br class="ltx_break">Speech Music and Hearing Department (TMH)
<br class="ltx_break">SE-100 44 Stockholm, Sweden
<br class="ltx_break">giampi@speech.kth.se
<br class="ltx_break"><a target="_blank" href="http://www.speech.kth.se/~giampi" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.speech.kth.se/~giampi</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p"><span id="id2.id1.1" class="ltx_text">This paper is concerned with automatic continuous speech recognition using trainable systems. The aim of this work is to build acoustic models for spoken Swedish. This is done employing hidden Markov models and using the SpeechDat database to train their parameters. Acoustic modeling has been worked out at a phonetic level, allowing general speech recognition applications, even though a simplified task (digits and natural number recognition) has been considered for model evaluation. Different kinds of phone models have been tested, including context independent models and two variations of context dependent models. Furthermore many experiments have been done with bigram language models to tune some of the system parameters. System performance over various speaker subsets with different sex, age and dialect has also been examined. Results are compared to previous similar studies showing a remarkable improvement.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The field of speech signal analysis has been in the center of attention for many years because of the many possible applications, but also because, with the many disciplines involved in it, it represents a challenge for many scientists. The applications, related mostly to telecommunication problems, have as a goal the possibility for human beings to exchange information with other human beeings, or with automatic systems, in the most natural and efficient way: speaking. In this context the speech recognition enterprise is probably the most ambitious. Its goal is to build “intelligent” machines that can “hear” and “understand” spoken information, in spite of the natural ambiguity and complexity of natural languages. In thirty years, improvements that could not even be thought of before have been worked out, but still the objective of a robust machine, able to recognize different speakers in different situations, is a very difficult task. The difficulty of the problem increases if we try to build systems for a large set of speakers and for a generic task (large vocabulary).</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The thesis summarized in this article describes an attempt to build robust speaker-independent acoustic models for spoken Swedish over the telephone line. A collection of utterances spoken by 1000 speakers (the SpeechDat database) has been used as a statistical base from which models have been developed. The recognition task considered includes a small vocabulary (86 words), but continuous speech is accepted. Furthermore the model structure is chosen with regard to the possibility to apply these models in different contexts and tasks. To evaluate this flexibility the models have been tested on another database from the Waxholm project (vocabulary of 635 words). Different applications are possible for this kind of models: they have already been employed in the recognition part of a complex dialog systems (August project <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>), or in a speaker verification system (TVIT project <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>). This article contains a documentation of the steps that lead to the creation and development of the acoustic models.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Speech material</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Model set building and testing are based on a database developed in the SpeechDat project. This database is a subset of the 5000 speakers Swedish database, containing recordings of 1000 subjects. For each speaker (session) a variety of different items are provided for different tasks.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Only a part of them has been used in training and testing the models.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Subjects</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Speakers are selected randomly within a population of interest including all possible types of speakers. Sweden has been divided into seven main dialect areas (“South Swedish”, “Gotheburg, west and middle Swedish”, “East, middle Swedish”, “Swedish as spoken in Gotland”, “Swedish as spoken in Bergslagen”, “Swedish as spoken in Norrland”, “Swedish as spoken in Finland”) . This division does not regards genuine dialects, but rather the spoken language used by most people in the areas defined.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Recordings</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Speech files are recorded through the telephone line and stored in an 8bit, 8kHz, A-law format. For each audio file, an ASCII label file is provided, containing information about sex, age, accent, region, environment, telephone type, and a transcription of the uttered sentence or word.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Items</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The items used in model training contain for each speaker:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">9 phonetically rich sentences (S1-S9 in SpeechDat symbols)</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">4 phonetically rich words (W1-W4)</p>
</div>
</li>
</ul>
<p id="S2.SS3.p1.2" class="ltx_p">while the items used for development tests and evaluation tests are:</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p">1 sequence of 10 isolated digits (B1)</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p">1 sheet number (5+ digits) (C1)</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p">1 telephone number (9-11 digits) (C2)</p>
</div>
</li>
<li id="S2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i4.p1" class="ltx_para">
<p id="S2.I2.i4.p1.1" class="ltx_p">1 credit card number (16 digits) (C3)</p>
</div>
</li>
<li id="S2.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i5.p1" class="ltx_para">
<p id="S2.I2.i5.p1.1" class="ltx_p">1 PIN code (6 digits) (C4)</p>
</div>
</li>
<li id="S2.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i6.p1" class="ltx_para">
<p id="S2.I2.i6.p1.1" class="ltx_p">1 isolated digit (I1)</p>
</div>
</li>
<li id="S2.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i7.p1" class="ltx_para">
<p id="S2.I2.i7.p1.1" class="ltx_p">1 currency money amount (M1)</p>
</div>
</li>
<li id="S2.I2.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i8.p1" class="ltx_para">
<p id="S2.I2.i8.p1.1" class="ltx_p">1 natural number (N1)</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Noise transcriptions</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Noise occurrence is transcribed in the label files for the following cases:</p>
<dl id="S2.I3" class="ltx_description">
<dt id="S2.I3.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S2.I3.ix1.1.1.1" class="ltx_text ltx_font_bold">filled pause</span></span></dt>
<dd class="ltx_item">
<div id="S2.I3.ix1.p1" class="ltx_para">
<p id="S2.I3.ix1.p1.1" class="ltx_p">([fil] in the SpeechDat symbology): is the sound produced in case of hesitation.</p>
</div>
</dd>
<dt id="S2.I3.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S2.I3.ix2.1.1.1" class="ltx_text ltx_font_bold">speaker noise</span></span></dt>
<dd class="ltx_item">
<div id="S2.I3.ix2.p1" class="ltx_para">
<p id="S2.I3.ix2.p1.1" class="ltx_p">([spk]): every time a speaker produces a sound not directly related to a phoneme generation.</p>
</div>
</dd>
<dt id="S2.I3.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S2.I3.ix3.1.1.1" class="ltx_text ltx_font_bold">stationary noise</span></span></dt>
<dd class="ltx_item">
<div id="S2.I3.ix3.p1" class="ltx_para">
<p id="S2.I3.ix3.p1.1" class="ltx_p">([sta]): environmental noise which extends during the whole utterance.</p>
</div>
</dd>
<dt id="S2.I3.ix4" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S2.I3.ix4.1.1.1" class="ltx_text ltx_font_bold">intermittent noise</span></span></dt>
<dd class="ltx_item">
<div id="S2.I3.ix4.p1" class="ltx_para">
<p id="S2.I3.ix4.p1.1" class="ltx_p">([int]): transient environmental noise extending in a few milliseconds and possibly repeating more than once.</p>
</div>
</dd>
<dt id="S2.I3.ix5" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S2.I3.ix5.1.1.1" class="ltx_text ltx_font_bold">mispronounced word</span></span></dt>
<dd class="ltx_item">
<div id="S2.I3.ix5.p1" class="ltx_para">
<p id="S2.I3.ix5.p1.1" class="ltx_p">(*word)</p>
</div>
</dd>
<dt id="S2.I3.ix6" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S2.I3.ix6.1.1.1" class="ltx_text ltx_font_bold">unintelligible speech</span></span></dt>
<dd class="ltx_item">
<div id="S2.I3.ix6.p1" class="ltx_para">
<p id="S2.I3.ix6.p1.1" class="ltx_p">(**)</p>
</div>
</dd>
<dt id="S2.I3.ix7" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S2.I3.ix7.1.1.1" class="ltx_text ltx_font_bold">truncation</span></span></dt>
<dd class="ltx_item">
<div id="S2.I3.ix7.p1" class="ltx_para">
<p id="S2.I3.ix7.p1.5" class="ltx_p">(<math id="S2.I3.ix7.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.I3.ix7.p1.1.m1.1a"><mo id="S2.I3.ix7.p1.1.m1.1.1" xref="S2.I3.ix7.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.I3.ix7.p1.1.m1.1b"><csymbol cd="latexml" id="S2.I3.ix7.p1.1.m1.1.1.cmml" xref="S2.I3.ix7.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.ix7.p1.1.m1.1c">\sim</annotation></semantics></math>): <math id="S2.I3.ix7.p1.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.I3.ix7.p1.2.m2.1a"><mo id="S2.I3.ix7.p1.2.m2.1.1" xref="S2.I3.ix7.p1.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.I3.ix7.p1.2.m2.1b"><csymbol cd="latexml" id="S2.I3.ix7.p1.2.m2.1.1.cmml" xref="S2.I3.ix7.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.ix7.p1.2.m2.1c">\sim</annotation></semantics></math>utterance, utterance<math id="S2.I3.ix7.p1.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.I3.ix7.p1.3.m3.1a"><mo id="S2.I3.ix7.p1.3.m3.1.1" xref="S2.I3.ix7.p1.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.I3.ix7.p1.3.m3.1b"><csymbol cd="latexml" id="S2.I3.ix7.p1.3.m3.1.1.cmml" xref="S2.I3.ix7.p1.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.ix7.p1.3.m3.1c">\sim</annotation></semantics></math>, <math id="S2.I3.ix7.p1.4.m4.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.I3.ix7.p1.4.m4.1a"><mo id="S2.I3.ix7.p1.4.m4.1.1" xref="S2.I3.ix7.p1.4.m4.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.I3.ix7.p1.4.m4.1b"><csymbol cd="latexml" id="S2.I3.ix7.p1.4.m4.1.1.cmml" xref="S2.I3.ix7.p1.4.m4.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.ix7.p1.4.m4.1c">\sim</annotation></semantics></math>utterance<math id="S2.I3.ix7.p1.5.m5.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.I3.ix7.p1.5.m5.1a"><mo id="S2.I3.ix7.p1.5.m5.1.1" xref="S2.I3.ix7.p1.5.m5.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.I3.ix7.p1.5.m5.1b"><csymbol cd="latexml" id="S2.I3.ix7.p1.5.m5.1.1.cmml" xref="S2.I3.ix7.p1.5.m5.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.ix7.p1.5.m5.1c">\sim</annotation></semantics></math>.</p>
</div>
</dd>
</dl>
<p id="S2.SS4.p1.2" class="ltx_p">For all these symbols a particular model must be introduced.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Speaker subsets</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">In the SpeechDat documentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> a way to design evaluation tests is proposed. For 1000 speaker databases a set of 200 speakers should be reserved for these tests, and the other 800 speakers should be used for <em id="S2.SS5.p1.1.1" class="ltx_emph ltx_font_italic">training</em>. In our case training includes many experiments, and in order to compare them, <em id="S2.SS5.p1.1.2" class="ltx_emph ltx_font_italic">development tests</em> are needed before the final <em id="S2.SS5.p1.1.3" class="ltx_emph ltx_font_italic">evaluation test</em>. For this reason a subset of 50 speakers was extracted from the training set.</p>
</div>
<div id="S2.SS5.p2" class="ltx_para">
<p id="S2.SS5.p2.1" class="ltx_p">In order to maintain the same balance as the full FDB database, speakers for each subset are selected using a controlled random selection algorithm: speakers are divided into different cells with regard on region and gender. Then an opportune number of speakers is randomly selected from each cell to form the required subset.</p>
</div>
</section>
<section id="S2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Statistics</h3>

<div id="S2.SS6.p1" class="ltx_para">
<p id="S2.SS6.p1.1" class="ltx_p">Results from model tests are presented as mean error rate figures over speakers in the development (50 speakers) and evaluation (200 speakers) subsets respectively. To give an idea of the consistency of these results, some statistics on the database are given in this section.</p>
</div>
<div id="S2.SS6.p2" class="ltx_para">
<p id="S2.SS6.p2.1" class="ltx_p">In Table <a href="#S2.T1" title="Table 1 ‣ 2.6 Statistics ‣ 2 Speech material ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, speakers are divided according to gender and age, while in Table <a href="#S2.T2" title="Table 2 ‣ 2.6 Statistics ‣ 2 Speech material ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> the distinction is based on dialect regions.</p>
</div>
<div id="S2.SS6.p3" class="ltx_para">
<p id="S2.SS6.p3.1" class="ltx_p">These distinctions will be particularly useful in Section <a href="#S5.SS2" title="5.2 Results per individual speaker ‣ 5 Results ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>, where per speaker results are discussed. Tables show how the balance in the full database is preserved in each subset. They also show how some groups of speakers are not well represented in the evaluation subset. For example no Finnish speakers nor speakers from Gotland are in this subset. Furthermore speakers from Bergslagen do not represent a good statistical base, a fact to consider when discussing results. The same can be said about young and old speakers in the age distinction.</p>
</div>
<div id="S2.SS6.p4" class="ltx_para">
<p id="S2.SS6.p4.1" class="ltx_p">To give an idea of the consistency of results, in Section <a href="#S5.SS2" title="5.2 Results per individual speaker ‣ 5 Results ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> the standard deviation is reported in addition to the mean values in the case of results for speaker subsets.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.4.5.1" class="ltx_tr">
<th id="S2.T1.4.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">Sex</th>
<th id="S2.T1.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Age</th>
<th id="S2.T1.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Train</th>
<th id="S2.T1.4.5.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Development</th>
<th id="S2.T1.4.5.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Evaluation</th>
<th id="S2.T1.4.5.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Total</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">F</th>
<th id="S2.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">young (<math id="S2.T1.1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.m1.1b"><lt id="S2.T1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.m1.1c">&lt;</annotation></semantics></math>16)</th>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21</td>
<td id="S2.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S2.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5</td>
<td id="S2.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27</td>
</tr>
<tr id="S2.T1.4.6.1" class="ltx_tr">
<th id="S2.T1.4.6.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S2.T1.4.6.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">middle</th>
<td id="S2.T1.4.6.1.3" class="ltx_td ltx_align_center ltx_border_r">396</td>
<td id="S2.T1.4.6.1.4" class="ltx_td ltx_align_center ltx_border_r">26</td>
<td id="S2.T1.4.6.1.5" class="ltx_td ltx_align_center ltx_border_r">106</td>
<td id="S2.T1.4.6.1.6" class="ltx_td ltx_align_center ltx_border_r">528</td>
</tr>
<tr id="S2.T1.2.2" class="ltx_tr">
<th id="S2.T1.2.2.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S2.T1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">old (<math id="S2.T1.2.2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T1.2.2.1.m1.1a"><mo id="S2.T1.2.2.1.m1.1.1" xref="S2.T1.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.1.m1.1b"><gt id="S2.T1.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.1.m1.1c">&gt;</annotation></semantics></math>65)</th>
<td id="S2.T1.2.2.3" class="ltx_td ltx_align_center ltx_border_r">16</td>
<td id="S2.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_r">1</td>
<td id="S2.T1.2.2.5" class="ltx_td ltx_align_center ltx_border_r">5</td>
<td id="S2.T1.2.2.6" class="ltx_td ltx_align_center ltx_border_r">22</td>
</tr>
<tr id="S2.T1.4.7.2" class="ltx_tr">
<th id="S2.T1.4.7.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S2.T1.4.7.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">tot</th>
<td id="S2.T1.4.7.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">433</td>
<td id="S2.T1.4.7.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28</td>
<td id="S2.T1.4.7.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">116</td>
<td id="S2.T1.4.7.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">577</td>
</tr>
<tr id="S2.T1.3.3" class="ltx_tr">
<th id="S2.T1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">M</th>
<th id="S2.T1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">young (<math id="S2.T1.3.3.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T1.3.3.1.m1.1a"><mo id="S2.T1.3.3.1.m1.1.1" xref="S2.T1.3.3.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.1.m1.1b"><lt id="S2.T1.3.3.1.m1.1.1.cmml" xref="S2.T1.3.3.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.1.m1.1c">&lt;</annotation></semantics></math>16)</th>
<td id="S2.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14</td>
<td id="S2.T1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S2.T1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S2.T1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16</td>
</tr>
<tr id="S2.T1.4.8.3" class="ltx_tr">
<th id="S2.T1.4.8.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S2.T1.4.8.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">middle</th>
<td id="S2.T1.4.8.3.3" class="ltx_td ltx_align_center ltx_border_r">284</td>
<td id="S2.T1.4.8.3.4" class="ltx_td ltx_align_center ltx_border_r">20</td>
<td id="S2.T1.4.8.3.5" class="ltx_td ltx_align_center ltx_border_r">80</td>
<td id="S2.T1.4.8.3.6" class="ltx_td ltx_align_center ltx_border_r">384</td>
</tr>
<tr id="S2.T1.4.4" class="ltx_tr">
<th id="S2.T1.4.4.2" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r"></th>
<th id="S2.T1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">old (<math id="S2.T1.4.4.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.T1.4.4.1.m1.1a"><mo id="S2.T1.4.4.1.m1.1.1" xref="S2.T1.4.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.1.m1.1b"><gt id="S2.T1.4.4.1.m1.1.1.cmml" xref="S2.T1.4.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.1.m1.1c">&gt;</annotation></semantics></math>65)</th>
<td id="S2.T1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">19</td>
<td id="S2.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_r">1</td>
<td id="S2.T1.4.4.5" class="ltx_td ltx_align_center ltx_border_r">3</td>
<td id="S2.T1.4.4.6" class="ltx_td ltx_align_center ltx_border_r">23</td>
</tr>
<tr id="S2.T1.4.9.4" class="ltx_tr">
<th id="S2.T1.4.9.4.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S2.T1.4.9.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">tot</th>
<td id="S2.T1.4.9.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">317</td>
<td id="S2.T1.4.9.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">22</td>
<td id="S2.T1.4.9.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">84</td>
<td id="S2.T1.4.9.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">423</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Number of speakers in three age ranges for female (F) speakers and male (M) speakers respectively in the Training, Development and Evaluation subsets</figcaption>
</figure>
<figure id="S2.T2" class="ltx_table">
<table id="S2.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.1.1.1" class="ltx_tr">
<th id="S2.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">Region</th>
<th id="S2.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Train</th>
<th id="S2.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Development</th>
<th id="S2.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Evaluation</th>
<th id="S2.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Total</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.1.2.1" class="ltx_tr">
<th id="S2.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Bergslagen</th>
<td id="S2.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24</td>
<td id="S2.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3</td>
<td id="S2.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S2.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31</td>
</tr>
<tr id="S2.T2.1.3.2" class="ltx_tr">
<th id="S2.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">EastMiddle</th>
<td id="S2.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">281</td>
<td id="S2.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">9</td>
<td id="S2.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">85</td>
<td id="S2.T2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">375</td>
</tr>
<tr id="S2.T2.1.4.3" class="ltx_tr">
<th id="S2.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Gothenburg</th>
<td id="S2.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">131</td>
<td id="S2.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">12</td>
<td id="S2.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">29</td>
<td id="S2.T2.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">172</td>
</tr>
<tr id="S2.T2.1.5.4" class="ltx_tr">
<th id="S2.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Norrland</th>
<td id="S2.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">164</td>
<td id="S2.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">11</td>
<td id="S2.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">46</td>
<td id="S2.T2.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r">221</td>
</tr>
<tr id="S2.T2.1.6.5" class="ltx_tr">
<th id="S2.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">South</th>
<td id="S2.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">112</td>
<td id="S2.T2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">9</td>
<td id="S2.T2.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">28</td>
<td id="S2.T2.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r">149</td>
</tr>
<tr id="S2.T2.1.7.6" class="ltx_tr">
<th id="S2.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Finnish</th>
<td id="S2.T2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">3</td>
<td id="S2.T2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S2.T2.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S2.T2.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r">5</td>
</tr>
<tr id="S2.T2.1.8.7" class="ltx_tr">
<th id="S2.T2.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Gotland</th>
<td id="S2.T2.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r">6</td>
<td id="S2.T2.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S2.T2.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S2.T2.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r">8</td>
</tr>
<tr id="S2.T2.1.9.8" class="ltx_tr">
<th id="S2.T2.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">Other</th>
<td id="S2.T2.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r">29</td>
<td id="S2.T2.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S2.T2.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r">8</td>
<td id="S2.T2.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r">39</td>
</tr>
<tr id="S2.T2.1.10.9" class="ltx_tr">
<th id="S2.T2.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_l ltx_border_r ltx_border_t">Total</th>
<td id="S2.T2.1.10.9.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">750</td>
<td id="S2.T2.1.10.9.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">50</td>
<td id="S2.T2.1.10.9.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">200</td>
<td id="S2.T2.1.10.9.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">1000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Number of speakers belonging to different dialectal areas in the Training, Development and Evaluation subsets</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model sets</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Model set building is based on hidden Markov models (HMMs). Models are employed for “target” speech including up to 46 Swedish phonemes, and for “non-target” speech including four noise models, one silence model and a word boundary.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2404.16547/assets/figures/models.png" id="S3.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="208" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Model topology for different applications</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Target speech</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">All phones, except plosives, are modeled by a three emitting state HMM as depicted in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Model sets ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a). The choice of topology in HMM applications is often made in the attempt to obtain a good balance between forward transitions and transitions back to the same state. This balance seems to be more important than the correspondence between different states and different parts of the same phoneme realization. This is true for all the steady sounds, such as vowels, fricatives, nasals and so on. The sound produced in the case of plosives (<span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">B</span>, <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_bold">D</span>, <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_bold">2D</span>, <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_bold">G</span>, <span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_bold">K</span>, <span id="S3.SS1.p1.1.6" class="ltx_text ltx_font_bold">P</span>, <span id="S3.SS1.p1.1.7" class="ltx_text ltx_font_bold">T</span>, <span id="S3.SS1.p1.1.8" class="ltx_text ltx_font_bold">2T</span>), on the other hand, has an important temporal structure, and states in the corresponding HMM should match each different acoustic segment. In a previous work (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>), these phonemes have been modeled by a concatenation of two HMMs with three emitting states. In our opinion this method can be inaccurate because of the fast time evolution of plosives. For this reason each plosive is modeled by an HMM with four emitting state (Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Model sets ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b)).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Non-target speech</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Noise models are: <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">extral</span> (SpeechDat mark: [spk]) for speaker noise such as lips smack, <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_bold">Öh</span> ([fil]) for hesitation between a word and another, <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_bold">noise</span> ([int]) for non speaker intermittent noise typical of the telephone lines. It is not possible to use an HMM to soak up the stationary noise ([sta]) because this disturbance is extended to the whole utterance. The acoustic characteristics of this noise are in part held by the models parameters which are estimated both on “good” and noisy files.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">sil</span> model is a three state HMM (Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Model sets ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a)) trained on silence frames of the utterance. In the recognition task it is used at the beginning or at the end of a sentence in the attempt to model the extra time in the recording session over the spoken utterance.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The <span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">#</span> model is employed for word boundaries: it has a symbolic use, representing the boundaries between words at the phone level and allowing different context expansion methods. A second reason for its use is to model the silence between one word and another. In continuous speech these silence segments can be very short. For this reason <span id="S3.SS2.p3.1.2" class="ltx_text ltx_font_bold">#</span> has only one emitting state as depicted in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Model sets ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(d). This state is tied (shared parameters) with the central state of the <span id="S3.SS2.p3.1.3" class="ltx_text ltx_font_bold">sil</span> model. Furthermore a direct transition from the first (non emitting) state to the last (non emitting) state is allowed in the case that words are connected without any pause. Finally the <span id="S3.SS2.p3.1.4" class="ltx_text ltx_font_bold">garbage</span>
model (Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Model sets ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(c)) is created to allow using files containing pronunciation errors (* in SpeechDat), unintelligible speech (**) and truncation (<math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mo id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><csymbol cd="latexml" id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\sim</annotation></semantics></math>). This model is not used during the recognition phase because only “clean” files are used in this case.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Retroflex allophones</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">During this work the lexicon file has been changed, to include <em id="S3.SS3.p1.1.1" class="ltx_emph ltx_font_italic">retroflex allophones </em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> in a first time considered too rare. For this reason in each experiment two model sets have been tested including (<em id="S3.SS3.p1.1.2" class="ltx_emph ltx_font_italic">new lexicon</em>) or not (<em id="S3.SS3.p1.1.3" class="ltx_emph ltx_font_italic">old lexicon</em>) models for these sounds.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Training</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Training consists of applying an embedded version of the Baum-Welsh algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. For every speech file (output sequence) a label file with a phoneme transcription is loaded and used to create an HMM for the whole utterance concatenating all models corresponding to the sequence of labels.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The <span id="S4.p2.1.1" class="ltx_text ltx_font_bold">garbage </span> model is first trained apart on generic speech and then included in the model set.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Development tests</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">During training development tests are an important tool in the attempt to compare different experiments and to suggest new possible ways of improvement. These experiments consist of a word level recognition on a small subset of speakers reserved for this task. The number of speakers (50) involved in these tests is too low to guarantee a statistical consistency of the results, nevertheless these tests can be used for a comparative evaluation of different model sets. When scoring the system, two alternative parameters are taken into account: correct words and accuracy. Their definition depends on the the algorithm used to align the output of the system to the reference transcription. This algorithm is an optimal string match based on dynamic programming <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Once the optimal alignment has been found, the number of substitution errors (S), deletion errors (D) and insertion errors (I) can be calculated.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Correct words (PC) are then defined as:</p>
<p id="S4.SS1.p2.2" class="ltx_p ltx_align_center">PC = (N-D-S)/N 100%</p>
<p id="S4.SS1.p2.3" class="ltx_p">While the definition for accuracy (A) is:</p>
<p id="S4.SS1.p2.4" class="ltx_p ltx_align_center">A = (N-D-S-I)/N 100%</p>
<p id="S4.SS1.p2.5" class="ltx_p">Files chosen for evaluation contains isolated digits, connected digits and natural numbers.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In the following sections development tests results (50 speakers) are presented for different experiments.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Monophones</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">Nine iterations of the Baum-Welch re-estimation have been performed. As showed in Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1.1 Monophones ‣ 4.1 Development tests ‣ 4 Training ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, recognition performance in terms of word accuracy is improved until the fifth iteration in the case of old lexicon models. Further iterations can be avoided since they don’t bring better results. In some cases, as we will see, the performance is even reduced. This is because, with too many iterations, HMM parameters tend to fit too well the training data (and hence the training speaker characteristics) and have no more freedom to generalize to new speakers (evaluation data). In terms of Gaussian parameters it means that the variances tend to be too low (narrow Gaussian shape) to include new speaker variations.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2404.16547/assets/figures/msbmono.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="226" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Development test (50 speakers). Monophones, 1, 2, 4, 8 Gaussian terms,
old lexicon (blue line) and new lexicon (red line)</figcaption>
</figure>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Triphones</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">The construction of context dependent models has been shown to be a good alternative method in the attempt to improving accuracy: Two expansion methods have been tested:</p>
<dl id="S4.I1" class="ltx_description">
<dt id="S4.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S4.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">within-word context expansion</span></span></dt>
<dd class="ltx_item">
<div id="S4.I1.ix1.p1" class="ltx_para">
<p id="S4.I1.ix1.p1.1" class="ltx_p">in which phonemes at word boundaries are expanded as diphones. This method is easier to apply in the recognition phase because models created during the network expansion depend only on the words in the dictionary and not on their sequence in the sentence hypothesis. This means that avoiding unseen context dependent models is easy, especially if the words included in the recognition task are also present in the training data.</p>
</div>
</dd>
<dt id="S4.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S4.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">cross word context expansion</span></span></dt>
<dd class="ltx_item">
<div id="S4.I1.ix2.p1" class="ltx_para">
<p id="S4.I1.ix2.p1.1" class="ltx_p">This method results in the generation of a lower number of diphones (only phonemes at sentence boundaries are expanded as diphones). On the other hand, the number of triphone occurrences is increased, rising the context information, and sometimes the number of occurrence for a single model.</p>
</div>
</dd>
</dl>
<p id="S4.SS1.SSS2.p1.2" class="ltx_p">Model sets obtained has been subjected to a first iteration of the Baum-Welsh algorithm. For most models the training data was not sufficient, thus a <em id="S4.SS1.SSS2.p1.2.1" class="ltx_emph ltx_font_italic">tree clustering</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> procedure has been applied. Several threshold values have been used in order to find a good trade-off between the number of states in the model set (model variability) and size of available data (estimation robustness).</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">Model sets obtained with different threshold values have been trained separately. Then these models have been tested to find the optimal value for the threshold. Results are shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1.2 Triphones ‣ 4.1 Development tests ‣ 4 Training ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F3.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/2404.16547/assets/figures/tbthw.png" id="S4.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="452" alt="Refer to caption">
<p id="S4.F3.1.1" class="ltx_p ltx_align_center">a</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F3.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/2404.16547/assets/figures/tbthc.png" id="S4.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="452" alt="Refer to caption">
<p id="S4.F3.2.1" class="ltx_p ltx_align_center">b</p>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>a: Tree clustering threshold optimization (Accuracy/# of states): within word context expanded models, old lexicon (blue line), new lexicon (red line). b: Tree clustering threshold optimization (Accuracy/# of states): cross word context expanded models, old lexicon (blue line), new lexicon (red line)</figcaption>
</figure>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<p id="S4.SS1.SSS2.p3.1" class="ltx_p">The optimal model set contains 2020 states in the case of within-word context expansion and the old lexicon (tb) and 2440 states for the new lexicon (ntb); and 1758 and 2862 respectively for old and new lexicon (tnb and ntnb) and cross-word context expansion. Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1.2 Triphones ‣ 4.1 Development tests ‣ 4 Training ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> also shows that models not including retroflex allophones perform better also in the case of context dependent modeling. The best models have been developed by adding Gaussian mixture terms to the output probability distributions.</p>
</div>
<div id="S4.SS1.SSS2.p4" class="ltx_para">
<p id="S4.SS1.SSS2.p4.1" class="ltx_p">Results obtained with this method are shown in Figures <a href="#S4.F4" title="Figure 4 ‣ 4.1.2 Triphones ‣ 4.1 Development tests ‣ 4 Training ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> a and b, respectively for within-word and cross-word context expansion. Figures show nine iterations and models with 2, 4 and 8 Gaussian distributions per mixture. As in the case of monophones, the difference in performance between models including or excluding retroflex allophones is reduced as the mixture size is increased. Within-word models perform better than cross-word models, probably because the number of contexts is lower (6770 models instead of 9681), allowing a more robust parameter estimation. Anyway these results are affected by the specificity of the task. The advantage of using cross word context expansion would probably be higher in a generic speech recognition task in which an higher number of words is involved and sentences are uttered in a more continuous way.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F4.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/2404.16547/assets/figures/msbtw.png" id="S4.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="453" alt="Refer to caption">
<p id="S4.F4.1.1" class="ltx_p ltx_align_center">a</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S4.F4.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/2404.16547/assets/figures/msbtc.png" id="S4.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="454" alt="Refer to caption">
<p id="S4.F4.2.1" class="ltx_p ltx_align_center">b</p>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>a: Accuracy/iterations 1, 2, 4, 8 Gaussian terms: within word context expansion, old lexicon (blue line) and new lexicon (red line). b: Accuracy/iterations 1, 2, 4, 8 Gaussian terms: cross word context expansion, old lexicon (blue line) and new lexicon (red line)</figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Model sets selected according to the development tests described in the previous section have been tested on the 200 speakers subset to obtain more reliable results. This section presents overall results and per individual speaker results obtained on the evaluating subset.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Overall results</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">From the development tests (Section <a href="#S4.SS1" title="4.1 Development tests ‣ 4 Training ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>) the best models were selected and tested on the evaluation material. Results obtained with these tests are shown in Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Overall results ‣ 5 Results ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Experiment</th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">1 mix</th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">2 mix</th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">4 mix</th>
<th id="S5.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">8 mix</th>
<th id="S5.T3.1.1.1.6" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
<th id="S5.T3.1.1.1.7" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
<th id="S5.T3.1.1.1.8" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
<th id="S5.T3.1.1.1.9" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
</tr>
<tr id="S5.T3.1.2.2" class="ltx_tr">
<th id="S5.T3.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S5.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Corr</th>
<th id="S5.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Acc</th>
<th id="S5.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Corr</th>
<th id="S5.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Acc</th>
<th id="S5.T3.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Corr</th>
<th id="S5.T3.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Acc</th>
<th id="S5.T3.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Corr</th>
<th id="S5.T3.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Acc</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.3.1" class="ltx_tr">
<td id="S5.T3.1.3.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">mb</td>
<td id="S5.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.4</td>
<td id="S5.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.4</td>
<td id="S5.T3.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.6</td>
<td id="S5.T3.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">69.5</td>
<td id="S5.T3.1.3.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75.6</td>
<td id="S5.T3.1.3.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.3</td>
<td id="S5.T3.1.3.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.9</td>
<td id="S5.T3.1.3.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.0</td>
</tr>
<tr id="S5.T3.1.4.2" class="ltx_tr">
<td id="S5.T3.1.4.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">nmb</td>
<td id="S5.T3.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r">68.1</td>
<td id="S5.T3.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r">63.1</td>
<td id="S5.T3.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">71.5</td>
<td id="S5.T3.1.4.2.5" class="ltx_td ltx_align_center ltx_border_r">67.9</td>
<td id="S5.T3.1.4.2.6" class="ltx_td ltx_align_center ltx_border_r">75.1</td>
<td id="S5.T3.1.4.2.7" class="ltx_td ltx_align_center ltx_border_r">71.3</td>
<td id="S5.T3.1.4.2.8" class="ltx_td ltx_align_center ltx_border_r">79.1</td>
<td id="S5.T3.1.4.2.9" class="ltx_td ltx_align_center ltx_border_r">75.5</td>
</tr>
<tr id="S5.T3.1.5.3" class="ltx_tr">
<td id="S5.T3.1.5.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">ctba</td>
<td id="S5.T3.1.5.3.2" class="ltx_td ltx_border_r"></td>
<td id="S5.T3.1.5.3.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T3.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">89.5</td>
<td id="S5.T3.1.5.3.5" class="ltx_td ltx_align_center ltx_border_r">87.4</td>
<td id="S5.T3.1.5.3.6" class="ltx_td ltx_align_center ltx_border_r">90.7</td>
<td id="S5.T3.1.5.3.7" class="ltx_td ltx_align_center ltx_border_r">88.5</td>
<td id="S5.T3.1.5.3.8" class="ltx_td ltx_align_center ltx_border_r">90.8</td>
<td id="S5.T3.1.5.3.9" class="ltx_td ltx_align_center ltx_border_r">88.6</td>
</tr>
<tr id="S5.T3.1.6.4" class="ltx_tr">
<td id="S5.T3.1.6.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">cntba</td>
<td id="S5.T3.1.6.4.2" class="ltx_td ltx_border_r"></td>
<td id="S5.T3.1.6.4.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T3.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">89.1</td>
<td id="S5.T3.1.6.4.5" class="ltx_td ltx_align_center ltx_border_r">86.4</td>
<td id="S5.T3.1.6.4.6" class="ltx_td ltx_align_center ltx_border_r">90.3</td>
<td id="S5.T3.1.6.4.7" class="ltx_td ltx_align_center ltx_border_r">88.1</td>
<td id="S5.T3.1.6.4.8" class="ltx_td ltx_align_center ltx_border_r">90.5</td>
<td id="S5.T3.1.6.4.9" class="ltx_td ltx_align_center ltx_border_r">88.3</td>
</tr>
<tr id="S5.T3.1.7.5" class="ltx_tr">
<td id="S5.T3.1.7.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">ctnba</td>
<td id="S5.T3.1.7.5.2" class="ltx_td ltx_border_r"></td>
<td id="S5.T3.1.7.5.3" class="ltx_td ltx_border_r"></td>
<td id="S5.T3.1.7.5.4" class="ltx_td ltx_align_center ltx_border_r">86.1</td>
<td id="S5.T3.1.7.5.5" class="ltx_td ltx_align_center ltx_border_r">81.8</td>
<td id="S5.T3.1.7.5.6" class="ltx_td ltx_align_center ltx_border_r">87.8</td>
<td id="S5.T3.1.7.5.7" class="ltx_td ltx_align_center ltx_border_r">84.0</td>
<td id="S5.T3.1.7.5.8" class="ltx_td ltx_align_center ltx_border_r">88.4</td>
<td id="S5.T3.1.7.5.9" class="ltx_td ltx_align_center ltx_border_r">84.8</td>
</tr>
<tr id="S5.T3.1.8.6" class="ltx_tr">
<td id="S5.T3.1.8.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">cntnba</td>
<td id="S5.T3.1.8.6.2" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S5.T3.1.8.6.3" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S5.T3.1.8.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">86.8</td>
<td id="S5.T3.1.8.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">84.2</td>
<td id="S5.T3.1.8.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">88.4</td>
<td id="S5.T3.1.8.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">86.1</td>
<td id="S5.T3.1.8.6.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">88.9</td>
<td id="S5.T3.1.8.6.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">86.5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S5.T3.10.1" class="ltx_text ltx_font_bold">Acc</span>uracy and <span id="S5.T3.11.2" class="ltx_text ltx_font_bold">Corr</span>ect words for evaluation tests (200 speakers): <span id="S5.T3.12.3" class="ltx_text ltx_font_bold">mb</span> = monophones, old lexicon; <span id="S5.T3.13.4" class="ltx_text ltx_font_bold">nmb</span> = monophones new lexicon; <span id="S5.T3.14.5" class="ltx_text ltx_font_bold">ctba</span> = triphones, within word context expansion, old lexicon; <span id="S5.T3.15.6" class="ltx_text ltx_font_bold">cntba</span> = triphones, within word context expansion, new lexicon; <span id="S5.T3.16.7" class="ltx_text ltx_font_bold">ctnba</span> = triphones, cross word context expansion, old lexicon; <span id="S5.T3.17.8" class="ltx_text ltx_font_bold">cntnba</span> = triphones, cross word context expansion, new lexicon</figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">In the table correct words and accuracy are reported for each experiment. The best result (88.6% of accuracy) is obtained with within-word context expanded models and eight mixture terms. As can be seen in the table, monophone accuracy rises when the number of mixture terms is increased from four to eight. This means that probably better results can be obtained if the number of mixture terms is further increased. In the case of context dependent models the increase of accuracy from four terms models to eight terms models is quite low. Old lexicon models perform better in general than new lexicon models. Models excluding cross-word context information perform better than models including it.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Results per individual speaker</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">SpeechDat database is built on a wide range of speaker characteristics (Section <a href="#S2" title="2 Speech material ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). For this reason it is interesting to show per speaker results. Often in speaker independent recognition tasks, speakers are divided into “goats” and “sheeps” depending on results obtained. “Goats” are those speakers for which bad results are obtained, while “sheep” speakers are well recognized by the system. The definition of the threshold separating these groups is arbitrary and depends on the application.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.F5.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/2404.16547/assets/figures/spkrs.png" id="S5.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="424" alt="Refer to caption">
<p id="S5.F5.1.1" class="ltx_p ltx_align_center">a</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.F5.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/2404.16547/assets/figures/spkrslog.png" id="S5.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="427" alt="Refer to caption">
<p id="S5.F5.2.1" class="ltx_p ltx_align_center">b</p>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>a: Number of speakers in 10% Accuracy ranges. Number of speakers in 10% Accuracy ranges (log)</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.3" class="ltx_p">In Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2 Results per individual speaker ‣ 5 Results ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> a, the number of speakers for which results are in ranges of ten percent of accuracy are shown. No speaker in the evaluation subset has results below 40% of accuracy. If we set the boundary between “goats” and “sheeps” at 80% of accuracy, 36 speakers of the 200 in the evaluation subset belong to the “goats” group while the other 153 are “sheeps”. In Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2 Results per individual speaker ‣ 5 Results ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> b, the same data is plotted in a logarithmic scale showing a linear behavior (<math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="y\approx m+ax" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">y</mi><mo id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">≈</mo><mrow id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml"><mi id="S5.SS2.p2.1.m1.1.1.3.2" xref="S5.SS2.p2.1.m1.1.1.3.2.cmml">m</mi><mo id="S5.SS2.p2.1.m1.1.1.3.1" xref="S5.SS2.p2.1.m1.1.1.3.1.cmml">+</mo><mrow id="S5.SS2.p2.1.m1.1.1.3.3" xref="S5.SS2.p2.1.m1.1.1.3.3.cmml"><mi id="S5.SS2.p2.1.m1.1.1.3.3.2" xref="S5.SS2.p2.1.m1.1.1.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.1.m1.1.1.3.3.1" xref="S5.SS2.p2.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S5.SS2.p2.1.m1.1.1.3.3.3" xref="S5.SS2.p2.1.m1.1.1.3.3.3.cmml">x</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><approx id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1"></approx><ci id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">𝑦</ci><apply id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3"><plus id="S5.SS2.p2.1.m1.1.1.3.1.cmml" xref="S5.SS2.p2.1.m1.1.1.3.1"></plus><ci id="S5.SS2.p2.1.m1.1.1.3.2.cmml" xref="S5.SS2.p2.1.m1.1.1.3.2">𝑚</ci><apply id="S5.SS2.p2.1.m1.1.1.3.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3.3"><times id="S5.SS2.p2.1.m1.1.1.3.3.1.cmml" xref="S5.SS2.p2.1.m1.1.1.3.3.1"></times><ci id="S5.SS2.p2.1.m1.1.1.3.3.2.cmml" xref="S5.SS2.p2.1.m1.1.1.3.3.2">𝑎</ci><ci id="S5.SS2.p2.1.m1.1.1.3.3.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3.3.3">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">y\approx m+ax</annotation></semantics></math>). Knowing <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mi id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><ci id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">m</annotation></semantics></math> and <math id="S5.SS2.p2.3.m3.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S5.SS2.p2.3.m3.1a"><mi id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><ci id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">a</annotation></semantics></math> can be useful to predict results when new speakers are added to the evaluation set, or to evaluate new developments in the systems.</p>
</div>
<figure id="S5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.F6.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/2404.16547/assets/figures/female.png" id="S5.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="422" alt="Refer to caption">
<p id="S5.F6.1.1" class="ltx_p ltx_align_center">a</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.F6.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/2404.16547/assets/figures/male.png" id="S5.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="422" alt="Refer to caption">
<p id="S5.F6.2.1" class="ltx_p ltx_align_center">b</p>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Accuracy (left bar) and Correct Words (right bar) for young (<math id="S5.F6.5.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.F6.5.m1.1b"><mo id="S5.F6.5.m1.1.1" xref="S5.F6.5.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.F6.5.m1.1c"><lt id="S5.F6.5.m1.1.1.cmml" xref="S5.F6.5.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.F6.5.m1.1d">&lt;</annotation></semantics></math>16), middle, and old (<math id="S5.F6.6.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.F6.6.m2.1b"><mo id="S5.F6.6.m2.1.1" xref="S5.F6.6.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.F6.6.m2.1c"><gt id="S5.F6.6.m2.1.1.cmml" xref="S5.F6.6.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.F6.6.m2.1d">&gt;</annotation></semantics></math>65) speakers. a) female, b) male.</figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2 Results per individual speaker ‣ 5 Results ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> reports results according to sex and age of the speaker, Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2 Results per individual speaker ‣ 5 Results ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> a include results on female speakers and Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2 Results per individual speaker ‣ 5 Results ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> b on male speakers. In the figure the left bar for each age group displays the accuracy, while the right bar displays the percentage number of correct words. An error bar is also includeded showing the standard deviation in each group (the number of speakers in each group is reported in Table <a href="#S2.T1" title="Table 1 ‣ 2.6 Statistics ‣ 2 Speech material ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Results seem to be sex independent, even though female speakers are better recognized (probably because they are more numerous in the database).</p>
</div>
<figure id="S5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.F7.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/2404.16547/assets/figures/dlctold.png" id="S5.F7.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="423" alt="Refer to caption">
<p id="S5.F7.1.1" class="ltx_p ltx_align_center">a</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.F7.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:216.8pt;">
<img src="/html/2404.16547/assets/figures/dlctnew.png" id="S5.F7.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="422" alt="Refer to caption">
<p id="S5.F7.2.1" class="ltx_p ltx_align_center">b</p>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Accuracy (left bar) and Correct Words (right bar) for speakers from different regions. Old lexicon (a) and new lexicon (b)</figcaption>
</figure>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2 Results per individual speaker ‣ 5 Results ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows results depending on the region the speakers call from. In this case Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2 Results per individual speaker ‣ 5 Results ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> a shows results obtained with the new lexicon while Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2 Results per individual speaker ‣ 5 Results ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> b is for old lexicon models results. Speech uttered by speakers from the south of Sweden seems to be more hard to recognize, while speakers from Bergslagen give the best results. An unexpected result refers to speakers from east-middle Sweden, region containing the Stockholm district, and hence the grate part of the Swedish population. In spite of the large amount of training data, for these speakers results are not as high as we would expect. The last comment on Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2 Results per individual speaker ‣ 5 Results ‣ Developing Acoustic Models for Automatic Speech Recognition in Swedish1footnote 11footnote 1This paper is a summary of the author’s Master Thesis that was published in June 1999 on The European Student Journal of Language and Speech (http://www.essex.ac.uk/web-sls/papers/99-01/99-01.html)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> refers to the lexicon. The same trend in per-dialect results is obtained including or excluding retroflex allophones. In the case of southern speakers we would expect lower results for models including the retroflex allophones because speakers living in this region do not make the distinction between normal and retroflex tongue position. In spite of this results are quite similar for southern speakers, new or old lexicon.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Evaluating on Other Databases</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">To judge the system performance it would be important to compare results with those obtained with other systems in similar conditions. In our case it is not possible to refer to a previous work on the Swedish SpeechDat database. Comparison is made referring to two experiments which contain substantial differences from the SpeechDat context. For these differences, on the other hand, an evaluation of the model flexibility is possible. The two experiments taken into account refer to the Waxholm project and to the Norwegian SpeechDat project and will be described in the next sections.</p>
</div>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1 </span>The Waxholm Database</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.1" class="ltx_p">The Waxholm database presents many differences if compared to the SpeechDat database.</p>
</div>
<div id="S5.SS3.SSS1.p2" class="ltx_para">
<p id="S5.SS3.SSS1.p2.1" class="ltx_p">First of all it was developed in the Waxholm project in the attempt to create a dialogue system for information about boat traffic, restaurants and accommodations in the Stockholm archipelago. The corpus of sentences included is hence affected by this task. The number of speakers (mostly men) is low if compared to the SpeechDat collection. 50% Speakers have an accent typical of the Stockholm area. Speech sampling and labeling are also different (16kHz and phone level by-hand transcriptions). Down-sampling audio files has been necessary because models developed in this work are built for telephone speech. Doing this part of the spectral information in the speech files has been lost.</p>
</div>
<div id="S5.SS3.SSS1.p3" class="ltx_para">
<p id="S5.SS3.SSS1.p3.1" class="ltx_p">Models are then tested on the same subset of ten speakers used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> on a generic word recognition task, even if model parameters have been tuned in this work with reference to a digit and natural number recognition task. Within-word context expansion models with eight Gaussian distributions per mixture scored 89.9% of accuracy, while in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> 86.6% of accuracy was reached with sixteen Gaussian distributions triphones.</p>
</div>
<div id="S5.SS3.SSS1.p4" class="ltx_para">
<p id="S5.SS3.SSS1.p4.1" class="ltx_p">This prove how these models, in spite of the task adopted in this work, can be employed in a more wide range of applications.</p>
</div>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2 </span>The Norwegian SpeechDat database</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p id="S5.SS3.SSS2.p1.1" class="ltx_p">In Norway similar experiments to those made in this work have been done for Norwegian in the SpeechDat project <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Results are not directly comparable because in our case the same network (loop of words, bigram) has been used for a wide range of different items including for example isolated digits for which a grammar definition allowing only one word (digit) for utterance would be more efficient. However, these results are always similar, even though Norwegian models have been trained on 816 speakers instead of the 750 in our experiments, and the complete database corpus has been used, while only phonetically reach sentences and words have been used in our experiments.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Overall results on the evaluation material have shown that models excluding retroflex allophones in general perform better than models including them. This conclusion is surely affected by the task, in fact only a few words included in the recognition task (fyrtio, fjorton, arton, kontokort) contain the allophone 2T, and there is no occurrence of the other allophones. This means that splitting these models in normal and retroflex versions (as in the new lexicon) results only in a lower amount of data for the normal (non retroflex) models mostly used in this task.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Furthermore models including only within-word contexts seem to perform better than models including cross word context information. This result is also affected by the task, first because one of the items included in the evaluating material consists of isolated digits (no cross-word context is available), and second because when uttering digits and natural numbers speakers tend to separate each word to make the sequence clear. In a generic speech recognition task in which a higher number of words is involved and sentences are uttered in a more continuous way, probably the advantage of using cross-word context expansion would be higher.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Per speaker results have shown how models fit quite well to different classes of speakers, with some exceptions. Finally testing models on the Waxholm database has shown the flexibility of these models in spite of the simple task they have been built for.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Further improvements</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Results for monophone models showed a considerable improvement when passing from four to eight Gaussian terms. For this reason it is likely that further improvements are still possible adding more Gaussian terms. In the case of context dependent models this possibility seems to be more problematic, depending on the fact that the amount of data is not sufficient to train the large number of parameters included in these models. An attempt to reduce this problem could be the use of a model set with a lower number of states respect to the optimal value (tree clustering threshold optimisation) as a base to add Gaussian distributions. The Tree clustering threshold optimisation, indeed, is executed on single distribution models, and there is no reason to think that the number of states in the model set is optimal also when increasing the number of Gaussian parameters.
One experiment in this direction has been tested without good results.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">However, the availability of the full 5000 speakers database, will reduce the data scarcity problem allowing the use of more complex models (more Gaussian distributions, lower number of states to be clustered to reduce the number of parameters). Using this database, different strategies will be possible, as for example the creation of two different model sets for female and male speakers, or the creation of dialect dependent models (for example particular models could be built for speakers from the south, which seem to have different characteristics from all the others in Sweden).</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Another problem consists in the stationary noise that affects many files in the database. This disturbance, typical of the telephone line, is changing from utterance to utterance affecting the accuracy of acoustic models. A way to reduce the effects of stationary noise could be subtracting the mean energy over each utterance to the mel-cepstral coefficients that constitute the observation sequence to the recognition system. This method, however, requires that the whole utterance is acquired before starting the recognition process, excluding real time applications, in which the speech signal is recorded and analysed on line.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
TVIT project.

</span>
<span class="ltx_bibblock">Speech, Music and Hearing department, KTH, Sweden.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
G. Chollet, F. Tore Johansen, B. Lindberg, and F. Senia.

</span>
<span class="ltx_bibblock">Test set definition and specification.

</span>
<span class="ltx_bibblock">Technical Report LE2-4001-SD1.3.4, Consortium and CEC, dec 1997.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Robert Edward Donovan.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Trainable Speech Synthesis</span>.

</span>
<span class="ltx_bibblock">PhD thesis, Cambridge University Engineering Department, Trumpington
Street Cambridge CB2 1PZ England, 1996.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Gunnar Fant.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Speech Sounds and Features</span>.

</span>
<span class="ltx_bibblock">The MIT Press Cambridge, Massachusetts and London, England, 1973.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Joakim Gustafson, Nikolaj Lindberg, Magnus Lundeberg, and Eva-Lena Svensson.

</span>
<span class="ltx_bibblock">The august spoken dialogue system.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of EuroSpeech</span>, 1999.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Finn Tore Johansen, Ingunn Amdal, and Knut Kvale.

</span>
<span class="ltx_bibblock">The norwegian part of speechdat: A european speech database for
creation of voice driven teleservices.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">NORSIG</span>, 1997.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jr. John R. Deller, John G. Proakis, and John H. L. Hansen.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Discrete-Time Processing od Speech Signals</span>.

</span>
<span class="ltx_bibblock">Macmillian Publishing Company, 866 Third Avenue, New York, New York
10022, 1993.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Håkan Melin.

</span>
<span class="ltx_bibblock">On word boundary detection in digital-based speaker verification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">La Reconnaissance du Locuteur et ses Applications
Commerciales et Criminalistiques</span>, pages 46–49, 1998.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Kåre Sjölander.

</span>
<span class="ltx_bibblock">Continuous speech recognition with hidden markov models.

</span>
<span class="ltx_bibblock">Master’s thesis, Kungliga Tekniska Högskolan Department of
Speech, Music and Hearing, Drottning Kristinas väg 31 100 44 Stockholm,
1996.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Steve Young, Julian Odell, Dave Ollason, Valtcho Valtchev, and Phil Woodland.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">The HTK Book</span>.

</span>
<span class="ltx_bibblock">Entropic Cambridge University Laboratory, dec 1997.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="Giampiero Salvi"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="speech recognition hmms"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="Developing Acoustic Models for Automatic Speech Recognition in Swedish"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.16546" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.16547" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.16547">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.16547" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.16548" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 20:58:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
