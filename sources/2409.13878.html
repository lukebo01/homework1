<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.13878] Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models</title><meta property="og:description" content="Transfer learning is commonly employed to leverage large, pre-trained models and perform fine-tuning for downstream tasks. The most prevalent pre-trained models are initially trained using ImageNet. However, their abil…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.13878">

<!--Generated on Sat Oct  5 20:16:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
deep learning,  transfer learning,  underwater acoustic target recognition.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amirmohammad Mohammadi, Tejashri Kelhe, Davelle Carreiro, Alexandra Van Dine, and Joshua Peeples
</span><span class="ltx_author_notes">DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited. This material is based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Under Secretary of Defense for Research and Engineering. <sup id="id1.1.id1" class="ltx_sup">©</sup> 2024 Massachusetts Institute of Technology. Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than as specifically authorized by the U.S. Government may violate any copyrights that exist in this work. Portions of this research were conducted with the advanced computing resources provided by Texas A&amp;M High Performance Research Computing. Code is publicly available at <a target="_blank" href="https://github.com/Advanced-Vision-and-Learning-Lab/PANN_Models_DeepShip" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Advanced-Vision-and-Learning-Lab/PANN_Models_DeepShip</a>.Amirmohammad Mohammadi, Tejashri Kelhe, and Joshua Peeples are with the Department of Electrical and Computer Engineering, Texas A&amp;M University, College Station, TX, USA (e-mail: amir.m@tamu.edu; tkelhe@tamu.edu; jpeeples@tamu.edu).Davelle Carreiro and Alexandra Van Dine are with the Massachusetts Institute of Technology Lincoln Laboratory, Lexington, MA, USA (e-mail: davelle.carreiro@ll.mit.edu; alexandra.vandine@ll.mit.edu).</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Transfer learning is commonly employed to leverage large, pre-trained models and perform fine-tuning for downstream tasks. The most prevalent pre-trained models are initially trained using ImageNet. However, their ability to generalize can vary across different data modalities. This study compares pre-trained Audio Neural Networks (PANNs) and ImageNet pre-trained models within the context of underwater acoustic target recognition (UATR). It was observed that the ImageNet pre-trained models slightly out-perform pre-trained audio models in passive sonar classification. We also analyzed the impact of audio sampling rates for model pre-training and fine-tuning. This study contributes to transfer learning applications of UATR, illustrating the potential of pre-trained models to address limitations caused by scarce, labeled data in the UATR domain.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
deep learning, transfer learning, underwater acoustic target recognition.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deep learning is used frequently for audio classification tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> due to its ability to automatically extract relevant data features and identify complex patterns. These tasks have broad applications from healthcare<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, urban development <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, and environmental monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> to underwater acoustic target recognition (UATR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. In a subset of the latter application family, passive sonar can be used to identify objects at the water surface or underwater using knowledge of target acoustics signatures and expected propagation through the water column. This capability impacts numerous maritime tasks including analysis of biological life cycles<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, assisted search and rescue operations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, monitor of maritime traffic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, and analysis of unknown sound sources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Obtaining large, labeled, publicly available datasets for UATR is difficult <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. To overcome this challenge, transfer learning can be used to leverage large, pre-trained models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Transfer learning has several advantages including data efficiency, shorter training time, and increased performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Our work leverages pre-trained Audio Neural Networks (PANNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and ImageNet pre-trained models via PyTorch Image Models (TIMMs)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, to the domain of UATR using a publicly available dataset, DeepShip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">PANN models are trained on the large-scale AudioSet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, which has over 5,000 hours of audio recordings from YouTube videos. These models are proposed to be transferable to other audio pattern recognition tasks. The TIMM library includes a number amount of models, many of which are trained on ImageNet1K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. PANNs and TIMMs aim to leverage knowledge gained from large datasets and transfer knowledge to other, seemingly unrelated, tasks. Our work aims to shed light on the usefulness of different types of pre-trained models for passive sonar classification. This study is also the first to thoroughly investigate PANNs for the DeepShip dataset. Related work <span id="S1.p3.1.1" class="ltx_text"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite></span> used PANNs for a smaller, passive sonar dataset (ShipsEar <span id="S1.p3.1.2" class="ltx_text"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite></span>).</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Method</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Model Preparation</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The PANN CNN14 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> was used for the first portion of investigation since this model was pre-trained across three different data sampling rates of 8, 16, and 32 kHz on the AudioSet dataset. We investigated the performance of these pre-trained models on DeepShip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> data sampled at the different rates in order to determine if the sampling frequency of a) data and b) the pre-trained model impacts classification performance. The second portion of investigation focused on the five top-performing PANN models (CNN14, ResNet38, MobileNetV1, Res1dNet31, and Wavegram-Logmel-CNN) and a number of pre-trained ImageNet1K models from TIMM (ResNet50, DenseNet201, MobilenetV3-large-100, ConvNeXtV2-tiny, and RegNety-320). TIMM models were chosen to be either equivalent to their PANN counterparts (<span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">e.g.</span>, MobileNets and ResNets) or were recently published (<span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_italic">i.e.,</span> ConvNeXtV2-tiny).</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.13878/assets/x1.png" id="S2.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="232" height="83" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.13878/assets/Figures/Moham2.png" id="S2.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="116" height="77" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.13878/assets/x2.png" id="S2.F1.sf3.g1" class="ltx_graphics ltx_img_landscape" width="259" height="85" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overall framework is shown. (a) Data Preprocessing: The audio waveform is transformed into a logarithmic mel-frequency spectrogram. (b) Data Augmentation: SpectAugmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and Mixup <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> are added during training to improve model performance. (c) Data Analysis: Input spectrograms are processed using networks pre-trained on AudioSet (PANN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>) or ImageNet (TIMM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>).</figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">To adapt both PANN and TIMM models for the UATR task, full fine-tuning of the models was performed. As noted by prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, full fine-tuning maximizes performance as opposed to training from scratch or freezing portions of the networks. For both PANN and TIMM models, the waveform signals were first converted to spectrograms before being passed into the models.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Data Preparation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The DeepShip dataset includes four classes of different ship types labeled as cargo, passengership, tanker, and tug. To investigate the impact of frequency resolution on model performance, the signals were resampled to frequencies of 8 kHz, 16 kHz, and 32 kHz and then segmented into intervals of five seconds where each of these segments is an individual example for the training paradigm. Three datasets were, therefore, generated based on sampling rate where each contained a total of 609 recordings and 33,770 segments.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The dataset was partitioned into training, validation, and test sets with ratios of 70%, 10%, and 20%, respectively, using a method similar to that described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, which splits the dataset based on recordings. To prevent data leakage, if a specific recording segment was chosen for training, all other segments in that recording were also chosen for training. The dataset split was completed using stratification to maintain similar distribution of all four classes across the sets. To reproduce these partitions across experiments, the dataset split was initially written and saved in a file that was used to generate subsequent data partitions. Specifically, the dataset was split into training, validation, and testing sets with 428 recordings (23,088 segments), 60 recordings (3,974 segments), and 121 recordings (6,708 segments), respectively. Normalization was applied to each data split using a minimum-maximum approach, i.e., by computing the global minimum and maximum values from the training dataset.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Each segment was converted into a spectrogram using the Short-Time Fourier Transform (STFT) with a Hann window of size 1024 and a hop length of 320, a parameterization which captures the frequency content of the signal over time. The resulting spectrogram is then transformed into a logarithmic mel-frequency spectrogram by applying a logarithmic mel-frequency filter bank with 64 mel filters, which compresses the frequency information into the mel-frequency scale. The lower and upper cut-off frequencies for the mel-frequency filters are set to 50 Hz and 14 kHz, respectively, to filter out low-frequency noise and minimize aliasing effects.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Data Augmentation</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, to improve model robustness to variations in the data, SpecAugmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> is applied on the spectrogram, a technique which incorporates transformations such as time and frequency masking. Mixup <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is applied to the spectrogram to further augment the data by mixing the inputs and targets of two samples, thereby incorporating adversarial training examples. Each data augmentation technique was used for both PANN and TIMM models for a fair comparison. Because TIMM models are initially trained on 3-channel (RGB) ImageNet data, when using TIMM models for single-channel inputs (<span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>, spectrograms), the weights of the first convolutional layer are summed across the three input kernels. Aggregation of the weights allows the pre-trained TIMM models to be applied to single channel inputs without needing to modify the remaining portion of the model’s structure. PANN models, in contrast, are able to be applied directly to the passive sonar data as the pre-training was accomplished with single channel data features.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experimental Procedure</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">For the first experimental investigation, each CNN14 model was fine-tuned, pre-trained at three distinct sampling rates, and resampled the DeepShip dataset to match these frequencies. This resulted in the evaluation of nine data-model combinations. For the CNN14 models pre-trained on 16 kHz and 8 kHz, the window length, hop size, and upper cut-off frequency were adjusted to half and one-quarter, respectively, relative to the 32 kHz model. This scaling was necessary to maintain consistent resolution across different sampling rates, so that each model would ingest spectrograms appropriate to the model configuration.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To make data augmentation consistent across different data sampling rates, the time mask width for SpecAugmentation was adjusted based on the ratio of the data sample rate to the model sample rate. The formula used is given by Equation <span id="S3.p2.1.1" class="ltx_text"><a href="#S3.E1" title="In III Experimental Procedure ‣ Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></span>:</p>
</div>
<div id="S3.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="W_{\text{mask}}=W_{\text{base}}\times\left(\frac{r_{\text{data}}}{r_{\text{model}}}\right)" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.2" xref="S3.E1.m1.1.2.cmml"><msub id="S3.E1.m1.1.2.2" xref="S3.E1.m1.1.2.2.cmml"><mi id="S3.E1.m1.1.2.2.2" xref="S3.E1.m1.1.2.2.2.cmml">W</mi><mtext id="S3.E1.m1.1.2.2.3" xref="S3.E1.m1.1.2.2.3a.cmml">mask</mtext></msub><mo id="S3.E1.m1.1.2.1" xref="S3.E1.m1.1.2.1.cmml">=</mo><mrow id="S3.E1.m1.1.2.3" xref="S3.E1.m1.1.2.3.cmml"><msub id="S3.E1.m1.1.2.3.2" xref="S3.E1.m1.1.2.3.2.cmml"><mi id="S3.E1.m1.1.2.3.2.2" xref="S3.E1.m1.1.2.3.2.2.cmml">W</mi><mtext id="S3.E1.m1.1.2.3.2.3" xref="S3.E1.m1.1.2.3.2.3a.cmml">base</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.2.3.1" xref="S3.E1.m1.1.2.3.1.cmml">×</mo><mrow id="S3.E1.m1.1.2.3.3.2" xref="S3.E1.m1.1.1.cmml"><mo id="S3.E1.m1.1.2.3.3.2.1" xref="S3.E1.m1.1.1.cmml">(</mo><mfrac id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">r</mi><mtext id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3a.cmml">data</mtext></msub><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">r</mi><mtext id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3a.cmml">model</mtext></msub></mfrac><mo id="S3.E1.m1.1.2.3.3.2.2" xref="S3.E1.m1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.2.cmml" xref="S3.E1.m1.1.2"><eq id="S3.E1.m1.1.2.1.cmml" xref="S3.E1.m1.1.2.1"></eq><apply id="S3.E1.m1.1.2.2.cmml" xref="S3.E1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.2.1.cmml" xref="S3.E1.m1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.1.2.2.2.cmml" xref="S3.E1.m1.1.2.2.2">𝑊</ci><ci id="S3.E1.m1.1.2.2.3a.cmml" xref="S3.E1.m1.1.2.2.3"><mtext mathsize="70%" id="S3.E1.m1.1.2.2.3.cmml" xref="S3.E1.m1.1.2.2.3">mask</mtext></ci></apply><apply id="S3.E1.m1.1.2.3.cmml" xref="S3.E1.m1.1.2.3"><times id="S3.E1.m1.1.2.3.1.cmml" xref="S3.E1.m1.1.2.3.1"></times><apply id="S3.E1.m1.1.2.3.2.cmml" xref="S3.E1.m1.1.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.2.3.2.1.cmml" xref="S3.E1.m1.1.2.3.2">subscript</csymbol><ci id="S3.E1.m1.1.2.3.2.2.cmml" xref="S3.E1.m1.1.2.3.2.2">𝑊</ci><ci id="S3.E1.m1.1.2.3.2.3a.cmml" xref="S3.E1.m1.1.2.3.2.3"><mtext mathsize="70%" id="S3.E1.m1.1.2.3.2.3.cmml" xref="S3.E1.m1.1.2.3.2.3">base</mtext></ci></apply><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.2.3.3.2"><divide id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.2.3.3.2"></divide><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">𝑟</ci><ci id="S3.E1.m1.1.1.2.3a.cmml" xref="S3.E1.m1.1.1.2.3"><mtext mathsize="70%" id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3">data</mtext></ci></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">𝑟</ci><ci id="S3.E1.m1.1.1.3.3a.cmml" xref="S3.E1.m1.1.1.3.3"><mtext mathsize="70%" id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3">model</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">W_{\text{mask}}=W_{\text{base}}\times\left(\frac{r_{\text{data}}}{r_{\text{model}}}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.3" class="ltx_p">where <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="W_{\text{base}}" display="inline"><semantics id="S3.p4.1.m1.1a"><msub id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml"><mi id="S3.p4.1.m1.1.1.2" xref="S3.p4.1.m1.1.1.2.cmml">W</mi><mtext id="S3.p4.1.m1.1.1.3" xref="S3.p4.1.m1.1.1.3a.cmml">base</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><apply id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p4.1.m1.1.1.1.cmml" xref="S3.p4.1.m1.1.1">subscript</csymbol><ci id="S3.p4.1.m1.1.1.2.cmml" xref="S3.p4.1.m1.1.1.2">𝑊</ci><ci id="S3.p4.1.m1.1.1.3a.cmml" xref="S3.p4.1.m1.1.1.3"><mtext mathsize="70%" id="S3.p4.1.m1.1.1.3.cmml" xref="S3.p4.1.m1.1.1.3">base</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">W_{\text{base}}</annotation></semantics></math> is set to 64. For example, when both the model sample rate (<math id="S3.p4.2.m2.1" class="ltx_Math" alttext="r_{\text{model}}" display="inline"><semantics id="S3.p4.2.m2.1a"><msub id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml"><mi id="S3.p4.2.m2.1.1.2" xref="S3.p4.2.m2.1.1.2.cmml">r</mi><mtext id="S3.p4.2.m2.1.1.3" xref="S3.p4.2.m2.1.1.3a.cmml">model</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p4.2.m2.1.1.1.cmml" xref="S3.p4.2.m2.1.1">subscript</csymbol><ci id="S3.p4.2.m2.1.1.2.cmml" xref="S3.p4.2.m2.1.1.2">𝑟</ci><ci id="S3.p4.2.m2.1.1.3a.cmml" xref="S3.p4.2.m2.1.1.3"><mtext mathsize="70%" id="S3.p4.2.m2.1.1.3.cmml" xref="S3.p4.2.m2.1.1.3">model</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">r_{\text{model}}</annotation></semantics></math>) and the data sample rate (<math id="S3.p4.3.m3.1" class="ltx_Math" alttext="r_{\text{data}}" display="inline"><semantics id="S3.p4.3.m3.1a"><msub id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml"><mi id="S3.p4.3.m3.1.1.2" xref="S3.p4.3.m3.1.1.2.cmml">r</mi><mtext id="S3.p4.3.m3.1.1.3" xref="S3.p4.3.m3.1.1.3a.cmml">data</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><apply id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p4.3.m3.1.1.1.cmml" xref="S3.p4.3.m3.1.1">subscript</csymbol><ci id="S3.p4.3.m3.1.1.2.cmml" xref="S3.p4.3.m3.1.1.2">𝑟</ci><ci id="S3.p4.3.m3.1.1.3a.cmml" xref="S3.p4.3.m3.1.1.3"><mtext mathsize="70%" id="S3.p4.3.m3.1.1.3.cmml" xref="S3.p4.3.m3.1.1.3">data</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">r_{\text{data}}</annotation></semantics></math>) are 32 kHz, the time mask width remains 64. However, if the data sample rate is 16 kHz whereas the model sample rate is 32 kHz, the time mask width is adjusted to 32.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">As a result, the number of time frames are also halved (from 501 to 251) in this example. This ensures appropriate scaling between augmentation schema and the data sample rate. The comparison of pre-trained audio (PANN) and image (TIMM) models was performed using a fixed audio frequency resolution of 32 kHz as all available PANN models (except CNN14) were pre-trained on 32 kHz data.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">All models were trained using a learning rate of 5e-5, a batch size of 64, and Adam optimizer for 100 epochs with a patience setting of 50 epochs based on validation loss. The batch size was set to 32 for the impact of sampling rate experiments in Section <span id="S3.p6.1.1" class="ltx_text"><a href="#S4.SS1" title="IV-A Impact of Sampling Rate ‣ IV Results and Discussion ‣ Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a></span>. All experiments were completed on an NVIDIA A40 GPU with each experiment conducted over three random runs of initialization to evaluate the reproducibility of the results. The overall framework is depicted in Fig. <a href="#S2.F1" title="Figure 1 ‣ II-A Model Preparation ‣ II Method ‣ Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Models are then fine-tuned on this data for classification. During training, data augmentation is incorporated as described in Section <a href="#S2.SS3" title="II-C Data Augmentation ‣ II Method ‣ Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a>.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Results and Discussion</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Impact of Sampling Rate</span>
</h3>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2409.13878/assets/x3.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="258" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Average test accuracy across three experimental runs at different sampling rates with <math id="S4.F2.3.m1.1" class="ltx_Math" alttext="\pm 1" display="inline"><semantics id="S4.F2.3.m1.1b"><mrow id="S4.F2.3.m1.1.1" xref="S4.F2.3.m1.1.1.cmml"><mo id="S4.F2.3.m1.1.1b" xref="S4.F2.3.m1.1.1.cmml">±</mo><mn id="S4.F2.3.m1.1.1.2" xref="S4.F2.3.m1.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.3.m1.1c"><apply id="S4.F2.3.m1.1.1.cmml" xref="S4.F2.3.m1.1.1"><csymbol cd="latexml" id="S4.F2.3.m1.1.1.1.cmml" xref="S4.F2.3.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S4.F2.3.m1.1.1.2.cmml" xref="S4.F2.3.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.3.m1.1d">\pm 1</annotation></semantics></math> standard deviation for CNN14 models. Each color represents a different model, with the symbol of each model centered on the average test accuracy and the error bars show <math id="S4.F2.4.m2.1" class="ltx_Math" alttext="\pm 1" display="inline"><semantics id="S4.F2.4.m2.1b"><mrow id="S4.F2.4.m2.1.1" xref="S4.F2.4.m2.1.1.cmml"><mo id="S4.F2.4.m2.1.1b" xref="S4.F2.4.m2.1.1.cmml">±</mo><mn id="S4.F2.4.m2.1.1.2" xref="S4.F2.4.m2.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.4.m2.1c"><apply id="S4.F2.4.m2.1.1.cmml" xref="S4.F2.4.m2.1.1"><csymbol cd="latexml" id="S4.F2.4.m2.1.1.1.cmml" xref="S4.F2.4.m2.1.1">plus-or-minus</csymbol><cn type="integer" id="S4.F2.4.m2.1.1.2.cmml" xref="S4.F2.4.m2.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.4.m2.1d">\pm 1</annotation></semantics></math> standard deviation.</figcaption>
</figure>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Average Test Accuracy Across Three Experimental Runs at Different Sampling Rates With <math id="S4.T1.2.m1.1" class="ltx_Math" alttext="\pm 1" display="inline"><semantics id="S4.T1.2.m1.1b"><mrow id="S4.T1.2.m1.1.1" xref="S4.T1.2.m1.1.1.cmml"><mo id="S4.T1.2.m1.1.1b" xref="S4.T1.2.m1.1.1.cmml">±</mo><mn id="S4.T1.2.m1.1.1.2" xref="S4.T1.2.m1.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.m1.1c"><apply id="S4.T1.2.m1.1.1.cmml" xref="S4.T1.2.m1.1.1"><csymbol cd="latexml" id="S4.T1.2.m1.1.1.1.cmml" xref="S4.T1.2.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S4.T1.2.m1.1.1.2.cmml" xref="S4.T1.2.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.m1.1d">\pm 1</annotation></semantics></math> Standard Deviation. The Highest Average Test Accuracy Is Bolded.</figcaption>
<table id="S4.T1.20" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.20.19.1" class="ltx_tr">
<th id="S4.T1.20.19.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T1.20.19.1.1.1" class="ltx_text">Data Sampling Rate</span></th>
<th id="S4.T1.20.19.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">Models</th>
</tr>
<tr id="S4.T1.20.20.2" class="ltx_tr">
<th id="S4.T1.20.20.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">CNN14_8K</th>
<th id="S4.T1.20.20.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">CNN14_16K</th>
<th id="S4.T1.20.20.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">CNN14_32K</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.5.3" class="ltx_tr">
<th id="S4.T1.5.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">2 kHz</th>
<td id="S4.T1.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.3.1.1.m1.1" class="ltx_Math" alttext="74.2\pm 0.5\%" display="inline"><semantics id="S4.T1.3.1.1.m1.1a"><mrow id="S4.T1.3.1.1.m1.1.1" xref="S4.T1.3.1.1.m1.1.1.cmml"><mn id="S4.T1.3.1.1.m1.1.1.2" xref="S4.T1.3.1.1.m1.1.1.2.cmml">74.2</mn><mo id="S4.T1.3.1.1.m1.1.1.1" xref="S4.T1.3.1.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.3.1.1.m1.1.1.3" xref="S4.T1.3.1.1.m1.1.1.3.cmml"><mn id="S4.T1.3.1.1.m1.1.1.3.2" xref="S4.T1.3.1.1.m1.1.1.3.2.cmml">0.5</mn><mo id="S4.T1.3.1.1.m1.1.1.3.1" xref="S4.T1.3.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.1.1.m1.1b"><apply id="S4.T1.3.1.1.m1.1.1.cmml" xref="S4.T1.3.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.3.1.1.m1.1.1.1.cmml" xref="S4.T1.3.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.3.1.1.m1.1.1.2.cmml" xref="S4.T1.3.1.1.m1.1.1.2">74.2</cn><apply id="S4.T1.3.1.1.m1.1.1.3.cmml" xref="S4.T1.3.1.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.3.1.1.m1.1.1.3.1.cmml" xref="S4.T1.3.1.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.3.1.1.m1.1.1.3.2.cmml" xref="S4.T1.3.1.1.m1.1.1.3.2">0.5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.1.1.m1.1c">74.2\pm 0.5\%</annotation></semantics></math></td>
<td id="S4.T1.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.4.2.2.m1.1" class="ltx_Math" alttext="74.2\pm 1.3\%" display="inline"><semantics id="S4.T1.4.2.2.m1.1a"><mrow id="S4.T1.4.2.2.m1.1.1" xref="S4.T1.4.2.2.m1.1.1.cmml"><mn id="S4.T1.4.2.2.m1.1.1.2" xref="S4.T1.4.2.2.m1.1.1.2.cmml">74.2</mn><mo id="S4.T1.4.2.2.m1.1.1.1" xref="S4.T1.4.2.2.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.4.2.2.m1.1.1.3" xref="S4.T1.4.2.2.m1.1.1.3.cmml"><mn id="S4.T1.4.2.2.m1.1.1.3.2" xref="S4.T1.4.2.2.m1.1.1.3.2.cmml">1.3</mn><mo id="S4.T1.4.2.2.m1.1.1.3.1" xref="S4.T1.4.2.2.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.4.2.2.m1.1b"><apply id="S4.T1.4.2.2.m1.1.1.cmml" xref="S4.T1.4.2.2.m1.1.1"><csymbol cd="latexml" id="S4.T1.4.2.2.m1.1.1.1.cmml" xref="S4.T1.4.2.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.4.2.2.m1.1.1.2.cmml" xref="S4.T1.4.2.2.m1.1.1.2">74.2</cn><apply id="S4.T1.4.2.2.m1.1.1.3.cmml" xref="S4.T1.4.2.2.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.4.2.2.m1.1.1.3.1.cmml" xref="S4.T1.4.2.2.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.4.2.2.m1.1.1.3.2.cmml" xref="S4.T1.4.2.2.m1.1.1.3.2">1.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.2.2.m1.1c">74.2\pm 1.3\%</annotation></semantics></math></td>
<td id="S4.T1.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.5.3.3.m1.1" class="ltx_Math" alttext="73.6\pm 0.7\%" display="inline"><semantics id="S4.T1.5.3.3.m1.1a"><mrow id="S4.T1.5.3.3.m1.1.1" xref="S4.T1.5.3.3.m1.1.1.cmml"><mn id="S4.T1.5.3.3.m1.1.1.2" xref="S4.T1.5.3.3.m1.1.1.2.cmml">73.6</mn><mo id="S4.T1.5.3.3.m1.1.1.1" xref="S4.T1.5.3.3.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.5.3.3.m1.1.1.3" xref="S4.T1.5.3.3.m1.1.1.3.cmml"><mn id="S4.T1.5.3.3.m1.1.1.3.2" xref="S4.T1.5.3.3.m1.1.1.3.2.cmml">0.7</mn><mo id="S4.T1.5.3.3.m1.1.1.3.1" xref="S4.T1.5.3.3.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.5.3.3.m1.1b"><apply id="S4.T1.5.3.3.m1.1.1.cmml" xref="S4.T1.5.3.3.m1.1.1"><csymbol cd="latexml" id="S4.T1.5.3.3.m1.1.1.1.cmml" xref="S4.T1.5.3.3.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.5.3.3.m1.1.1.2.cmml" xref="S4.T1.5.3.3.m1.1.1.2">73.6</cn><apply id="S4.T1.5.3.3.m1.1.1.3.cmml" xref="S4.T1.5.3.3.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.5.3.3.m1.1.1.3.1.cmml" xref="S4.T1.5.3.3.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.5.3.3.m1.1.1.3.2.cmml" xref="S4.T1.5.3.3.m1.1.1.3.2">0.7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.3.3.m1.1c">73.6\pm 0.7\%</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.8.6" class="ltx_tr">
<th id="S4.T1.8.6.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">4 kHz</th>
<td id="S4.T1.6.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.6.4.1.m1.1" class="ltx_Math" alttext="72.8\pm 0.7\%" display="inline"><semantics id="S4.T1.6.4.1.m1.1a"><mrow id="S4.T1.6.4.1.m1.1.1" xref="S4.T1.6.4.1.m1.1.1.cmml"><mn id="S4.T1.6.4.1.m1.1.1.2" xref="S4.T1.6.4.1.m1.1.1.2.cmml">72.8</mn><mo id="S4.T1.6.4.1.m1.1.1.1" xref="S4.T1.6.4.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.6.4.1.m1.1.1.3" xref="S4.T1.6.4.1.m1.1.1.3.cmml"><mn id="S4.T1.6.4.1.m1.1.1.3.2" xref="S4.T1.6.4.1.m1.1.1.3.2.cmml">0.7</mn><mo id="S4.T1.6.4.1.m1.1.1.3.1" xref="S4.T1.6.4.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.6.4.1.m1.1b"><apply id="S4.T1.6.4.1.m1.1.1.cmml" xref="S4.T1.6.4.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.6.4.1.m1.1.1.1.cmml" xref="S4.T1.6.4.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.6.4.1.m1.1.1.2.cmml" xref="S4.T1.6.4.1.m1.1.1.2">72.8</cn><apply id="S4.T1.6.4.1.m1.1.1.3.cmml" xref="S4.T1.6.4.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.6.4.1.m1.1.1.3.1.cmml" xref="S4.T1.6.4.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.6.4.1.m1.1.1.3.2.cmml" xref="S4.T1.6.4.1.m1.1.1.3.2">0.7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.4.1.m1.1c">72.8\pm 0.7\%</annotation></semantics></math></td>
<td id="S4.T1.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.7.5.2.m1.1" class="ltx_Math" alttext="73.9\pm 0.3\%" display="inline"><semantics id="S4.T1.7.5.2.m1.1a"><mrow id="S4.T1.7.5.2.m1.1.1" xref="S4.T1.7.5.2.m1.1.1.cmml"><mn id="S4.T1.7.5.2.m1.1.1.2" xref="S4.T1.7.5.2.m1.1.1.2.cmml">73.9</mn><mo id="S4.T1.7.5.2.m1.1.1.1" xref="S4.T1.7.5.2.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.7.5.2.m1.1.1.3" xref="S4.T1.7.5.2.m1.1.1.3.cmml"><mn id="S4.T1.7.5.2.m1.1.1.3.2" xref="S4.T1.7.5.2.m1.1.1.3.2.cmml">0.3</mn><mo id="S4.T1.7.5.2.m1.1.1.3.1" xref="S4.T1.7.5.2.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.7.5.2.m1.1b"><apply id="S4.T1.7.5.2.m1.1.1.cmml" xref="S4.T1.7.5.2.m1.1.1"><csymbol cd="latexml" id="S4.T1.7.5.2.m1.1.1.1.cmml" xref="S4.T1.7.5.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.7.5.2.m1.1.1.2.cmml" xref="S4.T1.7.5.2.m1.1.1.2">73.9</cn><apply id="S4.T1.7.5.2.m1.1.1.3.cmml" xref="S4.T1.7.5.2.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.7.5.2.m1.1.1.3.1.cmml" xref="S4.T1.7.5.2.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.7.5.2.m1.1.1.3.2.cmml" xref="S4.T1.7.5.2.m1.1.1.3.2">0.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.5.2.m1.1c">73.9\pm 0.3\%</annotation></semantics></math></td>
<td id="S4.T1.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.8.6.3.m1.1" class="ltx_Math" alttext="\mathbf{75.1\pm 0.3\%}" display="inline"><semantics id="S4.T1.8.6.3.m1.1a"><mrow id="S4.T1.8.6.3.m1.1.1" xref="S4.T1.8.6.3.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S4.T1.8.6.3.m1.1.1.2" xref="S4.T1.8.6.3.m1.1.1.2.cmml">75.1</mn><mo id="S4.T1.8.6.3.m1.1.1.1" xref="S4.T1.8.6.3.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.8.6.3.m1.1.1.3" xref="S4.T1.8.6.3.m1.1.1.3.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S4.T1.8.6.3.m1.1.1.3.2" xref="S4.T1.8.6.3.m1.1.1.3.2.cmml">0.3</mn><mo id="S4.T1.8.6.3.m1.1.1.3.1" xref="S4.T1.8.6.3.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.8.6.3.m1.1b"><apply id="S4.T1.8.6.3.m1.1.1.cmml" xref="S4.T1.8.6.3.m1.1.1"><csymbol cd="latexml" id="S4.T1.8.6.3.m1.1.1.1.cmml" xref="S4.T1.8.6.3.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.8.6.3.m1.1.1.2.cmml" xref="S4.T1.8.6.3.m1.1.1.2">75.1</cn><apply id="S4.T1.8.6.3.m1.1.1.3.cmml" xref="S4.T1.8.6.3.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.8.6.3.m1.1.1.3.1.cmml" xref="S4.T1.8.6.3.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.8.6.3.m1.1.1.3.2.cmml" xref="S4.T1.8.6.3.m1.1.1.3.2">0.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.6.3.m1.1c">\mathbf{75.1\pm 0.3\%}</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.11.9" class="ltx_tr">
<th id="S4.T1.11.9.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">8 kHz</th>
<td id="S4.T1.9.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.9.7.1.m1.1" class="ltx_Math" alttext="70.4\pm 0.6\%" display="inline"><semantics id="S4.T1.9.7.1.m1.1a"><mrow id="S4.T1.9.7.1.m1.1.1" xref="S4.T1.9.7.1.m1.1.1.cmml"><mn id="S4.T1.9.7.1.m1.1.1.2" xref="S4.T1.9.7.1.m1.1.1.2.cmml">70.4</mn><mo id="S4.T1.9.7.1.m1.1.1.1" xref="S4.T1.9.7.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.9.7.1.m1.1.1.3" xref="S4.T1.9.7.1.m1.1.1.3.cmml"><mn id="S4.T1.9.7.1.m1.1.1.3.2" xref="S4.T1.9.7.1.m1.1.1.3.2.cmml">0.6</mn><mo id="S4.T1.9.7.1.m1.1.1.3.1" xref="S4.T1.9.7.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.9.7.1.m1.1b"><apply id="S4.T1.9.7.1.m1.1.1.cmml" xref="S4.T1.9.7.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.9.7.1.m1.1.1.1.cmml" xref="S4.T1.9.7.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.9.7.1.m1.1.1.2.cmml" xref="S4.T1.9.7.1.m1.1.1.2">70.4</cn><apply id="S4.T1.9.7.1.m1.1.1.3.cmml" xref="S4.T1.9.7.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.9.7.1.m1.1.1.3.1.cmml" xref="S4.T1.9.7.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.9.7.1.m1.1.1.3.2.cmml" xref="S4.T1.9.7.1.m1.1.1.3.2">0.6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.7.1.m1.1c">70.4\pm 0.6\%</annotation></semantics></math></td>
<td id="S4.T1.10.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.10.8.2.m1.1" class="ltx_Math" alttext="72.0\pm 0.6\%" display="inline"><semantics id="S4.T1.10.8.2.m1.1a"><mrow id="S4.T1.10.8.2.m1.1.1" xref="S4.T1.10.8.2.m1.1.1.cmml"><mn id="S4.T1.10.8.2.m1.1.1.2" xref="S4.T1.10.8.2.m1.1.1.2.cmml">72.0</mn><mo id="S4.T1.10.8.2.m1.1.1.1" xref="S4.T1.10.8.2.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.10.8.2.m1.1.1.3" xref="S4.T1.10.8.2.m1.1.1.3.cmml"><mn id="S4.T1.10.8.2.m1.1.1.3.2" xref="S4.T1.10.8.2.m1.1.1.3.2.cmml">0.6</mn><mo id="S4.T1.10.8.2.m1.1.1.3.1" xref="S4.T1.10.8.2.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.10.8.2.m1.1b"><apply id="S4.T1.10.8.2.m1.1.1.cmml" xref="S4.T1.10.8.2.m1.1.1"><csymbol cd="latexml" id="S4.T1.10.8.2.m1.1.1.1.cmml" xref="S4.T1.10.8.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.10.8.2.m1.1.1.2.cmml" xref="S4.T1.10.8.2.m1.1.1.2">72.0</cn><apply id="S4.T1.10.8.2.m1.1.1.3.cmml" xref="S4.T1.10.8.2.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.10.8.2.m1.1.1.3.1.cmml" xref="S4.T1.10.8.2.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.10.8.2.m1.1.1.3.2.cmml" xref="S4.T1.10.8.2.m1.1.1.3.2">0.6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.8.2.m1.1c">72.0\pm 0.6\%</annotation></semantics></math></td>
<td id="S4.T1.11.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.11.9.3.m1.1" class="ltx_Math" alttext="73.0\pm 0.6\%" display="inline"><semantics id="S4.T1.11.9.3.m1.1a"><mrow id="S4.T1.11.9.3.m1.1.1" xref="S4.T1.11.9.3.m1.1.1.cmml"><mn id="S4.T1.11.9.3.m1.1.1.2" xref="S4.T1.11.9.3.m1.1.1.2.cmml">73.0</mn><mo id="S4.T1.11.9.3.m1.1.1.1" xref="S4.T1.11.9.3.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.11.9.3.m1.1.1.3" xref="S4.T1.11.9.3.m1.1.1.3.cmml"><mn id="S4.T1.11.9.3.m1.1.1.3.2" xref="S4.T1.11.9.3.m1.1.1.3.2.cmml">0.6</mn><mo id="S4.T1.11.9.3.m1.1.1.3.1" xref="S4.T1.11.9.3.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.11.9.3.m1.1b"><apply id="S4.T1.11.9.3.m1.1.1.cmml" xref="S4.T1.11.9.3.m1.1.1"><csymbol cd="latexml" id="S4.T1.11.9.3.m1.1.1.1.cmml" xref="S4.T1.11.9.3.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.11.9.3.m1.1.1.2.cmml" xref="S4.T1.11.9.3.m1.1.1.2">73.0</cn><apply id="S4.T1.11.9.3.m1.1.1.3.cmml" xref="S4.T1.11.9.3.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.11.9.3.m1.1.1.3.1.cmml" xref="S4.T1.11.9.3.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.11.9.3.m1.1.1.3.2.cmml" xref="S4.T1.11.9.3.m1.1.1.3.2">0.6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.9.3.m1.1c">73.0\pm 0.6\%</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.14.12" class="ltx_tr">
<th id="S4.T1.14.12.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">16 kHz</th>
<td id="S4.T1.12.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.12.10.1.m1.1" class="ltx_Math" alttext="66.9\pm 0.6\%" display="inline"><semantics id="S4.T1.12.10.1.m1.1a"><mrow id="S4.T1.12.10.1.m1.1.1" xref="S4.T1.12.10.1.m1.1.1.cmml"><mn id="S4.T1.12.10.1.m1.1.1.2" xref="S4.T1.12.10.1.m1.1.1.2.cmml">66.9</mn><mo id="S4.T1.12.10.1.m1.1.1.1" xref="S4.T1.12.10.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.12.10.1.m1.1.1.3" xref="S4.T1.12.10.1.m1.1.1.3.cmml"><mn id="S4.T1.12.10.1.m1.1.1.3.2" xref="S4.T1.12.10.1.m1.1.1.3.2.cmml">0.6</mn><mo id="S4.T1.12.10.1.m1.1.1.3.1" xref="S4.T1.12.10.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.12.10.1.m1.1b"><apply id="S4.T1.12.10.1.m1.1.1.cmml" xref="S4.T1.12.10.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.12.10.1.m1.1.1.1.cmml" xref="S4.T1.12.10.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.12.10.1.m1.1.1.2.cmml" xref="S4.T1.12.10.1.m1.1.1.2">66.9</cn><apply id="S4.T1.12.10.1.m1.1.1.3.cmml" xref="S4.T1.12.10.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.12.10.1.m1.1.1.3.1.cmml" xref="S4.T1.12.10.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.12.10.1.m1.1.1.3.2.cmml" xref="S4.T1.12.10.1.m1.1.1.3.2">0.6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.10.1.m1.1c">66.9\pm 0.6\%</annotation></semantics></math></td>
<td id="S4.T1.13.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.13.11.2.m1.1" class="ltx_Math" alttext="69.5\pm 0.4\%" display="inline"><semantics id="S4.T1.13.11.2.m1.1a"><mrow id="S4.T1.13.11.2.m1.1.1" xref="S4.T1.13.11.2.m1.1.1.cmml"><mn id="S4.T1.13.11.2.m1.1.1.2" xref="S4.T1.13.11.2.m1.1.1.2.cmml">69.5</mn><mo id="S4.T1.13.11.2.m1.1.1.1" xref="S4.T1.13.11.2.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.13.11.2.m1.1.1.3" xref="S4.T1.13.11.2.m1.1.1.3.cmml"><mn id="S4.T1.13.11.2.m1.1.1.3.2" xref="S4.T1.13.11.2.m1.1.1.3.2.cmml">0.4</mn><mo id="S4.T1.13.11.2.m1.1.1.3.1" xref="S4.T1.13.11.2.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.13.11.2.m1.1b"><apply id="S4.T1.13.11.2.m1.1.1.cmml" xref="S4.T1.13.11.2.m1.1.1"><csymbol cd="latexml" id="S4.T1.13.11.2.m1.1.1.1.cmml" xref="S4.T1.13.11.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.13.11.2.m1.1.1.2.cmml" xref="S4.T1.13.11.2.m1.1.1.2">69.5</cn><apply id="S4.T1.13.11.2.m1.1.1.3.cmml" xref="S4.T1.13.11.2.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.13.11.2.m1.1.1.3.1.cmml" xref="S4.T1.13.11.2.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.13.11.2.m1.1.1.3.2.cmml" xref="S4.T1.13.11.2.m1.1.1.3.2">0.4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.11.2.m1.1c">69.5\pm 0.4\%</annotation></semantics></math></td>
<td id="S4.T1.14.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.14.12.3.m1.1" class="ltx_Math" alttext="71.5\pm 0.6\%" display="inline"><semantics id="S4.T1.14.12.3.m1.1a"><mrow id="S4.T1.14.12.3.m1.1.1" xref="S4.T1.14.12.3.m1.1.1.cmml"><mn id="S4.T1.14.12.3.m1.1.1.2" xref="S4.T1.14.12.3.m1.1.1.2.cmml">71.5</mn><mo id="S4.T1.14.12.3.m1.1.1.1" xref="S4.T1.14.12.3.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.14.12.3.m1.1.1.3" xref="S4.T1.14.12.3.m1.1.1.3.cmml"><mn id="S4.T1.14.12.3.m1.1.1.3.2" xref="S4.T1.14.12.3.m1.1.1.3.2.cmml">0.6</mn><mo id="S4.T1.14.12.3.m1.1.1.3.1" xref="S4.T1.14.12.3.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.14.12.3.m1.1b"><apply id="S4.T1.14.12.3.m1.1.1.cmml" xref="S4.T1.14.12.3.m1.1.1"><csymbol cd="latexml" id="S4.T1.14.12.3.m1.1.1.1.cmml" xref="S4.T1.14.12.3.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.14.12.3.m1.1.1.2.cmml" xref="S4.T1.14.12.3.m1.1.1.2">71.5</cn><apply id="S4.T1.14.12.3.m1.1.1.3.cmml" xref="S4.T1.14.12.3.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.14.12.3.m1.1.1.3.1.cmml" xref="S4.T1.14.12.3.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.14.12.3.m1.1.1.3.2.cmml" xref="S4.T1.14.12.3.m1.1.1.3.2">0.6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.12.3.m1.1c">71.5\pm 0.6\%</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.17.15" class="ltx_tr">
<th id="S4.T1.17.15.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">32 kHz</th>
<td id="S4.T1.15.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.15.13.1.m1.1" class="ltx_Math" alttext="67.3\pm 0.4\%" display="inline"><semantics id="S4.T1.15.13.1.m1.1a"><mrow id="S4.T1.15.13.1.m1.1.1" xref="S4.T1.15.13.1.m1.1.1.cmml"><mn id="S4.T1.15.13.1.m1.1.1.2" xref="S4.T1.15.13.1.m1.1.1.2.cmml">67.3</mn><mo id="S4.T1.15.13.1.m1.1.1.1" xref="S4.T1.15.13.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.15.13.1.m1.1.1.3" xref="S4.T1.15.13.1.m1.1.1.3.cmml"><mn id="S4.T1.15.13.1.m1.1.1.3.2" xref="S4.T1.15.13.1.m1.1.1.3.2.cmml">0.4</mn><mo id="S4.T1.15.13.1.m1.1.1.3.1" xref="S4.T1.15.13.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.15.13.1.m1.1b"><apply id="S4.T1.15.13.1.m1.1.1.cmml" xref="S4.T1.15.13.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.15.13.1.m1.1.1.1.cmml" xref="S4.T1.15.13.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.15.13.1.m1.1.1.2.cmml" xref="S4.T1.15.13.1.m1.1.1.2">67.3</cn><apply id="S4.T1.15.13.1.m1.1.1.3.cmml" xref="S4.T1.15.13.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.15.13.1.m1.1.1.3.1.cmml" xref="S4.T1.15.13.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.15.13.1.m1.1.1.3.2.cmml" xref="S4.T1.15.13.1.m1.1.1.3.2">0.4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.13.1.m1.1c">67.3\pm 0.4\%</annotation></semantics></math></td>
<td id="S4.T1.16.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.16.14.2.m1.1" class="ltx_Math" alttext="67.3\pm 0.6\%" display="inline"><semantics id="S4.T1.16.14.2.m1.1a"><mrow id="S4.T1.16.14.2.m1.1.1" xref="S4.T1.16.14.2.m1.1.1.cmml"><mn id="S4.T1.16.14.2.m1.1.1.2" xref="S4.T1.16.14.2.m1.1.1.2.cmml">67.3</mn><mo id="S4.T1.16.14.2.m1.1.1.1" xref="S4.T1.16.14.2.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.16.14.2.m1.1.1.3" xref="S4.T1.16.14.2.m1.1.1.3.cmml"><mn id="S4.T1.16.14.2.m1.1.1.3.2" xref="S4.T1.16.14.2.m1.1.1.3.2.cmml">0.6</mn><mo id="S4.T1.16.14.2.m1.1.1.3.1" xref="S4.T1.16.14.2.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.16.14.2.m1.1b"><apply id="S4.T1.16.14.2.m1.1.1.cmml" xref="S4.T1.16.14.2.m1.1.1"><csymbol cd="latexml" id="S4.T1.16.14.2.m1.1.1.1.cmml" xref="S4.T1.16.14.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.16.14.2.m1.1.1.2.cmml" xref="S4.T1.16.14.2.m1.1.1.2">67.3</cn><apply id="S4.T1.16.14.2.m1.1.1.3.cmml" xref="S4.T1.16.14.2.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.16.14.2.m1.1.1.3.1.cmml" xref="S4.T1.16.14.2.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.16.14.2.m1.1.1.3.2.cmml" xref="S4.T1.16.14.2.m1.1.1.3.2">0.6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.14.2.m1.1c">67.3\pm 0.6\%</annotation></semantics></math></td>
<td id="S4.T1.17.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T1.17.15.3.m1.1" class="ltx_Math" alttext="70.3\pm 0.2\%" display="inline"><semantics id="S4.T1.17.15.3.m1.1a"><mrow id="S4.T1.17.15.3.m1.1.1" xref="S4.T1.17.15.3.m1.1.1.cmml"><mn id="S4.T1.17.15.3.m1.1.1.2" xref="S4.T1.17.15.3.m1.1.1.2.cmml">70.3</mn><mo id="S4.T1.17.15.3.m1.1.1.1" xref="S4.T1.17.15.3.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.17.15.3.m1.1.1.3" xref="S4.T1.17.15.3.m1.1.1.3.cmml"><mn id="S4.T1.17.15.3.m1.1.1.3.2" xref="S4.T1.17.15.3.m1.1.1.3.2.cmml">0.2</mn><mo id="S4.T1.17.15.3.m1.1.1.3.1" xref="S4.T1.17.15.3.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.17.15.3.m1.1b"><apply id="S4.T1.17.15.3.m1.1.1.cmml" xref="S4.T1.17.15.3.m1.1.1"><csymbol cd="latexml" id="S4.T1.17.15.3.m1.1.1.1.cmml" xref="S4.T1.17.15.3.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.17.15.3.m1.1.1.2.cmml" xref="S4.T1.17.15.3.m1.1.1.2">70.3</cn><apply id="S4.T1.17.15.3.m1.1.1.3.cmml" xref="S4.T1.17.15.3.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.17.15.3.m1.1.1.3.1.cmml" xref="S4.T1.17.15.3.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.17.15.3.m1.1.1.3.2.cmml" xref="S4.T1.17.15.3.m1.1.1.3.2">0.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.15.3.m1.1c">70.3\pm 0.2\%</annotation></semantics></math></td>
</tr>
<tr id="S4.T1.20.18" class="ltx_tr">
<th id="S4.T1.20.18.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">64 kHz</th>
<td id="S4.T1.18.16.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T1.18.16.1.m1.1" class="ltx_Math" alttext="63.1\pm 1.3\%" display="inline"><semantics id="S4.T1.18.16.1.m1.1a"><mrow id="S4.T1.18.16.1.m1.1.1" xref="S4.T1.18.16.1.m1.1.1.cmml"><mn id="S4.T1.18.16.1.m1.1.1.2" xref="S4.T1.18.16.1.m1.1.1.2.cmml">63.1</mn><mo id="S4.T1.18.16.1.m1.1.1.1" xref="S4.T1.18.16.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.18.16.1.m1.1.1.3" xref="S4.T1.18.16.1.m1.1.1.3.cmml"><mn id="S4.T1.18.16.1.m1.1.1.3.2" xref="S4.T1.18.16.1.m1.1.1.3.2.cmml">1.3</mn><mo id="S4.T1.18.16.1.m1.1.1.3.1" xref="S4.T1.18.16.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.18.16.1.m1.1b"><apply id="S4.T1.18.16.1.m1.1.1.cmml" xref="S4.T1.18.16.1.m1.1.1"><csymbol cd="latexml" id="S4.T1.18.16.1.m1.1.1.1.cmml" xref="S4.T1.18.16.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.18.16.1.m1.1.1.2.cmml" xref="S4.T1.18.16.1.m1.1.1.2">63.1</cn><apply id="S4.T1.18.16.1.m1.1.1.3.cmml" xref="S4.T1.18.16.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.18.16.1.m1.1.1.3.1.cmml" xref="S4.T1.18.16.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.18.16.1.m1.1.1.3.2.cmml" xref="S4.T1.18.16.1.m1.1.1.3.2">1.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.18.16.1.m1.1c">63.1\pm 1.3\%</annotation></semantics></math></td>
<td id="S4.T1.19.17.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T1.19.17.2.m1.1" class="ltx_Math" alttext="65.9\pm 1.1\%" display="inline"><semantics id="S4.T1.19.17.2.m1.1a"><mrow id="S4.T1.19.17.2.m1.1.1" xref="S4.T1.19.17.2.m1.1.1.cmml"><mn id="S4.T1.19.17.2.m1.1.1.2" xref="S4.T1.19.17.2.m1.1.1.2.cmml">65.9</mn><mo id="S4.T1.19.17.2.m1.1.1.1" xref="S4.T1.19.17.2.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.19.17.2.m1.1.1.3" xref="S4.T1.19.17.2.m1.1.1.3.cmml"><mn id="S4.T1.19.17.2.m1.1.1.3.2" xref="S4.T1.19.17.2.m1.1.1.3.2.cmml">1.1</mn><mo id="S4.T1.19.17.2.m1.1.1.3.1" xref="S4.T1.19.17.2.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.19.17.2.m1.1b"><apply id="S4.T1.19.17.2.m1.1.1.cmml" xref="S4.T1.19.17.2.m1.1.1"><csymbol cd="latexml" id="S4.T1.19.17.2.m1.1.1.1.cmml" xref="S4.T1.19.17.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.19.17.2.m1.1.1.2.cmml" xref="S4.T1.19.17.2.m1.1.1.2">65.9</cn><apply id="S4.T1.19.17.2.m1.1.1.3.cmml" xref="S4.T1.19.17.2.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.19.17.2.m1.1.1.3.1.cmml" xref="S4.T1.19.17.2.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.19.17.2.m1.1.1.3.2.cmml" xref="S4.T1.19.17.2.m1.1.1.3.2">1.1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.19.17.2.m1.1c">65.9\pm 1.1\%</annotation></semantics></math></td>
<td id="S4.T1.20.18.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S4.T1.20.18.3.m1.1" class="ltx_Math" alttext="69.6\pm 1.1\%" display="inline"><semantics id="S4.T1.20.18.3.m1.1a"><mrow id="S4.T1.20.18.3.m1.1.1" xref="S4.T1.20.18.3.m1.1.1.cmml"><mn id="S4.T1.20.18.3.m1.1.1.2" xref="S4.T1.20.18.3.m1.1.1.2.cmml">69.6</mn><mo id="S4.T1.20.18.3.m1.1.1.1" xref="S4.T1.20.18.3.m1.1.1.1.cmml">±</mo><mrow id="S4.T1.20.18.3.m1.1.1.3" xref="S4.T1.20.18.3.m1.1.1.3.cmml"><mn id="S4.T1.20.18.3.m1.1.1.3.2" xref="S4.T1.20.18.3.m1.1.1.3.2.cmml">1.1</mn><mo id="S4.T1.20.18.3.m1.1.1.3.1" xref="S4.T1.20.18.3.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.20.18.3.m1.1b"><apply id="S4.T1.20.18.3.m1.1.1.cmml" xref="S4.T1.20.18.3.m1.1.1"><csymbol cd="latexml" id="S4.T1.20.18.3.m1.1.1.1.cmml" xref="S4.T1.20.18.3.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T1.20.18.3.m1.1.1.2.cmml" xref="S4.T1.20.18.3.m1.1.1.2">69.6</cn><apply id="S4.T1.20.18.3.m1.1.1.3.cmml" xref="S4.T1.20.18.3.m1.1.1.3"><csymbol cd="latexml" id="S4.T1.20.18.3.m1.1.1.3.1.cmml" xref="S4.T1.20.18.3.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T1.20.18.3.m1.1.1.3.2.cmml" xref="S4.T1.20.18.3.m1.1.1.3.2">1.1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.20.18.3.m1.1c">69.6\pm 1.1\%</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The impact of different frequency resolutions on the performance of various CNN14 models is summarized in Table <a href="#S4.T1" title="TABLE I ‣ IV-A Impact of Sampling Rate ‣ IV Results and Discussion ‣ Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. Results show that the CNN14_32k model achieves the highest accuracy on held-out test data sampled at 4 kHz with <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="75.1\pm 0.3\%" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">75.1</mn><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">±</mo><mrow id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml"><mn id="S4.SS1.p1.1.m1.1.1.3.2" xref="S4.SS1.p1.1.m1.1.1.3.2.cmml">0.3</mn><mo id="S4.SS1.p1.1.m1.1.1.3.1" xref="S4.SS1.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">75.1</cn><apply id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS1.p1.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.SS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS1.p1.1.m1.1.1.3.2">0.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">75.1\pm 0.3\%</annotation></semantics></math>. Fig. <a href="#S4.F2" title="Figure 2 ‣ IV-A Impact of Sampling Rate ‣ IV Results and Discussion ‣ Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>illustrates the average test accuracy across different data sampling rates. The error bars represent the standard deviation across three experimental runs. In this visual, clear trends can be observed over the various data sampling rates. The CNN14_32K model was robust across the different sampling rates as opposed to the other lower sampling rate models.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">These results suggest that the pre-trained models perform better when tested over lower data sample rates, while higher data sample rates do not necessarily improve accuracy. One advantage of a lower data sampling rate is reduced computational cost since spectrogram features have smaller sizes which require less memory. Also, all three models perform comparably when the data sampling rate is low. This finding suggests that higher frequency resolution training does not necessarily decrease a model’s ability to generalize to lower frequency resolutions. Rather, it appears that models are able to extract useful features even with reduced data resolution during fine-tuning.
The 32 kHz model was pre-trained on data with finer details (32,000 samples per second) compared to the 8 and 16 kHz models. Coarser DeepShip samples are still able to be correctly classified using the knowledge gained from AudioSet pre-training. Therefore, models pre-trained with higher frequency resolution data can be robust to fine-tuning data with different sampling rates, a finding which benefits applications where alternate sampling rates are used to process data, a fact true of many UATR datasets.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Impact of Pre-training Data</span>
</h3>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Average Test Accuracy for PANN and TIMM Models Across Three Experimental Runs of Random Initialization With <math id="S4.T2.2.m1.1" class="ltx_Math" alttext="\pm 1" display="inline"><semantics id="S4.T2.2.m1.1b"><mrow id="S4.T2.2.m1.1.1" xref="S4.T2.2.m1.1.1.cmml"><mo id="S4.T2.2.m1.1.1b" xref="S4.T2.2.m1.1.1.cmml">±</mo><mn id="S4.T2.2.m1.1.1.2" xref="S4.T2.2.m1.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.m1.1c"><apply id="S4.T2.2.m1.1.1.cmml" xref="S4.T2.2.m1.1.1"><csymbol cd="latexml" id="S4.T2.2.m1.1.1.1.cmml" xref="S4.T2.2.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S4.T2.2.m1.1.1.2.cmml" xref="S4.T2.2.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.m1.1d">\pm 1</annotation></semantics></math> Standard Deviation. The Highest Average Test Accuracy Is Bolded.</figcaption>
<table id="S4.T2.12" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.12.11.1" class="ltx_tr">
<th id="S4.T2.12.11.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Type</th>
<th id="S4.T2.12.11.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Model Name</th>
<th id="S4.T2.12.11.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
<th id="S4.T2.12.11.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Parameters</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.1" class="ltx_tr">
<td id="S4.T2.3.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="5"><span id="S4.T2.3.1.2.1" class="ltx_text">PANN</span></td>
<td id="S4.T2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CNN14-32k</td>
<td id="S4.T2.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.3.1.1.m1.1" class="ltx_Math" alttext="70.6\pm 0.8\%" display="inline"><semantics id="S4.T2.3.1.1.m1.1a"><mrow id="S4.T2.3.1.1.m1.1.1" xref="S4.T2.3.1.1.m1.1.1.cmml"><mn id="S4.T2.3.1.1.m1.1.1.2" xref="S4.T2.3.1.1.m1.1.1.2.cmml">70.6</mn><mo id="S4.T2.3.1.1.m1.1.1.1" xref="S4.T2.3.1.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T2.3.1.1.m1.1.1.3" xref="S4.T2.3.1.1.m1.1.1.3.cmml"><mn id="S4.T2.3.1.1.m1.1.1.3.2" xref="S4.T2.3.1.1.m1.1.1.3.2.cmml">0.8</mn><mo id="S4.T2.3.1.1.m1.1.1.3.1" xref="S4.T2.3.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.3.1.1.m1.1b"><apply id="S4.T2.3.1.1.m1.1.1.cmml" xref="S4.T2.3.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.3.1.1.m1.1.1.1.cmml" xref="S4.T2.3.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T2.3.1.1.m1.1.1.2.cmml" xref="S4.T2.3.1.1.m1.1.1.2">70.6</cn><apply id="S4.T2.3.1.1.m1.1.1.3.cmml" xref="S4.T2.3.1.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.3.1.1.m1.1.1.3.1.cmml" xref="S4.T2.3.1.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T2.3.1.1.m1.1.1.3.2.cmml" xref="S4.T2.3.1.1.m1.1.1.3.2">0.8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.1.1.m1.1c">70.6\pm 0.8\%</annotation></semantics></math></td>
<td id="S4.T2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79.7M</td>
</tr>
<tr id="S4.T2.4.2" class="ltx_tr">
<td id="S4.T2.4.2.2" class="ltx_td ltx_align_center ltx_border_r">Wavegram-Logmel-CNN</td>
<td id="S4.T2.4.2.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.4.2.1.m1.1" class="ltx_Math" alttext="68.6\pm 2.7\%" display="inline"><semantics id="S4.T2.4.2.1.m1.1a"><mrow id="S4.T2.4.2.1.m1.1.1" xref="S4.T2.4.2.1.m1.1.1.cmml"><mn id="S4.T2.4.2.1.m1.1.1.2" xref="S4.T2.4.2.1.m1.1.1.2.cmml">68.6</mn><mo id="S4.T2.4.2.1.m1.1.1.1" xref="S4.T2.4.2.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T2.4.2.1.m1.1.1.3" xref="S4.T2.4.2.1.m1.1.1.3.cmml"><mn id="S4.T2.4.2.1.m1.1.1.3.2" xref="S4.T2.4.2.1.m1.1.1.3.2.cmml">2.7</mn><mo id="S4.T2.4.2.1.m1.1.1.3.1" xref="S4.T2.4.2.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.2.1.m1.1b"><apply id="S4.T2.4.2.1.m1.1.1.cmml" xref="S4.T2.4.2.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.4.2.1.m1.1.1.1.cmml" xref="S4.T2.4.2.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T2.4.2.1.m1.1.1.2.cmml" xref="S4.T2.4.2.1.m1.1.1.2">68.6</cn><apply id="S4.T2.4.2.1.m1.1.1.3.cmml" xref="S4.T2.4.2.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.4.2.1.m1.1.1.3.1.cmml" xref="S4.T2.4.2.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T2.4.2.1.m1.1.1.3.2.cmml" xref="S4.T2.4.2.1.m1.1.1.3.2">2.7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.2.1.m1.1c">68.6\pm 2.7\%</annotation></semantics></math></td>
<td id="S4.T2.4.2.3" class="ltx_td ltx_align_center ltx_border_r">80.0M</td>
</tr>
<tr id="S4.T2.5.3" class="ltx_tr">
<td id="S4.T2.5.3.2" class="ltx_td ltx_align_center ltx_border_r">MobileNetV1</td>
<td id="S4.T2.5.3.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.5.3.1.m1.1" class="ltx_Math" alttext="67.1\pm 0.3\%" display="inline"><semantics id="S4.T2.5.3.1.m1.1a"><mrow id="S4.T2.5.3.1.m1.1.1" xref="S4.T2.5.3.1.m1.1.1.cmml"><mn id="S4.T2.5.3.1.m1.1.1.2" xref="S4.T2.5.3.1.m1.1.1.2.cmml">67.1</mn><mo id="S4.T2.5.3.1.m1.1.1.1" xref="S4.T2.5.3.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T2.5.3.1.m1.1.1.3" xref="S4.T2.5.3.1.m1.1.1.3.cmml"><mn id="S4.T2.5.3.1.m1.1.1.3.2" xref="S4.T2.5.3.1.m1.1.1.3.2.cmml">0.3</mn><mo id="S4.T2.5.3.1.m1.1.1.3.1" xref="S4.T2.5.3.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.3.1.m1.1b"><apply id="S4.T2.5.3.1.m1.1.1.cmml" xref="S4.T2.5.3.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.5.3.1.m1.1.1.1.cmml" xref="S4.T2.5.3.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T2.5.3.1.m1.1.1.2.cmml" xref="S4.T2.5.3.1.m1.1.1.2">67.1</cn><apply id="S4.T2.5.3.1.m1.1.1.3.cmml" xref="S4.T2.5.3.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.5.3.1.m1.1.1.3.1.cmml" xref="S4.T2.5.3.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T2.5.3.1.m1.1.1.3.2.cmml" xref="S4.T2.5.3.1.m1.1.1.3.2">0.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.3.1.m1.1c">67.1\pm 0.3\%</annotation></semantics></math></td>
<td id="S4.T2.5.3.3" class="ltx_td ltx_align_center ltx_border_r">4.3M</td>
</tr>
<tr id="S4.T2.6.4" class="ltx_tr">
<td id="S4.T2.6.4.2" class="ltx_td ltx_align_center ltx_border_r">ResNet38</td>
<td id="S4.T2.6.4.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.6.4.1.m1.1" class="ltx_Math" alttext="66.3\pm 0.3\%" display="inline"><semantics id="S4.T2.6.4.1.m1.1a"><mrow id="S4.T2.6.4.1.m1.1.1" xref="S4.T2.6.4.1.m1.1.1.cmml"><mn id="S4.T2.6.4.1.m1.1.1.2" xref="S4.T2.6.4.1.m1.1.1.2.cmml">66.3</mn><mo id="S4.T2.6.4.1.m1.1.1.1" xref="S4.T2.6.4.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T2.6.4.1.m1.1.1.3" xref="S4.T2.6.4.1.m1.1.1.3.cmml"><mn id="S4.T2.6.4.1.m1.1.1.3.2" xref="S4.T2.6.4.1.m1.1.1.3.2.cmml">0.3</mn><mo id="S4.T2.6.4.1.m1.1.1.3.1" xref="S4.T2.6.4.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.4.1.m1.1b"><apply id="S4.T2.6.4.1.m1.1.1.cmml" xref="S4.T2.6.4.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.6.4.1.m1.1.1.1.cmml" xref="S4.T2.6.4.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T2.6.4.1.m1.1.1.2.cmml" xref="S4.T2.6.4.1.m1.1.1.2">66.3</cn><apply id="S4.T2.6.4.1.m1.1.1.3.cmml" xref="S4.T2.6.4.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.6.4.1.m1.1.1.3.1.cmml" xref="S4.T2.6.4.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T2.6.4.1.m1.1.1.3.2.cmml" xref="S4.T2.6.4.1.m1.1.1.3.2">0.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.4.1.m1.1c">66.3\pm 0.3\%</annotation></semantics></math></td>
<td id="S4.T2.6.4.3" class="ltx_td ltx_align_center ltx_border_r">72.7M</td>
</tr>
<tr id="S4.T2.7.5" class="ltx_tr">
<td id="S4.T2.7.5.2" class="ltx_td ltx_align_center ltx_border_r">Res1dNet31</td>
<td id="S4.T2.7.5.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.7.5.1.m1.1" class="ltx_Math" alttext="63.2\pm 1.8\%" display="inline"><semantics id="S4.T2.7.5.1.m1.1a"><mrow id="S4.T2.7.5.1.m1.1.1" xref="S4.T2.7.5.1.m1.1.1.cmml"><mn id="S4.T2.7.5.1.m1.1.1.2" xref="S4.T2.7.5.1.m1.1.1.2.cmml">63.2</mn><mo id="S4.T2.7.5.1.m1.1.1.1" xref="S4.T2.7.5.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T2.7.5.1.m1.1.1.3" xref="S4.T2.7.5.1.m1.1.1.3.cmml"><mn id="S4.T2.7.5.1.m1.1.1.3.2" xref="S4.T2.7.5.1.m1.1.1.3.2.cmml">1.8</mn><mo id="S4.T2.7.5.1.m1.1.1.3.1" xref="S4.T2.7.5.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.5.1.m1.1b"><apply id="S4.T2.7.5.1.m1.1.1.cmml" xref="S4.T2.7.5.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.7.5.1.m1.1.1.1.cmml" xref="S4.T2.7.5.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T2.7.5.1.m1.1.1.2.cmml" xref="S4.T2.7.5.1.m1.1.1.2">63.2</cn><apply id="S4.T2.7.5.1.m1.1.1.3.cmml" xref="S4.T2.7.5.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.7.5.1.m1.1.1.3.1.cmml" xref="S4.T2.7.5.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T2.7.5.1.m1.1.1.3.2.cmml" xref="S4.T2.7.5.1.m1.1.1.3.2">1.8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.5.1.m1.1c">63.2\pm 1.8\%</annotation></semantics></math></td>
<td id="S4.T2.7.5.3" class="ltx_td ltx_align_center ltx_border_r">79.4M</td>
</tr>
<tr id="S4.T2.8.6" class="ltx_tr">
<td id="S4.T2.8.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" rowspan="5"><span id="S4.T2.8.6.2.1" class="ltx_text">TIMM</span></td>
<td id="S4.T2.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ConvNeXtV2-tiny</td>
<td id="S4.T2.8.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.8.6.1.m1.1" class="ltx_Math" alttext="\mathbf{73.7\pm 0.8\%}" display="inline"><semantics id="S4.T2.8.6.1.m1.1a"><mrow id="S4.T2.8.6.1.m1.1.1" xref="S4.T2.8.6.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S4.T2.8.6.1.m1.1.1.2" xref="S4.T2.8.6.1.m1.1.1.2.cmml">73.7</mn><mo id="S4.T2.8.6.1.m1.1.1.1" xref="S4.T2.8.6.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T2.8.6.1.m1.1.1.3" xref="S4.T2.8.6.1.m1.1.1.3.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S4.T2.8.6.1.m1.1.1.3.2" xref="S4.T2.8.6.1.m1.1.1.3.2.cmml">0.8</mn><mo id="S4.T2.8.6.1.m1.1.1.3.1" xref="S4.T2.8.6.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.6.1.m1.1b"><apply id="S4.T2.8.6.1.m1.1.1.cmml" xref="S4.T2.8.6.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.8.6.1.m1.1.1.1.cmml" xref="S4.T2.8.6.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T2.8.6.1.m1.1.1.2.cmml" xref="S4.T2.8.6.1.m1.1.1.2">73.7</cn><apply id="S4.T2.8.6.1.m1.1.1.3.cmml" xref="S4.T2.8.6.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.8.6.1.m1.1.1.3.1.cmml" xref="S4.T2.8.6.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T2.8.6.1.m1.1.1.3.2.cmml" xref="S4.T2.8.6.1.m1.1.1.3.2">0.8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.6.1.m1.1c">\mathbf{73.7\pm 0.8\%}</annotation></semantics></math></td>
<td id="S4.T2.8.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.9M</td>
</tr>
<tr id="S4.T2.9.7" class="ltx_tr">
<td id="S4.T2.9.7.2" class="ltx_td ltx_align_center ltx_border_r">RegNety-320</td>
<td id="S4.T2.9.7.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.9.7.1.m1.1" class="ltx_Math" alttext="72.5\pm 1.1\%" display="inline"><semantics id="S4.T2.9.7.1.m1.1a"><mrow id="S4.T2.9.7.1.m1.1.1" xref="S4.T2.9.7.1.m1.1.1.cmml"><mn id="S4.T2.9.7.1.m1.1.1.2" xref="S4.T2.9.7.1.m1.1.1.2.cmml">72.5</mn><mo id="S4.T2.9.7.1.m1.1.1.1" xref="S4.T2.9.7.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T2.9.7.1.m1.1.1.3" xref="S4.T2.9.7.1.m1.1.1.3.cmml"><mn id="S4.T2.9.7.1.m1.1.1.3.2" xref="S4.T2.9.7.1.m1.1.1.3.2.cmml">1.1</mn><mo id="S4.T2.9.7.1.m1.1.1.3.1" xref="S4.T2.9.7.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.7.1.m1.1b"><apply id="S4.T2.9.7.1.m1.1.1.cmml" xref="S4.T2.9.7.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.9.7.1.m1.1.1.1.cmml" xref="S4.T2.9.7.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T2.9.7.1.m1.1.1.2.cmml" xref="S4.T2.9.7.1.m1.1.1.2">72.5</cn><apply id="S4.T2.9.7.1.m1.1.1.3.cmml" xref="S4.T2.9.7.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.9.7.1.m1.1.1.3.1.cmml" xref="S4.T2.9.7.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T2.9.7.1.m1.1.1.3.2.cmml" xref="S4.T2.9.7.1.m1.1.1.3.2">1.1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.7.1.m1.1c">72.5\pm 1.1\%</annotation></semantics></math></td>
<td id="S4.T2.9.7.3" class="ltx_td ltx_align_center ltx_border_r">141M</td>
</tr>
<tr id="S4.T2.10.8" class="ltx_tr">
<td id="S4.T2.10.8.2" class="ltx_td ltx_align_center ltx_border_r">DenseNet201</td>
<td id="S4.T2.10.8.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.10.8.1.m1.1" class="ltx_Math" alttext="68.5\pm 2.0\%" display="inline"><semantics id="S4.T2.10.8.1.m1.1a"><mrow id="S4.T2.10.8.1.m1.1.1" xref="S4.T2.10.8.1.m1.1.1.cmml"><mn id="S4.T2.10.8.1.m1.1.1.2" xref="S4.T2.10.8.1.m1.1.1.2.cmml">68.5</mn><mo id="S4.T2.10.8.1.m1.1.1.1" xref="S4.T2.10.8.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T2.10.8.1.m1.1.1.3" xref="S4.T2.10.8.1.m1.1.1.3.cmml"><mn id="S4.T2.10.8.1.m1.1.1.3.2" xref="S4.T2.10.8.1.m1.1.1.3.2.cmml">2.0</mn><mo id="S4.T2.10.8.1.m1.1.1.3.1" xref="S4.T2.10.8.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.10.8.1.m1.1b"><apply id="S4.T2.10.8.1.m1.1.1.cmml" xref="S4.T2.10.8.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.10.8.1.m1.1.1.1.cmml" xref="S4.T2.10.8.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T2.10.8.1.m1.1.1.2.cmml" xref="S4.T2.10.8.1.m1.1.1.2">68.5</cn><apply id="S4.T2.10.8.1.m1.1.1.3.cmml" xref="S4.T2.10.8.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.10.8.1.m1.1.1.3.1.cmml" xref="S4.T2.10.8.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T2.10.8.1.m1.1.1.3.2.cmml" xref="S4.T2.10.8.1.m1.1.1.3.2">2.0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.8.1.m1.1c">68.5\pm 2.0\%</annotation></semantics></math></td>
<td id="S4.T2.10.8.3" class="ltx_td ltx_align_center ltx_border_r">18.1M</td>
</tr>
<tr id="S4.T2.11.9" class="ltx_tr">
<td id="S4.T2.11.9.2" class="ltx_td ltx_align_center ltx_border_r">ResNet50</td>
<td id="S4.T2.11.9.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.11.9.1.m1.1" class="ltx_Math" alttext="68.2\pm 0.7\%" display="inline"><semantics id="S4.T2.11.9.1.m1.1a"><mrow id="S4.T2.11.9.1.m1.1.1" xref="S4.T2.11.9.1.m1.1.1.cmml"><mn id="S4.T2.11.9.1.m1.1.1.2" xref="S4.T2.11.9.1.m1.1.1.2.cmml">68.2</mn><mo id="S4.T2.11.9.1.m1.1.1.1" xref="S4.T2.11.9.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T2.11.9.1.m1.1.1.3" xref="S4.T2.11.9.1.m1.1.1.3.cmml"><mn id="S4.T2.11.9.1.m1.1.1.3.2" xref="S4.T2.11.9.1.m1.1.1.3.2.cmml">0.7</mn><mo id="S4.T2.11.9.1.m1.1.1.3.1" xref="S4.T2.11.9.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.11.9.1.m1.1b"><apply id="S4.T2.11.9.1.m1.1.1.cmml" xref="S4.T2.11.9.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.11.9.1.m1.1.1.1.cmml" xref="S4.T2.11.9.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T2.11.9.1.m1.1.1.2.cmml" xref="S4.T2.11.9.1.m1.1.1.2">68.2</cn><apply id="S4.T2.11.9.1.m1.1.1.3.cmml" xref="S4.T2.11.9.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.11.9.1.m1.1.1.3.1.cmml" xref="S4.T2.11.9.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T2.11.9.1.m1.1.1.3.2.cmml" xref="S4.T2.11.9.1.m1.1.1.3.2">0.7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.9.1.m1.1c">68.2\pm 0.7\%</annotation></semantics></math></td>
<td id="S4.T2.11.9.3" class="ltx_td ltx_align_center ltx_border_r">23.5M</td>
</tr>
<tr id="S4.T2.12.10" class="ltx_tr">
<td id="S4.T2.12.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">MobilenetV3-large-100</td>
<td id="S4.T2.12.10.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><math id="S4.T2.12.10.1.m1.1" class="ltx_Math" alttext="65.4\pm 0.7\%" display="inline"><semantics id="S4.T2.12.10.1.m1.1a"><mrow id="S4.T2.12.10.1.m1.1.1" xref="S4.T2.12.10.1.m1.1.1.cmml"><mn id="S4.T2.12.10.1.m1.1.1.2" xref="S4.T2.12.10.1.m1.1.1.2.cmml">65.4</mn><mo id="S4.T2.12.10.1.m1.1.1.1" xref="S4.T2.12.10.1.m1.1.1.1.cmml">±</mo><mrow id="S4.T2.12.10.1.m1.1.1.3" xref="S4.T2.12.10.1.m1.1.1.3.cmml"><mn id="S4.T2.12.10.1.m1.1.1.3.2" xref="S4.T2.12.10.1.m1.1.1.3.2.cmml">0.7</mn><mo id="S4.T2.12.10.1.m1.1.1.3.1" xref="S4.T2.12.10.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.12.10.1.m1.1b"><apply id="S4.T2.12.10.1.m1.1.1.cmml" xref="S4.T2.12.10.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.12.10.1.m1.1.1.1.cmml" xref="S4.T2.12.10.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T2.12.10.1.m1.1.1.2.cmml" xref="S4.T2.12.10.1.m1.1.1.2">65.4</cn><apply id="S4.T2.12.10.1.m1.1.1.3.cmml" xref="S4.T2.12.10.1.m1.1.1.3"><csymbol cd="latexml" id="S4.T2.12.10.1.m1.1.1.3.1.cmml" xref="S4.T2.12.10.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="S4.T2.12.10.1.m1.1.1.3.2.cmml" xref="S4.T2.12.10.1.m1.1.1.3.2">0.7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.10.1.m1.1c">65.4\pm 0.7\%</annotation></semantics></math></td>
<td id="S4.T2.12.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">4.2M</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">The comparison between PANN and TIMM models is presented in Table <a href="#S4.T2" title="TABLE II ‣ IV-B Impact of Pre-training Data ‣ IV Results and Discussion ‣ Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. Within the PANN models, the CNN14-32k shows the highest accuracy at 70.6 <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mo id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\pm</annotation></semantics></math> 0.8%, suggesting that this model is better suited to the task of passive sonar classification. This result is also consistent with that shown in the PANN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> work asserting that CNN14 was the best model across a variety of audio datasets. The PANN Wavegram-Logmel-CNN also performs competitively, considering the larger error margin. Amongst the TIMM models, the ConvNeXtV2-tiny model stands out with the highest test accuracy of 73.7 <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mo id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><csymbol cd="latexml" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\pm</annotation></semantics></math> 0.8%. RegNety-320 also performs competitively, however with five times the number of model parameters. The results show that more complex models (<span id="S4.SS2.p1.2.1" class="ltx_text ltx_font_italic">i.e.</span>, more parameters) do not necessarily lead to a proportional increase in accuracy. The ConvNeXTV2-tiny model’s self-supervised training approach which uses a fully convolutional masked autoencoder may provide some explanation as to its high comparative performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> .</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Despite having a lower number of parameters, the MobileNetV1 still performs competitively with more computationally expensive models. The model’s efficient use of parameters could be useful for applications requiring lightweight architectures without sacrificing accuracy. When compared to the pre-trained ImageNet version (MobilenetV3-large-100), there is a statistically significant difference in performance. This result is intuitive as one would expect that PANN models should out-perform models pre-trained on image data when fine-tuning on sonar data as it is a much closer analog to audio data than photo imagery. However, we do not see this trend overall as most of the PANN models are outperformed by the TIMM models. Pre-training on a large dataset such as ImageNet appears to capture features that can be broadly useful in other data modalities (such as passive sonar).</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Best PANN and TIMM Models Comparison</span>
</h3>

<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.13878/assets/Figures/Moham5.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_img_square" width="141" height="141" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.13878/assets/Figures/Moham6.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_img_square" width="141" height="141" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Average confusion matrices for best PANN model (a) CNN14-32k (70.6 <math id="S4.F3.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.F3.4.m1.1b"><mo id="S4.F3.4.m1.1.1" xref="S4.F3.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.F3.4.m1.1c"><csymbol cd="latexml" id="S4.F3.4.m1.1.1.cmml" xref="S4.F3.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.4.m1.1d">\pm</annotation></semantics></math> 0.8) and best TIMM model (b) ConvNeXtV2-tiny (73.7 <math id="S4.F3.5.m2.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.F3.5.m2.1b"><mo id="S4.F3.5.m2.1.1" xref="S4.F3.5.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.F3.5.m2.1c"><csymbol cd="latexml" id="S4.F3.5.m2.1.1.cmml" xref="S4.F3.5.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.5.m2.1d">\pm</annotation></semantics></math> 0.8) across three experimental runs. The average test accuracy <math id="S4.F3.6.m3.1" class="ltx_Math" alttext="\pm 1" display="inline"><semantics id="S4.F3.6.m3.1b"><mrow id="S4.F3.6.m3.1.1" xref="S4.F3.6.m3.1.1.cmml"><mo id="S4.F3.6.m3.1.1b" xref="S4.F3.6.m3.1.1.cmml">±</mo><mn id="S4.F3.6.m3.1.1.2" xref="S4.F3.6.m3.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.6.m3.1c"><apply id="S4.F3.6.m3.1.1.cmml" xref="S4.F3.6.m3.1.1"><csymbol cd="latexml" id="S4.F3.6.m3.1.1.1.cmml" xref="S4.F3.6.m3.1.1">plus-or-minus</csymbol><cn type="integer" id="S4.F3.6.m3.1.1.2.cmml" xref="S4.F3.6.m3.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.6.m3.1d">\pm 1</annotation></semantics></math> standard deviation is shown in parentheses.</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To further analyze the performance of the models, confusion matrix was referenced. This visualization provides deeper insights into the model performance characteristics beyond accuracy metrics. According to the confusion matrices in Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-C Best PANN and TIMM Models Comparison ‣ IV Results and Discussion ‣ Transfer Learning for Passive Sonar Classification using Pre-trained Audio and ImageNet Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrating the best PANN and TIMM models, the CNN14 model better identifies the tanker class when compared to the ConvNeXtV2-tiny model on average. This result indicates that the CNN14 has learned distinctive features specific to this class, which can be more useful in applications where precise classification of vessel types is required. It is worth noting the lower standard deviation for ConvNeXtV2-tiny, suggesting a more consistent performance across different trials, and therefore, potentially a more robust model. Despite ConvNeXtV2-tiny outperforming other models, the model had difficulty differentiating between cargo and tanker as well as between passengership and tug classes. This result suggests that the learned features are not distinct enough. Additional spectrogam features or other learning paradigms (<span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">e.g.</span>, contrastive learning) may help capture unique acoustic signatures of each class.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This investigation into the application of pre-trained models to passive sonar classification using the DeepShip dataset contributes to community understanding of the influence signal processing parameterization and model pre-training can have on classifier performance. It was confirmed that adjusting the data sampling rate can impact the performance of neural networks, with the CNN14-32k model displaying higher performance across lower frequencies. Furthermore, our comparative analysis of PANN and TIMM models showed that models trained on visual datasets slightly out-performed those trained on audio datasets when fine-tuned and applied to passive sonar classification tasks. Results, herein, show the importance of exploring architectures based on their performance across data modalities, rather than strictly applying models on the same data used for pre-training.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Future work could explore the integration of multi-modal data to further improve the accuracy of classification models in complex acoustic environments. Additionally, self-supervised learning approaches such as masked autoencoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> may assist in more effectively modifying the feature representation of all models for UATR. Other interesting areas of study include investigating parameter efficient transfer learning approaches such as adaptors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> instead of fully fine-tuning the models in order to limit computational expense. Given their widely touted state-of-the-art capabilities, comparison to transformer architectures as opposed to convolution neural networks would also be an interesting research topic to explore.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K. Zaman, M. Sah, C. Direkoglu, and M. Unoki, “A survey of audio classification using deep learning,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 11, pp. 106 620–106 649, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Esposito, G. Uehara, and A. Spanias, “Quantum machine learning for audio classification with applications to healthcare,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2022 13th International Conference on Information, Intelligence, Systems &amp; Applications (IISA)</em>.   IEEE, 2022, pp. 1–4.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Tyagi, K. Aggarwal, D. Kumar, S. Garg <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Urban sound classification for audio analysis using long short term memory,” <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">NEU Journal for Artificial Intelligence and Internet of Things</em>, vol. 1, no. 1, pp. 1–11, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
W. Mu, B. Yin, X. Huang, J. Xu, and Z. Du, “Environmental sound classification using temporal-frequency attention based convolutional neural network,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Scientific Reports</em>, vol. 11, no. 1, p. 21552, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S. Tian, D. Chen, H. Wang, and J. Liu, “Deep convolution stack for waveform in underwater acoustic target recognition,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Scientific reports</em>, vol. 11, no. 1, p. 9614, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M. Thomas, B. Martin, K. Kowarski, B. Gaudet, and S. Matwin, “Marine mammal species classification using convolutional neural networks and a novel acoustic representation,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2019, Würzburg, Germany, September 16–20, 2019, Proceedings, Part III</em>.   Springer, 2020, pp. 290–305.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
E. V. Carrera and C. Soria, “Positioning of autonomous underwater vehicles using machine learning techniques,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2023 IEEE Seventh Ecuador Technical Chapters Meeting (ECTM)</em>, 2023, pp. 1–6.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
B. Beckler, A. Pfau, M. Orescanin, S. Atchley, N. Villemez, J. E. Joseph, C. W. Miller, and T. Margolina, “Multilabel classification of heterogeneous underwater soundscapes with bayesian deep learning,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Oceanic Engineering</em>, vol. 47, no. 4, pp. 1143–1154, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Yang, K. Lee, Y. Choo, and K. Kim, “Underwater acoustic research trends with machine learning: Passive sonar applications,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Journal of Ocean Engineering and Technology</em>, vol. 34, no. 3, pp. 227–236, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
X. Yao, S. Liu, J. Chen, S. Yan, F. Ji, H. Liu, and J. Chen, “Underwater acoustic target classification using scattering transform with small sample size,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Sensors Journal</em>, 2024.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
X. Li, Y. Grandvalet, F. Davoine, J. Cheng, Y. Cui, H. Zhang, S. Belongie, Y.-H. Tsai, and M.-H. Yang, “Transfer learning in computer vision tasks: Remember where you come from,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Image and Vision Computing</em>, vol. 93, p. 103853, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He, “A comprehensive survey on transfer learning,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, vol. 109, no. 1, pp. 43–76, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, “Panns: Large-scale pretrained audio neural networks for audio pattern recognition,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 28, pp. 2880–2894, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
R. Wightman, “Pytorch image models,” <a target="_blank" href="https://github.com/rwightman/pytorch-image-models" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/rwightman/pytorch-image-models</a>, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Irfan, Z. Jiangbin, S. Ali, M. Iqbal, Z. Masood, and U. Hamid, “Deepship: An underwater acoustic benchmark dataset and a separable convolution based autoencoder for classification,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, vol. 183, p. 115270, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset for audio events,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.   IEEE, 2017, pp. 776–780.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 2009, pp. 248–255.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
F. Wu, H. Yao, and H. Wang, “Recognizing the state of motion by ship-radiated noise using time-frequency swin-transformer,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Oceanic Engineering</em>, 2024.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
D. Santos-Domínguez, S. Torres-Guijarro, A. Cardenal-López, and A. Pena-Gimenez, “Shipsear: An underwater vessel noise database,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Applied Acoustics</em>, vol. 113, pp. 64–69, 2016.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, “Specaugment: A simple data augmentation method for automatic speech recognition,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.08779</em>, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical risk minimization,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR)</em>, 2018.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Ritu, E. Barnes, R. Martell, A. Van Dine, and J. Peeples, “Histogram layer time delay neural networks for passive sonar classification,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. Woo, S. Debnath, R. Hu, X. Chen, Z. Liu, I. S. Kweon, and S. Xie, “Convnext v2: Co-designing and scaling convnets with masked autoencoders,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 16 133–16 142.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, “Parameter-efficient transfer learning for nlp,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2019, pp. 2790–2799.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.13877" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.13878" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.13878">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.13878" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.13879" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 20:16:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
