<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.13502] ManWav: The First Manchu ASR Model</title><meta property="og:description" content="This study addresses the widening gap in Automatic Speech Recognition (ASR) research between high resource and extremely low resource languages, with a particular focus on Manchu, a critically endangered language. Manc…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ManWav: The First Manchu ASR Model">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ManWav: The First Manchu ASR Model">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.13502">

<!--Generated on Fri Jul  5 17:45:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span id="id1.id1" class="ltx_text ltx_font_italic">ManWav</span>: The First Manchu ASR Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jean Seo,  Minha Kang,  Sungjoo Byun, Sangah Lee 
<br class="ltx_break">Seoul National University 
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter">{seemdog, alsgk1123, byunsj, sanalee}@snu.ac.kr</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">This study addresses the widening gap in Automatic Speech Recognition (ASR) research between high resource and extremely low resource languages, with a particular focus on Manchu, a critically endangered language. Manchu exemplifies the challenges faced by marginalized linguistic communities in accessing state-of-the-art technologies. In a pioneering effort, we introduce the first-ever Manchu ASR model <span id="id3.id1.1" class="ltx_text ltx_font_italic">ManWav</span>, leveraging Wav2Vec2-XLSR-53. The results of the first Manchu ASR is promising, especially when trained with our augmented data. Wav2Vec2-XLSR-53 fine-tuned with augmented data demonstrates a 0.02 drop in CER and 0.13 drop in WER compared to the same base model fine-tuned with original data.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The landscape of Automatic Speech Recognition (ASR) research has centered around high resource languages such as English. This concentrated attention on high resource languages has deepened the divide between research advancements. While research on English ASR encompasses diverse linguistic variations, including accented and noised speech, the same cannot be said for many low resource languages, though a few basic research including <cite class="ltx_cite ltx_citemacro_citet">Safonova et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Zhou et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite> exist. Astonishingly, not a single basic ASR model has been developed for Manchu to date, highlighting a critical void in linguistic inclusivity within the realm of ASR technology.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The development of a Manchu ASR model holds particular importance in the field of linguistics, as there are no more native speakers of Manchu. Consequently, the available data, whether text or audio, for linguistic study is limited and cannot be replenished. Therefore, it is crucial to maximize the utilization of existing data. However, due to the scarcity of individuals capable of transcribing Manchu audio data, unlabeled data remain unused. If transcribed, this data could prove to be invaluable resource for Manchu research and preservation. Even though the performance of the Manchu ASR system may not be perfect, it would be immensely helpful if it could provide draft transcriptions. This would enable researchers to revise and incorporate them into their studies.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This paper sets out to address the significant gap between high and low resource languages by developing the inaugural Manchu ASR model. This endeavor is underscored by the scarcity of linguistic resources, prompting us to collect all existing Manchu audio data from <cite class="ltx_cite ltx_citemacro_citet">Kim et al. (<a href="#bib.bib10" title="" class="ltx_ref">2008</a>)</cite> in one channel. We try to maximize the cross-lingual capabilities of Wav2Vec2-XLSR-53 <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite> by fine-tuning the model with Manchu audio data. The performance of the Manchu ASR model is further enhanced through data augmentation.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The contributions of this study are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Collecting Manchu audio data in an unified format and correcting corresponding transcriptions</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Developing the very first Manchu ASR model with augmented data</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Manchu Language</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The Manchu language, a member of the Tungusic linguistic family, has its roots among the Manchu people of Northeast China and boasts a significant historical role as the official language of the Qing dynasty (1644-1912). Presently, the language confronts a dire state of endangerment, officially denoted a dead language with no more native speakers left.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">There have been some efforts to employ technological solutions in the preservation and revitalization of Manchu. These endeavors include the Manchu spell checker <cite class="ltx_cite ltx_citemacro_citep">(You, <a href="#bib.bib18" title="" class="ltx_ref">2014</a>)</cite>, Manchu-Korean machine translation <cite class="ltx_cite ltx_citemacro_citep">(Seo et al., <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>, and Manchu NER/POS tagging models <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a href="#bib.bib11" title="" class="ltx_ref">2024</a>)</cite>. However, due to the paucity of data, the studies above face challenges and no ASR model has been yet developed.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Materials</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">This study leverages Colloquial Manchu data provided by <cite class="ltx_cite ltx_citemacro_citet">Kim et al. (<a href="#bib.bib10" title="" class="ltx_ref">2008</a>)</cite>, in which Colloquial Manchu data is gathered as part of ASK REAL project (Altaic Society of Korea, Researches on Endangered Altaic Languagess <cite class="ltx_cite ltx_citemacro_citep">(Choi et al., <a href="#bib.bib3" title="" class="ltx_ref">2012</a>)</cite>). This audio data represents the dialect of Sanjiazi village, located in the Youyi Dowoerzu Manzu Ke’er-kezizu township, Fuyu county, Heilongjiang Province.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The recording took place from February 7th to 14th, 2006 in Qiqihar, Heilongjiang Province, with Mr. Meng Xianxiao (73 years old at that moment). Though Chinese being his first language, Mr. Meng Xianxiao sufficiently served as the speaker, acquiring a comprehensive ability of Manchu by the age of 12.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The data we use in this study is the recordings of the basic conversational expressions and the sentences for grammatical analysis. The length of each recording is 32 minutes and 58 minutes, for a total of 90 minutes. Corresponding transcriptions are basically provided by <cite class="ltx_cite ltx_citemacro_citet">Kim et al. (<a href="#bib.bib10" title="" class="ltx_ref">2008</a>)</cite> and went through some revisions by a Manchu researcher from Seoul National University for better precision.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Transcription</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The phoneme transcription system in this study is based on <cite class="ltx_cite ltx_citemacro_citet">Kim et al. (<a href="#bib.bib10" title="" class="ltx_ref">2008</a>)</cite>. While it shares similarities with the International Phonetic Alphabet (IPA), our system incorporates some distinctions. Specifically, /b, d, g/ represent voiceless unaspirated stops, and /p, t, k/ denote voiceless aspirated stops. Notably, Colloquial Manchu lacks voiced stops, making this transcription system more practical than using diacritic /<sup id="S3.SS2.p1.1.1" class="ltx_sup">h</sup>/ to indicate aspiration. Next, /ǰ, č, š/ denote voiceless palatal sounds. In IPA system, corresponding sound symbols are [, ç, ]. But /ǰ/ is not voiced unlike [], and /č/ is the aspirated sound, [č<sup id="S3.SS2.p1.1.2" class="ltx_sup">h</sup>]. Some examples can be found in Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Transcription ‣ 3 Data ‣ ManWav: The First Manchu ASR Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:206pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(103.8pt,-58.0pt) scale(2.2885213226435,2.2885213226435) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Transcription</span></th>
<th id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">IPA</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">miŋ nj bitk sw.</td>
<td id="S3.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">miŋ ni pitk sw.</td>
</tr>
<tr id="S3.T1.1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.1.3.2.1" class="ltx_td ltx_align_center" colspan="2">(Translation: My mother is a teacher.)</td>
</tr>
<tr id="S3.T1.1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_t">došn ǰo.</td>
<td id="S3.T1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">to zn d zo.</td>
</tr>
<tr id="S3.T1.1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b" colspan="2">(Translation: Come on in.)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Examples of our transcription, IPA, and corresponding translation.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data Augmentation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The scarcity of speech datasets from native Manchu speakers presents a significant challenge, necessitating the adoption of various data augmentation methods. Audio data augmentation methods used to simulate different acoustic environments include:</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Additive noise</span>: Adding background noise to the audio samples.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Clipping</span>: Involves cutting short the audio signals.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Reverberation</span>: Applying reverberation effects.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Time dropout</span>: Randomly removing segments of the audio.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">By implementing the above techniques through WavAugment<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/facebookresearch/WavAugment</span></span></span> provided by <cite class="ltx_cite ltx_citemacro_citet">Kharitonov et al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>, we expand the dataset by 100% respectively, to a total of 400%, significantly enriching the available train data. Notable is the fact that data augmentation is implemented after the separation of train and test data, ensuring more reliable test results by preventing overlap between the train and test sets. The size of data before and after augmentation is described in Table <a href="#S3.T2" title="Table 2 ‣ 3.3 Data Augmentation ‣ 3 Data ‣ ManWav: The First Manchu ASR Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Before Augmentation</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Duration</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<td id="S3.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">train</td>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">81 min</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<td id="S3.T2.1.3.2.1" class="ltx_td ltx_align_center">test</td>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_center">9.5 min</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<td id="S3.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.4.3.1.1" class="ltx_text ltx_font_bold">After Augmentation</span></td>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.4.3.2.1" class="ltx_text ltx_font_bold">Duration</span></td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<td id="S3.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_t">train</td>
<td id="S3.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t">326.5 min</td>
</tr>
<tr id="S3.T2.1.6.5" class="ltx_tr">
<td id="S3.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_border_b">test</td>
<td id="S3.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b">9.5 min</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The duration of audio files(.wav) in minutes before and after augmentation.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Wav2Vec2-XLSR-53 <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite> is utilized as the base model. Wav2Vec2-XLSR-53 is a multilingual self-supervised learning (SSL) model from Meta AI<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://ai.meta.com/</span></span></span> pre-trained with 53 languages. A Wav2Vec2-XLSR-53 model is fine-tuned in two different types of data, leading to two separate fine-tuned models: one with original Manchu data, and the other with augmented Manchu data. We name the model trained with augmented data <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">ManWav</span>. The fine-tuning process is conducted through HuggingSound <cite class="ltx_cite ltx_citemacro_citep">(Grosman, <a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Setup</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Our experiments are conducted using an NVIDIA A100 GPU. We fine-tune our models with learning rate 3e-4, batch size 16, and dropout rate of 0.1. We train Wav2Vec2-XLSR-53 with 400% augmented data for 1 epoch. On the other hand, Wav2Vec2-XLSR-53 with original data is trained for 5 epochs, ensuring identical train data size for fair comparison.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Result and Discussion</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Result</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We use Character Error Rate (CER) and Word Error Rate (WER) as evaluation metrics. CER assesses the accuracy of character transcription, while WER measures the correctness of word recognition. Scores closer to 0 represent better performances in both metrics. WER and CER are the most common and essential metrics in gauging the overall performance of ASR systems.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The experimental results prove the significance of data augmentation in fine-tuning the base model. As depicted in Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Result ‣ 5 Result and Discussion ‣ ManWav: The First Manchu ASR Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, using augmented data at the training stage clearly improves the performance, specifically dropping CER by 0.02 and WER by 0.13, indicating the effectiveness using augmented data described in Section <a href="#S3.SS3" title="3.3 Data Augmentation ‣ 3 Data ‣ ManWav: The First Manchu ASR Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Moreover, Table <a href="#S5.T4" title="Table 4 ‣ 5.1 Result ‣ 5 Result and Discussion ‣ ManWav: The First Manchu ASR Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the promising capabilities of <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">ManWav</span> in the Manchu speech recognition task. The achieved accuracy is particularly noteworthy given the limited availability of Manchu speech data and considering that Wav2Vec2-XLSR-53 is not initially pre-trained on Manchu.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Data Augmentation</span></th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">CER</span></th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">WER</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T3.1.2.1.1.1" class="ltx_text ltx_font_bold">before</span></th>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.13</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.44</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<th id="S5.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T3.1.3.2.1.1" class="ltx_text ltx_font_bold">after</span></th>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T3.1.3.2.2.1" class="ltx_text ltx_font_bold">0.11</span></td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T3.1.3.2.3.1" class="ltx_text ltx_font_bold">0.31</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The performance of Wav2Vec2-XLSR-53 each trained with data before and after augmentation.</figcaption>
</figure>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Model Prediction</span></td>
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Actual Transcription</span></td>
</tr>
<tr id="S5.T4.1.2.2" class="ltx_tr">
<td id="S5.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">si jawuči bi gl jaam si jawuči bi gl jaam</td>
<td id="S5.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">si jawuči bi gl jaam si jawuči bi gl jaam</td>
</tr>
<tr id="S5.T4.1.3.3" class="ltx_tr">
<td id="S5.T4.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r">tl <span id="S5.T4.1.3.3.1.1" class="ltx_text" style="color:#FF0000;">am</span> <span id="S5.T4.1.3.3.1.2" class="ltx_text" style="color:#FF0000;">dulk</span> ani mk iči bo alx</td>
<td id="S5.T4.1.3.3.2" class="ltx_td ltx_align_center">tl <span id="S5.T4.1.3.3.2.1" class="ltx_text" style="color:#0000FF;">am</span> <span id="S5.T4.1.3.3.2.2" class="ltx_text" style="color:#0000FF;">dulk</span> ani mk iči bo alx</td>
</tr>
<tr id="S5.T4.1.4.4" class="ltx_tr">
<td id="S5.T4.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r">bi sajw wak bi sajw wak</td>
<td id="S5.T4.1.4.4.2" class="ltx_td ltx_align_center">bi sajw wak bi sajw wak</td>
</tr>
<tr id="S5.T4.1.5.5" class="ltx_tr">
<td id="S5.T4.1.5.5.1" class="ltx_td ltx_align_center ltx_border_r">bi sisk bitk xolal ba d jom mutulko</td>
<td id="S5.T4.1.5.5.2" class="ltx_td ltx_align_center">bi sisk bitk xolal ba d jom mutulko</td>
</tr>
<tr id="S5.T4.1.6.6" class="ltx_tr">
<td id="S5.T4.1.6.6.1" class="ltx_td ltx_align_center ltx_border_r">min do bitk xolal ba joxo</td>
<td id="S5.T4.1.6.6.2" class="ltx_td ltx_align_center">min do bitk xolal ba joxo</td>
</tr>
<tr id="S5.T4.1.7.7" class="ltx_tr">
<td id="S5.T4.1.7.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">odun gjak <span id="S5.T4.1.7.7.1.1" class="ltx_text" style="color:#FF0000;">šaxulo</span> odun gjak <span id="S5.T4.1.7.7.1.2" class="ltx_text" style="color:#FF0000;">šaxulo</span>
</td>
<td id="S5.T4.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b">odun gjak <span id="S5.T4.1.7.7.2.1" class="ltx_text" style="color:#0000FF;">šawulo</span> odun gjak <span id="S5.T4.1.7.7.2.2" class="ltx_text" style="color:#0000FF;">šawulo</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Examples of inference results from <span id="S5.T4.3.1" class="ltx_text ltx_font_italic">ManWav</span>. Wrong predictions are marked red and the corresponding answers are marked blue.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Linguistic Analysis</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Taking into account the linguistic characteristics of Manchu, we classify the most common errors in <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">ManWav</span> into the following four categories: (1) confusion involving //, (2) confusion and nasalizing of nasal sounds in word-final positions, (3) assimilation between stops, and (4) confusion between /w/ and /x/.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">First, there are some uncaptured or mismatched // sounds in the inference results, particularly in word-final or between sonorants (e.g., /l/) and stops. This occurs because // can be neutralized with other vowels or even deleted, posing challenges in accurate transcription. As shown in table <a href="#S5.T4" title="Table 4 ‣ 5.1 Result ‣ 5 Result and Discussion ‣ ManWav: The First Manchu ASR Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the locative marker <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_italic">de</span> and <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_italic">am</span> ‘dad’ are sometimes captured as <span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_italic">d</span> and <span id="S5.SS2.p2.1.4" class="ltx_text ltx_font_italic">am</span>, indicating apocope of //. The loss of // is also evident in <span id="S5.SS2.p2.1.5" class="ltx_text ltx_font_italic">dulke</span>, which originally included // between the sonorant /l/ and the stop /k/.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">Moreover, nasal sounds /n/ and /m/ in word-final positions are frequently overlooked during inference. This could be attributed to the nature of nasal sounds, as they tend to be fused with subsequent vowels, resulting in nasalized vowels, or they may be omitted altogether. The word <span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_italic">gunin</span> ‘thought’ is an instance of this phenomenon. It is often transcribed as <span id="S5.SS2.p3.1.2" class="ltx_text ltx_font_italic">gunim</span>, where the final /n/ appears as /m/. The occurrence of nasal stops can sometimes be mistaken for the deletion of the nasalized preceding vowel. For example, the /n/ sound in <span id="S5.SS2.p3.1.3" class="ltx_text ltx_font_italic">ilan</span> ‘three’ typically nasalizes the following vowels and then is deleted. However, our model erroneously retained the nasal sound in the transcription <span id="S5.SS2.p3.1.4" class="ltx_text ltx_font_italic">ilan</span>, preserving the final /n/.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Third, the inference results contain pairs that have undergone assimilation based on the articulated position. These pairs were not transcribed as assimilated forms, but this kind of assimilation is a highly productive phenomenon in natural languages. For instance, the /mg/ sequence in <span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_italic">damgu</span> ‘tobacco’ became /Ng/ in our inference results. This is unsurprising since both /N/ and /g/ are velar whereas /m/ is bilabial.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">Lastly, confusion between intervocalic /w/ and /x/ is frequently observed. To be specific, <span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_italic">šawulo</span> ‘cold’ is recognized as <span id="S5.SS2.p5.1.2" class="ltx_text ltx_font_italic">šaxulo</span> in our model. Given that /w/ is the labial approximant and /x/ is the palatal approximant, it can be noted that these two sounds occupy distinct articulatory positions. However, there is no equivalent unvoiced sound for /w/, and discerning the voicing of approximants becomes challenging when they are in intervocalic positions.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.1" class="ltx_p">The above four types of mismatch and corresponding examples are elaborated in Table <a href="#S5.T5" title="Table 5 ‣ 5.2 Linguistic Analysis ‣ 5 Result and Discussion ‣ ManWav: The First Manchu ASR Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<table id="S5.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Mismatch Types</span></th>
<th id="S5.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S5.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">Examples</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.1.2.1" class="ltx_tr">
<td id="S5.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">(1)  / __#, R__C</td>
<td id="S5.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">d : d, am : am, dulk : dulk</td>
</tr>
<tr id="S5.T5.1.3.2" class="ltx_tr">
<td id="S5.T5.1.3.2.1" class="ltx_td ltx_align_center">(2) n, m / __#</td>
<td id="S5.T5.1.3.2.2" class="ltx_td ltx_align_center">gunin : gunim, ilan : ila</td>
</tr>
<tr id="S5.T5.1.4.3" class="ltx_tr">
<td id="S5.T5.1.4.3.1" class="ltx_td ltx_align_center">(3) assimilation</td>
<td id="S5.T5.1.4.3.2" class="ltx_td ltx_align_center">damgu : daNgu</td>
</tr>
<tr id="S5.T5.1.5.4" class="ltx_tr">
<td id="S5.T5.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b">(4) w : x / V__V</td>
<td id="S5.T5.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b">šaxulo : šawulo</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Observed mismatch examples from the inference results written in phonological notations. R refers to sonorants, C consonants, and V vowels. # means boundary of words; __# means word-final position.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>ASR research in low-resource languages</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">There exist some endeavors to apply ASR to low-resource languages. For example, <cite class="ltx_cite ltx_citemacro_citet">Safonova et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite> collect a speech dataset in the Chukchi language and train an XLSR model. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Qin et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite> improve low-resource Tibetan ASR while <cite class="ltx_cite ltx_citemacro_citet">Jimerson and Prud’hommeaux (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite> introduce a fully functional ASR system tailored for Seneca, an endangered indigenous language of North America. <cite class="ltx_cite ltx_citemacro_citet">Singh et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> propose an effective self-training approach capable of generating accurate pseudo-labels for unlabeled low-resource speech, particularly for the Punjabi language. Furthermore, <cite class="ltx_cite ltx_citemacro_citet">Zhou et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite> explore training strategies for efficient data utilization and <cite class="ltx_cite ltx_citemacro_citet">Bartelds et al. (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite> investigate data augmentation methods to enhance ASR systems for low-resource scenarios. Other efforts for multilingual ASR or adapting to low-resource scenarios include Kaldi-toolkit<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://kaldi-asr.org/index.html</span></span></span>, IARPA Babel project<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://www.iarpa.gov/research-programs/babel</span></span></span>. However, as an extremely endangered language, Manchu has been isolated from all these efforts.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Wav2Vec 2.0</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">The core innovation of Wav2Vec 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Baevski et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite> lies in its ability to effectively capture the contextual information in speech through its Transformer-based architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>. Wav2Vec 2.0 leverages self-supervised training, allowing the training of an ASR model with a minimal amount of labeled data, provided there is an ample supply of unlabeled data. Wav2Vec 2.0 is effective not only in capturing diverse dialects but also in accommodating various languages. XLSR <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite> is built on Wav2Vec 2.0 and learns cross-lingual speech representations from raw waveform of speech in multiple languages. XLSR-53 is particularly pretrained on 53 languages, and fine-tuned for Connectionist Temporal Classification(CTC) speech recognition. CTC is a technique used in encoder-only transformer models such as Wav2Vec 2.0, HuBERT <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al., <a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite> and M-CTC-T <cite class="ltx_cite ltx_citemacro_citep">(Lugosch et al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Future Work</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">As an extremely low resource language, Manchu has often been overlooked in linguistic technology. In an effort to maximize the utilization of available Manchu data, the development of an ASR system is essential. We introduce <span id="S7.p1.1.1" class="ltx_text ltx_font_italic">ManWav</span>, which involves fine-tuning Wav2Vec2-XLSR-53 on augmented Manchu audio data, with the aim of providing a valuable tool for the study and preservation of Manchu. As the addition of a decoder to an ASR model is known to boost the inference performance <cite class="ltx_cite ltx_citemacro_citep">(Karita et al., <a href="#bib.bib8" title="" class="ltx_ref">2019</a>; Zeyer et al., <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>, enhancing the inference quality with the help of a language model should be studied in the future.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The primary constraint of this research lies in the scarcity of Manchu audio data. As the audio data used in this research consists only of Colloquial Manchu from one speaker, utilizing <span id="Sx1.p1.1.1" class="ltx_text ltx_font_italic">ManWav</span> in other domains would not show optimized performances, given that ASR models are usually heavily domain-dependent.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The project paves the way for further innovations in the field and emphasizes the importance of inclusivity in technological advancements, ensuring that the benefits of state-of-the-art technologies are accessible to all linguistic groups, regardless of their resource status. To support further ASR studies on endangered languages, we plan to release <span id="Sx2.p1.1.1" class="ltx_text ltx_font_italic">ManWav</span> in public.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2006.11477" title="" class="ltx_ref ltx_href">wav2vec 2.0: A framework for self-supervised learning of speech representations</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bartelds et al. (2023)</span>
<span class="ltx_bibblock">
Martijn Bartelds, Nay San, Bradley McDonnell, Dan Jurafsky, and Martijn Wieling. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.10951" title="" class="ltx_ref ltx_href">Making more of little data: Improving low-resource automatic speech recognition using data augmentation</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al. (2012)</span>
<span class="ltx_bibblock">
Wonho Choi, Hyunjo You, and Juwon Kim. 2012.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.35638/ijih.2012..7.013" title="" class="ltx_ref ltx_href">The documentation of endangered altaic languages and the creation of a digital archive to safeguard linguistic diversity</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">International Journal of Intangible Heritage</em>, 0(7):103–111.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2006.13979" title="" class="ltx_ref ltx_href">Unsupervised cross-lingual representation learning for speech recognition</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grosman (2022)</span>
<span class="ltx_bibblock">
Jonatas Grosman. 2022.

</span>
<span class="ltx_bibblock">HuggingSound: A toolkit for speech-related tasks based on Hugging Face’s tools.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/jonatasgrosman/huggingsound" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/jonatasgrosman/huggingsound</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2021)</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2106.07447" title="" class="ltx_ref ltx_href">Hubert: Self-supervised speech representation learning by masked prediction of hidden units</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jimerson and Prud’hommeaux (2018)</span>
<span class="ltx_bibblock">
Robbie Jimerson and Emily Prud’hommeaux. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/L18-1657" title="" class="ltx_ref ltx_href">ASR for documenting acutely under-resourced indigenous languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</em>, Miyazaki, Japan. European Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karita et al. (2019)</span>
<span class="ltx_bibblock">
Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang, Masao Someki, Nelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, Shinji Watanabe, Takenori Yoshimura, and Wangyou Zhang. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/asru46091.2019.9003750" title="" class="ltx_ref ltx_href">A comparative study on transformer vs rnn in speech applications</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>. IEEE.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kharitonov et al. (2020)</span>
<span class="ltx_bibblock">
Eugene Kharitonov, Morgane Rivière, Gabriel Synnaeve, Lior Wolf, Pierre-Emmanuel Mazaré, Matthijs Douze, and Emmanuel Dupoux. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2007.00991" title="" class="ltx_ref ltx_href">Data augmenting contrastive learning of speech representations in the time domain</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2008)</span>
<span class="ltx_bibblock">
Juwon Kim, Dongho Ko, Chaoke D. O., and Boldyrev B. V. Han Youfeng, Piao Lianyu. 2008.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Materials of Spoken Manchu</em>.

</span>
<span class="ltx_bibblock">Seoul National University Press.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2024)</span>
<span class="ltx_bibblock">
Sangah Lee, Sungjoo Byun, Jean Seo, and Minha Kang. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2024.lrec-main.961" title="" class="ltx_ref ltx_href">ManNER &amp; ManPOS: Pioneering NLP for endangered Manchu language</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</em>, pages 11030–11039, Torino, Italia. ELRA and ICCL.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lugosch et al. (2022)</span>
<span class="ltx_bibblock">
Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2111.00161" title="" class="ltx_ref ltx_href">Pseudo-labeling for massively multilingual speech recognition</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2022)</span>
<span class="ltx_bibblock">
S. Qin, L. Wang, S. Li, Longbiao Wang, Sheng Li, Jianwu Dang, and Lixin Pan. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1186/s13636-021-00233-4" title="" class="ltx_ref ltx_href">Improving low-resource tibetan end-to-end asr by multilingual and multilevel unit modeling</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">EURASIP Journal on Audio, Speech, and Music Processing</em>, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Safonova et al. (2022)</span>
<span class="ltx_bibblock">
Anastasia Safonova, Tatiana Yudina, Emil Nadimanov, and Cydnie Davenport. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2210.05726" title="" class="ltx_ref ltx_href">Automatic speech recognition of low-resource languages based on chukchi</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seo et al. (2023)</span>
<span class="ltx_bibblock">
Jean Seo, Sungjoo Byun, Minha Kang, and Sangah Lee. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2311.17492" title="" class="ltx_ref ltx_href">Mergen: The first manchu-korean machine translation model trained on augmented data</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2023)</span>
<span class="ltx_bibblock">
Satwinder Singh, Feng Hou, and Ruili Wang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.05269" title="" class="ltx_ref ltx_href">A novel self-training approach for low-resource speech recognition</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2023)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1706.03762" title="" class="ltx_ref ltx_href">Attention is all you need</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">You (2014)</span>
<span class="ltx_bibblock">
Hyun-Jo You. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.15816/ask.2014..24.003" title="" class="ltx_ref ltx_href">A manchu speller: With a practical introduction to the natural language processing of minority languages</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Altai Hakpo</em>, 24:39–67.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeyer et al. (2019)</span>
<span class="ltx_bibblock">
Albert Zeyer, Parnia Bahar, Kazuki Irie, Ralf Schluter, and Hermann Ney. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ASRU46091.2019.9004025" title="" class="ltx_ref ltx_href">A comparison of transformer and lstm encoder decoder models for asr</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, pages 8–15.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2022)</span>
<span class="ltx_bibblock">
Zhikai Zhou, Wei Wang, Wangyou Zhang, and Yanmin Qian. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP43922.2022.9747543" title="" class="ltx_ref ltx_href">Exploring effective data utilization for low-resource speech recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 8192–8196.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.13501" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.13502" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.13502">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.13502" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.13503" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 17:45:39 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
