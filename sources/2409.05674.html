<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.05674] Evaluation of real-time transcriptions using end-to-end ASR models</title><meta property="og:description" content="Automatic Speech Recognition (ASR) or Speech-to-text (STT) has greatly evolved in the last few years. Traditional architectures based on pipelines have been replaced by joint end-to-end (E2E) architectures that simplif…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Evaluation of real-time transcriptions using end-to-end ASR models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Evaluation of real-time transcriptions using end-to-end ASR models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.05674">

<!--Generated on Sun Oct  6 00:49:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on April 2024.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Evaluation of real-time transcriptions using end-to-end ASR models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Carlos Arriaga, Alejandro Pozo, Javier Conde, Alvaro Alonso
</span></span>
</div>
<div class="ltx_dates">(April 2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Automatic Speech Recognition (ASR) or Speech-to-text (STT) has greatly evolved in the last few years. Traditional architectures based on pipelines have been replaced by joint end-to-end (E2E) architectures that simplify and streamline the model training process. In addition, new AI training methods, such as weak-supervised learning have reduced the need for high-quality audio datasets for model training. However, despite all these advancements, little to no research has been done on real-time transcription.</p>
<p id="id2.id2" class="ltx_p">In real-time scenarios, the audio is not pre-recorded, and the input audio must be fragmented to be processed by the ASR systems. To achieve real-time requirements, these fragments must be as short as possible to reduce latency. However, audio cannot be split at any point as dividing an utterance into two separate fragments will generate an incorrect transcription. Also, shorter fragments provide less context for the ASR model. For this reason, it is necessary to design and test different splitting algorithms to optimize the quality and delay of the resulting transcription.</p>
<p id="id3.id3" class="ltx_p">In this paper, three audio splitting algorithms are evaluated with different ASR models to determine their impact on both the quality of the transcription and the end-to-end delay. The algorithms are fragmentation at fixed intervals, voice activity detection (VAD), and fragmentation with feedback. The results are compared to the performance of the same model, without audio fragmentation, to determine the effects of this division.</p>
<p id="id4.id4" class="ltx_p">The results show that VAD fragmentation provides the best quality with the highest delay, whereas fragmentation at fixed intervals provides the lowest quality and the lowest delay.
The newly proposed feedback algorithm exchanges a 2-4% increase in WER for a reduction of 1.5-2s delay, respectively, to the VAD splitting.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p">This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Automatic Speech Recognition (ASR) or Speech-to-Text (STT) is the use of Artificial Intelligence to transform human speech into text. ASR started in 1952 with Audrey, a digit recognition system developed by Bell Labs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. With the evolution of machine learning (ML), artificial intelligence (AI) and available hardware <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the ASR systems have evolved from this original system.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">The first ASR architectures were made up of a series of models that together compose a processing pipeline. Each model was responsible for a different task (acoustic model, language model, phoneme inventory, etc.). Different models were trained independently and could have their own architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. With the evolution of AI training techniques, modern architectures have shifted to end-to-end (E2E) deep learning architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. This resulted in a single joint model that uses a single set of training data, avoiding the need to work with diverse sources of knowledge. It also reduces assumptions made about the data and simplifies the training process.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">E2E models are trained with substantial amounts of labeled audio data. On the one hand, some models are trained with high-quality audio repositories such as GigaSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> (10.000 hours) or The People’s Speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> (30.000 hours), which provide a dataset of audio files with their respective transcriptions with a variety of topics and accents. On the other hand, models such as Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> were trained using a higher order of magnitude of lower-quality data, using weak supervision. Research has been conducted to develop unsupervised learning techniques to train ASR models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, to reduce the cost and complexity of training new models. These ASR models have the potential to improve human transcriptions and remove human perception and context from the results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. In these E2E ASR systems, multiple network models have been used for ASR such as Long-Short-term Memory (LSTM) or Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">The current most popular ASR systems are Whisper, Wav2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, and Kaldi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Whisper and Wav2vec use E2E architectures, whereas Kaldi follows the traditional approach to ASR of a pipeline with different submodules, each responsible for a different task. All these three systems are designed for batch translations, where all the audio is pre-recorded prior to the transcription process.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">Despite these advances in ASR, most studies focus on batch transcriptions and do not study the generation of real-time transcriptions. The main focus of real-time ASR research is on improving the model endpointer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, the model responsible for detecting when a user has stopped speaking. In these experiments, authors report subsecond delays; however, they define delay as the time elapsed between the time a user stops speaking and the transcription is generated. For real-time transcriptions, measured delays must be the time elapsed between when a word is pronounced and when it is transcribed.
The main difference between real-time transcriptions compared to batch transcriptions is the existence of an algorithm responsible for deciding when to send the audio samples for processing. In the batch scenario, the audio is not processed until the speech ends; however, in the real-time scenario, samples have to be sent during the speech. A naive approach could be to split the audio at fixed intervals. However, with this approach, words can be divided into different fragments, resulting in incorrect transcription. A common metric used to measure the performance of an ASR system is the Real Time Factor (RTF), which is defined as the time of an utterance divided by the time it takes the system to process it. A system is considered real-time ready if the RTF is less than one, which means that the system can transcribe a speech faster than it is being spoken. However, this metric does not take into account other factors that affect the end-to-end delay, such as the time the samples are buffered. For this reason, a new definition of delay must be created for real-time systems.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">There have been experiments to generate real-time transcriptions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, however, they are based on commercial and proprietary systems and models. In addition, some research has been done on real-time transcription models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. These real-time transcription systems can be used to improve the accessibility of deaf or elderly persons <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> to all web applications that are based on audio, such as online education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, online meetings, etc.</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">Research has been conducted to improve audio segmentation for ASR systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. In that article, the authors proposed a CIF-based predictor to segment audio into fragments. However, they do not measure the end-to-end delay of the system, compare the performance of their segmentation compared to a batch scenario, nor provide an architecture for implementing and real-time ASR system. Some of the algorithms tested by the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, such as VAD splitting and fixed intervals, will be tested in this paper.
A similar article on real-time transcriptions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> designs and implements a custom Kaldi pipeline that generates real-time transcriptions. However, they do not test with E2E systems and only test sending individual utterances.</p>
</div>
<div id="S1.p8" class="ltx_para ltx_noindent">
<p id="S1.p8.1" class="ltx_p">Due to the lack of real-time ASR systems and models, in this article, we evaluate the performance of different combinations of batch ASR models and audio splitting algorithms with the objective of generating real-time transcriptions. A comparison between batch processing and real-time processing is performed to determine the possible degradation in transcription quality caused by the audio splitting process. In addition, the delay introduced by the different combinations of ASR models and audio splitting algorithms is measured to determine their viability in a real-time scenario. To perform these comparisons, an architecture for generating real-time transcriptions is defined and implemented to measure the performance of the models and algorithms. Existing algorithms such as VAD splitting and fixed-interval splitting are tested. Finally, a new feedback algorithm is defined and evaluated with the objective of improving the quality and reducing the latency of the two previously mentioned algorithms.</p>
</div>
<div id="S1.p9" class="ltx_para ltx_noindent">
<p id="S1.p9.1" class="ltx_p">This paper is organized in the following sections. Section <a href="#S2" title="2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the test scenario, algorithms used, evaluation metrics, and output normalization. Section <a href="#S3" title="3 Results ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results obtained following the proposed methodology. Section <a href="#S4" title="4 Conclusion and future work ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the conclusions obtained and possible future work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Testing methodology</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">In this section, the methodology designed to evaluate the performance of different combinations of batch ASR models and audio splitting algorithms is detailed. In addition, the architecture designed to make the comparison is introduced with a series of definitions that will be used in Section <a href="#S3" title="3 Results ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Finally, the algorithms used for the delay measurement and the output normalization process are presented.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">The methodology focuses on measuring the performance of the models in a real-time scenario when audio has to be transcribed during a speech. The baseline for the comparison is the performance of the same model in a batch scenario, where all the audio is pre-recorded prior to its processing. For each model, its performance is measured first in a batch experiment and then using the real-time architecture proposed in the following section. Different models and audio splitting algorithms are tested to compare their performance and delay. The objective is to measure the effects of splitting the audio into different fragments instead of processing the entire recording.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p">The dataset used for these experiments is GigaSpeech<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. This repository includes 10.000 hours of transcribed audio. However, the transcriptions were found not to be perfectly accurate. To solve this, the evaluation and testing subsets of GigaSpeech were used. These are composed of 52 hours of transcribed audio, annotated by professional humans. GigaSpeech’s documentation states that part of the testing subset was manually collected to provide better coverage on both topics and audio conditions. The details of these subsets are presented in Table <a href="#S2.T1" title="Table 1 ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Dev and Test dataset details</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Source</td>
<td id="S2.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Files</td>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Duration(h)</td>
<td id="S2.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Audio Conditions</td>
</tr>
<tr id="S2.T1.1.2" class="ltx_tr">
<td id="S2.T1.1.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Podcast</td>
<td id="S2.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40</td>
<td id="S2.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.85</td>
<td id="S2.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S2.T1.1.2.4.1" class="ltx_text"></span> <span id="S2.T1.1.2.4.2" class="ltx_text">
<span id="S2.T1.1.2.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.1.2.4.2.1.1" class="ltx_tr">
<span id="S2.T1.1.2.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Clean background or music.</span></span>
<span id="S2.T1.1.2.4.2.1.2" class="ltx_tr">
<span id="S2.T1.1.2.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Indoor.</span></span>
<span id="S2.T1.1.2.4.2.1.3" class="ltx_tr">
<span id="S2.T1.1.2.4.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">Near-field.</span></span>
<span id="S2.T1.1.2.4.2.1.4" class="ltx_tr">
<span id="S2.T1.1.2.4.2.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center">Spontaneous.</span></span>
</span></span><span id="S2.T1.1.2.4.3" class="ltx_text"></span></td>
</tr>
<tr id="S2.T1.1.3" class="ltx_tr">
<td id="S2.T1.1.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">YouTube</td>
<td id="S2.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">120</td>
<td id="S2.T1.1.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">29.15</td>
<td id="S2.T1.1.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T1.1.3.4.1" class="ltx_text"></span> <span id="S2.T1.1.3.4.2" class="ltx_text">
<span id="S2.T1.1.3.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S2.T1.1.3.4.2.1.1" class="ltx_tr">
<span id="S2.T1.1.3.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Clean or noisy background.</span></span>
<span id="S2.T1.1.3.4.2.1.2" class="ltx_tr">
<span id="S2.T1.1.3.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Indoor and outdoor.</span></span>
<span id="S2.T1.1.3.4.2.1.3" class="ltx_tr">
<span id="S2.T1.1.3.4.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">Near and far field.</span></span>
<span id="S2.T1.1.3.4.2.1.4" class="ltx_tr">
<span id="S2.T1.1.3.4.2.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center">Reading or spontaneous</span></span>
</span></span><span id="S2.T1.1.3.4.3" class="ltx_text"></span></td>
</tr>
</table>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Test scenario</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">In this section, the architecture and ASR models used to generate real-time transcriptions are detailed, as well as the testing process. For the architecture used, a generic definition of the components is provided first. Then, the implementation of the aforementioned component used in these experiments is detailed.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Architecture definition</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">The main objective is to provide a generic architecture definition that is not limited by specific technologies or protocols. Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1.1 Architecture definition ‣ 2.1 Test scenario ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides a diagram of its different components, which are:</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2409.05674/assets/figures/arqTran.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Architecture for real-time transcription generation</figcaption>
</figure>
<div id="S2.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Audio input: Audio that enters the system to be transcribed.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Audio extractor: Process that captures the audio from the audio input and transforms it into a suitable format for the rest of the system.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Audio splitter: Process that receives the audio from the audio extractor and optimally splits it into fragments for processing. The aforementioned audio-splitting algorithms are implemented inside this component.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">ASR cluster: Responsible for transcribing the received audio samples.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i5.p1" class="ltx_para">
<p id="S2.I1.i5.p1.1" class="ltx_p">ASR cluster-audio splitter connection: Connection between the ASR cluster and the audio splitter, used to transmit the audio fragments.</p>
</div>
</li>
<li id="S2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i6.p1" class="ltx_para">
<p id="S2.I1.i6.p1.1" class="ltx_p">Display: Component that receives the transcription and renders it.</p>
</div>
</li>
<li id="S2.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i7.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i7.p1.1" class="ltx_p">ASR cluster-display connection: Connection between the ASR cluster and the display, used to transmit the resulting transcription.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Architecture implementation</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">This section provides a reference implementation of the previously proposed architecture. It will be used to test the different algorithms and scenarios proposed. The objective is to provide an implementation that runs independently of the operating system or computer architecture. For this reason, the implementation is based on open-source Web technologies, so any web browser can access it. Web browsers offer a series of standard APIs that run code independently of the hardware or system architecture that will be used in the following components. In addition, with this approach, it can be used in a variety of web applications and services, such as videoconference.</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para ltx_noindent">
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para ltx_noindent">
<p id="S2.I2.i1.p1.1" class="ltx_p">Audio input: In this implementation, the audio input is the microphone captured by a web browser that accesses the ASR application.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para ltx_noindent">
<p id="S2.I2.i2.p1.1" class="ltx_p">Audio extraction: For accessing the audio and processing it, the MediaDevices API <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> is used. The audio input is captured using the getUserMedia method from the MediaDevices API. Audio is sampled at 16000 kHz with 16 bits per sample.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para ltx_noindent">
<p id="S2.I2.i3.p1.1" class="ltx_p">Audio splitter: The Web Audio API <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> is a system to control audio on the Web. It allows developers to create a processing graph composed of audio nodes. Through this API, audio samples from the Audio Extractor can be accessed and processed. All the processing is done in the context of an AudioContext. The AudioContext represents a filtering graph composed of audio processing nodes and a common configuration. The audio splitting is performed inside this filtering graph. Different nodes are created to implement the different audio splitting algorithms. To reduce the computational power required on the server side, audio splitting is offloaded to the clients. Moving the transcription process to the clients is currently not possible as it requires high computation power and specialized equipment such as GPUs. The audio is received from the audio extractor and then handled to the Client-ASR connection.</p>
</div>
</li>
<li id="S2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i4.p1" class="ltx_para ltx_noindent">
<p id="S2.I2.i4.p1.1" class="ltx_p">Audio Splitter-ASR cluster and ASR cluster- display connection: For these connections, the technology selected is WebSockets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. They allow for bidirectional communication between clients and servers, allowing the system to use the same websocket for both connections. In this implementation, they are used to send raw audio samples from the audio splitter and receive transcribed text. Other protocols such as RTP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> or WebRTC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> used for multimedia streaming were not considered because most of their features, such as bandwidth adaptation, codec negotiation, or time synchronization, are not needed. This is because raw samples are sent without any additional metadata. In addition, these channels do not offer a return channel suitable for text format. The selected library for implementing WebSockets on the client is socket.io<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://socket.io/</span></span></span>.</p>
</div>
</li>
<li id="S2.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i5.p1" class="ltx_para">
<p id="S2.I2.i5.p1.1" class="ltx_p">ASR Cluster: The cluster receives audio fragments from the WebSockets, transcribes them, and sends them back through the socket.
For the ASR model, Whisper was selected due to its ease of deployment and high performance. In addition, trained models are provided directly by OpenAI. The implementation used is a high performance version of Whisper in C++ <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/ggerganov/whisper.cpp</span></span></span>.
For each connection received, a new instance of the Whisper model is created. All audio received from a socket is processed by the same instance. These instances are balanced among available hardware resources. The socket server library used is socketioxide<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/Totodore/socketioxide</span></span></span>, a Rust implementation of the socket.io library.</p>
</div>
</li>
<li id="S2.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i6.p1" class="ltx_para ltx_noindent">
<p id="S2.I2.i6.p1.1" class="ltx_p">Display: The display is a HTML<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> element in the application view, in which the received transcription is rendered.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>ASR models</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">For this experiment, three models are selected from the ones provided by OpenAI: tiny <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://huggingface.co/openai/whisper-tiny</span></span></span>, base<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://huggingface.co/openai/whisper-base</span></span></span> and large <span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://huggingface.co/openai/whisper-large-v2</span></span></span>. Their details are presented in Table <a href="#S2.T2" title="Table 2 ‣ 2.1.3 ASR models ‣ 2.1 Test scenario ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S2.SS1.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS3.p2.1" class="ltx_p">One issue that this implementation of Whisper presents is that sometimes during a silence the model repeats the last output instead of marking the silence.
This issue was particularly prominent in the latest version of the large model. For this reason, even though there is a newer version 3 of the large model <span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://huggingface.co/openai/whisper-large-v3</span></span></span>, the test was carried out with version 2.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Whisper models disk and memory size</figcaption>
<table id="S2.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T2.3.4" class="ltx_tr">
<td id="S2.T2.3.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Model</td>
<td id="S2.T2.3.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Disk Size</td>
<td id="S2.T2.3.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Memory Size</td>
<td id="S2.T2.3.4.4" class="ltx_td"></td>
</tr>
<tr id="S2.T2.1.1" class="ltx_tr">
<td id="S2.T2.1.1.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S2.T2.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">75 MiB</td>
<td id="S2.T2.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<math id="S2.T2.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.T2.1.1.1.m1.1a"><mo id="S2.T2.1.1.1.m1.1.1" xref="S2.T2.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.m1.1b"><csymbol cd="latexml" id="S2.T2.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.m1.1c">\sim</annotation></semantics></math>273 MB</td>
<td id="S2.T2.1.1.4" class="ltx_td"></td>
</tr>
<tr id="S2.T2.2.2" class="ltx_tr">
<td id="S2.T2.2.2.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S2.T2.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">142 MiB</td>
<td id="S2.T2.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<math id="S2.T2.2.2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.T2.2.2.1.m1.1a"><mo id="S2.T2.2.2.1.m1.1.1" xref="S2.T2.2.2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.1.m1.1b"><csymbol cd="latexml" id="S2.T2.2.2.1.m1.1.1.cmml" xref="S2.T2.2.2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.1.m1.1c">\sim</annotation></semantics></math>388 MB</td>
<td id="S2.T2.2.2.4" class="ltx_td"></td>
</tr>
<tr id="S2.T2.3.3" class="ltx_tr">
<td id="S2.T2.3.3.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Large</td>
<td id="S2.T2.3.3.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">2.9 GiB</td>
<td id="S2.T2.3.3.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<math id="S2.T2.3.3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.T2.3.3.1.m1.1a"><mo id="S2.T2.3.3.1.m1.1.1" xref="S2.T2.3.3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.3.3.1.m1.1b"><csymbol cd="latexml" id="S2.T2.3.3.1.m1.1.1.cmml" xref="S2.T2.3.3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.3.1.m1.1c">\sim</annotation></semantics></math>3.9 GB</td>
<td id="S2.T2.3.3.4" class="ltx_td"></td>
</tr>
</table>
</figure>
</section>
<section id="S2.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>Batch testing</h4>

<div id="S2.SS1.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS4.p1.1" class="ltx_p">To compare the performance of the various audio splitting algorithms, the audio files are transcribed in batches. To transcribe the files from the GigaSpeech evaluation dataset, they had to be converted from OPUS to WAV due to implementation requirements. The Ffmpeg<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>https://ffmpeg.org/</span></span></span> library was used for this conversion. After that, all audio files were transcribed one by one using a shell script, and the resulting transcriptions were stored for future analysis.</p>
</div>
</section>
<section id="S2.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.5 </span>Real-time testing</h4>

<div id="S2.SS1.SSS5.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS5.p1.1" class="ltx_p">To automatize the testing process, Selenium<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>https://www.selenium.dev/</span></span></span>, an open-source project to automate browsers, is used to simulate clients. Selenium’s WebDriver is an interface that allows users to launch and control web browser instances. For the Web browser, Google Chrome<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>https://www.google.com/chrome/</span></span></span> was selected because it has the option to use audio and video files as a false microphone or camera. Using this option, for each audio file in the dataset, a headless (without graphic interface) browser is launched with the audio file as the microphone. This false microphone is the audio input of the architecture, as defined in Section <a href="#S2.SS1.SSS1" title="2.1.1 Architecture definition ‣ 2.1 Test scenario ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1.1</span></a>. This allows the system to use the same audio files in both batch and real-time transcriptions. In addition, this implementation simulates a real user who accesses the system via a web browser.</p>
</div>
<div id="S2.SS1.SSS5.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS5.p2.1" class="ltx_p">In Chrome, when the audio file used as the microphone reaches the end, it loops back from the beginning. To avoid transcribing the same audio multiple times and having to manually check every transcription, each browser instance only runs until the audio file reaches the end for the first time. Because the WAV format stores raw samples with a fixed sample rate, the duration of the file can be calculated based on its size. The time needed is calculated using the formula provided in Equation <a href="#S2.E1" title="In 2.1.5 Real-time testing ‣ 2.1 Test scenario ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>:</p>
</div>
<div id="S2.SS1.SSS5.p3" class="ltx_para ltx_noindent">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="T(s)=Size(bytes)*8/16000(Hz)/16(b/sample);" display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2.1" xref="S2.E1.m1.2.2.1.1.cmml"><mrow id="S2.E1.m1.2.2.1.1" xref="S2.E1.m1.2.2.1.1.cmml"><mrow id="S2.E1.m1.2.2.1.1.5" xref="S2.E1.m1.2.2.1.1.5.cmml"><mi id="S2.E1.m1.2.2.1.1.5.2" xref="S2.E1.m1.2.2.1.1.5.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.5.1" xref="S2.E1.m1.2.2.1.1.5.1.cmml">​</mo><mrow id="S2.E1.m1.2.2.1.1.5.3.2" xref="S2.E1.m1.2.2.1.1.5.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.5.3.2.1" xref="S2.E1.m1.2.2.1.1.5.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">s</mi><mo stretchy="false" id="S2.E1.m1.2.2.1.1.5.3.2.2" xref="S2.E1.m1.2.2.1.1.5.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.1.1.4" xref="S2.E1.m1.2.2.1.1.4.cmml">=</mo><mrow id="S2.E1.m1.2.2.1.1.3" xref="S2.E1.m1.2.2.1.1.3.cmml"><mrow id="S2.E1.m1.2.2.1.1.2.2" xref="S2.E1.m1.2.2.1.1.2.2.cmml"><mrow id="S2.E1.m1.2.2.1.1.2.2.2" xref="S2.E1.m1.2.2.1.1.2.2.2.cmml"><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.4" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2a" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.5" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.5.cmml">z</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2b" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.6" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2c" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1a" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1b" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.5" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1c" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.6" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.6.cmml">s</mi></mrow><mo rspace="0.055em" stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">∗</mo><mn id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">8</mn></mrow><mo id="S2.E1.m1.2.2.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.2.cmml">/</mo><mn id="S2.E1.m1.2.2.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.3.cmml">16000</mn></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.2.2.2.3" xref="S2.E1.m1.2.2.1.1.2.2.2.3.cmml">​</mo><mrow id="S2.E1.m1.2.2.1.1.2.2.2.2.1" xref="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.2.2.2.2.1.2" xref="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.1.1.2.2.2.2.1.1" xref="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.2" xref="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.1" xref="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.3" xref="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.3.cmml">z</mi></mrow><mo stretchy="false" id="S2.E1.m1.2.2.1.1.2.2.2.2.1.3" xref="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.1.1.2.2.3" xref="S2.E1.m1.2.2.1.1.2.2.3.cmml">/</mo><mn id="S2.E1.m1.2.2.1.1.2.2.4" xref="S2.E1.m1.2.2.1.1.2.2.4.cmml">16</mn></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.3.4" xref="S2.E1.m1.2.2.1.1.3.4.cmml">​</mo><mrow id="S2.E1.m1.2.2.1.1.3.3.1" xref="S2.E1.m1.2.2.1.1.3.3.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.3.3.1.2" xref="S2.E1.m1.2.2.1.1.3.3.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.1.1.3.3.1.1" xref="S2.E1.m1.2.2.1.1.3.3.1.1.cmml"><mrow id="S2.E1.m1.2.2.1.1.3.3.1.1.2" xref="S2.E1.m1.2.2.1.1.3.3.1.1.2.cmml"><mi id="S2.E1.m1.2.2.1.1.3.3.1.1.2.2" xref="S2.E1.m1.2.2.1.1.3.3.1.1.2.2.cmml">b</mi><mo id="S2.E1.m1.2.2.1.1.3.3.1.1.2.1" xref="S2.E1.m1.2.2.1.1.3.3.1.1.2.1.cmml">/</mo><mi id="S2.E1.m1.2.2.1.1.3.3.1.1.2.3" xref="S2.E1.m1.2.2.1.1.3.3.1.1.2.3.cmml">s</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.3.3.1.1.1" xref="S2.E1.m1.2.2.1.1.3.3.1.1.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.3.3.1.1.3" xref="S2.E1.m1.2.2.1.1.3.3.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.3.3.1.1.1a" xref="S2.E1.m1.2.2.1.1.3.3.1.1.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.3.3.1.1.4" xref="S2.E1.m1.2.2.1.1.3.3.1.1.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.3.3.1.1.1b" xref="S2.E1.m1.2.2.1.1.3.3.1.1.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.3.3.1.1.5" xref="S2.E1.m1.2.2.1.1.3.3.1.1.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.3.3.1.1.1c" xref="S2.E1.m1.2.2.1.1.3.3.1.1.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.3.3.1.1.6" xref="S2.E1.m1.2.2.1.1.3.3.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.3.3.1.1.1d" xref="S2.E1.m1.2.2.1.1.3.3.1.1.1.cmml">​</mo><mi id="S2.E1.m1.2.2.1.1.3.3.1.1.7" xref="S2.E1.m1.2.2.1.1.3.3.1.1.7.cmml">e</mi></mrow><mo stretchy="false" id="S2.E1.m1.2.2.1.1.3.3.1.3" xref="S2.E1.m1.2.2.1.1.3.3.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.2.2.1.2" xref="S2.E1.m1.2.2.1.1.cmml">;</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.1.1.cmml" xref="S2.E1.m1.2.2.1"><eq id="S2.E1.m1.2.2.1.1.4.cmml" xref="S2.E1.m1.2.2.1.1.4"></eq><apply id="S2.E1.m1.2.2.1.1.5.cmml" xref="S2.E1.m1.2.2.1.1.5"><times id="S2.E1.m1.2.2.1.1.5.1.cmml" xref="S2.E1.m1.2.2.1.1.5.1"></times><ci id="S2.E1.m1.2.2.1.1.5.2.cmml" xref="S2.E1.m1.2.2.1.1.5.2">𝑇</ci><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝑠</ci></apply><apply id="S2.E1.m1.2.2.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.3"><times id="S2.E1.m1.2.2.1.1.3.4.cmml" xref="S2.E1.m1.2.2.1.1.3.4"></times><apply id="S2.E1.m1.2.2.1.1.2.2.cmml" xref="S2.E1.m1.2.2.1.1.2.2"><divide id="S2.E1.m1.2.2.1.1.2.2.3.cmml" xref="S2.E1.m1.2.2.1.1.2.2.3"></divide><apply id="S2.E1.m1.2.2.1.1.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2"><times id="S2.E1.m1.2.2.1.1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2.3"></times><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1"><divide id="S2.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.2"></divide><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1"><times id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2"></times><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1"><times id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2"></times><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.3">𝑆</ci><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.4">𝑖</ci><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.5.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.5">𝑧</ci><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.6.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.6">𝑒</ci><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1"><times id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">𝑏</ci><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3">𝑦</ci><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4">𝑡</ci><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.5">𝑒</ci><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.6.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.6">𝑠</ci></apply></apply><cn type="integer" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3">8</cn></apply><cn type="integer" id="S2.E1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.3">16000</cn></apply><apply id="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2.2.1"><times id="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.1"></times><ci id="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.2">𝐻</ci><ci id="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.2.2.2.2.1.1.3">𝑧</ci></apply></apply><cn type="integer" id="S2.E1.m1.2.2.1.1.2.2.4.cmml" xref="S2.E1.m1.2.2.1.1.2.2.4">16</cn></apply><apply id="S2.E1.m1.2.2.1.1.3.3.1.1.cmml" xref="S2.E1.m1.2.2.1.1.3.3.1"><times id="S2.E1.m1.2.2.1.1.3.3.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.3.3.1.1.1"></times><apply id="S2.E1.m1.2.2.1.1.3.3.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.3.3.1.1.2"><divide id="S2.E1.m1.2.2.1.1.3.3.1.1.2.1.cmml" xref="S2.E1.m1.2.2.1.1.3.3.1.1.2.1"></divide><ci id="S2.E1.m1.2.2.1.1.3.3.1.1.2.2.cmml" xref="S2.E1.m1.2.2.1.1.3.3.1.1.2.2">𝑏</ci><ci id="S2.E1.m1.2.2.1.1.3.3.1.1.2.3.cmml" xref="S2.E1.m1.2.2.1.1.3.3.1.1.2.3">𝑠</ci></apply><ci id="S2.E1.m1.2.2.1.1.3.3.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.3.3.1.1.3">𝑎</ci><ci id="S2.E1.m1.2.2.1.1.3.3.1.1.4.cmml" xref="S2.E1.m1.2.2.1.1.3.3.1.1.4">𝑚</ci><ci id="S2.E1.m1.2.2.1.1.3.3.1.1.5.cmml" xref="S2.E1.m1.2.2.1.1.3.3.1.1.5">𝑝</ci><ci id="S2.E1.m1.2.2.1.1.3.3.1.1.6.cmml" xref="S2.E1.m1.2.2.1.1.3.3.1.1.6">𝑙</ci><ci id="S2.E1.m1.2.2.1.1.3.3.1.1.7.cmml" xref="S2.E1.m1.2.2.1.1.3.3.1.1.7">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">T(s)=Size(bytes)*8/16000(Hz)/16(b/sample);</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.SSS5.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS5.p4.1" class="ltx_p">The transcriptions received by the ASR cluster are then stored for future analysis.</p>
</div>
</section>
<section id="S2.SS1.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.6 </span>Architecture deployment</h4>

<div id="S2.SS1.SSS6.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS6.p1.1" class="ltx_p">All the components defined in the architecture were deployed on the same machine. The selected hardware and operating system are detailed in Table <a href="#S2.T3" title="Table 3 ‣ 2.1.6 Architecture deployment ‣ 2.1 Test scenario ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></p>
</div>
<figure id="S2.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Hardware specifications</figcaption>
<table id="S2.T3.1" class="ltx_tabular ltx_centering ltx_align_top">
<tr id="S2.T3.1.1" class="ltx_tr">
<td id="S2.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">CPU</td>
<td id="S2.T3.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">RAM</td>
<td id="S2.T3.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">DISK</td>
<td id="S2.T3.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">OS</td>
</tr>
<tr id="S2.T3.1.2" class="ltx_tr">
<td id="S2.T3.1.2.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Intel® Core™ i9-12900 × 24</td>
<td id="S2.T3.1.2.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">3 x 8GB M323R1GB4BB0</td>
<td id="S2.T3.1.2.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">SSD 2TB</td>
<td id="S2.T3.1.2.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Ubuntu 20.04.6 LTS</td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Audio splitting</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">In this section, the three algorithms that will be tested in the next section are presented.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Fixed interval</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">A first naive algorithm splits audio fragments at fixed intervals, without checking if the utterances are split into different fragments. Different intervals will be tested to measure the impact of the duration of the fragment on delay and performance. In this implementation, the audio extractor captures the samples at an unfixed rate, so they are stored until all of the samples from each interval are received. The implementation is presented in Algorithm <a href="#alg1" title="Algorithm 1 ‣ 2.2.1 Fixed interval ‣ 2.2 Audio splitting ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Fixed interval</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">nSeconds = N

</div>
<div id="alg1.l2" class="ltx_listingline">sampleRate = 16000

</div>
<div id="alg1.l3" class="ltx_listingline">maxSamples = 16000*nSeconds

</div>
<div id="alg1.l4" class="ltx_listingline">samplesArray= Array[]

</div>
<div id="alg1.l5" class="ltx_listingline">
<span id="alg1.l5.1" class="ltx_text ltx_font_bold">while</span> transcribing <span id="alg1.l5.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l6" class="ltx_listingline">     newSamples = receiveSamples();

</div>
<div id="alg1.l7" class="ltx_listingline">     samplesArray.push(newSamples)

</div>
<div id="alg1.l8" class="ltx_listingline">     <span id="alg1.l8.1" class="ltx_text ltx_font_bold">if</span> samplesArray.length &gt;maxSamples <span id="alg1.l8.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l9" class="ltx_listingline">         samplesFromPeriod = samplesArray[0, maxSamples]

</div>
<div id="alg1.l10" class="ltx_listingline">         sendSamples(samplesFromPeriod);

</div>
<div id="alg1.l11" class="ltx_listingline">         samplesArray.splice(0, maxSamples);

</div>
<div id="alg1.l12" class="ltx_listingline">     <span id="alg1.l12.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l12.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg1.l13" class="ltx_listingline">
<span id="alg1.l13.1" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l13.2" class="ltx_text ltx_font_bold">while</span>
</div>
</div>
</figure>
<div id="S2.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">This algorithm is used as a reference for the next algorithms. Two different fragmentation intervals, 2 and 3 seconds, were tested to compare its effects on the resulting transcription.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>VAD based</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">The VAD based algorithm tries to solve the word splitting issues caused by the previous algorithm. The objective of this algorithm is to avoid dividing words into different fragments to improve the performance of the system.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">In contrast, with end-of-query detectors, VAD algorithms detect the silence between words, instead of only detecting the end of the speech. In this implementation, a JavaScript VAD library<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>https://github.com/kdavis-mozilla/vad.js</span></span></span> is used.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p">The VAD is a state machine with two states: silence and voice. The samples are stored in both states. When the transition from voice to silence is triggered, all stored samples are sent to the ASR cluster. The state machine is presented in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2.2 VAD based ‣ 2.2 Audio splitting ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2409.05674/assets/figures/vad.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="134" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>VAD state machine</figcaption>
</figure>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Feedback</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">The last algorithm tries to reduce the delay of the VAD-based algorithm while maintaining the quality. Instead of waiting for a silence, samples are sent at a lower interval, and to avoid word separation, previous audio samples are used in the transcription.
For each iteration, some audio samples from the previous iteration are used to maintain the context.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS3.p2.1" class="ltx_p">In this implementation, Algorithm <a href="#alg1" title="Algorithm 1 ‣ 2.2.1 Fixed interval ‣ 2.2 Audio splitting ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> generates the audio fragments in the client. Then, in the ASR cluster, Algorithm <a href="#alg2" title="Algorithm 2 ‣ 2.2.3 Feedback ‣ 2.2 Audio splitting ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> calculates the new transcription.
By default, Whisper is configured to use the previous input as a prompt for the next transcription, to maintain the context. This option is deactivated as the audio fragments are no longer contiguous.</p>
</div>
<figure id="alg2" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg2.2.1.1" class="ltx_text ltx_font_bold">Algorithm 2</span> </span> Feedback</figcaption>
<div id="alg2.3" class="ltx_listing ltx_listing">
<div id="alg2.l1" class="ltx_listingline">nSecondsFeedback = N

</div>
<div id="alg2.l2" class="ltx_listingline">sampleRate = 16000

</div>
<div id="alg2.l3" class="ltx_listingline">maxSamples = 16000*nSecondsFeedback

</div>
<div id="alg2.l4" class="ltx_listingline">
<span id="alg2.l4.1" class="ltx_text ltx_font_bold">procedure</span> <span id="alg2.l4.2" class="ltx_text ltx_font_smallcaps">transcribe</span>(previousSamples[])

</div>
<div id="alg2.l5" class="ltx_listingline">     newSamples = receiveSamples();

</div>
<div id="alg2.l6" class="ltx_listingline">     previousSamples.push(newSamples);

</div>
<div id="alg2.l7" class="ltx_listingline">     firstSample = previousSamples.length - maxSamples;

</div>
<div id="alg2.l8" class="ltx_listingline">     <span id="alg2.l8.1" class="ltx_text ltx_font_bold">if</span> previousSamples.length - maxSample &lt;0  <span id="alg2.l8.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg2.l9" class="ltx_listingline">         firstSample = 0

</div>
<div id="alg2.l10" class="ltx_listingline">     <span id="alg2.l10.1" class="ltx_text ltx_font_bold">end</span> <span id="alg2.l10.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg2.l11" class="ltx_listingline">     previousSamples.splice(firstSample,previousSamples.length)

</div>
<div id="alg2.l12" class="ltx_listingline">     transcription = transcribe(previousSamples)

</div>
<div id="alg2.l13" class="ltx_listingline">     sendTranscription(transcription)

</div>
<div id="alg2.l14" class="ltx_listingline">
<span id="alg2.l14.1" class="ltx_text ltx_font_bold">end</span> <span id="alg2.l14.2" class="ltx_text ltx_font_bold">procedure</span>
</div>
</div>
</figure>
<figure id="alg3" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg3.2.1.1" class="ltx_text ltx_font_bold">Algorithm 3</span> </span> Transcription merge</figcaption>
<div id="alg3.3" class="ltx_listing ltx_listing">
<div id="alg3.l1" class="ltx_listingline">nWords = N

</div>
<div id="alg3.l2" class="ltx_listingline">wordsChecked = M

</div>
<div id="alg3.l3" class="ltx_listingline">
<span id="alg3.l3.1" class="ltx_text ltx_font_bold">procedure</span> <span id="alg3.l3.2" class="ltx_text ltx_font_smallcaps">mergeTranscription</span>(previousTranscription)

</div>
<div id="alg3.l4" class="ltx_listingline">     len = previousTranscription.length

</div>
<div id="alg3.l5" class="ltx_listingline">     previousWords = previousTranscription[len-N-1,len-1]

</div>
<div id="alg3.l6" class="ltx_listingline">     newTranscriptionWords = []

</div>
<div id="alg3.l7" class="ltx_listingline">     previousIndex = -1;

</div>
<div id="alg3.l8" class="ltx_listingline">     newIndex = -1;

</div>
<div id="alg3.l9" class="ltx_listingline">     <span id="alg3.l9.1" class="ltx_text ltx_font_bold">for</span> let i = 0; i &lt;newTranscriptionWords.length-2; i++ <span id="alg3.l9.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg3.l10" class="ltx_listingline">         <span id="alg3.l10.1" class="ltx_text ltx_font_bold">for</span> let j = 2; j &lt;nWords; j++ <span id="alg3.l10.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg3.l11" class="ltx_listingline">              <span id="alg3.l11.1" class="ltx_text ltx_font_bold">for</span> let k = 0; k &lt;wordsChecked; k++ <span id="alg3.l11.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg3.l12" class="ltx_listingline">                  <span id="alg3.l12.1" class="ltx_text ltx_font_bold">if</span> newTranscriptionWords[i+k] == previousWords[nWords-j] <span id="alg3.l12.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg3.l13" class="ltx_listingline">                       previousIndex = j;

</div>
<div id="alg3.l14" class="ltx_listingline">                       newIndex = i;

</div>
<div id="alg3.l15" class="ltx_listingline">                  <span id="alg3.l15.1" class="ltx_text ltx_font_bold">end</span> <span id="alg3.l15.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg3.l16" class="ltx_listingline">              <span id="alg3.l16.1" class="ltx_text ltx_font_bold">end</span> <span id="alg3.l16.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg3.l17" class="ltx_listingline">         <span id="alg3.l17.1" class="ltx_text ltx_font_bold">end</span> <span id="alg3.l17.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg3.l18" class="ltx_listingline">     <span id="alg3.l18.1" class="ltx_text ltx_font_bold">end</span> <span id="alg3.l18.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg3.l19" class="ltx_listingline">     <span id="alg3.l19.1" class="ltx_text ltx_font_bold">if</span> previousIndex != -1 <span id="alg3.l19.2" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg3.l20" class="ltx_listingline">         previousTranscription.removeLastN(previousIndex)

</div>
<div id="alg3.l21" class="ltx_listingline">         newWords = newTranscriptionWords[newIndex, newTranscriptionWords.length -1]

</div>
<div id="alg3.l22" class="ltx_listingline">         previousTranscription.push(newWords);

</div>
<div id="alg3.l23" class="ltx_listingline">     <span id="alg3.l23.1" class="ltx_text ltx_font_bold">else</span>previousTranscription.push(newTranscriptionWords)

</div>
<div id="alg3.l24" class="ltx_listingline">     <span id="alg3.l24.1" class="ltx_text ltx_font_bold">end</span> <span id="alg3.l24.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg3.l25" class="ltx_listingline">
<span id="alg3.l25.1" class="ltx_text ltx_font_bold">end</span> <span id="alg3.l25.2" class="ltx_text ltx_font_bold">procedure</span>
</div>
</div>
</figure>
<div id="S2.SS2.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS3.p3.1" class="ltx_p">Because the output will have repeated words from the previous fragment due to the feedback, the resulting transcription has to be merged with the new one. To merge both transcriptions, we implement a new algorithm presented in Algorithm <a href="#alg3" title="Algorithm 3 ‣ 2.2.3 Feedback ‣ 2.2 Audio splitting ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. It compares the last N words of the previous transcription with the new transcription. If an exact match of M consecutive words is found, the old transcription is replaced by the new one from that point. Depending on the values of N and M, the number of false positives and match misses varies. This is because the previous transcription and the new transcriptions can differ as different samples in different context are processed. If, for example, a large section of the previous transcriptions is searched and only one word is checked, the transcription merge will be inaccurate due to repeated words or articles. On the other hand, if a small section of the previous transcriptions is searched and a large sequence of words is checked, a match cannot be found if transcriptions differ. In this case, the algorithm adds the new transcription at the end. This is implemented for cases where there are large silences, and the new transcription has no relation to the previous one. This process is illustrated in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2.3 Feedback ‣ 2.2 Audio splitting ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In future work, the merge process can be performed by training and an AI model that combines both transcriptions.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2409.05674/assets/figures/merge.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="73" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Merge of the previous and new transcription</figcaption>
</figure>
<div id="S2.SS2.SSS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS3.p4.1" class="ltx_p">For this implementation, the following values were chosen for the algorithm after experimental experience.</p>
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p id="S2.I3.i1.p1.1" class="ltx_p">wordsChecked = 2</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p id="S2.I3.i2.p1.1" class="ltx_p">nWords = 7</p>
</div>
</li>
<li id="S2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i3.p1" class="ltx_para">
<p id="S2.I3.i3.p1.1" class="ltx_p">nSecondsFeedback = 4</p>
</div>
</li>
<li id="S2.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i4.p1" class="ltx_para ltx_noindent">
<p id="S2.I3.i4.p1.1" class="ltx_p">nSeconds(split algorithm) = 2</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS2.SSS3.p5" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS3.p5.1" class="ltx_p">Decreasing nSeconds resulted in a substantially higher WER. Setting nSecondsFeedback at a lower value reduced the quality of the transcription, while increasing it did not improve the results and caused a higher delay. The values selected for wordsChecked and nWords achieved a compromise between match misses and incorrect substitutions. In future work, this algorithm can be improved using an artificial intelligence model that adapts these parameters to the quality of the transcription.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Evaluation metrics and definitions</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">This section introduces the different metrics used to measure the performance of the models. Also, definitions for end-to-end delay and model comparison are provided.</p>
</div>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Metrics</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">The parameters measured to determine the quality of transmission are:</p>
</div>
<div id="S2.SS3.SSS1.p2" class="ltx_para ltx_noindent">
<ul id="S2.I4" class="ltx_itemize">
<li id="S2.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i1.p1" class="ltx_para">
<p id="S2.I4.i1.p1.1" class="ltx_p">Model performance: To measure the performance of the models, three different parameters are measured: word error rate (WER), match error rate (MER), and word information loss (WIL)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
</li>
<li id="S2.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i2.p1" class="ltx_para">
<p id="S2.I4.i2.p1.1" class="ltx_p">End-to-end delay: Delay since a word is pronounced until the transcription appears in the client application.</p>
</div>
</li>
<li id="S2.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i3.p1" class="ltx_para ltx_noindent">
<p id="S2.I4.i3.p1.1" class="ltx_p">Quality/delay: For the real time scenarios, the relationship between transcription accuracy and delay are be studied.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS3.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS1.p3.1" class="ltx_p">The library to calculate the WER, MER, and WIL is Jiwer<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>https://pypi.org/project/jiwer/</span></span></span>. It also provides utilities to normalize the input, such as expanding English contractions, removing extra white spaces… For each audio file, the batch transcription and the real-time transcription are compared to obtain the WER, MER and WIL.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>E2E delay definition</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">The objective of this section is to provide a delay definition that includes all the factors that affect the E2E delay. We define delay in a real-time system transcription system as the time elapsed between a user pronounces a word and its transcription is presented to the user. The total delay (<math id="S2.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="D_{T}" display="inline"><semantics id="S2.SS3.SSS2.p1.1.m1.1a"><msub id="S2.SS3.SSS2.p1.1.m1.1.1" xref="S2.SS3.SSS2.p1.1.m1.1.1.cmml"><mi id="S2.SS3.SSS2.p1.1.m1.1.1.2" xref="S2.SS3.SSS2.p1.1.m1.1.1.2.cmml">D</mi><mi id="S2.SS3.SSS2.p1.1.m1.1.1.3" xref="S2.SS3.SSS2.p1.1.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.1.m1.1b"><apply id="S2.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1.2">𝐷</ci><ci id="S2.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.1.m1.1c">D_{T}</annotation></semantics></math>) is defined as:</p>
</div>
<div id="S2.SS3.SSS2.p2" class="ltx_para ltx_noindent">
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.1" class="ltx_Math" alttext="D_{T}=D_{s}+D_{p}+D_{t}." display="block"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><mrow id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><msub id="S2.E2.m1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.1.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.2.2.cmml">D</mi><mi id="S2.E2.m1.1.1.1.1.2.3" xref="S2.E2.m1.1.1.1.1.2.3.cmml">T</mi></msub><mo id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S2.E2.m1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.3.cmml"><msub id="S2.E2.m1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.3.2.cmml"><mi id="S2.E2.m1.1.1.1.1.3.2.2" xref="S2.E2.m1.1.1.1.1.3.2.2.cmml">D</mi><mi id="S2.E2.m1.1.1.1.1.3.2.3" xref="S2.E2.m1.1.1.1.1.3.2.3.cmml">s</mi></msub><mo id="S2.E2.m1.1.1.1.1.3.1" xref="S2.E2.m1.1.1.1.1.3.1.cmml">+</mo><msub id="S2.E2.m1.1.1.1.1.3.3" xref="S2.E2.m1.1.1.1.1.3.3.cmml"><mi id="S2.E2.m1.1.1.1.1.3.3.2" xref="S2.E2.m1.1.1.1.1.3.3.2.cmml">D</mi><mi id="S2.E2.m1.1.1.1.1.3.3.3" xref="S2.E2.m1.1.1.1.1.3.3.3.cmml">p</mi></msub><mo id="S2.E2.m1.1.1.1.1.3.1a" xref="S2.E2.m1.1.1.1.1.3.1.cmml">+</mo><msub id="S2.E2.m1.1.1.1.1.3.4" xref="S2.E2.m1.1.1.1.1.3.4.cmml"><mi id="S2.E2.m1.1.1.1.1.3.4.2" xref="S2.E2.m1.1.1.1.1.3.4.2.cmml">D</mi><mi id="S2.E2.m1.1.1.1.1.3.4.3" xref="S2.E2.m1.1.1.1.1.3.4.3.cmml">t</mi></msub></mrow></mrow><mo lspace="0em" id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><eq id="S2.E2.m1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"></eq><apply id="S2.E2.m1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.2.2">𝐷</ci><ci id="S2.E2.m1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.2.3">𝑇</ci></apply><apply id="S2.E2.m1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.3"><plus id="S2.E2.m1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3.1"></plus><apply id="S2.E2.m1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.2.1.cmml" xref="S2.E2.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.2.2.cmml" xref="S2.E2.m1.1.1.1.1.3.2.2">𝐷</ci><ci id="S2.E2.m1.1.1.1.1.3.2.3.cmml" xref="S2.E2.m1.1.1.1.1.3.2.3">𝑠</ci></apply><apply id="S2.E2.m1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.3.1.cmml" xref="S2.E2.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.3.2.cmml" xref="S2.E2.m1.1.1.1.1.3.3.2">𝐷</ci><ci id="S2.E2.m1.1.1.1.1.3.3.3.cmml" xref="S2.E2.m1.1.1.1.1.3.3.3">𝑝</ci></apply><apply id="S2.E2.m1.1.1.1.1.3.4.cmml" xref="S2.E2.m1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3.4.1.cmml" xref="S2.E2.m1.1.1.1.1.3.4">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.3.4.2.cmml" xref="S2.E2.m1.1.1.1.1.3.4.2">𝐷</ci><ci id="S2.E2.m1.1.1.1.1.3.4.3.cmml" xref="S2.E2.m1.1.1.1.1.3.4.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">D_{T}=D_{s}+D_{p}+D_{t}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS2.p3.6" class="ltx_p">With <math id="S2.SS3.SSS2.p3.1.m1.1" class="ltx_Math" alttext="D_{s}" display="inline"><semantics id="S2.SS3.SSS2.p3.1.m1.1a"><msub id="S2.SS3.SSS2.p3.1.m1.1.1" xref="S2.SS3.SSS2.p3.1.m1.1.1.cmml"><mi id="S2.SS3.SSS2.p3.1.m1.1.1.2" xref="S2.SS3.SSS2.p3.1.m1.1.1.2.cmml">D</mi><mi id="S2.SS3.SSS2.p3.1.m1.1.1.3" xref="S2.SS3.SSS2.p3.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p3.1.m1.1b"><apply id="S2.SS3.SSS2.p3.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p3.1.m1.1.1.1.cmml" xref="S2.SS3.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p3.1.m1.1.1.2.cmml" xref="S2.SS3.SSS2.p3.1.m1.1.1.2">𝐷</ci><ci id="S2.SS3.SSS2.p3.1.m1.1.1.3.cmml" xref="S2.SS3.SSS2.p3.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p3.1.m1.1c">D_{s}</annotation></semantics></math> being the delay caused by audio splitting, <math id="S2.SS3.SSS2.p3.2.m2.1" class="ltx_Math" alttext="D_{p}" display="inline"><semantics id="S2.SS3.SSS2.p3.2.m2.1a"><msub id="S2.SS3.SSS2.p3.2.m2.1.1" xref="S2.SS3.SSS2.p3.2.m2.1.1.cmml"><mi id="S2.SS3.SSS2.p3.2.m2.1.1.2" xref="S2.SS3.SSS2.p3.2.m2.1.1.2.cmml">D</mi><mi id="S2.SS3.SSS2.p3.2.m2.1.1.3" xref="S2.SS3.SSS2.p3.2.m2.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p3.2.m2.1b"><apply id="S2.SS3.SSS2.p3.2.m2.1.1.cmml" xref="S2.SS3.SSS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p3.2.m2.1.1.1.cmml" xref="S2.SS3.SSS2.p3.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p3.2.m2.1.1.2.cmml" xref="S2.SS3.SSS2.p3.2.m2.1.1.2">𝐷</ci><ci id="S2.SS3.SSS2.p3.2.m2.1.1.3.cmml" xref="S2.SS3.SSS2.p3.2.m2.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p3.2.m2.1c">D_{p}</annotation></semantics></math> being the delay caused by audio processing, and <math id="S2.SS3.SSS2.p3.3.m3.1" class="ltx_Math" alttext="D_{t}" display="inline"><semantics id="S2.SS3.SSS2.p3.3.m3.1a"><msub id="S2.SS3.SSS2.p3.3.m3.1.1" xref="S2.SS3.SSS2.p3.3.m3.1.1.cmml"><mi id="S2.SS3.SSS2.p3.3.m3.1.1.2" xref="S2.SS3.SSS2.p3.3.m3.1.1.2.cmml">D</mi><mi id="S2.SS3.SSS2.p3.3.m3.1.1.3" xref="S2.SS3.SSS2.p3.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p3.3.m3.1b"><apply id="S2.SS3.SSS2.p3.3.m3.1.1.cmml" xref="S2.SS3.SSS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p3.3.m3.1.1.1.cmml" xref="S2.SS3.SSS2.p3.3.m3.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p3.3.m3.1.1.2.cmml" xref="S2.SS3.SSS2.p3.3.m3.1.1.2">𝐷</ci><ci id="S2.SS3.SSS2.p3.3.m3.1.1.3.cmml" xref="S2.SS3.SSS2.p3.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p3.3.m3.1c">D_{t}</annotation></semantics></math> the transmission delay introduced by the splitter-ASR cluster connection. The value of <math id="S2.SS3.SSS2.p3.4.m4.1" class="ltx_Math" alttext="D_{p}" display="inline"><semantics id="S2.SS3.SSS2.p3.4.m4.1a"><msub id="S2.SS3.SSS2.p3.4.m4.1.1" xref="S2.SS3.SSS2.p3.4.m4.1.1.cmml"><mi id="S2.SS3.SSS2.p3.4.m4.1.1.2" xref="S2.SS3.SSS2.p3.4.m4.1.1.2.cmml">D</mi><mi id="S2.SS3.SSS2.p3.4.m4.1.1.3" xref="S2.SS3.SSS2.p3.4.m4.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p3.4.m4.1b"><apply id="S2.SS3.SSS2.p3.4.m4.1.1.cmml" xref="S2.SS3.SSS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p3.4.m4.1.1.1.cmml" xref="S2.SS3.SSS2.p3.4.m4.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p3.4.m4.1.1.2.cmml" xref="S2.SS3.SSS2.p3.4.m4.1.1.2">𝐷</ci><ci id="S2.SS3.SSS2.p3.4.m4.1.1.3.cmml" xref="S2.SS3.SSS2.p3.4.m4.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p3.4.m4.1c">D_{p}</annotation></semantics></math> varies depending on the hardware used to run the models and the different processes. On the other hand, <math id="S2.SS3.SSS2.p3.5.m5.1" class="ltx_Math" alttext="D_{s}" display="inline"><semantics id="S2.SS3.SSS2.p3.5.m5.1a"><msub id="S2.SS3.SSS2.p3.5.m5.1.1" xref="S2.SS3.SSS2.p3.5.m5.1.1.cmml"><mi id="S2.SS3.SSS2.p3.5.m5.1.1.2" xref="S2.SS3.SSS2.p3.5.m5.1.1.2.cmml">D</mi><mi id="S2.SS3.SSS2.p3.5.m5.1.1.3" xref="S2.SS3.SSS2.p3.5.m5.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p3.5.m5.1b"><apply id="S2.SS3.SSS2.p3.5.m5.1.1.cmml" xref="S2.SS3.SSS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p3.5.m5.1.1.1.cmml" xref="S2.SS3.SSS2.p3.5.m5.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p3.5.m5.1.1.2.cmml" xref="S2.SS3.SSS2.p3.5.m5.1.1.2">𝐷</ci><ci id="S2.SS3.SSS2.p3.5.m5.1.1.3.cmml" xref="S2.SS3.SSS2.p3.5.m5.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p3.5.m5.1c">D_{s}</annotation></semantics></math> is independent of it, since it only depends on the time the audio splitting algorithms buffer the samples. <math id="S2.SS3.SSS2.p3.6.m6.1" class="ltx_Math" alttext="D_{t}" display="inline"><semantics id="S2.SS3.SSS2.p3.6.m6.1a"><msub id="S2.SS3.SSS2.p3.6.m6.1.1" xref="S2.SS3.SSS2.p3.6.m6.1.1.cmml"><mi id="S2.SS3.SSS2.p3.6.m6.1.1.2" xref="S2.SS3.SSS2.p3.6.m6.1.1.2.cmml">D</mi><mi id="S2.SS3.SSS2.p3.6.m6.1.1.3" xref="S2.SS3.SSS2.p3.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p3.6.m6.1b"><apply id="S2.SS3.SSS2.p3.6.m6.1.1.cmml" xref="S2.SS3.SSS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p3.6.m6.1.1.1.cmml" xref="S2.SS3.SSS2.p3.6.m6.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p3.6.m6.1.1.2.cmml" xref="S2.SS3.SSS2.p3.6.m6.1.1.2">𝐷</ci><ci id="S2.SS3.SSS2.p3.6.m6.1.1.3.cmml" xref="S2.SS3.SSS2.p3.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p3.6.m6.1c">D_{t}</annotation></semantics></math> depends on the geographical position of the components and the connection between them. It is independent of both algorithm and model.</p>
</div>
</section>
<section id="S2.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.3 </span>Delay measurement algorithm</h4>

<div id="S2.SS3.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS3.p1.1" class="ltx_p">To accurately measure the transcription delay, reference transcriptions must include time annotations. GigaSpeech’s transcriptions are divided into segments with start and end timestamp. For this reason, the only known timestamps are when the first word of the segment starts to be pronounced and when the last word is pronounced. Because of this, to measure the delay, only the first word of each segment will be used for the delay measurement.</p>
</div>
<div id="S2.SS3.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS3.p2.1" class="ltx_p">These segments do not include silences or parts with music or other sounds, so the start and end timestamps are consistent. These silences or non-conversational sounds are annotated as such and removed from our measurements.</p>
</div>
<div id="S2.SS3.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS3.p3.1" class="ltx_p">To measure the delay, the timestamp when the audio file starts playing is stored. Then, every time a transcription from the ASR cluster is received, it is stored with its corresponding timestamp. All of the words included in these transcriptions are stored with the same timestamp as all words are received at the same time. The difference between this timestamp with the one stored at the beginning is compared with the time annotations from GigaSpeechs’ transcriptions. The delay varies depending on the position of the word in the segment. Words that appear at the start of a fragment have an increased delay as their audio samples have to be stored until the fragment is complete. For this reason the process is repeated multiple times to average the results.</p>
</div>
<div id="S2.SS3.SSS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS3.p4.1" class="ltx_p">Because in the resulting transcription, each word does not only appear once (words such as ”the” or ”it” have a high number of repetitions), the correct repetition has to be found to compare timestamps. For this reason, words are searched with their context. We define context as the M words before and after the selected word. To reduce false positives due to common expressions such as ”I am not”, ”there was a”, … a sliding-window algorithm is used to reduce the search width. The algorithm is presented in Algorithm <a href="#alg4" title="Algorithm 4 ‣ 2.3.3 Delay measurement algorithm ‣ 2.3 Evaluation metrics and definitions ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S2.SS3.SSS3.p5" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS3.p5.1" class="ltx_p">For the first word searched, the algorithm checks if it exists in the first N segments. All of the segments where the word is found are stored. After that, for each of those segments, it is checked if the word appears with its context. For the next iteration, the next word is searched in the range [i, N + i], i being the index of the last word found in its context. In the case where a word is not found, the number of segments checked is increased until a new match is found.</p>
</div>
<figure id="alg4" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg4.2.1.1" class="ltx_text ltx_font_bold">Algorithm 4</span> </span> Word delay calculation</figcaption>
<div id="alg4.3" class="ltx_listing ltx_listing">
<div id="alg4.l1" class="ltx_listingline">searchindex= 0

</div>
<div id="alg4.l2" class="ltx_listingline">searchWidth = N

</div>
<div id="alg4.l3" class="ltx_listingline">originalTranscriptionSegments = Array[]

</div>
<div id="alg4.l4" class="ltx_listingline">whisperTranscription = Array[]

</div>
<div id="alg4.l5" class="ltx_listingline">
<span id="alg4.l5.1" class="ltx_text ltx_font_bold">for</span> word in originalTranscriptionSegments <span id="alg4.l5.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg4.l6" class="ltx_listingline">     <span id="alg4.l6.1" class="ltx_text ltx_font_bold">for</span> segment in whisperTranscription[searchIndex, searchIndex + searchWidth] <span id="alg4.l6.2" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg4.l7" class="ltx_listingline">         <span id="alg4.l7.1" class="ltx_text ltx_font_bold">if</span> word in segment <span id="alg4.l7.2" class="ltx_text ltx_font_bold">then</span>:

</div>
<div id="alg4.l8" class="ltx_listingline">              <span id="alg4.l8.1" class="ltx_text ltx_font_bold">if</span> (isContextInSegment(word.context, segment)) <span id="alg4.l8.2" class="ltx_text ltx_font_bold">then</span>:

</div>
<div id="alg4.l9" class="ltx_listingline">                  measuresDelay(word, segment)

</div>
<div id="alg4.l10" class="ltx_listingline">                  searchWidth = N

</div>
<div id="alg4.l11" class="ltx_listingline">                  searchIndex = segment.index

</div>
<div id="alg4.l12" class="ltx_listingline">              <span id="alg4.l12.1" class="ltx_text ltx_font_bold">end</span> <span id="alg4.l12.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg4.l13" class="ltx_listingline">         <span id="alg4.l13.1" class="ltx_text ltx_font_bold">else</span>
</div>
<div id="alg4.l14" class="ltx_listingline">              searchWidh += M

</div>
<div id="alg4.l15" class="ltx_listingline">         <span id="alg4.l15.1" class="ltx_text ltx_font_bold">end</span> <span id="alg4.l15.2" class="ltx_text ltx_font_bold">if</span>
</div>
<div id="alg4.l16" class="ltx_listingline">     <span id="alg4.l16.1" class="ltx_text ltx_font_bold">end</span> <span id="alg4.l16.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg4.l17" class="ltx_listingline">
<span id="alg4.l17.1" class="ltx_text ltx_font_bold">end</span> <span id="alg4.l17.2" class="ltx_text ltx_font_bold">for</span>
</div>
</div>
</figure>
<div id="S2.SS3.SSS3.p6" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS3.p6.1" class="ltx_p">For the feedback algorithm, it had to be taken into account that words can be replaced by the merge algorithm. To solve this, words that were replaced by the merge were not included in the delay calculation. Following the example in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2.3 Feedback ‣ 2.2 Audio splitting ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, only the words ”speedcuber. The vision for the” were stored with their corresponding timestamp.</p>
</div>
</section>
<section id="S2.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.4 </span>Algorithms and model comparison</h4>

<div id="S2.SS3.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS4.p1.2" class="ltx_p">We define that an algorithm-model combination <math id="S2.SS3.SSS4.p1.1.m1.1" class="ltx_Math" alttext="C_{1}" display="inline"><semantics id="S2.SS3.SSS4.p1.1.m1.1a"><msub id="S2.SS3.SSS4.p1.1.m1.1.1" xref="S2.SS3.SSS4.p1.1.m1.1.1.cmml"><mi id="S2.SS3.SSS4.p1.1.m1.1.1.2" xref="S2.SS3.SSS4.p1.1.m1.1.1.2.cmml">C</mi><mn id="S2.SS3.SSS4.p1.1.m1.1.1.3" xref="S2.SS3.SSS4.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS4.p1.1.m1.1b"><apply id="S2.SS3.SSS4.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS4.p1.1.m1.1.1.1.cmml" xref="S2.SS3.SSS4.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS4.p1.1.m1.1.1.2.cmml" xref="S2.SS3.SSS4.p1.1.m1.1.1.2">𝐶</ci><cn type="integer" id="S2.SS3.SSS4.p1.1.m1.1.1.3.cmml" xref="S2.SS3.SSS4.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS4.p1.1.m1.1c">C_{1}</annotation></semantics></math> is better than another combination <math id="S2.SS3.SSS4.p1.2.m2.1" class="ltx_Math" alttext="C_{2}" display="inline"><semantics id="S2.SS3.SSS4.p1.2.m2.1a"><msub id="S2.SS3.SSS4.p1.2.m2.1.1" xref="S2.SS3.SSS4.p1.2.m2.1.1.cmml"><mi id="S2.SS3.SSS4.p1.2.m2.1.1.2" xref="S2.SS3.SSS4.p1.2.m2.1.1.2.cmml">C</mi><mn id="S2.SS3.SSS4.p1.2.m2.1.1.3" xref="S2.SS3.SSS4.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS4.p1.2.m2.1b"><apply id="S2.SS3.SSS4.p1.2.m2.1.1.cmml" xref="S2.SS3.SSS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS4.p1.2.m2.1.1.1.cmml" xref="S2.SS3.SSS4.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.SSS4.p1.2.m2.1.1.2.cmml" xref="S2.SS3.SSS4.p1.2.m2.1.1.2">𝐶</ci><cn type="integer" id="S2.SS3.SSS4.p1.2.m2.1.1.3.cmml" xref="S2.SS3.SSS4.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS4.p1.2.m2.1c">C_{2}</annotation></semantics></math> if it satisfies the conditions stated in Equation <a href="#S2.E4" title="In 2.3.4 Algorithms and model comparison ‣ 2.3 Evaluation metrics and definitions ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and the premises stated in Equation <a href="#S2.E3" title="In 2.3.4 Algorithms and model comparison ‣ 2.3 Evaluation metrics and definitions ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S2.SS3.SSS4.p2" class="ltx_para ltx_noindent">
<table id="S4.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E3.m1.2" class="ltx_Math" alttext="\displaystyle Premise:(D_{t_{c1}}=D_{t_{c2}})\land(H_{1}=H_{2})" display="inline"><semantics id="S2.E3.m1.2a"><mrow id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml"><mrow id="S2.E3.m1.2.2.4" xref="S2.E3.m1.2.2.4.cmml"><mi id="S2.E3.m1.2.2.4.2" xref="S2.E3.m1.2.2.4.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.4.1" xref="S2.E3.m1.2.2.4.1.cmml">​</mo><mi id="S2.E3.m1.2.2.4.3" xref="S2.E3.m1.2.2.4.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.4.1a" xref="S2.E3.m1.2.2.4.1.cmml">​</mo><mi id="S2.E3.m1.2.2.4.4" xref="S2.E3.m1.2.2.4.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.4.1b" xref="S2.E3.m1.2.2.4.1.cmml">​</mo><mi id="S2.E3.m1.2.2.4.5" xref="S2.E3.m1.2.2.4.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.4.1c" xref="S2.E3.m1.2.2.4.1.cmml">​</mo><mi id="S2.E3.m1.2.2.4.6" xref="S2.E3.m1.2.2.4.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.4.1d" xref="S2.E3.m1.2.2.4.1.cmml">​</mo><mi id="S2.E3.m1.2.2.4.7" xref="S2.E3.m1.2.2.4.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.4.1e" xref="S2.E3.m1.2.2.4.1.cmml">​</mo><mi id="S2.E3.m1.2.2.4.8" xref="S2.E3.m1.2.2.4.8.cmml">e</mi></mrow><mo lspace="0.278em" rspace="0.278em" id="S2.E3.m1.2.2.3" xref="S2.E3.m1.2.2.3.cmml">:</mo><mrow id="S2.E3.m1.2.2.2" xref="S2.E3.m1.2.2.2.cmml"><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><msub id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2.2" xref="S2.E3.m1.1.1.1.1.1.1.2.2.cmml">D</mi><msub id="S2.E3.m1.1.1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2.3.2" xref="S2.E3.m1.1.1.1.1.1.1.2.3.2.cmml">t</mi><mrow id="S2.E3.m1.1.1.1.1.1.1.2.3.3" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2.3.3.2" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.1.1.1.1.2.3.3.1" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3.1.cmml">​</mo><mn id="S2.E3.m1.1.1.1.1.1.1.2.3.3.3" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3.3.cmml">1</mn></mrow></msub></msub><mo id="S2.E3.m1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.cmml">=</mo><msub id="S2.E3.m1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.1.3.2.cmml">D</mi><msub id="S2.E3.m1.1.1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.3.3.2" xref="S2.E3.m1.1.1.1.1.1.1.3.3.2.cmml">t</mi><mrow id="S2.E3.m1.1.1.1.1.1.1.3.3.3" xref="S2.E3.m1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.3.3.3.2" xref="S2.E3.m1.1.1.1.1.1.1.3.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.1.1.1.1.3.3.3.1" xref="S2.E3.m1.1.1.1.1.1.1.3.3.3.1.cmml">​</mo><mn id="S2.E3.m1.1.1.1.1.1.1.3.3.3.3" xref="S2.E3.m1.1.1.1.1.1.1.3.3.3.3.cmml">2</mn></mrow></msub></msub></mrow><mo stretchy="false" id="S2.E3.m1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.E3.m1.2.2.2.3" xref="S2.E3.m1.2.2.2.3.cmml">∧</mo><mrow id="S2.E3.m1.2.2.2.2.1" xref="S2.E3.m1.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.2.2.1.2" xref="S2.E3.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S2.E3.m1.2.2.2.2.1.1" xref="S2.E3.m1.2.2.2.2.1.1.cmml"><msub id="S2.E3.m1.2.2.2.2.1.1.2" xref="S2.E3.m1.2.2.2.2.1.1.2.cmml"><mi id="S2.E3.m1.2.2.2.2.1.1.2.2" xref="S2.E3.m1.2.2.2.2.1.1.2.2.cmml">H</mi><mn id="S2.E3.m1.2.2.2.2.1.1.2.3" xref="S2.E3.m1.2.2.2.2.1.1.2.3.cmml">1</mn></msub><mo id="S2.E3.m1.2.2.2.2.1.1.1" xref="S2.E3.m1.2.2.2.2.1.1.1.cmml">=</mo><msub id="S2.E3.m1.2.2.2.2.1.1.3" xref="S2.E3.m1.2.2.2.2.1.1.3.cmml"><mi id="S2.E3.m1.2.2.2.2.1.1.3.2" xref="S2.E3.m1.2.2.2.2.1.1.3.2.cmml">H</mi><mn id="S2.E3.m1.2.2.2.2.1.1.3.3" xref="S2.E3.m1.2.2.2.2.1.1.3.3.cmml">2</mn></msub></mrow><mo stretchy="false" id="S2.E3.m1.2.2.2.2.1.3" xref="S2.E3.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.2b"><apply id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2"><ci id="S2.E3.m1.2.2.3.cmml" xref="S2.E3.m1.2.2.3">:</ci><apply id="S2.E3.m1.2.2.4.cmml" xref="S2.E3.m1.2.2.4"><times id="S2.E3.m1.2.2.4.1.cmml" xref="S2.E3.m1.2.2.4.1"></times><ci id="S2.E3.m1.2.2.4.2.cmml" xref="S2.E3.m1.2.2.4.2">𝑃</ci><ci id="S2.E3.m1.2.2.4.3.cmml" xref="S2.E3.m1.2.2.4.3">𝑟</ci><ci id="S2.E3.m1.2.2.4.4.cmml" xref="S2.E3.m1.2.2.4.4">𝑒</ci><ci id="S2.E3.m1.2.2.4.5.cmml" xref="S2.E3.m1.2.2.4.5">𝑚</ci><ci id="S2.E3.m1.2.2.4.6.cmml" xref="S2.E3.m1.2.2.4.6">𝑖</ci><ci id="S2.E3.m1.2.2.4.7.cmml" xref="S2.E3.m1.2.2.4.7">𝑠</ci><ci id="S2.E3.m1.2.2.4.8.cmml" xref="S2.E3.m1.2.2.4.8">𝑒</ci></apply><apply id="S2.E3.m1.2.2.2.cmml" xref="S2.E3.m1.2.2.2"><and id="S2.E3.m1.2.2.2.3.cmml" xref="S2.E3.m1.2.2.2.3"></and><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><eq id="S2.E3.m1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1"></eq><apply id="S2.E3.m1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2">𝐷</ci><apply id="S2.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.2">𝑡</ci><apply id="S2.E3.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3"><times id="S2.E3.m1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3.1"></times><ci id="S2.E3.m1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3.2">𝑐</ci><cn type="integer" id="S2.E3.m1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3.3">1</cn></apply></apply></apply><apply id="S2.E3.m1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3.2">𝐷</ci><apply id="S2.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3.3.2">𝑡</ci><apply id="S2.E3.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3.3.3"><times id="S2.E3.m1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3.3.3.1"></times><ci id="S2.E3.m1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3.3.3.2">𝑐</ci><cn type="integer" id="S2.E3.m1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3.3.3.3">2</cn></apply></apply></apply></apply><apply id="S2.E3.m1.2.2.2.2.1.1.cmml" xref="S2.E3.m1.2.2.2.2.1"><eq id="S2.E3.m1.2.2.2.2.1.1.1.cmml" xref="S2.E3.m1.2.2.2.2.1.1.1"></eq><apply id="S2.E3.m1.2.2.2.2.1.1.2.cmml" xref="S2.E3.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.2.1.1.2.1.cmml" xref="S2.E3.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S2.E3.m1.2.2.2.2.1.1.2.2.cmml" xref="S2.E3.m1.2.2.2.2.1.1.2.2">𝐻</ci><cn type="integer" id="S2.E3.m1.2.2.2.2.1.1.2.3.cmml" xref="S2.E3.m1.2.2.2.2.1.1.2.3">1</cn></apply><apply id="S2.E3.m1.2.2.2.2.1.1.3.cmml" xref="S2.E3.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.2.1.1.3.1.cmml" xref="S2.E3.m1.2.2.2.2.1.1.3">subscript</csymbol><ci id="S2.E3.m1.2.2.2.2.1.1.3.2.cmml" xref="S2.E3.m1.2.2.2.2.1.1.3.2">𝐻</ci><cn type="integer" id="S2.E3.m1.2.2.2.2.1.1.3.3.cmml" xref="S2.E3.m1.2.2.2.2.1.1.3.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.2c">\displaystyle Premise:(D_{t_{c1}}=D_{t_{c2}})\land(H_{1}=H_{2})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S2.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E4.m1.2" class="ltx_Math" alttext="\displaystyle C_{1}&gt;C_{2}\iff(Q_{C_{1}}&gt;Q_{C_{2}})\land(D_{T_{C_{1}}}&lt;D_{T_{C_{2}}})" display="inline"><semantics id="S2.E4.m1.2a"><mrow id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml"><mrow id="S2.E4.m1.2.2.4" xref="S2.E4.m1.2.2.4.cmml"><msub id="S2.E4.m1.2.2.4.2" xref="S2.E4.m1.2.2.4.2.cmml"><mi id="S2.E4.m1.2.2.4.2.2" xref="S2.E4.m1.2.2.4.2.2.cmml">C</mi><mn id="S2.E4.m1.2.2.4.2.3" xref="S2.E4.m1.2.2.4.2.3.cmml">1</mn></msub><mo id="S2.E4.m1.2.2.4.1" xref="S2.E4.m1.2.2.4.1.cmml">&gt;</mo><msub id="S2.E4.m1.2.2.4.3" xref="S2.E4.m1.2.2.4.3.cmml"><mi id="S2.E4.m1.2.2.4.3.2" xref="S2.E4.m1.2.2.4.3.2.cmml">C</mi><mn id="S2.E4.m1.2.2.4.3.3" xref="S2.E4.m1.2.2.4.3.3.cmml">2</mn></msub></mrow><mo stretchy="false" id="S2.E4.m1.2.2.3" xref="S2.E4.m1.2.2.3.cmml">⇔</mo><mrow id="S2.E4.m1.2.2.2" xref="S2.E4.m1.2.2.2.cmml"><mrow id="S2.E4.m1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.cmml"><msub id="S2.E4.m1.1.1.1.1.1.1.2" xref="S2.E4.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.2.2" xref="S2.E4.m1.1.1.1.1.1.1.2.2.cmml">Q</mi><msub id="S2.E4.m1.1.1.1.1.1.1.2.3" xref="S2.E4.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.2.3.2" xref="S2.E4.m1.1.1.1.1.1.1.2.3.2.cmml">C</mi><mn id="S2.E4.m1.1.1.1.1.1.1.2.3.3" xref="S2.E4.m1.1.1.1.1.1.1.2.3.3.cmml">1</mn></msub></msub><mo id="S2.E4.m1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.cmml">&gt;</mo><msub id="S2.E4.m1.1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.3.2" xref="S2.E4.m1.1.1.1.1.1.1.3.2.cmml">Q</mi><msub id="S2.E4.m1.1.1.1.1.1.1.3.3" xref="S2.E4.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E4.m1.1.1.1.1.1.1.3.3.2" xref="S2.E4.m1.1.1.1.1.1.1.3.3.2.cmml">C</mi><mn id="S2.E4.m1.1.1.1.1.1.1.3.3.3" xref="S2.E4.m1.1.1.1.1.1.1.3.3.3.cmml">2</mn></msub></msub></mrow><mo stretchy="false" id="S2.E4.m1.1.1.1.1.1.3" xref="S2.E4.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.E4.m1.2.2.2.3" xref="S2.E4.m1.2.2.2.3.cmml">∧</mo><mrow id="S2.E4.m1.2.2.2.2.1" xref="S2.E4.m1.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.2.2.2.2.1.2" xref="S2.E4.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S2.E4.m1.2.2.2.2.1.1" xref="S2.E4.m1.2.2.2.2.1.1.cmml"><msub id="S2.E4.m1.2.2.2.2.1.1.2" xref="S2.E4.m1.2.2.2.2.1.1.2.cmml"><mi id="S2.E4.m1.2.2.2.2.1.1.2.2" xref="S2.E4.m1.2.2.2.2.1.1.2.2.cmml">D</mi><msub id="S2.E4.m1.2.2.2.2.1.1.2.3" xref="S2.E4.m1.2.2.2.2.1.1.2.3.cmml"><mi id="S2.E4.m1.2.2.2.2.1.1.2.3.2" xref="S2.E4.m1.2.2.2.2.1.1.2.3.2.cmml">T</mi><msub id="S2.E4.m1.2.2.2.2.1.1.2.3.3" xref="S2.E4.m1.2.2.2.2.1.1.2.3.3.cmml"><mi id="S2.E4.m1.2.2.2.2.1.1.2.3.3.2" xref="S2.E4.m1.2.2.2.2.1.1.2.3.3.2.cmml">C</mi><mn id="S2.E4.m1.2.2.2.2.1.1.2.3.3.3" xref="S2.E4.m1.2.2.2.2.1.1.2.3.3.3.cmml">1</mn></msub></msub></msub><mo id="S2.E4.m1.2.2.2.2.1.1.1" xref="S2.E4.m1.2.2.2.2.1.1.1.cmml">&lt;</mo><msub id="S2.E4.m1.2.2.2.2.1.1.3" xref="S2.E4.m1.2.2.2.2.1.1.3.cmml"><mi id="S2.E4.m1.2.2.2.2.1.1.3.2" xref="S2.E4.m1.2.2.2.2.1.1.3.2.cmml">D</mi><msub id="S2.E4.m1.2.2.2.2.1.1.3.3" xref="S2.E4.m1.2.2.2.2.1.1.3.3.cmml"><mi id="S2.E4.m1.2.2.2.2.1.1.3.3.2" xref="S2.E4.m1.2.2.2.2.1.1.3.3.2.cmml">T</mi><msub id="S2.E4.m1.2.2.2.2.1.1.3.3.3" xref="S2.E4.m1.2.2.2.2.1.1.3.3.3.cmml"><mi id="S2.E4.m1.2.2.2.2.1.1.3.3.3.2" xref="S2.E4.m1.2.2.2.2.1.1.3.3.3.2.cmml">C</mi><mn id="S2.E4.m1.2.2.2.2.1.1.3.3.3.3" xref="S2.E4.m1.2.2.2.2.1.1.3.3.3.3.cmml">2</mn></msub></msub></msub></mrow><mo stretchy="false" id="S2.E4.m1.2.2.2.2.1.3" xref="S2.E4.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.2b"><apply id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2"><csymbol cd="latexml" id="S2.E4.m1.2.2.3.cmml" xref="S2.E4.m1.2.2.3">iff</csymbol><apply id="S2.E4.m1.2.2.4.cmml" xref="S2.E4.m1.2.2.4"><gt id="S2.E4.m1.2.2.4.1.cmml" xref="S2.E4.m1.2.2.4.1"></gt><apply id="S2.E4.m1.2.2.4.2.cmml" xref="S2.E4.m1.2.2.4.2"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.4.2.1.cmml" xref="S2.E4.m1.2.2.4.2">subscript</csymbol><ci id="S2.E4.m1.2.2.4.2.2.cmml" xref="S2.E4.m1.2.2.4.2.2">𝐶</ci><cn type="integer" id="S2.E4.m1.2.2.4.2.3.cmml" xref="S2.E4.m1.2.2.4.2.3">1</cn></apply><apply id="S2.E4.m1.2.2.4.3.cmml" xref="S2.E4.m1.2.2.4.3"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.4.3.1.cmml" xref="S2.E4.m1.2.2.4.3">subscript</csymbol><ci id="S2.E4.m1.2.2.4.3.2.cmml" xref="S2.E4.m1.2.2.4.3.2">𝐶</ci><cn type="integer" id="S2.E4.m1.2.2.4.3.3.cmml" xref="S2.E4.m1.2.2.4.3.3">2</cn></apply></apply><apply id="S2.E4.m1.2.2.2.cmml" xref="S2.E4.m1.2.2.2"><and id="S2.E4.m1.2.2.2.3.cmml" xref="S2.E4.m1.2.2.2.3"></and><apply id="S2.E4.m1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1"><gt id="S2.E4.m1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.1"></gt><apply id="S2.E4.m1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.2.2">𝑄</ci><apply id="S2.E4.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.2.3.2">𝐶</ci><cn type="integer" id="S2.E4.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S2.E4.m1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.3.2">𝑄</ci><apply id="S2.E4.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E4.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E4.m1.1.1.1.1.1.1.3.3.2">𝐶</ci><cn type="integer" id="S2.E4.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E4.m1.1.1.1.1.1.1.3.3.3">2</cn></apply></apply></apply><apply id="S2.E4.m1.2.2.2.2.1.1.cmml" xref="S2.E4.m1.2.2.2.2.1"><lt id="S2.E4.m1.2.2.2.2.1.1.1.cmml" xref="S2.E4.m1.2.2.2.2.1.1.1"></lt><apply id="S2.E4.m1.2.2.2.2.1.1.2.cmml" xref="S2.E4.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.2.2.1.1.2.1.cmml" xref="S2.E4.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S2.E4.m1.2.2.2.2.1.1.2.2.cmml" xref="S2.E4.m1.2.2.2.2.1.1.2.2">𝐷</ci><apply id="S2.E4.m1.2.2.2.2.1.1.2.3.cmml" xref="S2.E4.m1.2.2.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.2.2.1.1.2.3.1.cmml" xref="S2.E4.m1.2.2.2.2.1.1.2.3">subscript</csymbol><ci id="S2.E4.m1.2.2.2.2.1.1.2.3.2.cmml" xref="S2.E4.m1.2.2.2.2.1.1.2.3.2">𝑇</ci><apply id="S2.E4.m1.2.2.2.2.1.1.2.3.3.cmml" xref="S2.E4.m1.2.2.2.2.1.1.2.3.3"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.2.2.1.1.2.3.3.1.cmml" xref="S2.E4.m1.2.2.2.2.1.1.2.3.3">subscript</csymbol><ci id="S2.E4.m1.2.2.2.2.1.1.2.3.3.2.cmml" xref="S2.E4.m1.2.2.2.2.1.1.2.3.3.2">𝐶</ci><cn type="integer" id="S2.E4.m1.2.2.2.2.1.1.2.3.3.3.cmml" xref="S2.E4.m1.2.2.2.2.1.1.2.3.3.3">1</cn></apply></apply></apply><apply id="S2.E4.m1.2.2.2.2.1.1.3.cmml" xref="S2.E4.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.2.2.1.1.3.1.cmml" xref="S2.E4.m1.2.2.2.2.1.1.3">subscript</csymbol><ci id="S2.E4.m1.2.2.2.2.1.1.3.2.cmml" xref="S2.E4.m1.2.2.2.2.1.1.3.2">𝐷</ci><apply id="S2.E4.m1.2.2.2.2.1.1.3.3.cmml" xref="S2.E4.m1.2.2.2.2.1.1.3.3"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.2.2.1.1.3.3.1.cmml" xref="S2.E4.m1.2.2.2.2.1.1.3.3">subscript</csymbol><ci id="S2.E4.m1.2.2.2.2.1.1.3.3.2.cmml" xref="S2.E4.m1.2.2.2.2.1.1.3.3.2">𝑇</ci><apply id="S2.E4.m1.2.2.2.2.1.1.3.3.3.cmml" xref="S2.E4.m1.2.2.2.2.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.2.2.1.1.3.3.3.1.cmml" xref="S2.E4.m1.2.2.2.2.1.1.3.3.3">subscript</csymbol><ci id="S2.E4.m1.2.2.2.2.1.1.3.3.3.2.cmml" xref="S2.E4.m1.2.2.2.2.1.1.3.3.3.2">𝐶</ci><cn type="integer" id="S2.E4.m1.2.2.2.2.1.1.3.3.3.3.cmml" xref="S2.E4.m1.2.2.2.2.1.1.3.3.3.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.2c">\displaystyle C_{1}&gt;C_{2}\iff(Q_{C_{1}}&gt;Q_{C_{2}})\land(D_{T_{C_{1}}}&lt;D_{T_{C_{2}}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.SSS4.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS4.p3.5" class="ltx_p"><math id="S2.SS3.SSS4.p3.1.m1.1" class="ltx_Math" alttext="Q_{i}" display="inline"><semantics id="S2.SS3.SSS4.p3.1.m1.1a"><msub id="S2.SS3.SSS4.p3.1.m1.1.1" xref="S2.SS3.SSS4.p3.1.m1.1.1.cmml"><mi id="S2.SS3.SSS4.p3.1.m1.1.1.2" xref="S2.SS3.SSS4.p3.1.m1.1.1.2.cmml">Q</mi><mi id="S2.SS3.SSS4.p3.1.m1.1.1.3" xref="S2.SS3.SSS4.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS4.p3.1.m1.1b"><apply id="S2.SS3.SSS4.p3.1.m1.1.1.cmml" xref="S2.SS3.SSS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS4.p3.1.m1.1.1.1.cmml" xref="S2.SS3.SSS4.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS4.p3.1.m1.1.1.2.cmml" xref="S2.SS3.SSS4.p3.1.m1.1.1.2">𝑄</ci><ci id="S2.SS3.SSS4.p3.1.m1.1.1.3.cmml" xref="S2.SS3.SSS4.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS4.p3.1.m1.1c">Q_{i}</annotation></semantics></math> is the quality of a transcription, <math id="S2.SS3.SSS4.p3.2.m2.1" class="ltx_Math" alttext="D_{T_{i}}" display="inline"><semantics id="S2.SS3.SSS4.p3.2.m2.1a"><msub id="S2.SS3.SSS4.p3.2.m2.1.1" xref="S2.SS3.SSS4.p3.2.m2.1.1.cmml"><mi id="S2.SS3.SSS4.p3.2.m2.1.1.2" xref="S2.SS3.SSS4.p3.2.m2.1.1.2.cmml">D</mi><msub id="S2.SS3.SSS4.p3.2.m2.1.1.3" xref="S2.SS3.SSS4.p3.2.m2.1.1.3.cmml"><mi id="S2.SS3.SSS4.p3.2.m2.1.1.3.2" xref="S2.SS3.SSS4.p3.2.m2.1.1.3.2.cmml">T</mi><mi id="S2.SS3.SSS4.p3.2.m2.1.1.3.3" xref="S2.SS3.SSS4.p3.2.m2.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS4.p3.2.m2.1b"><apply id="S2.SS3.SSS4.p3.2.m2.1.1.cmml" xref="S2.SS3.SSS4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS4.p3.2.m2.1.1.1.cmml" xref="S2.SS3.SSS4.p3.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.SSS4.p3.2.m2.1.1.2.cmml" xref="S2.SS3.SSS4.p3.2.m2.1.1.2">𝐷</ci><apply id="S2.SS3.SSS4.p3.2.m2.1.1.3.cmml" xref="S2.SS3.SSS4.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.SSS4.p3.2.m2.1.1.3.1.cmml" xref="S2.SS3.SSS4.p3.2.m2.1.1.3">subscript</csymbol><ci id="S2.SS3.SSS4.p3.2.m2.1.1.3.2.cmml" xref="S2.SS3.SSS4.p3.2.m2.1.1.3.2">𝑇</ci><ci id="S2.SS3.SSS4.p3.2.m2.1.1.3.3.cmml" xref="S2.SS3.SSS4.p3.2.m2.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS4.p3.2.m2.1c">D_{T_{i}}</annotation></semantics></math> is its end-to-end delay as defined in Equation <a href="#S2.E2" title="In 2.3.2 E2E delay definition ‣ 2.3 Evaluation metrics and definitions ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <math id="S2.SS3.SSS4.p3.3.m3.1" class="ltx_Math" alttext="D_{t_{i}}" display="inline"><semantics id="S2.SS3.SSS4.p3.3.m3.1a"><msub id="S2.SS3.SSS4.p3.3.m3.1.1" xref="S2.SS3.SSS4.p3.3.m3.1.1.cmml"><mi id="S2.SS3.SSS4.p3.3.m3.1.1.2" xref="S2.SS3.SSS4.p3.3.m3.1.1.2.cmml">D</mi><msub id="S2.SS3.SSS4.p3.3.m3.1.1.3" xref="S2.SS3.SSS4.p3.3.m3.1.1.3.cmml"><mi id="S2.SS3.SSS4.p3.3.m3.1.1.3.2" xref="S2.SS3.SSS4.p3.3.m3.1.1.3.2.cmml">t</mi><mi id="S2.SS3.SSS4.p3.3.m3.1.1.3.3" xref="S2.SS3.SSS4.p3.3.m3.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS4.p3.3.m3.1b"><apply id="S2.SS3.SSS4.p3.3.m3.1.1.cmml" xref="S2.SS3.SSS4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS4.p3.3.m3.1.1.1.cmml" xref="S2.SS3.SSS4.p3.3.m3.1.1">subscript</csymbol><ci id="S2.SS3.SSS4.p3.3.m3.1.1.2.cmml" xref="S2.SS3.SSS4.p3.3.m3.1.1.2">𝐷</ci><apply id="S2.SS3.SSS4.p3.3.m3.1.1.3.cmml" xref="S2.SS3.SSS4.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.SSS4.p3.3.m3.1.1.3.1.cmml" xref="S2.SS3.SSS4.p3.3.m3.1.1.3">subscript</csymbol><ci id="S2.SS3.SSS4.p3.3.m3.1.1.3.2.cmml" xref="S2.SS3.SSS4.p3.3.m3.1.1.3.2">𝑡</ci><ci id="S2.SS3.SSS4.p3.3.m3.1.1.3.3.cmml" xref="S2.SS3.SSS4.p3.3.m3.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS4.p3.3.m3.1c">D_{t_{i}}</annotation></semantics></math> the transmission delay. <math id="S2.SS3.SSS4.p3.4.m4.1" class="ltx_Math" alttext="H_{i}" display="inline"><semantics id="S2.SS3.SSS4.p3.4.m4.1a"><msub id="S2.SS3.SSS4.p3.4.m4.1.1" xref="S2.SS3.SSS4.p3.4.m4.1.1.cmml"><mi id="S2.SS3.SSS4.p3.4.m4.1.1.2" xref="S2.SS3.SSS4.p3.4.m4.1.1.2.cmml">H</mi><mi id="S2.SS3.SSS4.p3.4.m4.1.1.3" xref="S2.SS3.SSS4.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS4.p3.4.m4.1b"><apply id="S2.SS3.SSS4.p3.4.m4.1.1.cmml" xref="S2.SS3.SSS4.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS4.p3.4.m4.1.1.1.cmml" xref="S2.SS3.SSS4.p3.4.m4.1.1">subscript</csymbol><ci id="S2.SS3.SSS4.p3.4.m4.1.1.2.cmml" xref="S2.SS3.SSS4.p3.4.m4.1.1.2">𝐻</ci><ci id="S2.SS3.SSS4.p3.4.m4.1.1.3.cmml" xref="S2.SS3.SSS4.p3.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS4.p3.4.m4.1c">H_{i}</annotation></semantics></math> is the hardware used to compute <math id="S2.SS3.SSS4.p3.5.m5.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S2.SS3.SSS4.p3.5.m5.1a"><msub id="S2.SS3.SSS4.p3.5.m5.1.1" xref="S2.SS3.SSS4.p3.5.m5.1.1.cmml"><mi id="S2.SS3.SSS4.p3.5.m5.1.1.2" xref="S2.SS3.SSS4.p3.5.m5.1.1.2.cmml">C</mi><mi id="S2.SS3.SSS4.p3.5.m5.1.1.3" xref="S2.SS3.SSS4.p3.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS4.p3.5.m5.1b"><apply id="S2.SS3.SSS4.p3.5.m5.1.1.cmml" xref="S2.SS3.SSS4.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS4.p3.5.m5.1.1.1.cmml" xref="S2.SS3.SSS4.p3.5.m5.1.1">subscript</csymbol><ci id="S2.SS3.SSS4.p3.5.m5.1.1.2.cmml" xref="S2.SS3.SSS4.p3.5.m5.1.1.2">𝐶</ci><ci id="S2.SS3.SSS4.p3.5.m5.1.1.3.cmml" xref="S2.SS3.SSS4.p3.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS4.p3.5.m5.1c">C_{i}</annotation></semantics></math>, which means that both combinations have to be computed using the same hardware.</p>
</div>
<div id="S2.SS3.SSS4.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS4.p4.1" class="ltx_p">Only a combination that has lower delay and lower WER is considered better, as depending on the use case requirements a certain combination can be over the maximum delay or the minimum quality.</p>
</div>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Output normalization</h3>

<div id="S2.SS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.p1.1" class="ltx_p">The transcriptions of the GigaSpeech library and the ones generated by Whisper have different formats. The following process was carried out to normalize both texts before comparing them with Jiwer.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para ltx_noindent">
<ul id="S2.I5" class="ltx_itemize">
<li id="S2.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i1.p1" class="ltx_para">
<p id="S2.I5.i1.p1.1" class="ltx_p">GigaSpeech’s transcriptions do not include punctuation symbols and instead use annotations such as &lt;PERIOD&gt; or &lt;COMMA&gt;. They were replaced by punctuation marks.</p>
</div>
</li>
<li id="S2.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i2.p1" class="ltx_para">
<p id="S2.I5.i2.p1.1" class="ltx_p">Different annotations are used in both transcriptions to indicate the presence of music or silence. They were all removed for the transcriptions.</p>
</div>
</li>
<li id="S2.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i3.p1" class="ltx_para">
<p id="S2.I5.i3.p1.1" class="ltx_p">Numbers appear in GigaSpeech as words, while Whisper uses numbers. All numbers where replaced by their textual version.</p>
</div>
</li>
<li id="S2.I5.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i4.p1" class="ltx_para">
<p id="S2.I5.i4.p1.1" class="ltx_p">Using Jiwer, English contractions were expanded.</p>
</div>
</li>
<li id="S2.I5.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i5.p1" class="ltx_para">
<p id="S2.I5.i5.p1.1" class="ltx_p">URL where present in most of the podcast due to sponsorships. Whispers transcriptions used symbols such as / or dots whereas GigaSpeechs transcriptions used text. Whispers symbols where replaced by their textual representation.</p>
</div>
</li>
<li id="S2.I5.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i6.p1" class="ltx_para">
<p id="S2.I5.i6.p1.1" class="ltx_p">Whisper adds the symbol ”♪” to the transcription when someone sings. This symbol was removed.</p>
</div>
</li>
<li id="S2.I5.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i7.p1" class="ltx_para ltx_noindent">
<p id="S2.I5.i7.p1.1" class="ltx_p">Multiple white spaces between words where removed.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS4.p3" class="ltx_para ltx_noindent">
<p id="S2.SS4.p3.1" class="ltx_p">Lastly, one of the audio files of the dataset was mostly in Spanish. The transcription associated with that file only contained the sentences that were spoken in English. Whisper did translate and transcribe the entire audio file, whereas the original transcription only included the English part, resulting in a high WER due to insertion errors. For this reason, this file was removed from the dataset.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para ltx_noindent">
<p id="S2.SS4.p4.1" class="ltx_p">As presented in Section <a href="#S2.SS3" title="2.3 Evaluation metrics and definitions ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>, the selected implementation of Whisper produces an artifact in which the same output is sometimes repeated during a silence. These repeated sentences were removed, even though they can be considered errors during the transcription process. The reason for removing them is that the measured parameters are heavily influenced by these repetitions. This affects the comparison between models, since the better one would be the one with fewer artifacts.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this section, the results obtained are presented. All algorithms presented in the previous section were tested with the three selected models. First, the resulting WER, MER and WIL are compared to the reference, which is the performance of the model with batch processing. This comparison is used to determine how each audio splitting algorithm affects the quality of the resulting transcription. Then the delay introduced by the different combinations of algorithms and model is compared. Finally, these combinations are evaluated taking both delay and quality into consideration.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model performance</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">The WER, WIL, and MER are measured for each model as proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. The results obtained for all experiments are collected in Table <a href="#S3.T4" title="Table 4 ‣ 3.1 Model performance ‣ 3 Results ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. However, with the available hardware, it was not possible to run the large model in real-time scenarios. The audio fragments were being sent faster than the model was able to process them, resulting in CPU saturation and the ASR cluster crashing. The performance of the large model in a batch transcription is taken as a reference for when total delay is not a consideration. As observed, the best WER, MER and WIL are obtained by the large model in batch mode.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>WER, WIL and MER of the different models and audio splitting algorithms</figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T4.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Model</td>
<td id="S3.T4.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Audio splitting algorithm</td>
<td id="S3.T4.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">WER</td>
<td id="S3.T4.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">MER</td>
<td id="S3.T4.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">WIL</td>
</tr>
<tr id="S3.T4.1.2" class="ltx_tr">
<td id="S3.T4.1.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Large</td>
<td id="S3.T4.1.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Batch</td>
<td id="S3.T4.1.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.1323</td>
<td id="S3.T4.1.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.1249</td>
<td id="S3.T4.1.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.1718</td>
</tr>
<tr id="S3.T4.1.3" class="ltx_tr">
<td id="S3.T4.1.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T4.1.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Batch</td>
<td id="S3.T4.1.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.1748</td>
<td id="S3.T4.1.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.1672</td>
<td id="S3.T4.1.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2326</td>
</tr>
<tr id="S3.T4.1.4" class="ltx_tr">
<td id="S3.T4.1.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T4.1.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3 second fragment</td>
<td id="S3.T4.1.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3050</td>
<td id="S3.T4.1.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2834</td>
<td id="S3.T4.1.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3928</td>
</tr>
<tr id="S3.T4.1.5" class="ltx_tr">
<td id="S3.T4.1.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T4.1.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2 second fragment</td>
<td id="S3.T4.1.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3458</td>
<td id="S3.T4.1.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3127</td>
<td id="S3.T4.1.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4390</td>
</tr>
<tr id="S3.T4.1.6" class="ltx_tr">
<td id="S3.T4.1.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T4.1.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">VAD</td>
<td id="S3.T4.1.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2551</td>
<td id="S3.T4.1.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2396</td>
<td id="S3.T4.1.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3358</td>
</tr>
<tr id="S3.T4.1.7" class="ltx_tr">
<td id="S3.T4.1.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T4.1.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Feedback</td>
<td id="S3.T4.1.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2908</td>
<td id="S3.T4.1.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2679</td>
<td id="S3.T4.1.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3516</td>
</tr>
<tr id="S3.T4.1.8" class="ltx_tr">
<td id="S3.T4.1.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S3.T4.1.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Batch</td>
<td id="S3.T4.1.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.1646</td>
<td id="S3.T4.1.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.1574</td>
<td id="S3.T4.1.8.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2126</td>
</tr>
<tr id="S3.T4.1.9" class="ltx_tr">
<td id="S3.T4.1.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S3.T4.1.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3 second fragment</td>
<td id="S3.T4.1.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2735</td>
<td id="S3.T4.1.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2544</td>
<td id="S3.T4.1.9.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3461</td>
</tr>
<tr id="S3.T4.1.10" class="ltx_tr">
<td id="S3.T4.1.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S3.T4.1.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2 second fragment</td>
<td id="S3.T4.1.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3386</td>
<td id="S3.T4.1.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.3089</td>
<td id="S3.T4.1.10.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4136</td>
</tr>
<tr id="S3.T4.1.11" class="ltx_tr">
<td id="S3.T4.1.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S3.T4.1.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">VAD</td>
<td id="S3.T4.1.11.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2304</td>
<td id="S3.T4.1.11.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.2304</td>
<td id="S3.T4.1.11.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.29705</td>
</tr>
<tr id="S3.T4.1.12" class="ltx_tr">
<td id="S3.T4.1.12.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S3.T4.1.12.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Feedback</td>
<td id="S3.T4.1.12.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.2536</td>
<td id="S3.T4.1.12.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.2301</td>
<td id="S3.T4.1.12.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">0.3058</td>
</tr>
</table>
</figure>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">Comparing models, for the batch processing, larger models provide better results. In real-time processing, using the same splitting algorithm, the best results are obtained with the base model, which is also the larger one. This is an expected result, as larger models are expected to generate better transcriptions.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p">Comparing algorithms, the best results for both the tiny and base models are obtained with the VAD algorithm, followed by the feedback algorithm. Even with the word correction performed in the feedback algorithm, the WER is 3,57% and 2,32% higher than with VAD splitting.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p">The fixed interval algorithm performs the worst of the three, as utterances can be separated into different fragments. Comparing the 2 second fixed splitting to VAD splitting there is an increase of 9,07% and 10,82% for the tiny and base models respectively. It can be observed that shorter split intervals generate worse results than larger intervals. Increasing the splitting interval from 2 to 3 seconds reduces the WER in 4,08% and 6,51% for the tiny and base models respectively. This is because with larger fragments, less fragments are generated, so fewer utterances can be incorrectly divided.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>End-to-end delay </h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">The end-to-end delay is measured using the proposed methodology. The aim of these measures is to determine the effects of the audio-splitting algorithms on the end-to-end delay.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">Whisper is designed to transcribe audio in 30 second windows. When transcribing shorter audios, the implementation used pads the audio with zeroes. Because of this, a first hypothesis was that <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="D_{p}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝐷</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">D_{p}</annotation></semantics></math> was independent of the audio duration if it was shorter than this 30 seconds, since it will be padded with zeros until it reaches 30 seconds. This hypothesis was tested by measuring the delay caused by audio segments of different duration. The results of this experiment are presented in Table <a href="#S3.T5" title="Table 5 ‣ 3.2 End-to-end delay ‣ 3 Results ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span> <math id="S3.T5.2.m1.1" class="ltx_Math" alttext="D_{p}" display="inline"><semantics id="S3.T5.2.m1.1b"><msub id="S3.T5.2.m1.1.1" xref="S3.T5.2.m1.1.1.cmml"><mi id="S3.T5.2.m1.1.1.2" xref="S3.T5.2.m1.1.1.2.cmml">D</mi><mi id="S3.T5.2.m1.1.1.3" xref="S3.T5.2.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T5.2.m1.1c"><apply id="S3.T5.2.m1.1.1.cmml" xref="S3.T5.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T5.2.m1.1.1.1.cmml" xref="S3.T5.2.m1.1.1">subscript</csymbol><ci id="S3.T5.2.m1.1.1.2.cmml" xref="S3.T5.2.m1.1.1.2">𝐷</ci><ci id="S3.T5.2.m1.1.1.3.cmml" xref="S3.T5.2.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.2.m1.1d">D_{p}</annotation></semantics></math> measurement of different segments duration</figcaption>
<table id="S3.T5.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T5.3.1" class="ltx_tr">
<td id="S3.T5.3.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Segment duration</td>
<td id="S3.T5.3.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Model</td>
<td id="S3.T5.3.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Processing delay (ms)</td>
</tr>
<tr id="S3.T5.3.2" class="ltx_tr">
<td id="S3.T5.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">2s</td>
<td id="S3.T5.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T5.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">503</td>
</tr>
<tr id="S3.T5.3.3" class="ltx_tr">
<td id="S3.T5.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">3s</td>
<td id="S3.T5.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T5.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">517</td>
</tr>
<tr id="S3.T5.3.4" class="ltx_tr">
<td id="S3.T5.3.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">5s</td>
<td id="S3.T5.3.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T5.3.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">548</td>
</tr>
<tr id="S3.T5.3.5" class="ltx_tr">
<td id="S3.T5.3.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">10s</td>
<td id="S3.T5.3.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T5.3.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">608</td>
</tr>
<tr id="S3.T5.3.6" class="ltx_tr">
<td id="S3.T5.3.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">2s</td>
<td id="S3.T5.3.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Base</td>
<td id="S3.T5.3.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1042</td>
</tr>
<tr id="S3.T5.3.7" class="ltx_tr">
<td id="S3.T5.3.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">3s</td>
<td id="S3.T5.3.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Base</td>
<td id="S3.T5.3.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1072</td>
</tr>
<tr id="S3.T5.3.8" class="ltx_tr">
<td id="S3.T5.3.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">5s</td>
<td id="S3.T5.3.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Base</td>
<td id="S3.T5.3.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1089</td>
</tr>
<tr id="S3.T5.3.9" class="ltx_tr">
<td id="S3.T5.3.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">10s</td>
<td id="S3.T5.3.9.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Base</td>
<td id="S3.T5.3.9.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">1132</td>
</tr>
</table>
</figure>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p">Results show that longer segments require more processing time, introducing a larger delay. This refutes the initial hypothesis of <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="D_{p}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝐷</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">D_{p}</annotation></semantics></math> being independent of the segment duration if it was shorter than 30 seconds. This affects the feedback algorithm, as storing larger segments will result in a higher delay because of the larger segment.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p">After that, the delay of the different combinations of models and audio-splitting techniques were measured. The results obtained are presented in Table <a href="#S3.T6" title="Table 6 ‣ 3.2 End-to-end delay ‣ 3 Results ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. For the batch scenarios, delay was not measured, as the definition provided in Section <a href="#S2.SS3.SSS2" title="2.3.2 E2E delay definition ‣ 2.3 Evaluation metrics and definitions ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.2</span></a> does not apply to this scenario.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p">The measures obtained will vary depending on the hardware. As stated in Equation <a href="#S2.E2" title="In 2.3.2 E2E delay definition ‣ 2.3 Evaluation metrics and definitions ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the total delay is the sum of the splitting delay, the processing delay and transmission delay. Because of this, with more computational resources, the processing time can be reduced.</p>
</div>
<figure id="S3.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Real-time delay of the different scenarios</figcaption>
<table id="S3.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T6.1.1" class="ltx_tr">
<td id="S3.T6.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Model</td>
<td id="S3.T6.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Audio Splitting</td>
<td id="S3.T6.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Delay(ms)</td>
<td id="S3.T6.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Nº Measures</td>
</tr>
<tr id="S3.T6.1.2" class="ltx_tr">
<td id="S3.T6.1.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T6.1.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3 seconds fragment</td>
<td id="S3.T6.1.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2244</td>
<td id="S3.T6.1.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">679</td>
</tr>
<tr id="S3.T6.1.3" class="ltx_tr">
<td id="S3.T6.1.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T6.1.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2 seconds fragment</td>
<td id="S3.T6.1.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1702</td>
<td id="S3.T6.1.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">369</td>
</tr>
<tr id="S3.T6.1.4" class="ltx_tr">
<td id="S3.T6.1.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T6.1.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">VAD</td>
<td id="S3.T6.1.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3521</td>
<td id="S3.T6.1.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1066</td>
</tr>
<tr id="S3.T6.1.5" class="ltx_tr">
<td id="S3.T6.1.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T6.1.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Feedback</td>
<td id="S3.T6.1.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2000</td>
<td id="S3.T6.1.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">367</td>
</tr>
<tr id="S3.T6.1.6" class="ltx_tr">
<td id="S3.T6.1.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S3.T6.1.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3 second fragment</td>
<td id="S3.T6.1.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2783</td>
<td id="S3.T6.1.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">744</td>
</tr>
<tr id="S3.T6.1.7" class="ltx_tr">
<td id="S3.T6.1.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S3.T6.1.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2 second fragment</td>
<td id="S3.T6.1.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2269</td>
<td id="S3.T6.1.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">391</td>
</tr>
<tr id="S3.T6.1.8" class="ltx_tr">
<td id="S3.T6.1.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S3.T6.1.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">VAD</td>
<td id="S3.T6.1.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">4483</td>
<td id="S3.T6.1.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1019</td>
</tr>
<tr id="S3.T6.1.9" class="ltx_tr">
<td id="S3.T6.1.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S3.T6.1.9.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Feedback</td>
<td id="S3.T6.1.9.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">2496</td>
<td id="S3.T6.1.9.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">419</td>
</tr>
</table>
</figure>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.1" class="ltx_p">The results show that using the same audio splitting algorithm, smaller models result in lower delays. This is an expected result, as smaller models require less computation time.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para ltx_noindent">
<p id="S3.SS2.p7.1" class="ltx_p">Comparing algorithms, the VAD algorithm introduces the highest delay of all models. This happens because audio samples are buffered until silence is found, which usually happens after a sentence ends. So, even though this algorithm generates better transcriptions, it introduces a significant delay for a real-time system.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para ltx_noindent">
<p id="S3.SS2.p8.2" class="ltx_p">For fixed audio-splitting measures, the minimum expected delay was half the segment duration. This is the average time since a word is pronounced until the fragment is sent for processing. All four measures taken comply with it. The difference in delay for 3 seconds and 2 seconds is higher than 0.5 seconds, half the difference in segment duration, because, as tested in the previous experiment, longer audio fragments take longer to process. So, even though larger audio fragments produce better transcriptions, they introduce a larger delay in both <math id="S3.SS2.p8.1.m1.1" class="ltx_Math" alttext="D_{p}" display="inline"><semantics id="S3.SS2.p8.1.m1.1a"><msub id="S3.SS2.p8.1.m1.1.1" xref="S3.SS2.p8.1.m1.1.1.cmml"><mi id="S3.SS2.p8.1.m1.1.1.2" xref="S3.SS2.p8.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS2.p8.1.m1.1.1.3" xref="S3.SS2.p8.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.1b"><apply id="S3.SS2.p8.1.m1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.1.m1.1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p8.1.m1.1.1.2.cmml" xref="S3.SS2.p8.1.m1.1.1.2">𝐷</ci><ci id="S3.SS2.p8.1.m1.1.1.3.cmml" xref="S3.SS2.p8.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.1c">D_{p}</annotation></semantics></math> and <math id="S3.SS2.p8.2.m2.1" class="ltx_Math" alttext="D_{s}" display="inline"><semantics id="S3.SS2.p8.2.m2.1a"><msub id="S3.SS2.p8.2.m2.1.1" xref="S3.SS2.p8.2.m2.1.1.cmml"><mi id="S3.SS2.p8.2.m2.1.1.2" xref="S3.SS2.p8.2.m2.1.1.2.cmml">D</mi><mi id="S3.SS2.p8.2.m2.1.1.3" xref="S3.SS2.p8.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.2.m2.1b"><apply id="S3.SS2.p8.2.m2.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.2.m2.1.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p8.2.m2.1.1.2.cmml" xref="S3.SS2.p8.2.m2.1.1.2">𝐷</ci><ci id="S3.SS2.p8.2.m2.1.1.3.cmml" xref="S3.SS2.p8.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.2.m2.1c">D_{s}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p9" class="ltx_para ltx_noindent">
<p id="S3.SS2.p9.1" class="ltx_p">The performance of the feedback algorithm is again placed in the middle of the other two. Even though audio fragments are sent at the same rate as in the 2 second fixed algorithm, due to the 4 seconds of feedback introduced, it takes longer to process the fragments. There is a significant decrease of 1.521s and 1.987s in delay in the feedback algorithm compared to the VAD algorithm for the tiny and base mode. As measured in the previous section, this is exchanged for a WER increase of 3,57% and 2,32%. In relative terms, this means that the feedback algorithm takes 43,46% and 44,33% less time than the VAD algorithm for an increase of 13,99% and 10,50% in the WER.</p>
</div>
<div id="S3.SS2.p10" class="ltx_para ltx_noindent">
<p id="S3.SS2.p10.1" class="ltx_p">The number of delay measures reported in Table <a href="#S3.T6" title="Table 6 ‣ 3.2 End-to-end delay ‣ 3 Results ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> varies for each experiment due the WER. In all experiments, the same words were searched in the same context. However, for each word searched, the algorithm has to find both the word in the search window and the word within the context. Because of this, a higher WER will reduce the number of words found, as there is no match between the original transcription and the one generated by Whisper. The number of measures is lower for the feedback algorithm compared to other algorithms despite its high WER, because segments are shorter as the overwritten part is removed. For this reason, fewer words are found in the search window.</p>
</div>
<div id="S3.SS2.p11" class="ltx_para ltx_noindent">
<p id="S3.SS2.p11.1" class="ltx_p">The details of the measurements taken are shown in Table <a href="#S3.T7" title="Table 7 ‣ 3.2 End-to-end delay ‣ 3 Results ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Words not found are the number of words that were not found in the search window. Context not found is the number of words that were located in the search window, but the entire context was not present. The words that were found with their context are reported in the context found column. Finally, some measures with negative delays were labeled as false positives and removed from the delay measures. These negative values are impossible, as a word cannot be processed before being spoken and are caused by common repeated expressions.</p>
</div>
<figure id="S3.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Detailed information about real-time delay measures</figcaption>
<table id="S3.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T7.1.1" class="ltx_tr">
<td id="S3.T7.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Model</td>
<td id="S3.T7.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Audio Splitting</td>
<td id="S3.T7.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Words searched</td>
<td id="S3.T7.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Word not found</td>
<td id="S3.T7.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Context not found</td>
<td id="S3.T7.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Context found</td>
<td id="S3.T7.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">False Positives</td>
</tr>
<tr id="S3.T7.1.2" class="ltx_tr">
<td id="S3.T7.1.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T7.1.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3 seconds</td>
<td id="S3.T7.1.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">16985</td>
<td id="S3.T7.1.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">4342</td>
<td id="S3.T7.1.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">11952</td>
<td id="S3.T7.1.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">691</td>
<td id="S3.T7.1.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">12</td>
</tr>
<tr id="S3.T7.1.3" class="ltx_tr">
<td id="S3.T7.1.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T7.1.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2 seconds</td>
<td id="S3.T7.1.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">16985</td>
<td id="S3.T7.1.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">5246</td>
<td id="S3.T7.1.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">11368</td>
<td id="S3.T7.1.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">372</td>
<td id="S3.T7.1.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3</td>
</tr>
<tr id="S3.T7.1.4" class="ltx_tr">
<td id="S3.T7.1.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T7.1.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">VAD</td>
<td id="S3.T7.1.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">16985</td>
<td id="S3.T7.1.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3754</td>
<td id="S3.T7.1.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">12152</td>
<td id="S3.T7.1.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1076</td>
<td id="S3.T7.1.4.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">10</td>
</tr>
<tr id="S3.T7.1.5" class="ltx_tr">
<td id="S3.T7.1.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tiny</td>
<td id="S3.T7.1.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Feedback</td>
<td id="S3.T7.1.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">16985</td>
<td id="S3.T7.1.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">6694</td>
<td id="S3.T7.1.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">9923</td>
<td id="S3.T7.1.5.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">368</td>
<td id="S3.T7.1.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1</td>
</tr>
<tr id="S3.T7.1.6" class="ltx_tr">
<td id="S3.T7.1.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S3.T7.1.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3 second</td>
<td id="S3.T7.1.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">16985</td>
<td id="S3.T7.1.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">4480</td>
<td id="S3.T7.1.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">11753</td>
<td id="S3.T7.1.6.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">752</td>
<td id="S3.T7.1.6.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">8</td>
</tr>
<tr id="S3.T7.1.7" class="ltx_tr">
<td id="S3.T7.1.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S3.T7.1.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2 second</td>
<td id="S3.T7.1.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">16985</td>
<td id="S3.T7.1.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">5321</td>
<td id="S3.T7.1.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">11271</td>
<td id="S3.T7.1.7.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">393</td>
<td id="S3.T7.1.7.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
</tr>
<tr id="S3.T7.1.8" class="ltx_tr">
<td id="S3.T7.1.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S3.T7.1.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">VAD</td>
<td id="S3.T7.1.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">16985</td>
<td id="S3.T7.1.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3813</td>
<td id="S3.T7.1.8.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">12144</td>
<td id="S3.T7.1.8.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1028</td>
<td id="S3.T7.1.8.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">9</td>
</tr>
<tr id="S3.T7.1.9" class="ltx_tr">
<td id="S3.T7.1.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Base</td>
<td id="S3.T7.1.9.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Feedback</td>
<td id="S3.T7.1.9.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">16985</td>
<td id="S3.T7.1.9.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">6824</td>
<td id="S3.T7.1.9.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">9737</td>
<td id="S3.T7.1.9.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">424</td>
<td id="S3.T7.1.9.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">5</td>
</tr>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Quality-delay matrix</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">In this 2-dimensional matrix, we aim to classify models based on their transcription quality and end-to-end delay.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">This matrix helps to visualize which algorithms are better than others by the definition provided in Section <a href="#S2.SS3.SSS4" title="2.3.4 Algorithms and model comparison ‣ 2.3 Evaluation metrics and definitions ‣ 2 Testing methodology ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.4</span></a>, as they are the ones whose coordinates are lower in both axes.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2409.05674/assets/figures/output.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="211" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>WER-Delay matrix</figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p">All combinations tested are represented in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.3 Quality-delay matrix ‣ 3 Results ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. In the matrix it can be observed that by our definition the combination of 3 second fixed splitting with the tiny model is worse than the combination of the tiny model with the feedback algorithm. Also, the feedback algorithm with the base model is better than both the VAD algorithm with the tiny model and the 3 second fixed splitting with the base model.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p">By our definition, there is no other combination that can be considered better than other. For example, the combination of 2 second fixed splitting with the tiny model has a substantially higher WER than the rest but has the lowest delay, so depending on the application requirements it can be the best suited.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.2" class="ltx_p">Depending on the hardware, all these combinations can move on the delay axis. So, an algorithm with low WER that is not suitable for real-time with a certain CPU, could be used with GPU acceleration. There is a maximum reduction in delay that can be achieved by improving the hardware as <math id="S3.SS3.p5.1.m1.1" class="ltx_Math" alttext="D_{p}" display="inline"><semantics id="S3.SS3.p5.1.m1.1a"><msub id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml"><mi id="S3.SS3.p5.1.m1.1.1.2" xref="S3.SS3.p5.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS3.p5.1.m1.1.1.3" xref="S3.SS3.p5.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><apply id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p5.1.m1.1.1.2.cmml" xref="S3.SS3.p5.1.m1.1.1.2">𝐷</ci><ci id="S3.SS3.p5.1.m1.1.1.3.cmml" xref="S3.SS3.p5.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">D_{p}</annotation></semantics></math> is only one part of <math id="S3.SS3.p5.2.m2.1" class="ltx_Math" alttext="D_{T}" display="inline"><semantics id="S3.SS3.p5.2.m2.1a"><msub id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml"><mi id="S3.SS3.p5.2.m2.1.1.2" xref="S3.SS3.p5.2.m2.1.1.2.cmml">D</mi><mi id="S3.SS3.p5.2.m2.1.1.3" xref="S3.SS3.p5.2.m2.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><apply id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.2.m2.1.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p5.2.m2.1.1.2.cmml" xref="S3.SS3.p5.2.m2.1.1.2">𝐷</ci><ci id="S3.SS3.p5.2.m2.1.1.3.cmml" xref="S3.SS3.p5.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">D_{T}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion and future work</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In this paper, different algorithms for generating real-time transcriptions have been tested. Transcription quality and end-to-end delay have been measured for the different combinations of models and algorithms to determine their viability.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p">The conclusions obtained are that larger models introduce a higher delay than smaller models in exchange for a lower WER, MER and WIL. Using the same model, VAD splitting was the best performing algorithm as it does not split utterances into different fragments. The resulting WER, MER and WIL were very similar to the ones obtained by the batch scenario. However, they introduce a significant delay of 3.5 and 4.4 seconds for the tiny and base model respectively.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p">The fixed interval algorithm introduces a much lower delay than the VAD algorithm in exchange for a much lower quality due to utterances being divided into different fragments. Shorter fragmentation intervals lead to lower delays and lower quality due to more utterances being divided.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p">The introduction of feedback from previous segments reduces the WER, WIL and MER of the fixed interval algorithm in exchange for a higher delay. However, the resulting delay is significantly lower than the VAD algorithm.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p">In conclusion, different algorithms for splitting audio have been tested to generate transcriptions in real time. Depending on the requirements of quality and delay some algorithms perform better than others. Fixed audio splitting has the lowest quality and delay whereas VAD based splitting have the highest quality and delay. Feedback based algorithms have both a higher delay than fixed splitting and lower quality than VAD based splitting. However, compared to the VAD algorithm, they provide a significant decrease in delay of 1.521 and 1.987 seconds compared to the relatively lower decrease in quality of 0 3.57% and 2.32% for the tiny and base model respectively.</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p">Finally, the tested model and algorithms have a significant increase in WER, MER and WIL compared to batch processing. The best result of 0.2304 WER for the base model with VAD splitting is significantly worse than the 0.1748 achieved for the batch scenario with the tiny model. For this reason, further research regarding audio splitting and real-time models have to be performed to enable the use of this technology in real-word applications. This can partially be caused by the AudioExtractor, as it is resampling the original audio. For this reason, the effects of this component on the resulting transcription quality must be investigated.</p>
</div>
<div id="S4.p7" class="ltx_para ltx_noindent">
<p id="S4.p7.1" class="ltx_p">To reduce the amount of computation power required by the service provider, further research can be performed to move the computation to the client. With WebAssembly<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, ASR models and systems could be embedded in the browser to locally generate transcriptions, generating a distributed ASR system.</p>
</div>
<div id="S4.p8" class="ltx_para ltx_noindent">
<p id="S4.p8.1" class="ltx_p">In addition, Whisper’s architecture has been designed to work with audio fragments of 30 seconds length. The usage of models designed to work with smaller fragments or utterances could improve the WER of the system and reduce CPU/GPU usage.</p>
</div>
<div id="S4.p9" class="ltx_para ltx_noindent">
<p id="S4.p9.1" class="ltx_p">For future work, this proposed architecture and implementation must be integrated into different applications to achieve the objectives listed in Section <a href="#S1" title="1 Introduction ‣ Evaluation of real-time transcriptions using end-to-end ASR models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. This includes investigating its integration with technologies such as WebRTC<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> for videoconferencing.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K. Davis, R. Biddulph, and S. Balashek, “Automatic recognition of spoken digits,” <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Journal of the Acoustical Society of America</span>, vol. 24, pp. 637–642, 1952.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. A. Talib, S. Majzoub, Q. Nasir, and D. Jamal, “A systematic literature review on hardware implementation of artificial intelligence algorithms,” <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">The Journal of Supercomputing</span>, vol. 77, no. 2, pp. 1897–1938, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B. Juang and L. Rabiner, “Automatic speech recognition - a brief history of the technology development,” 01 2005.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
K. S and E. Chandra, “A review on automatic speech recognition architecture and approaches,” <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">International Journal of Signal Processing, Image Processing and Pattern Recognition</span>, vol. 9, pp. 393–404, 04 2016.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
R. Prabhavalkar, T. Hori, T. N. Sainath, R. Schlüter, and S. Watanabe, “End-to-end speech recognition: A survey,” <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Li <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Recent advances in end-to-end automatic speech recognition,” <span id="bib.bib6.2.2" class="ltx_text ltx_font_italic">APSIPA Transactions on Signal and Information Processing</span>, vol. 11, no. 1, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
G. Chen, S. Chai, G. Wang, J. Du, W.-Q. Zhang, C. Weng, D. Su, D. Povey, J. Trmal, J. Zhang, M. Jin, S. Khudanpur, S. Watanabe, S. Zhao, W. Zou, X. Li, X. Yao, Y. Wang, Y. Wang, Z. You, and Z. Yan, “Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio,” in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2021</span>, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
D. Galvez, G. Diamos, J. Ciro, J. F. Cerón, K. Achorn, A. Gopi, D. Kanter, M. Lam, M. Mazumder, and V. J. Reddi, “The people’s speech: A large-scale diverse english speech recognition dataset for commercial usage,” <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2111.09344</span>, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust speech recognition via large-scale weak supervision,” in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pp. 28492–28518, PMLR, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Baevski, W.-N. Hsu, A. CONNEAU, and M. Auli, “Unsupervised speech recognition,” in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span> (M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), vol. 34, pp. 27826–27839, Curran Associates, Inc., 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. Bucholtz, “The politics of transcription,” <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Journal of Pragmatics</span>, vol. 32, no. 10, pp. 1439–1465, 2000.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Zeyer, P. Bahar, K. Irie, R. Schlüter, and H. Ney, “A comparison of transformer and lstm encoder decoder models for asr,” in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</span>, pp. 8–15, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 33, pp. 12449–12460, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely, “The kaldi speech recognition toolkit,” in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">IEEE 2011 Workshop on Automatic Speech Recognition and Understanding</span>, IEEE Signal Processing Society, Dec. 2011.

</span>
<span class="ltx_bibblock">IEEE Catalog No.: CFP11SRW-USB.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
B. Li, S.-y. Chang, T. N. Sainath, R. Pang, Y. He, T. Strohman, and Y. Wu, “Towards fast and accurate streaming end-to-end asr,” in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pp. 6069–6073, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S.-Y. Chang, R. Prabhavalkar, Y. He, T. N. Sainath, and G. Simko, “Joint endpointing and decoding with end-to-end models,” in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pp. 5626–5630, IEEE, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
W. R. Huang, S.-Y. Chang, T. N. Sainath, Y. He, D. Rybach, R. David, R. Prabhavalkar, C. Allauzen, C. Peyser, and T. D. Strohman, “E2e segmentation in a two-pass cascaded encoder asr model,” in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pp. 1–5, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
R. Ranchal, T. Taber-Doughty, Y. Guo, K. Bain, H. Martin, J. Paul Robinson, and B. S. Duerstock, “Using speech recognition for real-time captioning and lecture transcription in the classroom,” <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Learning Technologies</span>, vol. 6, no. 4, pp. 299–311, 2013.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
E. Nacimiento-García, C. S. González-González, and F. L. Gutiérrez-Vela, “Automatic captions on video calls: a must for the older adults,” <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Universal Access in the Information Society</span>, vol. 23, no. 1, pp. 75–98, 2024.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
C. S. Leow, T. Hayakawa, H. Nishizaki, and N. Kitaoka, “Development of a low-latency and real-time automatic speech recognition system,” in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">2020 IEEE 9th Global Conference on Consumer Electronics (GCCE)</span>, pp. 925–928, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
F. M. Rodrigues, A. M. Abreu, I. Holmström, and A. Mineiro, “E-learning is a burden for the deaf and hard of hearing,” <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Scientific Reports</span>, vol. 12, no. 1, p. 9346, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
L. B. Elliot, M. S. Stinson, B. G. McKee, V. S. Everhart, and P. J. Francis, “College students’ perceptions of the c-print speech-to-text transcription system,” <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Journal of deaf studies and deaf education</span>, vol. 6, no. 4, pp. 285–298, 2001.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y. Shu, H. Luo, S. Zhang, L. Wang, and J. Dang, “A cif-based speech segmentation method for streaming e2e asr,” <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Letters</span>, vol. 30, pp. 344–348, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. Ren, J. Liu, X. Tan, C. Zhang, T. Qin, Z. Zhao, and T.-Y. Liu, “Simulspeech: End-to-end simultaneous speech to text translation,” in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</span>, pp. 3787–3796, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
C. Jennings, B. Aboba, J.-I. Bruaroey, H. Boström, and Y. Fablet, “Media capture and streams,” W3C candidate recommendation, W3C, May 2024.

</span>
<span class="ltx_bibblock">https://www.w3.org/TR/mediacapture-streams/.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
P. Adenot and H. Choi, “Web audio api,” W3C recommendation, W3C, June 2021.

</span>
<span class="ltx_bibblock">https://www.w3.org/TR/webaudio/.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
W3C, “Websockets,” W3C living standard, W3C, Jan. 2024.

</span>
<span class="ltx_bibblock">https://websockets.spec.whatwg.org/.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
H. Schulzrinne, S. L. Casner, R. Frederick, and V. Jacobson, “RTP: A Transport Protocol for Real-Time Applications.” RFC 3550, July 2003.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
H. T. Alvestrand, “Overview: Real-Time Protocols for Browser-Based Applications.” RFC 8825, Jan. 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Web Hypertext Application Technology Working Group, “HTML,” Living Standard, Web Hypertext Application Technology Working Group, July 2024.

</span>
<span class="ltx_bibblock">https://html.spec.whatwg.org/.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. C. Morris, V. Maier, and P. Green, “From wer and ril to mer and wil: improved evaluation measures for connected speech recognition,” in <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Eighth International Conference on Spoken Language Processing</span>, 2004.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
W3C, “Webassembly core specification,” W3C working draft, W3C, Apr. 2024.

</span>
<span class="ltx_bibblock">https://www.w3.org/TR/wasm-core-2/.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.05673" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.05674" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.05674">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.05674" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.05675" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 00:49:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
