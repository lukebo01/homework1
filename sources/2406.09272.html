<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.09272] Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos</title><meta property="og:description" content="Generating realistic audio for human interactions
is important for many applications, such as creating sound effects for films or virtual reality games.
Existing approaches implicitly assume total correspondence betwee…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.09272">

<!--Generated on Fri Jul  5 19:41:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="audio-visual learning egocentric video understanding action sound generation">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">(eccv)                Package eccv Warning: Package ‘hyperref’ is loaded with option ‘pagebackref’, which is *not* recommended for camera-ready version</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>
University of Texas at Austin </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>FAIR, Meta
</span></span></span>
<h1 class="ltx_title ltx_title_document">Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Changan Chen<sup id="id3.2.id1" class="ltx_sup"><span id="id3.2.id1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Puyuan Peng<sup id="id4.2.id1" class="ltx_sup"><span id="id4.2.id1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ami Baid
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zihui Xue
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Wei-Ning Hsu
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Harwath
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kristen Grauman
</span><span class="ltx_author_notes">11</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Generating realistic audio for human interactions
is important for many applications, such as creating sound effects for films or virtual reality games.
Existing approaches implicitly assume total correspondence between the video and audio during training, yet many sounds happen off-screen and have weak to no correspondence with the visuals—resulting in uncontrolled ambient sounds or hallucinations at test time.
We propose a novel <em id="id5.id1.1" class="ltx_emph ltx_font_italic">ambient-aware</em> audio generation model, AV-LDM. We devise a novel audio-conditioning mechanism to learn to disentangle foreground action sounds from the ambient background sounds in in-the-wild training videos. Given a novel silent video, our model uses retrieval-augmented generation to create audio that matches the visual content both semantically and temporally.
We train and evaluate our model on two in-the-wild egocentric video datasets Ego4D and EPIC-KITCHENS.
Our model outperforms an array of existing methods,
allows controllable generation of the ambient sound, and even shows promise for generalizing to computer graphics game clips.
Overall, our work is the first to focus video-to-audio generation faithfully on the observed visual content despite training from uncurated clips with natural background sounds.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>audio-visual learning egocentric video understanding action sound generation
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">${}^{*}$</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">${}^{*}$</sup><span class="ltx_note_type">footnotetext: </span>indicates equal contribution.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">We interact with objects around us in our daily lives and these actions often produce sound as a result of physical interactions, e.g., clicking on a mouse, closing a door, or cutting vegetables. The distinct characteristics of these <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">action sounds</em> depend upon the type of action being performed, the shape and material composition of the objects being acted upon, the amount of force being applied, and so forth.
Vision not only captures <em id="S1.p1.1.2" class="ltx_emph ltx_font_italic">what</em> physical interaction happens but also informs us <em id="S1.p1.1.3" class="ltx_emph ltx_font_italic">when</em> the interaction happens, suggesting the possiblity of synthesizing semantically plausible and temporally synchronous action sounds from silent videos alone. This capability would accelerate many real-world applications, such as text-to-video generation, generating sound effects for films (Foley), or sound effect generation for virtual reality (VR) and video games.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.09272/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Real-world audio consists of both foreground action sounds (whose causes are visible in the FoV) and background ambient sounds that are generated by sources offscreen. Whereas prior work is agnostic to this division when performing generation,
our method is ambient-aware and disentangles action sound from ambient sound. Our key technical insight is how to train with in-the-wild videos exhibiting natural ambient sounds, while still learning to factor out their effects on generation. The green arrows reference how we condition generation on sound from a related, but time-distinct, video clip to achieve this.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Some prior work studies impact sound synthesis from videos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> while others target more general video-to-audio generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
All these methods <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">implicitly assume total correspondence between the video and audio</em> and aim to generate the whole target audio from the video. However, this strategy falls short
for in-the-wild training videos, which are rife with off-screen ambient sounds, e.g.,
traffic noise,
people talking, or A/C running. While some of these ambient sounds are weakly correlated with the visual scene, such as the wind blowing in an outdoor environment, many of them have no visual correspondence, such as off-screen speech or a stationary buzzing noise from the fridge.
Existing methods are not able to disentangle action sounds from ambient sounds and treat them as a whole,
leading to uncontrolled generation of ambient sounds at test time and sometimes even hallucination, e.g., random action or ambient sounds. This is particularly problematic for generating action sounds because they are often subtle and transient compared to the ambient sounds. For example, trained in the traditional way, a model given a scene that looks like a noisy restaurant risks generating “restaurant-like" ambient sounds, while ignoring the actual movements and activities of the foreground actions, such as a person stirring their coffee with a metal spoon.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">How can we disentangle the foreground action sounds from background ambient sounds for in-the-wild video data <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">without</em> ground truth separated streams?
Simply applying a noise removal algorithm on the target audio does not work well since in-the-wild blind source separation of general sounds from a single microphone is still an open challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.
The key observation we have is that while action sounds are highly localized in time, ambient sounds tend to persist across time.
Given this observation, we propose a simple but effective solution to disentangle ambient and action sounds: during training, in addition to the input video clip, we also condition the generation model on an audio clip from the same long video as the input video clip but from different timestamps. See <a href="#S1.F1" title="In 1 Introduction ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>. By doing so, we lift the burden of generating energy-dominating ambient sounds and encourage the model to focus on learning action cues from the visual frames to generate action sounds.
At test time, we do not assume access to (even other clips of) the ground truth video/audio. Instead, we propose to retrieve an audio segment from the training set with an audio-visual similarity scoring model, inspired by recent ideas in retrieval-augmented generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. This benefits examples where the visual scene has a weak correlation with the ambient sound that is appealing to capture, e.g., outdoor environments.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Existing action sound generation work relies on either clean, manually-collected data that has a limited number of action categories <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, or videos crawled from YouTube based on predefined taxonomies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
To expand the boundary of action sound generation to in-the-wild human actions, we
take advantage of recent large-scale egocentric video datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Though our model is not tailored to egocentric video in any way, there are two main benefits of using these datasets: 1) egocentric videos provide a close view of human actions compared to exocentric videos, where hand-object interactions are much smaller from a distance and often occluded, and 2) these datasets have timestamped narrations describing atomic actions. We design an automatic pipeline to extract and process clips from Ego4D, and curate Ego4D-Sounds with 1.2 million audio-visual
action clips.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our idea of disentangling action and ambient sounds implicitly in training is model-agnostic. In this paper, we instantiate it by designing an
audio-visual latent diffusion model (AV-LDM) that conditions on both modality streams for audio generation. We evaluate our AV-LDM against recent works on a wide variety of metrics and show that our model outperforms the existing methods significantly on both Ego4D-Sounds and EPIC-KITCHENS. We conduct a human evaluation study that shows our model synthesizes plausible action sounds according to the video. <span id="S1.p5.1.1" class="ltx_text ltx_font_bold">Please see/listen for yourself in our supplementary video!</span>
We also show promising preliminary results on virtual reality game clips.
To the best of our knowledge, this is the first work that demonstrates the disentanglement of foreground action sounds from background sounds for action-to-sound generation on in-the-wild videos.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Action Sound Generation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">A pioneering work <span id="S2.SS1.p1.1.1" class="ltx_text" style="color:#000000;">for capturing human-generated action sounds</span> collects videos where people hit, scratch, or prod objects with a drumstick <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. <span id="S2.SS1.p1.1.2" class="ltx_text" style="color:#000000;">This is an early inspirational effort, though it</span> is by design limited in the type of actions. The robotics community also studies this problem by using robotic platforms to collect collision sounds and analyze or synthesize them from video <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Other works approach this problem by building a simulation for collision events <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>; however, it is hard for computational approaches to simulate the impact or <span id="S2.SS1.p1.1.3" class="ltx_text" style="color:#000000;">general</span> action sounds due to the complexity of the physical interactions.
<span id="S2.SS1.p1.1.4" class="ltx_text" style="color:#000000;">Existing methods demonstrate good synthesis results when the data are noise-free. However, they are not equipped to learn from in-the-wild action videos, where the action sound is always coupled with ambient sound.
We propose an ambient-aware model to deal with this issue head-on and also introduce the Ego4D-Sounds dataset to expand action sound synthesis to in-the-wild actions.
</span></p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Egocentric Video Understanding <span id="S2.SS2.1.1" class="ltx_text" style="color:#000000;">with Audio</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Understanding human activities in videos has long been a core challenge of computer vision. Early research studies activity recognition from exocentric videos such as UCF101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, Kinetics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, or ActivityNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Recent work <span id="S2.SS2.p1.1.1" class="ltx_text" style="color:#000000;">explores the egocentric setting and introduces</span>
large egocentric datasets such as Ego4D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> or EPIC-KITCHENS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Leveraging both the video and audio streams in egocentric videos, many
interesting tasks are enhanced, such as action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, localization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, active speaker localization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, sounding object localization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>,
and state-aware visual representations from audible interactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.
Most related to our work is SoundingActions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> that learns visual representations of actions that make sounds, <span id="S2.SS2.p1.1.2" class="ltx_text" style="color:#000000;">which is valuable for indexing and recognition problem settings, but ill-equipped for generation, as we show later when retrieval models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> perform poorly for generating synchronized audio.</span>
All existing audio-visual learning for egocentric video focuses on perception, i.e., understanding what happens in the video. In contrast, we target the video-to-audio generation problem. <span id="S2.SS2.p1.1.3" class="ltx_text" style="color:#000000;">Furthermore, relative to any of the above, our idea to implicitly learn to disentangle the action sound from ambient sounds is novel.</span></p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Diffusion Models and Conditional Audio Generation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Diffusion models have attracted significant attention recently because of their high fidelity generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. Initially proposed for image generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, dthey have also been successfully applied to speech and audio generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Benefitting from classifier-free guidance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and large-scale representation learning, AudioLDM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and Make-An-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> have demonstrated successful diffusion-based <em id="S2.SS3.p1.1.1" class="ltx_emph ltx_font_italic">text</em>-to-audio generation.
More recently, Diff-Foley <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> adapts latent diffusion models for video-to-audio generation by first conducting audio-video contrastive learning and then video-conditioned audio generation. While this approach demonstrates promising results, it does not <span id="S2.SS3.p1.1.2" class="ltx_text" style="color:#000000;">address</span> the background ambient sound problem.
Inspired by recent work on retrieval-augmented generation (RAG) for text  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and image generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, <span id="S2.SS3.p1.1.3" class="ltx_text" style="color:#000000;">we show how our audio-conditioning insight carries over to inference time via a retrieval component of the model.</span>
Conditional video-to-audio generation conditions on either a physics prior to guide diffusion-based impact sound generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> or, in CondFoleyGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, another <span id="S2.SS3.p1.1.4" class="ltx_text" style="color:#000000;">video clip</span>
to
modify the characteristics of the action sound. Our method also considers
additional conditioning signals to control the output, <span id="S2.SS3.p1.1.5" class="ltx_text" style="color:#000000;">but for a very different purpose; our model
is the first to
address foreground/background sound disentanglement in generation.</span></p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Ambient-aware Action Sound Generation</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2406.09272/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="78" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Illustration of the harm of ambient sound in video-to-audio generation.
In this example, this person is closing a packet of ginger powder, which makes some rustling sound (red circled in the middle). There is also some buzzing sound semantically irrelevant to the visual scene in the background, which dominates the energy of the spectrogram. On the right-hand side, we show a prediction made by a vanilla model that misses the action sound but predicts the ambient sound.</span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We first discuss our high-level idea of how to guide the generation model to disentangle action sounds from ambient sounds. We then extend the latent diffusion models (LDM) to accommodate both audio and video conditions, which we name AV-LDM. We also discuss our pretraining stage.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Action-to-Sound Generation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.6" class="ltx_p">Given a video <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="V\in\mathbb{R}^{(T*S_{V})\times H\times W\times 3}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.2" xref="S3.SS1.p1.1.m1.1.2.cmml"><mi id="S3.SS1.p1.1.m1.1.2.2" xref="S3.SS1.p1.1.m1.1.2.2.cmml">V</mi><mo id="S3.SS1.p1.1.m1.1.2.1" xref="S3.SS1.p1.1.m1.1.2.1.cmml">∈</mo><msup id="S3.SS1.p1.1.m1.1.2.3" xref="S3.SS1.p1.1.m1.1.2.3.cmml"><mi id="S3.SS1.p1.1.m1.1.2.3.2" xref="S3.SS1.p1.1.m1.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.1.m1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml">∗</mo><msub id="S3.SS1.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.3.2.cmml">S</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.3.3.cmml">V</mi></msub></mrow><mo rspace="0.055em" stretchy="false" id="S3.SS1.p1.1.m1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.2.cmml">×</mo><mi id="S3.SS1.p1.1.m1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1.2a" xref="S3.SS1.p1.1.m1.1.1.1.2.cmml">×</mo><mi id="S3.SS1.p1.1.m1.1.1.1.4" xref="S3.SS1.p1.1.m1.1.1.1.4.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1.2b" xref="S3.SS1.p1.1.m1.1.1.1.2.cmml">×</mo><mn id="S3.SS1.p1.1.m1.1.1.1.5" xref="S3.SS1.p1.1.m1.1.1.1.5.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.2"><in id="S3.SS1.p1.1.m1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.2.1"></in><ci id="S3.SS1.p1.1.m1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.2.2">𝑉</ci><apply id="S3.SS1.p1.1.m1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.2.3.1.cmml" xref="S3.SS1.p1.1.m1.1.2.3">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.2.3.2.cmml" xref="S3.SS1.p1.1.m1.1.2.3.2">ℝ</ci><apply id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.2"></times><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1"></times><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.2">𝑇</ci><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.3.2">𝑆</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.3.3">𝑉</ci></apply></apply><ci id="S3.SS1.p1.1.m1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.3">𝐻</ci><ci id="S3.SS1.p1.1.m1.1.1.1.4.cmml" xref="S3.SS1.p1.1.m1.1.1.1.4">𝑊</ci><cn type="integer" id="S3.SS1.p1.1.m1.1.1.1.5.cmml" xref="S3.SS1.p1.1.m1.1.1.1.5">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">V\in\mathbb{R}^{(T*S_{V})\times H\times W\times 3}</annotation></semantics></math>, where <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">T</annotation></semantics></math> is the duration of the video and <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="S_{V}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">S</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">V</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑆</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">S_{V}</annotation></semantics></math> is the video sample rate, and the accompanying audio waveform <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="A\in\mathbb{R}^{1\times(T*S_{A})}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.2" xref="S3.SS1.p1.4.m4.1.2.cmml"><mi id="S3.SS1.p1.4.m4.1.2.2" xref="S3.SS1.p1.4.m4.1.2.2.cmml">A</mi><mo id="S3.SS1.p1.4.m4.1.2.1" xref="S3.SS1.p1.4.m4.1.2.1.cmml">∈</mo><msup id="S3.SS1.p1.4.m4.1.2.3" xref="S3.SS1.p1.4.m4.1.2.3.cmml"><mi id="S3.SS1.p1.4.m4.1.2.3.2" xref="S3.SS1.p1.4.m4.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml"><mn id="S3.SS1.p1.4.m4.1.1.1.3" xref="S3.SS1.p1.4.m4.1.1.1.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.4.m4.1.1.1.2" xref="S3.SS1.p1.4.m4.1.1.1.2.cmml">×</mo><mrow id="S3.SS1.p1.4.m4.1.1.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.4.m4.1.1.1.1.1.2" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.4.m4.1.1.1.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.1.1.1.1.2" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.4.m4.1.1.1.1.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.1.cmml">∗</mo><msub id="S3.SS1.p1.4.m4.1.1.1.1.1.1.3" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.2.cmml">S</mi><mi id="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.3.cmml">A</mi></msub></mrow><mo stretchy="false" id="S3.SS1.p1.4.m4.1.1.1.1.1.3" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.2.cmml" xref="S3.SS1.p1.4.m4.1.2"><in id="S3.SS1.p1.4.m4.1.2.1.cmml" xref="S3.SS1.p1.4.m4.1.2.1"></in><ci id="S3.SS1.p1.4.m4.1.2.2.cmml" xref="S3.SS1.p1.4.m4.1.2.2">𝐴</ci><apply id="S3.SS1.p1.4.m4.1.2.3.cmml" xref="S3.SS1.p1.4.m4.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.2.3.1.cmml" xref="S3.SS1.p1.4.m4.1.2.3">superscript</csymbol><ci id="S3.SS1.p1.4.m4.1.2.3.2.cmml" xref="S3.SS1.p1.4.m4.1.2.3.2">ℝ</ci><apply id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"><times id="S3.SS1.p1.4.m4.1.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.1.2"></times><cn type="integer" id="S3.SS1.p1.4.m4.1.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.1.3">1</cn><apply id="S3.SS1.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1"><times id="S3.SS1.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.1"></times><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.2">𝑇</ci><apply id="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.2">𝑆</ci><ci id="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.4.m4.1.1.1.1.1.1.3.3">𝐴</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">A\in\mathbb{R}^{1\times(T*S_{A})}</annotation></semantics></math>, where <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="S_{A}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">S</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝑆</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">S_{A}</annotation></semantics></math> is the audio sample rate,
our goal is to model the conditional distribution <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="p(A|V)" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">​</mo><mrow id="S3.SS1.p1.6.m6.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.6.m6.1.1.1.1.2" xref="S3.SS1.p1.6.m6.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.6.m6.1.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.1.1.1.2" xref="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.SS1.p1.6.m6.1.1.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.p1.6.m6.1.1.1.1.1.3" xref="S3.SS1.p1.6.m6.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.SS1.p1.6.m6.1.1.1.1.3" xref="S3.SS1.p1.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><times id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2"></times><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">𝑝</ci><apply id="S3.SS1.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.2">𝐴</ci><ci id="S3.SS1.p1.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1.1.3">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">p(A|V)</annotation></semantics></math> for video-to-audio generation.
During training we observe natural video coupled with its audio, whereas at inference time we have only a silent video—e.g., could be an output from text-to-video generation, or a VR/video game clip, or simply a real-world video for which we want to generate new plausible sounds.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Disentangling Action and Ambient Sounds</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Learning a video-to-audio generation model using in-the-wild egocentric videos is challenging because of entangled foreground action and background ambient sounds, as illustrated in <a href="#S3.F2" title="In 3 Ambient-aware Action Sound Generation ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. More specifically, the reasons are two-fold: 1) while action sounds are usually of very short duration, ambient sounds can last the entire clip, and therefore dominate the loss, leading to low-quality action sound generation; 2) while some ambient sounds might be semantically related to the visual scene such as bird chirping in the woods, in many cases, ambient sounds are difficult to infer from the visual scene because they are the results of the use of certain microphones, recording conditions, people speaking, off-screen actions, etc. Forcing a generation model to learn those background sounds from video results in hallucinations during inference (see examples in <a href="#S5.F6" title="In 5.1 Evaluation ‣ 5 Experiments ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Therefore, it’s beneficial to proactively disentangle action sounds and ambient sounds during training. However, separating in-the-wild ambient sounds is still an open challenge as recent models rely on supervised training on artificially mixed sounds, for which the ground truth complex masks can be obtained <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. Simply applying off-the-shelf noise reduction methods to training data leads to poor performance, as we will show in <a href="#S5" title="5 Experiments ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.8" class="ltx_p">While it is difficult to <em id="S3.SS2.p3.8.1" class="ltx_emph ltx_font_italic">explicitly</em> separate the ambient and action sound in the target audio,
our key observation is that ambient sounds are usually fairly stationary across time.
Given this observation, we propose a simple but effective method to achieve the disentanglement. During training, in addition to video clip <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">V</annotation></semantics></math>, we also provide the model an audio clip <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="A_{n}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">A</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">𝐴</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">A_{n}</annotation></semantics></math> that comes from the same training video but a different timestamp as the input video clip (see <a href="#S3.F3" title="In 3.2 Disentangling Action and Ambient Sounds ‣ 3 Ambient-aware Action Sound Generation ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>). Therefore, instead of modeling <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="p(A|V)" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mrow id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">​</mo><mrow id="S3.SS2.p3.3.m3.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.3.m3.1.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p3.3.m3.1.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.2" xref="S3.SS2.p3.3.m3.1.1.1.1.1.2.cmml">A</mi><mo fence="false" id="S3.SS2.p3.3.m3.1.1.1.1.1.1" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS2.p3.3.m3.1.1.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.SS2.p3.3.m3.1.1.1.1.3" xref="S3.SS2.p3.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><times id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2"></times><ci id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3">𝑝</ci><apply id="S3.SS2.p3.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.2">𝐴</ci><ci id="S3.SS2.p3.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.1.1.1.3">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">p(A|V)</annotation></semantics></math>, we model <math id="S3.SS2.p3.4.m4.2" class="ltx_Math" alttext="p(A|V,A_{n})" display="inline"><semantics id="S3.SS2.p3.4.m4.2a"><mrow id="S3.SS2.p3.4.m4.2.2" xref="S3.SS2.p3.4.m4.2.2.cmml"><mi id="S3.SS2.p3.4.m4.2.2.3" xref="S3.SS2.p3.4.m4.2.2.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p3.4.m4.2.2.2" xref="S3.SS2.p3.4.m4.2.2.2.cmml">​</mo><mrow id="S3.SS2.p3.4.m4.2.2.1.1" xref="S3.SS2.p3.4.m4.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p3.4.m4.2.2.1.1.2" xref="S3.SS2.p3.4.m4.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS2.p3.4.m4.2.2.1.1.1" xref="S3.SS2.p3.4.m4.2.2.1.1.1.cmml"><mi id="S3.SS2.p3.4.m4.2.2.1.1.1.3" xref="S3.SS2.p3.4.m4.2.2.1.1.1.3.cmml">A</mi><mo fence="false" id="S3.SS2.p3.4.m4.2.2.1.1.1.2" xref="S3.SS2.p3.4.m4.2.2.1.1.1.2.cmml">|</mo><mrow id="S3.SS2.p3.4.m4.2.2.1.1.1.1.1" xref="S3.SS2.p3.4.m4.2.2.1.1.1.1.2.cmml"><mi id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">V</mi><mo id="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.2" xref="S3.SS2.p3.4.m4.2.2.1.1.1.1.2.cmml">,</mo><msub id="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1" xref="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1.2" xref="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1.2.cmml">A</mi><mi id="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1.3" xref="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1.3.cmml">n</mi></msub></mrow></mrow><mo stretchy="false" id="S3.SS2.p3.4.m4.2.2.1.1.3" xref="S3.SS2.p3.4.m4.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.2b"><apply id="S3.SS2.p3.4.m4.2.2.cmml" xref="S3.SS2.p3.4.m4.2.2"><times id="S3.SS2.p3.4.m4.2.2.2.cmml" xref="S3.SS2.p3.4.m4.2.2.2"></times><ci id="S3.SS2.p3.4.m4.2.2.3.cmml" xref="S3.SS2.p3.4.m4.2.2.3">𝑝</ci><apply id="S3.SS2.p3.4.m4.2.2.1.1.1.cmml" xref="S3.SS2.p3.4.m4.2.2.1.1"><csymbol cd="latexml" id="S3.SS2.p3.4.m4.2.2.1.1.1.2.cmml" xref="S3.SS2.p3.4.m4.2.2.1.1.1.2">conditional</csymbol><ci id="S3.SS2.p3.4.m4.2.2.1.1.1.3.cmml" xref="S3.SS2.p3.4.m4.2.2.1.1.1.3">𝐴</ci><list id="S3.SS2.p3.4.m4.2.2.1.1.1.1.2.cmml" xref="S3.SS2.p3.4.m4.2.2.1.1.1.1.1"><ci id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">𝑉</ci><apply id="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1.2">𝐴</ci><ci id="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.4.m4.2.2.1.1.1.1.1.1.3">𝑛</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.2c">p(A|V,A_{n})</annotation></semantics></math>. Given the hypothesis that <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="A_{n}" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><msub id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mi id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml">A</mi><mi id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2">𝐴</ci><ci id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">A_{n}</annotation></semantics></math> is likely to share ambient sound characteristics with <math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mi id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><ci id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">A</annotation></semantics></math>, it can take away the burden of learning weakly correlated or even uncorrelated ambient sounds from visual input alone, and encourages the model to focus on learning action features from the visual input.
For the selection of <math id="S3.SS2.p3.7.m7.1" class="ltx_Math" alttext="A_{n}" display="inline"><semantics id="S3.SS2.p3.7.m7.1a"><msub id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml"><mi id="S3.SS2.p3.7.m7.1.1.2" xref="S3.SS2.p3.7.m7.1.1.2.cmml">A</mi><mi id="S3.SS2.p3.7.m7.1.1.3" xref="S3.SS2.p3.7.m7.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><apply id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.2">𝐴</ci><ci id="S3.SS2.p3.7.m7.1.1.3.cmml" xref="S3.SS2.p3.7.m7.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">A_{n}</annotation></semantics></math>, we
randomly sample one audio clip from the nearest <math id="S3.SS2.p3.8.m8.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS2.p3.8.m8.1a"><mi id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><ci id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">X</annotation></semantics></math> clips in time.
While there is no guarantee that the sampled audio shares exactly the same ambient sound with the target audio, their ambient sounds should largely overlap since they are close in time, which provides a consistent learning signal to help the model learn the disentanglement.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2406.09272/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.4.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.5.2" class="ltx_text" style="font-size:90%;">Audio condition selection and the model architecture.
<span id="S3.F3.5.2.1" class="ltx_text ltx_font_bold">Left</span>: During training, we randomly sample a neighbor audio clip as the audio condition. For inference, we query the training set audio with the (silent) input video and retrieve an audio clip that has the highest audio-visual similarity with the input video using our trained AV-Sim model (<a href="#S3.SS5" title="3.5 Audio-Visual Representation Learning ‣ 3 Ambient-aware Action Sound Generation ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>).
<span id="S3.F3.5.2.2" class="ltx_text ltx_font_bold">Right</span>: We represent audio waveforms as spectrograms and use a latent diffusion model to generate the spectrogram conditioned on both the input video and the audio condition. At test time, we use a trained vocoder network to transform the spectrogram to a waveform.
</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Retrieval Augmented Generation and Controllable Generation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">While during training we have access to the clips in the same long video as the input clip, we of course cannot access that information at test time. How we select <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="A_{n}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">A</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝐴</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">A_{n}</annotation></semantics></math> at test time depends on the purpose of the generation. We consider two use cases: <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">action-ambient joint</span> generation and <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_italic">action-focused</span> generation. In the first scenario, we would like the model to generate both the action sound and the ambient sound that is plausible for the visual environment. This is, for example, useful for generating sound effects for videos. In the latter scenario, we would like the model to focus the generation on action sounds and <em id="S3.SS3.p1.1.3" class="ltx_emph ltx_font_italic">minimize</em> ambient sounds, which is useful, for example, for generating sounds for games. <a href="#S3.F4" title="In 3.3 Retrieval Augmented Generation and Controllable Generation ‣ 3 Ambient-aware Action Sound Generation ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a> depicts the two scenarios.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">For action-ambient joint generation, we want <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="A_{n}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">A</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝐴</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">A_{n}</annotation></semantics></math> to be semantically relevant to the visual scene. Inspired by recent work in retrieval augmented regeneration, we propose to retrieve audio such that:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="A_{n}=\operatorname*{arg\,max}_{A_{i}\in\mathcal{D}}{\text{AV-Sim}(A_{i},V)}," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.3.2" xref="S3.E1.m1.2.2.1.1.3.2.cmml">A</mi><mi id="S3.E1.m1.2.2.1.1.3.3" xref="S3.E1.m1.2.2.1.1.3.3.cmml">n</mi></msub><mo id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml"><munder id="S3.E1.m1.2.2.1.1.1.3.1" xref="S3.E1.m1.2.2.1.1.1.3.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.3.1.2" xref="S3.E1.m1.2.2.1.1.1.3.1.2.cmml"><mi id="S3.E1.m1.2.2.1.1.1.3.1.2.2" xref="S3.E1.m1.2.2.1.1.1.3.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.3.1.2.1" xref="S3.E1.m1.2.2.1.1.1.3.1.2.1.cmml">​</mo><mi id="S3.E1.m1.2.2.1.1.1.3.1.2.3" xref="S3.E1.m1.2.2.1.1.1.3.1.2.3.cmml">max</mi></mrow><mrow id="S3.E1.m1.2.2.1.1.1.3.1.3" xref="S3.E1.m1.2.2.1.1.1.3.1.3.cmml"><msub id="S3.E1.m1.2.2.1.1.1.3.1.3.2" xref="S3.E1.m1.2.2.1.1.1.3.1.3.2.cmml"><mi id="S3.E1.m1.2.2.1.1.1.3.1.3.2.2" xref="S3.E1.m1.2.2.1.1.1.3.1.3.2.2.cmml">A</mi><mi id="S3.E1.m1.2.2.1.1.1.3.1.3.2.3" xref="S3.E1.m1.2.2.1.1.1.3.1.3.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.1.1.1.3.1.3.1" xref="S3.E1.m1.2.2.1.1.1.3.1.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.2.1.1.1.3.1.3.3" xref="S3.E1.m1.2.2.1.1.1.3.1.3.3.cmml">𝒟</mi></mrow></munder><mo id="S3.E1.m1.2.2.1.1.1.3a" xref="S3.E1.m1.2.2.1.1.1.3.cmml">⁡</mo><mtext id="S3.E1.m1.2.2.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.3.2a.cmml">AV-Sim</mtext></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">(</mo><msub id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml">A</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">,</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">V</mi><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.4" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"></eq><apply id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2">𝐴</ci><ci id="S3.E1.m1.2.2.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.3.3">𝑛</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2"></times><apply id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3"><apply id="S3.E1.m1.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.3.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1">subscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.3.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1.2"><times id="S3.E1.m1.2.2.1.1.1.3.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1.2.1"></times><ci id="S3.E1.m1.2.2.1.1.1.3.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1.2.2">arg</ci><ci id="S3.E1.m1.2.2.1.1.1.3.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1.2.3">max</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.3.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1.3"><in id="S3.E1.m1.2.2.1.1.1.3.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1.3.1"></in><apply id="S3.E1.m1.2.2.1.1.1.3.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.3.1.3.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1.3.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.3.1.3.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1.3.2.2">𝐴</ci><ci id="S3.E1.m1.2.2.1.1.1.3.1.3.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1.3.2.3">𝑖</ci></apply><ci id="S3.E1.m1.2.2.1.1.1.3.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1.3.3">𝒟</ci></apply></apply><ci id="S3.E1.m1.2.2.1.1.1.3.2a.cmml" xref="S3.E1.m1.2.2.1.1.1.3.2"><mtext id="S3.E1.m1.2.2.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.2">AV-Sim</mtext></ci></apply><interval closure="open" id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2">𝐴</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3">𝑖</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑉</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">A_{n}=\operatorname*{arg\,max}_{A_{i}\in\mathcal{D}}{\text{AV-Sim}(A_{i},V)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.6" class="ltx_p">where <math id="S3.SS3.p2.2.m1.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S3.SS3.p2.2.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.2.m1.1.1" xref="S3.SS3.p2.2.m1.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m1.1b"><ci id="S3.SS3.p2.2.m1.1.1.cmml" xref="S3.SS3.p2.2.m1.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m1.1c">\mathcal{D}</annotation></semantics></math> is the dataset of all training audio clips and <math id="S3.SS3.p2.3.m2.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS3.p2.3.m2.1a"><mi id="S3.SS3.p2.3.m2.1.1" xref="S3.SS3.p2.3.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m2.1b"><ci id="S3.SS3.p2.3.m2.1.1.cmml" xref="S3.SS3.p2.3.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m2.1c">V</annotation></semantics></math> is the (silent) input video. <math id="S3.SS3.p2.4.m3.2" class="ltx_Math" alttext="\text{AV-Sim}(A,V)" display="inline"><semantics id="S3.SS3.p2.4.m3.2a"><mrow id="S3.SS3.p2.4.m3.2.3" xref="S3.SS3.p2.4.m3.2.3.cmml"><mtext id="S3.SS3.p2.4.m3.2.3.2" xref="S3.SS3.p2.4.m3.2.3.2a.cmml">AV-Sim</mtext><mo lspace="0em" rspace="0em" id="S3.SS3.p2.4.m3.2.3.1" xref="S3.SS3.p2.4.m3.2.3.1.cmml">​</mo><mrow id="S3.SS3.p2.4.m3.2.3.3.2" xref="S3.SS3.p2.4.m3.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS3.p2.4.m3.2.3.3.2.1" xref="S3.SS3.p2.4.m3.2.3.3.1.cmml">(</mo><mi id="S3.SS3.p2.4.m3.1.1" xref="S3.SS3.p2.4.m3.1.1.cmml">A</mi><mo id="S3.SS3.p2.4.m3.2.3.3.2.2" xref="S3.SS3.p2.4.m3.2.3.3.1.cmml">,</mo><mi id="S3.SS3.p2.4.m3.2.2" xref="S3.SS3.p2.4.m3.2.2.cmml">V</mi><mo stretchy="false" id="S3.SS3.p2.4.m3.2.3.3.2.3" xref="S3.SS3.p2.4.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m3.2b"><apply id="S3.SS3.p2.4.m3.2.3.cmml" xref="S3.SS3.p2.4.m3.2.3"><times id="S3.SS3.p2.4.m3.2.3.1.cmml" xref="S3.SS3.p2.4.m3.2.3.1"></times><ci id="S3.SS3.p2.4.m3.2.3.2a.cmml" xref="S3.SS3.p2.4.m3.2.3.2"><mtext id="S3.SS3.p2.4.m3.2.3.2.cmml" xref="S3.SS3.p2.4.m3.2.3.2">AV-Sim</mtext></ci><interval closure="open" id="S3.SS3.p2.4.m3.2.3.3.1.cmml" xref="S3.SS3.p2.4.m3.2.3.3.2"><ci id="S3.SS3.p2.4.m3.1.1.cmml" xref="S3.SS3.p2.4.m3.1.1">𝐴</ci><ci id="S3.SS3.p2.4.m3.2.2.cmml" xref="S3.SS3.p2.4.m3.2.2">𝑉</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m3.2c">\text{AV-Sim}(A,V)</annotation></semantics></math> is a similarity scoring function that measures the similarity between <math id="S3.SS3.p2.5.m4.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS3.p2.5.m4.1a"><mi id="S3.SS3.p2.5.m4.1.1" xref="S3.SS3.p2.5.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m4.1b"><ci id="S3.SS3.p2.5.m4.1.1.cmml" xref="S3.SS3.p2.5.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m4.1c">A</annotation></semantics></math> and <math id="S3.SS3.p2.6.m5.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS3.p2.6.m5.1a"><mi id="S3.SS3.p2.6.m5.1.1" xref="S3.SS3.p2.6.m5.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m5.1b"><ci id="S3.SS3.p2.6.m5.1.1.cmml" xref="S3.SS3.p2.6.m5.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m5.1c">V</annotation></semantics></math>, which we will cover in <a href="#S3.SS5" title="3.5 Audio-Visual Representation Learning ‣ 3 Ambient-aware Action Sound Generation ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.2" class="ltx_p">For action-focused generation, we want <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="A_{n}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><msub id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">A</mi><mi id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">𝐴</ci><ci id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">A_{n}</annotation></semantics></math> to have minimal ambient level. We find simply filling <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="A_{n}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><msub id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">A</mi><mi id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">𝐴</ci><ci id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">A_{n}</annotation></semantics></math> with all zeros results in poor performance, likely because it is too far out of the training distribution. Instead, we find conditioning the generation on a low-ambient sound will hint the model to focus on action sound generation and generate minimal ambient sound. See <a href="#S5.SS3" title="5.3 Ambient Sound Control ‣ 5 Experiments ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2406.09272/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="277" height="128" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Two inference settings: “action-ambient joint generation” and “action-focused generation”. In the first setting, we condition on audio retrieved from the training set and aim to generate both plausible action and ambient sounds. In the second setting, we specify an audio file with low ambient sound and the model focuses on generating plausible action sounds while minimizing the ambient sounds.
</span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Audio-Visual Latent Diffusion Model</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">While the above idea of disentanglement is universal and not specific to any model architecture, here we instantiate this idea on diffusion models due to their success in audio generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. We extend the latent diffusion model to accommodate our audio-visual conditions, thus yielding an audio-visual latent diffusion model (AV-LDM).</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.10" class="ltx_p"><a href="#S3.F3" title="In 3.2 Disentangling Action and Ambient Sounds ‣ 3 Ambient-aware Action Sound Generation ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a> (right) shows the architecture of our model. During training, given audio waveform target <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mi id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">A</annotation></semantics></math>, we first compute the mel-spectrogram <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="x_{0}\in\mathbb{R}^{T\times D_{\text{mel}}}" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mrow id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml"><msub id="S3.SS4.p2.2.m2.1.1.2" xref="S3.SS4.p2.2.m2.1.1.2.cmml"><mi id="S3.SS4.p2.2.m2.1.1.2.2" xref="S3.SS4.p2.2.m2.1.1.2.2.cmml">x</mi><mn id="S3.SS4.p2.2.m2.1.1.2.3" xref="S3.SS4.p2.2.m2.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS4.p2.2.m2.1.1.1" xref="S3.SS4.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS4.p2.2.m2.1.1.3" xref="S3.SS4.p2.2.m2.1.1.3.cmml"><mi id="S3.SS4.p2.2.m2.1.1.3.2" xref="S3.SS4.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p2.2.m2.1.1.3.3" xref="S3.SS4.p2.2.m2.1.1.3.3.cmml"><mi id="S3.SS4.p2.2.m2.1.1.3.3.2" xref="S3.SS4.p2.2.m2.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p2.2.m2.1.1.3.3.1" xref="S3.SS4.p2.2.m2.1.1.3.3.1.cmml">×</mo><msub id="S3.SS4.p2.2.m2.1.1.3.3.3" xref="S3.SS4.p2.2.m2.1.1.3.3.3.cmml"><mi id="S3.SS4.p2.2.m2.1.1.3.3.3.2" xref="S3.SS4.p2.2.m2.1.1.3.3.3.2.cmml">D</mi><mtext id="S3.SS4.p2.2.m2.1.1.3.3.3.3" xref="S3.SS4.p2.2.m2.1.1.3.3.3.3a.cmml">mel</mtext></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><apply id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1"><in id="S3.SS4.p2.2.m2.1.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1.1"></in><apply id="S3.SS4.p2.2.m2.1.1.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.2.1.cmml" xref="S3.SS4.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.2.2.cmml" xref="S3.SS4.p2.2.m2.1.1.2.2">𝑥</ci><cn type="integer" id="S3.SS4.p2.2.m2.1.1.2.3.cmml" xref="S3.SS4.p2.2.m2.1.1.2.3">0</cn></apply><apply id="S3.SS4.p2.2.m2.1.1.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.3.1.cmml" xref="S3.SS4.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.3.2.cmml" xref="S3.SS4.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS4.p2.2.m2.1.1.3.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3"><times id="S3.SS4.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.1"></times><ci id="S3.SS4.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.2">𝑇</ci><apply id="S3.SS4.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS4.p2.2.m2.1.1.3.3.3.1.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.3">subscript</csymbol><ci id="S3.SS4.p2.2.m2.1.1.3.3.3.2.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.3.2">𝐷</ci><ci id="S3.SS4.p2.2.m2.1.1.3.3.3.3a.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.3.3"><mtext mathsize="50%" id="S3.SS4.p2.2.m2.1.1.3.3.3.3.cmml" xref="S3.SS4.p2.2.m2.1.1.3.3.3.3">mel</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">x_{0}\in\mathbb{R}^{T\times D_{\text{mel}}}</annotation></semantics></math>, where <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="D_{\text{mel}}" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><msub id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><mi id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml">D</mi><mtext id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3a.cmml">mel</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2">𝐷</ci><ci id="S3.SS4.p2.3.m3.1.1.3a.cmml" xref="S3.SS4.p2.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3">mel</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">D_{\text{mel}}</annotation></semantics></math> is the number of mel bins. We then use a pretrained Variational Autoencoder (VAE) to compress the mel-spectrogram <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="x_{0}" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><msub id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><mi id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">x</mi><mn id="S3.SS4.p2.4.m4.1.1.3" xref="S3.SS4.p2.4.m4.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">𝑥</ci><cn type="integer" id="S3.SS4.p2.4.m4.1.1.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">x_{0}</annotation></semantics></math> to a latent representation <math id="S3.SS4.p2.5.m5.1" class="ltx_Math" alttext="z_{0}\in\mathbb{R}^{C^{\prime}\times H^{\prime}\times W^{\prime}}" display="inline"><semantics id="S3.SS4.p2.5.m5.1a"><mrow id="S3.SS4.p2.5.m5.1.1" xref="S3.SS4.p2.5.m5.1.1.cmml"><msub id="S3.SS4.p2.5.m5.1.1.2" xref="S3.SS4.p2.5.m5.1.1.2.cmml"><mi id="S3.SS4.p2.5.m5.1.1.2.2" xref="S3.SS4.p2.5.m5.1.1.2.2.cmml">z</mi><mn id="S3.SS4.p2.5.m5.1.1.2.3" xref="S3.SS4.p2.5.m5.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS4.p2.5.m5.1.1.1" xref="S3.SS4.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS4.p2.5.m5.1.1.3" xref="S3.SS4.p2.5.m5.1.1.3.cmml"><mi id="S3.SS4.p2.5.m5.1.1.3.2" xref="S3.SS4.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p2.5.m5.1.1.3.3" xref="S3.SS4.p2.5.m5.1.1.3.3.cmml"><msup id="S3.SS4.p2.5.m5.1.1.3.3.2" xref="S3.SS4.p2.5.m5.1.1.3.3.2.cmml"><mi id="S3.SS4.p2.5.m5.1.1.3.3.2.2" xref="S3.SS4.p2.5.m5.1.1.3.3.2.2.cmml">C</mi><mo id="S3.SS4.p2.5.m5.1.1.3.3.2.3" xref="S3.SS4.p2.5.m5.1.1.3.3.2.3.cmml">′</mo></msup><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p2.5.m5.1.1.3.3.1" xref="S3.SS4.p2.5.m5.1.1.3.3.1.cmml">×</mo><msup id="S3.SS4.p2.5.m5.1.1.3.3.3" xref="S3.SS4.p2.5.m5.1.1.3.3.3.cmml"><mi id="S3.SS4.p2.5.m5.1.1.3.3.3.2" xref="S3.SS4.p2.5.m5.1.1.3.3.3.2.cmml">H</mi><mo id="S3.SS4.p2.5.m5.1.1.3.3.3.3" xref="S3.SS4.p2.5.m5.1.1.3.3.3.3.cmml">′</mo></msup><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p2.5.m5.1.1.3.3.1a" xref="S3.SS4.p2.5.m5.1.1.3.3.1.cmml">×</mo><msup id="S3.SS4.p2.5.m5.1.1.3.3.4" xref="S3.SS4.p2.5.m5.1.1.3.3.4.cmml"><mi id="S3.SS4.p2.5.m5.1.1.3.3.4.2" xref="S3.SS4.p2.5.m5.1.1.3.3.4.2.cmml">W</mi><mo id="S3.SS4.p2.5.m5.1.1.3.3.4.3" xref="S3.SS4.p2.5.m5.1.1.3.3.4.3.cmml">′</mo></msup></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m5.1b"><apply id="S3.SS4.p2.5.m5.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1"><in id="S3.SS4.p2.5.m5.1.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1.1"></in><apply id="S3.SS4.p2.5.m5.1.1.2.cmml" xref="S3.SS4.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.1.1.2.1.cmml" xref="S3.SS4.p2.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS4.p2.5.m5.1.1.2.2.cmml" xref="S3.SS4.p2.5.m5.1.1.2.2">𝑧</ci><cn type="integer" id="S3.SS4.p2.5.m5.1.1.2.3.cmml" xref="S3.SS4.p2.5.m5.1.1.2.3">0</cn></apply><apply id="S3.SS4.p2.5.m5.1.1.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.1.1.3.1.cmml" xref="S3.SS4.p2.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS4.p2.5.m5.1.1.3.2.cmml" xref="S3.SS4.p2.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS4.p2.5.m5.1.1.3.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3"><times id="S3.SS4.p2.5.m5.1.1.3.3.1.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.1"></times><apply id="S3.SS4.p2.5.m5.1.1.3.3.2.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.1.1.3.3.2.1.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.2">superscript</csymbol><ci id="S3.SS4.p2.5.m5.1.1.3.3.2.2.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.2.2">𝐶</ci><ci id="S3.SS4.p2.5.m5.1.1.3.3.2.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.2.3">′</ci></apply><apply id="S3.SS4.p2.5.m5.1.1.3.3.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.1.1.3.3.3.1.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.3">superscript</csymbol><ci id="S3.SS4.p2.5.m5.1.1.3.3.3.2.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.3.2">𝐻</ci><ci id="S3.SS4.p2.5.m5.1.1.3.3.3.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.3.3">′</ci></apply><apply id="S3.SS4.p2.5.m5.1.1.3.3.4.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.4"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.1.1.3.3.4.1.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.4">superscript</csymbol><ci id="S3.SS4.p2.5.m5.1.1.3.3.4.2.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.4.2">𝑊</ci><ci id="S3.SS4.p2.5.m5.1.1.3.3.4.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.4.3">′</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m5.1c">z_{0}\in\mathbb{R}^{C^{\prime}\times H^{\prime}\times W^{\prime}}</annotation></semantics></math>, where <math id="S3.SS4.p2.6.m6.1" class="ltx_Math" alttext="z_{0}" display="inline"><semantics id="S3.SS4.p2.6.m6.1a"><msub id="S3.SS4.p2.6.m6.1.1" xref="S3.SS4.p2.6.m6.1.1.cmml"><mi id="S3.SS4.p2.6.m6.1.1.2" xref="S3.SS4.p2.6.m6.1.1.2.cmml">z</mi><mn id="S3.SS4.p2.6.m6.1.1.3" xref="S3.SS4.p2.6.m6.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m6.1b"><apply id="S3.SS4.p2.6.m6.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.6.m6.1.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.p2.6.m6.1.1.2.cmml" xref="S3.SS4.p2.6.m6.1.1.2">𝑧</ci><cn type="integer" id="S3.SS4.p2.6.m6.1.1.3.cmml" xref="S3.SS4.p2.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.6.m6.1c">z_{0}</annotation></semantics></math> is the generation target of the LDM. We condition the generation on both the video feature <math id="S3.SS4.p2.7.m7.2" class="ltx_Math" alttext="c_{v}\in\mathbb{R}^{T_{v},D_{c}}" display="inline"><semantics id="S3.SS4.p2.7.m7.2a"><mrow id="S3.SS4.p2.7.m7.2.3" xref="S3.SS4.p2.7.m7.2.3.cmml"><msub id="S3.SS4.p2.7.m7.2.3.2" xref="S3.SS4.p2.7.m7.2.3.2.cmml"><mi id="S3.SS4.p2.7.m7.2.3.2.2" xref="S3.SS4.p2.7.m7.2.3.2.2.cmml">c</mi><mi id="S3.SS4.p2.7.m7.2.3.2.3" xref="S3.SS4.p2.7.m7.2.3.2.3.cmml">v</mi></msub><mo id="S3.SS4.p2.7.m7.2.3.1" xref="S3.SS4.p2.7.m7.2.3.1.cmml">∈</mo><msup id="S3.SS4.p2.7.m7.2.3.3" xref="S3.SS4.p2.7.m7.2.3.3.cmml"><mi id="S3.SS4.p2.7.m7.2.3.3.2" xref="S3.SS4.p2.7.m7.2.3.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p2.7.m7.2.2.2.2" xref="S3.SS4.p2.7.m7.2.2.2.3.cmml"><msub id="S3.SS4.p2.7.m7.1.1.1.1.1" xref="S3.SS4.p2.7.m7.1.1.1.1.1.cmml"><mi id="S3.SS4.p2.7.m7.1.1.1.1.1.2" xref="S3.SS4.p2.7.m7.1.1.1.1.1.2.cmml">T</mi><mi id="S3.SS4.p2.7.m7.1.1.1.1.1.3" xref="S3.SS4.p2.7.m7.1.1.1.1.1.3.cmml">v</mi></msub><mo id="S3.SS4.p2.7.m7.2.2.2.2.3" xref="S3.SS4.p2.7.m7.2.2.2.3.cmml">,</mo><msub id="S3.SS4.p2.7.m7.2.2.2.2.2" xref="S3.SS4.p2.7.m7.2.2.2.2.2.cmml"><mi id="S3.SS4.p2.7.m7.2.2.2.2.2.2" xref="S3.SS4.p2.7.m7.2.2.2.2.2.2.cmml">D</mi><mi id="S3.SS4.p2.7.m7.2.2.2.2.2.3" xref="S3.SS4.p2.7.m7.2.2.2.2.2.3.cmml">c</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.7.m7.2b"><apply id="S3.SS4.p2.7.m7.2.3.cmml" xref="S3.SS4.p2.7.m7.2.3"><in id="S3.SS4.p2.7.m7.2.3.1.cmml" xref="S3.SS4.p2.7.m7.2.3.1"></in><apply id="S3.SS4.p2.7.m7.2.3.2.cmml" xref="S3.SS4.p2.7.m7.2.3.2"><csymbol cd="ambiguous" id="S3.SS4.p2.7.m7.2.3.2.1.cmml" xref="S3.SS4.p2.7.m7.2.3.2">subscript</csymbol><ci id="S3.SS4.p2.7.m7.2.3.2.2.cmml" xref="S3.SS4.p2.7.m7.2.3.2.2">𝑐</ci><ci id="S3.SS4.p2.7.m7.2.3.2.3.cmml" xref="S3.SS4.p2.7.m7.2.3.2.3">𝑣</ci></apply><apply id="S3.SS4.p2.7.m7.2.3.3.cmml" xref="S3.SS4.p2.7.m7.2.3.3"><csymbol cd="ambiguous" id="S3.SS4.p2.7.m7.2.3.3.1.cmml" xref="S3.SS4.p2.7.m7.2.3.3">superscript</csymbol><ci id="S3.SS4.p2.7.m7.2.3.3.2.cmml" xref="S3.SS4.p2.7.m7.2.3.3.2">ℝ</ci><list id="S3.SS4.p2.7.m7.2.2.2.3.cmml" xref="S3.SS4.p2.7.m7.2.2.2.2"><apply id="S3.SS4.p2.7.m7.1.1.1.1.1.cmml" xref="S3.SS4.p2.7.m7.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS4.p2.7.m7.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p2.7.m7.1.1.1.1.1.2.cmml" xref="S3.SS4.p2.7.m7.1.1.1.1.1.2">𝑇</ci><ci id="S3.SS4.p2.7.m7.1.1.1.1.1.3.cmml" xref="S3.SS4.p2.7.m7.1.1.1.1.1.3">𝑣</ci></apply><apply id="S3.SS4.p2.7.m7.2.2.2.2.2.cmml" xref="S3.SS4.p2.7.m7.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p2.7.m7.2.2.2.2.2.1.cmml" xref="S3.SS4.p2.7.m7.2.2.2.2.2">subscript</csymbol><ci id="S3.SS4.p2.7.m7.2.2.2.2.2.2.cmml" xref="S3.SS4.p2.7.m7.2.2.2.2.2.2">𝐷</ci><ci id="S3.SS4.p2.7.m7.2.2.2.2.2.3.cmml" xref="S3.SS4.p2.7.m7.2.2.2.2.2.3">𝑐</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.7.m7.2c">c_{v}\in\mathbb{R}^{T_{v},D_{c}}</annotation></semantics></math> and audio feature <math id="S3.SS4.p2.8.m8.2" class="ltx_Math" alttext="c_{a}\in\mathbb{R}^{T_{a},D_{c}}" display="inline"><semantics id="S3.SS4.p2.8.m8.2a"><mrow id="S3.SS4.p2.8.m8.2.3" xref="S3.SS4.p2.8.m8.2.3.cmml"><msub id="S3.SS4.p2.8.m8.2.3.2" xref="S3.SS4.p2.8.m8.2.3.2.cmml"><mi id="S3.SS4.p2.8.m8.2.3.2.2" xref="S3.SS4.p2.8.m8.2.3.2.2.cmml">c</mi><mi id="S3.SS4.p2.8.m8.2.3.2.3" xref="S3.SS4.p2.8.m8.2.3.2.3.cmml">a</mi></msub><mo id="S3.SS4.p2.8.m8.2.3.1" xref="S3.SS4.p2.8.m8.2.3.1.cmml">∈</mo><msup id="S3.SS4.p2.8.m8.2.3.3" xref="S3.SS4.p2.8.m8.2.3.3.cmml"><mi id="S3.SS4.p2.8.m8.2.3.3.2" xref="S3.SS4.p2.8.m8.2.3.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p2.8.m8.2.2.2.2" xref="S3.SS4.p2.8.m8.2.2.2.3.cmml"><msub id="S3.SS4.p2.8.m8.1.1.1.1.1" xref="S3.SS4.p2.8.m8.1.1.1.1.1.cmml"><mi id="S3.SS4.p2.8.m8.1.1.1.1.1.2" xref="S3.SS4.p2.8.m8.1.1.1.1.1.2.cmml">T</mi><mi id="S3.SS4.p2.8.m8.1.1.1.1.1.3" xref="S3.SS4.p2.8.m8.1.1.1.1.1.3.cmml">a</mi></msub><mo id="S3.SS4.p2.8.m8.2.2.2.2.3" xref="S3.SS4.p2.8.m8.2.2.2.3.cmml">,</mo><msub id="S3.SS4.p2.8.m8.2.2.2.2.2" xref="S3.SS4.p2.8.m8.2.2.2.2.2.cmml"><mi id="S3.SS4.p2.8.m8.2.2.2.2.2.2" xref="S3.SS4.p2.8.m8.2.2.2.2.2.2.cmml">D</mi><mi id="S3.SS4.p2.8.m8.2.2.2.2.2.3" xref="S3.SS4.p2.8.m8.2.2.2.2.2.3.cmml">c</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.8.m8.2b"><apply id="S3.SS4.p2.8.m8.2.3.cmml" xref="S3.SS4.p2.8.m8.2.3"><in id="S3.SS4.p2.8.m8.2.3.1.cmml" xref="S3.SS4.p2.8.m8.2.3.1"></in><apply id="S3.SS4.p2.8.m8.2.3.2.cmml" xref="S3.SS4.p2.8.m8.2.3.2"><csymbol cd="ambiguous" id="S3.SS4.p2.8.m8.2.3.2.1.cmml" xref="S3.SS4.p2.8.m8.2.3.2">subscript</csymbol><ci id="S3.SS4.p2.8.m8.2.3.2.2.cmml" xref="S3.SS4.p2.8.m8.2.3.2.2">𝑐</ci><ci id="S3.SS4.p2.8.m8.2.3.2.3.cmml" xref="S3.SS4.p2.8.m8.2.3.2.3">𝑎</ci></apply><apply id="S3.SS4.p2.8.m8.2.3.3.cmml" xref="S3.SS4.p2.8.m8.2.3.3"><csymbol cd="ambiguous" id="S3.SS4.p2.8.m8.2.3.3.1.cmml" xref="S3.SS4.p2.8.m8.2.3.3">superscript</csymbol><ci id="S3.SS4.p2.8.m8.2.3.3.2.cmml" xref="S3.SS4.p2.8.m8.2.3.3.2">ℝ</ci><list id="S3.SS4.p2.8.m8.2.2.2.3.cmml" xref="S3.SS4.p2.8.m8.2.2.2.2"><apply id="S3.SS4.p2.8.m8.1.1.1.1.1.cmml" xref="S3.SS4.p2.8.m8.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.8.m8.1.1.1.1.1.1.cmml" xref="S3.SS4.p2.8.m8.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p2.8.m8.1.1.1.1.1.2.cmml" xref="S3.SS4.p2.8.m8.1.1.1.1.1.2">𝑇</ci><ci id="S3.SS4.p2.8.m8.1.1.1.1.1.3.cmml" xref="S3.SS4.p2.8.m8.1.1.1.1.1.3">𝑎</ci></apply><apply id="S3.SS4.p2.8.m8.2.2.2.2.2.cmml" xref="S3.SS4.p2.8.m8.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p2.8.m8.2.2.2.2.2.1.cmml" xref="S3.SS4.p2.8.m8.2.2.2.2.2">subscript</csymbol><ci id="S3.SS4.p2.8.m8.2.2.2.2.2.2.cmml" xref="S3.SS4.p2.8.m8.2.2.2.2.2.2">𝐷</ci><ci id="S3.SS4.p2.8.m8.2.2.2.2.2.3.cmml" xref="S3.SS4.p2.8.m8.2.2.2.2.2.3">𝑐</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.8.m8.2c">c_{a}\in\mathbb{R}^{T_{a},D_{c}}</annotation></semantics></math>.
We extract the video feature with a pretrained video encoder (see <a href="#S3.SS5" title="3.5 Audio-Visual Representation Learning ‣ 3 Ambient-aware Action Sound Generation ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>) from <math id="S3.SS4.p2.9.m9.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS4.p2.9.m9.1a"><mi id="S3.SS4.p2.9.m9.1.1" xref="S3.SS4.p2.9.m9.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.9.m9.1b"><ci id="S3.SS4.p2.9.m9.1.1.cmml" xref="S3.SS4.p2.9.m9.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.9.m9.1c">V</annotation></semantics></math>. We extract the audio feature from the audio condition <math id="S3.SS4.p2.10.m10.1" class="ltx_Math" alttext="A_{n}" display="inline"><semantics id="S3.SS4.p2.10.m10.1a"><msub id="S3.SS4.p2.10.m10.1.1" xref="S3.SS4.p2.10.m10.1.1.cmml"><mi id="S3.SS4.p2.10.m10.1.1.2" xref="S3.SS4.p2.10.m10.1.1.2.cmml">A</mi><mi id="S3.SS4.p2.10.m10.1.1.3" xref="S3.SS4.p2.10.m10.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.10.m10.1b"><apply id="S3.SS4.p2.10.m10.1.1.cmml" xref="S3.SS4.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS4.p2.10.m10.1.1.1.cmml" xref="S3.SS4.p2.10.m10.1.1">subscript</csymbol><ci id="S3.SS4.p2.10.m10.1.1.2.cmml" xref="S3.SS4.p2.10.m10.1.1.2">𝐴</ci><ci id="S3.SS4.p2.10.m10.1.1.3.cmml" xref="S3.SS4.p2.10.m10.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.10.m10.1c">A_{n}</annotation></semantics></math> with the same VAE encoder and then transform the feature into 1-d vector with a multilayer perceptron (MLP).</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.3" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, we use cross attention where the query is produced by <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">z</mi><mi id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝑧</ci><ci id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">z_{t}</annotation></semantics></math>, which is the sample diffusion step <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">t</annotation></semantics></math>, and key and value are produced by 
<br class="ltx_break"><math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="\text{concat}([\text{Pos}_{v}+c_{v};\text{Pos}_{a}+c_{a}])" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><mrow id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml"><mtext id="S3.SS4.p3.3.m3.1.1.3" xref="S3.SS4.p3.3.m3.1.1.3a.cmml">concat</mtext><mo lspace="0em" rspace="0em" id="S3.SS4.p3.3.m3.1.1.2" xref="S3.SS4.p3.3.m3.1.1.2.cmml">​</mo><mrow id="S3.SS4.p3.3.m3.1.1.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml"><mo stretchy="false" id="S3.SS4.p3.3.m3.1.1.1.1.2" xref="S3.SS4.p3.3.m3.1.1.cmml">(</mo><mrow id="S3.SS4.p3.3.m3.1.1.1.1.1.2" xref="S3.SS4.p3.3.m3.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS4.p3.3.m3.1.1.1.1.1.2.3" xref="S3.SS4.p3.3.m3.1.1.1.1.1.3.cmml">[</mo><mrow id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.cmml"><msub id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2.cmml"><mtext id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2.2" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2.2a.cmml">Pos</mtext><mi id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2.3" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2.3.cmml">v</mi></msub><mo id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.1" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3.2" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3.2.cmml">c</mi><mi id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3.3" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3.3.cmml">v</mi></msub></mrow><mo id="S3.SS4.p3.3.m3.1.1.1.1.1.2.4" xref="S3.SS4.p3.3.m3.1.1.1.1.1.3.cmml">;</mo><mrow id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.cmml"><msub id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2.cmml"><mtext id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2.2" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2.2a.cmml">Pos</mtext><mi id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2.3" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2.3.cmml">a</mi></msub><mo id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.1" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.1.cmml">+</mo><msub id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3.cmml"><mi id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3.2" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3.2.cmml">c</mi><mi id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3.3" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3.3.cmml">a</mi></msub></mrow><mo stretchy="false" id="S3.SS4.p3.3.m3.1.1.1.1.1.2.5" xref="S3.SS4.p3.3.m3.1.1.1.1.1.3.cmml">]</mo></mrow><mo stretchy="false" id="S3.SS4.p3.3.m3.1.1.1.1.3" xref="S3.SS4.p3.3.m3.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><apply id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1"><times id="S3.SS4.p3.3.m3.1.1.2.cmml" xref="S3.SS4.p3.3.m3.1.1.2"></times><ci id="S3.SS4.p3.3.m3.1.1.3a.cmml" xref="S3.SS4.p3.3.m3.1.1.3"><mtext id="S3.SS4.p3.3.m3.1.1.3.cmml" xref="S3.SS4.p3.3.m3.1.1.3">concat</mtext></ci><list id="S3.SS4.p3.3.m3.1.1.1.1.1.3.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2"><apply id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1"><plus id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.1"></plus><apply id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2.2a.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2.2"><mtext id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2.2">Pos</mtext></ci><ci id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.2.3">𝑣</ci></apply><apply id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3.2">𝑐</ci><ci id="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.1.1.3.3">𝑣</ci></apply></apply><apply id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2"><plus id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.1"></plus><apply id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2.2a.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2.2"><mtext id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2.2.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2.2">Pos</mtext></ci><ci id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2.3.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.2.3">𝑎</ci></apply><apply id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3.1.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3">subscript</csymbol><ci id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3.2.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3.2">𝑐</ci><ci id="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3.3.cmml" xref="S3.SS4.p3.3.m3.1.1.1.1.1.2.2.3.3">𝑎</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">\text{concat}([\text{Pos}_{v}+c_{v};\text{Pos}_{a}+c_{a}])</annotation></semantics></math>, where Pos denotes learnable positional embeddings.
The model is trained with the denoising objective:</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.7" class="ltx_Math" alttext="\mathcal{L}=\mathbb{E}_{t\sim\text{uniform}(1,T),z_{0},\epsilon_{t}}\|\epsilon_{t}-\epsilon_{\theta}(\mathbf{x}_{t},t,c_{v},c_{a})\|^{2}," display="block"><semantics id="S3.Ex1.m1.7a"><mrow id="S3.Ex1.m1.7.7.1" xref="S3.Ex1.m1.7.7.1.1.cmml"><mrow id="S3.Ex1.m1.7.7.1.1" xref="S3.Ex1.m1.7.7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.7.7.1.1.3" xref="S3.Ex1.m1.7.7.1.1.3.cmml">ℒ</mi><mo id="S3.Ex1.m1.7.7.1.1.2" xref="S3.Ex1.m1.7.7.1.1.2.cmml">=</mo><mrow id="S3.Ex1.m1.7.7.1.1.1" xref="S3.Ex1.m1.7.7.1.1.1.cmml"><msub id="S3.Ex1.m1.7.7.1.1.1.3" xref="S3.Ex1.m1.7.7.1.1.1.3.cmml"><mi id="S3.Ex1.m1.7.7.1.1.1.3.2" xref="S3.Ex1.m1.7.7.1.1.1.3.2.cmml">𝔼</mi><mrow id="S3.Ex1.m1.5.5.5" xref="S3.Ex1.m1.5.5.5.cmml"><mi id="S3.Ex1.m1.5.5.5.7" xref="S3.Ex1.m1.5.5.5.7.cmml">t</mi><mo id="S3.Ex1.m1.5.5.5.6" xref="S3.Ex1.m1.5.5.5.6.cmml">∼</mo><mrow id="S3.Ex1.m1.5.5.5.5.3" xref="S3.Ex1.m1.5.5.5.5.4.cmml"><mrow id="S3.Ex1.m1.3.3.3.3.1.1" xref="S3.Ex1.m1.3.3.3.3.1.1.cmml"><mtext id="S3.Ex1.m1.3.3.3.3.1.1.2" xref="S3.Ex1.m1.3.3.3.3.1.1.2a.cmml">uniform</mtext><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.3.3.1.1.1" xref="S3.Ex1.m1.3.3.3.3.1.1.1.cmml">​</mo><mrow id="S3.Ex1.m1.3.3.3.3.1.1.3.2" xref="S3.Ex1.m1.3.3.3.3.1.1.3.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.3.3.3.3.1.1.3.2.1" xref="S3.Ex1.m1.3.3.3.3.1.1.3.1.cmml">(</mo><mn id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml">1</mn><mo id="S3.Ex1.m1.3.3.3.3.1.1.3.2.2" xref="S3.Ex1.m1.3.3.3.3.1.1.3.1.cmml">,</mo><mi id="S3.Ex1.m1.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.cmml">T</mi><mo stretchy="false" id="S3.Ex1.m1.3.3.3.3.1.1.3.2.3" xref="S3.Ex1.m1.3.3.3.3.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.5.5.5.5.3.4" xref="S3.Ex1.m1.5.5.5.5.4.cmml">,</mo><msub id="S3.Ex1.m1.4.4.4.4.2.2" xref="S3.Ex1.m1.4.4.4.4.2.2.cmml"><mi id="S3.Ex1.m1.4.4.4.4.2.2.2" xref="S3.Ex1.m1.4.4.4.4.2.2.2.cmml">z</mi><mn id="S3.Ex1.m1.4.4.4.4.2.2.3" xref="S3.Ex1.m1.4.4.4.4.2.2.3.cmml">0</mn></msub><mo id="S3.Ex1.m1.5.5.5.5.3.5" xref="S3.Ex1.m1.5.5.5.5.4.cmml">,</mo><msub id="S3.Ex1.m1.5.5.5.5.3.3" xref="S3.Ex1.m1.5.5.5.5.3.3.cmml"><mi id="S3.Ex1.m1.5.5.5.5.3.3.2" xref="S3.Ex1.m1.5.5.5.5.3.3.2.cmml">ϵ</mi><mi id="S3.Ex1.m1.5.5.5.5.3.3.3" xref="S3.Ex1.m1.5.5.5.5.3.3.3.cmml">t</mi></msub></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.7.7.1.1.1.2" xref="S3.Ex1.m1.7.7.1.1.1.2.cmml">​</mo><msup id="S3.Ex1.m1.7.7.1.1.1.1" xref="S3.Ex1.m1.7.7.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.7.7.1.1.1.1.1.1" xref="S3.Ex1.m1.7.7.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.Ex1.m1.7.7.1.1.1.1.1.1.2" xref="S3.Ex1.m1.7.7.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.cmml"><msub id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5.cmml"><mi id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5.2" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5.2.cmml">ϵ</mi><mi id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5.3" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5.3.cmml">t</mi></msub><mo id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.4" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.4.cmml">−</mo><mrow id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.cmml"><msub id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5.cmml"><mi id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5.2" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5.2.cmml">ϵ</mi><mi id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5.3" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.4" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.4.cmml">​</mo><mrow id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.4.cmml"><mo stretchy="false" id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.4" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.4.cmml">(</mo><msub id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.2.cmml">𝐱</mi><mi id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.5" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.4.cmml">,</mo><mi id="S3.Ex1.m1.6.6" xref="S3.Ex1.m1.6.6.cmml">t</mi><mo id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.6" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.4.cmml">,</mo><msub id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2.2.cmml">c</mi><mi id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2.3.cmml">v</mi></msub><mo id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.7" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.4.cmml">,</mo><msub id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3.2" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3.2.cmml">c</mi><mi id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3.3" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3.3.cmml">a</mi></msub><mo stretchy="false" id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.8" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.4.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.Ex1.m1.7.7.1.1.1.1.1.1.3" xref="S3.Ex1.m1.7.7.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.Ex1.m1.7.7.1.1.1.1.3" xref="S3.Ex1.m1.7.7.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow><mo id="S3.Ex1.m1.7.7.1.2" xref="S3.Ex1.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.7b"><apply id="S3.Ex1.m1.7.7.1.1.cmml" xref="S3.Ex1.m1.7.7.1"><eq id="S3.Ex1.m1.7.7.1.1.2.cmml" xref="S3.Ex1.m1.7.7.1.1.2"></eq><ci id="S3.Ex1.m1.7.7.1.1.3.cmml" xref="S3.Ex1.m1.7.7.1.1.3">ℒ</ci><apply id="S3.Ex1.m1.7.7.1.1.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1"><times id="S3.Ex1.m1.7.7.1.1.1.2.cmml" xref="S3.Ex1.m1.7.7.1.1.1.2"></times><apply id="S3.Ex1.m1.7.7.1.1.1.3.cmml" xref="S3.Ex1.m1.7.7.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.7.7.1.1.1.3.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.7.7.1.1.1.3.2.cmml" xref="S3.Ex1.m1.7.7.1.1.1.3.2">𝔼</ci><apply id="S3.Ex1.m1.5.5.5.cmml" xref="S3.Ex1.m1.5.5.5"><csymbol cd="latexml" id="S3.Ex1.m1.5.5.5.6.cmml" xref="S3.Ex1.m1.5.5.5.6">similar-to</csymbol><ci id="S3.Ex1.m1.5.5.5.7.cmml" xref="S3.Ex1.m1.5.5.5.7">𝑡</ci><list id="S3.Ex1.m1.5.5.5.5.4.cmml" xref="S3.Ex1.m1.5.5.5.5.3"><apply id="S3.Ex1.m1.3.3.3.3.1.1.cmml" xref="S3.Ex1.m1.3.3.3.3.1.1"><times id="S3.Ex1.m1.3.3.3.3.1.1.1.cmml" xref="S3.Ex1.m1.3.3.3.3.1.1.1"></times><ci id="S3.Ex1.m1.3.3.3.3.1.1.2a.cmml" xref="S3.Ex1.m1.3.3.3.3.1.1.2"><mtext mathsize="70%" id="S3.Ex1.m1.3.3.3.3.1.1.2.cmml" xref="S3.Ex1.m1.3.3.3.3.1.1.2">uniform</mtext></ci><interval closure="open" id="S3.Ex1.m1.3.3.3.3.1.1.3.1.cmml" xref="S3.Ex1.m1.3.3.3.3.1.1.3.2"><cn type="integer" id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1">1</cn><ci id="S3.Ex1.m1.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2">𝑇</ci></interval></apply><apply id="S3.Ex1.m1.4.4.4.4.2.2.cmml" xref="S3.Ex1.m1.4.4.4.4.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.4.4.2.2.1.cmml" xref="S3.Ex1.m1.4.4.4.4.2.2">subscript</csymbol><ci id="S3.Ex1.m1.4.4.4.4.2.2.2.cmml" xref="S3.Ex1.m1.4.4.4.4.2.2.2">𝑧</ci><cn type="integer" id="S3.Ex1.m1.4.4.4.4.2.2.3.cmml" xref="S3.Ex1.m1.4.4.4.4.2.2.3">0</cn></apply><apply id="S3.Ex1.m1.5.5.5.5.3.3.cmml" xref="S3.Ex1.m1.5.5.5.5.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.5.5.3.3.1.cmml" xref="S3.Ex1.m1.5.5.5.5.3.3">subscript</csymbol><ci id="S3.Ex1.m1.5.5.5.5.3.3.2.cmml" xref="S3.Ex1.m1.5.5.5.5.3.3.2">italic-ϵ</ci><ci id="S3.Ex1.m1.5.5.5.5.3.3.3.cmml" xref="S3.Ex1.m1.5.5.5.5.3.3.3">𝑡</ci></apply></list></apply></apply><apply id="S3.Ex1.m1.7.7.1.1.1.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.7.7.1.1.1.1.2.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1">superscript</csymbol><apply id="S3.Ex1.m1.7.7.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex1.m1.7.7.1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1"><minus id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.4.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.4"></minus><apply id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5">subscript</csymbol><ci id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5.2.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5.2">italic-ϵ</ci><ci id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5.3.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.5.3">𝑡</ci></apply><apply id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3"><times id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.4.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.4"></times><apply id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5"><csymbol cd="ambiguous" id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5">subscript</csymbol><ci id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5.2.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5.2">italic-ϵ</ci><ci id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5.3.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.5.3">𝜃</ci></apply><vector id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3"><apply id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.2">𝐱</ci><ci id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.Ex1.m1.6.6.cmml" xref="S3.Ex1.m1.6.6">𝑡</ci><apply id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2.2">𝑐</ci><ci id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.2.2.2.2.3">𝑣</ci></apply><apply id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3">subscript</csymbol><ci id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3.2">𝑐</ci><ci id="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.1.1.1.3.3.3.3.3">𝑎</ci></apply></vector></apply></apply></apply><cn type="integer" id="S3.Ex1.m1.7.7.1.1.1.1.3.cmml" xref="S3.Ex1.m1.7.7.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.7c">\mathcal{L}=\mathbb{E}_{t\sim\text{uniform}(1,T),z_{0},\epsilon_{t}}\|\epsilon_{t}-\epsilon_{\theta}(\mathbf{x}_{t},t,c_{v},c_{a})\|^{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS4.p3.7" class="ltx_p">where <math id="S3.SS4.p3.4.m1.1" class="ltx_Math" alttext="\epsilon_{t}" display="inline"><semantics id="S3.SS4.p3.4.m1.1a"><msub id="S3.SS4.p3.4.m1.1.1" xref="S3.SS4.p3.4.m1.1.1.cmml"><mi id="S3.SS4.p3.4.m1.1.1.2" xref="S3.SS4.p3.4.m1.1.1.2.cmml">ϵ</mi><mi id="S3.SS4.p3.4.m1.1.1.3" xref="S3.SS4.p3.4.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m1.1b"><apply id="S3.SS4.p3.4.m1.1.1.cmml" xref="S3.SS4.p3.4.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.4.m1.1.1.1.cmml" xref="S3.SS4.p3.4.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.4.m1.1.1.2.cmml" xref="S3.SS4.p3.4.m1.1.1.2">italic-ϵ</ci><ci id="S3.SS4.p3.4.m1.1.1.3.cmml" xref="S3.SS4.p3.4.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m1.1c">\epsilon_{t}</annotation></semantics></math> is the standard Gaussian noise sampled for diffusion step <math id="S3.SS4.p3.5.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS4.p3.5.m2.1a"><mi id="S3.SS4.p3.5.m2.1.1" xref="S3.SS4.p3.5.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m2.1b"><ci id="S3.SS4.p3.5.m2.1.1.cmml" xref="S3.SS4.p3.5.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m2.1c">t</annotation></semantics></math>, and <math id="S3.SS4.p3.6.m3.4" class="ltx_Math" alttext="\epsilon_{\theta}(\mathbf{x}_{t},t,c_{v},c_{a})" display="inline"><semantics id="S3.SS4.p3.6.m3.4a"><mrow id="S3.SS4.p3.6.m3.4.4" xref="S3.SS4.p3.6.m3.4.4.cmml"><msub id="S3.SS4.p3.6.m3.4.4.5" xref="S3.SS4.p3.6.m3.4.4.5.cmml"><mi id="S3.SS4.p3.6.m3.4.4.5.2" xref="S3.SS4.p3.6.m3.4.4.5.2.cmml">ϵ</mi><mi id="S3.SS4.p3.6.m3.4.4.5.3" xref="S3.SS4.p3.6.m3.4.4.5.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS4.p3.6.m3.4.4.4" xref="S3.SS4.p3.6.m3.4.4.4.cmml">​</mo><mrow id="S3.SS4.p3.6.m3.4.4.3.3" xref="S3.SS4.p3.6.m3.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS4.p3.6.m3.4.4.3.3.4" xref="S3.SS4.p3.6.m3.4.4.3.4.cmml">(</mo><msub id="S3.SS4.p3.6.m3.2.2.1.1.1" xref="S3.SS4.p3.6.m3.2.2.1.1.1.cmml"><mi id="S3.SS4.p3.6.m3.2.2.1.1.1.2" xref="S3.SS4.p3.6.m3.2.2.1.1.1.2.cmml">𝐱</mi><mi id="S3.SS4.p3.6.m3.2.2.1.1.1.3" xref="S3.SS4.p3.6.m3.2.2.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS4.p3.6.m3.4.4.3.3.5" xref="S3.SS4.p3.6.m3.4.4.3.4.cmml">,</mo><mi id="S3.SS4.p3.6.m3.1.1" xref="S3.SS4.p3.6.m3.1.1.cmml">t</mi><mo id="S3.SS4.p3.6.m3.4.4.3.3.6" xref="S3.SS4.p3.6.m3.4.4.3.4.cmml">,</mo><msub id="S3.SS4.p3.6.m3.3.3.2.2.2" xref="S3.SS4.p3.6.m3.3.3.2.2.2.cmml"><mi id="S3.SS4.p3.6.m3.3.3.2.2.2.2" xref="S3.SS4.p3.6.m3.3.3.2.2.2.2.cmml">c</mi><mi id="S3.SS4.p3.6.m3.3.3.2.2.2.3" xref="S3.SS4.p3.6.m3.3.3.2.2.2.3.cmml">v</mi></msub><mo id="S3.SS4.p3.6.m3.4.4.3.3.7" xref="S3.SS4.p3.6.m3.4.4.3.4.cmml">,</mo><msub id="S3.SS4.p3.6.m3.4.4.3.3.3" xref="S3.SS4.p3.6.m3.4.4.3.3.3.cmml"><mi id="S3.SS4.p3.6.m3.4.4.3.3.3.2" xref="S3.SS4.p3.6.m3.4.4.3.3.3.2.cmml">c</mi><mi id="S3.SS4.p3.6.m3.4.4.3.3.3.3" xref="S3.SS4.p3.6.m3.4.4.3.3.3.3.cmml">a</mi></msub><mo stretchy="false" id="S3.SS4.p3.6.m3.4.4.3.3.8" xref="S3.SS4.p3.6.m3.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m3.4b"><apply id="S3.SS4.p3.6.m3.4.4.cmml" xref="S3.SS4.p3.6.m3.4.4"><times id="S3.SS4.p3.6.m3.4.4.4.cmml" xref="S3.SS4.p3.6.m3.4.4.4"></times><apply id="S3.SS4.p3.6.m3.4.4.5.cmml" xref="S3.SS4.p3.6.m3.4.4.5"><csymbol cd="ambiguous" id="S3.SS4.p3.6.m3.4.4.5.1.cmml" xref="S3.SS4.p3.6.m3.4.4.5">subscript</csymbol><ci id="S3.SS4.p3.6.m3.4.4.5.2.cmml" xref="S3.SS4.p3.6.m3.4.4.5.2">italic-ϵ</ci><ci id="S3.SS4.p3.6.m3.4.4.5.3.cmml" xref="S3.SS4.p3.6.m3.4.4.5.3">𝜃</ci></apply><vector id="S3.SS4.p3.6.m3.4.4.3.4.cmml" xref="S3.SS4.p3.6.m3.4.4.3.3"><apply id="S3.SS4.p3.6.m3.2.2.1.1.1.cmml" xref="S3.SS4.p3.6.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.6.m3.2.2.1.1.1.1.cmml" xref="S3.SS4.p3.6.m3.2.2.1.1.1">subscript</csymbol><ci id="S3.SS4.p3.6.m3.2.2.1.1.1.2.cmml" xref="S3.SS4.p3.6.m3.2.2.1.1.1.2">𝐱</ci><ci id="S3.SS4.p3.6.m3.2.2.1.1.1.3.cmml" xref="S3.SS4.p3.6.m3.2.2.1.1.1.3">𝑡</ci></apply><ci id="S3.SS4.p3.6.m3.1.1.cmml" xref="S3.SS4.p3.6.m3.1.1">𝑡</ci><apply id="S3.SS4.p3.6.m3.3.3.2.2.2.cmml" xref="S3.SS4.p3.6.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.p3.6.m3.3.3.2.2.2.1.cmml" xref="S3.SS4.p3.6.m3.3.3.2.2.2">subscript</csymbol><ci id="S3.SS4.p3.6.m3.3.3.2.2.2.2.cmml" xref="S3.SS4.p3.6.m3.3.3.2.2.2.2">𝑐</ci><ci id="S3.SS4.p3.6.m3.3.3.2.2.2.3.cmml" xref="S3.SS4.p3.6.m3.3.3.2.2.2.3">𝑣</ci></apply><apply id="S3.SS4.p3.6.m3.4.4.3.3.3.cmml" xref="S3.SS4.p3.6.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS4.p3.6.m3.4.4.3.3.3.1.cmml" xref="S3.SS4.p3.6.m3.4.4.3.3.3">subscript</csymbol><ci id="S3.SS4.p3.6.m3.4.4.3.3.3.2.cmml" xref="S3.SS4.p3.6.m3.4.4.3.3.3.2">𝑐</ci><ci id="S3.SS4.p3.6.m3.4.4.3.3.3.3.cmml" xref="S3.SS4.p3.6.m3.4.4.3.3.3.3">𝑎</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m3.4c">\epsilon_{\theta}(\mathbf{x}_{t},t,c_{v},c_{a})</annotation></semantics></math> is the model estimation of it (<math id="S3.SS4.p3.7.m4.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS4.p3.7.m4.1a"><mi id="S3.SS4.p3.7.m4.1.1" xref="S3.SS4.p3.7.m4.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.7.m4.1b"><ci id="S3.SS4.p3.7.m4.1.1.cmml" xref="S3.SS4.p3.7.m4.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.7.m4.1c">\theta</annotation></semantics></math> represents model parameters).</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.11" class="ltx_p">The reverse process can be parameterized as:</p>
<table id="S7.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex2.m1.3" class="ltx_Math" alttext="\displaystyle p(z_{T})=\mathcal{N}(0,I)," display="inline"><semantics id="S3.Ex2.m1.3a"><mrow id="S3.Ex2.m1.3.3.1" xref="S3.Ex2.m1.3.3.1.1.cmml"><mrow id="S3.Ex2.m1.3.3.1.1" xref="S3.Ex2.m1.3.3.1.1.cmml"><mrow id="S3.Ex2.m1.3.3.1.1.1" xref="S3.Ex2.m1.3.3.1.1.1.cmml"><mi id="S3.Ex2.m1.3.3.1.1.1.3" xref="S3.Ex2.m1.3.3.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.3.3.1.1.1.2" xref="S3.Ex2.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.Ex2.m1.3.3.1.1.1.1.1" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.3.3.1.1.1.1.1.2" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.cmml">(</mo><msub id="S3.Ex2.m1.3.3.1.1.1.1.1.1" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.3.3.1.1.1.1.1.1.2" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.Ex2.m1.3.3.1.1.1.1.1.1.3" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.3.cmml">T</mi></msub><mo stretchy="false" id="S3.Ex2.m1.3.3.1.1.1.1.1.3" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex2.m1.3.3.1.1.2" xref="S3.Ex2.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.Ex2.m1.3.3.1.1.3" xref="S3.Ex2.m1.3.3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.3.3.1.1.3.2" xref="S3.Ex2.m1.3.3.1.1.3.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.3.3.1.1.3.1" xref="S3.Ex2.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="S3.Ex2.m1.3.3.1.1.3.3.2" xref="S3.Ex2.m1.3.3.1.1.3.3.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.3.3.1.1.3.3.2.1" xref="S3.Ex2.m1.3.3.1.1.3.3.1.cmml">(</mo><mn id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml">0</mn><mo id="S3.Ex2.m1.3.3.1.1.3.3.2.2" xref="S3.Ex2.m1.3.3.1.1.3.3.1.cmml">,</mo><mi id="S3.Ex2.m1.2.2" xref="S3.Ex2.m1.2.2.cmml">I</mi><mo stretchy="false" id="S3.Ex2.m1.3.3.1.1.3.3.2.3" xref="S3.Ex2.m1.3.3.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex2.m1.3.3.1.2" xref="S3.Ex2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.3b"><apply id="S3.Ex2.m1.3.3.1.1.cmml" xref="S3.Ex2.m1.3.3.1"><eq id="S3.Ex2.m1.3.3.1.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2"></eq><apply id="S3.Ex2.m1.3.3.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1"><times id="S3.Ex2.m1.3.3.1.1.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.2"></times><ci id="S3.Ex2.m1.3.3.1.1.1.3.cmml" xref="S3.Ex2.m1.3.3.1.1.1.3">𝑝</ci><apply id="S3.Ex2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.Ex2.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1.1.3">𝑇</ci></apply></apply><apply id="S3.Ex2.m1.3.3.1.1.3.cmml" xref="S3.Ex2.m1.3.3.1.1.3"><times id="S3.Ex2.m1.3.3.1.1.3.1.cmml" xref="S3.Ex2.m1.3.3.1.1.3.1"></times><ci id="S3.Ex2.m1.3.3.1.1.3.2.cmml" xref="S3.Ex2.m1.3.3.1.1.3.2">𝒩</ci><interval closure="open" id="S3.Ex2.m1.3.3.1.1.3.3.1.cmml" xref="S3.Ex2.m1.3.3.1.1.3.3.2"><cn type="integer" id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1">0</cn><ci id="S3.Ex2.m1.2.2.cmml" xref="S3.Ex2.m1.2.2">𝐼</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.3c">\displaystyle p(z_{T})=\mathcal{N}(0,I),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex3.m1.2" class="ltx_Math" alttext="\displaystyle p_{\theta}(z_{t-1}|z_{t})=\mathcal{N}(z_{t-1};\frac{1}{\sqrt{\alpha_{t}}}\Big{(}z_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{\theta}(z_{t},t,c_{v},c_{a})\Big{)},\sigma_{t}^{2}I)," display="inline"><semantics id="S3.Ex3.m1.2a"><mrow id="S3.Ex3.m1.2.2.1" xref="S3.Ex3.m1.2.2.1.1.cmml"><mrow id="S3.Ex3.m1.2.2.1.1" xref="S3.Ex3.m1.2.2.1.1.cmml"><mrow id="S3.Ex3.m1.2.2.1.1.1" xref="S3.Ex3.m1.2.2.1.1.1.cmml"><msub id="S3.Ex3.m1.2.2.1.1.1.3" xref="S3.Ex3.m1.2.2.1.1.1.3.cmml"><mi id="S3.Ex3.m1.2.2.1.1.1.3.2" xref="S3.Ex3.m1.2.2.1.1.1.3.2.cmml">p</mi><mi id="S3.Ex3.m1.2.2.1.1.1.3.3" xref="S3.Ex3.m1.2.2.1.1.1.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.2.2.1.1.1.2" xref="S3.Ex3.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.Ex3.m1.2.2.1.1.1.1.1" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex3.m1.2.2.1.1.1.1.1.2" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex3.m1.2.2.1.1.1.1.1.1" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.cmml"><msub id="S3.Ex3.m1.2.2.1.1.1.1.1.1.2" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.2.cmml">z</mi><mrow id="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.cmml"><mi id="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.2" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.2.cmml">t</mi><mo id="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.1" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.1.cmml">−</mo><mn id="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.3" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo fence="false" id="S3.Ex3.m1.2.2.1.1.1.1.1.1.1" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.Ex3.m1.2.2.1.1.1.1.1.1.3" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex3.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.3.2.cmml">z</mi><mi id="S3.Ex3.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo stretchy="false" id="S3.Ex3.m1.2.2.1.1.1.1.1.3" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex3.m1.2.2.1.1.5" xref="S3.Ex3.m1.2.2.1.1.5.cmml">=</mo><mrow id="S3.Ex3.m1.2.2.1.1.4" xref="S3.Ex3.m1.2.2.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex3.m1.2.2.1.1.4.5" xref="S3.Ex3.m1.2.2.1.1.4.5.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.2.2.1.1.4.4" xref="S3.Ex3.m1.2.2.1.1.4.4.cmml">​</mo><mrow id="S3.Ex3.m1.2.2.1.1.4.3.3" xref="S3.Ex3.m1.2.2.1.1.4.3.4.cmml"><mo stretchy="false" id="S3.Ex3.m1.2.2.1.1.4.3.3.4" xref="S3.Ex3.m1.2.2.1.1.4.3.4.cmml">(</mo><msub id="S3.Ex3.m1.2.2.1.1.2.1.1.1" xref="S3.Ex3.m1.2.2.1.1.2.1.1.1.cmml"><mi id="S3.Ex3.m1.2.2.1.1.2.1.1.1.2" xref="S3.Ex3.m1.2.2.1.1.2.1.1.1.2.cmml">z</mi><mrow id="S3.Ex3.m1.2.2.1.1.2.1.1.1.3" xref="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.cmml"><mi id="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.2" xref="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.2.cmml">t</mi><mo id="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.1" xref="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.1.cmml">−</mo><mn id="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.3" xref="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.Ex3.m1.2.2.1.1.4.3.3.5" xref="S3.Ex3.m1.2.2.1.1.4.3.4.cmml">;</mo><mrow id="S3.Ex3.m1.2.2.1.1.3.2.2.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.cmml"><mstyle displaystyle="true" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.cmml"><mfrac id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3a" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.cmml"><mn id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.2.cmml">1</mn><msqrt id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.cmml"><msub id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2.cmml"><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2.2.cmml">α</mi><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2.3.cmml">t</mi></msub></msqrt></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.2.cmml">​</mo><mrow id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.cmml"><mo maxsize="160%" minsize="160%" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.cmml"><msub id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5.cmml"><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5.2.cmml">z</mi><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5.3.cmml">t</mi></msub><mo id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.4" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.4.cmml">−</mo><mrow id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.cmml"><mstyle displaystyle="true" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.cmml"><mfrac id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5a" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.cmml"><mrow id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.cmml"><mn id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.2.cmml">1</mn><mo id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.1" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.1.cmml">−</mo><msub id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3.cmml"><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3.2.cmml">α</mi><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3.3.cmml">t</mi></msub></mrow><msqrt id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.cmml"><mrow id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.cmml"><mn id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.2.cmml">1</mn><mo id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.1" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.1.cmml">−</mo><msub id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.cmml"><mover accent="true" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.2.cmml"><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.2.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.2.2.cmml">α</mi><mo id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.2.1" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.2.1.cmml">¯</mo></mover><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.3.cmml">t</mi></msub></mrow></msqrt></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.4" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.4.cmml">​</mo><msub id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6.cmml"><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6.2.cmml">ϵ</mi><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.4a" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.4.cmml">​</mo><mrow id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.4.cmml"><mo stretchy="false" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.4" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.4.cmml">(</mo><msub id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.5" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.4.cmml">,</mo><mi id="S3.Ex3.m1.1.1" xref="S3.Ex3.m1.1.1.cmml">t</mi><mo id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.6" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.4.cmml">,</mo><msub id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2.cmml"><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2.2.cmml">c</mi><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2.3.cmml">v</mi></msub><mo id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.7" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.4.cmml">,</mo><msub id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3.cmml"><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3.2" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3.2.cmml">c</mi><mi id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3.3.cmml">a</mi></msub><mo stretchy="false" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.8" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.4.cmml">)</mo></mrow></mrow></mrow><mo maxsize="160%" minsize="160%" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.3" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex3.m1.2.2.1.1.4.3.3.6" xref="S3.Ex3.m1.2.2.1.1.4.3.4.cmml">,</mo><mrow id="S3.Ex3.m1.2.2.1.1.4.3.3.3" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.cmml"><msubsup id="S3.Ex3.m1.2.2.1.1.4.3.3.3.2" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.cmml"><mi id="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.2.2" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.2.2.cmml">σ</mi><mi id="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.2.3" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.2.3.cmml">t</mi><mn id="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.3" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.3.cmml">2</mn></msubsup><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.2.2.1.1.4.3.3.3.1" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.1.cmml">​</mo><mi id="S3.Ex3.m1.2.2.1.1.4.3.3.3.3" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.3.cmml">I</mi></mrow><mo stretchy="false" id="S3.Ex3.m1.2.2.1.1.4.3.3.7" xref="S3.Ex3.m1.2.2.1.1.4.3.4.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex3.m1.2.2.1.2" xref="S3.Ex3.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.2b"><apply id="S3.Ex3.m1.2.2.1.1.cmml" xref="S3.Ex3.m1.2.2.1"><eq id="S3.Ex3.m1.2.2.1.1.5.cmml" xref="S3.Ex3.m1.2.2.1.1.5"></eq><apply id="S3.Ex3.m1.2.2.1.1.1.cmml" xref="S3.Ex3.m1.2.2.1.1.1"><times id="S3.Ex3.m1.2.2.1.1.1.2.cmml" xref="S3.Ex3.m1.2.2.1.1.1.2"></times><apply id="S3.Ex3.m1.2.2.1.1.1.3.cmml" xref="S3.Ex3.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.1.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.1.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.1.3.2">𝑝</ci><ci id="S3.Ex3.m1.2.2.1.1.1.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.1.3.3">𝜃</ci></apply><apply id="S3.Ex3.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.2">𝑧</ci><apply id="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3"><minus id="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.1"></minus><ci id="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.2">𝑡</ci><cn type="integer" id="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.Ex3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.3.2">𝑧</ci><ci id="S3.Ex3.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply><apply id="S3.Ex3.m1.2.2.1.1.4.cmml" xref="S3.Ex3.m1.2.2.1.1.4"><times id="S3.Ex3.m1.2.2.1.1.4.4.cmml" xref="S3.Ex3.m1.2.2.1.1.4.4"></times><ci id="S3.Ex3.m1.2.2.1.1.4.5.cmml" xref="S3.Ex3.m1.2.2.1.1.4.5">𝒩</ci><list id="S3.Ex3.m1.2.2.1.1.4.3.4.cmml" xref="S3.Ex3.m1.2.2.1.1.4.3.3"><apply id="S3.Ex3.m1.2.2.1.1.2.1.1.1.cmml" xref="S3.Ex3.m1.2.2.1.1.2.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.2.1.1.1.1.cmml" xref="S3.Ex3.m1.2.2.1.1.2.1.1.1">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.2.1.1.1.2.cmml" xref="S3.Ex3.m1.2.2.1.1.2.1.1.1.2">𝑧</ci><apply id="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.cmml" xref="S3.Ex3.m1.2.2.1.1.2.1.1.1.3"><minus id="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.1"></minus><ci id="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.2">𝑡</ci><cn type="integer" id="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.2.1.1.1.3.3">1</cn></apply></apply><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2"><times id="S3.Ex3.m1.2.2.1.1.3.2.2.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.2"></times><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3"><divide id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3"></divide><cn type="integer" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.2">1</cn><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3"><root id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3a.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3"></root><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2.2">𝛼</ci><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.3.3.2.3">𝑡</ci></apply></apply></apply><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1"><minus id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.4.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.4"></minus><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5.2">𝑧</ci><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.5.3">𝑡</ci></apply><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3"><times id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.4.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.4"></times><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5"><divide id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5"></divide><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2"><minus id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.1"></minus><cn type="integer" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.2">1</cn><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3.2">𝛼</ci><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.2.3.3">𝑡</ci></apply></apply><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3"><root id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3a.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3"></root><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2"><minus id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.1"></minus><cn type="integer" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.2">1</cn><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3">subscript</csymbol><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.2"><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.2.1">¯</ci><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.2.2">𝛼</ci></apply><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.5.3.2.3.3">𝑡</ci></apply></apply></apply></apply><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6.2">italic-ϵ</ci><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.6.3">𝜃</ci></apply><vector id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.4.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3"><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.Ex3.m1.1.1.cmml" xref="S3.Ex3.m1.1.1">𝑡</ci><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2.2">𝑐</ci><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.2.2.2.2.3">𝑣</ci></apply><apply id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3.2">𝑐</ci><ci id="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.3.2.2.2.1.1.1.3.3.3.3.3">𝑎</ci></apply></vector></apply></apply></apply><apply id="S3.Ex3.m1.2.2.1.1.4.3.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3"><times id="S3.Ex3.m1.2.2.1.1.4.3.3.3.1.cmml" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.1"></times><apply id="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.cmml" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.2">superscript</csymbol><apply id="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.2.1.cmml" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.2">subscript</csymbol><ci id="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.2.2.cmml" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.2.2">𝜎</ci><ci id="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.2.3">𝑡</ci></apply><cn type="integer" id="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.3.cmml" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.2.3">2</cn></apply><ci id="S3.Ex3.m1.2.2.1.1.4.3.3.3.3.cmml" xref="S3.Ex3.m1.2.2.1.1.4.3.3.3.3">𝐼</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.2c">\displaystyle p_{\theta}(z_{t-1}|z_{t})=\mathcal{N}(z_{t-1};\frac{1}{\sqrt{\alpha_{t}}}\Big{(}z_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\epsilon_{\theta}(z_{t},t,c_{v},c_{a})\Big{)},\sigma_{t}^{2}I),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS4.p4.4" class="ltx_p">where <math id="S3.SS4.p4.1.m1.1" class="ltx_Math" alttext="\alpha_{t}" display="inline"><semantics id="S3.SS4.p4.1.m1.1a"><msub id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml"><mi id="S3.SS4.p4.1.m1.1.1.2" xref="S3.SS4.p4.1.m1.1.1.2.cmml">α</mi><mi id="S3.SS4.p4.1.m1.1.1.3" xref="S3.SS4.p4.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><apply id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.1.m1.1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p4.1.m1.1.1.2.cmml" xref="S3.SS4.p4.1.m1.1.1.2">𝛼</ci><ci id="S3.SS4.p4.1.m1.1.1.3.cmml" xref="S3.SS4.p4.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">\alpha_{t}</annotation></semantics></math> and <math id="S3.SS4.p4.2.m2.1" class="ltx_Math" alttext="\sigma_{t}" display="inline"><semantics id="S3.SS4.p4.2.m2.1a"><msub id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml"><mi id="S3.SS4.p4.2.m2.1.1.2" xref="S3.SS4.p4.2.m2.1.1.2.cmml">σ</mi><mi id="S3.SS4.p4.2.m2.1.1.3" xref="S3.SS4.p4.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><apply id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.2.m2.1.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.p4.2.m2.1.1.2.cmml" xref="S3.SS4.p4.2.m2.1.1.2">𝜎</ci><ci id="S3.SS4.p4.2.m2.1.1.3.cmml" xref="S3.SS4.p4.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">\sigma_{t}</annotation></semantics></math> are determined by noise schedule of the diffusion process. To generate audio during inference, we first sample standard Gaussian noise <math id="S3.SS4.p4.3.m3.1" class="ltx_Math" alttext="z_{T}" display="inline"><semantics id="S3.SS4.p4.3.m3.1a"><msub id="S3.SS4.p4.3.m3.1.1" xref="S3.SS4.p4.3.m3.1.1.cmml"><mi id="S3.SS4.p4.3.m3.1.1.2" xref="S3.SS4.p4.3.m3.1.1.2.cmml">z</mi><mi id="S3.SS4.p4.3.m3.1.1.3" xref="S3.SS4.p4.3.m3.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.3.m3.1b"><apply id="S3.SS4.p4.3.m3.1.1.cmml" xref="S3.SS4.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.3.m3.1.1.1.cmml" xref="S3.SS4.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p4.3.m3.1.1.2.cmml" xref="S3.SS4.p4.3.m3.1.1.2">𝑧</ci><ci id="S3.SS4.p4.3.m3.1.1.3.cmml" xref="S3.SS4.p4.3.m3.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.3.m3.1c">z_{T}</annotation></semantics></math>, and then apply classifier free guidance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> to estimate <math id="S3.SS4.p4.4.m4.1" class="ltx_Math" alttext="\tilde{\epsilon}_{\theta}" display="inline"><semantics id="S3.SS4.p4.4.m4.1a"><msub id="S3.SS4.p4.4.m4.1.1" xref="S3.SS4.p4.4.m4.1.1.cmml"><mover accent="true" id="S3.SS4.p4.4.m4.1.1.2" xref="S3.SS4.p4.4.m4.1.1.2.cmml"><mi id="S3.SS4.p4.4.m4.1.1.2.2" xref="S3.SS4.p4.4.m4.1.1.2.2.cmml">ϵ</mi><mo id="S3.SS4.p4.4.m4.1.1.2.1" xref="S3.SS4.p4.4.m4.1.1.2.1.cmml">~</mo></mover><mi id="S3.SS4.p4.4.m4.1.1.3" xref="S3.SS4.p4.4.m4.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.4.m4.1b"><apply id="S3.SS4.p4.4.m4.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.4.m4.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1">subscript</csymbol><apply id="S3.SS4.p4.4.m4.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.2"><ci id="S3.SS4.p4.4.m4.1.1.2.1.cmml" xref="S3.SS4.p4.4.m4.1.1.2.1">~</ci><ci id="S3.SS4.p4.4.m4.1.1.2.2.cmml" xref="S3.SS4.p4.4.m4.1.1.2.2">italic-ϵ</ci></apply><ci id="S3.SS4.p4.4.m4.1.1.3.cmml" xref="S3.SS4.p4.4.m4.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.4.m4.1c">\tilde{\epsilon}_{\theta}</annotation></semantics></math> as</p>
<table id="S3.Ex4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex4.m1.6" class="ltx_Math" alttext="\tilde{\epsilon}_{t}(z_{t},t,c_{v},c_{a})=\omega\epsilon_{\theta}(z_{t},t,c_{v},c_{a})+(1-\omega)\epsilon_{\theta}(z_{t},t,\emptyset,\emptyset)," display="block"><semantics id="S3.Ex4.m1.6a"><mrow id="S3.Ex4.m1.6.6.1" xref="S3.Ex4.m1.6.6.1.1.cmml"><mrow id="S3.Ex4.m1.6.6.1.1" xref="S3.Ex4.m1.6.6.1.1.cmml"><mrow id="S3.Ex4.m1.6.6.1.1.3" xref="S3.Ex4.m1.6.6.1.1.3.cmml"><msub id="S3.Ex4.m1.6.6.1.1.3.5" xref="S3.Ex4.m1.6.6.1.1.3.5.cmml"><mover accent="true" id="S3.Ex4.m1.6.6.1.1.3.5.2" xref="S3.Ex4.m1.6.6.1.1.3.5.2.cmml"><mi id="S3.Ex4.m1.6.6.1.1.3.5.2.2" xref="S3.Ex4.m1.6.6.1.1.3.5.2.2.cmml">ϵ</mi><mo id="S3.Ex4.m1.6.6.1.1.3.5.2.1" xref="S3.Ex4.m1.6.6.1.1.3.5.2.1.cmml">~</mo></mover><mi id="S3.Ex4.m1.6.6.1.1.3.5.3" xref="S3.Ex4.m1.6.6.1.1.3.5.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.6.6.1.1.3.4" xref="S3.Ex4.m1.6.6.1.1.3.4.cmml">​</mo><mrow id="S3.Ex4.m1.6.6.1.1.3.3.3" xref="S3.Ex4.m1.6.6.1.1.3.3.4.cmml"><mo stretchy="false" id="S3.Ex4.m1.6.6.1.1.3.3.3.4" xref="S3.Ex4.m1.6.6.1.1.3.3.4.cmml">(</mo><msub id="S3.Ex4.m1.6.6.1.1.1.1.1.1" xref="S3.Ex4.m1.6.6.1.1.1.1.1.1.cmml"><mi id="S3.Ex4.m1.6.6.1.1.1.1.1.1.2" xref="S3.Ex4.m1.6.6.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.Ex4.m1.6.6.1.1.1.1.1.1.3" xref="S3.Ex4.m1.6.6.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.Ex4.m1.6.6.1.1.3.3.3.5" xref="S3.Ex4.m1.6.6.1.1.3.3.4.cmml">,</mo><mi id="S3.Ex4.m1.1.1" xref="S3.Ex4.m1.1.1.cmml">t</mi><mo id="S3.Ex4.m1.6.6.1.1.3.3.3.6" xref="S3.Ex4.m1.6.6.1.1.3.3.4.cmml">,</mo><msub id="S3.Ex4.m1.6.6.1.1.2.2.2.2" xref="S3.Ex4.m1.6.6.1.1.2.2.2.2.cmml"><mi id="S3.Ex4.m1.6.6.1.1.2.2.2.2.2" xref="S3.Ex4.m1.6.6.1.1.2.2.2.2.2.cmml">c</mi><mi id="S3.Ex4.m1.6.6.1.1.2.2.2.2.3" xref="S3.Ex4.m1.6.6.1.1.2.2.2.2.3.cmml">v</mi></msub><mo id="S3.Ex4.m1.6.6.1.1.3.3.3.7" xref="S3.Ex4.m1.6.6.1.1.3.3.4.cmml">,</mo><msub id="S3.Ex4.m1.6.6.1.1.3.3.3.3" xref="S3.Ex4.m1.6.6.1.1.3.3.3.3.cmml"><mi id="S3.Ex4.m1.6.6.1.1.3.3.3.3.2" xref="S3.Ex4.m1.6.6.1.1.3.3.3.3.2.cmml">c</mi><mi id="S3.Ex4.m1.6.6.1.1.3.3.3.3.3" xref="S3.Ex4.m1.6.6.1.1.3.3.3.3.3.cmml">a</mi></msub><mo stretchy="false" id="S3.Ex4.m1.6.6.1.1.3.3.3.8" xref="S3.Ex4.m1.6.6.1.1.3.3.4.cmml">)</mo></mrow></mrow><mo id="S3.Ex4.m1.6.6.1.1.9" xref="S3.Ex4.m1.6.6.1.1.9.cmml">=</mo><mrow id="S3.Ex4.m1.6.6.1.1.8" xref="S3.Ex4.m1.6.6.1.1.8.cmml"><mrow id="S3.Ex4.m1.6.6.1.1.6.3" xref="S3.Ex4.m1.6.6.1.1.6.3.cmml"><mi id="S3.Ex4.m1.6.6.1.1.6.3.5" xref="S3.Ex4.m1.6.6.1.1.6.3.5.cmml">ω</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.6.6.1.1.6.3.4" xref="S3.Ex4.m1.6.6.1.1.6.3.4.cmml">​</mo><msub id="S3.Ex4.m1.6.6.1.1.6.3.6" xref="S3.Ex4.m1.6.6.1.1.6.3.6.cmml"><mi id="S3.Ex4.m1.6.6.1.1.6.3.6.2" xref="S3.Ex4.m1.6.6.1.1.6.3.6.2.cmml">ϵ</mi><mi id="S3.Ex4.m1.6.6.1.1.6.3.6.3" xref="S3.Ex4.m1.6.6.1.1.6.3.6.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.6.6.1.1.6.3.4a" xref="S3.Ex4.m1.6.6.1.1.6.3.4.cmml">​</mo><mrow id="S3.Ex4.m1.6.6.1.1.6.3.3.3" xref="S3.Ex4.m1.6.6.1.1.6.3.3.4.cmml"><mo stretchy="false" id="S3.Ex4.m1.6.6.1.1.6.3.3.3.4" xref="S3.Ex4.m1.6.6.1.1.6.3.3.4.cmml">(</mo><msub id="S3.Ex4.m1.6.6.1.1.4.1.1.1.1" xref="S3.Ex4.m1.6.6.1.1.4.1.1.1.1.cmml"><mi id="S3.Ex4.m1.6.6.1.1.4.1.1.1.1.2" xref="S3.Ex4.m1.6.6.1.1.4.1.1.1.1.2.cmml">z</mi><mi id="S3.Ex4.m1.6.6.1.1.4.1.1.1.1.3" xref="S3.Ex4.m1.6.6.1.1.4.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.Ex4.m1.6.6.1.1.6.3.3.3.5" xref="S3.Ex4.m1.6.6.1.1.6.3.3.4.cmml">,</mo><mi id="S3.Ex4.m1.2.2" xref="S3.Ex4.m1.2.2.cmml">t</mi><mo id="S3.Ex4.m1.6.6.1.1.6.3.3.3.6" xref="S3.Ex4.m1.6.6.1.1.6.3.3.4.cmml">,</mo><msub id="S3.Ex4.m1.6.6.1.1.5.2.2.2.2" xref="S3.Ex4.m1.6.6.1.1.5.2.2.2.2.cmml"><mi id="S3.Ex4.m1.6.6.1.1.5.2.2.2.2.2" xref="S3.Ex4.m1.6.6.1.1.5.2.2.2.2.2.cmml">c</mi><mi id="S3.Ex4.m1.6.6.1.1.5.2.2.2.2.3" xref="S3.Ex4.m1.6.6.1.1.5.2.2.2.2.3.cmml">v</mi></msub><mo id="S3.Ex4.m1.6.6.1.1.6.3.3.3.7" xref="S3.Ex4.m1.6.6.1.1.6.3.3.4.cmml">,</mo><msub id="S3.Ex4.m1.6.6.1.1.6.3.3.3.3" xref="S3.Ex4.m1.6.6.1.1.6.3.3.3.3.cmml"><mi id="S3.Ex4.m1.6.6.1.1.6.3.3.3.3.2" xref="S3.Ex4.m1.6.6.1.1.6.3.3.3.3.2.cmml">c</mi><mi id="S3.Ex4.m1.6.6.1.1.6.3.3.3.3.3" xref="S3.Ex4.m1.6.6.1.1.6.3.3.3.3.3.cmml">a</mi></msub><mo stretchy="false" id="S3.Ex4.m1.6.6.1.1.6.3.3.3.8" xref="S3.Ex4.m1.6.6.1.1.6.3.3.4.cmml">)</mo></mrow></mrow><mo id="S3.Ex4.m1.6.6.1.1.8.6" xref="S3.Ex4.m1.6.6.1.1.8.6.cmml">+</mo><mrow id="S3.Ex4.m1.6.6.1.1.8.5" xref="S3.Ex4.m1.6.6.1.1.8.5.cmml"><mrow id="S3.Ex4.m1.6.6.1.1.7.4.1.1" xref="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.cmml"><mo stretchy="false" id="S3.Ex4.m1.6.6.1.1.7.4.1.1.2" xref="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.cmml">(</mo><mrow id="S3.Ex4.m1.6.6.1.1.7.4.1.1.1" xref="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.cmml"><mn id="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.2" xref="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.2.cmml">1</mn><mo id="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.1" xref="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.1.cmml">−</mo><mi id="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.3" xref="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.3.cmml">ω</mi></mrow><mo stretchy="false" id="S3.Ex4.m1.6.6.1.1.7.4.1.1.3" xref="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.6.6.1.1.8.5.3" xref="S3.Ex4.m1.6.6.1.1.8.5.3.cmml">​</mo><msub id="S3.Ex4.m1.6.6.1.1.8.5.4" xref="S3.Ex4.m1.6.6.1.1.8.5.4.cmml"><mi id="S3.Ex4.m1.6.6.1.1.8.5.4.2" xref="S3.Ex4.m1.6.6.1.1.8.5.4.2.cmml">ϵ</mi><mi id="S3.Ex4.m1.6.6.1.1.8.5.4.3" xref="S3.Ex4.m1.6.6.1.1.8.5.4.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.6.6.1.1.8.5.3a" xref="S3.Ex4.m1.6.6.1.1.8.5.3.cmml">​</mo><mrow id="S3.Ex4.m1.6.6.1.1.8.5.2.1" xref="S3.Ex4.m1.6.6.1.1.8.5.2.2.cmml"><mo stretchy="false" id="S3.Ex4.m1.6.6.1.1.8.5.2.1.2" xref="S3.Ex4.m1.6.6.1.1.8.5.2.2.cmml">(</mo><msub id="S3.Ex4.m1.6.6.1.1.8.5.2.1.1" xref="S3.Ex4.m1.6.6.1.1.8.5.2.1.1.cmml"><mi id="S3.Ex4.m1.6.6.1.1.8.5.2.1.1.2" xref="S3.Ex4.m1.6.6.1.1.8.5.2.1.1.2.cmml">z</mi><mi id="S3.Ex4.m1.6.6.1.1.8.5.2.1.1.3" xref="S3.Ex4.m1.6.6.1.1.8.5.2.1.1.3.cmml">t</mi></msub><mo id="S3.Ex4.m1.6.6.1.1.8.5.2.1.3" xref="S3.Ex4.m1.6.6.1.1.8.5.2.2.cmml">,</mo><mi id="S3.Ex4.m1.3.3" xref="S3.Ex4.m1.3.3.cmml">t</mi><mo id="S3.Ex4.m1.6.6.1.1.8.5.2.1.4" xref="S3.Ex4.m1.6.6.1.1.8.5.2.2.cmml">,</mo><mi mathvariant="normal" id="S3.Ex4.m1.4.4" xref="S3.Ex4.m1.4.4.cmml">∅</mi><mo id="S3.Ex4.m1.6.6.1.1.8.5.2.1.5" xref="S3.Ex4.m1.6.6.1.1.8.5.2.2.cmml">,</mo><mi mathvariant="normal" id="S3.Ex4.m1.5.5" xref="S3.Ex4.m1.5.5.cmml">∅</mi><mo stretchy="false" id="S3.Ex4.m1.6.6.1.1.8.5.2.1.6" xref="S3.Ex4.m1.6.6.1.1.8.5.2.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.Ex4.m1.6.6.1.2" xref="S3.Ex4.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex4.m1.6b"><apply id="S3.Ex4.m1.6.6.1.1.cmml" xref="S3.Ex4.m1.6.6.1"><eq id="S3.Ex4.m1.6.6.1.1.9.cmml" xref="S3.Ex4.m1.6.6.1.1.9"></eq><apply id="S3.Ex4.m1.6.6.1.1.3.cmml" xref="S3.Ex4.m1.6.6.1.1.3"><times id="S3.Ex4.m1.6.6.1.1.3.4.cmml" xref="S3.Ex4.m1.6.6.1.1.3.4"></times><apply id="S3.Ex4.m1.6.6.1.1.3.5.cmml" xref="S3.Ex4.m1.6.6.1.1.3.5"><csymbol cd="ambiguous" id="S3.Ex4.m1.6.6.1.1.3.5.1.cmml" xref="S3.Ex4.m1.6.6.1.1.3.5">subscript</csymbol><apply id="S3.Ex4.m1.6.6.1.1.3.5.2.cmml" xref="S3.Ex4.m1.6.6.1.1.3.5.2"><ci id="S3.Ex4.m1.6.6.1.1.3.5.2.1.cmml" xref="S3.Ex4.m1.6.6.1.1.3.5.2.1">~</ci><ci id="S3.Ex4.m1.6.6.1.1.3.5.2.2.cmml" xref="S3.Ex4.m1.6.6.1.1.3.5.2.2">italic-ϵ</ci></apply><ci id="S3.Ex4.m1.6.6.1.1.3.5.3.cmml" xref="S3.Ex4.m1.6.6.1.1.3.5.3">𝑡</ci></apply><vector id="S3.Ex4.m1.6.6.1.1.3.3.4.cmml" xref="S3.Ex4.m1.6.6.1.1.3.3.3"><apply id="S3.Ex4.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.6.6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.6.6.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex4.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S3.Ex4.m1.6.6.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.Ex4.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S3.Ex4.m1.6.6.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.Ex4.m1.1.1.cmml" xref="S3.Ex4.m1.1.1">𝑡</ci><apply id="S3.Ex4.m1.6.6.1.1.2.2.2.2.cmml" xref="S3.Ex4.m1.6.6.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.6.6.1.1.2.2.2.2.1.cmml" xref="S3.Ex4.m1.6.6.1.1.2.2.2.2">subscript</csymbol><ci id="S3.Ex4.m1.6.6.1.1.2.2.2.2.2.cmml" xref="S3.Ex4.m1.6.6.1.1.2.2.2.2.2">𝑐</ci><ci id="S3.Ex4.m1.6.6.1.1.2.2.2.2.3.cmml" xref="S3.Ex4.m1.6.6.1.1.2.2.2.2.3">𝑣</ci></apply><apply id="S3.Ex4.m1.6.6.1.1.3.3.3.3.cmml" xref="S3.Ex4.m1.6.6.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.Ex4.m1.6.6.1.1.3.3.3.3.1.cmml" xref="S3.Ex4.m1.6.6.1.1.3.3.3.3">subscript</csymbol><ci id="S3.Ex4.m1.6.6.1.1.3.3.3.3.2.cmml" xref="S3.Ex4.m1.6.6.1.1.3.3.3.3.2">𝑐</ci><ci id="S3.Ex4.m1.6.6.1.1.3.3.3.3.3.cmml" xref="S3.Ex4.m1.6.6.1.1.3.3.3.3.3">𝑎</ci></apply></vector></apply><apply id="S3.Ex4.m1.6.6.1.1.8.cmml" xref="S3.Ex4.m1.6.6.1.1.8"><plus id="S3.Ex4.m1.6.6.1.1.8.6.cmml" xref="S3.Ex4.m1.6.6.1.1.8.6"></plus><apply id="S3.Ex4.m1.6.6.1.1.6.3.cmml" xref="S3.Ex4.m1.6.6.1.1.6.3"><times id="S3.Ex4.m1.6.6.1.1.6.3.4.cmml" xref="S3.Ex4.m1.6.6.1.1.6.3.4"></times><ci id="S3.Ex4.m1.6.6.1.1.6.3.5.cmml" xref="S3.Ex4.m1.6.6.1.1.6.3.5">𝜔</ci><apply id="S3.Ex4.m1.6.6.1.1.6.3.6.cmml" xref="S3.Ex4.m1.6.6.1.1.6.3.6"><csymbol cd="ambiguous" id="S3.Ex4.m1.6.6.1.1.6.3.6.1.cmml" xref="S3.Ex4.m1.6.6.1.1.6.3.6">subscript</csymbol><ci id="S3.Ex4.m1.6.6.1.1.6.3.6.2.cmml" xref="S3.Ex4.m1.6.6.1.1.6.3.6.2">italic-ϵ</ci><ci id="S3.Ex4.m1.6.6.1.1.6.3.6.3.cmml" xref="S3.Ex4.m1.6.6.1.1.6.3.6.3">𝜃</ci></apply><vector id="S3.Ex4.m1.6.6.1.1.6.3.3.4.cmml" xref="S3.Ex4.m1.6.6.1.1.6.3.3.3"><apply id="S3.Ex4.m1.6.6.1.1.4.1.1.1.1.cmml" xref="S3.Ex4.m1.6.6.1.1.4.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.6.6.1.1.4.1.1.1.1.1.cmml" xref="S3.Ex4.m1.6.6.1.1.4.1.1.1.1">subscript</csymbol><ci id="S3.Ex4.m1.6.6.1.1.4.1.1.1.1.2.cmml" xref="S3.Ex4.m1.6.6.1.1.4.1.1.1.1.2">𝑧</ci><ci id="S3.Ex4.m1.6.6.1.1.4.1.1.1.1.3.cmml" xref="S3.Ex4.m1.6.6.1.1.4.1.1.1.1.3">𝑡</ci></apply><ci id="S3.Ex4.m1.2.2.cmml" xref="S3.Ex4.m1.2.2">𝑡</ci><apply id="S3.Ex4.m1.6.6.1.1.5.2.2.2.2.cmml" xref="S3.Ex4.m1.6.6.1.1.5.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.6.6.1.1.5.2.2.2.2.1.cmml" xref="S3.Ex4.m1.6.6.1.1.5.2.2.2.2">subscript</csymbol><ci id="S3.Ex4.m1.6.6.1.1.5.2.2.2.2.2.cmml" xref="S3.Ex4.m1.6.6.1.1.5.2.2.2.2.2">𝑐</ci><ci id="S3.Ex4.m1.6.6.1.1.5.2.2.2.2.3.cmml" xref="S3.Ex4.m1.6.6.1.1.5.2.2.2.2.3">𝑣</ci></apply><apply id="S3.Ex4.m1.6.6.1.1.6.3.3.3.3.cmml" xref="S3.Ex4.m1.6.6.1.1.6.3.3.3.3"><csymbol cd="ambiguous" id="S3.Ex4.m1.6.6.1.1.6.3.3.3.3.1.cmml" xref="S3.Ex4.m1.6.6.1.1.6.3.3.3.3">subscript</csymbol><ci id="S3.Ex4.m1.6.6.1.1.6.3.3.3.3.2.cmml" xref="S3.Ex4.m1.6.6.1.1.6.3.3.3.3.2">𝑐</ci><ci id="S3.Ex4.m1.6.6.1.1.6.3.3.3.3.3.cmml" xref="S3.Ex4.m1.6.6.1.1.6.3.3.3.3.3">𝑎</ci></apply></vector></apply><apply id="S3.Ex4.m1.6.6.1.1.8.5.cmml" xref="S3.Ex4.m1.6.6.1.1.8.5"><times id="S3.Ex4.m1.6.6.1.1.8.5.3.cmml" xref="S3.Ex4.m1.6.6.1.1.8.5.3"></times><apply id="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.cmml" xref="S3.Ex4.m1.6.6.1.1.7.4.1.1"><minus id="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.1.cmml" xref="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.1"></minus><cn type="integer" id="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.2.cmml" xref="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.2">1</cn><ci id="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.3.cmml" xref="S3.Ex4.m1.6.6.1.1.7.4.1.1.1.3">𝜔</ci></apply><apply id="S3.Ex4.m1.6.6.1.1.8.5.4.cmml" xref="S3.Ex4.m1.6.6.1.1.8.5.4"><csymbol cd="ambiguous" id="S3.Ex4.m1.6.6.1.1.8.5.4.1.cmml" xref="S3.Ex4.m1.6.6.1.1.8.5.4">subscript</csymbol><ci id="S3.Ex4.m1.6.6.1.1.8.5.4.2.cmml" xref="S3.Ex4.m1.6.6.1.1.8.5.4.2">italic-ϵ</ci><ci id="S3.Ex4.m1.6.6.1.1.8.5.4.3.cmml" xref="S3.Ex4.m1.6.6.1.1.8.5.4.3">𝜃</ci></apply><vector id="S3.Ex4.m1.6.6.1.1.8.5.2.2.cmml" xref="S3.Ex4.m1.6.6.1.1.8.5.2.1"><apply id="S3.Ex4.m1.6.6.1.1.8.5.2.1.1.cmml" xref="S3.Ex4.m1.6.6.1.1.8.5.2.1.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.6.6.1.1.8.5.2.1.1.1.cmml" xref="S3.Ex4.m1.6.6.1.1.8.5.2.1.1">subscript</csymbol><ci id="S3.Ex4.m1.6.6.1.1.8.5.2.1.1.2.cmml" xref="S3.Ex4.m1.6.6.1.1.8.5.2.1.1.2">𝑧</ci><ci id="S3.Ex4.m1.6.6.1.1.8.5.2.1.1.3.cmml" xref="S3.Ex4.m1.6.6.1.1.8.5.2.1.1.3">𝑡</ci></apply><ci id="S3.Ex4.m1.3.3.cmml" xref="S3.Ex4.m1.3.3">𝑡</ci><emptyset id="S3.Ex4.m1.4.4.cmml" xref="S3.Ex4.m1.4.4"></emptyset><emptyset id="S3.Ex4.m1.5.5.cmml" xref="S3.Ex4.m1.5.5"></emptyset></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex4.m1.6c">\tilde{\epsilon}_{t}(z_{t},t,c_{v},c_{a})=\omega\epsilon_{\theta}(z_{t},t,c_{v},c_{a})+(1-\omega)\epsilon_{\theta}(z_{t},t,\emptyset,\emptyset),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS4.p4.10" class="ltx_p">where <math id="S3.SS4.p4.5.m1.1" class="ltx_Math" alttext="\emptyset" display="inline"><semantics id="S3.SS4.p4.5.m1.1a"><mi mathvariant="normal" id="S3.SS4.p4.5.m1.1.1" xref="S3.SS4.p4.5.m1.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.5.m1.1b"><emptyset id="S3.SS4.p4.5.m1.1.1.cmml" xref="S3.SS4.p4.5.m1.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.5.m1.1c">\emptyset</annotation></semantics></math> denotes zero tensor. For the above estimation to be more precise, during training, we randomly replace <math id="S3.SS4.p4.6.m2.1" class="ltx_Math" alttext="c_{v}" display="inline"><semantics id="S3.SS4.p4.6.m2.1a"><msub id="S3.SS4.p4.6.m2.1.1" xref="S3.SS4.p4.6.m2.1.1.cmml"><mi id="S3.SS4.p4.6.m2.1.1.2" xref="S3.SS4.p4.6.m2.1.1.2.cmml">c</mi><mi id="S3.SS4.p4.6.m2.1.1.3" xref="S3.SS4.p4.6.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.6.m2.1b"><apply id="S3.SS4.p4.6.m2.1.1.cmml" xref="S3.SS4.p4.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.6.m2.1.1.1.cmml" xref="S3.SS4.p4.6.m2.1.1">subscript</csymbol><ci id="S3.SS4.p4.6.m2.1.1.2.cmml" xref="S3.SS4.p4.6.m2.1.1.2">𝑐</ci><ci id="S3.SS4.p4.6.m2.1.1.3.cmml" xref="S3.SS4.p4.6.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.6.m2.1c">c_{v}</annotation></semantics></math> with <math id="S3.SS4.p4.7.m3.1" class="ltx_Math" alttext="\emptyset" display="inline"><semantics id="S3.SS4.p4.7.m3.1a"><mi mathvariant="normal" id="S3.SS4.p4.7.m3.1.1" xref="S3.SS4.p4.7.m3.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.7.m3.1b"><emptyset id="S3.SS4.p4.7.m3.1.1.cmml" xref="S3.SS4.p4.7.m3.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.7.m3.1c">\emptyset</annotation></semantics></math> with probability <math id="S3.SS4.p4.8.m4.1" class="ltx_Math" alttext="0.2" display="inline"><semantics id="S3.SS4.p4.8.m4.1a"><mn id="S3.SS4.p4.8.m4.1.1" xref="S3.SS4.p4.8.m4.1.1.cmml">0.2</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.8.m4.1b"><cn type="float" id="S3.SS4.p4.8.m4.1.1.cmml" xref="S3.SS4.p4.8.m4.1.1">0.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.8.m4.1c">0.2</annotation></semantics></math>. As for <math id="S3.SS4.p4.9.m5.1" class="ltx_Math" alttext="c_{a}" display="inline"><semantics id="S3.SS4.p4.9.m5.1a"><msub id="S3.SS4.p4.9.m5.1.1" xref="S3.SS4.p4.9.m5.1.1.cmml"><mi id="S3.SS4.p4.9.m5.1.1.2" xref="S3.SS4.p4.9.m5.1.1.2.cmml">c</mi><mi id="S3.SS4.p4.9.m5.1.1.3" xref="S3.SS4.p4.9.m5.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.9.m5.1b"><apply id="S3.SS4.p4.9.m5.1.1.cmml" xref="S3.SS4.p4.9.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.9.m5.1.1.1.cmml" xref="S3.SS4.p4.9.m5.1.1">subscript</csymbol><ci id="S3.SS4.p4.9.m5.1.1.2.cmml" xref="S3.SS4.p4.9.m5.1.1.2">𝑐</ci><ci id="S3.SS4.p4.9.m5.1.1.3.cmml" xref="S3.SS4.p4.9.m5.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.9.m5.1c">c_{a}</annotation></semantics></math>, we found dropping it even with even a small probability harms the performance,
and therefore we always condition the LDM with <math id="S3.SS4.p4.10.m6.1" class="ltx_Math" alttext="c_{a}" display="inline"><semantics id="S3.SS4.p4.10.m6.1a"><msub id="S3.SS4.p4.10.m6.1.1" xref="S3.SS4.p4.10.m6.1.1.cmml"><mi id="S3.SS4.p4.10.m6.1.1.2" xref="S3.SS4.p4.10.m6.1.1.2.cmml">c</mi><mi id="S3.SS4.p4.10.m6.1.1.3" xref="S3.SS4.p4.10.m6.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.10.m6.1b"><apply id="S3.SS4.p4.10.m6.1.1.cmml" xref="S3.SS4.p4.10.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.10.m6.1.1.1.cmml" xref="S3.SS4.p4.10.m6.1.1">subscript</csymbol><ci id="S3.SS4.p4.10.m6.1.1.2.cmml" xref="S3.SS4.p4.10.m6.1.1.2">𝑐</ci><ci id="S3.SS4.p4.10.m6.1.1.3.cmml" xref="S3.SS4.p4.10.m6.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.10.m6.1c">c_{a}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">During inference, we use DPM-Solver <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> on LDM to sample a latent representation,
which is then upsampled into a mel-spectrogram by the decoder of VAE. Lastly, we use a vocoder (HiFi-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>) model to generate waveform from the mel-spectrogram.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Audio-Visual Representation Learning</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.5" class="ltx_p">Generating semantically and temporally synchronized action sounds from video requires the video encoder to capture these relevant features. In addition, we would like to train a video model and an audio model whose representations align in the embedding space to support retrieval-augmented generation discussed in <a href="#S3.SS3" title="3.3 Retrieval Augmented Generation and Controllable Generation ‣ 3 Ambient-aware Action Sound Generation ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>. For this purpose, we train a video encoder and audio encoder contrastively to optimize the following objective:</p>
<table id="S3.Ex5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex5.m1.8" class="ltx_Math" alttext="\text{AV-Sim}(A,V)=-\frac{1}{|\mathcal{B}|}\sum_{t\in\mathcal{B}}\log\frac{\exp(e_{A}^{t}e_{V}^{t}/\tau)}{\sum_{l\in\mathcal{B}}\exp(e_{A}^{t}e_{V}^{l}/\tau)}," display="block"><semantics id="S3.Ex5.m1.8a"><mrow id="S3.Ex5.m1.8.8.1" xref="S3.Ex5.m1.8.8.1.1.cmml"><mrow id="S3.Ex5.m1.8.8.1.1" xref="S3.Ex5.m1.8.8.1.1.cmml"><mrow id="S3.Ex5.m1.8.8.1.1.2" xref="S3.Ex5.m1.8.8.1.1.2.cmml"><mtext id="S3.Ex5.m1.8.8.1.1.2.2" xref="S3.Ex5.m1.8.8.1.1.2.2a.cmml">AV-Sim</mtext><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.8.8.1.1.2.1" xref="S3.Ex5.m1.8.8.1.1.2.1.cmml">​</mo><mrow id="S3.Ex5.m1.8.8.1.1.2.3.2" xref="S3.Ex5.m1.8.8.1.1.2.3.1.cmml"><mo stretchy="false" id="S3.Ex5.m1.8.8.1.1.2.3.2.1" xref="S3.Ex5.m1.8.8.1.1.2.3.1.cmml">(</mo><mi id="S3.Ex5.m1.6.6" xref="S3.Ex5.m1.6.6.cmml">A</mi><mo id="S3.Ex5.m1.8.8.1.1.2.3.2.2" xref="S3.Ex5.m1.8.8.1.1.2.3.1.cmml">,</mo><mi id="S3.Ex5.m1.7.7" xref="S3.Ex5.m1.7.7.cmml">V</mi><mo stretchy="false" id="S3.Ex5.m1.8.8.1.1.2.3.2.3" xref="S3.Ex5.m1.8.8.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex5.m1.8.8.1.1.1" xref="S3.Ex5.m1.8.8.1.1.1.cmml">=</mo><mrow id="S3.Ex5.m1.8.8.1.1.3" xref="S3.Ex5.m1.8.8.1.1.3.cmml"><mo id="S3.Ex5.m1.8.8.1.1.3a" xref="S3.Ex5.m1.8.8.1.1.3.cmml">−</mo><mrow id="S3.Ex5.m1.8.8.1.1.3.2" xref="S3.Ex5.m1.8.8.1.1.3.2.cmml"><mfrac id="S3.Ex5.m1.1.1" xref="S3.Ex5.m1.1.1.cmml"><mn id="S3.Ex5.m1.1.1.3" xref="S3.Ex5.m1.1.1.3.cmml">1</mn><mrow id="S3.Ex5.m1.1.1.1.3" xref="S3.Ex5.m1.1.1.1.2.cmml"><mo stretchy="false" id="S3.Ex5.m1.1.1.1.3.1" xref="S3.Ex5.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex5.m1.1.1.1.1" xref="S3.Ex5.m1.1.1.1.1.cmml">ℬ</mi><mo stretchy="false" id="S3.Ex5.m1.1.1.1.3.2" xref="S3.Ex5.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.8.8.1.1.3.2.1" xref="S3.Ex5.m1.8.8.1.1.3.2.1.cmml">​</mo><mrow id="S3.Ex5.m1.8.8.1.1.3.2.2" xref="S3.Ex5.m1.8.8.1.1.3.2.2.cmml"><munder id="S3.Ex5.m1.8.8.1.1.3.2.2.1" xref="S3.Ex5.m1.8.8.1.1.3.2.2.1.cmml"><mo movablelimits="false" id="S3.Ex5.m1.8.8.1.1.3.2.2.1.2" xref="S3.Ex5.m1.8.8.1.1.3.2.2.1.2.cmml">∑</mo><mrow id="S3.Ex5.m1.8.8.1.1.3.2.2.1.3" xref="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.cmml"><mi id="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.2" xref="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.2.cmml">t</mi><mo id="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.1" xref="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.3" xref="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.3.cmml">ℬ</mi></mrow></munder><mrow id="S3.Ex5.m1.8.8.1.1.3.2.2.2" xref="S3.Ex5.m1.8.8.1.1.3.2.2.2.cmml"><mi id="S3.Ex5.m1.8.8.1.1.3.2.2.2.1" xref="S3.Ex5.m1.8.8.1.1.3.2.2.2.1.cmml">log</mi><mo lspace="0.167em" id="S3.Ex5.m1.8.8.1.1.3.2.2.2a" xref="S3.Ex5.m1.8.8.1.1.3.2.2.2.cmml">⁡</mo><mfrac id="S3.Ex5.m1.5.5" xref="S3.Ex5.m1.5.5.cmml"><mrow id="S3.Ex5.m1.3.3.2.2" xref="S3.Ex5.m1.3.3.2.3.cmml"><mi id="S3.Ex5.m1.2.2.1.1" xref="S3.Ex5.m1.2.2.1.1.cmml">exp</mi><mo id="S3.Ex5.m1.3.3.2.2a" xref="S3.Ex5.m1.3.3.2.3.cmml">⁡</mo><mrow id="S3.Ex5.m1.3.3.2.2.1" xref="S3.Ex5.m1.3.3.2.3.cmml"><mo stretchy="false" id="S3.Ex5.m1.3.3.2.2.1.2" xref="S3.Ex5.m1.3.3.2.3.cmml">(</mo><mrow id="S3.Ex5.m1.3.3.2.2.1.1" xref="S3.Ex5.m1.3.3.2.2.1.1.cmml"><mrow id="S3.Ex5.m1.3.3.2.2.1.1.2" xref="S3.Ex5.m1.3.3.2.2.1.1.2.cmml"><msubsup id="S3.Ex5.m1.3.3.2.2.1.1.2.2" xref="S3.Ex5.m1.3.3.2.2.1.1.2.2.cmml"><mi id="S3.Ex5.m1.3.3.2.2.1.1.2.2.2.2" xref="S3.Ex5.m1.3.3.2.2.1.1.2.2.2.2.cmml">e</mi><mi id="S3.Ex5.m1.3.3.2.2.1.1.2.2.2.3" xref="S3.Ex5.m1.3.3.2.2.1.1.2.2.2.3.cmml">A</mi><mi id="S3.Ex5.m1.3.3.2.2.1.1.2.2.3" xref="S3.Ex5.m1.3.3.2.2.1.1.2.2.3.cmml">t</mi></msubsup><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.3.3.2.2.1.1.2.1" xref="S3.Ex5.m1.3.3.2.2.1.1.2.1.cmml">​</mo><msubsup id="S3.Ex5.m1.3.3.2.2.1.1.2.3" xref="S3.Ex5.m1.3.3.2.2.1.1.2.3.cmml"><mi id="S3.Ex5.m1.3.3.2.2.1.1.2.3.2.2" xref="S3.Ex5.m1.3.3.2.2.1.1.2.3.2.2.cmml">e</mi><mi id="S3.Ex5.m1.3.3.2.2.1.1.2.3.2.3" xref="S3.Ex5.m1.3.3.2.2.1.1.2.3.2.3.cmml">V</mi><mi id="S3.Ex5.m1.3.3.2.2.1.1.2.3.3" xref="S3.Ex5.m1.3.3.2.2.1.1.2.3.3.cmml">t</mi></msubsup></mrow><mo id="S3.Ex5.m1.3.3.2.2.1.1.1" xref="S3.Ex5.m1.3.3.2.2.1.1.1.cmml">/</mo><mi id="S3.Ex5.m1.3.3.2.2.1.1.3" xref="S3.Ex5.m1.3.3.2.2.1.1.3.cmml">τ</mi></mrow><mo stretchy="false" id="S3.Ex5.m1.3.3.2.2.1.3" xref="S3.Ex5.m1.3.3.2.3.cmml">)</mo></mrow></mrow><mrow id="S3.Ex5.m1.5.5.4" xref="S3.Ex5.m1.5.5.4.cmml"><msub id="S3.Ex5.m1.5.5.4.3" xref="S3.Ex5.m1.5.5.4.3.cmml"><mo id="S3.Ex5.m1.5.5.4.3.2" xref="S3.Ex5.m1.5.5.4.3.2.cmml">∑</mo><mrow id="S3.Ex5.m1.5.5.4.3.3" xref="S3.Ex5.m1.5.5.4.3.3.cmml"><mi id="S3.Ex5.m1.5.5.4.3.3.2" xref="S3.Ex5.m1.5.5.4.3.3.2.cmml">l</mi><mo id="S3.Ex5.m1.5.5.4.3.3.1" xref="S3.Ex5.m1.5.5.4.3.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex5.m1.5.5.4.3.3.3" xref="S3.Ex5.m1.5.5.4.3.3.3.cmml">ℬ</mi></mrow></msub><mrow id="S3.Ex5.m1.5.5.4.2.1" xref="S3.Ex5.m1.5.5.4.2.2.cmml"><mi id="S3.Ex5.m1.4.4.3.1" xref="S3.Ex5.m1.4.4.3.1.cmml">exp</mi><mo id="S3.Ex5.m1.5.5.4.2.1a" xref="S3.Ex5.m1.5.5.4.2.2.cmml">⁡</mo><mrow id="S3.Ex5.m1.5.5.4.2.1.1" xref="S3.Ex5.m1.5.5.4.2.2.cmml"><mo stretchy="false" id="S3.Ex5.m1.5.5.4.2.1.1.2" xref="S3.Ex5.m1.5.5.4.2.2.cmml">(</mo><mrow id="S3.Ex5.m1.5.5.4.2.1.1.1" xref="S3.Ex5.m1.5.5.4.2.1.1.1.cmml"><mrow id="S3.Ex5.m1.5.5.4.2.1.1.1.2" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.cmml"><msubsup id="S3.Ex5.m1.5.5.4.2.1.1.1.2.2" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.cmml"><mi id="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.2.2" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.2.2.cmml">e</mi><mi id="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.2.3" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.2.3.cmml">A</mi><mi id="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.3" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.3.cmml">t</mi></msubsup><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.5.5.4.2.1.1.1.2.1" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.1.cmml">​</mo><msubsup id="S3.Ex5.m1.5.5.4.2.1.1.1.2.3" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.cmml"><mi id="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.2.2" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.2.2.cmml">e</mi><mi id="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.2.3" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.2.3.cmml">V</mi><mi id="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.3" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.3.cmml">l</mi></msubsup></mrow><mo id="S3.Ex5.m1.5.5.4.2.1.1.1.1" xref="S3.Ex5.m1.5.5.4.2.1.1.1.1.cmml">/</mo><mi id="S3.Ex5.m1.5.5.4.2.1.1.1.3" xref="S3.Ex5.m1.5.5.4.2.1.1.1.3.cmml">τ</mi></mrow><mo stretchy="false" id="S3.Ex5.m1.5.5.4.2.1.1.3" xref="S3.Ex5.m1.5.5.4.2.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow></mrow></mrow></mrow><mo id="S3.Ex5.m1.8.8.1.2" xref="S3.Ex5.m1.8.8.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex5.m1.8b"><apply id="S3.Ex5.m1.8.8.1.1.cmml" xref="S3.Ex5.m1.8.8.1"><eq id="S3.Ex5.m1.8.8.1.1.1.cmml" xref="S3.Ex5.m1.8.8.1.1.1"></eq><apply id="S3.Ex5.m1.8.8.1.1.2.cmml" xref="S3.Ex5.m1.8.8.1.1.2"><times id="S3.Ex5.m1.8.8.1.1.2.1.cmml" xref="S3.Ex5.m1.8.8.1.1.2.1"></times><ci id="S3.Ex5.m1.8.8.1.1.2.2a.cmml" xref="S3.Ex5.m1.8.8.1.1.2.2"><mtext id="S3.Ex5.m1.8.8.1.1.2.2.cmml" xref="S3.Ex5.m1.8.8.1.1.2.2">AV-Sim</mtext></ci><interval closure="open" id="S3.Ex5.m1.8.8.1.1.2.3.1.cmml" xref="S3.Ex5.m1.8.8.1.1.2.3.2"><ci id="S3.Ex5.m1.6.6.cmml" xref="S3.Ex5.m1.6.6">𝐴</ci><ci id="S3.Ex5.m1.7.7.cmml" xref="S3.Ex5.m1.7.7">𝑉</ci></interval></apply><apply id="S3.Ex5.m1.8.8.1.1.3.cmml" xref="S3.Ex5.m1.8.8.1.1.3"><minus id="S3.Ex5.m1.8.8.1.1.3.1.cmml" xref="S3.Ex5.m1.8.8.1.1.3"></minus><apply id="S3.Ex5.m1.8.8.1.1.3.2.cmml" xref="S3.Ex5.m1.8.8.1.1.3.2"><times id="S3.Ex5.m1.8.8.1.1.3.2.1.cmml" xref="S3.Ex5.m1.8.8.1.1.3.2.1"></times><apply id="S3.Ex5.m1.1.1.cmml" xref="S3.Ex5.m1.1.1"><divide id="S3.Ex5.m1.1.1.2.cmml" xref="S3.Ex5.m1.1.1"></divide><cn type="integer" id="S3.Ex5.m1.1.1.3.cmml" xref="S3.Ex5.m1.1.1.3">1</cn><apply id="S3.Ex5.m1.1.1.1.2.cmml" xref="S3.Ex5.m1.1.1.1.3"><abs id="S3.Ex5.m1.1.1.1.2.1.cmml" xref="S3.Ex5.m1.1.1.1.3.1"></abs><ci id="S3.Ex5.m1.1.1.1.1.cmml" xref="S3.Ex5.m1.1.1.1.1">ℬ</ci></apply></apply><apply id="S3.Ex5.m1.8.8.1.1.3.2.2.cmml" xref="S3.Ex5.m1.8.8.1.1.3.2.2"><apply id="S3.Ex5.m1.8.8.1.1.3.2.2.1.cmml" xref="S3.Ex5.m1.8.8.1.1.3.2.2.1"><csymbol cd="ambiguous" id="S3.Ex5.m1.8.8.1.1.3.2.2.1.1.cmml" xref="S3.Ex5.m1.8.8.1.1.3.2.2.1">subscript</csymbol><sum id="S3.Ex5.m1.8.8.1.1.3.2.2.1.2.cmml" xref="S3.Ex5.m1.8.8.1.1.3.2.2.1.2"></sum><apply id="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.cmml" xref="S3.Ex5.m1.8.8.1.1.3.2.2.1.3"><in id="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.1.cmml" xref="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.1"></in><ci id="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.2.cmml" xref="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.2">𝑡</ci><ci id="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.3.cmml" xref="S3.Ex5.m1.8.8.1.1.3.2.2.1.3.3">ℬ</ci></apply></apply><apply id="S3.Ex5.m1.8.8.1.1.3.2.2.2.cmml" xref="S3.Ex5.m1.8.8.1.1.3.2.2.2"><log id="S3.Ex5.m1.8.8.1.1.3.2.2.2.1.cmml" xref="S3.Ex5.m1.8.8.1.1.3.2.2.2.1"></log><apply id="S3.Ex5.m1.5.5.cmml" xref="S3.Ex5.m1.5.5"><divide id="S3.Ex5.m1.5.5.5.cmml" xref="S3.Ex5.m1.5.5"></divide><apply id="S3.Ex5.m1.3.3.2.3.cmml" xref="S3.Ex5.m1.3.3.2.2"><exp id="S3.Ex5.m1.2.2.1.1.cmml" xref="S3.Ex5.m1.2.2.1.1"></exp><apply id="S3.Ex5.m1.3.3.2.2.1.1.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1"><divide id="S3.Ex5.m1.3.3.2.2.1.1.1.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.1"></divide><apply id="S3.Ex5.m1.3.3.2.2.1.1.2.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2"><times id="S3.Ex5.m1.3.3.2.2.1.1.2.1.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.1"></times><apply id="S3.Ex5.m1.3.3.2.2.1.1.2.2.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex5.m1.3.3.2.2.1.1.2.2.1.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.2">superscript</csymbol><apply id="S3.Ex5.m1.3.3.2.2.1.1.2.2.2.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex5.m1.3.3.2.2.1.1.2.2.2.1.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.2">subscript</csymbol><ci id="S3.Ex5.m1.3.3.2.2.1.1.2.2.2.2.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.2.2.2">𝑒</ci><ci id="S3.Ex5.m1.3.3.2.2.1.1.2.2.2.3.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.2.2.3">𝐴</ci></apply><ci id="S3.Ex5.m1.3.3.2.2.1.1.2.2.3.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.2.3">𝑡</ci></apply><apply id="S3.Ex5.m1.3.3.2.2.1.1.2.3.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex5.m1.3.3.2.2.1.1.2.3.1.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.3">superscript</csymbol><apply id="S3.Ex5.m1.3.3.2.2.1.1.2.3.2.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex5.m1.3.3.2.2.1.1.2.3.2.1.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.3">subscript</csymbol><ci id="S3.Ex5.m1.3.3.2.2.1.1.2.3.2.2.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.3.2.2">𝑒</ci><ci id="S3.Ex5.m1.3.3.2.2.1.1.2.3.2.3.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.3.2.3">𝑉</ci></apply><ci id="S3.Ex5.m1.3.3.2.2.1.1.2.3.3.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.2.3.3">𝑡</ci></apply></apply><ci id="S3.Ex5.m1.3.3.2.2.1.1.3.cmml" xref="S3.Ex5.m1.3.3.2.2.1.1.3">𝜏</ci></apply></apply><apply id="S3.Ex5.m1.5.5.4.cmml" xref="S3.Ex5.m1.5.5.4"><apply id="S3.Ex5.m1.5.5.4.3.cmml" xref="S3.Ex5.m1.5.5.4.3"><csymbol cd="ambiguous" id="S3.Ex5.m1.5.5.4.3.1.cmml" xref="S3.Ex5.m1.5.5.4.3">subscript</csymbol><sum id="S3.Ex5.m1.5.5.4.3.2.cmml" xref="S3.Ex5.m1.5.5.4.3.2"></sum><apply id="S3.Ex5.m1.5.5.4.3.3.cmml" xref="S3.Ex5.m1.5.5.4.3.3"><in id="S3.Ex5.m1.5.5.4.3.3.1.cmml" xref="S3.Ex5.m1.5.5.4.3.3.1"></in><ci id="S3.Ex5.m1.5.5.4.3.3.2.cmml" xref="S3.Ex5.m1.5.5.4.3.3.2">𝑙</ci><ci id="S3.Ex5.m1.5.5.4.3.3.3.cmml" xref="S3.Ex5.m1.5.5.4.3.3.3">ℬ</ci></apply></apply><apply id="S3.Ex5.m1.5.5.4.2.2.cmml" xref="S3.Ex5.m1.5.5.4.2.1"><exp id="S3.Ex5.m1.4.4.3.1.cmml" xref="S3.Ex5.m1.4.4.3.1"></exp><apply id="S3.Ex5.m1.5.5.4.2.1.1.1.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1"><divide id="S3.Ex5.m1.5.5.4.2.1.1.1.1.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.1"></divide><apply id="S3.Ex5.m1.5.5.4.2.1.1.1.2.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2"><times id="S3.Ex5.m1.5.5.4.2.1.1.1.2.1.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.1"></times><apply id="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.1.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.2">superscript</csymbol><apply id="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.2.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.2.1.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.2">subscript</csymbol><ci id="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.2.2.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.2.2">𝑒</ci><ci id="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.2.3.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.2.3">𝐴</ci></apply><ci id="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.3.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.2.3">𝑡</ci></apply><apply id="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.1.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.3">superscript</csymbol><apply id="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.2.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.2.1.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.3">subscript</csymbol><ci id="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.2.2.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.2.2">𝑒</ci><ci id="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.2.3.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.2.3">𝑉</ci></apply><ci id="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.3.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.2.3.3">𝑙</ci></apply></apply><ci id="S3.Ex5.m1.5.5.4.2.1.1.1.3.cmml" xref="S3.Ex5.m1.5.5.4.2.1.1.1.3">𝜏</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex5.m1.8c">\text{AV-Sim}(A,V)=-\frac{1}{|\mathcal{B}|}\sum_{t\in\mathcal{B}}\log\frac{\exp(e_{A}^{t}e_{V}^{t}/\tau)}{\sum_{l\in\mathcal{B}}\exp(e_{A}^{t}e_{V}^{l}/\tau)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS5.p1.4" class="ltx_p">where <math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{B}" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">ℬ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><ci id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">ℬ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">\mathcal{B}</annotation></semantics></math> is the current batch of data, <math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="e_{A}^{t}" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><msubsup id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mi id="S3.SS5.p1.2.m2.1.1.2.2" xref="S3.SS5.p1.2.m2.1.1.2.2.cmml">e</mi><mi id="S3.SS5.p1.2.m2.1.1.2.3" xref="S3.SS5.p1.2.m2.1.1.2.3.cmml">A</mi><mi id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">superscript</csymbol><apply id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.2.1.cmml" xref="S3.SS5.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p1.2.m2.1.1.2.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2.2">𝑒</ci><ci id="S3.SS5.p1.2.m2.1.1.2.3.cmml" xref="S3.SS5.p1.2.m2.1.1.2.3">𝐴</ci></apply><ci id="S3.SS5.p1.2.m2.1.1.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">e_{A}^{t}</annotation></semantics></math> and <math id="S3.SS5.p1.3.m3.1" class="ltx_Math" alttext="e_{V}^{t}" display="inline"><semantics id="S3.SS5.p1.3.m3.1a"><msubsup id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml"><mi id="S3.SS5.p1.3.m3.1.1.2.2" xref="S3.SS5.p1.3.m3.1.1.2.2.cmml">e</mi><mi id="S3.SS5.p1.3.m3.1.1.2.3" xref="S3.SS5.p1.3.m3.1.1.2.3.cmml">V</mi><mi id="S3.SS5.p1.3.m3.1.1.3" xref="S3.SS5.p1.3.m3.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><apply id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.3.m3.1.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1">superscript</csymbol><apply id="S3.SS5.p1.3.m3.1.1.2.cmml" xref="S3.SS5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.3.m3.1.1.2.1.cmml" xref="S3.SS5.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p1.3.m3.1.1.2.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2.2">𝑒</ci><ci id="S3.SS5.p1.3.m3.1.1.2.3.cmml" xref="S3.SS5.p1.3.m3.1.1.2.3">𝑉</ci></apply><ci id="S3.SS5.p1.3.m3.1.1.3.cmml" xref="S3.SS5.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">e_{V}^{t}</annotation></semantics></math> are normalized embeddings of the audio and video features, <math id="S3.SS5.p1.4.m4.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS5.p1.4.m4.1a"><mi id="S3.SS5.p1.4.m4.1.1" xref="S3.SS5.p1.4.m4.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.4.m4.1b"><ci id="S3.SS5.p1.4.m4.1.1.cmml" xref="S3.SS5.p1.4.m4.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m4.1c">\tau</annotation></semantics></math> is a temperature parameter. To leverage the full power of narrations on Ego4D, we initialize the video encoder weights from models pre-trained on video and language from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Implementation Details</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.11" class="ltx_p">We use Ego4D-Sounds (see <a href="#S4" title="4 The Ego4D-Sounds Dataset ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a>) to train our AV-LDM. Video is sampled at 5FPS and audio is sampled at 16kHz. Video is passed through the pre-trained video encoder to produce condition features <math id="S3.SS6.p1.1.m1.1" class="ltx_Math" alttext="c_{v}\in\mathbb{R}^{16\times 768}" display="inline"><semantics id="S3.SS6.p1.1.m1.1a"><mrow id="S3.SS6.p1.1.m1.1.1" xref="S3.SS6.p1.1.m1.1.1.cmml"><msub id="S3.SS6.p1.1.m1.1.1.2" xref="S3.SS6.p1.1.m1.1.1.2.cmml"><mi id="S3.SS6.p1.1.m1.1.1.2.2" xref="S3.SS6.p1.1.m1.1.1.2.2.cmml">c</mi><mi id="S3.SS6.p1.1.m1.1.1.2.3" xref="S3.SS6.p1.1.m1.1.1.2.3.cmml">v</mi></msub><mo id="S3.SS6.p1.1.m1.1.1.1" xref="S3.SS6.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS6.p1.1.m1.1.1.3" xref="S3.SS6.p1.1.m1.1.1.3.cmml"><mi id="S3.SS6.p1.1.m1.1.1.3.2" xref="S3.SS6.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS6.p1.1.m1.1.1.3.3" xref="S3.SS6.p1.1.m1.1.1.3.3.cmml"><mn id="S3.SS6.p1.1.m1.1.1.3.3.2" xref="S3.SS6.p1.1.m1.1.1.3.3.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS6.p1.1.m1.1.1.3.3.1" xref="S3.SS6.p1.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS6.p1.1.m1.1.1.3.3.3" xref="S3.SS6.p1.1.m1.1.1.3.3.3.cmml">768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><apply id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1"><in id="S3.SS6.p1.1.m1.1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1.1"></in><apply id="S3.SS6.p1.1.m1.1.1.2.cmml" xref="S3.SS6.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p1.1.m1.1.1.2.1.cmml" xref="S3.SS6.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS6.p1.1.m1.1.1.2.2.cmml" xref="S3.SS6.p1.1.m1.1.1.2.2">𝑐</ci><ci id="S3.SS6.p1.1.m1.1.1.2.3.cmml" xref="S3.SS6.p1.1.m1.1.1.2.3">𝑣</ci></apply><apply id="S3.SS6.p1.1.m1.1.1.3.cmml" xref="S3.SS6.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p1.1.m1.1.1.3.1.cmml" xref="S3.SS6.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS6.p1.1.m1.1.1.3.2.cmml" xref="S3.SS6.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS6.p1.1.m1.1.1.3.3.cmml" xref="S3.SS6.p1.1.m1.1.1.3.3"><times id="S3.SS6.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS6.p1.1.m1.1.1.3.3.1"></times><cn type="integer" id="S3.SS6.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS6.p1.1.m1.1.1.3.3.2">16</cn><cn type="integer" id="S3.SS6.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS6.p1.1.m1.1.1.3.3.3">768</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">c_{v}\in\mathbb{R}^{16\times 768}</annotation></semantics></math>. The audio waveform is transformed into a mel-spectrogram with a hop size of 256 and 128 mel bins. The mel-spectrogram is then passed to the VAE encoder with padding in the temporal dimension to produce target <math id="S3.SS6.p1.2.m2.1" class="ltx_Math" alttext="z_{0}\in\mathbb{R}^{4\times 16\times 24}" display="inline"><semantics id="S3.SS6.p1.2.m2.1a"><mrow id="S3.SS6.p1.2.m2.1.1" xref="S3.SS6.p1.2.m2.1.1.cmml"><msub id="S3.SS6.p1.2.m2.1.1.2" xref="S3.SS6.p1.2.m2.1.1.2.cmml"><mi id="S3.SS6.p1.2.m2.1.1.2.2" xref="S3.SS6.p1.2.m2.1.1.2.2.cmml">z</mi><mn id="S3.SS6.p1.2.m2.1.1.2.3" xref="S3.SS6.p1.2.m2.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS6.p1.2.m2.1.1.1" xref="S3.SS6.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS6.p1.2.m2.1.1.3" xref="S3.SS6.p1.2.m2.1.1.3.cmml"><mi id="S3.SS6.p1.2.m2.1.1.3.2" xref="S3.SS6.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS6.p1.2.m2.1.1.3.3" xref="S3.SS6.p1.2.m2.1.1.3.3.cmml"><mn id="S3.SS6.p1.2.m2.1.1.3.3.2" xref="S3.SS6.p1.2.m2.1.1.3.3.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS6.p1.2.m2.1.1.3.3.1" xref="S3.SS6.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS6.p1.2.m2.1.1.3.3.3" xref="S3.SS6.p1.2.m2.1.1.3.3.3.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS6.p1.2.m2.1.1.3.3.1a" xref="S3.SS6.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS6.p1.2.m2.1.1.3.3.4" xref="S3.SS6.p1.2.m2.1.1.3.3.4.cmml">24</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.2.m2.1b"><apply id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1"><in id="S3.SS6.p1.2.m2.1.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1.1"></in><apply id="S3.SS6.p1.2.m2.1.1.2.cmml" xref="S3.SS6.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p1.2.m2.1.1.2.1.cmml" xref="S3.SS6.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS6.p1.2.m2.1.1.2.2.cmml" xref="S3.SS6.p1.2.m2.1.1.2.2">𝑧</ci><cn type="integer" id="S3.SS6.p1.2.m2.1.1.2.3.cmml" xref="S3.SS6.p1.2.m2.1.1.2.3">0</cn></apply><apply id="S3.SS6.p1.2.m2.1.1.3.cmml" xref="S3.SS6.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p1.2.m2.1.1.3.1.cmml" xref="S3.SS6.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS6.p1.2.m2.1.1.3.2.cmml" xref="S3.SS6.p1.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS6.p1.2.m2.1.1.3.3.cmml" xref="S3.SS6.p1.2.m2.1.1.3.3"><times id="S3.SS6.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS6.p1.2.m2.1.1.3.3.1"></times><cn type="integer" id="S3.SS6.p1.2.m2.1.1.3.3.2.cmml" xref="S3.SS6.p1.2.m2.1.1.3.3.2">4</cn><cn type="integer" id="S3.SS6.p1.2.m2.1.1.3.3.3.cmml" xref="S3.SS6.p1.2.m2.1.1.3.3.3">16</cn><cn type="integer" id="S3.SS6.p1.2.m2.1.1.3.3.4.cmml" xref="S3.SS6.p1.2.m2.1.1.3.3.4">24</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">z_{0}\in\mathbb{R}^{4\times 16\times 24}</annotation></semantics></math>. The audio condition is processed the same way except that we use an additional MLP to process VAE’s output to produce <math id="S3.SS6.p1.3.m3.1" class="ltx_Math" alttext="c_{a}\in\mathbb{R}^{24\times 768}" display="inline"><semantics id="S3.SS6.p1.3.m3.1a"><mrow id="S3.SS6.p1.3.m3.1.1" xref="S3.SS6.p1.3.m3.1.1.cmml"><msub id="S3.SS6.p1.3.m3.1.1.2" xref="S3.SS6.p1.3.m3.1.1.2.cmml"><mi id="S3.SS6.p1.3.m3.1.1.2.2" xref="S3.SS6.p1.3.m3.1.1.2.2.cmml">c</mi><mi id="S3.SS6.p1.3.m3.1.1.2.3" xref="S3.SS6.p1.3.m3.1.1.2.3.cmml">a</mi></msub><mo id="S3.SS6.p1.3.m3.1.1.1" xref="S3.SS6.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS6.p1.3.m3.1.1.3" xref="S3.SS6.p1.3.m3.1.1.3.cmml"><mi id="S3.SS6.p1.3.m3.1.1.3.2" xref="S3.SS6.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS6.p1.3.m3.1.1.3.3" xref="S3.SS6.p1.3.m3.1.1.3.3.cmml"><mn id="S3.SS6.p1.3.m3.1.1.3.3.2" xref="S3.SS6.p1.3.m3.1.1.3.3.2.cmml">24</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS6.p1.3.m3.1.1.3.3.1" xref="S3.SS6.p1.3.m3.1.1.3.3.1.cmml">×</mo><mn id="S3.SS6.p1.3.m3.1.1.3.3.3" xref="S3.SS6.p1.3.m3.1.1.3.3.3.cmml">768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.3.m3.1b"><apply id="S3.SS6.p1.3.m3.1.1.cmml" xref="S3.SS6.p1.3.m3.1.1"><in id="S3.SS6.p1.3.m3.1.1.1.cmml" xref="S3.SS6.p1.3.m3.1.1.1"></in><apply id="S3.SS6.p1.3.m3.1.1.2.cmml" xref="S3.SS6.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m3.1.1.2.1.cmml" xref="S3.SS6.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS6.p1.3.m3.1.1.2.2.cmml" xref="S3.SS6.p1.3.m3.1.1.2.2">𝑐</ci><ci id="S3.SS6.p1.3.m3.1.1.2.3.cmml" xref="S3.SS6.p1.3.m3.1.1.2.3">𝑎</ci></apply><apply id="S3.SS6.p1.3.m3.1.1.3.cmml" xref="S3.SS6.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m3.1.1.3.1.cmml" xref="S3.SS6.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS6.p1.3.m3.1.1.3.2.cmml" xref="S3.SS6.p1.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS6.p1.3.m3.1.1.3.3.cmml" xref="S3.SS6.p1.3.m3.1.1.3.3"><times id="S3.SS6.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS6.p1.3.m3.1.1.3.3.1"></times><cn type="integer" id="S3.SS6.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS6.p1.3.m3.1.1.3.3.2">24</cn><cn type="integer" id="S3.SS6.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS6.p1.3.m3.1.1.3.3.3">768</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.3.m3.1c">c_{a}\in\mathbb{R}^{24\times 768}</annotation></semantics></math>. We load the weights of VAE and LDM from the pretrained Stable Diffusion to speed up training, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, and VAE is kept frozen during training. LDM is trained for <math id="S3.SS6.p1.4.m4.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS6.p1.4.m4.1a"><mn id="S3.SS6.p1.4.m4.1.1" xref="S3.SS6.p1.4.m4.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.4.m4.1b"><cn type="integer" id="S3.SS6.p1.4.m4.1.1.cmml" xref="S3.SS6.p1.4.m4.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.4.m4.1c">8</annotation></semantics></math> epochs with batch size <math id="S3.SS6.p1.5.m5.1" class="ltx_Math" alttext="720" display="inline"><semantics id="S3.SS6.p1.5.m5.1a"><mn id="S3.SS6.p1.5.m5.1.1" xref="S3.SS6.p1.5.m5.1.1.cmml">720</mn><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.5.m5.1b"><cn type="integer" id="S3.SS6.p1.5.m5.1.1.cmml" xref="S3.SS6.p1.5.m5.1.1">720</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.5.m5.1c">720</annotation></semantics></math> on Ego4D-Sounds with the AdamW optimizer with learning rate <math id="S3.SS6.p1.6.m6.1" class="ltx_Math" alttext="1e-4" display="inline"><semantics id="S3.SS6.p1.6.m6.1a"><mrow id="S3.SS6.p1.6.m6.1.1" xref="S3.SS6.p1.6.m6.1.1.cmml"><mrow id="S3.SS6.p1.6.m6.1.1.2" xref="S3.SS6.p1.6.m6.1.1.2.cmml"><mn id="S3.SS6.p1.6.m6.1.1.2.2" xref="S3.SS6.p1.6.m6.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S3.SS6.p1.6.m6.1.1.2.1" xref="S3.SS6.p1.6.m6.1.1.2.1.cmml">​</mo><mi id="S3.SS6.p1.6.m6.1.1.2.3" xref="S3.SS6.p1.6.m6.1.1.2.3.cmml">e</mi></mrow><mo id="S3.SS6.p1.6.m6.1.1.1" xref="S3.SS6.p1.6.m6.1.1.1.cmml">−</mo><mn id="S3.SS6.p1.6.m6.1.1.3" xref="S3.SS6.p1.6.m6.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.6.m6.1b"><apply id="S3.SS6.p1.6.m6.1.1.cmml" xref="S3.SS6.p1.6.m6.1.1"><minus id="S3.SS6.p1.6.m6.1.1.1.cmml" xref="S3.SS6.p1.6.m6.1.1.1"></minus><apply id="S3.SS6.p1.6.m6.1.1.2.cmml" xref="S3.SS6.p1.6.m6.1.1.2"><times id="S3.SS6.p1.6.m6.1.1.2.1.cmml" xref="S3.SS6.p1.6.m6.1.1.2.1"></times><cn type="integer" id="S3.SS6.p1.6.m6.1.1.2.2.cmml" xref="S3.SS6.p1.6.m6.1.1.2.2">1</cn><ci id="S3.SS6.p1.6.m6.1.1.2.3.cmml" xref="S3.SS6.p1.6.m6.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S3.SS6.p1.6.m6.1.1.3.cmml" xref="S3.SS6.p1.6.m6.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.6.m6.1c">1e-4</annotation></semantics></math>. During inference, we use <math id="S3.SS6.p1.7.m7.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S3.SS6.p1.7.m7.1a"><mn id="S3.SS6.p1.7.m7.1.1" xref="S3.SS6.p1.7.m7.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.7.m7.1b"><cn type="integer" id="S3.SS6.p1.7.m7.1.1.cmml" xref="S3.SS6.p1.7.m7.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.7.m7.1c">25</annotation></semantics></math> sampling steps with classifier-free guidance scale <math id="S3.SS6.p1.8.m8.1" class="ltx_Math" alttext="\omega=6.5" display="inline"><semantics id="S3.SS6.p1.8.m8.1a"><mrow id="S3.SS6.p1.8.m8.1.1" xref="S3.SS6.p1.8.m8.1.1.cmml"><mi id="S3.SS6.p1.8.m8.1.1.2" xref="S3.SS6.p1.8.m8.1.1.2.cmml">ω</mi><mo id="S3.SS6.p1.8.m8.1.1.1" xref="S3.SS6.p1.8.m8.1.1.1.cmml">=</mo><mn id="S3.SS6.p1.8.m8.1.1.3" xref="S3.SS6.p1.8.m8.1.1.3.cmml">6.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.8.m8.1b"><apply id="S3.SS6.p1.8.m8.1.1.cmml" xref="S3.SS6.p1.8.m8.1.1"><eq id="S3.SS6.p1.8.m8.1.1.1.cmml" xref="S3.SS6.p1.8.m8.1.1.1"></eq><ci id="S3.SS6.p1.8.m8.1.1.2.cmml" xref="S3.SS6.p1.8.m8.1.1.2">𝜔</ci><cn type="float" id="S3.SS6.p1.8.m8.1.1.3.cmml" xref="S3.SS6.p1.8.m8.1.1.3">6.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.8.m8.1c">\omega=6.5</annotation></semantics></math>. For HiFi-GAN, we train it on a combination of 0.5s segments from Ego4D<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, Epic-Kitchens <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and AudioSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. We use AdamW to train HiFi-GAN with a learning rate of <math id="S3.SS6.p1.9.m9.1" class="ltx_Math" alttext="2e-4" display="inline"><semantics id="S3.SS6.p1.9.m9.1a"><mrow id="S3.SS6.p1.9.m9.1.1" xref="S3.SS6.p1.9.m9.1.1.cmml"><mrow id="S3.SS6.p1.9.m9.1.1.2" xref="S3.SS6.p1.9.m9.1.1.2.cmml"><mn id="S3.SS6.p1.9.m9.1.1.2.2" xref="S3.SS6.p1.9.m9.1.1.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS6.p1.9.m9.1.1.2.1" xref="S3.SS6.p1.9.m9.1.1.2.1.cmml">​</mo><mi id="S3.SS6.p1.9.m9.1.1.2.3" xref="S3.SS6.p1.9.m9.1.1.2.3.cmml">e</mi></mrow><mo id="S3.SS6.p1.9.m9.1.1.1" xref="S3.SS6.p1.9.m9.1.1.1.cmml">−</mo><mn id="S3.SS6.p1.9.m9.1.1.3" xref="S3.SS6.p1.9.m9.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.9.m9.1b"><apply id="S3.SS6.p1.9.m9.1.1.cmml" xref="S3.SS6.p1.9.m9.1.1"><minus id="S3.SS6.p1.9.m9.1.1.1.cmml" xref="S3.SS6.p1.9.m9.1.1.1"></minus><apply id="S3.SS6.p1.9.m9.1.1.2.cmml" xref="S3.SS6.p1.9.m9.1.1.2"><times id="S3.SS6.p1.9.m9.1.1.2.1.cmml" xref="S3.SS6.p1.9.m9.1.1.2.1"></times><cn type="integer" id="S3.SS6.p1.9.m9.1.1.2.2.cmml" xref="S3.SS6.p1.9.m9.1.1.2.2">2</cn><ci id="S3.SS6.p1.9.m9.1.1.2.3.cmml" xref="S3.SS6.p1.9.m9.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S3.SS6.p1.9.m9.1.1.3.cmml" xref="S3.SS6.p1.9.m9.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.9.m9.1c">2e-4</annotation></semantics></math> and batch size of <math id="S3.SS6.p1.10.m10.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.SS6.p1.10.m10.1a"><mn id="S3.SS6.p1.10.m10.1.1" xref="S3.SS6.p1.10.m10.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.10.m10.1b"><cn type="integer" id="S3.SS6.p1.10.m10.1.1.cmml" xref="S3.SS6.p1.10.m10.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.10.m10.1c">64</annotation></semantics></math> for 120k steps. We set the number of random nearby audio samples <math id="S3.SS6.p1.11.m11.1" class="ltx_Math" alttext="X=6" display="inline"><semantics id="S3.SS6.p1.11.m11.1a"><mrow id="S3.SS6.p1.11.m11.1.1" xref="S3.SS6.p1.11.m11.1.1.cmml"><mi id="S3.SS6.p1.11.m11.1.1.2" xref="S3.SS6.p1.11.m11.1.1.2.cmml">X</mi><mo id="S3.SS6.p1.11.m11.1.1.1" xref="S3.SS6.p1.11.m11.1.1.1.cmml">=</mo><mn id="S3.SS6.p1.11.m11.1.1.3" xref="S3.SS6.p1.11.m11.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.11.m11.1b"><apply id="S3.SS6.p1.11.m11.1.1.cmml" xref="S3.SS6.p1.11.m11.1.1"><eq id="S3.SS6.p1.11.m11.1.1.1.cmml" xref="S3.SS6.p1.11.m11.1.1.1"></eq><ci id="S3.SS6.p1.11.m11.1.1.2.cmml" xref="S3.SS6.p1.11.m11.1.1.2">𝑋</ci><cn type="integer" id="S3.SS6.p1.11.m11.1.1.3.cmml" xref="S3.SS6.p1.11.m11.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.11.m11.1c">X=6</annotation></semantics></math>. See more details in Supp.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>The Ego4D-Sounds Dataset</h2>

<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2406.09272/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="112" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Example clips in Ego4D-Sounds. We show one video frame, the action description, and the sound for each example. Note how these actions are subtle and long-tail, usually not present in typical</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F5.4" class="ltx_p ltx_figure_panel ltx_align_center">video datasets.</p>
</div>
</div>
</figure>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Datasets</th>
<th id="S4.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Clips</th>
<th id="S4.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Language</th>
<th id="S4.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Action Types</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.1" class="ltx_tr">
<td id="S4.T1.2.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">The Greatest Hits <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</td>
<td id="S4.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">46.6K</td>
<td id="S4.T1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">✗</td>
<td id="S4.T1.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Hit, scratch, prod</td>
</tr>
<tr id="S4.T1.2.3.2" class="ltx_tr">
<td id="S4.T1.2.3.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">VGG-Sound <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</td>
<td id="S4.T1.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">200K</td>
<td id="S4.T1.2.3.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Video tags</td>
<td id="S4.T1.2.3.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Not action-specific</td>
</tr>
<tr id="S4.T1.2.4.3" class="ltx_tr">
<td id="S4.T1.2.4.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">EPIC-SOUNDS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>
</td>
<td id="S4.T1.2.4.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">117.6K</td>
<td id="S4.T1.2.4.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Audio labels</td>
<td id="S4.T1.2.4.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Kitchen actions</td>
</tr>
<tr id="S4.T1.2.5.4" class="ltx_tr">
<td id="S4.T1.2.5.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Ego4D-Sounds</td>
<td id="S4.T1.2.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">1.2M</td>
<td id="S4.T1.2.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">Action narrations</td>
<td id="S4.T1.2.5.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">In-the-wild actions</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Comparison with other audio-visual action datasets. Ego4D-Sounds not only has one order of magnitude more clips, but it is also coupled with language descriptions, supporting evaluation of sound generation based on semantics.
</span></figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Next we describe our efforts to curate Ego4D-Sounds, an audio-video dataset for human action sound generation.
Our goal is to curate a high-quality dataset for action-audio correspondence for action-to-sound generation, addressing the issue of limited action types in the existing impact sound datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Ego4D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> is an existing large-scale egocentric video dataset that has more than 3,600 hours of video recordings depicting hundreds of daily activities; 2,113 of those hours have audio available. It also has time-stamped narrations that are free-form sentences describing the current activity performed by the camera-wearer.
We first utilize the narration timestamps in Ego4D to extract clips. However, not all clips have meaningful action sounds and there are many actions like “talk with someone", “look around", “turn around" that have low audio-visual correspondence. We then use an automatic pipeline to process all extracted clips to create the Ego4D-Sounds dataset, which has 1.2 million audio-visual action clips. Similarly, for the test set, we curate 11k clips for evaluation. See Supp. for more details on the data processing pipeline.
We show examples in <a href="#S4.F5" title="In 4 The Ego4D-Sounds Dataset ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a> and comparison with other datasets in <a href="#S4.T1" title="In 4 The Ego4D-Sounds Dataset ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.3" class="ltx_p">For all resulting clips, we extract them as 3s clips with <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mn id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">×</mo><mn id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><times id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></times><cn type="integer" id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">224</cn><cn type="integer" id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">224\times 224</annotation></semantics></math> image resolution at <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S4.p3.2.m2.1a"><mn id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><cn type="integer" id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">30</annotation></semantics></math> FPS. For audio, we extract them as a single channel with a <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="16000" display="inline"><semantics id="S4.p3.3.m3.1a"><mn id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">16000</mn><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><cn type="integer" id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">16000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">16000</annotation></semantics></math> sample rate.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">To evaluate the performance of our model, we use the following metrics:</p>
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">Fréchet Audio Distance (FAD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>: evaluates the quality of generated audio clips against ground truth audio clips by measuring the similarity between their distributions.
We use the public pytorch implementation. <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/gudgud96/frechet-audio-distance" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/gudgud96/frechet-audio-distance</a></span></span></span></p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">Audio-visual synchronization (AV-Sync) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>: a binary classification model that classifies whether the video and generated audio streams are synchronized. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, we create negative examples by either shift audio temporally or sample audio from a different video clip. See more details in Supp.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">Contrastive language-audio contrastive (CLAP) scores <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>: evaluates the semantic similarity between the generated audio and the action description. We finetune the CLAP model <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/LAION-AI/CLAP" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/LAION-AI/CLAP</a></span></span></span> on the Ego4D-Sounds data and compute scores for the generated audio and the narration at test time.</p>
</div>
</li>
</ol>
<p id="S5.SS1.p1.2" class="ltx_p">These metrics measure different aspects of generation collectively, including the distribution of generated samples compared to the ground truth clips, synchronization with the video, and the semantic alignment with the action description.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">We compare with the following baseline methods:</p>
<ol id="S5.I2" class="ltx_enumerate">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p">Retrieval: we retrieve the audio from the training set using the <span id="S5.I2.i1.p1.1.1" class="ltx_text ltx_markedasmath">AV-Sim</span> model introduced in <a href="#S3.SS5" title="3.5 Audio-Visual Representation Learning ‣ 3 Ambient-aware Action Sound Generation ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>. This method represents retrieval-based generation models such as ImageBind <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p">Spec-VQGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>: a video-to-audio model that generates audio based on a codebook of spectrograms. We run their pre-trained model on our test set.</p>
</div>
</li>
<li id="S5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I2.i3.p1" class="ltx_para">
<p id="S5.I2.i3.p1.1" class="ltx_p">Diff-Foley <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>: a recent LDM-based model. We follow their fine-tuning steps on egocentric videos to train on our dataset.</p>
</div>
</li>
</ol>
<p id="S5.SS1.p2.2" class="ltx_p">Neither learning-based model has the ability to tackle the ambient sound, whereas our model disentangles it from the action sound.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">In addition, we also evaluate the following ablations: “w/o vocoder": we replace the trained HiFi-GAN vocoder with Griffin-Lim; “w/o cond": we remove the audio condition at training time; “w/o cond + denoiser": we use an off-the-shelf model to denoise the target audio <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/timsainb/noisereduce" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/timsainb/noisereduce</a></span></span></span>; “w/ random test cond": we use random audio from the training set as the condition instead of retrieving audio with the highest <span id="S5.SS1.p3.1.1" class="ltx_text ltx_markedasmath">AV-Sim</span> score.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.3.3" class="ltx_tr">
<td id="S5.T2.3.3.4" class="ltx_td ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S5.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">FAD <math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mo stretchy="false" id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">AV-Sync (%)<math id="S5.T2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T2.2.2.2.m1.1a"><mo stretchy="false" id="S5.T2.2.2.2.m1.1.1" xref="S5.T2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.m1.1b"><ci id="S5.T2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T2.3.3.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">CLAP<math id="S5.T2.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T2.3.3.3.m1.1a"><mo stretchy="false" id="S5.T2.3.3.3.m1.1.1" xref="S5.T2.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.m1.1b"><ci id="S5.T2.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T2.3.4.1" class="ltx_tr">
<td id="S5.T2.3.4.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Ground Truth (Upper Bound)</td>
<td id="S5.T2.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.0000</td>
<td id="S5.T2.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">77.69</td>
<td id="S5.T2.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.2698</td>
</tr>
<tr id="S5.T2.3.5.2" class="ltx_tr">
<td id="S5.T2.3.5.2.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Retrieval</td>
<td id="S5.T2.3.5.2.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.8353</td>
<td id="S5.T2.3.5.2.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">11.84</td>
<td id="S5.T2.3.5.2.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.0335</td>
</tr>
<tr id="S5.T2.3.6.3" class="ltx_tr">
<td id="S5.T2.3.6.3.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Spec-VQGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S5.T2.3.6.3.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.9017</td>
<td id="S5.T2.3.6.3.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.12</td>
<td id="S5.T2.3.6.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.0140</td>
</tr>
<tr id="S5.T2.3.7.4" class="ltx_tr">
<td id="S5.T2.3.7.4.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Diff-Foley <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S5.T2.3.7.4.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3.5608</td>
<td id="S5.T2.3.7.4.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5.98</td>
<td id="S5.T2.3.7.4.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.0346</td>
</tr>
<tr id="S5.T2.3.8.5" class="ltx_tr">
<td id="S5.T2.3.8.5.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Ours w/o vocoder</td>
<td id="S5.T2.3.8.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">4.9282</td>
<td id="S5.T2.3.8.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">29.60</td>
<td id="S5.T2.3.8.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.1319</td>
</tr>
<tr id="S5.T2.3.9.6" class="ltx_tr">
<td id="S5.T2.3.9.6.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Ours w/o cond + denoiser</td>
<td id="S5.T2.3.9.6.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.4676</td>
<td id="S5.T2.3.9.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.09</td>
<td id="S5.T2.3.9.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.0009</td>
</tr>
<tr id="S5.T2.3.10.7" class="ltx_tr">
<td id="S5.T2.3.10.7.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Ours w/o cond</td>
<td id="S5.T2.3.10.7.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.4681</td>
<td id="S5.T2.3.10.7.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">39.63</td>
<td id="S5.T2.3.10.7.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.1418</td>
</tr>
<tr id="S5.T2.3.11.8" class="ltx_tr">
<td id="S5.T2.3.11.8.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Ours w/ random test cond</td>
<td id="S5.T2.3.11.8.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.0635</td>
<td id="S5.T2.3.11.8.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28.74</td>
<td id="S5.T2.3.11.8.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.1278</td>
</tr>
<tr id="S5.T2.3.12.9" class="ltx_tr">
<td id="S5.T2.3.12.9.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">AV-LDM (Ours)</td>
<td id="S5.T2.3.12.9.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T2.3.12.9.2.1" class="ltx_text ltx_font_bold">0.9999</span></td>
<td id="S5.T2.3.12.9.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T2.3.12.9.3.1" class="ltx_text ltx_font_bold">45.74</span></td>
<td id="S5.T2.3.12.9.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T2.3.12.9.4.1" class="ltx_text ltx_font_bold">0.1435</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.5.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.6.2" class="ltx_text" style="font-size:90%;">Results on Ego4D-Sounds test set.
We also report the performance of the ground truth audio, which gives the upper bound value for each metric.</span></figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2406.09272/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="218" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.3.2" class="ltx_text" style="font-size:90%;">Qualitative example. We show the frames of each video followed by the waveform/spectrogram of various baseline methods.
Our model generates the most synchronized sounds.
</span></figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Results on Ego4D-Sounds</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In this section, we evaluate the ambient-sound joint generation setting with retrieval augmented generation.
The results are shown in <a href="#S5.T2" title="In 5.1 Evaluation ‣ 5 Experiments ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. Compared to all three baselines, we outperform them on all three metrics by a large margin. While the Retrieval baseline retrieves natural sounds from the training set and has a low FAD score compared to Spec-VQGAN and Diff-Foley, both its AV-Sync accuracy and CLAP scores are very low. Diff-Foley has a higher performance than Spec-VQGAN since it has been trained on this task, but it still largely underperforms our model w/o cond, likely because their video features do not generalize to the egocentric setting well.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">For ablations, “Ours w/o cond" has a much worse FAD score compared to the full model, showing the importance of our ambient-aware training. As expected, “Ours w/o cond + denoiser" has very low scores on AV-Sync and CLAP since existing noise reduction algorithms are far from perfect.
We also test our model by conditioning it on a random audio segment at test time instead of the one retrieved with the highest audio-visual similarity and its performance also gets worse, verifying the effectiveness of our retrieval-based solution.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">We show two qualitative examples in <a href="#S5.F6" title="In 5.1 Evaluation ‣ 5 Experiments ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>
comparing our model with several baselines and we show that our model synthesizes both more synchronized and more plausible sounds. To fully evaluate our results, it is important to view the supplementary video.
</p>
</div>
<figure id="S5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.09272/assets/figures/varying_ambient_ambient.png" id="S5.F7.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="371" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F7.sf1.3.2" class="ltx_text" style="font-size:90%;">Varying ambient level condition</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.09272/assets/figures/varying_ambient_fad.png" id="S5.F7.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="371" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F7.sf2.3.2" class="ltx_text" style="font-size:90%;">Audio generation accuracy (FAD)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.3.2" class="ltx_text" style="font-size:90%;">The achieved ambient level and accuracy of the prediction as a function of the input ambient levels. (a): we show the ambient level of our model changes according to the ambient level in the audio condition while the ambient level of “Ours w/o cond" and the original audio stay constant, illustrating the controllability of our model. (b) FAD is low for most input ambient levels unless it goes too extreme (too low or too high), showing our model generates high-quality action sounds even when varying output ambient levels.
</span></figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ambient Sound Control</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">By disentangling action sounds from ambient sounds, our model allows taking any given sound as the condition at test time.
To examine whether our model truly relies on the audio condition to learn the ambient sound information, we test the model by providing audio conditions of various ambient levels and then calculate the ambient level in the generated audio. The ambient level is defined as the lowest energy of any 0.5s audio segment in a 3s audio.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">The results are shown in <a href="#S5.F7" title="In 5.2 Results on Ego4D-Sounds ‣ 5 Experiments ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a>, where we also plot the ambient levels of “Ours w/o cond" and the original audio.
Our model changes the ambient sound level according to the input ambient (shown in <a href="#S5.F7.sf1" title="In Figure 7 ‣ 5.2 Results on Ego4D-Sounds ‣ 5 Experiments ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7(a)</span></a>) while still synthesizing plausible action sounds (shown in <a href="#S5.F7.sf2" title="In Figure 7 ‣ 5.2 Results on Ego4D-Sounds ‣ 5 Experiments ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7(b)</span></a>). FAD spikes when the condition ambient is too low or too high, most likely because the generated ambient sound is out of distribution since the original audio always has some ambient sounds.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p"><a href="#S5.F8" title="In 5.3 Ambient Sound Control ‣ 5 Experiments ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a> shows example outputs from our model and several baselines. The examples
show how our model generates plausible action sounds when conditioned on a low-ambient sound for action-focused generation. We can see that the action-focused setting generates similar action sounds as the action-ambient setting while having a minimal ambient level. While by definition we lack a good evaluation of this setting (there is no ground truth audio source separation for the data), our model shows an emerging capability of generating clean action sounds although it has never been explicitly trained to do so.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2406.09272/assets/x7.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="135" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.3.2" class="ltx_text" style="font-size:90%;">Visualization of action-focused generation. For both examples, Diff-Foley <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, Ours w/o cond or Ours (action-ambient generation) generate plausible action sounds along with ambient sounds. In contrast, our model conditioned on a low ambient sound generates plausible action sounds (see green boxes) with minimal ambient sound.
</span></figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Human Evaluation</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">To further validate the performance of various models, we conduct a subjective human evaluation.
In each survey, we provide 30 questions and each question has 5 videos with the same visuals but different audio samples. For each video, we ask the participant to select the video(s) whose audio 1) is most semantically plausible and temporally synchronized with the video and 2) has the least ambient sounds. We invite 20 participants to complete the survey and compute the average voting for all 30 examples. See the survey interface and guidelines in Supp.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p"><a href="#S5.T3" title="In 5.4 Human Evaluation ‣ 5 Experiments ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows the results. Overall, all learning-based methods generate reasonable action sounds while our model (action-ambient) has the highest score for action-sound quality compared to other methods. Although ours (action-focused) has a slightly lower action-sound score, it has significantly less ambient sound. This is likely because sometimes the low-ambient condition can lead the model to suppress some minor action sounds.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.2.1.1" class="ltx_tr">
<th id="S5.T3.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt"></th>
<th id="S5.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Action sound quality</th>
<th id="S5.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Least ambient sound</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.2.2.1" class="ltx_tr">
<td id="S5.T3.2.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Retrieval</td>
<td id="S5.T3.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.5%</td>
<td id="S5.T3.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">12.5%</td>
</tr>
<tr id="S5.T3.2.3.2" class="ltx_tr">
<td id="S5.T3.2.3.2.1" class="ltx_td ltx_align_center ltx_border_r">Diff-Foley <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S5.T3.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r">47.5%</td>
<td id="S5.T3.2.3.2.3" class="ltx_td ltx_align_center">12.5%</td>
</tr>
<tr id="S5.T3.2.4.3" class="ltx_tr">
<td id="S5.T3.2.4.3.1" class="ltx_td ltx_align_center ltx_border_r">AV-LDM w/o cond</td>
<td id="S5.T3.2.4.3.2" class="ltx_td ltx_align_center ltx_border_r">55.0%</td>
<td id="S5.T3.2.4.3.3" class="ltx_td ltx_align_center">17.5%</td>
</tr>
<tr id="S5.T3.2.5.4" class="ltx_tr">
<td id="S5.T3.2.5.4.1" class="ltx_td ltx_align_center ltx_border_r">AV-LDM (action-focused)</td>
<td id="S5.T3.2.5.4.2" class="ltx_td ltx_align_center ltx_border_r">60.0%</td>
<td id="S5.T3.2.5.4.3" class="ltx_td ltx_align_center"><span id="S5.T3.2.5.4.3.1" class="ltx_text ltx_font_bold">97.5%</span></td>
</tr>
<tr id="S5.T3.2.6.5" class="ltx_tr">
<td id="S5.T3.2.6.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">AV-LDM (action-ambient)</td>
<td id="S5.T3.2.6.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T3.2.6.5.2.1" class="ltx_text ltx_font_bold">72.5%</span></td>
<td id="S5.T3.2.6.5.3" class="ltx_td ltx_align_center ltx_border_bb">22.5%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.4.2" class="ltx_text" style="font-size:90%;">Survey results showing user preferences. Higher is better.
Our model in the action-ambient joint generation setting scores highest for action sound quality, showing its ability to produce action-relevant sounds despite training with in-the-wild data. Ours in the action-focused generation setting scores highest for the least ambient sound, at a slight drop in action sound quality score, showing the ability to eliminate background sounds when requested by the user.
</span></figcaption>
</figure>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Results on EPIC-KITCHENS</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">To evaluate whether our model generalizes to other datasets, we also test our model on the EPIC-KITCHENS dataset.
We first sample 1000 3s clips on EPIC-KITCHENS and then
evaluate the retrieval baseline, Diff-Foley, Ours w/o cond, and our full model on these data and then compute the FAD and AV-Sync scores for them.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p">Results are shown in <a href="#S5.T4" title="In 5.5 Results on EPIC-KITCHENS ‣ 5 Experiments ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>. Similar to what we observe on Ego4D-Sounds, our model outperforms other models on FAD and AV-Sync by a large margin, showing ours learns better to generate action sounds from visuals, which also transfer to other datasets.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.2.3.1" class="ltx_tr">
<th id="S5.T4.2.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"></th>
<th id="S5.T4.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">GT</th>
<th id="S5.T4.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Retrieval</th>
<th id="S5.T4.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Diff-Foley</th>
<th id="S5.T4.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">Ours w/o cond</th>
<th id="S5.T4.2.3.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;">AV-LDM (Ours)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">FAD <math id="S5.T4.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.1.1.1.m1.1a"><mo stretchy="false" id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<td id="S5.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.0000</td>
<td id="S5.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">1.9618</td>
<td id="S5.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">3.4649</td>
<td id="S5.T4.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">1.4731</td>
<td id="S5.T4.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T4.1.1.6.1" class="ltx_text ltx_font_bold">1.3200</span></td>
</tr>
<tr id="S5.T4.2.2" class="ltx_tr">
<th id="S5.T4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">AV-Sync (%) <math id="S5.T4.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.2.2.1.m1.1a"><mo stretchy="false" id="S5.T4.2.2.1.m1.1.1" xref="S5.T4.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.1.m1.1b"><ci id="S5.T4.2.2.1.m1.1.1.cmml" xref="S5.T4.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<td id="S5.T4.2.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">73.94</td>
<td id="S5.T4.2.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">13.84</td>
<td id="S5.T4.2.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">14.19</td>
<td id="S5.T4.2.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">50.42</td>
<td id="S5.T4.2.2.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S5.T4.2.2.6.1" class="ltx_text ltx_font_bold">59.26</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.4.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.5.2" class="ltx_text" style="font-size:90%;">Results on Epic-Kichens. GT stands for Ground Truth.</span></figcaption>
</figure>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Demo on VR Cooking Game</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p">One compelling application of action-to-sound generation is to generate sound effects for games in virtual reality, where simulating complex hand-object interactions is non-trivial. To examine whether our learned model generalizes to VR games, we collect game videos of a cooking VR game “Clash Of Chefs” from YouTube and test our model without fine-tuning. Preliminary results suggest our model can generate synced action sounds (see <a href="#S5.F9" title="In 5.6 Demo on VR Cooking Game ‣ 5 Experiments ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a> and Supp). This suggests promising future in learning action-to-sound models from real-world egocentric videos and applying them to VR games to give a game user an immersive audio-visual experience that dynamically adjusts to their own actions.</p>
</div>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2406.09272/assets/x8.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="162" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S5.F9.3.2" class="ltx_text" style="font-size:90%;">We apply our model on a VR cooking game clip where the person cuts a sushi roll three times. Our model successfully predicts the 3 cutting sounds.</span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We investigate the problem of generating sounds for human actions in egocentric videos.
We propose an ambient-aware approach that disentangles the action sound from the ambient sound, allowing successful generation after training with diverse in-the-wild data, as well as controllable conditioning on ambient sound levels.
We show that our model outperforms
existing methods and baselines—both quantitatively and through human subject studies. Overall, it significantly broadens the scope of relevant training sources for achieving action-precise sound generation.
In future work we aim to explore the possibilities for sim2real translation of our learned audio generation models to synthetic imagery inputs, e.g., for VR game applications.</p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Acknowledgments:</h4>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p1.1" class="ltx_p">UT Austin is supported in part by the IFML NSF AI Institute. Wei-Ning Hsu helped advise the project only, and all the work and data processing were done outside of Meta.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for video understanding? In: Proceedings of the 38th International Conference on Machine Learning. pp. 813–824 (2021)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Blattmann, A., Rombach, R., Oktay, K., Ommer, B.: Retrieval-augmented diffusion models. ArXiv <span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">abs/2204.11824</span> (2022), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:248377386" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:248377386</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., van den Driessche, G., Lespiau, J.B., Damoc, B., Clark, A., de Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T.W., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J.W., Elsen, E., Sifre, L.: Improving language models by retrieving from trillions of tokens. In: International Conference on Machine Learning (2021), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:244954723" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:244954723</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chen, C., Ashutosh, K., Girdhar, R., Harwath, D., Grauman, K.: Soundingactions: Learning how actions sound from narrated egocentric videos. In: CVPR (2024)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chen, H., Xie, W., Vedaldi, A., Zisserman, A.: Vggsound: A large-scale audio-visual dataset. In: ICASSP (2020)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chen, W., Hu, H., Saharia, C., Cohen, W.W.: Re-imagen: Retrieval-augmented text-to-image generator. ArXiv <span id="bib.bib6.1.1" class="ltx_text ltx_font_bold">abs/2209.14491</span> (2022), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:252596087" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:252596087</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Clarke, S., Gao, R., Wang, M., Rau, M., Xu, J., Wang, J.H., James, D.L., Wu, J.: Realimpact: A dataset of impact sound fields for real objects. In: Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (2023)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Clarke, S., Heravi, N., Rau, M., Gao, R., Wu, J., James, D., Bohg, J.: Diffimpact: Differentiable rendering and identification of impact sounds. In: 5th Annual Conference on Robot Learning (2021)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E., Moltisanti, D., Munro, J., Perrett, T.: Scaling egocentric vision: The epic-kitchens dataset. In: ECCV (2018)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. ArXiv <span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">abs/2105.05233</span> (2021), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:234357997" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:234357997</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Du, Y., Chen, Z., Salamon, J., Russell, B., Owens, A.: Conditional generation of audio from video via foley analogies. In: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2426–2436 (2023)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Fabian Caba Heilbron, Victor Escorcia, B.G., Niebles, J.C.: Activitynet: A large-scale video benchmark for human activity understanding. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 961–970 (2015)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Gan, C., Schwartz, J., Alter, S., Mrowca, D., Schrimpf, M., Traer, J., De Freitas, J., Kubilius, J., Bhandwaldar, A., Haber, N., Sano, M., Kim, K., Wang, E., Lingelbach, M., Curtis, A., Feigelis, K., Bear, D.M., Gutfreund, D., Cox, D., Torralba, A., DiCarlo, J.J., Tenenbaum, J.B., McDermott, J.H., Yamins, D.L.K.: Threedworld: A platform for interactive multi-modal physical simulation. In: NeurIPS Datasets and Benchmarks Track (2021)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Gandhi, D., Gupta, A., Pinto, L.: Swoosh! rattle! thump! - actions that sound. In: RSS (2022)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Gemmeke, J.F., Ellis, D.P.W., Freedman, D., Jansen, A., Lawrence, W., Moore, R.C., Plakal, M., Ritter, M.: Audio set: An ontology and human-labeled dataset for audio events. In: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 776–780 (2017)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K.V., Joulin, A., Misra, I.: Imagebind: One embedding space to bind them all. In: CVPR (2023)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Gong, Y., Chung, Y.A., Glass, J.: Ast: Audio spectrogram transformer. In: InterSpeech (2021)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., Martin, M., Nagarajan, T., Radosavovic, I., Ramakrishnan, S.K., Ryan, F., Sharma, J., Wray, M., Xu, M., Xu, E.Z., Zhao, C., Bansal, S., Batra, D., Cartillier, V., Crane, S., Do, T., Doulaty, M., Erapalli, A., Feichtenhofer, C., Fragomeni, A., Fu, Q., Gebreselasie, A., Gonzalez, C., Hillis, J., Huang, X., Huang, Y., Jia, W., Khoo, W., Kolar, J., Kottur, S., Kumar, A., Landini, F., Li, C., Li, Y., Li, Z., Mangalam, K., Modhugu, R., Munro, J., Murrell, T., Nishiyasu, T., Price, W., Puentes, P.R., Ramazanova, M., Sari, L., Somasundaram, K., Southerland, A., Sugano, Y., Tao, R., Vo, M., Wang, Y., Wu, X., Yagi, T., Zhao, Z., Zhu, Y., Arbelaez, P., Crandall, D., Damen, D., Farinella, G.M., Fuegen, C., Ghanem, B., Ithapu, V.K., Jawahar, C.V., Joo, H., Kitani, K., Li, H., Newcombe, R., Oliva, A., Park, H.S., Rehg, J.M., Sato, Y., Shi, J., Shou, M.Z., Torralba, A., Torresani, L., Yan, M., Malik, J.:
Ego4d: Around the world in 3,000 hours of egocentric video. In: 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 18973–18990 (2022)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Guu, K., Lee, K., Tung, Z., Pasupat, P., Chang, M.W.: Realm: Retrieval-augmented language model pre-training. ArXiv <span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">abs/2002.08909</span> (2020), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:211204736" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:211204736</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS (2020)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Ho, J., Salimans, T.: Classifier-free diffusion guidance (2022)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Huang, C., Tian, Y., Kumar, A., Xu, C.: Egocentric audio-visual object localization. In: CVPR (2023)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Huang, R., Huang, J.B., Yang, D., Ren, Y., Liu, L., Li, M., Ye, Z., Liu, J., Yin, X., Zhao, Z.: Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. ArXiv <span id="bib.bib23.1.1" class="ltx_text ltx_font_bold">abs/2301.12661</span> (2023), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:256390046" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:256390046</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Huh, J., Chalk, J., Kazakos, E., Damen, D., Zisserman, A.: Epic-sounds: A large-scale dataset of actions that sound. In: ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 1–5 (2023)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Iashin, V., Rahtu, E.: Taming visually guided sound generation. In: BMVC (2021)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Jiang, H., Murdock, C., Ithapu, V.K.: Egocentric deep multi-channel audio-visual active speaker localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10544–10552 (2022)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., Zisserman, A.: The kinetics human action video dataset. CoRR <span id="bib.bib27.1.1" class="ltx_text ltx_font_bold">abs/1705.06950</span> (2017), <a target="_blank" href="http://arxiv.org/abs/1705.06950" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1705.06950</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Kazakos, E., Nagrani, A., Zisserman, A., Damen, D.: Epic-fusion: Audio-visual temporal binding for egocentric action recognition. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV). pp. 5491–5500 (2019)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., Lewis, M.: Generalization through memorization: Nearest neighbor language models. ArXiv <span id="bib.bib29.1.1" class="ltx_text ltx_font_bold">abs/1911.00172</span> (2019), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:207870430" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:207870430</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Kilgour, K., Zuluaga, M., Roblek, D., Sharifi, M.: Fréchet audio distance: A metric for evaluating music enhancement algorithms. arxiv (2018)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR (2015)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Kong, J., Kim, J., Bae, J.: Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. Advances in Neural Information Processing Systems <span id="bib.bib32.1.1" class="ltx_text ltx_font_bold">33</span>, 17022–17033 (2020)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Kong, Z., Ping, W., Huang, J., Zhao, K., Catanzaro, B.: Diffwave: A versatile diffusion model for audio synthesis. ArXiv <span id="bib.bib33.1.1" class="ltx_text ltx_font_bold">abs/2009.09761</span> (2020), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:221818900" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:221818900</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., tau Yih, W., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-augmented generation for knowledge-intensive nlp tasks. ArXiv <span id="bib.bib34.1.1" class="ltx_text ltx_font_bold">abs/2005.11401</span> (2020), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:218869575" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:218869575</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Lin, K.Q., Wang, J., Soldan, M., Wray, M., Yan, R., Xu, Z., Gao, D., Tu, R.C., Zhao, W., Kong, W., Cai, C., HongFa, W., Damen, D., Ghanem, B., Liu, W., Shou, M.Z.: Egocentric video-language pretraining. In: Advances in Neural Information Processing Systems (2022)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Liu, H., Chen, Z., Yuan, Y., Mei, X., Liu, X., Mandic, D.P., Wang, W., Plumbley, M..: Audioldm: Text-to-audio generation with latent diffusion models. In: International Conference on Machine Learning (2023), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:256390486" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:256390486</a>

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., Zhu, J.: Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927 (2022)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Luo, S., Yan, C., Hu, C., Zhao, H.: Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. In: NeurIPS (2023)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Mittal, H., Morgado, P., Jain, U., Gupta, A.: Learning state-aware visual representations from audible interactions. In: Oh, A.H., Agarwal, A., Belgrave, D., Cho, K. (eds.) Advances in Neural Information Processing Systems (2022), <a target="_blank" href="https://openreview.net/forum?id=AhbTKBlM7X" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=AhbTKBlM7X</a>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In: International Conference on Machine Learning (2021), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:245335086" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:245335086</a>

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Owens, A., Isola, P., McDermott, J., Torralba, A., Adelson, E.H., Freeman, W.T.: Visually indicated sounds. In: CVPR (2016)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Popov, V., Vovk, I., Gogoryan, V., Sadekova, T., Kudinov, M.A.: Grad-tts: A diffusion probabilistic model for text-to-speech. In: International Conference on Machine Learning (2021), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:234483016" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:234483016</a>

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Ramazanova, M., Escorcia, V., Heilbron, F.C., Zhao, C., Ghanem, B.: Owl (observe, watch, listen): Localizing actions in egocentric video via audiovisual temporal context (2022)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, S.K.S., Ayan, B.K., Mahdavi, S.S., Lopes, R.G., Salimans, T., Ho, J., Fleet, D.J., Norouzi, M.: Photorealistic text-to-image diffusion models with deep language understanding. ArXiv <span id="bib.bib45.1.1" class="ltx_text ltx_font_bold">abs/2205.11487</span> (2022), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:248986576" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:248986576</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Song, Y., Ermon, S.: Generative modeling by estimating gradients of the data distribution. In: Neural Information Processing Systems (2019), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:196470871" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:196470871</a>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes from videos in the wild. CoRR (2012)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Su, K., Qian, K., Shlizerman, E., Torralba, A., Gan, C.: Physics-driven diffusion models for impact sound synthesis from videos. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 9749–9759 (2023), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:257805229" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:257805229</a>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Wang, D., Chen, J.: Supervised speech separation based on deep learning: An overview. arxiv (201)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Wu*, Y., Chen*, K., Zhang*, T., Hui*, Y., Berg-Kirkpatrick, T., Dubnov, S.: Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In: IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP (2023)

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Yang, D., Yu, J., Wang, H., Wang, W., Weng, C., Zou, Y., Yu, D.: Diffsound: Discrete diffusion model for text-to-sound generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing <span id="bib.bib51.1.1" class="ltx_text ltx_font_bold">31</span>, 1720–1733 (2022), <a target="_blank" href="https://api.semanticscholar.org/CorpusID:250698823" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:250698823</a>

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Supplementary</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this supplementary material, we provide additional details about:</p>
<ol id="S7.I1" class="ltx_enumerate">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.1" class="ltx_p">Supplementary video for qualitative examples (referenced in Sec. 1).</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.1" class="ltx_p">Additional implementation details (referenced in Sec. 3).</p>
</div>
</li>
<li id="S7.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S7.I1.i3.p1" class="ltx_para">
<p id="S7.I1.i3.p1.1" class="ltx_p">Dataset details (referenced in Sec. 4).</p>
</div>
</li>
<li id="S7.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S7.I1.i4.p1" class="ltx_para">
<p id="S7.I1.i4.p1.1" class="ltx_p">Evaluation metric details (referenced in Sec. 5).</p>
</div>
</li>
<li id="S7.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S7.I1.i5.p1" class="ltx_para">
<p id="S7.I1.i5.p1.1" class="ltx_p">Human evaluation details (referenced in Sec. 5).</p>
</div>
</li>
</ol>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Supplementary Video</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">In this video, we include examples of Ego4D-Sounds clips, qualitative examples on unseen Ego4D clips, and qualitative examples on VR games. Wear headphones to hear the sound.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Additional Implementation Details</h3>

<div id="S7.SS2.p1" class="ltx_para ltx_noindent">
<p id="S7.SS2.p1.12" class="ltx_p"><span id="S7.SS2.p1.12.1" class="ltx_text ltx_font_bold">Audio-Visual LDM.</span> Our Ego4D-Sounds clips are 3 seconds long. For model training and inference, we sample audio waveform at 16000Hz. We use FFT size 1024, mel bins 128, hop size 256 to transform the 3-second audio waveform into a mel-spectrogram of length 188, which we then pad in the temporal dimension to <math id="S7.SS2.p1.1.m1.1" class="ltx_Math" alttext="192" display="inline"><semantics id="S7.SS2.p1.1.m1.1a"><mn id="S7.SS2.p1.1.m1.1.1" xref="S7.SS2.p1.1.m1.1.1.cmml">192</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.1.m1.1b"><cn type="integer" id="S7.SS2.p1.1.m1.1.1.cmml" xref="S7.SS2.p1.1.m1.1.1">192</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.1.m1.1c">192</annotation></semantics></math>.
To speed up training, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, we load VAE and diffusion model weights from the pre-trained Stable Diffusion model. Note that Stable Diffusion expect image as the input/target, and therefore we duplicate the mel-spectrogram in the channel dimension and to achieve size <math id="S7.SS2.p1.2.m2.1" class="ltx_Math" alttext="x_{0}\in\mathbb{R}^{3\times 128\times 192}" display="inline"><semantics id="S7.SS2.p1.2.m2.1a"><mrow id="S7.SS2.p1.2.m2.1.1" xref="S7.SS2.p1.2.m2.1.1.cmml"><msub id="S7.SS2.p1.2.m2.1.1.2" xref="S7.SS2.p1.2.m2.1.1.2.cmml"><mi id="S7.SS2.p1.2.m2.1.1.2.2" xref="S7.SS2.p1.2.m2.1.1.2.2.cmml">x</mi><mn id="S7.SS2.p1.2.m2.1.1.2.3" xref="S7.SS2.p1.2.m2.1.1.2.3.cmml">0</mn></msub><mo id="S7.SS2.p1.2.m2.1.1.1" xref="S7.SS2.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S7.SS2.p1.2.m2.1.1.3" xref="S7.SS2.p1.2.m2.1.1.3.cmml"><mi id="S7.SS2.p1.2.m2.1.1.3.2" xref="S7.SS2.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S7.SS2.p1.2.m2.1.1.3.3" xref="S7.SS2.p1.2.m2.1.1.3.3.cmml"><mn id="S7.SS2.p1.2.m2.1.1.3.3.2" xref="S7.SS2.p1.2.m2.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS2.p1.2.m2.1.1.3.3.1" xref="S7.SS2.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S7.SS2.p1.2.m2.1.1.3.3.3" xref="S7.SS2.p1.2.m2.1.1.3.3.3.cmml">128</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS2.p1.2.m2.1.1.3.3.1a" xref="S7.SS2.p1.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S7.SS2.p1.2.m2.1.1.3.3.4" xref="S7.SS2.p1.2.m2.1.1.3.3.4.cmml">192</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.2.m2.1b"><apply id="S7.SS2.p1.2.m2.1.1.cmml" xref="S7.SS2.p1.2.m2.1.1"><in id="S7.SS2.p1.2.m2.1.1.1.cmml" xref="S7.SS2.p1.2.m2.1.1.1"></in><apply id="S7.SS2.p1.2.m2.1.1.2.cmml" xref="S7.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S7.SS2.p1.2.m2.1.1.2.1.cmml" xref="S7.SS2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S7.SS2.p1.2.m2.1.1.2.2.cmml" xref="S7.SS2.p1.2.m2.1.1.2.2">𝑥</ci><cn type="integer" id="S7.SS2.p1.2.m2.1.1.2.3.cmml" xref="S7.SS2.p1.2.m2.1.1.2.3">0</cn></apply><apply id="S7.SS2.p1.2.m2.1.1.3.cmml" xref="S7.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p1.2.m2.1.1.3.1.cmml" xref="S7.SS2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S7.SS2.p1.2.m2.1.1.3.2.cmml" xref="S7.SS2.p1.2.m2.1.1.3.2">ℝ</ci><apply id="S7.SS2.p1.2.m2.1.1.3.3.cmml" xref="S7.SS2.p1.2.m2.1.1.3.3"><times id="S7.SS2.p1.2.m2.1.1.3.3.1.cmml" xref="S7.SS2.p1.2.m2.1.1.3.3.1"></times><cn type="integer" id="S7.SS2.p1.2.m2.1.1.3.3.2.cmml" xref="S7.SS2.p1.2.m2.1.1.3.3.2">3</cn><cn type="integer" id="S7.SS2.p1.2.m2.1.1.3.3.3.cmml" xref="S7.SS2.p1.2.m2.1.1.3.3.3">128</cn><cn type="integer" id="S7.SS2.p1.2.m2.1.1.3.3.4.cmml" xref="S7.SS2.p1.2.m2.1.1.3.3.4">192</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.2.m2.1c">x_{0}\in\mathbb{R}^{3\times 128\times 192}</annotation></semantics></math>, passing <math id="S7.SS2.p1.3.m3.1" class="ltx_Math" alttext="x_{0}" display="inline"><semantics id="S7.SS2.p1.3.m3.1a"><msub id="S7.SS2.p1.3.m3.1.1" xref="S7.SS2.p1.3.m3.1.1.cmml"><mi id="S7.SS2.p1.3.m3.1.1.2" xref="S7.SS2.p1.3.m3.1.1.2.cmml">x</mi><mn id="S7.SS2.p1.3.m3.1.1.3" xref="S7.SS2.p1.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.3.m3.1b"><apply id="S7.SS2.p1.3.m3.1.1.cmml" xref="S7.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S7.SS2.p1.3.m3.1.1.1.cmml" xref="S7.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S7.SS2.p1.3.m3.1.1.2.cmml" xref="S7.SS2.p1.3.m3.1.1.2">𝑥</ci><cn type="integer" id="S7.SS2.p1.3.m3.1.1.3.cmml" xref="S7.SS2.p1.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.3.m3.1c">x_{0}</annotation></semantics></math> to the VAE encoder, we get compressed latent representation <math id="S7.SS2.p1.4.m4.1" class="ltx_Math" alttext="z_{0}\in\mathbb{R}^{4\times 16\times 24}" display="inline"><semantics id="S7.SS2.p1.4.m4.1a"><mrow id="S7.SS2.p1.4.m4.1.1" xref="S7.SS2.p1.4.m4.1.1.cmml"><msub id="S7.SS2.p1.4.m4.1.1.2" xref="S7.SS2.p1.4.m4.1.1.2.cmml"><mi id="S7.SS2.p1.4.m4.1.1.2.2" xref="S7.SS2.p1.4.m4.1.1.2.2.cmml">z</mi><mn id="S7.SS2.p1.4.m4.1.1.2.3" xref="S7.SS2.p1.4.m4.1.1.2.3.cmml">0</mn></msub><mo id="S7.SS2.p1.4.m4.1.1.1" xref="S7.SS2.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S7.SS2.p1.4.m4.1.1.3" xref="S7.SS2.p1.4.m4.1.1.3.cmml"><mi id="S7.SS2.p1.4.m4.1.1.3.2" xref="S7.SS2.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S7.SS2.p1.4.m4.1.1.3.3" xref="S7.SS2.p1.4.m4.1.1.3.3.cmml"><mn id="S7.SS2.p1.4.m4.1.1.3.3.2" xref="S7.SS2.p1.4.m4.1.1.3.3.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS2.p1.4.m4.1.1.3.3.1" xref="S7.SS2.p1.4.m4.1.1.3.3.1.cmml">×</mo><mn id="S7.SS2.p1.4.m4.1.1.3.3.3" xref="S7.SS2.p1.4.m4.1.1.3.3.3.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS2.p1.4.m4.1.1.3.3.1a" xref="S7.SS2.p1.4.m4.1.1.3.3.1.cmml">×</mo><mn id="S7.SS2.p1.4.m4.1.1.3.3.4" xref="S7.SS2.p1.4.m4.1.1.3.3.4.cmml">24</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.4.m4.1b"><apply id="S7.SS2.p1.4.m4.1.1.cmml" xref="S7.SS2.p1.4.m4.1.1"><in id="S7.SS2.p1.4.m4.1.1.1.cmml" xref="S7.SS2.p1.4.m4.1.1.1"></in><apply id="S7.SS2.p1.4.m4.1.1.2.cmml" xref="S7.SS2.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S7.SS2.p1.4.m4.1.1.2.1.cmml" xref="S7.SS2.p1.4.m4.1.1.2">subscript</csymbol><ci id="S7.SS2.p1.4.m4.1.1.2.2.cmml" xref="S7.SS2.p1.4.m4.1.1.2.2">𝑧</ci><cn type="integer" id="S7.SS2.p1.4.m4.1.1.2.3.cmml" xref="S7.SS2.p1.4.m4.1.1.2.3">0</cn></apply><apply id="S7.SS2.p1.4.m4.1.1.3.cmml" xref="S7.SS2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p1.4.m4.1.1.3.1.cmml" xref="S7.SS2.p1.4.m4.1.1.3">superscript</csymbol><ci id="S7.SS2.p1.4.m4.1.1.3.2.cmml" xref="S7.SS2.p1.4.m4.1.1.3.2">ℝ</ci><apply id="S7.SS2.p1.4.m4.1.1.3.3.cmml" xref="S7.SS2.p1.4.m4.1.1.3.3"><times id="S7.SS2.p1.4.m4.1.1.3.3.1.cmml" xref="S7.SS2.p1.4.m4.1.1.3.3.1"></times><cn type="integer" id="S7.SS2.p1.4.m4.1.1.3.3.2.cmml" xref="S7.SS2.p1.4.m4.1.1.3.3.2">4</cn><cn type="integer" id="S7.SS2.p1.4.m4.1.1.3.3.3.cmml" xref="S7.SS2.p1.4.m4.1.1.3.3.3">16</cn><cn type="integer" id="S7.SS2.p1.4.m4.1.1.3.3.4.cmml" xref="S7.SS2.p1.4.m4.1.1.3.3.4">24</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.4.m4.1c">z_{0}\in\mathbb{R}^{4\times 16\times 24}</annotation></semantics></math>. For conditioning, videos are sampled at 5 FPS, passed through the video encoder and a linear projection layer that produces features of size <math id="S7.SS2.p1.5.m5.1" class="ltx_Math" alttext="c_{v}\in\mathbb{R}^{16\times 768}" display="inline"><semantics id="S7.SS2.p1.5.m5.1a"><mrow id="S7.SS2.p1.5.m5.1.1" xref="S7.SS2.p1.5.m5.1.1.cmml"><msub id="S7.SS2.p1.5.m5.1.1.2" xref="S7.SS2.p1.5.m5.1.1.2.cmml"><mi id="S7.SS2.p1.5.m5.1.1.2.2" xref="S7.SS2.p1.5.m5.1.1.2.2.cmml">c</mi><mi id="S7.SS2.p1.5.m5.1.1.2.3" xref="S7.SS2.p1.5.m5.1.1.2.3.cmml">v</mi></msub><mo id="S7.SS2.p1.5.m5.1.1.1" xref="S7.SS2.p1.5.m5.1.1.1.cmml">∈</mo><msup id="S7.SS2.p1.5.m5.1.1.3" xref="S7.SS2.p1.5.m5.1.1.3.cmml"><mi id="S7.SS2.p1.5.m5.1.1.3.2" xref="S7.SS2.p1.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S7.SS2.p1.5.m5.1.1.3.3" xref="S7.SS2.p1.5.m5.1.1.3.3.cmml"><mn id="S7.SS2.p1.5.m5.1.1.3.3.2" xref="S7.SS2.p1.5.m5.1.1.3.3.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS2.p1.5.m5.1.1.3.3.1" xref="S7.SS2.p1.5.m5.1.1.3.3.1.cmml">×</mo><mn id="S7.SS2.p1.5.m5.1.1.3.3.3" xref="S7.SS2.p1.5.m5.1.1.3.3.3.cmml">768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.5.m5.1b"><apply id="S7.SS2.p1.5.m5.1.1.cmml" xref="S7.SS2.p1.5.m5.1.1"><in id="S7.SS2.p1.5.m5.1.1.1.cmml" xref="S7.SS2.p1.5.m5.1.1.1"></in><apply id="S7.SS2.p1.5.m5.1.1.2.cmml" xref="S7.SS2.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S7.SS2.p1.5.m5.1.1.2.1.cmml" xref="S7.SS2.p1.5.m5.1.1.2">subscript</csymbol><ci id="S7.SS2.p1.5.m5.1.1.2.2.cmml" xref="S7.SS2.p1.5.m5.1.1.2.2">𝑐</ci><ci id="S7.SS2.p1.5.m5.1.1.2.3.cmml" xref="S7.SS2.p1.5.m5.1.1.2.3">𝑣</ci></apply><apply id="S7.SS2.p1.5.m5.1.1.3.cmml" xref="S7.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p1.5.m5.1.1.3.1.cmml" xref="S7.SS2.p1.5.m5.1.1.3">superscript</csymbol><ci id="S7.SS2.p1.5.m5.1.1.3.2.cmml" xref="S7.SS2.p1.5.m5.1.1.3.2">ℝ</ci><apply id="S7.SS2.p1.5.m5.1.1.3.3.cmml" xref="S7.SS2.p1.5.m5.1.1.3.3"><times id="S7.SS2.p1.5.m5.1.1.3.3.1.cmml" xref="S7.SS2.p1.5.m5.1.1.3.3.1"></times><cn type="integer" id="S7.SS2.p1.5.m5.1.1.3.3.2.cmml" xref="S7.SS2.p1.5.m5.1.1.3.3.2">16</cn><cn type="integer" id="S7.SS2.p1.5.m5.1.1.3.3.3.cmml" xref="S7.SS2.p1.5.m5.1.1.3.3.3">768</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.5.m5.1c">c_{v}\in\mathbb{R}^{16\times 768}</annotation></semantics></math>.
Audio condition is also a 3-second clip and is processed the same way as the target audio to get <math id="S7.SS2.p1.6.m6.1" class="ltx_Math" alttext="\tilde{c}_{a}\in\mathbb{R}^{4\times 16\times 24}" display="inline"><semantics id="S7.SS2.p1.6.m6.1a"><mrow id="S7.SS2.p1.6.m6.1.1" xref="S7.SS2.p1.6.m6.1.1.cmml"><msub id="S7.SS2.p1.6.m6.1.1.2" xref="S7.SS2.p1.6.m6.1.1.2.cmml"><mover accent="true" id="S7.SS2.p1.6.m6.1.1.2.2" xref="S7.SS2.p1.6.m6.1.1.2.2.cmml"><mi id="S7.SS2.p1.6.m6.1.1.2.2.2" xref="S7.SS2.p1.6.m6.1.1.2.2.2.cmml">c</mi><mo id="S7.SS2.p1.6.m6.1.1.2.2.1" xref="S7.SS2.p1.6.m6.1.1.2.2.1.cmml">~</mo></mover><mi id="S7.SS2.p1.6.m6.1.1.2.3" xref="S7.SS2.p1.6.m6.1.1.2.3.cmml">a</mi></msub><mo id="S7.SS2.p1.6.m6.1.1.1" xref="S7.SS2.p1.6.m6.1.1.1.cmml">∈</mo><msup id="S7.SS2.p1.6.m6.1.1.3" xref="S7.SS2.p1.6.m6.1.1.3.cmml"><mi id="S7.SS2.p1.6.m6.1.1.3.2" xref="S7.SS2.p1.6.m6.1.1.3.2.cmml">ℝ</mi><mrow id="S7.SS2.p1.6.m6.1.1.3.3" xref="S7.SS2.p1.6.m6.1.1.3.3.cmml"><mn id="S7.SS2.p1.6.m6.1.1.3.3.2" xref="S7.SS2.p1.6.m6.1.1.3.3.2.cmml">4</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS2.p1.6.m6.1.1.3.3.1" xref="S7.SS2.p1.6.m6.1.1.3.3.1.cmml">×</mo><mn id="S7.SS2.p1.6.m6.1.1.3.3.3" xref="S7.SS2.p1.6.m6.1.1.3.3.3.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS2.p1.6.m6.1.1.3.3.1a" xref="S7.SS2.p1.6.m6.1.1.3.3.1.cmml">×</mo><mn id="S7.SS2.p1.6.m6.1.1.3.3.4" xref="S7.SS2.p1.6.m6.1.1.3.3.4.cmml">24</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.6.m6.1b"><apply id="S7.SS2.p1.6.m6.1.1.cmml" xref="S7.SS2.p1.6.m6.1.1"><in id="S7.SS2.p1.6.m6.1.1.1.cmml" xref="S7.SS2.p1.6.m6.1.1.1"></in><apply id="S7.SS2.p1.6.m6.1.1.2.cmml" xref="S7.SS2.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S7.SS2.p1.6.m6.1.1.2.1.cmml" xref="S7.SS2.p1.6.m6.1.1.2">subscript</csymbol><apply id="S7.SS2.p1.6.m6.1.1.2.2.cmml" xref="S7.SS2.p1.6.m6.1.1.2.2"><ci id="S7.SS2.p1.6.m6.1.1.2.2.1.cmml" xref="S7.SS2.p1.6.m6.1.1.2.2.1">~</ci><ci id="S7.SS2.p1.6.m6.1.1.2.2.2.cmml" xref="S7.SS2.p1.6.m6.1.1.2.2.2">𝑐</ci></apply><ci id="S7.SS2.p1.6.m6.1.1.2.3.cmml" xref="S7.SS2.p1.6.m6.1.1.2.3">𝑎</ci></apply><apply id="S7.SS2.p1.6.m6.1.1.3.cmml" xref="S7.SS2.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p1.6.m6.1.1.3.1.cmml" xref="S7.SS2.p1.6.m6.1.1.3">superscript</csymbol><ci id="S7.SS2.p1.6.m6.1.1.3.2.cmml" xref="S7.SS2.p1.6.m6.1.1.3.2">ℝ</ci><apply id="S7.SS2.p1.6.m6.1.1.3.3.cmml" xref="S7.SS2.p1.6.m6.1.1.3.3"><times id="S7.SS2.p1.6.m6.1.1.3.3.1.cmml" xref="S7.SS2.p1.6.m6.1.1.3.3.1"></times><cn type="integer" id="S7.SS2.p1.6.m6.1.1.3.3.2.cmml" xref="S7.SS2.p1.6.m6.1.1.3.3.2">4</cn><cn type="integer" id="S7.SS2.p1.6.m6.1.1.3.3.3.cmml" xref="S7.SS2.p1.6.m6.1.1.3.3.3">16</cn><cn type="integer" id="S7.SS2.p1.6.m6.1.1.3.3.4.cmml" xref="S7.SS2.p1.6.m6.1.1.3.3.4">24</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.6.m6.1c">\tilde{c}_{a}\in\mathbb{R}^{4\times 16\times 24}</annotation></semantics></math>, it is then projected to a <math id="S7.SS2.p1.7.m7.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S7.SS2.p1.7.m7.1a"><mn id="S7.SS2.p1.7.m7.1.1" xref="S7.SS2.p1.7.m7.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.7.m7.1b"><cn type="integer" id="S7.SS2.p1.7.m7.1.1.cmml" xref="S7.SS2.p1.7.m7.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.7.m7.1c">2</annotation></semantics></math> dimensional tensor of shape <math id="S7.SS2.p1.8.m8.1" class="ltx_Math" alttext="c_{a}\in\mathbb{R}^{24\times 768}" display="inline"><semantics id="S7.SS2.p1.8.m8.1a"><mrow id="S7.SS2.p1.8.m8.1.1" xref="S7.SS2.p1.8.m8.1.1.cmml"><msub id="S7.SS2.p1.8.m8.1.1.2" xref="S7.SS2.p1.8.m8.1.1.2.cmml"><mi id="S7.SS2.p1.8.m8.1.1.2.2" xref="S7.SS2.p1.8.m8.1.1.2.2.cmml">c</mi><mi id="S7.SS2.p1.8.m8.1.1.2.3" xref="S7.SS2.p1.8.m8.1.1.2.3.cmml">a</mi></msub><mo id="S7.SS2.p1.8.m8.1.1.1" xref="S7.SS2.p1.8.m8.1.1.1.cmml">∈</mo><msup id="S7.SS2.p1.8.m8.1.1.3" xref="S7.SS2.p1.8.m8.1.1.3.cmml"><mi id="S7.SS2.p1.8.m8.1.1.3.2" xref="S7.SS2.p1.8.m8.1.1.3.2.cmml">ℝ</mi><mrow id="S7.SS2.p1.8.m8.1.1.3.3" xref="S7.SS2.p1.8.m8.1.1.3.3.cmml"><mn id="S7.SS2.p1.8.m8.1.1.3.3.2" xref="S7.SS2.p1.8.m8.1.1.3.3.2.cmml">24</mn><mo lspace="0.222em" rspace="0.222em" id="S7.SS2.p1.8.m8.1.1.3.3.1" xref="S7.SS2.p1.8.m8.1.1.3.3.1.cmml">×</mo><mn id="S7.SS2.p1.8.m8.1.1.3.3.3" xref="S7.SS2.p1.8.m8.1.1.3.3.3.cmml">768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.8.m8.1b"><apply id="S7.SS2.p1.8.m8.1.1.cmml" xref="S7.SS2.p1.8.m8.1.1"><in id="S7.SS2.p1.8.m8.1.1.1.cmml" xref="S7.SS2.p1.8.m8.1.1.1"></in><apply id="S7.SS2.p1.8.m8.1.1.2.cmml" xref="S7.SS2.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S7.SS2.p1.8.m8.1.1.2.1.cmml" xref="S7.SS2.p1.8.m8.1.1.2">subscript</csymbol><ci id="S7.SS2.p1.8.m8.1.1.2.2.cmml" xref="S7.SS2.p1.8.m8.1.1.2.2">𝑐</ci><ci id="S7.SS2.p1.8.m8.1.1.2.3.cmml" xref="S7.SS2.p1.8.m8.1.1.2.3">𝑎</ci></apply><apply id="S7.SS2.p1.8.m8.1.1.3.cmml" xref="S7.SS2.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S7.SS2.p1.8.m8.1.1.3.1.cmml" xref="S7.SS2.p1.8.m8.1.1.3">superscript</csymbol><ci id="S7.SS2.p1.8.m8.1.1.3.2.cmml" xref="S7.SS2.p1.8.m8.1.1.3.2">ℝ</ci><apply id="S7.SS2.p1.8.m8.1.1.3.3.cmml" xref="S7.SS2.p1.8.m8.1.1.3.3"><times id="S7.SS2.p1.8.m8.1.1.3.3.1.cmml" xref="S7.SS2.p1.8.m8.1.1.3.3.1"></times><cn type="integer" id="S7.SS2.p1.8.m8.1.1.3.3.2.cmml" xref="S7.SS2.p1.8.m8.1.1.3.3.2">24</cn><cn type="integer" id="S7.SS2.p1.8.m8.1.1.3.3.3.cmml" xref="S7.SS2.p1.8.m8.1.1.3.3.3">768</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.8.m8.1c">c_{a}\in\mathbb{R}^{24\times 768}</annotation></semantics></math>. For classifier-free guidance, we set the scale <math id="S7.SS2.p1.9.m9.1" class="ltx_Math" alttext="\omega=6.5" display="inline"><semantics id="S7.SS2.p1.9.m9.1a"><mrow id="S7.SS2.p1.9.m9.1.1" xref="S7.SS2.p1.9.m9.1.1.cmml"><mi id="S7.SS2.p1.9.m9.1.1.2" xref="S7.SS2.p1.9.m9.1.1.2.cmml">ω</mi><mo id="S7.SS2.p1.9.m9.1.1.1" xref="S7.SS2.p1.9.m9.1.1.1.cmml">=</mo><mn id="S7.SS2.p1.9.m9.1.1.3" xref="S7.SS2.p1.9.m9.1.1.3.cmml">6.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.9.m9.1b"><apply id="S7.SS2.p1.9.m9.1.1.cmml" xref="S7.SS2.p1.9.m9.1.1"><eq id="S7.SS2.p1.9.m9.1.1.1.cmml" xref="S7.SS2.p1.9.m9.1.1.1"></eq><ci id="S7.SS2.p1.9.m9.1.1.2.cmml" xref="S7.SS2.p1.9.m9.1.1.2">𝜔</ci><cn type="float" id="S7.SS2.p1.9.m9.1.1.3.cmml" xref="S7.SS2.p1.9.m9.1.1.3">6.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.9.m9.1c">\omega=6.5</annotation></semantics></math>, and use DPM-Solver <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> for accelerated inference using only <math id="S7.SS2.p1.10.m10.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S7.SS2.p1.10.m10.1a"><mn id="S7.SS2.p1.10.m10.1.1" xref="S7.SS2.p1.10.m10.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.10.m10.1b"><cn type="integer" id="S7.SS2.p1.10.m10.1.1.cmml" xref="S7.SS2.p1.10.m10.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.10.m10.1c">25</annotation></semantics></math> sampling steps.
For the mel-spectrogram to waveform vocoder HiFi-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we train the model from scratch with the mel-spectrogram processing hyperparameters aligned with that of our AV-LDM. During training, we freeze the pre-trained VAE, and train the LDM model on Ego4D-Sounds for 8 epochs with batch size 720. We use the AdamW optimizer with a learning rate of <math id="S7.SS2.p1.11.m11.1" class="ltx_Math" alttext="1e-4" display="inline"><semantics id="S7.SS2.p1.11.m11.1a"><mrow id="S7.SS2.p1.11.m11.1.1" xref="S7.SS2.p1.11.m11.1.1.cmml"><mrow id="S7.SS2.p1.11.m11.1.1.2" xref="S7.SS2.p1.11.m11.1.1.2.cmml"><mn id="S7.SS2.p1.11.m11.1.1.2.2" xref="S7.SS2.p1.11.m11.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S7.SS2.p1.11.m11.1.1.2.1" xref="S7.SS2.p1.11.m11.1.1.2.1.cmml">​</mo><mi id="S7.SS2.p1.11.m11.1.1.2.3" xref="S7.SS2.p1.11.m11.1.1.2.3.cmml">e</mi></mrow><mo id="S7.SS2.p1.11.m11.1.1.1" xref="S7.SS2.p1.11.m11.1.1.1.cmml">−</mo><mn id="S7.SS2.p1.11.m11.1.1.3" xref="S7.SS2.p1.11.m11.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.11.m11.1b"><apply id="S7.SS2.p1.11.m11.1.1.cmml" xref="S7.SS2.p1.11.m11.1.1"><minus id="S7.SS2.p1.11.m11.1.1.1.cmml" xref="S7.SS2.p1.11.m11.1.1.1"></minus><apply id="S7.SS2.p1.11.m11.1.1.2.cmml" xref="S7.SS2.p1.11.m11.1.1.2"><times id="S7.SS2.p1.11.m11.1.1.2.1.cmml" xref="S7.SS2.p1.11.m11.1.1.2.1"></times><cn type="integer" id="S7.SS2.p1.11.m11.1.1.2.2.cmml" xref="S7.SS2.p1.11.m11.1.1.2.2">1</cn><ci id="S7.SS2.p1.11.m11.1.1.2.3.cmml" xref="S7.SS2.p1.11.m11.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S7.SS2.p1.11.m11.1.1.3.cmml" xref="S7.SS2.p1.11.m11.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.11.m11.1c">1e-4</annotation></semantics></math>. HiFi-GAN is trained on a combination of 0.5s clips from Ego4D<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, Epic-Kitchens <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and AudioSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. We use AdamW to train HiFi-GAN with a learning rate of <math id="S7.SS2.p1.12.m12.1" class="ltx_Math" alttext="2e-4" display="inline"><semantics id="S7.SS2.p1.12.m12.1a"><mrow id="S7.SS2.p1.12.m12.1.1" xref="S7.SS2.p1.12.m12.1.1.cmml"><mrow id="S7.SS2.p1.12.m12.1.1.2" xref="S7.SS2.p1.12.m12.1.1.2.cmml"><mn id="S7.SS2.p1.12.m12.1.1.2.2" xref="S7.SS2.p1.12.m12.1.1.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S7.SS2.p1.12.m12.1.1.2.1" xref="S7.SS2.p1.12.m12.1.1.2.1.cmml">​</mo><mi id="S7.SS2.p1.12.m12.1.1.2.3" xref="S7.SS2.p1.12.m12.1.1.2.3.cmml">e</mi></mrow><mo id="S7.SS2.p1.12.m12.1.1.1" xref="S7.SS2.p1.12.m12.1.1.1.cmml">−</mo><mn id="S7.SS2.p1.12.m12.1.1.3" xref="S7.SS2.p1.12.m12.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.12.m12.1b"><apply id="S7.SS2.p1.12.m12.1.1.cmml" xref="S7.SS2.p1.12.m12.1.1"><minus id="S7.SS2.p1.12.m12.1.1.1.cmml" xref="S7.SS2.p1.12.m12.1.1.1"></minus><apply id="S7.SS2.p1.12.m12.1.1.2.cmml" xref="S7.SS2.p1.12.m12.1.1.2"><times id="S7.SS2.p1.12.m12.1.1.2.1.cmml" xref="S7.SS2.p1.12.m12.1.1.2.1"></times><cn type="integer" id="S7.SS2.p1.12.m12.1.1.2.2.cmml" xref="S7.SS2.p1.12.m12.1.1.2.2">2</cn><ci id="S7.SS2.p1.12.m12.1.1.2.3.cmml" xref="S7.SS2.p1.12.m12.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S7.SS2.p1.12.m12.1.1.3.cmml" xref="S7.SS2.p1.12.m12.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.12.m12.1c">2e-4</annotation></semantics></math> and a batch size of 64 for 120k steps.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.2" class="ltx_p"><span id="S7.SS2.p2.2.1" class="ltx_text ltx_font_bold">Audio-visual representation learning.</span> We use Timesformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> as the video encoder, and AST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> as the audio encoder. We train video and audio encoders for 5 epochs with batch size <math id="S7.SS2.p2.1.m1.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S7.SS2.p2.1.m1.1a"><mn id="S7.SS2.p2.1.m1.1.1" xref="S7.SS2.p2.1.m1.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.1.m1.1b"><cn type="integer" id="S7.SS2.p2.1.m1.1.1.cmml" xref="S7.SS2.p2.1.m1.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.1.m1.1c">256</annotation></semantics></math>. We use the InfoNCE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> loss and Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> with a learning rate <math id="S7.SS2.p2.2.m2.1" class="ltx_Math" alttext="1e-4" display="inline"><semantics id="S7.SS2.p2.2.m2.1a"><mrow id="S7.SS2.p2.2.m2.1.1" xref="S7.SS2.p2.2.m2.1.1.cmml"><mrow id="S7.SS2.p2.2.m2.1.1.2" xref="S7.SS2.p2.2.m2.1.1.2.cmml"><mn id="S7.SS2.p2.2.m2.1.1.2.2" xref="S7.SS2.p2.2.m2.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S7.SS2.p2.2.m2.1.1.2.1" xref="S7.SS2.p2.2.m2.1.1.2.1.cmml">​</mo><mi id="S7.SS2.p2.2.m2.1.1.2.3" xref="S7.SS2.p2.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="S7.SS2.p2.2.m2.1.1.1" xref="S7.SS2.p2.2.m2.1.1.1.cmml">−</mo><mn id="S7.SS2.p2.2.m2.1.1.3" xref="S7.SS2.p2.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p2.2.m2.1b"><apply id="S7.SS2.p2.2.m2.1.1.cmml" xref="S7.SS2.p2.2.m2.1.1"><minus id="S7.SS2.p2.2.m2.1.1.1.cmml" xref="S7.SS2.p2.2.m2.1.1.1"></minus><apply id="S7.SS2.p2.2.m2.1.1.2.cmml" xref="S7.SS2.p2.2.m2.1.1.2"><times id="S7.SS2.p2.2.m2.1.1.2.1.cmml" xref="S7.SS2.p2.2.m2.1.1.2.1"></times><cn type="integer" id="S7.SS2.p2.2.m2.1.1.2.2.cmml" xref="S7.SS2.p2.2.m2.1.1.2.2">1</cn><ci id="S7.SS2.p2.2.m2.1.1.2.3.cmml" xref="S7.SS2.p2.2.m2.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S7.SS2.p2.2.m2.1.1.3.cmml" xref="S7.SS2.p2.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p2.2.m2.1c">1e-4</annotation></semantics></math>.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Dataset Details</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">To evaluate the effectiveness of our proposed ambient-aware action sound generation model, we leverage Ego4D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, a large-scale egocentric video dataset for daily human activities. While our model is capable of disentangling action sound from ambient sound, there is little value in learning on data that only contain ambient sounds or background speech. Our goal is to curate an in-the-wild action dataset that has meaningful action sounds. We design a four-stage pipeline consisting of both learning-based tagging tools and rule-based filters to curate the Ego4D-Sounds dataset. To be consistent with the public splits of Ego4D benchmarks, we use all 7.5K videos in the training set, where we extract 3.8M clips centered at the narrations’ timestamps with the left and right margins being 1.5s, i.e. each clip is 3s long, <span id="S7.SS3.p1.1.1" class="ltx_text" style="color:#000000;">which we find to be sufficiently long enough to capture the narrated action.</span></p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p">We first remove all clips without sounds, resulting in 2.5M clips. We then filter the above clips based on the scenarios. Each Ego4D video has a scenario label, categorizing the activity depicted in the video. We go through all scenario labels and pick 28 scenarios that are mainly social scenarios, e.g., "playing board games", "attending a party", "talking with friends", where majority of the sounds are speech or off-screen sounds with no on-screen actions. We remove videos with these tags, resulting in 3.1K videos and 1.7M clips.</p>
</div>
<div id="S7.SS3.p3" class="ltx_para">
<p id="S7.SS3.p3.1" class="ltx_p">While the previous stage removed videos for social scenarios as a whole, there are still many clips that have only speech or background music. To detect these clips, we use an off-the-shelf audio tagging tool to tag the remaining clips. The goal is to remove clips that have solely off-screen sounds, i.e., speech and music. So we threshold the tagged probability at <math id="S7.SS3.p3.1.m1.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S7.SS3.p3.1.m1.1a"><mn id="S7.SS3.p3.1.m1.1.1" xref="S7.SS3.p3.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.p3.1.m1.1b"><cn type="float" id="S7.SS3.p3.1.m1.1.1.cmml" xref="S7.SS3.p3.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p3.1.m1.1c">0.5</annotation></semantics></math>, i.e., removing clips that most likely only contain off-screen sounds and not action sounds.
This filtering process further removed 0.5M clips, with 1.23M clips remaining.
</p>
</div>
<div id="S7.SS3.p4" class="ltx_para">
<p id="S7.SS3.p4.1" class="ltx_p">Lastly, we also observe that in a long video clip, there are silent periods when no sounding action occurs. Based on this observation, we devise an energy-based filtering process, i.e. we normalize the amplitude of each clip with respect to the maximum amplitude of audio in the video. We then convert the amplitude to dB and remove clips with energy below -60 dB. This results in 1.18M clips.</p>
</div>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>Evaluation Metric Details</h3>

<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.1" class="ltx_p">For the audio-visual synchronization (AV-Sync) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> metric, we train a synchronization binary classification model on Ego4D-Sounds, and use it to judge whether the generated audio is synchronized with the video. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, to construct the input to the classification model, we input paired and synced video and audio, unpaired video and audio, and paired but unsynced video and audio 50%, 25%, and 25% of the time respectively. The model uses Timesformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> as the video encoder, AST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> as the audio encoder, and a 3 layer MLP as the classification head which takes the CLS tokens from the two encoders, concatenates them in the feature dimension, and produces a probability indicating the synchronization. We train this model for 30k steps with AdamW optimizer, which achieves a classification accuracy of 70.6% on the validation set.</p>
</div>
</section>
<section id="S7.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5 </span>Human Evaluation Details</h3>

<div id="S7.SS5.p1" class="ltx_para">
<p id="S7.SS5.p1.1" class="ltx_p">For the human evaluation, we first compile a guideline document, clarifying and defining the objectives of the survey and what the participant should be looking for. There are two main objectives: 1) select video(s) with the most plausible action sounds (e.g., object collisions, water running) that are semantically and temporally matching with the visual frames, and 2) select video(s) with the least ambient noise. We also provide multiple positive and negative examples for each criterion in the guideline document. We ask participants to read the guidelines before doing the survey. We show one example of the survey interface in <a href="#S7.F10" title="In 7.5 Human Evaluation Details ‣ 7 Supplementary ‣ Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<figure id="S7.F10" class="ltx_figure"><img src="/html/2406.09272/assets/figures/survey.png" id="S7.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="265" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S7.F10.3.2" class="ltx_text" style="font-size:90%;">Survey interface. For each example, we ask participants to select video(s) that are most plausible with the video and the video(s) that have the least ambient sound.</span></figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.09271" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.09272" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.09272">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.09272" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.09273" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 19:41:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
