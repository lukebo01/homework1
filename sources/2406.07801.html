<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.07801] PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models</title><meta property="og:description" content="Recently, there have been attempts to integrate various speech processing tasks into a unified model. However, few previous works directly demonstrated that joint optimization of diverse tasks in multitask speech model…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.07801">

<!--Generated on Sat Jul  6 00:55:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.2" class="ltx_ERROR undefined">\interspeechfinaltrue</span><span id="p1.3" class="ltx_ERROR undefined">\name</span>
<p id="p1.1" class="ltx_p">RunyanYang
<span id="p1.1.1" class="ltx_ERROR undefined">\name</span>HuibaoYang
<span id="p1.1.2" class="ltx_ERROR undefined">\name</span>XiqingZhang
<span id="p1.1.3" class="ltx_ERROR undefined">\name</span>TiantianYe
<span id="p1.1.4" class="ltx_ERROR undefined">\name</span>YingLiu
<span id="p1.1.5" class="ltx_ERROR undefined">\name</span>YingyingGao
<span id="p1.1.6" class="ltx_ERROR undefined">\name</span>
<br class="ltx_break">ShileiZhang<sup id="p1.1.7" class="ltx_sup">∗</sup>
<span id="p1.1.8" class="ltx_ERROR undefined">\name</span>ChaoDeng
<span id="p1.1.9" class="ltx_ERROR undefined">\name</span>JunlanFeng




</p>
</div>
<h1 class="ltx_title ltx_title_document">PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Recently, there have been attempts to integrate various speech processing tasks into a unified model. However, few previous works directly demonstrated that joint optimization of diverse tasks in multitask speech models has positive influence on the performance of individual tasks. In this paper we present a multitask speech model – PolySpeech, which supports speech recognition, speech synthesis, and two speech classification tasks. PolySpeech takes multi-modal language model as its core structure and uses semantic representations as speech inputs. We introduce semantic speech embedding tokenization and speech reconstruction methods to PolySpeech, enabling efficient generation of high-quality speech for any given speaker. PolySpeech shows competitiveness across various tasks compared to single-task models. In our experiments, multitask optimization achieves performance comparable to single-task optimization and is especially beneficial for specific tasks.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>multitask framework, auto-regressive language model, ASR, TTS, speech classification
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recently researchers have proposed model architectures and training techniques to aggregate various speech processing tasks into one unified framework. Performance of these frameworks demonstrate the capability of deep learning models like Transformer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to learn and express the knowledge in various modalities required for diverse speech tasks.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Classic encoder-decoder architecture has been explored to model multiple speech tasks. For example,
Whisper<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is capable of performing speech processing tasks such as speech recognition, speech translation, and language identification, utilizing large-scale multilingual supervised speech as training data for an encoder-decoder Transformer.
There are also recent attempts to aggregate multiple speech tasks in simpler decoder-only language model (LM) structures instead of encoder-decoder. VioLA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, AudioPaLM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, LauraGPT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, SpeechGPT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, etc. support speech transcription and generation tasks by modeling both speech and text representations with Transformer LMs. Speech representations used by these models can be categorised into two types – acoustic ones and semantic ones. Acoustic speech representations are generated from speech waveform through speech codec methods, e.g. EnCodec<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and SoundStream<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Semantic representations are usually extracted from self-supervised learning models such as wav2vec<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and HuBERT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Previous literature<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> has analyzed the performance differences between the proposed multitask speech models and existing single-task models, but few studies compare these models under fair enough experimental conditions, such as same supervised training data.
The aim of this paper is to demonstrate that multitask models are in deed competitive in performance on various tasks compared to single-task models.
We intend to find out whether joint optimization of various speech tasks within a single auto-regressive Transformer decoder framework improves performance comparing to single-task optimization.
We are also curious whether the multitask model benefits more from semantic speech representations or from acoustic ones.
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we propose a multitask speech model framework, which we call PolySpeech. PolySpeech's core structure is a multi-modal decoder-only Transformer LM, which autoregressively predicts speech or text tokens. We integrate tasks of speech recognition (ASR), speech synthesis (TTS), spoken language identification (LID), and gender identification (GID) in PolySpeech. These tasks, covering three main types of speech tasks – transcription, generation, and classification, are jointly optimized in a supervised manner. We prefer to using semantic speech representations rather than using acoustic ones in PolySpeech for better performance. PolySpeech is highly flexible and can be further extended to other speech tasks.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Discretization of speech representations is crucial for autoregressive prediction models to generate speech. In preliminary experiments, we have found that the k-means discretization method used in AudioLM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and Spear-TTS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> would lead to loss of acoustic information. We tried to resynthesize HuBERT k-means tokens of Mandarin Chinese speech with a HiFi-GAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> vocoder and observed tone inaccuracies in synthesized speech, which are unacceptable for Mandarin. For better speech generation performance, we use a codec method for semantic embedding of speech in PolySpeech, which preserves more complete acoustic information in discrete tokens than the k-means method.
Focusing on producing high quality speech while controlling acoustic conditions, we also design a semantic speech token decoder that generates speech waveform given discrete semantic tokens and a speech prompt from any speaker.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We summarize the contribution of this paper as follows:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a multitask speech model based on multi-modal LM and semantic speech representations, which is competitive across various tasks compared to single-task models.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We introduce a high-fidelity speech codec method and a semantic speech token decoder to the multitask speech model, enabling efficient speech generation for any given speaker.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We conduct meaningful experiments, demonstrating that multitask optimization achieves performance on par with single-task optimization and is beneficial for certain tasks.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>PolySpeech</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section we describe PolySpeech, our proposed multitask speech model framework. A diagram of PolySpeech is illustrated in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Model architecture ‣ 2 PolySpeech ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Model architecture</h3>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2406.07801/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="248" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A brief diagram of the PolySpeech framework.</figcaption>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">As shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Model architecture ‣ 2 PolySpeech ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, PolySpeech consists of three basic parts – a decoder-only Transformer-based multi-modal LM, tokenizers for inputs, and de-tokenizers for outputs.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The multi-modal Transformer LM is the core structure of PolySpeech. Given discrete tokens of a speech or text sequence, which is the source sequence of a speech task, the LM predicts the task's target token sequence autoregressively. The target sequence is also in speech or text modality, depending on task. The LM embeds input token sequences from each modality into a continuous vector space using modal-specific token embedding layers, and uses task-specific linear layers to generate output target logits. When we take multi-dimensional speech tokens as model input, such as 8-dimensional EnCodec<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> tokens, we use different embedding layers to embed tokens from each dimension and average each embedding layer's output.
We also attempt to directly use continuous embedding vectors instead of using discrete tokens as the LM's input source speech sequence. In this case, a linear layer is used to map the source embedding vectors into the LM's input vector space.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Besides the LM, PolySpeech also contains input tokenizer and output de-tokenizer modules for each modality. These modules are prepared before the training of the LM and not optimized together with the LM. For text modality, the tokenizer maps original text into sequences of text tokens, while the de-tokenizer maps the LM's output tokens to text. For speech modality, the tokenizer can be any type of audio encoder that converts speech waveform into discrete speech tokens or continuous speech embeddings, such as EnCodec or HuBERT, while the de-tokenizer is a speech token decoder that constructs waveform given the LM's output tokens.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Model input and output</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Source and target sequence of a task are concatenated as the input of the multi-modal LM. A task-specific ``task ID'' token in inserted between the source and target sequences, enabling PolySpeech to distinguish different tasks. PolySpeech handles task ID tokens with a special token embedding layer.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">During training, ground-truth target sequence is shifted left and used as training targets. Cross entropy loss is computed between training targets and logits predicted by the LM. In addition, an upper triangular attention mask is applied to the target sequence to ensure causality.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">During inference, first the source sequence and task ID are feed into the LM, and then the model predict target sequence in an autoregressive manner. Top-K sampling and beam search methods are used for inference with speech and text modality outputs, respectively. The inference process ends when the LM outputs an ``end of sentence (EOS)'' token.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">In the following sections, we will describe the input and output sequences as well as speech tokenizers and de-tokenizers for each task. Brief information can also be found in Table <a href="#S2.T1" title="Table 1 ‣ 2.2 Model input and output ‣ 2 PolySpeech ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Source and target sequences of each task.</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Task</span></td>
<td id="S2.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Source sequence</span></td>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Target sequence</span></td>
</tr>
<tr id="S2.T1.1.2" class="ltx_tr">
<td id="S2.T1.1.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.2.1.1" class="ltx_text" style="font-size:80%;">ASR</span></td>
<td id="S2.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.2.2.1" class="ltx_text" style="font-size:80%;">speech tokens/embeddings</span></td>
<td id="S2.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.2.3.1" class="ltx_text" style="font-size:80%;">text tokens</span></td>
</tr>
<tr id="S2.T1.1.3" class="ltx_tr">
<td id="S2.T1.1.3.1" class="ltx_td ltx_align_center"><span id="S2.T1.1.3.1.1" class="ltx_text" style="font-size:80%;">TTS</span></td>
<td id="S2.T1.1.3.2" class="ltx_td ltx_align_center"><span id="S2.T1.1.3.2.1" class="ltx_text" style="font-size:80%;">text tokens</span></td>
<td id="S2.T1.1.3.3" class="ltx_td ltx_align_center"><span id="S2.T1.1.3.3.1" class="ltx_text" style="font-size:80%;">speech tokens</span></td>
</tr>
<tr id="S2.T1.1.4" class="ltx_tr">
<td id="S2.T1.1.4.1" class="ltx_td ltx_align_center"><span id="S2.T1.1.4.1.1" class="ltx_text" style="font-size:80%;">LID</span></td>
<td id="S2.T1.1.4.2" class="ltx_td ltx_align_center"><span id="S2.T1.1.4.2.1" class="ltx_text" style="font-size:80%;">speech tokens/embeddings</span></td>
<td id="S2.T1.1.4.3" class="ltx_td ltx_align_center"><span id="S2.T1.1.4.3.1" class="ltx_text" style="font-size:80%;">language tag</span></td>
</tr>
<tr id="S2.T1.1.5" class="ltx_tr">
<td id="S2.T1.1.5.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.1.5.1.1" class="ltx_text" style="font-size:80%;">GID</span></td>
<td id="S2.T1.1.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.1.5.2.1" class="ltx_text" style="font-size:80%;">speech tokens/embeddings</span></td>
<td id="S2.T1.1.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.1.5.3.1" class="ltx_text" style="font-size:80%;">gender tag</span></td>
</tr>
</table>
</figure>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>ASR</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">Source and target sequences of the ASR task are in speech and text modalities, respectively. Either semantic-based speech token/embedding or acoustic speech token/embedding can be used as source sequence.
We prefer to semantic speech token/embeddings rather than acoustic ones for better performance.
Text transcript of the speech utterance is converted to a token sequence using lexicons.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>TTS</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">PolySpeech is able to synthesize speech for any in-domain or out-of-domain speaker.
The source and target sequences of TTS are exactly inverse to those of ASR.
In TTS task, the LM predicts tokens' logits for target speech, which means that we must represent speech in a discrete form. The discretization method we take will be presented later in Section <a href="#S2.SS3.SSS1" title="2.3.1 Semantic speech embedding tokenization ‣ 2.3 Speech tokenizer and de-tokenizer for TTS ‣ 2 PolySpeech ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.1</span></a>.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">To accomplish TTS for any given speaker, PolySpeech receives a speech prompt for speaker information. During inference, we first sequentially input the following tokens into the LM, and then predict target speech tokens: the speech prompt's transcript text tokens, TTS source text tokens, TTS task ID, and the speech prompt's speech tokens. The speech prompt is also fed into the speech de-tokenizer, which will be introduced in Section <a href="#S2.SS3.SSS2" title="2.3.2 Speech waveform generation ‣ 2.3 Speech tokenizer and de-tokenizer for TTS ‣ 2 PolySpeech ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.2</span></a>.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Classification tasks (LID and GID)</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">The speech classification tasks share the same source sequence with ASR. The target sequence is a classification tag, which can also be considered as a special text token sequence of length 1.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Speech tokenizer and de-tokenizer for TTS</h3>

<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Semantic speech embedding tokenization</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">In order to efficiently descritize semantic speech embeddings for TTS task, we apply a codec method, which we call Semantic Speech Embedding Tokenization (SSET). In SSET, a model that shares a similar structure as AudioDec<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> is used, which consists of a convolutional encoder-decoder network and a residual vector quantizer (RVQ). During training, both input and output are continuous semantic speech representation, and the objective function is L2 loss. In PolySpeech we use
the output of the first RVQ layer as discrete speech tokens. The SSET we proposed allows for a high-fidelity preservation of acoustic information.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2406.07801/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="258" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Diagram of the semantic speech token decoder.</figcaption>
</figure>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Speech waveform generation</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">Drawing inspiration from VITS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, we design a non-autoregressive semantic token decoder to generate speech waveforms using semantic speech tokens. The decoder's model architecture is illustrated in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.3.1 Semantic speech embedding tokenization ‣ 2.3 Speech tokenizer and de-tokenizer for TTS ‣ 2 PolySpeech ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We have made several essential modifications on the basis of the VITS architecture for fast and efficient waveform generation. First, since we use semantic speech tokens rather than text, which are already aligned to the speech waveform, we omit the alignment estimation methods used in VITS. Second, speech prompts are used to control acoustic features, enabling TTS for any given speaker. We use a speaker encoder consisting of LSTM, linear layer, and ReLU activation function to extract the speaker embedding of the speech prompt as an input to the decoder. What's more, we replace the HiFi-GAN vocoder with VOCOS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, which generates high-fidelity speech and accelerates inference speed.</p>
</div>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Parameter initialization and multitask optimization</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Before PolySpeech training, we initialize the multi-modal LM's parameters as well as the text token embedding layer parameters using a well-trained text-modality-only Transformer LM.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">For each batch of the PolySpeech optimization process, we randomly select a task and calculate gradient using training data from this task. The probability of each task being selected depends on the proportion of this task's training data volume to the total training data volume of all tasks. The gradients of several batches are accumulated for a single update of the model.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental settings</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Standard Mandarin Chinese speech corpora are used for ASR and TTS tasks. Aishell-1<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, Aishell-2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, WenetSpeech<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, KeSpeech<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and our internal Mandarin dialog corpora are used together as the training set. The total amount of speech used for ASR and TTS training is 26k hours. We use development subsets of the four open-source corpora for validation and use Aishell-1 test for evaluation. In ASR experiments, we also train models solely on Aishell-1 to ensure a fairer comparison with baseline and enhance experiment efficiency.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Training data of LID and GID tasks consists of speech in four Chinese dialects – Standard Mandarin, Cantonese, Hokkien, and Southwestern (SW) Mandarin – as well as English. Information of these corpora is illustrated in Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Datasets ‣ 3 Experimental settings ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We resample the training data to maintain balance in data volume for each classification category. For LID, we randomly sample 500 hours of speech from the corpora of each language. For GID, we randomly sample 500 hours of speech for each gender (male and female) from the entire corpora. Validation sets are sampled from the same sources of corresponding training sets.
We use out-of-distribution data to evaluate classification performance of models. LID test set information is listed in Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Datasets ‣ 3 Experimental settings ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
For GID, We use ASR-SCKwsptSC
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://magichub.com/datasets/mandarin-chinese-scripted-speech-corpus-keyword-spotting/</span></span></span>
<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://magichub.com/datasets/mandarin-chinese-scripted-speech-corpus-keyword-spotting-2/</span></span></span>
as test set, which contains 9613 and 5962 utterances from female and male speakers, respectively. Utterances shorter than 1.6s are excluded from classification training, validation, and test sets.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">WuDaoCorpora Text<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is used as the training data for the text-only LM mentioned in Section <a href="#S2.SS4" title="2.4 Parameter initialization and multitask optimization ‣ 2 PolySpeech ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>LID and GID training corpora (before sampling).</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Dataset</span></td>
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Language(dialect)</span></td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Hours</span></td>
<td id="S3.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Style</span></td>
</tr>
<tr id="S3.T2.1.2" class="ltx_tr">
<td id="S3.T2.1.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.2.1.1" class="ltx_text" style="font-size:80%;">Aishell-2</span></td>
<td id="S3.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.2.2.1" class="ltx_text" style="font-size:80%;">Standard Mandarin</span></td>
<td id="S3.T2.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.2.3.1" class="ltx_text" style="font-size:80%;">1000</span></td>
<td id="S3.T2.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.2.4.1" class="ltx_text" style="font-size:80%;">reading</span></td>
</tr>
<tr id="S3.T2.1.3" class="ltx_tr">
<td id="S3.T2.1.3.1" class="ltx_td ltx_align_center"><span id="S3.T2.1.3.1.1" class="ltx_text" style="font-size:80%;">internal</span></td>
<td id="S3.T2.1.3.2" class="ltx_td ltx_align_center"><span id="S3.T2.1.3.2.1" class="ltx_text" style="font-size:80%;">Standard Mandarin</span></td>
<td id="S3.T2.1.3.3" class="ltx_td ltx_align_center"><span id="S3.T2.1.3.3.1" class="ltx_text" style="font-size:80%;">1787</span></td>
<td id="S3.T2.1.3.4" class="ltx_td ltx_align_center"><span id="S3.T2.1.3.4.1" class="ltx_text" style="font-size:80%;">dialog</span></td>
</tr>
<tr id="S3.T2.1.4" class="ltx_tr">
<td id="S3.T2.1.4.1" class="ltx_td ltx_align_center"><span id="S3.T2.1.4.1.1" class="ltx_text" style="font-size:80%;">internal</span></td>
<td id="S3.T2.1.4.2" class="ltx_td ltx_align_center"><span id="S3.T2.1.4.2.1" class="ltx_text" style="font-size:80%;">Cantonese</span></td>
<td id="S3.T2.1.4.3" class="ltx_td ltx_align_center"><span id="S3.T2.1.4.3.1" class="ltx_text" style="font-size:80%;">1250</span></td>
<td id="S3.T2.1.4.4" class="ltx_td ltx_align_center"><span id="S3.T2.1.4.4.1" class="ltx_text" style="font-size:80%;">dialog</span></td>
</tr>
<tr id="S3.T2.1.5" class="ltx_tr">
<td id="S3.T2.1.5.1" class="ltx_td ltx_align_center"><span id="S3.T2.1.5.1.1" class="ltx_text" style="font-size:80%;">internal</span></td>
<td id="S3.T2.1.5.2" class="ltx_td ltx_align_center"><span id="S3.T2.1.5.2.1" class="ltx_text" style="font-size:80%;">Hokkien</span></td>
<td id="S3.T2.1.5.3" class="ltx_td ltx_align_center"><span id="S3.T2.1.5.3.1" class="ltx_text" style="font-size:80%;">451</span></td>
<td id="S3.T2.1.5.4" class="ltx_td ltx_align_center"><span id="S3.T2.1.5.4.1" class="ltx_text" style="font-size:80%;">dialog</span></td>
</tr>
<tr id="S3.T2.1.6" class="ltx_tr">
<td id="S3.T2.1.6.1" class="ltx_td ltx_align_center"><span id="S3.T2.1.6.1.1" class="ltx_text" style="font-size:80%;">internal</span></td>
<td id="S3.T2.1.6.2" class="ltx_td ltx_align_center"><span id="S3.T2.1.6.2.1" class="ltx_text" style="font-size:80%;">SW Mandarin</span></td>
<td id="S3.T2.1.6.3" class="ltx_td ltx_align_center"><span id="S3.T2.1.6.3.1" class="ltx_text" style="font-size:80%;">1500</span></td>
<td id="S3.T2.1.6.4" class="ltx_td ltx_align_center"><span id="S3.T2.1.6.4.1" class="ltx_text" style="font-size:80%;">dialog</span></td>
</tr>
<tr id="S3.T2.1.7" class="ltx_tr">
<td id="S3.T2.1.7.1" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S3.T2.1.7.1.1" class="ltx_text" style="font-size:80%;">LibriSpeech</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.1.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S3.T2.1.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T2.1.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.7.2.1" class="ltx_text" style="font-size:80%;">English</span></td>
<td id="S3.T2.1.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.7.3.1" class="ltx_text" style="font-size:80%;">961</span></td>
<td id="S3.T2.1.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.7.4.1" class="ltx_text" style="font-size:80%;">reading</span></td>
</tr>
</table>
</figure>
<figure id="S3.T3" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>LID test set information.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Dataset Name</span></td>
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Language(dialect)</span></td>
<td id="S3.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;"># utts</span></td>
</tr>
<tr id="S3.T3.1.2" class="ltx_tr">
<td id="S3.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T3.1.2.1.1" class="ltx_text" style="font-size:80%;">FLEURS</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S3.T3.1.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S3.T3.1.2.1.4" class="ltx_text" style="font-size:80%;"> yue</span>
</td>
<td id="S3.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.1.2.2.1" class="ltx_text" style="font-size:80%;">Cantonese</span></td>
<td id="S3.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.1.2.3.1" class="ltx_text" style="font-size:80%;">819</span></td>
</tr>
<tr id="S3.T3.1.3" class="ltx_tr">
<td id="S3.T3.1.3.1" class="ltx_td ltx_align_center">
<span id="S3.T3.1.3.1.1" class="ltx_text" style="font-size:80%;">Common Voice</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T3.1.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S3.T3.1.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S3.T3.1.3.1.4" class="ltx_text" style="font-size:80%;"> nan-tw</span>
</td>
<td id="S3.T3.1.3.2" class="ltx_td ltx_align_center"><span id="S3.T3.1.3.2.1" class="ltx_text" style="font-size:80%;">Hokkien</span></td>
<td id="S3.T3.1.3.3" class="ltx_td ltx_align_center">
<span id="S3.T3.1.3.3.1" class="ltx_text" style="font-size:80%;">1203</span><sup id="S3.T3.1.3.3.2" class="ltx_sup"><span id="S3.T3.1.3.3.2.1" class="ltx_text" style="font-size:80%;">*</span></sup>
</td>
</tr>
<tr id="S3.T3.1.4" class="ltx_tr">
<td id="S3.T3.1.4.1" class="ltx_td ltx_align_center">
<span id="S3.T3.1.4.1.1" class="ltx_text" style="font-size:80%;">ASR-SCWuhDiaDuSC</span><span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span id="footnote3.1" class="ltx_text" style="font-size:80%;">https://magichub.com/datasets/wuhan-dialect-scripted-speech-corpus-daily-use-sentence/</span></span></span></span>
</td>
<td id="S3.T3.1.4.2" class="ltx_td ltx_align_center"><span id="S3.T3.1.4.2.1" class="ltx_text" style="font-size:80%;">SW Mandarin</span></td>
<td id="S3.T3.1.4.3" class="ltx_td ltx_align_center">
<span id="S3.T3.1.4.3.1" class="ltx_text" style="font-size:80%;">1000</span><sup id="S3.T3.1.4.3.2" class="ltx_sup"><span id="S3.T3.1.4.3.2.1" class="ltx_text" style="font-size:80%;">*</span></sup>
</td>
</tr>
<tr id="S3.T3.1.5" class="ltx_tr">
<td id="S3.T3.1.5.1" class="ltx_td ltx_align_center"><span id="S3.T3.1.5.1.1" class="ltx_text" style="font-size:80%;">FLEURS cmn</span></td>
<td id="S3.T3.1.5.2" class="ltx_td ltx_align_center"><span id="S3.T3.1.5.2.1" class="ltx_text" style="font-size:80%;">Standard Mandarin</span></td>
<td id="S3.T3.1.5.3" class="ltx_td ltx_align_center"><span id="S3.T3.1.5.3.1" class="ltx_text" style="font-size:80%;">944</span></td>
</tr>
<tr id="S3.T3.1.6" class="ltx_tr">
<td id="S3.T3.1.6.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T3.1.6.1.1" class="ltx_text" style="font-size:80%;">FLEURS en</span></td>
<td id="S3.T3.1.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T3.1.6.2.1" class="ltx_text" style="font-size:80%;">English</span></td>
<td id="S3.T3.1.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T3.1.6.3.1" class="ltx_text" style="font-size:80%;">647</span></td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S3.I1" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">*</span> 
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p"><span id="S3.I1.ix1.p1.1.1" class="ltx_text" style="font-size:80%;">Randomly sampled from the original dataset.</span></p>
</div>
</li>
</ul>
</div>
</div>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model and training settings</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The multi-modal LM in PolySpeech is a 12-layer Transformer.
Each layer includes 12 attention heads, 768 hidden units, and 3,072 feed-forward units.
Chinese characters are used as ASR output and TTS input, along with Latin letters and special labels.
The source sequences of ASR, LID, and GID tasks are 768-dimensional embedding vectors extracted from the output of the ninth encoder layer of chinese-hubert-base<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://huggingface.co/TencentGameMate/chinese-hubert-base</span></span></span>. Polyspeech contains a total of 160M trainable parameters.
SSET described in Section <a href="#S2.SS3.SSS1" title="2.3.1 Semantic speech embedding tokenization ‣ 2.3 Speech tokenizer and de-tokenizer for TTS ‣ 2 PolySpeech ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.1</span></a> are applied to the outputs of the ninth layer of chinese-hubert-base to generate TTS targets. We use WenetSpeech to train the SSET model and configure the number of clusters to be 2048. The semantic token decoder is trained using Aishell-1 and WenetSpeech corpora.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">PolySpeech is trained with 16 NVIDIA V100 32GB GPUs, with a total batch size of 20 minutes of speech. Gradients are accumulated for 4 steps before a weight update is performed. Adam algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> with warm-up of 2k steps is used. We select the model with the lowest validation loss for evaluation.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation Metrics</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We use character error rate (CER) to evaluate Chinese ASR performance.
For TTS, we use two objective metrics. The first one is CER of the synthesized speech, which is evaluated with Whisper Large-v3 model. The second one is speaker encoder cosine similarity (SECS)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> calculated between synthesized speech and speech prompt using a speaker varification model<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://huggingface.co/microsoft/wavlm-base-plus-sv</span></span></span>. CER and SECS measure pronunciation accuracy and timbre similarity for the synthesized speech, respectively. Classification accuracy and Macro F1 score are used to evaluate performance of two speech classification tasks.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and analysis</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>ASR task</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">ASR results are exhibited in Table <a href="#S4.T4" title="Table 4 ‣ 4.1 ASR task ‣ 4 Results and analysis ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
We use result of a typical encoder-decoder transformer ASR model<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://github.com/espnet/espnet/blob/master/egs/aishell/asr1</span></span></span> trained with Espnet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> as a single-task baseline to compare PolySpeech with. We perform ASR decoding using beam search algorithm with beam size 5. No external LMs are used in decoding. Results of our final PolySpeech model are presented in Table <a href="#S4.T4" title="Table 4 ‣ 4.1 ASR task ‣ 4 Results and analysis ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and other result tables with <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">bold font</span>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">It is reported from the results of models trained on Aishell-1 that multiple-task-trained models show better performance than single-task-trained models.
With the training data increasing to 26k hours, multitask training no longer exhibits a significant improvement in ASR CER, remaining performance roughly comparable to single-task training.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">From the comparison of speech tokenizers, we find that the model employing continuous HuBERT embedding as input speech representation (``HuBERT'' in the table) yields the best ASR performance. HuBERT tokens generated by SSET perform not as good as continuous embeddings, but they are obviously superior to HuBERT k-means tokens. Semantic HuBERT tokens outperform acoustic EnCodec tokens. Results also indicate that text-only LM initialization improves performance of the ASR task, which involves text generation procedure.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>PolySpeech ASR results on Aishell-1 test.</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T4.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.1.1.1.1" class="ltx_text"></span><span id="S4.T4.1.1.1.2" class="ltx_text" style="font-size:80%;"> </span><span id="S4.T4.1.1.1.3" class="ltx_text" style="font-size:80%;">
<span id="S4.T4.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.1.1.1.3.1.1" class="ltx_tr">
<span id="S4.T4.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">ASR</span></span></span>
<span id="S4.T4.1.1.1.3.1.2" class="ltx_tr">
<span id="S4.T4.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">trainset</span></span></span>
</span></span><span id="S4.T4.1.1.1.4" class="ltx_text"></span><span id="S4.T4.1.1.1.5" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="S4.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.1.1.2.1" class="ltx_text"></span><span id="S4.T4.1.1.2.2" class="ltx_text" style="font-size:80%;"> </span><span id="S4.T4.1.1.2.3" class="ltx_text" style="font-size:80%;">
<span id="S4.T4.1.1.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.1.1.2.3.1.1" class="ltx_tr">
<span id="S4.T4.1.1.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.2.3.1.1.1.1" class="ltx_text ltx_font_bold">Training</span></span></span>
<span id="S4.T4.1.1.2.3.1.2" class="ltx_tr">
<span id="S4.T4.1.1.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.2.3.1.2.1.1" class="ltx_text ltx_font_bold">task(s)</span></span></span>
</span></span><span id="S4.T4.1.1.2.4" class="ltx_text"></span><span id="S4.T4.1.1.2.5" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="S4.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.1.1.3.1" class="ltx_text"></span><span id="S4.T4.1.1.3.2" class="ltx_text" style="font-size:80%;"> </span><span id="S4.T4.1.1.3.3" class="ltx_text" style="font-size:80%;">
<span id="S4.T4.1.1.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.1.1.3.3.1.1" class="ltx_tr">
<span id="S4.T4.1.1.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.3.3.1.1.1.1" class="ltx_text ltx_font_bold">ASR speech</span></span></span>
<span id="S4.T4.1.1.3.3.1.2" class="ltx_tr">
<span id="S4.T4.1.1.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.3.3.1.2.1.1" class="ltx_text ltx_font_bold">tokenizer</span></span></span>
</span></span><span id="S4.T4.1.1.3.4" class="ltx_text"></span><span id="S4.T4.1.1.3.5" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="S4.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.1.1.4.1" class="ltx_text"></span><span id="S4.T4.1.1.4.2" class="ltx_text" style="font-size:80%;"> </span><span id="S4.T4.1.1.4.3" class="ltx_text" style="font-size:80%;">
<span id="S4.T4.1.1.4.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.1.1.4.3.1.1" class="ltx_tr">
<span id="S4.T4.1.1.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.4.3.1.1.1.1" class="ltx_text ltx_font_bold">LM</span></span></span>
<span id="S4.T4.1.1.4.3.1.2" class="ltx_tr">
<span id="S4.T4.1.1.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.4.3.1.2.1.1" class="ltx_text ltx_font_bold">init.</span></span></span>
</span></span><span id="S4.T4.1.1.4.4" class="ltx_text"></span><span id="S4.T4.1.1.4.5" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="S4.T4.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T4.1.1.5.1" class="ltx_text"></span><span id="S4.T4.1.1.5.2" class="ltx_text" style="font-size:80%;"> </span><span id="S4.T4.1.1.5.3" class="ltx_text" style="font-size:80%;">
<span id="S4.T4.1.1.5.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.1.1.5.3.1.1" class="ltx_tr">
<span id="S4.T4.1.1.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.5.3.1.1.1.1" class="ltx_text ltx_font_bold">CER</span></span></span>
<span id="S4.T4.1.1.5.3.1.2" class="ltx_tr">
<span id="S4.T4.1.1.5.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T4.1.1.5.3.1.2.1.1" class="ltx_text ltx_font_bold">%</span></span></span>
</span></span><span id="S4.T4.1.1.5.4" class="ltx_text"></span><span id="S4.T4.1.1.5.5" class="ltx_text" style="font-size:80%;"></span>
</td>
</tr>
<tr id="S4.T4.1.2" class="ltx_tr">
<td id="S4.T4.1.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.2.1.1" class="ltx_text" style="font-size:80%;">Aishell-1</span></td>
<td id="S4.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span id="S4.T4.1.2.2.1" class="ltx_text" style="font-size:80%;">ESPNet Transformer (baseline)</span></td>
<td id="S4.T4.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.2.3.1" class="ltx_text" style="font-size:80%;">7.4</span></td>
</tr>
<tr id="S4.T4.1.3" class="ltx_tr">
<td id="S4.T4.1.3.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.1.1" class="ltx_text" style="font-size:80%;">Aishell-1</span></td>
<td id="S4.T4.1.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.2.1" class="ltx_text" style="font-size:80%;">ASR</span></td>
<td id="S4.T4.1.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.3.1" class="ltx_text" style="font-size:80%;">EnCodec</span></td>
<td id="S4.T4.1.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T4.1.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.5.1" class="ltx_text" style="font-size:80%;">18.1</span></td>
</tr>
<tr id="S4.T4.1.4" class="ltx_tr">
<td id="S4.T4.1.4.1" class="ltx_td ltx_align_center"><span id="S4.T4.1.4.1.1" class="ltx_text" style="font-size:80%;">Aishell-1</span></td>
<td id="S4.T4.1.4.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.4.2.1" class="ltx_text" style="font-size:80%;">ASR</span></td>
<td id="S4.T4.1.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.4.3.1" class="ltx_text" style="font-size:80%;">HuBERT k-means</span></td>
<td id="S4.T4.1.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.4.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T4.1.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.4.5.1" class="ltx_text" style="font-size:80%;">14.0</span></td>
</tr>
<tr id="S4.T4.1.5" class="ltx_tr">
<td id="S4.T4.1.5.1" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.1.1" class="ltx_text" style="font-size:80%;">Aishell-1</span></td>
<td id="S4.T4.1.5.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.2.1" class="ltx_text" style="font-size:80%;">ASR</span></td>
<td id="S4.T4.1.5.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.3.1" class="ltx_text" style="font-size:80%;">HuBERT codec</span></td>
<td id="S4.T4.1.5.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T4.1.5.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.5.1" class="ltx_text" style="font-size:80%;">8.1</span></td>
</tr>
<tr id="S4.T4.1.6" class="ltx_tr">
<td id="S4.T4.1.6.1" class="ltx_td ltx_align_center"><span id="S4.T4.1.6.1.1" class="ltx_text" style="font-size:80%;">Aishell-1</span></td>
<td id="S4.T4.1.6.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.6.2.1" class="ltx_text" style="font-size:80%;">ASR</span></td>
<td id="S4.T4.1.6.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.6.3.1" class="ltx_text" style="font-size:80%;">HuBERT</span></td>
<td id="S4.T4.1.6.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.6.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T4.1.6.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.6.5.1" class="ltx_text" style="font-size:80%;">7.1</span></td>
</tr>
<tr id="S4.T4.1.7" class="ltx_tr">
<td id="S4.T4.1.7.1" class="ltx_td ltx_align_center"><span id="S4.T4.1.7.1.1" class="ltx_text" style="font-size:80%;">Aishell-1</span></td>
<td id="S4.T4.1.7.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.7.2.1" class="ltx_text" style="font-size:80%;">ASR+TTS</span></td>
<td id="S4.T4.1.7.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.7.3.1" class="ltx_text" style="font-size:80%;">HuBERT</span></td>
<td id="S4.T4.1.7.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.7.4.1" class="ltx_text" style="font-size:80%;">✗</span></td>
<td id="S4.T4.1.7.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.7.5.1" class="ltx_text" style="font-size:80%;">6.7</span></td>
</tr>
<tr id="S4.T4.1.8" class="ltx_tr">
<td id="S4.T4.1.8.1" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.1.1" class="ltx_text" style="font-size:80%;">Aishell-1</span></td>
<td id="S4.T4.1.8.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.2.1" class="ltx_text" style="font-size:80%;">ASR+TTS</span></td>
<td id="S4.T4.1.8.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.3.1" class="ltx_text" style="font-size:80%;">HuBERT</span></td>
<td id="S4.T4.1.8.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T4.1.8.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.5.1" class="ltx_text" style="font-size:80%;">6.0</span></td>
</tr>
<tr id="S4.T4.1.9" class="ltx_tr">
<td id="S4.T4.1.9.1" class="ltx_td ltx_align_center"><span id="S4.T4.1.9.1.1" class="ltx_text" style="font-size:80%;">26k hrs</span></td>
<td id="S4.T4.1.9.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.9.2.1" class="ltx_text" style="font-size:80%;">ASR</span></td>
<td id="S4.T4.1.9.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.9.3.1" class="ltx_text" style="font-size:80%;">HuBERT</span></td>
<td id="S4.T4.1.9.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.9.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T4.1.9.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.9.5.1" class="ltx_text" style="font-size:80%;">2.7</span></td>
</tr>
<tr id="S4.T4.1.10" class="ltx_tr">
<td id="S4.T4.1.10.1" class="ltx_td ltx_align_center"><span id="S4.T4.1.10.1.1" class="ltx_text" style="font-size:80%;">26k hrs</span></td>
<td id="S4.T4.1.10.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.10.2.1" class="ltx_text" style="font-size:80%;">ASR+TTS</span></td>
<td id="S4.T4.1.10.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.10.3.1" class="ltx_text" style="font-size:80%;">HuBERT</span></td>
<td id="S4.T4.1.10.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.10.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T4.1.10.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.10.5.1" class="ltx_text" style="font-size:80%;">2.7</span></td>
</tr>
<tr id="S4.T4.1.11" class="ltx_tr">
<td id="S4.T4.1.11.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.11.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">26k hrs</span></td>
<td id="S4.T4.1.11.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.11.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">4 tasks</span></td>
<td id="S4.T4.1.11.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.11.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">HuBERT</span></td>
<td id="S4.T4.1.11.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.11.4.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T4.1.11.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.11.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">2.0</span></td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>TTS task</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In our preliminary experiments, we found that PolySpeech models trained solely on Aishell-1 were not able to provide stable enough TTS performance, especially when generating long speech.
When we expand the training data to 26k hours, the TTS performance get greatly improved. In Table <a href="#S4.T5" title="Table 5 ‣ 4.2 TTS task ‣ 4 Results and analysis ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we present PolySpeech TTS performance evaluated on Aishell-1 test subset. Since there are no other publicly available results of models trained on the same data set, we only show the performance of PolySpeech trained on different speech tasks, using original Aishell-1 test speech as topline. We set k to 5 for the top-k sampling in the TTS experiments.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Results indicate that the TTS performance of PolySpeech models trained on multiple tasks either surpasses or closely matches that of models trained solely on TTS. The CER of 4-tasks PolySpeech model increases by only absolute 0.3% comparing to the original speech and the SECS is larger than 0.9, which indicates that the synthesized speech is correct in pronunciation and closely resembles the speech prompt in timbre.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>PolySpeech TTS results on Aishell-1 test subset.</figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Training task(s)</span></td>
<td id="S4.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TTS trainset</span></td>
<td id="S4.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">CER %</span></td>
<td id="S4.T5.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T5.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SECS</span></td>
</tr>
<tr id="S4.T5.1.2" class="ltx_tr">
<td id="S4.T5.1.2.1" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T5.1.2.1.1" class="ltx_text" style="font-size:80%;">Original speech</span></td>
<td id="S4.T5.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.2.2.1" class="ltx_text" style="font-size:80%;">4.6</span></td>
<td id="S4.T5.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.2.3.1" class="ltx_text" style="font-size:80%;">0.94</span></td>
</tr>
<tr id="S4.T5.1.3" class="ltx_tr">
<td id="S4.T5.1.3.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.3.1.1" class="ltx_text" style="font-size:80%;">TTS</span></td>
<td id="S4.T5.1.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.3.2.1" class="ltx_text" style="font-size:80%;">26k hrs</span></td>
<td id="S4.T5.1.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.3.3.1" class="ltx_text" style="font-size:80%;">5.1</span></td>
<td id="S4.T5.1.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.3.4.1" class="ltx_text" style="font-size:80%;">0.91</span></td>
</tr>
<tr id="S4.T5.1.4" class="ltx_tr">
<td id="S4.T5.1.4.1" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.1.1" class="ltx_text" style="font-size:80%;">ASR+TTS</span></td>
<td id="S4.T5.1.4.2" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.2.1" class="ltx_text" style="font-size:80%;">26k hrs</span></td>
<td id="S4.T5.1.4.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.3.1" class="ltx_text" style="font-size:80%;">5.4</span></td>
<td id="S4.T5.1.4.4" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.4.1" class="ltx_text" style="font-size:80%;">0.91</span></td>
</tr>
<tr id="S4.T5.1.5" class="ltx_tr">
<td id="S4.T5.1.5.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.5.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">4 tasks</span></td>
<td id="S4.T5.1.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.5.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">26k hrs</span></td>
<td id="S4.T5.1.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.5.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">4.9</span></td>
<td id="S4.T5.1.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.5.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.91</span></td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Classification tasks (LID and GID)</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">LID and GID results on the test sets are shown in Table <a href="#S4.T6" title="Table 6 ‣ 4.3 Classification tasks (LID and GID) ‣ 4 Results and analysis ‣ PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
As a single-task baseline model, we fully finetuned Whisper Small on the same LID and GID training set as PolySpeech.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The finetuned Whisper baseline and PolySpeech models trained with EnCodec tokens work well on the validation set, with accuracy exceeding 99% (not listed in the table), but perform poorly on out-of-domain test sets, especially for the LID task.
Since each language's LID training speech come from different data sources, models trained with EnCodec acoustic tokens are likely to overfit to the acoustic conditions of each language's training set.
In contrast, models using HuBERT semantic embeddings demonstrate better performance on the test set, showing strong generalization ability.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Multitask training brings significant performance to models using HuBERT on LID task. This is because the model further maintains semantic discrimination abilities through the ASR task, which is beneficial to LID. And since we use Standard Mandarin speech in ASR training,
Standard Mandarin shows the largest LID accuracy improvement (from 81.3% to 91.8%) among the 5 languages and dialects. In GID task, both single-task and multitask models perform well on the test set.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>PolySpeech LID and GID results on test sets.</figcaption>
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T6.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T6.1.1.1.1" class="ltx_text" style="font-size:80%;"><span id="S4.T6.1.1.1.1.1" class="ltx_text"></span> <span id="S4.T6.1.1.1.1.2" class="ltx_text">
<span id="S4.T6.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T6.1.1.1.1.2.1.1" class="ltx_tr">
<span id="S4.T6.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T6.1.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Training</span></span></span>
<span id="S4.T6.1.1.1.1.2.1.2" class="ltx_tr">
<span id="S4.T6.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T6.1.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">task(s)</span></span></span>
</span></span> <span id="S4.T6.1.1.1.1.3" class="ltx_text"></span></span></td>
<td id="S4.T6.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T6.1.1.2.1" class="ltx_text" style="font-size:80%;"><span id="S4.T6.1.1.2.1.1" class="ltx_text"></span> <span id="S4.T6.1.1.2.1.2" class="ltx_text">
<span id="S4.T6.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T6.1.1.2.1.2.1.1" class="ltx_tr">
<span id="S4.T6.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T6.1.1.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Speech</span></span></span>
<span id="S4.T6.1.1.2.1.2.1.2" class="ltx_tr">
<span id="S4.T6.1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T6.1.1.2.1.2.1.2.1.1" class="ltx_text ltx_font_bold">tokenizer</span></span></span>
</span></span> <span id="S4.T6.1.1.2.1.3" class="ltx_text"></span></span></td>
<td id="S4.T6.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T6.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Accuracy %</span></td>
<td id="S4.T6.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T6.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Macro F1 %</span></td>
</tr>
<tr id="S4.T6.1.2" class="ltx_tr">
<td id="S4.T6.1.2.1" class="ltx_td ltx_align_center"><span id="S4.T6.1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">LID</span></td>
<td id="S4.T6.1.2.2" class="ltx_td ltx_align_center"><span id="S4.T6.1.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GID</span></td>
<td id="S4.T6.1.2.3" class="ltx_td ltx_align_center"><span id="S4.T6.1.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">LID</span></td>
<td id="S4.T6.1.2.4" class="ltx_td ltx_align_center"><span id="S4.T6.1.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GID</span></td>
</tr>
<tr id="S4.T6.1.3" class="ltx_tr">
<td id="S4.T6.1.3.1" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T6.1.3.1.1" class="ltx_text" style="font-size:80%;">Ft. Whisper (baseline)</span></td>
<td id="S4.T6.1.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.3.2.1" class="ltx_text" style="font-size:80%;">44.4</span></td>
<td id="S4.T6.1.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.3.3.1" class="ltx_text" style="font-size:80%;">97.7</span></td>
<td id="S4.T6.1.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.3.4.1" class="ltx_text" style="font-size:80%;">38.6</span></td>
<td id="S4.T6.1.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.3.5.1" class="ltx_text" style="font-size:80%;">97.5</span></td>
</tr>
<tr id="S4.T6.1.4" class="ltx_tr">
<td id="S4.T6.1.4.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.4.1.1" class="ltx_text" style="font-size:80%;">LID</span></td>
<td id="S4.T6.1.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.4.2.1" class="ltx_text" style="font-size:80%;">EnCodec</span></td>
<td id="S4.T6.1.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.4.3.1" class="ltx_text" style="font-size:80%;">43.4</span></td>
<td id="S4.T6.1.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.4.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T6.1.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.4.5.1" class="ltx_text" style="font-size:80%;">38.1</span></td>
<td id="S4.T6.1.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.4.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T6.1.5" class="ltx_tr">
<td id="S4.T6.1.5.1" class="ltx_td ltx_align_center"><span id="S4.T6.1.5.1.1" class="ltx_text" style="font-size:80%;">LID</span></td>
<td id="S4.T6.1.5.2" class="ltx_td ltx_align_center"><span id="S4.T6.1.5.2.1" class="ltx_text" style="font-size:80%;">HuBERT</span></td>
<td id="S4.T6.1.5.3" class="ltx_td ltx_align_center"><span id="S4.T6.1.5.3.1" class="ltx_text" style="font-size:80%;">90.9</span></td>
<td id="S4.T6.1.5.4" class="ltx_td ltx_align_center"><span id="S4.T6.1.5.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T6.1.5.5" class="ltx_td ltx_align_center"><span id="S4.T6.1.5.5.1" class="ltx_text" style="font-size:80%;">91.1</span></td>
<td id="S4.T6.1.5.6" class="ltx_td ltx_align_center"><span id="S4.T6.1.5.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T6.1.6" class="ltx_tr">
<td id="S4.T6.1.6.1" class="ltx_td ltx_align_center"><span id="S4.T6.1.6.1.1" class="ltx_text" style="font-size:80%;">GID</span></td>
<td id="S4.T6.1.6.2" class="ltx_td ltx_align_center"><span id="S4.T6.1.6.2.1" class="ltx_text" style="font-size:80%;">EnCodec</span></td>
<td id="S4.T6.1.6.3" class="ltx_td ltx_align_center"><span id="S4.T6.1.6.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T6.1.6.4" class="ltx_td ltx_align_center"><span id="S4.T6.1.6.4.1" class="ltx_text" style="font-size:80%;">95.9</span></td>
<td id="S4.T6.1.6.5" class="ltx_td ltx_align_center"><span id="S4.T6.1.6.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T6.1.6.6" class="ltx_td ltx_align_center"><span id="S4.T6.1.6.6.1" class="ltx_text" style="font-size:80%;">95.6</span></td>
</tr>
<tr id="S4.T6.1.7" class="ltx_tr">
<td id="S4.T6.1.7.1" class="ltx_td ltx_align_center"><span id="S4.T6.1.7.1.1" class="ltx_text" style="font-size:80%;">GID</span></td>
<td id="S4.T6.1.7.2" class="ltx_td ltx_align_center"><span id="S4.T6.1.7.2.1" class="ltx_text" style="font-size:80%;">HuBERT</span></td>
<td id="S4.T6.1.7.3" class="ltx_td ltx_align_center"><span id="S4.T6.1.7.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T6.1.7.4" class="ltx_td ltx_align_center"><span id="S4.T6.1.7.4.1" class="ltx_text" style="font-size:80%;">99.5</span></td>
<td id="S4.T6.1.7.5" class="ltx_td ltx_align_center"><span id="S4.T6.1.7.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T6.1.7.6" class="ltx_td ltx_align_center"><span id="S4.T6.1.7.6.1" class="ltx_text" style="font-size:80%;">99.5</span></td>
</tr>
<tr id="S4.T6.1.8" class="ltx_tr">
<td id="S4.T6.1.8.1" class="ltx_td ltx_align_center"><span id="S4.T6.1.8.1.1" class="ltx_text" style="font-size:80%;">4 tasks</span></td>
<td id="S4.T6.1.8.2" class="ltx_td ltx_align_center"><span id="S4.T6.1.8.2.1" class="ltx_text" style="font-size:80%;">EnCodec</span></td>
<td id="S4.T6.1.8.3" class="ltx_td ltx_align_center"><span id="S4.T6.1.8.3.1" class="ltx_text" style="font-size:80%;">44.2</span></td>
<td id="S4.T6.1.8.4" class="ltx_td ltx_align_center"><span id="S4.T6.1.8.4.1" class="ltx_text" style="font-size:80%;">96.1</span></td>
<td id="S4.T6.1.8.5" class="ltx_td ltx_align_center"><span id="S4.T6.1.8.5.1" class="ltx_text" style="font-size:80%;">38.6</span></td>
<td id="S4.T6.1.8.6" class="ltx_td ltx_align_center"><span id="S4.T6.1.8.6.1" class="ltx_text" style="font-size:80%;">95.8</span></td>
</tr>
<tr id="S4.T6.1.9" class="ltx_tr">
<td id="S4.T6.1.9.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.9.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">4 tasks</span></td>
<td id="S4.T6.1.9.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.9.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">HuBERT</span></td>
<td id="S4.T6.1.9.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.9.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">94.4</span></td>
<td id="S4.T6.1.9.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.9.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">99.0</span></td>
<td id="S4.T6.1.9.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.9.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">94.8</span></td>
<td id="S4.T6.1.9.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.9.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">98.9</span></td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper we present PolySpeech, a multitask speech model exhibiting competitiveness across diverse tasks when compared to single-task models. We also introduce a high-fidelity speech codec method along with an efficient semantic speech token decoder for efficient speech generation. Through experiments, we validate that multitask optimization achieves performance comparable to single-task optimization and offers advantages for certain tasks. In the future, we plan to integrate more speech tasks into PolySpeech, such as speech translation, multilingual ASR, and speech enhancement. We will also expand the model scale of PolySpeech to achieve better performance.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, ``Attention is all you need,'' in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 30</em>, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds.   Curran Associates, Inc., 2017, pp. 5998–6008.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Zhou, L. Dong, S. Xu, and B. Xu, ``Syllable-based sequence-to-sequence speech recognition with the Transformer in mandarin chinese,'' in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2018</em>, 2018, pp. 791–795.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, ``Robust speech recognition via large-scale weak supervision,'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2023, pp. 28 492–28 518.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
T. Wang, L. Zhou, Z. Zhang, Y. Wu, S. Liu, Y. Gaur, Z. Chen, J. Li, and F. Wei, ``Viola: Unified codec language models for speech recognition, synthesis, and translation,'' <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.16107</em>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Borsos, F. d. C. Quitry, P. Chen, D. E. Badawy, W. Han, E. Kharitonov <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Audiopalm: A large language model that can speak and listen,'' <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.12925</em>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Q. Chen, Y. Chu, Z. Gao, Z. Li, K. Hu, X. Zhou, J. Xu, Z. Ma, W. Wang, S. Zheng <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Lauragpt: Listen, attend, understand, and regenerate audio with gpt,'' <em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.04673</em>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
D. Zhang, S. Li, X. Zhang, J. Zhan, P. Wang, Y. Zhou, and X. Qiu, ``Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities,'' <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.11000</em>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Défossez, J. Copet, G. Synnaeve, and Y. Adi, ``High fidelity neural audio compression,'' <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.13438</em>, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi, ``Soundstream: An end-to-end neural audio codec,'' <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 30, pp. 495–507, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Schneider, A. Baevski, R. Collobert, and M. Auli, ``wav2vec: Unsupervised pre-training for speech recognition,'' <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.05862</em>, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, ``wav2vec 2.0: A framework for self-supervised learning of speech representations,'' <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 33, pp. 12 449–12 460, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu, ``W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.   IEEE, 2021, pp. 244–250.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, ``Hubert: Self-supervised speech representation learning by masked prediction of hidden units,'' <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 29, pp. 3451–3460, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Audiolm: a language modeling approach to audio generation,'' <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
E. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin, O. Pietquin, M. Sharifi, M. Tagliasacchi, and N. Zeghidour, ``Speak, read and prompt: High-fidelity text-to-speech with minimal supervision,'' <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.03540</em>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. Kong, J. Kim, and J. Bae, ``Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,'' <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 33, pp. 17 022–17 033, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y.-C. Wu, I. D. Gebru, D. Marković, and A. Richard, ``Audiodec: An open-source streaming high-fidelity neural audio codec,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. Kim, J. Kong, and J. Son, ``Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech,'' in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2021, pp. 5530–5540.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
H. Siuzdak, ``Vocos: Closing the gap between time-domain and fourier-based neural vocoders for high-quality audio synthesis,'' <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.00814</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, ``Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">2017 20th conference of the oriental chapter of the international coordinating committee on speech databases and speech I/O systems and assessment (O-COCOSDA)</em>.   IEEE, 2017, pp. 1–5.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Du, X. Na, X. Liu, and H. Bu, ``Aishell-2: Transforming mandarin asr research into industrial scale,'' <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.10583</em>, 2018.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
B. Zhang, H. Lv, P. Guo, Q. Shao, C. Yang, L. Xie, X. Xu, H. Bu, X. Chen, C. Zeng <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition,'' in <em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2022, pp. 6182–6186.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Z. Tang, D. Wang, Y. Xu, J. Sun, X. Lei, S. Zhao, c. wen, X. Tan, C. Xie, S. Zhou, R. Yan, C. Lv, Y. Han, W. Zou, and X. Li, ``Kespeech: An open source speech dataset of mandarin and its eight subdialects,'' in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks</em>, J. Vanschoren and S. Yeung, Eds., vol. 1.   Curran, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y. Cen, X. Zou, Z. Yang, and J. Tang, ``Wudaocorpora: A super large-scale chinese corpora for pre-training language models,'' <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">AI Open</em>, vol. 2, pp. 65–68, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, ``Librispeech: an asr corpus based on public domain audio books,'' in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.   IEEE, 2015, pp. 5206–5210.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and A. Bapna, ``Fleurs: Few-shot learning evaluation of universal representations of speech,'' <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.12446</em>, 2022. [Online]. Available: <a target="_blank" href="https://arxiv.org/abs/2205.12446" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2205.12446</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, ``Common voice: A massively-multilingual speech corpus,'' <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.06670</em>, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
D. Kingma and J. Ba, ``Adam: A method for stochastic optimization,'' in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015)</em>, 2015.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
E. Casanova, C. Shulby, E. Gölge, N. M. Müller, F. S. de Oliveira, A. C. Junior, A. d. S. Soares, S. M. Aluisio, and M. A. Ponti, ``Sc-glowtts: an efficient zero-shot multi-speaker text-to-speech model,'' <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.05557</em>, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai, ``ESPnet: End-to-end speech processing toolkit,'' in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of Interspeech</em>, 2018, pp. 2207–2211.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.07800" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.07801" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.07801">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.07801" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.07802" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Jul  6 00:55:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
