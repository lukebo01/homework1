<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.13431] Childrenâ€™s Speech Recognition through Discrete Token Enhancement</title><meta property="og:description" content="Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation prâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Childrenâ€™s Speech Recognition through Discrete Token Enhancement">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Childrenâ€™s Speech Recognition through Discrete Token Enhancement">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.13431">

<!--Generated on Sat Jul  6 00:15:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">Vrunda N. Sukhadia, Shammur Absar Chowdhury</p>
</div>
<h1 class="ltx_title ltx_title_document">Childrenâ€™s Speech Recognition through Discrete Token Enhancement</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others.
Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns.
In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset.
Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Child Speech Recognition, Discrete speech tokens, Ensembling, Multi-view clustering 
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">0</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">0</sup><span class="ltx_note_type">footnotetext: </span>This paper was accepted at Interspeech 2024.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Automatic Children's speech recognition has recently attracted significant attention from research communities. One of the main reasons for such attention is that children increasingly interact with voice-activated assistants and technologies. This trend underscores the potential benefits of ASR technologies tailored for children, which can revolutionize learning tools, such as automated reading assessments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and interactive reading tutors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> among others. These applications promise to enhance language acquisition for both native and non-native learners with immediate and multimodal feedback.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, designing children's ASR has its unique challenges. Unlike adults, children's ASR is limited in resources and is still considered a low-resource task.
This is because there is a lack of large-scale publicly available children data, and collecting and annotating such datasets are expensive and also face many difficulties due to privacy and ethical considerations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Moreover, many studies have consistently highlighted the disparities between child and adult ASR performance, especially in English, due to difficulties in acoustic and language modeling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
The variabilities seen in children's speech data are due to the differences in speech development rates (inter-speaker variability) and evolving pronunciation skills within an individual child over time (intra-speaker variability). Moreover, children's speech includes significant mispronunciations and disfluencies, making it harder to annotate and model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Self-supervised learning (SSL) models have shown remarkable improvement in performance for various speech tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, while reducing the dependency on extensively annotated datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Studies such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> have shown the efficacy of SSL models in improving child speech recognition, either using it for robust feature extractor or for finetuning the pre-trained model on specific datasets. Few studies have also been conducted to study the encoded information for children's speech present in the pre-trained SSL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> have highlighted the usefulness of discrete speech units to represent speech signals, and their effects on ASR performance. Such compression not only reduces the storage and transmission size but also retains the essential acoustic and linguistic information while handling speaker variability better. This strategy also has the potential to handle privacy concerns, always faced when dealing with children's data.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Therefore, in this study, we design an end-to-end English children's ASR system using discrete units as input to the models. Our proposed framework exploits the frame-level embeddings from pre-trained SSL models and quantizes them to a handful of discrete tokens considering representation either from a single SSL model (single view representation) or multiple (multi-view) SSL models using k-mean clustering models. These discrete tokens are then passed to an end-to-end ASR model.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We compare our proposed discrete ASR with an ASR trained on continuous embedding extracted from the pretrained HuBERT and WavLM model.
Additionally, we compare the designed ASR system with results obtained using the state-of-the-art Whisper model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> in both zero-shot and fine-tuned settings as the upper-bound for the study.
Furthermore, we show its efficacy when tested on unseen datasets, including (i) unseen domain, and (ii) non-native English datasets with both read and spontaneous speech style.
</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">Therefore, our contribution in this paper includes:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Design and benchmark End-to-end Discrete ASR for children speech for native and non-native children datasets.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Explore multi-view clustering strategy to design discrete tokens and compare it with the single-view method.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Show the potential of the discrete children ASR for children ASR, while testing the generalization capability for the unseen domain, speaking styles, and nativity compared to the state-of-the-art Whisper model family.</p>
</div>
</li>
</ul>
<p id="S1.p7.2" class="ltx_p">To the best of our knowledge, this is the first study to explore the effectiveness of discrete tokens in single and multi-view settings for children ASR.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.7" class="ltx_p">Figure <a href="#S2.F1" title="Figure 1 â€£ 2.1 Discrete Codebook â€£ 2 Methodology â€£ Childrenâ€™s Speech Recognition through Discrete Token Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> gives an overview of our proposed discrete Children ASR. Given an input utterance <math id="S2.p1.1.m1.4" class="ltx_Math" alttext="X=\left[x_{1},x_{2},\cdots,x_{T}\right]" display="inline"><semantics id="S2.p1.1.m1.4a"><mrow id="S2.p1.1.m1.4.4" xref="S2.p1.1.m1.4.4.cmml"><mi id="S2.p1.1.m1.4.4.5" xref="S2.p1.1.m1.4.4.5.cmml">X</mi><mo id="S2.p1.1.m1.4.4.4" xref="S2.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S2.p1.1.m1.4.4.3.3" xref="S2.p1.1.m1.4.4.3.4.cmml"><mo id="S2.p1.1.m1.4.4.3.3.4" xref="S2.p1.1.m1.4.4.3.4.cmml">[</mo><msub id="S2.p1.1.m1.2.2.1.1.1" xref="S2.p1.1.m1.2.2.1.1.1.cmml"><mi id="S2.p1.1.m1.2.2.1.1.1.2" xref="S2.p1.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S2.p1.1.m1.2.2.1.1.1.3" xref="S2.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.p1.1.m1.4.4.3.3.5" xref="S2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.p1.1.m1.3.3.2.2.2" xref="S2.p1.1.m1.3.3.2.2.2.cmml"><mi id="S2.p1.1.m1.3.3.2.2.2.2" xref="S2.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S2.p1.1.m1.3.3.2.2.2.3" xref="S2.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.p1.1.m1.4.4.3.3.6" xref="S2.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">â‹¯</mi><mo id="S2.p1.1.m1.4.4.3.3.7" xref="S2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.p1.1.m1.4.4.3.3.3" xref="S2.p1.1.m1.4.4.3.3.3.cmml"><mi id="S2.p1.1.m1.4.4.3.3.3.2" xref="S2.p1.1.m1.4.4.3.3.3.2.cmml">x</mi><mi id="S2.p1.1.m1.4.4.3.3.3.3" xref="S2.p1.1.m1.4.4.3.3.3.3.cmml">T</mi></msub><mo id="S2.p1.1.m1.4.4.3.3.8" xref="S2.p1.1.m1.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.4b"><apply id="S2.p1.1.m1.4.4.cmml" xref="S2.p1.1.m1.4.4"><eq id="S2.p1.1.m1.4.4.4.cmml" xref="S2.p1.1.m1.4.4.4"></eq><ci id="S2.p1.1.m1.4.4.5.cmml" xref="S2.p1.1.m1.4.4.5">ğ‘‹</ci><list id="S2.p1.1.m1.4.4.3.4.cmml" xref="S2.p1.1.m1.4.4.3.3"><apply id="S2.p1.1.m1.2.2.1.1.1.cmml" xref="S2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.p1.1.m1.2.2.1.1.1.2.cmml" xref="S2.p1.1.m1.2.2.1.1.1.2">ğ‘¥</ci><cn type="integer" id="S2.p1.1.m1.2.2.1.1.1.3.cmml" xref="S2.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S2.p1.1.m1.3.3.2.2.2.cmml" xref="S2.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p1.1.m1.3.3.2.2.2.1.cmml" xref="S2.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.p1.1.m1.3.3.2.2.2.2">ğ‘¥</ci><cn type="integer" id="S2.p1.1.m1.3.3.2.2.2.3.cmml" xref="S2.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">â‹¯</ci><apply id="S2.p1.1.m1.4.4.3.3.3.cmml" xref="S2.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.p1.1.m1.4.4.3.3.3.1.cmml" xref="S2.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S2.p1.1.m1.4.4.3.3.3.2.cmml" xref="S2.p1.1.m1.4.4.3.3.3.2">ğ‘¥</ci><ci id="S2.p1.1.m1.4.4.3.3.3.3.cmml" xref="S2.p1.1.m1.4.4.3.3.3.3">ğ‘‡</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.4c">X=\left[x_{1},x_{2},\cdots,x_{T}\right]</annotation></semantics></math> of <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">T</annotation></semantics></math> frames, the frame-level representation (<math id="S2.p1.3.m3.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">Z</annotation></semantics></math>) is first extracted from a <span id="S2.p1.7.1" class="ltx_text ltx_font_italic">SSL pretrained</span> model. A discrete codebook <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="\mathbb{C}" display="inline"><semantics id="S2.p1.4.m4.1a"><mi id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">â„‚</mi><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><ci id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1">â„‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">\mathbb{C}</annotation></semantics></math> is then trained with the frame-level <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S2.p1.5.m5.1a"><mi id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">Z</annotation></semantics></math> from the sampled utterances. For training the discrete codebook, we followed two different strategies utilizing either single representation or multi-view representation from pretrained models.
We then utilize the trained <math id="S2.p1.6.m6.1" class="ltx_Math" alttext="\mathbb{C}" display="inline"><semantics id="S2.p1.6.m6.1a"><mi id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml">â„‚</mi><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><ci id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1">â„‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">\mathbb{C}</annotation></semantics></math> to infer <math id="S2.p1.7.m7.1" class="ltx_Math" alttext="\hat{Z}=\mathbb{C}(Z)" display="inline"><semantics id="S2.p1.7.m7.1a"><mrow id="S2.p1.7.m7.1.2" xref="S2.p1.7.m7.1.2.cmml"><mover accent="true" id="S2.p1.7.m7.1.2.2" xref="S2.p1.7.m7.1.2.2.cmml"><mi id="S2.p1.7.m7.1.2.2.2" xref="S2.p1.7.m7.1.2.2.2.cmml">Z</mi><mo id="S2.p1.7.m7.1.2.2.1" xref="S2.p1.7.m7.1.2.2.1.cmml">^</mo></mover><mo id="S2.p1.7.m7.1.2.1" xref="S2.p1.7.m7.1.2.1.cmml">=</mo><mrow id="S2.p1.7.m7.1.2.3" xref="S2.p1.7.m7.1.2.3.cmml"><mi id="S2.p1.7.m7.1.2.3.2" xref="S2.p1.7.m7.1.2.3.2.cmml">â„‚</mi><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.1.2.3.1" xref="S2.p1.7.m7.1.2.3.1.cmml">â€‹</mo><mrow id="S2.p1.7.m7.1.2.3.3.2" xref="S2.p1.7.m7.1.2.3.cmml"><mo stretchy="false" id="S2.p1.7.m7.1.2.3.3.2.1" xref="S2.p1.7.m7.1.2.3.cmml">(</mo><mi id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml">Z</mi><mo stretchy="false" id="S2.p1.7.m7.1.2.3.3.2.2" xref="S2.p1.7.m7.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.1b"><apply id="S2.p1.7.m7.1.2.cmml" xref="S2.p1.7.m7.1.2"><eq id="S2.p1.7.m7.1.2.1.cmml" xref="S2.p1.7.m7.1.2.1"></eq><apply id="S2.p1.7.m7.1.2.2.cmml" xref="S2.p1.7.m7.1.2.2"><ci id="S2.p1.7.m7.1.2.2.1.cmml" xref="S2.p1.7.m7.1.2.2.1">^</ci><ci id="S2.p1.7.m7.1.2.2.2.cmml" xref="S2.p1.7.m7.1.2.2.2">ğ‘</ci></apply><apply id="S2.p1.7.m7.1.2.3.cmml" xref="S2.p1.7.m7.1.2.3"><times id="S2.p1.7.m7.1.2.3.1.cmml" xref="S2.p1.7.m7.1.2.3.1"></times><ci id="S2.p1.7.m7.1.2.3.2.cmml" xref="S2.p1.7.m7.1.2.3.2">â„‚</ci><ci id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.1c">\hat{Z}=\mathbb{C}(Z)</annotation></semantics></math>, and use the discrete labels as an input to the encoder-decoder ASR model.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Discrete Codebook</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.6" class="ltx_p">We opt for a simple vector quantization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> technique for approximating frame-level embeddings through a fixed codebook size. We utilize a sequence of continuous feature vectors <math id="S2.SS1.p1.1.m1.4" class="ltx_Math" alttext="Z=\{z_{1},z_{2},\ldots,z_{T}\}" display="inline"><semantics id="S2.SS1.p1.1.m1.4a"><mrow id="S2.SS1.p1.1.m1.4.4" xref="S2.SS1.p1.1.m1.4.4.cmml"><mi id="S2.SS1.p1.1.m1.4.4.5" xref="S2.SS1.p1.1.m1.4.4.5.cmml">Z</mi><mo id="S2.SS1.p1.1.m1.4.4.4" xref="S2.SS1.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S2.SS1.p1.1.m1.4.4.3.3" xref="S2.SS1.p1.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S2.SS1.p1.1.m1.4.4.3.3.4" xref="S2.SS1.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="S2.SS1.p1.1.m1.2.2.1.1.1" xref="S2.SS1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.2.2.1.1.1.2" xref="S2.SS1.p1.1.m1.2.2.1.1.1.2.cmml">z</mi><mn id="S2.SS1.p1.1.m1.2.2.1.1.1.3" xref="S2.SS1.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p1.1.m1.4.4.3.3.5" xref="S2.SS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.1.m1.3.3.2.2.2" xref="S2.SS1.p1.1.m1.3.3.2.2.2.cmml"><mi id="S2.SS1.p1.1.m1.3.3.2.2.2.2" xref="S2.SS1.p1.1.m1.3.3.2.2.2.2.cmml">z</mi><mn id="S2.SS1.p1.1.m1.3.3.2.2.2.3" xref="S2.SS1.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p1.1.m1.4.4.3.3.6" xref="S2.SS1.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">â€¦</mi><mo id="S2.SS1.p1.1.m1.4.4.3.3.7" xref="S2.SS1.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.1.m1.4.4.3.3.3" xref="S2.SS1.p1.1.m1.4.4.3.3.3.cmml"><mi id="S2.SS1.p1.1.m1.4.4.3.3.3.2" xref="S2.SS1.p1.1.m1.4.4.3.3.3.2.cmml">z</mi><mi id="S2.SS1.p1.1.m1.4.4.3.3.3.3" xref="S2.SS1.p1.1.m1.4.4.3.3.3.3.cmml">T</mi></msub><mo stretchy="false" id="S2.SS1.p1.1.m1.4.4.3.3.8" xref="S2.SS1.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.4b"><apply id="S2.SS1.p1.1.m1.4.4.cmml" xref="S2.SS1.p1.1.m1.4.4"><eq id="S2.SS1.p1.1.m1.4.4.4.cmml" xref="S2.SS1.p1.1.m1.4.4.4"></eq><ci id="S2.SS1.p1.1.m1.4.4.5.cmml" xref="S2.SS1.p1.1.m1.4.4.5">ğ‘</ci><set id="S2.SS1.p1.1.m1.4.4.3.4.cmml" xref="S2.SS1.p1.1.m1.4.4.3.3"><apply id="S2.SS1.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1.2">ğ‘§</ci><cn type="integer" id="S2.SS1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS1.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.3.3.2.2.2.1.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.2.2">ğ‘§</ci><cn type="integer" id="S2.SS1.p1.1.m1.3.3.2.2.2.3.cmml" xref="S2.SS1.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">â€¦</ci><apply id="S2.SS1.p1.1.m1.4.4.3.3.3.cmml" xref="S2.SS1.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.4.4.3.3.3.1.cmml" xref="S2.SS1.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.1.m1.4.4.3.3.3.2.cmml" xref="S2.SS1.p1.1.m1.4.4.3.3.3.2">ğ‘§</ci><ci id="S2.SS1.p1.1.m1.4.4.3.3.3.3.cmml" xref="S2.SS1.p1.1.m1.4.4.3.3.3.3">ğ‘‡</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.4c">Z=\{z_{1},z_{2},\ldots,z_{T}\}</annotation></semantics></math> and then assign each <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><msub id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">z</mi><mi id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">ğ‘§</ci><ci id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">z_{t}</annotation></semantics></math> to its nearest neighbor in the trained codebook, <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathbb{C}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">â„‚</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">â„‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\mathbb{C}</annotation></semantics></math>, with the code <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="Q_{i}\in\mathbb{C}" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mrow id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><msub id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2.2" xref="S2.SS1.p1.4.m4.1.1.2.2.cmml">Q</mi><mi id="S2.SS1.p1.4.m4.1.1.2.3" xref="S2.SS1.p1.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS1.p1.4.m4.1.1.1" xref="S2.SS1.p1.4.m4.1.1.1.cmml">âˆˆ</mo><mi id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml">â„‚</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><in id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1.1"></in><apply id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.2.1.cmml" xref="S2.SS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2.2">ğ‘„</ci><ci id="S2.SS1.p1.4.m4.1.1.2.3.cmml" xref="S2.SS1.p1.4.m4.1.1.2.3">ğ‘–</ci></apply><ci id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3">â„‚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">Q_{i}\in\mathbb{C}</annotation></semantics></math> assigned to the centroid <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="G_{i}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><msub id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">G</mi><mi id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">ğº</ci><ci id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">G_{i}</annotation></semantics></math>.
The resultant discrete labels are quantized sequence <math id="S2.SS1.p1.6.m6.4" class="ltx_Math" alttext="\hat{Z}=\{\hat{z}_{1},\hat{z}_{2},\ldots,\hat{z}_{T}\}" display="inline"><semantics id="S2.SS1.p1.6.m6.4a"><mrow id="S2.SS1.p1.6.m6.4.4" xref="S2.SS1.p1.6.m6.4.4.cmml"><mover accent="true" id="S2.SS1.p1.6.m6.4.4.5" xref="S2.SS1.p1.6.m6.4.4.5.cmml"><mi id="S2.SS1.p1.6.m6.4.4.5.2" xref="S2.SS1.p1.6.m6.4.4.5.2.cmml">Z</mi><mo id="S2.SS1.p1.6.m6.4.4.5.1" xref="S2.SS1.p1.6.m6.4.4.5.1.cmml">^</mo></mover><mo id="S2.SS1.p1.6.m6.4.4.4" xref="S2.SS1.p1.6.m6.4.4.4.cmml">=</mo><mrow id="S2.SS1.p1.6.m6.4.4.3.3" xref="S2.SS1.p1.6.m6.4.4.3.4.cmml"><mo stretchy="false" id="S2.SS1.p1.6.m6.4.4.3.3.4" xref="S2.SS1.p1.6.m6.4.4.3.4.cmml">{</mo><msub id="S2.SS1.p1.6.m6.2.2.1.1.1" xref="S2.SS1.p1.6.m6.2.2.1.1.1.cmml"><mover accent="true" id="S2.SS1.p1.6.m6.2.2.1.1.1.2" xref="S2.SS1.p1.6.m6.2.2.1.1.1.2.cmml"><mi id="S2.SS1.p1.6.m6.2.2.1.1.1.2.2" xref="S2.SS1.p1.6.m6.2.2.1.1.1.2.2.cmml">z</mi><mo id="S2.SS1.p1.6.m6.2.2.1.1.1.2.1" xref="S2.SS1.p1.6.m6.2.2.1.1.1.2.1.cmml">^</mo></mover><mn id="S2.SS1.p1.6.m6.2.2.1.1.1.3" xref="S2.SS1.p1.6.m6.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p1.6.m6.4.4.3.3.5" xref="S2.SS1.p1.6.m6.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.6.m6.3.3.2.2.2" xref="S2.SS1.p1.6.m6.3.3.2.2.2.cmml"><mover accent="true" id="S2.SS1.p1.6.m6.3.3.2.2.2.2" xref="S2.SS1.p1.6.m6.3.3.2.2.2.2.cmml"><mi id="S2.SS1.p1.6.m6.3.3.2.2.2.2.2" xref="S2.SS1.p1.6.m6.3.3.2.2.2.2.2.cmml">z</mi><mo id="S2.SS1.p1.6.m6.3.3.2.2.2.2.1" xref="S2.SS1.p1.6.m6.3.3.2.2.2.2.1.cmml">^</mo></mover><mn id="S2.SS1.p1.6.m6.3.3.2.2.2.3" xref="S2.SS1.p1.6.m6.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p1.6.m6.4.4.3.3.6" xref="S2.SS1.p1.6.m6.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">â€¦</mi><mo id="S2.SS1.p1.6.m6.4.4.3.3.7" xref="S2.SS1.p1.6.m6.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.6.m6.4.4.3.3.3" xref="S2.SS1.p1.6.m6.4.4.3.3.3.cmml"><mover accent="true" id="S2.SS1.p1.6.m6.4.4.3.3.3.2" xref="S2.SS1.p1.6.m6.4.4.3.3.3.2.cmml"><mi id="S2.SS1.p1.6.m6.4.4.3.3.3.2.2" xref="S2.SS1.p1.6.m6.4.4.3.3.3.2.2.cmml">z</mi><mo id="S2.SS1.p1.6.m6.4.4.3.3.3.2.1" xref="S2.SS1.p1.6.m6.4.4.3.3.3.2.1.cmml">^</mo></mover><mi id="S2.SS1.p1.6.m6.4.4.3.3.3.3" xref="S2.SS1.p1.6.m6.4.4.3.3.3.3.cmml">T</mi></msub><mo stretchy="false" id="S2.SS1.p1.6.m6.4.4.3.3.8" xref="S2.SS1.p1.6.m6.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.4b"><apply id="S2.SS1.p1.6.m6.4.4.cmml" xref="S2.SS1.p1.6.m6.4.4"><eq id="S2.SS1.p1.6.m6.4.4.4.cmml" xref="S2.SS1.p1.6.m6.4.4.4"></eq><apply id="S2.SS1.p1.6.m6.4.4.5.cmml" xref="S2.SS1.p1.6.m6.4.4.5"><ci id="S2.SS1.p1.6.m6.4.4.5.1.cmml" xref="S2.SS1.p1.6.m6.4.4.5.1">^</ci><ci id="S2.SS1.p1.6.m6.4.4.5.2.cmml" xref="S2.SS1.p1.6.m6.4.4.5.2">ğ‘</ci></apply><set id="S2.SS1.p1.6.m6.4.4.3.4.cmml" xref="S2.SS1.p1.6.m6.4.4.3.3"><apply id="S2.SS1.p1.6.m6.2.2.1.1.1.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.2.2.1.1.1.1.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1">subscript</csymbol><apply id="S2.SS1.p1.6.m6.2.2.1.1.1.2.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.2"><ci id="S2.SS1.p1.6.m6.2.2.1.1.1.2.1.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.2.1">^</ci><ci id="S2.SS1.p1.6.m6.2.2.1.1.1.2.2.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.2.2">ğ‘§</ci></apply><cn type="integer" id="S2.SS1.p1.6.m6.2.2.1.1.1.3.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS1.p1.6.m6.3.3.2.2.2.cmml" xref="S2.SS1.p1.6.m6.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.3.3.2.2.2.1.cmml" xref="S2.SS1.p1.6.m6.3.3.2.2.2">subscript</csymbol><apply id="S2.SS1.p1.6.m6.3.3.2.2.2.2.cmml" xref="S2.SS1.p1.6.m6.3.3.2.2.2.2"><ci id="S2.SS1.p1.6.m6.3.3.2.2.2.2.1.cmml" xref="S2.SS1.p1.6.m6.3.3.2.2.2.2.1">^</ci><ci id="S2.SS1.p1.6.m6.3.3.2.2.2.2.2.cmml" xref="S2.SS1.p1.6.m6.3.3.2.2.2.2.2">ğ‘§</ci></apply><cn type="integer" id="S2.SS1.p1.6.m6.3.3.2.2.2.3.cmml" xref="S2.SS1.p1.6.m6.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">â€¦</ci><apply id="S2.SS1.p1.6.m6.4.4.3.3.3.cmml" xref="S2.SS1.p1.6.m6.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.4.4.3.3.3.1.cmml" xref="S2.SS1.p1.6.m6.4.4.3.3.3">subscript</csymbol><apply id="S2.SS1.p1.6.m6.4.4.3.3.3.2.cmml" xref="S2.SS1.p1.6.m6.4.4.3.3.3.2"><ci id="S2.SS1.p1.6.m6.4.4.3.3.3.2.1.cmml" xref="S2.SS1.p1.6.m6.4.4.3.3.3.2.1">^</ci><ci id="S2.SS1.p1.6.m6.4.4.3.3.3.2.2.cmml" xref="S2.SS1.p1.6.m6.4.4.3.3.3.2.2">ğ‘§</ci></apply><ci id="S2.SS1.p1.6.m6.4.4.3.3.3.3.cmml" xref="S2.SS1.p1.6.m6.4.4.3.3.3.3">ğ‘‡</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.4c">\hat{Z}=\{\hat{z}_{1},\hat{z}_{2},\ldots,\hat{z}_{T}\}</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.6" class="ltx_p">To train the codebook, we opt for two different strategies: <span id="S2.SS1.p2.6.1" class="ltx_text ltx_font_italic">(i)</span> Single-View (<math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="D^{(S)}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><msup id="S2.SS1.p2.1.m1.1.2" xref="S2.SS1.p2.1.m1.1.2.cmml"><mi id="S2.SS1.p2.1.m1.1.2.2" xref="S2.SS1.p2.1.m1.1.2.2.cmml">D</mi><mrow id="S2.SS1.p2.1.m1.1.1.1.3" xref="S2.SS1.p2.1.m1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p2.1.m1.1.1.1.3.1" xref="S2.SS1.p2.1.m1.1.2.cmml">(</mo><mi id="S2.SS1.p2.1.m1.1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.1.cmml">S</mi><mo stretchy="false" id="S2.SS1.p2.1.m1.1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.2.1.cmml" xref="S2.SS1.p2.1.m1.1.2">superscript</csymbol><ci id="S2.SS1.p2.1.m1.1.2.2.cmml" xref="S2.SS1.p2.1.m1.1.2.2">ğ·</ci><ci id="S2.SS1.p2.1.m1.1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1.1">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">D^{(S)}</annotation></semantics></math>), and <span id="S2.SS1.p2.6.2" class="ltx_text ltx_font_italic">(ii)</span> Multi-View Codebook (<math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="D^{(MV)}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><msup id="S2.SS1.p2.2.m2.1.2" xref="S2.SS1.p2.2.m2.1.2.cmml"><mi id="S2.SS1.p2.2.m2.1.2.2" xref="S2.SS1.p2.2.m2.1.2.2.cmml">D</mi><mrow id="S2.SS1.p2.2.m2.1.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p2.2.m2.1.1.1.1.2" xref="S2.SS1.p2.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p2.2.m2.1.1.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.1.1.cmml"><mi id="S2.SS1.p2.2.m2.1.1.1.1.1.2" xref="S2.SS1.p2.2.m2.1.1.1.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.2.m2.1.1.1.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.SS1.p2.2.m2.1.1.1.1.1.3" xref="S2.SS1.p2.2.m2.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S2.SS1.p2.2.m2.1.1.1.1.3" xref="S2.SS1.p2.2.m2.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.2.cmml" xref="S2.SS1.p2.2.m2.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.2.1.cmml" xref="S2.SS1.p2.2.m2.1.2">superscript</csymbol><ci id="S2.SS1.p2.2.m2.1.2.2.cmml" xref="S2.SS1.p2.2.m2.1.2.2">ğ·</ci><apply id="S2.SS1.p2.2.m2.1.1.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1.1"><times id="S2.SS1.p2.2.m2.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1.1.1.1"></times><ci id="S2.SS1.p2.2.m2.1.1.1.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.1.1.1.2">ğ‘€</ci><ci id="S2.SS1.p2.2.m2.1.1.1.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.1.1.1.3">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">D^{(MV)}</annotation></semantics></math>). For the single-view strategy, we trained a simple k-means cluster model using representation from a pretrained SSL model.
Whereas, for the multi-view, we considered the representations (or views <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="V^{(1)}" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><msup id="S2.SS1.p2.3.m3.1.2" xref="S2.SS1.p2.3.m3.1.2.cmml"><mi id="S2.SS1.p2.3.m3.1.2.2" xref="S2.SS1.p2.3.m3.1.2.2.cmml">V</mi><mrow id="S2.SS1.p2.3.m3.1.1.1.3" xref="S2.SS1.p2.3.m3.1.2.cmml"><mo stretchy="false" id="S2.SS1.p2.3.m3.1.1.1.3.1" xref="S2.SS1.p2.3.m3.1.2.cmml">(</mo><mn id="S2.SS1.p2.3.m3.1.1.1.1" xref="S2.SS1.p2.3.m3.1.1.1.1.cmml">1</mn><mo stretchy="false" id="S2.SS1.p2.3.m3.1.1.1.3.2" xref="S2.SS1.p2.3.m3.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.2.cmml" xref="S2.SS1.p2.3.m3.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.3.m3.1.2.1.cmml" xref="S2.SS1.p2.3.m3.1.2">superscript</csymbol><ci id="S2.SS1.p2.3.m3.1.2.2.cmml" xref="S2.SS1.p2.3.m3.1.2.2">ğ‘‰</ci><cn type="integer" id="S2.SS1.p2.3.m3.1.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">V^{(1)}</annotation></semantics></math> and <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="V^{(2)}" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><msup id="S2.SS1.p2.4.m4.1.2" xref="S2.SS1.p2.4.m4.1.2.cmml"><mi id="S2.SS1.p2.4.m4.1.2.2" xref="S2.SS1.p2.4.m4.1.2.2.cmml">V</mi><mrow id="S2.SS1.p2.4.m4.1.1.1.3" xref="S2.SS1.p2.4.m4.1.2.cmml"><mo stretchy="false" id="S2.SS1.p2.4.m4.1.1.1.3.1" xref="S2.SS1.p2.4.m4.1.2.cmml">(</mo><mn id="S2.SS1.p2.4.m4.1.1.1.1" xref="S2.SS1.p2.4.m4.1.1.1.1.cmml">2</mn><mo stretchy="false" id="S2.SS1.p2.4.m4.1.1.1.3.2" xref="S2.SS1.p2.4.m4.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><apply id="S2.SS1.p2.4.m4.1.2.cmml" xref="S2.SS1.p2.4.m4.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.1.2.1.cmml" xref="S2.SS1.p2.4.m4.1.2">superscript</csymbol><ci id="S2.SS1.p2.4.m4.1.2.2.cmml" xref="S2.SS1.p2.4.m4.1.2.2">ğ‘‰</ci><cn type="integer" id="S2.SS1.p2.4.m4.1.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">V^{(2)}</annotation></semantics></math>) from two different SSL models and trained k-means clustering model.
Given the conditional independence of <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="V^{(1)}" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><msup id="S2.SS1.p2.5.m5.1.2" xref="S2.SS1.p2.5.m5.1.2.cmml"><mi id="S2.SS1.p2.5.m5.1.2.2" xref="S2.SS1.p2.5.m5.1.2.2.cmml">V</mi><mrow id="S2.SS1.p2.5.m5.1.1.1.3" xref="S2.SS1.p2.5.m5.1.2.cmml"><mo stretchy="false" id="S2.SS1.p2.5.m5.1.1.1.3.1" xref="S2.SS1.p2.5.m5.1.2.cmml">(</mo><mn id="S2.SS1.p2.5.m5.1.1.1.1" xref="S2.SS1.p2.5.m5.1.1.1.1.cmml">1</mn><mo stretchy="false" id="S2.SS1.p2.5.m5.1.1.1.3.2" xref="S2.SS1.p2.5.m5.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><apply id="S2.SS1.p2.5.m5.1.2.cmml" xref="S2.SS1.p2.5.m5.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.5.m5.1.2.1.cmml" xref="S2.SS1.p2.5.m5.1.2">superscript</csymbol><ci id="S2.SS1.p2.5.m5.1.2.2.cmml" xref="S2.SS1.p2.5.m5.1.2.2">ğ‘‰</ci><cn type="integer" id="S2.SS1.p2.5.m5.1.1.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">V^{(1)}</annotation></semantics></math> and <math id="S2.SS1.p2.6.m6.1" class="ltx_Math" alttext="V^{(2)}" display="inline"><semantics id="S2.SS1.p2.6.m6.1a"><msup id="S2.SS1.p2.6.m6.1.2" xref="S2.SS1.p2.6.m6.1.2.cmml"><mi id="S2.SS1.p2.6.m6.1.2.2" xref="S2.SS1.p2.6.m6.1.2.2.cmml">V</mi><mrow id="S2.SS1.p2.6.m6.1.1.1.3" xref="S2.SS1.p2.6.m6.1.2.cmml"><mo stretchy="false" id="S2.SS1.p2.6.m6.1.1.1.3.1" xref="S2.SS1.p2.6.m6.1.2.cmml">(</mo><mn id="S2.SS1.p2.6.m6.1.1.1.1" xref="S2.SS1.p2.6.m6.1.1.1.1.cmml">2</mn><mo stretchy="false" id="S2.SS1.p2.6.m6.1.1.1.3.2" xref="S2.SS1.p2.6.m6.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.1b"><apply id="S2.SS1.p2.6.m6.1.2.cmml" xref="S2.SS1.p2.6.m6.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.6.m6.1.2.1.cmml" xref="S2.SS1.p2.6.m6.1.2">superscript</csymbol><ci id="S2.SS1.p2.6.m6.1.2.2.cmml" xref="S2.SS1.p2.6.m6.1.2.2">ğ‘‰</ci><cn type="integer" id="S2.SS1.p2.6.m6.1.1.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.1c">V^{(2)}</annotation></semantics></math>, the strategy maximizes (M) the log-likelihood of each view, given the expected values for the hidden variables of the other view from the previous iteration and then calculate the expectation (E) for the hidden variables for the given view model parameters. Hence, optimizing for parameters with EM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> for both views.
The optimization process is terminated when the improvement in log-likelihood is plateaued for a fixed number of iterations in each view.
The final discrete label (during inference) is then assigned to the cluster that has the largest averaged posterior over both views.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div id="S2.F1.1" class="ltx_inline-block ltx_transformed_outer" style="width:195.7pt;height:203.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.3pt,18.0pt) scale(0.85,0.85) ;"><img src="/html/2406.13431/assets/image.png" id="S2.F1.1.g1" class="ltx_graphics ltx_img_square" width="314" height="332" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Discrete children ASR with single-view and multi-view discrete input.</figcaption>
</figure>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.2" class="ltx_p">The resultant discrete labels <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="\hat{Z}" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mover accent="true" id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml"><mi id="S2.SS1.p3.1.m1.1.1.2" xref="S2.SS1.p3.1.m1.1.1.2.cmml">Z</mi><mo id="S2.SS1.p3.1.m1.1.1.1" xref="S2.SS1.p3.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><apply id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1"><ci id="S2.SS1.p3.1.m1.1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1.1">^</ci><ci id="S2.SS1.p3.1.m1.1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.1.2">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">\hat{Z}</annotation></semantics></math> are temporarily aligned with the <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><mi id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><ci id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">Z</annotation></semantics></math> and include repeated or commonly co-existing units. We followed steps such as de-duplication and subword modeling to reduce such redundancies, as proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. For de-duplication, we merge the consecutive subsequences of identical tokens into a single token. Following, we transform the discrete sequence into meta-tokens sequence by using the Sentencepiece unigram model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pretrained SSL</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Given an input utterance, we extracted the representation using the following pretrained models:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">facebook/<span id="S2.I1.i1.p1.1.1.1" class="ltx_text ltx_font_italic">HuBERT</span>-large-ll60k</span>: <span id="S2.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">HuBERT</span> identifies acoustic units by employing a clustering method to generate target labels corresponding to input features. Subsequently, masking is employed on the input features, and training is carried out to minimize the masked prediction loss using cluster labels as targets.
This model comprises <span id="S2.I1.i1.p1.1.3" class="ltx_text ltx_font_bold">316M parameters</span>.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">microsoft/<span id="S2.I1.i2.p1.1.1.1" class="ltx_text ltx_font_italic">WavLM</span>-large:</span> WavLM introduces gated relative position bias into the transformer architecture. In addition to employing masked prediction loss akin to <span id="S2.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">HuBERT</span>, it also integrates a denoising task during self-supervised learning. This model comprises <span id="S2.I1.i2.p1.1.3" class="ltx_text ltx_font_bold">316M parameters</span>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>ASR Architecture</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">For the children ASR, E-Branchformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> encoder and Transformer decoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> architecture is trained jointly with Connectionist Temporal Classification (CTC)/attention multi-task learning.
E-Branchformer is an improved version of Branchformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> with two parallel Macaron-style feed-forward network branches, with one branch responsible for capturing global context using multi-head attention, while the second branch captures local contextual information using multi-layer perceptron with convolutional gating (cgMLP). Following, the two branches are merged by concatenation operation, a 1-D depth-wise convolution, and a linear projection. The transformer decoder is used as the decoder part for the sequence-to-sequence model. The transformer decoder comprises an extra masked self-attention layer on top of an MHSA and a feed-forward layer.
The hyperparameters used for experiments are as shown in TableÂ <a href="#S2.T1" title="Table 1 â€£ 2.3 ASR Architecture â€£ 2 Methodology â€£ Childrenâ€™s Speech Recognition through Discrete Token Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Discrete Children ASR Model Configuration </figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:152.1pt;height:236.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.4pt,13.1pt) scale(0.9,0.9) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r"><span id="S2.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Hyperparameters</span></th>
<th id="S2.T1.1.1.1.1.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column"><span id="S2.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Values</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Kernel Size</td>
<td id="S2.T1.1.1.2.1.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">31</td>
</tr>
<tr id="S2.T1.1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Feature dimension</td>
<td id="S2.T1.1.1.3.2.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">512</td>
</tr>
<tr id="S2.T1.1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"># encoder layers</td>
<td id="S2.T1.1.1.4.3.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">12</td>
</tr>
<tr id="S2.T1.1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Encoder units</td>
<td id="S2.T1.1.1.5.4.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">1024</td>
</tr>
<tr id="S2.T1.1.1.6.5" class="ltx_tr">
<td id="S2.T1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"># decoder layers</td>
<td id="S2.T1.1.1.6.5.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">6</td>
</tr>
<tr id="S2.T1.1.1.7.6" class="ltx_tr">
<td id="S2.T1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Decoder units</td>
<td id="S2.T1.1.1.7.6.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">2048</td>
</tr>
<tr id="S2.T1.1.1.8.7" class="ltx_tr">
<td id="S2.T1.1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Attention heads</td>
<td id="S2.T1.1.1.8.7.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">4</td>
</tr>
<tr id="S2.T1.1.1.9.8" class="ltx_tr">
<td id="S2.T1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S2.T1.1.1.9.8.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.9.8.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.9.8.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Number of target</td>
</tr>
<tr id="S2.T1.1.1.9.8.1.1.2" class="ltx_tr">
<td id="S2.T1.1.1.9.8.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">BPE (byte pair encoding)</td>
</tr>
</table>
</td>
<td id="S2.T1.1.1.9.8.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">5000</td>
</tr>
<tr id="S2.T1.1.1.10.9" class="ltx_tr">
<td id="S2.T1.1.1.10.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S2.T1.1.1.10.9.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.10.9.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.10.9.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Number of</td>
</tr>
<tr id="S2.T1.1.1.10.9.1.1.2" class="ltx_tr">
<td id="S2.T1.1.1.10.9.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">source BPE</td>
</tr>
</table>
</td>
<td id="S2.T1.1.1.10.9.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">6000</td>
</tr>
<tr id="S2.T1.1.1.11.10" class="ltx_tr">
<td id="S2.T1.1.1.11.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Number of clusters</td>
<td id="S2.T1.1.1.11.10.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">2000</td>
</tr>
<tr id="S2.T1.1.1.12.11" class="ltx_tr">
<td id="S2.T1.1.1.12.11.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">CTC weight</td>
<td id="S2.T1.1.1.12.11.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t">0.3</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Settings</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">The My Science Tutor (<span id="S3.SS1.p1.1.1.1" class="ltx_text ltx_font_italic">MyST</span>) Corpus</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> is a collection of American English datasets featuring child speech, totaling over 393 hours from grades 3 to 5. The dataset features dialogs between the virtual tutors and the students, discussing various scientific concepts.
For the empirical study, we opt for 221 hours of the transcribed dataset, filtering out the very short (0.1 seconds and below) and too long (60 seconds and above) utterances. This preprocessing helped to reduce the computation memory needed to train the models. Following, we use the official data splits as train (167.48 hours), validate (25.60 hours), and test (27.95 hours) dataset. For discrete codebook training, 10% of the training dataset, which amounts to 16.7 hours, is used.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">The CMU Kids Speech Corpus<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_medium">1</span></span><a target="_blank" href="http://www.ldc.upenn.edu/Catalog/LDC97S63.html" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium">http://www.ldc.upenn.edu/Catalog/LDC97S63.html</a></span></span></span></span> is a collection of children's speech datasets containing 76 speakers, where the majority of the speakers are from grades 1 to 3. The age range of the children spans from six to eleven years old, with a distribution of 24 male and 52 female speakers.
The whole dataset includes a total of 5180 read utterances. We opted to use only <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mo id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">â‰ˆ</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><approx id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\approx</annotation></semantics></math> 2.06 hours (22 % of total data) of read-sentences as the unseen domain and age test set.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Utterances considered for the test have in-depth error analysis; the ids and information are collected from <a target="_blank" href="https://isip.piconepress.com/projects/speech/databases/kids_speech" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://isip.piconepress.com/projects/speech/databases/kids_speech</a> </span></span></span></p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Non-Native children's speech corpus</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> is a collection of English read and spontaneous speech data from 20 bilingual (Telugu-English) children aged 8 to 12 with English proficiency. The dataset is gender-balanced (11 female and 9 male speakers) and is essential to test our proposed model's generalization capabilities for non-native speakers.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>ASR Experiments</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Baselines</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">We opt for two strong ASR baselines using the pretrained SSL models: HuBERT and WavLM. These models serve as feature extractors, providing rich and continuous contextual representations. The final input representation is obtained by computing the weighted sum of the embeddings from all layers. Following, we use the same encoder-decoder ASR architecture mentioned in Section <a href="#S2.SS3" title="2.3 ASR Architecture â€£ 2 Methodology â€£ Childrenâ€™s Speech Recognition through Discrete Token Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>. These baseline models serve as reference points to measure the relative performance of the Discrete ASR model.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Toplines</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">We compare the performances of the discrete ASRs with the readily available Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> models in Zero-shot and fine-tuned settings to understand the upper-bound performance. Whisper is a Transformer-based encoder-decoder model trained on 680,000 hours of labeled speech data annotated through weak supervision. These models underwent training using multilingual datasets.
For the zero-shot settings, we present upper-bound results using two different model sizes:<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://huggingface.co/openai/%7Bwhisper-small.en,whisper-medium.en%7D" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/openai/{whisper-small.en,whisper-medium.en}</a></span></span></span> small (244M parameters) and medium (769M parameters). We utilize checkpoints for models trained exclusively on English data for the ASR task using 563,000 hours of data.
We fine-tuned the whisper model with 55 hours of MyST training set to mimic few-shot. Similarly, we also evaluate the Whisper model fine-tuned on the entire MyST training data, utilizing both<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://huggingface.co/aadel4/%7Bkid-whisper-small-en-myst,kid-whisper-medium-en-myst%7D" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/aadel4/{kid-whisper-small-en-myst,kid-whisper-medium-en-myst}</a></span></span></span> the small and medium English-only model checkpoints.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model Training</h3>

<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Discrete Codebook Training</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.2" class="ltx_p">The codebook responsible for generating discrete tokens from the SSL features is trained using k-means clustering in both single-view and multi-view scenarios. Consistent settings are applied to ensure a fair comparison between the two methods. For all the settings, the number of clusters is set to 2000, motivated by the success reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, providing a fine granularity in representing the feature space. The k-means++ initialization method is used to enhance the clustering process.
Additionally, the number of random initializations (<math id="S3.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="N_{init}" display="inline"><semantics id="S3.SS3.SSS1.p1.1.m1.1a"><msub id="S3.SS3.SSS1.p1.1.m1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS1.p1.1.m1.1.1.2" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml">N</mi><mrow id="S3.SS3.SSS1.p1.1.m1.1.1.3" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS3.SSS1.p1.1.m1.1.1.3.2" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.1.m1.1.1.3.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.SSS1.p1.1.m1.1.1.3.3" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.1.m1.1.1.3.1a" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.SSS1.p1.1.m1.1.1.3.4" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.1.m1.1.1.3.1b" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.SSS1.p1.1.m1.1.1.3.5" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.1b"><apply id="S3.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.2">ğ‘</ci><apply id="S3.SS3.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.3"><times id="S3.SS3.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.1"></times><ci id="S3.SS3.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.2">ğ‘–</ci><ci id="S3.SS3.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.3">ğ‘›</ci><ci id="S3.SS3.SSS1.p1.1.m1.1.1.3.4.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.4">ğ‘–</ci><ci id="S3.SS3.SSS1.p1.1.m1.1.1.3.5.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.1c">N_{init}</annotation></semantics></math>) is set to 10, considering multiple starting points to achieve a better overall clustering solution. The maximum number of iterations (<math id="S3.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="max_{iter}" display="inline"><semantics id="S3.SS3.SSS1.p1.2.m2.1a"><mrow id="S3.SS3.SSS1.p1.2.m2.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSS1.p1.2.m2.1.1.2" xref="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.2.m2.1.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.SSS1.p1.2.m2.1.1.3" xref="S3.SS3.SSS1.p1.2.m2.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.2.m2.1.1.1a" xref="S3.SS3.SSS1.p1.2.m2.1.1.1.cmml">â€‹</mo><msub id="S3.SS3.SSS1.p1.2.m2.1.1.4" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.cmml"><mi id="S3.SS3.SSS1.p1.2.m2.1.1.4.2" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.2.cmml">x</mi><mrow id="S3.SS3.SSS1.p1.2.m2.1.1.4.3" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3.cmml"><mi id="S3.SS3.SSS1.p1.2.m2.1.1.4.3.2" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.2.m2.1.1.4.3.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3.1.cmml">â€‹</mo><mi id="S3.SS3.SSS1.p1.2.m2.1.1.4.3.3" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.2.m2.1.1.4.3.1a" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3.1.cmml">â€‹</mo><mi id="S3.SS3.SSS1.p1.2.m2.1.1.4.3.4" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p1.2.m2.1.1.4.3.1b" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3.1.cmml">â€‹</mo><mi id="S3.SS3.SSS1.p1.2.m2.1.1.4.3.5" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3.5.cmml">r</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.2.m2.1b"><apply id="S3.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1"><times id="S3.SS3.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.1"></times><ci id="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.2">ğ‘š</ci><ci id="S3.SS3.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.3">ğ‘</ci><apply id="S3.SS3.SSS1.p1.2.m2.1.1.4.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.2.m2.1.1.4.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.4">subscript</csymbol><ci id="S3.SS3.SSS1.p1.2.m2.1.1.4.2.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.2">ğ‘¥</ci><apply id="S3.SS3.SSS1.p1.2.m2.1.1.4.3.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3"><times id="S3.SS3.SSS1.p1.2.m2.1.1.4.3.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3.1"></times><ci id="S3.SS3.SSS1.p1.2.m2.1.1.4.3.2.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3.2">ğ‘–</ci><ci id="S3.SS3.SSS1.p1.2.m2.1.1.4.3.3.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3.3">ğ‘¡</ci><ci id="S3.SS3.SSS1.p1.2.m2.1.1.4.3.4.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3.4">ğ‘’</ci><ci id="S3.SS3.SSS1.p1.2.m2.1.1.4.3.5.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.4.3.5">ğ‘Ÿ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.2.m2.1c">max_{iter}</annotation></semantics></math>) is limited to 100.
These settings balance computational efficiency with clustering accuracy, effectively capturing the essential characteristics of the SSL features.
By maintaining these consistent parameters across both single-view and multi-view scenarios, we aim to provide a robust comparison of the clustering performance and the resulting impact on the discrete token generation.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>ASR Model training</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">The architecture specified in section <a href="#S2.SS3" title="2.3 ASR Architecture â€£ 2 Methodology â€£ Childrenâ€™s Speech Recognition through Discrete Token Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a> is adopted for training single-view and multi-view discrete ASR models. The <span id="S3.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_italic">ESPnet</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> recipe<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/espnet/espnet/tree/master/egs2/librispeech_100/asr2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/espnet/espnet/tree/master/egs2/librispeech_100/asr2</a></span></span></span> is employed for training, utilizing two 32GB V100 GPUs. The models are trained using a learning rate of 0.002, with a warmup learning rate scheduler and the Adam optimizer across 100 epochs. Additionally, to augment the training data and enhance model robustness, the SpecAugment technique is applied to the input, facilitating better generalization.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Reported WER (<math id="S3.T2.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.5.m1.1b"><mo stretchy="false" id="S3.T2.5.m1.1.1" xref="S3.T2.5.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T2.5.m1.1c"><ci id="S3.T2.5.m1.1.1.cmml" xref="S3.T2.5.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.m1.1d">\downarrow</annotation></semantics></math>) presenting the baselines (<span id="S3.T2.18.1" class="ltx_text ltx_font_italic">HuBERT</span>-E2E and <span id="S3.T2.19.2" class="ltx_text ltx_font_italic">WavLM</span>-E2E) and the topline results using Whisper pre-trained model in zero-shot (0) and fine-tuned with 55 hours and all (All) <span id="S3.T2.20.3" class="ltx_text ltx_font_italic">MyST</span> training data. <math id="S3.T2.6.m2.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T2.6.m2.1b"><mi mathvariant="normal" id="S3.T2.6.m2.1.1" xref="S3.T2.6.m2.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S3.T2.6.m2.1c"><ci id="S3.T2.6.m2.1.1.cmml" xref="S3.T2.6.m2.1.1">Î”</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.m2.1d">\Delta</annotation></semantics></math>= <math id="S3.T2.7.m3.1" class="ltx_math_unparsed" alttext="|WavLM-D^{(S)}-*|" display="inline"><semantics id="S3.T2.7.m3.1b"><mrow id="S3.T2.7.m3.1c"><mo fence="false" rspace="0.167em" stretchy="false" id="S3.T2.7.m3.1.2">|</mo><mi id="S3.T2.7.m3.1.3">W</mi><mi id="S3.T2.7.m3.1.4">a</mi><mi id="S3.T2.7.m3.1.5">v</mi><mi id="S3.T2.7.m3.1.6">L</mi><mi id="S3.T2.7.m3.1.7">M</mi><mo id="S3.T2.7.m3.1.8">âˆ’</mo><msup id="S3.T2.7.m3.1.9"><mi id="S3.T2.7.m3.1.9.2">D</mi><mrow id="S3.T2.7.m3.1.1.1.3"><mo stretchy="false" id="S3.T2.7.m3.1.1.1.3.1">(</mo><mi id="S3.T2.7.m3.1.1.1.1">S</mi><mo stretchy="false" id="S3.T2.7.m3.1.1.1.3.2">)</mo></mrow></msup><mo rspace="0em" id="S3.T2.7.m3.1.10">âˆ’</mo><mo lspace="0em" rspace="0em" id="S3.T2.7.m3.1.11">âˆ—</mo><mo fence="false" stretchy="false" id="S3.T2.7.m3.1.12">|</mo></mrow><annotation encoding="application/x-tex" id="S3.T2.7.m3.1d">|WavLM-D^{(S)}-*|</annotation></semantics></math>, where <sup id="S3.T2.21.4" class="ltx_sup">âˆ—</sup> is different ASR results. Whisper-S, M: Whisper small (244M parameters) and medium (769M) models. Discrete token results are reported using <span id="S3.T2.22.5" class="ltx_text ltx_font_italic">HuBERT</span> and <span id="S3.T2.23.6" class="ltx_text ltx_font_italic">WavLM</span> models here.</figcaption>
<div id="S3.T2.11" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:152.5pt;height:226.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.5pt,12.6pt) scale(0.9,0.9) ;">
<table id="S3.T2.11.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.9.1.1" class="ltx_tr">
<th id="S3.T2.9.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S3.T2.9.1.1.2.1" class="ltx_text ltx_font_bold">Models</span></th>
<td id="S3.T2.9.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S3.T2.9.1.1.1.1" class="ltx_text ltx_font_bold">WER</span> (<math id="S3.T2.9.1.1.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T2.9.1.1.1.m1.1a"><mi mathvariant="normal" id="S3.T2.9.1.1.1.m1.1.1" xref="S3.T2.9.1.1.1.m1.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S3.T2.9.1.1.1.m1.1b"><ci id="S3.T2.9.1.1.1.m1.1.1.cmml" xref="S3.T2.9.1.1.1.m1.1.1">Î”</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.9.1.1.1.m1.1c">\Delta</annotation></semantics></math>)</td>
</tr>
<tr id="S3.T2.11.3.4.1" class="ltx_tr">
<th id="S3.T2.11.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="2">Discrete Single-View ASRs</th>
</tr>
<tr id="S3.T2.10.2.2" class="ltx_tr">
<th id="S3.T2.10.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S3.T2.10.2.2.1.1" class="ltx_text ltx_font_italic">HuBERT</span>-<math id="S3.T2.10.2.2.1.m1.1" class="ltx_Math" alttext="D^{(S)}" display="inline"><semantics id="S3.T2.10.2.2.1.m1.1a"><msup id="S3.T2.10.2.2.1.m1.1.2" xref="S3.T2.10.2.2.1.m1.1.2.cmml"><mi id="S3.T2.10.2.2.1.m1.1.2.2" xref="S3.T2.10.2.2.1.m1.1.2.2.cmml">D</mi><mrow id="S3.T2.10.2.2.1.m1.1.1.1.3" xref="S3.T2.10.2.2.1.m1.1.2.cmml"><mo stretchy="false" id="S3.T2.10.2.2.1.m1.1.1.1.3.1" xref="S3.T2.10.2.2.1.m1.1.2.cmml">(</mo><mi id="S3.T2.10.2.2.1.m1.1.1.1.1" xref="S3.T2.10.2.2.1.m1.1.1.1.1.cmml">S</mi><mo stretchy="false" id="S3.T2.10.2.2.1.m1.1.1.1.3.2" xref="S3.T2.10.2.2.1.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T2.10.2.2.1.m1.1b"><apply id="S3.T2.10.2.2.1.m1.1.2.cmml" xref="S3.T2.10.2.2.1.m1.1.2"><csymbol cd="ambiguous" id="S3.T2.10.2.2.1.m1.1.2.1.cmml" xref="S3.T2.10.2.2.1.m1.1.2">superscript</csymbol><ci id="S3.T2.10.2.2.1.m1.1.2.2.cmml" xref="S3.T2.10.2.2.1.m1.1.2.2">ğ·</ci><ci id="S3.T2.10.2.2.1.m1.1.1.1.1.cmml" xref="S3.T2.10.2.2.1.m1.1.1.1.1">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.10.2.2.1.m1.1c">D^{(S)}</annotation></semantics></math>
</th>
<td id="S3.T2.10.2.2.2" class="ltx_td ltx_align_center ltx_border_t">15.65</td>
</tr>
<tr id="S3.T2.11.3.3" class="ltx_tr">
<th id="S3.T2.11.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S3.T2.11.3.3.1.1" class="ltx_text ltx_font_italic">WavLM</span>-<math id="S3.T2.11.3.3.1.m1.1" class="ltx_Math" alttext="D^{(S)}" display="inline"><semantics id="S3.T2.11.3.3.1.m1.1a"><msup id="S3.T2.11.3.3.1.m1.1.2" xref="S3.T2.11.3.3.1.m1.1.2.cmml"><mi id="S3.T2.11.3.3.1.m1.1.2.2" xref="S3.T2.11.3.3.1.m1.1.2.2.cmml">D</mi><mrow id="S3.T2.11.3.3.1.m1.1.1.1.3" xref="S3.T2.11.3.3.1.m1.1.2.cmml"><mo stretchy="false" id="S3.T2.11.3.3.1.m1.1.1.1.3.1" xref="S3.T2.11.3.3.1.m1.1.2.cmml">(</mo><mi id="S3.T2.11.3.3.1.m1.1.1.1.1" xref="S3.T2.11.3.3.1.m1.1.1.1.1.cmml">S</mi><mo stretchy="false" id="S3.T2.11.3.3.1.m1.1.1.1.3.2" xref="S3.T2.11.3.3.1.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T2.11.3.3.1.m1.1b"><apply id="S3.T2.11.3.3.1.m1.1.2.cmml" xref="S3.T2.11.3.3.1.m1.1.2"><csymbol cd="ambiguous" id="S3.T2.11.3.3.1.m1.1.2.1.cmml" xref="S3.T2.11.3.3.1.m1.1.2">superscript</csymbol><ci id="S3.T2.11.3.3.1.m1.1.2.2.cmml" xref="S3.T2.11.3.3.1.m1.1.2.2">ğ·</ci><ci id="S3.T2.11.3.3.1.m1.1.1.1.1.cmml" xref="S3.T2.11.3.3.1.m1.1.1.1.1">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.11.3.3.1.m1.1c">D^{(S)}</annotation></semantics></math>
</th>
<td id="S3.T2.11.3.3.2" class="ltx_td ltx_align_center">14.22</td>
</tr>
<tr id="S3.T2.11.3.5.2" class="ltx_tr">
<th id="S3.T2.11.3.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="2">Baseline ASRs</th>
</tr>
<tr id="S3.T2.11.3.6.3" class="ltx_tr">
<th id="S3.T2.11.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S3.T2.11.3.6.3.1.1" class="ltx_text ltx_font_italic">HuBERT</span>-E2E</th>
<td id="S3.T2.11.3.6.3.2" class="ltx_td ltx_align_center ltx_border_t">14.98 (0.67)</td>
</tr>
<tr id="S3.T2.11.3.7.4" class="ltx_tr">
<th id="S3.T2.11.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S3.T2.11.3.7.4.1.1" class="ltx_text ltx_font_italic">WavLM</span>-E2E</th>
<td id="S3.T2.11.3.7.4.2" class="ltx_td ltx_align_center">13.27 (0.95)</td>
</tr>
<tr id="S3.T2.11.3.8.5" class="ltx_tr">
<th id="S3.T2.11.3.8.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="2">Topline ASRs</th>
</tr>
<tr id="S3.T2.11.3.9.6" class="ltx_tr">
<th id="S3.T2.11.3.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Whisper-S (0)</th>
<td id="S3.T2.11.3.9.6.2" class="ltx_td ltx_align_center ltx_border_t">13.93 (0.29)</td>
</tr>
<tr id="S3.T2.11.3.10.7" class="ltx_tr">
<th id="S3.T2.11.3.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Whisper-M (0)</th>
<td id="S3.T2.11.3.10.7.2" class="ltx_td ltx_align_center">12.9 (1.32)</td>
</tr>
<tr id="S3.T2.11.3.11.8" class="ltx_tr">
<th id="S3.T2.11.3.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Whisper-S (55 hrs)</th>
<td id="S3.T2.11.3.11.8.2" class="ltx_td ltx_align_center ltx_border_t">13.23 (0.99)</td>
</tr>
<tr id="S3.T2.11.3.12.9" class="ltx_tr">
<th id="S3.T2.11.3.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Whisper-M (55 hrs)</th>
<td id="S3.T2.11.3.12.9.2" class="ltx_td ltx_align_center">14.4 (0.18)</td>
</tr>
<tr id="S3.T2.11.3.13.10" class="ltx_tr">
<th id="S3.T2.11.3.13.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Whisper-S (All)</th>
<td id="S3.T2.11.3.13.10.2" class="ltx_td ltx_align_center ltx_border_t">9.11 (5.11)</td>
</tr>
<tr id="S3.T2.11.3.14.11" class="ltx_tr">
<th id="S3.T2.11.3.14.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Whisper-M (All)</th>
<td id="S3.T2.11.3.14.11.2" class="ltx_td ltx_align_center ltx_border_bb">8.91 (5.31)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Reported WER (<math id="S3.T3.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T3.2.m1.1b"><mo stretchy="false" id="S3.T3.2.m1.1.1" xref="S3.T3.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.m1.1c"><ci id="S3.T3.2.m1.1.1.cmml" xref="S3.T3.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.m1.1d">\downarrow</annotation></semantics></math>) presenting the results with discrete labels using <span id="S3.T3.9.1" class="ltx_text ltx_font_italic">HuBERT</span> and <span id="S3.T3.10.2" class="ltx_text ltx_font_italic">WavLM</span> for single- and multi-view representation along with the topline results using Whisper pre-trained model in zero-shot (0) and fine-tuned with 55 hours and All <span id="S3.T3.11.3" class="ltx_text ltx_font_italic">MyST</span> training data. U: Unseen domain/data. Whisper-M: Whisper medium model (769M parameters). All discrete models have 40.36M parameters. CMUk: CMU kids test subset.</figcaption>
<div id="S3.T3.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:239.7pt;height:162pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.3pt,9.0pt) scale(0.9,0.9) ;">
<table id="S3.T3.5.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.5.3.4.1" class="ltx_tr">
<th id="S3.T3.5.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S3.T3.5.3.4.1.1.1" class="ltx_text ltx_font_bold">WER</span></th>
<td id="S3.T3.5.3.4.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.5.3.4.1.2.1" class="ltx_text ltx_font_bold">Seen</span></td>
<td id="S3.T3.5.3.4.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.5.3.4.1.3.1" class="ltx_text ltx_font_bold">U: Domain</span></td>
<td id="S3.T3.5.3.4.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S3.T3.5.3.4.1.4.1" class="ltx_text ltx_font_bold">U: Non-native</span></td>
</tr>
<tr id="S3.T3.5.3.5.2" class="ltx_tr">
<th id="S3.T3.5.3.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T3.5.3.5.2.1.1" class="ltx_text ltx_font_italic">Models</span></th>
<td id="S3.T3.5.3.5.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.5.3.5.2.2.1" class="ltx_text ltx_font_italic">MyST</span></td>
<td id="S3.T3.5.3.5.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.5.3.5.2.3.1" class="ltx_text ltx_font_italic">CMUk</span></td>
<td id="S3.T3.5.3.5.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.5.3.5.2.4.1" class="ltx_text ltx_font_italic">Read</span></td>
<td id="S3.T3.5.3.5.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.5.3.5.2.5.1" class="ltx_text ltx_font_italic">Spont.</span></td>
</tr>
<tr id="S3.T3.5.3.6.3" class="ltx_tr">
<th id="S3.T3.5.3.6.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5">Single-View Discrete Tokens</th>
</tr>
<tr id="S3.T3.3.1.1" class="ltx_tr">
<th id="S3.T3.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S3.T3.3.1.1.1.1" class="ltx_text ltx_font_italic">HuBERT</span>-<math id="S3.T3.3.1.1.1.m1.1" class="ltx_Math" alttext="D^{(S)}" display="inline"><semantics id="S3.T3.3.1.1.1.m1.1a"><msup id="S3.T3.3.1.1.1.m1.1.2" xref="S3.T3.3.1.1.1.m1.1.2.cmml"><mi id="S3.T3.3.1.1.1.m1.1.2.2" xref="S3.T3.3.1.1.1.m1.1.2.2.cmml">D</mi><mrow id="S3.T3.3.1.1.1.m1.1.1.1.3" xref="S3.T3.3.1.1.1.m1.1.2.cmml"><mo stretchy="false" id="S3.T3.3.1.1.1.m1.1.1.1.3.1" xref="S3.T3.3.1.1.1.m1.1.2.cmml">(</mo><mi id="S3.T3.3.1.1.1.m1.1.1.1.1" xref="S3.T3.3.1.1.1.m1.1.1.1.1.cmml">S</mi><mo stretchy="false" id="S3.T3.3.1.1.1.m1.1.1.1.3.2" xref="S3.T3.3.1.1.1.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T3.3.1.1.1.m1.1b"><apply id="S3.T3.3.1.1.1.m1.1.2.cmml" xref="S3.T3.3.1.1.1.m1.1.2"><csymbol cd="ambiguous" id="S3.T3.3.1.1.1.m1.1.2.1.cmml" xref="S3.T3.3.1.1.1.m1.1.2">superscript</csymbol><ci id="S3.T3.3.1.1.1.m1.1.2.2.cmml" xref="S3.T3.3.1.1.1.m1.1.2.2">ğ·</ci><ci id="S3.T3.3.1.1.1.m1.1.1.1.1.cmml" xref="S3.T3.3.1.1.1.m1.1.1.1.1">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.1.1.1.m1.1c">D^{(S)}</annotation></semantics></math>
</th>
<td id="S3.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_border_t">15.65</td>
<td id="S3.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_border_t">47.78</td>
<td id="S3.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_border_t">38.40</td>
<td id="S3.T3.3.1.1.5" class="ltx_td ltx_align_center ltx_border_t">64.63</td>
</tr>
<tr id="S3.T3.4.2.2" class="ltx_tr">
<th id="S3.T3.4.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S3.T3.4.2.2.1.1" class="ltx_text ltx_font_italic">WavLM</span>-<math id="S3.T3.4.2.2.1.m1.1" class="ltx_Math" alttext="D^{(S)}" display="inline"><semantics id="S3.T3.4.2.2.1.m1.1a"><msup id="S3.T3.4.2.2.1.m1.1.2" xref="S3.T3.4.2.2.1.m1.1.2.cmml"><mi id="S3.T3.4.2.2.1.m1.1.2.2" xref="S3.T3.4.2.2.1.m1.1.2.2.cmml">D</mi><mrow id="S3.T3.4.2.2.1.m1.1.1.1.3" xref="S3.T3.4.2.2.1.m1.1.2.cmml"><mo stretchy="false" id="S3.T3.4.2.2.1.m1.1.1.1.3.1" xref="S3.T3.4.2.2.1.m1.1.2.cmml">(</mo><mi id="S3.T3.4.2.2.1.m1.1.1.1.1" xref="S3.T3.4.2.2.1.m1.1.1.1.1.cmml">S</mi><mo stretchy="false" id="S3.T3.4.2.2.1.m1.1.1.1.3.2" xref="S3.T3.4.2.2.1.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T3.4.2.2.1.m1.1b"><apply id="S3.T3.4.2.2.1.m1.1.2.cmml" xref="S3.T3.4.2.2.1.m1.1.2"><csymbol cd="ambiguous" id="S3.T3.4.2.2.1.m1.1.2.1.cmml" xref="S3.T3.4.2.2.1.m1.1.2">superscript</csymbol><ci id="S3.T3.4.2.2.1.m1.1.2.2.cmml" xref="S3.T3.4.2.2.1.m1.1.2.2">ğ·</ci><ci id="S3.T3.4.2.2.1.m1.1.1.1.1.cmml" xref="S3.T3.4.2.2.1.m1.1.1.1.1">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.2.2.1.m1.1c">D^{(S)}</annotation></semantics></math>
</th>
<td id="S3.T3.4.2.2.2" class="ltx_td ltx_align_center"><span id="S3.T3.4.2.2.2.1" class="ltx_text ltx_font_bold">14.22</span></td>
<td id="S3.T3.4.2.2.3" class="ltx_td ltx_align_center"><span id="S3.T3.4.2.2.3.1" class="ltx_text ltx_font_bold">45.60</span></td>
<td id="S3.T3.4.2.2.4" class="ltx_td ltx_align_center"><span id="S3.T3.4.2.2.4.1" class="ltx_text ltx_font_bold">32.01</span></td>
<td id="S3.T3.4.2.2.5" class="ltx_td ltx_align_center"><span id="S3.T3.4.2.2.5.1" class="ltx_text ltx_font_bold">60.84</span></td>
</tr>
<tr id="S3.T3.5.3.7.4" class="ltx_tr">
<th id="S3.T3.5.3.7.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5">Multi-View Discrete Tokens</th>
</tr>
<tr id="S3.T3.5.3.3" class="ltx_tr">
<th id="S3.T3.5.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><math id="S3.T3.5.3.3.1.m1.1" class="ltx_Math" alttext="D^{(MV)}" display="inline"><semantics id="S3.T3.5.3.3.1.m1.1a"><msup id="S3.T3.5.3.3.1.m1.1.2" xref="S3.T3.5.3.3.1.m1.1.2.cmml"><mi id="S3.T3.5.3.3.1.m1.1.2.2" xref="S3.T3.5.3.3.1.m1.1.2.2.cmml">D</mi><mrow id="S3.T3.5.3.3.1.m1.1.1.1.1" xref="S3.T3.5.3.3.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.T3.5.3.3.1.m1.1.1.1.1.2" xref="S3.T3.5.3.3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.T3.5.3.3.1.m1.1.1.1.1.1" xref="S3.T3.5.3.3.1.m1.1.1.1.1.1.cmml"><mi id="S3.T3.5.3.3.1.m1.1.1.1.1.1.2" xref="S3.T3.5.3.3.1.m1.1.1.1.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.T3.5.3.3.1.m1.1.1.1.1.1.1" xref="S3.T3.5.3.3.1.m1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S3.T3.5.3.3.1.m1.1.1.1.1.1.3" xref="S3.T3.5.3.3.1.m1.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S3.T3.5.3.3.1.m1.1.1.1.1.3" xref="S3.T3.5.3.3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T3.5.3.3.1.m1.1b"><apply id="S3.T3.5.3.3.1.m1.1.2.cmml" xref="S3.T3.5.3.3.1.m1.1.2"><csymbol cd="ambiguous" id="S3.T3.5.3.3.1.m1.1.2.1.cmml" xref="S3.T3.5.3.3.1.m1.1.2">superscript</csymbol><ci id="S3.T3.5.3.3.1.m1.1.2.2.cmml" xref="S3.T3.5.3.3.1.m1.1.2.2">ğ·</ci><apply id="S3.T3.5.3.3.1.m1.1.1.1.1.1.cmml" xref="S3.T3.5.3.3.1.m1.1.1.1.1"><times id="S3.T3.5.3.3.1.m1.1.1.1.1.1.1.cmml" xref="S3.T3.5.3.3.1.m1.1.1.1.1.1.1"></times><ci id="S3.T3.5.3.3.1.m1.1.1.1.1.1.2.cmml" xref="S3.T3.5.3.3.1.m1.1.1.1.1.1.2">ğ‘€</ci><ci id="S3.T3.5.3.3.1.m1.1.1.1.1.1.3.cmml" xref="S3.T3.5.3.3.1.m1.1.1.1.1.1.3">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.3.3.1.m1.1c">D^{(MV)}</annotation></semantics></math></th>
<td id="S3.T3.5.3.3.2" class="ltx_td ltx_align_center ltx_border_t">15.37</td>
<td id="S3.T3.5.3.3.3" class="ltx_td ltx_align_center ltx_border_t">46.60</td>
<td id="S3.T3.5.3.3.4" class="ltx_td ltx_align_center ltx_border_t">38.20</td>
<td id="S3.T3.5.3.3.5" class="ltx_td ltx_align_center ltx_border_t">63.35</td>
</tr>
<tr id="S3.T3.5.3.8.5" class="ltx_tr">
<th id="S3.T3.5.3.8.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5">Topline</th>
</tr>
<tr id="S3.T3.5.3.9.6" class="ltx_tr">
<th id="S3.T3.5.3.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Whisper-M (0)</th>
<td id="S3.T3.5.3.9.6.2" class="ltx_td ltx_align_center ltx_border_t">12.9</td>
<td id="S3.T3.5.3.9.6.3" class="ltx_td ltx_align_center ltx_border_t">32.1</td>
<td id="S3.T3.5.3.9.6.4" class="ltx_td ltx_align_center ltx_border_t">30.38</td>
<td id="S3.T3.5.3.9.6.5" class="ltx_td ltx_align_center ltx_border_t">50.59</td>
</tr>
<tr id="S3.T3.5.3.10.7" class="ltx_tr">
<th id="S3.T3.5.3.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Whisper-M (All)</th>
<td id="S3.T3.5.3.10.7.2" class="ltx_td ltx_align_center ltx_border_bb">8.91</td>
<td id="S3.T3.5.3.10.7.3" class="ltx_td ltx_align_center ltx_border_bb">47.64</td>
<td id="S3.T3.5.3.10.7.4" class="ltx_td ltx_align_center ltx_border_bb">37.71</td>
<td id="S3.T3.5.3.10.7.5" class="ltx_td ltx_align_center ltx_border_bb">49.57</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We reported the Word Error Rate (WER) for all the ASRs. The WER results are computed on normalized text, utilizing the <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">BasicTextNormalizer</span> from Whisper <span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/openai/whisper/blob/main/whisper/normalizers/basic.py" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/openai/whisper/blob/main/whisper/normalizers/basic.py</a></span></span></span>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Traditional <span id="S4.SS1.1.1" class="ltx_text ltx_font_italic">vs</span> Discrete Input</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.8" class="ltx_p">Table <a href="#S3.T2" title="Table 2 â€£ 3.3.2 ASR Model training â€£ 3.3 Model Training â€£ 3 Experimental Settings â€£ Childrenâ€™s Speech Recognition through Discrete Token Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reports WER for discrete token ASRs and compares it with the baselines and variations of Whisper - small and medium models in zero-shot, few (55 hours) shots, and fully fine-tuned settings. From the reported WER, we observed that discrete tokens perform comparably to the <span id="S4.SS1.p1.8.1" class="ltx_text ltx_font_italic">HuBERT</span> and <span id="S4.SS1.p1.8.2" class="ltx_text ltx_font_italic">WavLM</span> end-to-end model, with a small performance drop of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\Delta(WER)=0.67" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mrow id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml"><mi mathvariant="normal" id="S4.SS1.p1.1.m1.1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.1.3.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.SS1.p1.1.m1.1.1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p1.1.m1.1.1.1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p1.1.m1.1.1.1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.1.1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.1.1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p1.1.m1.1.1.1.1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1.1.1.1.1.1a" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p1.1.m1.1.1.1.1.1.1.4" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.4.cmml">R</mi></mrow><mo stretchy="false" id="S4.SS1.p1.1.m1.1.1.1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">=</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">0.67</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><eq id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2"></eq><apply id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.1.2"></times><ci id="S4.SS1.p1.1.m1.1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.1.3">Î”</ci><apply id="S4.SS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.1"></times><ci id="S4.SS1.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.2">ğ‘Š</ci><ci id="S4.SS1.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.3">ğ¸</ci><ci id="S4.SS1.p1.1.m1.1.1.1.1.1.1.4.cmml" xref="S4.SS1.p1.1.m1.1.1.1.1.1.1.4">ğ‘…</ci></apply></apply><cn type="float" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">0.67</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\Delta(WER)=0.67</annotation></semantics></math> and <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\Delta(WER)=0.95" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mrow id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml"><mi mathvariant="normal" id="S4.SS1.p1.2.m2.1.1.1.3" xref="S4.SS1.p1.2.m2.1.1.1.3.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.2.m2.1.1.1.2" xref="S4.SS1.p1.2.m2.1.1.1.2.cmml">â€‹</mo><mrow id="S4.SS1.p1.2.m2.1.1.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p1.2.m2.1.1.1.1.1.2" xref="S4.SS1.p1.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p1.2.m2.1.1.1.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.1.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.1.1.1.1.2" xref="S4.SS1.p1.2.m2.1.1.1.1.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.2.m2.1.1.1.1.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p1.2.m2.1.1.1.1.1.1.3" xref="S4.SS1.p1.2.m2.1.1.1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.2.m2.1.1.1.1.1.1.1a" xref="S4.SS1.p1.2.m2.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p1.2.m2.1.1.1.1.1.1.4" xref="S4.SS1.p1.2.m2.1.1.1.1.1.1.4.cmml">R</mi></mrow><mo stretchy="false" id="S4.SS1.p1.2.m2.1.1.1.1.1.3" xref="S4.SS1.p1.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">=</mo><mn id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><eq id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2"></eq><apply id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1"><times id="S4.SS1.p1.2.m2.1.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.1.2"></times><ci id="S4.SS1.p1.2.m2.1.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.1.3">Î”</ci><apply id="S4.SS1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1.1.1"><times id="S4.SS1.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1.1.1.1.1"></times><ci id="S4.SS1.p1.2.m2.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.1.1.1.1.2">ğ‘Š</ci><ci id="S4.SS1.p1.2.m2.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.1.1.1.1.3">ğ¸</ci><ci id="S4.SS1.p1.2.m2.1.1.1.1.1.1.4.cmml" xref="S4.SS1.p1.2.m2.1.1.1.1.1.1.4">ğ‘…</ci></apply></apply><cn type="float" id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\Delta(WER)=0.95</annotation></semantics></math> respectively.
When compared with Whisper model variants (both zero- and few-shots), we noticed a maximum drop of <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\Delta(WER)=1.32" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mrow id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mrow id="S4.SS1.p1.3.m3.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.cmml"><mi mathvariant="normal" id="S4.SS1.p1.3.m3.1.1.1.3" xref="S4.SS1.p1.3.m3.1.1.1.3.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.1.1.1.2" xref="S4.SS1.p1.3.m3.1.1.1.2.cmml">â€‹</mo><mrow id="S4.SS1.p1.3.m3.1.1.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p1.3.m3.1.1.1.1.1.2" xref="S4.SS1.p1.3.m3.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p1.3.m3.1.1.1.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.1.1.1.cmml"><mi id="S4.SS1.p1.3.m3.1.1.1.1.1.1.2" xref="S4.SS1.p1.3.m3.1.1.1.1.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.1.1.1.1.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p1.3.m3.1.1.1.1.1.1.3" xref="S4.SS1.p1.3.m3.1.1.1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.1.1.1.1.1.1.1a" xref="S4.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p1.3.m3.1.1.1.1.1.1.4" xref="S4.SS1.p1.3.m3.1.1.1.1.1.1.4.cmml">R</mi></mrow><mo stretchy="false" id="S4.SS1.p1.3.m3.1.1.1.1.1.3" xref="S4.SS1.p1.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">=</mo><mn id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml">1.32</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><eq id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2"></eq><apply id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1"><times id="S4.SS1.p1.3.m3.1.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.1.2"></times><ci id="S4.SS1.p1.3.m3.1.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.1.3">Î”</ci><apply id="S4.SS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1.1.1"><times id="S4.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1.1.1.1.1"></times><ci id="S4.SS1.p1.3.m3.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.1.1.1.1.2">ğ‘Š</ci><ci id="S4.SS1.p1.3.m3.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.1.1.1.1.3">ğ¸</ci><ci id="S4.SS1.p1.3.m3.1.1.1.1.1.1.4.cmml" xref="S4.SS1.p1.3.m3.1.1.1.1.1.1.4">ğ‘…</ci></apply></apply><cn type="float" id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3">1.32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">\Delta(WER)=1.32</annotation></semantics></math>. While with full <span id="S4.SS1.p1.8.3" class="ltx_text ltx_font_italic">MyST</span> training data, the drop goes to <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="\Delta(WER)=5.31" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mrow id="S4.SS1.p1.4.m4.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.cmml"><mi mathvariant="normal" id="S4.SS1.p1.4.m4.1.1.1.3" xref="S4.SS1.p1.4.m4.1.1.1.3.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.1.1.1.2" xref="S4.SS1.p1.4.m4.1.1.1.2.cmml">â€‹</mo><mrow id="S4.SS1.p1.4.m4.1.1.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p1.4.m4.1.1.1.1.1.2" xref="S4.SS1.p1.4.m4.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p1.4.m4.1.1.1.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S4.SS1.p1.4.m4.1.1.1.1.1.1.2" xref="S4.SS1.p1.4.m4.1.1.1.1.1.1.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.1.1.1.1.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p1.4.m4.1.1.1.1.1.1.3" xref="S4.SS1.p1.4.m4.1.1.1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.1.1.1.1.1.1.1a" xref="S4.SS1.p1.4.m4.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p1.4.m4.1.1.1.1.1.1.4" xref="S4.SS1.p1.4.m4.1.1.1.1.1.1.4.cmml">R</mi></mrow><mo stretchy="false" id="S4.SS1.p1.4.m4.1.1.1.1.1.3" xref="S4.SS1.p1.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">=</mo><mn id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml">5.31</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><eq id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2"></eq><apply id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1"><times id="S4.SS1.p1.4.m4.1.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.1.2"></times><ci id="S4.SS1.p1.4.m4.1.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.1.3">Î”</ci><apply id="S4.SS1.p1.4.m4.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1.1.1"><times id="S4.SS1.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1.1.1.1.1"></times><ci id="S4.SS1.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.1.1.1.1.2">ğ‘Š</ci><ci id="S4.SS1.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.1.1.1.1.3">ğ¸</ci><ci id="S4.SS1.p1.4.m4.1.1.1.1.1.1.4.cmml" xref="S4.SS1.p1.4.m4.1.1.1.1.1.1.4">ğ‘…</ci></apply></apply><cn type="float" id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3">5.31</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">\Delta(WER)=5.31</annotation></semantics></math>. All the aforementioned reported <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="\Delta(*)" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mrow id="S4.SS1.p1.5.m5.1.2" xref="S4.SS1.p1.5.m5.1.2.cmml"><mi mathvariant="normal" id="S4.SS1.p1.5.m5.1.2.2" xref="S4.SS1.p1.5.m5.1.2.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.5.m5.1.2.1" xref="S4.SS1.p1.5.m5.1.2.1.cmml">â€‹</mo><mrow id="S4.SS1.p1.5.m5.1.2.3.2" xref="S4.SS1.p1.5.m5.1.2.cmml"><mo stretchy="false" id="S4.SS1.p1.5.m5.1.2.3.2.1" xref="S4.SS1.p1.5.m5.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">âˆ—</mo><mo stretchy="false" id="S4.SS1.p1.5.m5.1.2.3.2.2" xref="S4.SS1.p1.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><apply id="S4.SS1.p1.5.m5.1.2.cmml" xref="S4.SS1.p1.5.m5.1.2"><times id="S4.SS1.p1.5.m5.1.2.1.cmml" xref="S4.SS1.p1.5.m5.1.2.1"></times><ci id="S4.SS1.p1.5.m5.1.2.2.cmml" xref="S4.SS1.p1.5.m5.1.2.2">Î”</ci><times id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">\Delta(*)</annotation></semantics></math> is w.r.t <span id="S4.SS1.p1.8.4" class="ltx_text ltx_font_italic">WavLM</span>-<math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="D^{(S)}" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><msup id="S4.SS1.p1.6.m6.1.2" xref="S4.SS1.p1.6.m6.1.2.cmml"><mi id="S4.SS1.p1.6.m6.1.2.2" xref="S4.SS1.p1.6.m6.1.2.2.cmml">D</mi><mrow id="S4.SS1.p1.6.m6.1.1.1.3" xref="S4.SS1.p1.6.m6.1.2.cmml"><mo stretchy="false" id="S4.SS1.p1.6.m6.1.1.1.3.1" xref="S4.SS1.p1.6.m6.1.2.cmml">(</mo><mi id="S4.SS1.p1.6.m6.1.1.1.1" xref="S4.SS1.p1.6.m6.1.1.1.1.cmml">S</mi><mo stretchy="false" id="S4.SS1.p1.6.m6.1.1.1.3.2" xref="S4.SS1.p1.6.m6.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><apply id="S4.SS1.p1.6.m6.1.2.cmml" xref="S4.SS1.p1.6.m6.1.2"><csymbol cd="ambiguous" id="S4.SS1.p1.6.m6.1.2.1.cmml" xref="S4.SS1.p1.6.m6.1.2">superscript</csymbol><ci id="S4.SS1.p1.6.m6.1.2.2.cmml" xref="S4.SS1.p1.6.m6.1.2.2">ğ·</ci><ci id="S4.SS1.p1.6.m6.1.1.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1.1.1">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">D^{(S)}</annotation></semantics></math>. Considering the model sizes (Whisper Medium: 769M, Whisper Small: 242M, and Discrete Token ASR: 40.36M) and the extensive data utilized in Whisper's pre-training and subsequent fine-tuning, the performance of the Discrete Token ASR demonstrates nearly equivalent results while achieving an <math id="S4.SS1.p1.7.m7.1" class="ltx_Math" alttext="\approx 83\%" display="inline"><semantics id="S4.SS1.p1.7.m7.1a"><mrow id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml"><mi id="S4.SS1.p1.7.m7.1.1.2" xref="S4.SS1.p1.7.m7.1.1.2.cmml"></mi><mo id="S4.SS1.p1.7.m7.1.1.1" xref="S4.SS1.p1.7.m7.1.1.1.cmml">â‰ˆ</mo><mrow id="S4.SS1.p1.7.m7.1.1.3" xref="S4.SS1.p1.7.m7.1.1.3.cmml"><mn id="S4.SS1.p1.7.m7.1.1.3.2" xref="S4.SS1.p1.7.m7.1.1.3.2.cmml">83</mn><mo id="S4.SS1.p1.7.m7.1.1.3.1" xref="S4.SS1.p1.7.m7.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.1b"><apply id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1"><approx id="S4.SS1.p1.7.m7.1.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1.1"></approx><csymbol cd="latexml" id="S4.SS1.p1.7.m7.1.1.2.cmml" xref="S4.SS1.p1.7.m7.1.1.2">absent</csymbol><apply id="S4.SS1.p1.7.m7.1.1.3.cmml" xref="S4.SS1.p1.7.m7.1.1.3"><csymbol cd="latexml" id="S4.SS1.p1.7.m7.1.1.3.1.cmml" xref="S4.SS1.p1.7.m7.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS1.p1.7.m7.1.1.3.2.cmml" xref="S4.SS1.p1.7.m7.1.1.3.2">83</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.1c">\approx 83\%</annotation></semantics></math> reduction in model size compared to Whisper Small and a <math id="S4.SS1.p1.8.m8.1" class="ltx_Math" alttext="\approx 94\%" display="inline"><semantics id="S4.SS1.p1.8.m8.1a"><mrow id="S4.SS1.p1.8.m8.1.1" xref="S4.SS1.p1.8.m8.1.1.cmml"><mi id="S4.SS1.p1.8.m8.1.1.2" xref="S4.SS1.p1.8.m8.1.1.2.cmml"></mi><mo id="S4.SS1.p1.8.m8.1.1.1" xref="S4.SS1.p1.8.m8.1.1.1.cmml">â‰ˆ</mo><mrow id="S4.SS1.p1.8.m8.1.1.3" xref="S4.SS1.p1.8.m8.1.1.3.cmml"><mn id="S4.SS1.p1.8.m8.1.1.3.2" xref="S4.SS1.p1.8.m8.1.1.3.2.cmml">94</mn><mo id="S4.SS1.p1.8.m8.1.1.3.1" xref="S4.SS1.p1.8.m8.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.8.m8.1b"><apply id="S4.SS1.p1.8.m8.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1"><approx id="S4.SS1.p1.8.m8.1.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1.1"></approx><csymbol cd="latexml" id="S4.SS1.p1.8.m8.1.1.2.cmml" xref="S4.SS1.p1.8.m8.1.1.2">absent</csymbol><apply id="S4.SS1.p1.8.m8.1.1.3.cmml" xref="S4.SS1.p1.8.m8.1.1.3"><csymbol cd="latexml" id="S4.SS1.p1.8.m8.1.1.3.1.cmml" xref="S4.SS1.p1.8.m8.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS1.p1.8.m8.1.1.3.2.cmml" xref="S4.SS1.p1.8.m8.1.1.3.2">94</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.8.m8.1c">\approx 94\%</annotation></semantics></math> reduction compared to Whisper Medium.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.4" class="ltx_p">Moreover, discrete ASR efficiently reduces data sizes and input length as discussed above. For example, for a <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">T</annotation></semantics></math> second utterance, the raw input signal (of 16 kHz sampling rate and 16-bit signed integer form) will need <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="16\times 16000\times T" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mn id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.2.m2.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.cmml">Ã—</mo><mn id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml">16000</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.2.m2.1.1.1a" xref="S4.SS1.p2.2.m2.1.1.1.cmml">Ã—</mo><mi id="S4.SS1.p2.2.m2.1.1.4" xref="S4.SS1.p2.2.m2.1.1.4.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><times id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">16</cn><cn type="integer" id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3">16000</cn><ci id="S4.SS1.p2.2.m2.1.1.4.cmml" xref="S4.SS1.p2.2.m2.1.1.4">ğ‘‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">16\times 16000\times T</annotation></semantics></math> bits to encode; for SSL-based features with the rate of 50 frames per second, stored as float vectors and output embedding dimension of 1024 from one layer, we need <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="32X1024X50XT" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mn id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">32</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml">X</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.1.1.1a" xref="S4.SS1.p2.3.m3.1.1.1.cmml">â€‹</mo><mn id="S4.SS1.p2.3.m3.1.1.4" xref="S4.SS1.p2.3.m3.1.1.4.cmml">1024</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.1.1.1b" xref="S4.SS1.p2.3.m3.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p2.3.m3.1.1.5" xref="S4.SS1.p2.3.m3.1.1.5.cmml">X</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.1.1.1c" xref="S4.SS1.p2.3.m3.1.1.1.cmml">â€‹</mo><mn id="S4.SS1.p2.3.m3.1.1.6" xref="S4.SS1.p2.3.m3.1.1.6.cmml">50</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.1.1.1d" xref="S4.SS1.p2.3.m3.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p2.3.m3.1.1.7" xref="S4.SS1.p2.3.m3.1.1.7.cmml">X</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.1.1.1e" xref="S4.SS1.p2.3.m3.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p2.3.m3.1.1.8" xref="S4.SS1.p2.3.m3.1.1.8.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><times id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1"></times><cn type="integer" id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">32</cn><ci id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3">ğ‘‹</ci><cn type="integer" id="S4.SS1.p2.3.m3.1.1.4.cmml" xref="S4.SS1.p2.3.m3.1.1.4">1024</cn><ci id="S4.SS1.p2.3.m3.1.1.5.cmml" xref="S4.SS1.p2.3.m3.1.1.5">ğ‘‹</ci><cn type="integer" id="S4.SS1.p2.3.m3.1.1.6.cmml" xref="S4.SS1.p2.3.m3.1.1.6">50</cn><ci id="S4.SS1.p2.3.m3.1.1.7.cmml" xref="S4.SS1.p2.3.m3.1.1.7">ğ‘‹</ci><ci id="S4.SS1.p2.3.m3.1.1.8.cmml" xref="S4.SS1.p2.3.m3.1.1.8">ğ‘‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">32X1024X50XT</annotation></semantics></math> bits. For discrete labels, we only need <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="11X50XT" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mrow id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><mn id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml">11</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p2.4.m4.1.1.1" xref="S4.SS1.p2.4.m4.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p2.4.m4.1.1.3" xref="S4.SS1.p2.4.m4.1.1.3.cmml">X</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.4.m4.1.1.1a" xref="S4.SS1.p2.4.m4.1.1.1.cmml">â€‹</mo><mn id="S4.SS1.p2.4.m4.1.1.4" xref="S4.SS1.p2.4.m4.1.1.4.cmml">50</mn><mo lspace="0em" rspace="0em" id="S4.SS1.p2.4.m4.1.1.1b" xref="S4.SS1.p2.4.m4.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p2.4.m4.1.1.5" xref="S4.SS1.p2.4.m4.1.1.5.cmml">X</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.4.m4.1.1.1c" xref="S4.SS1.p2.4.m4.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p2.4.m4.1.1.6" xref="S4.SS1.p2.4.m4.1.1.6.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><times id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1.1"></times><cn type="integer" id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2">11</cn><ci id="S4.SS1.p2.4.m4.1.1.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3">ğ‘‹</ci><cn type="integer" id="S4.SS1.p2.4.m4.1.1.4.cmml" xref="S4.SS1.p2.4.m4.1.1.4">50</cn><ci id="S4.SS1.p2.4.m4.1.1.5.cmml" xref="S4.SS1.p2.4.m4.1.1.5">ğ‘‹</ci><ci id="S4.SS1.p2.4.m4.1.1.6.cmml" xref="S4.SS1.p2.4.m4.1.1.6">ğ‘‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">11X50XT</annotation></semantics></math> bits for a maximum of 2048 clusters (11-bit) without even considering further improvement with de-duplication of sequence and subword modeling.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Single-view <span id="S4.SS2.1.1" class="ltx_text ltx_font_italic">vs</span> Multi-view</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For the study, we exploit two simple ways to convert continuous speech features into discrete units. Using single-view and multi-view strategies, we reported the results on MyST in Table <a href="#S3.T3" title="Table 3 â€£ 3.3.2 ASR Model training â€£ 3.3 Model Training â€£ 3 Experimental Settings â€£ Childrenâ€™s Speech Recognition through Discrete Token Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We observed that in a single-view setup, the <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">WavLM</span> discrete tokens outperform the HuBERT discrete tokens by 1.43 WER.
We hypothesize that WavLM model embeddings are more robust due to its added utterance-mixing strategy, addressing the variability in child speech more efficiently.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.3" class="ltx_p">For multi-view setup, the performance of the <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="D^{(MV)}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><msup id="S4.SS2.p2.1.m1.1.2" xref="S4.SS2.p2.1.m1.1.2.cmml"><mi id="S4.SS2.p2.1.m1.1.2.2" xref="S4.SS2.p2.1.m1.1.2.2.cmml">D</mi><mrow id="S4.SS2.p2.1.m1.1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p2.1.m1.1.1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p2.1.m1.1.1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.1.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.SS2.p2.1.m1.1.1.1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S4.SS2.p2.1.m1.1.1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.2.1.cmml" xref="S4.SS2.p2.1.m1.1.2">superscript</csymbol><ci id="S4.SS2.p2.1.m1.1.2.2.cmml" xref="S4.SS2.p2.1.m1.1.2.2">ğ·</ci><apply id="S4.SS2.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1"><times id="S4.SS2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1.1.1"></times><ci id="S4.SS2.p2.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1.1.2">ğ‘€</ci><ci id="S4.SS2.p2.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.1.1.1.3">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">D^{(MV)}</annotation></semantics></math> is superior to the HuBERT-<math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="D^{(S)}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><msup id="S4.SS2.p2.2.m2.1.2" xref="S4.SS2.p2.2.m2.1.2.cmml"><mi id="S4.SS2.p2.2.m2.1.2.2" xref="S4.SS2.p2.2.m2.1.2.2.cmml">D</mi><mrow id="S4.SS2.p2.2.m2.1.1.1.3" xref="S4.SS2.p2.2.m2.1.2.cmml"><mo stretchy="false" id="S4.SS2.p2.2.m2.1.1.1.3.1" xref="S4.SS2.p2.2.m2.1.2.cmml">(</mo><mi id="S4.SS2.p2.2.m2.1.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.1.cmml">S</mi><mo stretchy="false" id="S4.SS2.p2.2.m2.1.1.1.3.2" xref="S4.SS2.p2.2.m2.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.2.cmml" xref="S4.SS2.p2.2.m2.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.2.1.cmml" xref="S4.SS2.p2.2.m2.1.2">superscript</csymbol><ci id="S4.SS2.p2.2.m2.1.2.2.cmml" xref="S4.SS2.p2.2.m2.1.2.2">ğ·</ci><ci id="S4.SS2.p2.2.m2.1.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1.1">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">D^{(S)}</annotation></semantics></math> model. However, WavLM-<math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="D^{(S)}" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><msup id="S4.SS2.p2.3.m3.1.2" xref="S4.SS2.p2.3.m3.1.2.cmml"><mi id="S4.SS2.p2.3.m3.1.2.2" xref="S4.SS2.p2.3.m3.1.2.2.cmml">D</mi><mrow id="S4.SS2.p2.3.m3.1.1.1.3" xref="S4.SS2.p2.3.m3.1.2.cmml"><mo stretchy="false" id="S4.SS2.p2.3.m3.1.1.1.3.1" xref="S4.SS2.p2.3.m3.1.2.cmml">(</mo><mi id="S4.SS2.p2.3.m3.1.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.1.cmml">S</mi><mo stretchy="false" id="S4.SS2.p2.3.m3.1.1.1.3.2" xref="S4.SS2.p2.3.m3.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.2.cmml" xref="S4.SS2.p2.3.m3.1.2"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.2.1.cmml" xref="S4.SS2.p2.3.m3.1.2">superscript</csymbol><ci id="S4.SS2.p2.3.m3.1.2.2.cmml" xref="S4.SS2.p2.3.m3.1.2.2">ğ·</ci><ci id="S4.SS2.p2.3.m3.1.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1.1">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">D^{(S)}</annotation></semantics></math> still outperforms both the variations. This potentially indicates that the selection of robust SSL models is essential to harness the power of multi-view discrete tokens. We keep this as a future exploration.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Generalization Capabilities</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To test the generalization capabilities of these discrete token ASRs, we evaluated two unseen test sets and reported WER with single-, multi-view discrete ASRs along with the Whisper medium models in zero-shot and full (fine-tuned with full training data as the discrete models) settings in TableÂ <a href="#S3.T3" title="Table 3 â€£ 3.3.2 ASR Model training â€£ 3.3 Model Training â€£ 3 Experimental Settings â€£ Childrenâ€™s Speech Recognition through Discrete Token Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
We observed similar performance patterns across the datasets â€“ with different age groups (CMU kids data), nativity (non-native data), and speaking style (read- and spontaneous corpus). Similar to our previous observation, WavLM-<math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="D^{(S)}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><msup id="S4.SS3.p1.1.m1.1.2" xref="S4.SS3.p1.1.m1.1.2.cmml"><mi id="S4.SS3.p1.1.m1.1.2.2" xref="S4.SS3.p1.1.m1.1.2.2.cmml">D</mi><mrow id="S4.SS3.p1.1.m1.1.1.1.3" xref="S4.SS3.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S4.SS3.p1.1.m1.1.1.1.3.1" xref="S4.SS3.p1.1.m1.1.2.cmml">(</mo><mi id="S4.SS3.p1.1.m1.1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.1.cmml">S</mi><mo stretchy="false" id="S4.SS3.p1.1.m1.1.1.1.3.2" xref="S4.SS3.p1.1.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.2.1.cmml" xref="S4.SS3.p1.1.m1.1.2">superscript</csymbol><ci id="S4.SS3.p1.1.m1.1.2.2.cmml" xref="S4.SS3.p1.1.m1.1.2.2">ğ·</ci><ci id="S4.SS3.p1.1.m1.1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1.1">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">D^{(S)}</annotation></semantics></math> outperforms all other discrete ASR systems and also gives comparable results to zero-shot Whisper models.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Example of Discrete ASR outputs</figcaption>
<div id="S4.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:244.9pt;height:85.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.6pt,4.8pt) scale(0.9,0.9) ;">
<table id="S4.T4.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.2.3.1" class="ltx_tr">
<th id="S4.T4.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T4.2.2.3.1.1.1" class="ltx_text ltx_font_bold">Ref:</span> A butterfly starts as an egg</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.2.4.1" class="ltx_tr">
<td id="S4.T4.2.2.4.1.1" class="ltx_td ltx_align_left ltx_border_t">
<table id="S4.T4.2.2.4.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.2.2.4.1.1.1.1" class="ltx_tr">
<td id="S4.T4.2.2.4.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">
<span id="S4.T4.2.2.4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Verbatim: </span>[noise] a butterfly starts <span id="S4.T4.2.2.4.1.1.1.1.1.2" class="ltx_text" style="color:#0000FF;">/EH/</span> [human_noise]</td>
</tr>
<tr id="S4.T4.2.2.4.1.1.1.2" class="ltx_tr">
<td id="S4.T4.2.2.4.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">an egg [human_noise] [noise]</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T4.1.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">WavLM-<math id="S4.T4.1.1.1.1.1.m1.1" class="ltx_Math" alttext="D^{(S)}" display="inline"><semantics id="S4.T4.1.1.1.1.1.m1.1a"><msup id="S4.T4.1.1.1.1.1.m1.1.2" xref="S4.T4.1.1.1.1.1.m1.1.2.cmml"><mi id="S4.T4.1.1.1.1.1.m1.1.2.2" xref="S4.T4.1.1.1.1.1.m1.1.2.2.cmml">D</mi><mrow id="S4.T4.1.1.1.1.1.m1.1.1.1.3" xref="S4.T4.1.1.1.1.1.m1.1.2.cmml"><mo stretchy="false" id="S4.T4.1.1.1.1.1.m1.1.1.1.3.1" xref="S4.T4.1.1.1.1.1.m1.1.2.cmml">(</mo><mi id="S4.T4.1.1.1.1.1.m1.1.1.1.1" xref="S4.T4.1.1.1.1.1.m1.1.1.1.1.cmml">S</mi><mo stretchy="false" id="S4.T4.1.1.1.1.1.m1.1.1.1.3.2" xref="S4.T4.1.1.1.1.1.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.1.1.m1.1.2.cmml" xref="S4.T4.1.1.1.1.1.m1.1.2"><csymbol cd="ambiguous" id="S4.T4.1.1.1.1.1.m1.1.2.1.cmml" xref="S4.T4.1.1.1.1.1.m1.1.2">superscript</csymbol><ci id="S4.T4.1.1.1.1.1.m1.1.2.2.cmml" xref="S4.T4.1.1.1.1.1.m1.1.2.2">ğ·</ci><ci id="S4.T4.1.1.1.1.1.m1.1.1.1.1.cmml" xref="S4.T4.1.1.1.1.1.m1.1.1.1.1">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.1.m1.1c">D^{(S)}</annotation></semantics></math>:</span> a butterfly starts <span id="S4.T4.1.1.1.1.2" class="ltx_text" style="color:#FF0000;">I</span> as an <span id="S4.T4.1.1.1.1.3" class="ltx_text" style="color:#FF0000;">X</span>
</td>
</tr>
<tr id="S4.T4.2.2.2" class="ltx_tr">
<td id="S4.T4.2.2.2.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<math id="S4.T4.2.2.2.1.m1.1" class="ltx_Math" alttext="D^{(MV)}" display="inline"><semantics id="S4.T4.2.2.2.1.m1.1a"><msup id="S4.T4.2.2.2.1.m1.1.2" xref="S4.T4.2.2.2.1.m1.1.2.cmml"><mi id="S4.T4.2.2.2.1.m1.1.2.2" xref="S4.T4.2.2.2.1.m1.1.2.2.cmml">D</mi><mrow id="S4.T4.2.2.2.1.m1.1.1.1.1" xref="S4.T4.2.2.2.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.T4.2.2.2.1.m1.1.1.1.1.2" xref="S4.T4.2.2.2.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S4.T4.2.2.2.1.m1.1.1.1.1.1" xref="S4.T4.2.2.2.1.m1.1.1.1.1.1.cmml"><mi id="S4.T4.2.2.2.1.m1.1.1.1.1.1.2" xref="S4.T4.2.2.2.1.m1.1.1.1.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.T4.2.2.2.1.m1.1.1.1.1.1.1" xref="S4.T4.2.2.2.1.m1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.T4.2.2.2.1.m1.1.1.1.1.1.3" xref="S4.T4.2.2.2.1.m1.1.1.1.1.1.3.cmml">V</mi></mrow><mo stretchy="false" id="S4.T4.2.2.2.1.m1.1.1.1.1.3" xref="S4.T4.2.2.2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.m1.1b"><apply id="S4.T4.2.2.2.1.m1.1.2.cmml" xref="S4.T4.2.2.2.1.m1.1.2"><csymbol cd="ambiguous" id="S4.T4.2.2.2.1.m1.1.2.1.cmml" xref="S4.T4.2.2.2.1.m1.1.2">superscript</csymbol><ci id="S4.T4.2.2.2.1.m1.1.2.2.cmml" xref="S4.T4.2.2.2.1.m1.1.2.2">ğ·</ci><apply id="S4.T4.2.2.2.1.m1.1.1.1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1.1.1"><times id="S4.T4.2.2.2.1.m1.1.1.1.1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1.1.1.1.1"></times><ci id="S4.T4.2.2.2.1.m1.1.1.1.1.1.2.cmml" xref="S4.T4.2.2.2.1.m1.1.1.1.1.1.2">ğ‘€</ci><ci id="S4.T4.2.2.2.1.m1.1.1.1.1.1.3.cmml" xref="S4.T4.2.2.2.1.m1.1.1.1.1.1.3">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.m1.1c">D^{(MV)}</annotation></semantics></math><span id="S4.T4.2.2.2.1.1" class="ltx_text ltx_font_bold">:</span> a butterfly starts <span id="S4.T4.2.2.2.1.2" class="ltx_text" style="color:#FF0000;">E</span> as an egg</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Error Analysis</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">For the study, we briefly studied the effect of added noises on the model performance. Our initial exploration suggests that, with the different errors present in all the discrete ASRs, the multi-view discrete ASR is closer to the verbatim form of the transcription.
For example, as shown in Table <a href="#S4.T4" title="Table 4 â€£ 4.3 Generalization Capabilities â€£ 4 Results â€£ Childrenâ€™s Speech Recognition through Discrete Token Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the multi-view ASR can recognize the word ``egg'' correctly, even though in spoken form the word is followed by significant human noises. Moreover, the inserted char ``E'' is closer to the phonemic <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="EH" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mi id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.SS4.p1.1.m1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><times id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1"></times><ci id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">ğ¸</ci><ci id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">EH</annotation></semantics></math> sound that was actually in the speech. Such fine-grained prediction could help to detect mispronunciation and disfluencies present in the data more effectively.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This study presents the first benchmark for children's speech recognition with discrete tokens as input. From our exploration of discrete children ASR, we observed a comparable ASR performance with a significant reduction in model size and computational costs. Moreover, the discrete ASR provides additional data privacy required when dealing with sensitive speech data like children's speech. Our findings reflect the potential for multi-view discrete ASR, exploiting ensemble information encoded in separate SSL models. Further future research will involve studying how to enhance these discrete tokens with views extracted from different SSL models with different ASR architectures.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K.Â Evanini and X.Â Wang, ``Automated speech scoring for non-native middle school students with multiple task types,'' in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the INTERSPEECH</em>, 2013.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J.Â Mostow, ``Why and how our automated reading tutor listens,'' in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Symposium on Automatic Detection of Errors in Pronunciation Training (ISADEPT)</em>, 2012.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
F.Â Claus, H.Â GamboaÂ Rosales, R.Â Petrick, H.-U. Hain, and R.Â Hoffmann, ``A survey about databases of children's speech,'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2013.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J.Â Wang, Y.Â Zhu, R.Â Fan, W.Â Chu, and A.Â Alwan, ``Low resource german asr with untranscribed data spoken by non-native children- interspeech 2021 shared task spapl system,'' in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S.Â Feng, B.Â M. Halpern, O.Â Kudina, and O.Â Scharenborg, ``Towards inclusive automatic speech recognition,'' <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em>, vol.Â 84, 2024.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S.Â Lee, A.Â Potamianos, and S.Â Narayanan, ``Acoustics of children's speech: Developmental changes of temporal and spectral parameters,'' <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">J. Acoustical Soc. Amer.</em>, 1999.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
B.Â L. Smith, ``Relationships between duration and temporal variability in children's speech,'' <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, 1992.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
L.Â L. Koenig, J.Â C. Lucero, and E.Â Perlman, ``Speech production variability in fricatives of children and adults: Results of functional data analysis,'' <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, 2008.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
L.Â L. Koenig and J.Â C. Lucero, ``Stop consonant voicing and intraoral pressure contours in women and children,'' <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, 2008.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S.Â Lee, A.Â Potamianos, and S.Â Narayanan, ``Acoustics of children's speech: Developmental changes of temporal and spectral parameters,'' <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, 1999.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
â€”â€”, ``Analysis of children's speech: Duration, pitch and formants,'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Fifth European Conference on Speech Communication and Technology</em>, 1997.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H.Â K. Vorperian and R.Â D. Kent, ``Vowel acoustic space development in children: A synthesis of acoustic and anatomic data,'' 2007.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J.Â S. Yaruss, R.Â M. Newman, and T.Â Flora, ``Language and disfluency in nonstuttering children's conversational speech,'' <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">J. Fluency Disord.</em>, 1999.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T.Â Tran, M.Â Tinkler, G.Â Yeung, A.Â Alwan, and M.Â Ostendorf, ``Analysis of disfluency in children's speech,'' in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A.Â Baevski, Y.Â Zhou, A.Â Mohamed, and M.Â Auli, ``wav2vec 2.0: A framework for self-supervised learning of speech representations,'' <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S.Â Chen, C.Â Wang, Z.Â Chen, Y.Â Wu, S.Â Liu, Z.Â Chen, J.Â Li, N.Â Kanda, T.Â Yoshioka, X.Â Xiao <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Wavlm: Large-scale self-supervised pre-training for full stack speech processing,'' <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A.Â Radford, J.Â W. Kim, T.Â Xu, G.Â Brockman, C.Â McLeavey, and I.Â Sutskever, ``Robust speech recognition via large-scale weak supervision,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.Â Â Â PMLR, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
R.Â Fan and A.Â Alwan, ``Draft: A novel framework to reduce domain shifting in self-supervised learning and its application to children's asr,'' in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2022</em>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
R.Â Fan, Y.Â Zhu, J.Â Wang, and A.Â Alwan, ``Towards better domain adaptation for self-supervised models: A case study of child asr,'' <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol.Â 16, no.Â 6, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
R.Â Jain, A.Â Barcovschi, M.Â Y. Yiwere, D.Â Bigioi, P.Â Corcoran, and H.Â Cucu, ``A wav2vec2-based experimental study on self-supervised learning methods to improve child speech recognition,'' <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
R.Â Lahiri, T.Â Feng, R.Â Hebbar, C.Â Lord, S.Â H. Kim, and S.Â Narayanan, ``Robust self supervised speech embeddings for child-adult classification in interactions involving children with autism,'' in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2023</em>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A.Â A. Attia, J.Â Liu, W.Â Ai, D.Â Demszky, and C.Â Espy-Wilson, ``Kid-whisper: Towards bridging the performance gap in automatic speech recognition for children vs. adults,'' <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.07927</em>, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
V.Â M. Shetty, S.Â M. Lulich, and A.Â Alwan, ``Developmental articulatory and acoustic features for six to ten year old children,'' in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2023</em>, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M.Â Lavechin, Y.Â Sy, H.Â Titeux, M.Â A.Â C. BlandÃ³n, O.Â RÃ¤sÃ¤nen, H.Â Bredin, E.Â Dupoux, and A.Â Cristia, ``Babyslm: language-acquisition-friendly benchmark of self-supervised spoken language models,'' in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2023</em>, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
G.Â Yeung and A.Â Alwan, ``On the difficulties of automatic speech recognition for kindergarten-aged children,'' in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Interspeech 2018</em>, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
X.Â Chang, B.Â Yan, Y.Â Fujita, T.Â Maekaku, and S.Â Watanabe, ``Exploration of efficient end-to-end asr using discretized input from self-supervised learning,'' <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18108</em>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
X.Â Chang, B.Â Yan, K.Â Choi, J.Â Jung, Y.Â Lu, S.Â Maiti, R.Â Sharma, J.Â Shi, J.Â Tian, S.Â Watanabe <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Exploring speech recognition, translation, and understanding with discrete speech units: A comparative study,'' <em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.15800</em>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y.Â E. Kheir, H.Â Mubarak, A.Â Ali, and S.Â A. Chowdhury, ``Beyond orthography: Automatic recovery of short vowels and dialectal sounds in arabic,'' in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)</em>, 2024.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A.Â Radford, J.Â W. Kim, T.Â Xu, G.Â Brockman, C.Â McLeavey, and I.Â Sutskever, ``Robust speech recognition via large-scale weak supervision,'' 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J.Â Makhoul, S.Â Roucos, and H.Â Gish, ``Vector quantization in speech coding,'' <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, 1985.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S.Â Bickel and T.Â Scheffer, ``Multi-view clustering,'' in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Fourth IEEE International Conference on Data Mining (ICDM'04)</em>, 2004.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
T.Â Kudo, ``Subword regularization: Improving neural network translation models with multiple subword candidates,'' in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
K.Â Kim, F.Â Wu, Y.Â Peng, J.Â Pan, P.Â Sridhar, K.Â J. Han, and S.Â Watanabe, ``E-branchformer: Branchformer with enhanced merging for speech recognition,'' 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
A.Â Vaswani, N.Â Shazeer, N.Â Parmar, J.Â Uszkoreit, L.Â Jones, A.Â N. Gomez, L.Â Kaiser, and I.Â Polosukhin, ``Attention is all you need,'' 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Y.Â Peng, S.Â Dalmia, I.Â Lane, and S.Â Watanabe, ``Branchformer: Parallel mlp-attention architectures to capture local and global context for speech recognition and understanding,'' 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
S.Â S. Pradhan, R.Â A. Cole, and W.Â H. Ward, ``My science tutor (myst) â€“ a large corpus of children's conversational speech,'' 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
K.Â Radha and M.Â Bansal, ``Audio augmentation for non-native childrenâ€™s speech recognition through discriminative learning,'' <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Entropy</em>, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
S.Â Watanabe, T.Â Hori, S.Â Karita, T.Â Hayashi, J.Â Nishitoba, Y.Â Unno, N.Â Enrique Yalta Soplin, J.Â Heymann, M.Â Wiesner, N.Â Chen, A.Â Renduchintala, and T.Â Ochiai, ``ESPnet: End-to-end speech processing toolkit,'' in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of Interspeech</em>, 2018.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.13430" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.13431" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.13431">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.13431" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.13432" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Jul  6 00:15:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
