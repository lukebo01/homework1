<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.17385] ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition</title><meta property="og:description" content="In this work, we revisit the problem of semi-supervised named entity recognition (NER) focusing on extremely light supervision, consisting of a lexicon containing only 10 examples per class.
We introduce ELLEN, a simpl…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.17385">

<!--Generated on Fri Apr  5 17:27:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">



<span id="id2.id1" class="ltx_text ltx_font_bold">
ELLEN:
E<span id="id2.id1.1" class="ltx_text ltx_font_italic">xtremely</span>
L<span id="id2.id1.2" class="ltx_text ltx_font_italic">ightly</span>
<span id="id2.id1.3" class="ltx_text ltx_font_italic">Supervised</span>
L<span id="id2.id1.4" class="ltx_text ltx_font_italic">earning</span>
<span id="id2.id1.5" class="ltx_text ltx_font_italic">For</span>
E<span id="id2.id1.6" class="ltx_text ltx_font_italic">fficient</span>
N<span id="id2.id1.7" class="ltx_text ltx_font_italic">amed</span>
<span id="id2.id1.8" class="ltx_text ltx_font_italic">Entity</span>
<span id="id2.id1.9" class="ltx_text ltx_font_italic">Recognition</span>
</span>

</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">In this work, we revisit the problem of semi-supervised named entity recognition (NER) focusing on extremely light supervision, consisting of a lexicon containing only 10 examples per class.
We introduce ELLEN, a simple, fully modular, neuro-symbolic method that blends fine-tuned language models with linguistic rules. These rules include insights such as “One Sense Per Discourse”, using a Masked Language Model as an unsupervised NER, leveraging part-of-speech tags to identify and eliminate unlabeled entities as false negatives, and other intuitions about classifier confidence scores in local and global context. ELLEN achieves very strong performance on the CoNLL-2003 dataset when using the minimal supervision from the lexicon above. It also outperforms most existing (and considerably more complex) semi-supervised NER methods under the same supervision settings commonly used in the literature (i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a zero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and achieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also achieves over 75% of the performance of a strong, fully supervised model trained on gold data. Our code is publicly <a target="_blank" href="https://github.com/hriaz17/ELLEN" title="" class="ltx_ref ltx_href">available</a>.

<br class="ltx_break">
<br class="ltx_break">
<span id="id3.id1.1" class="ltx_text ltx_font_bold">Keywords: </span>semi-supervised learning, named entity recognition, neuro-symbolic, rules, language models, modular architectures</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\NAT@set@cites</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center"><span id="id1.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">ELLEN:
E<span id="id1.p1.1.1.1" class="ltx_text ltx_font_italic">xtremely</span>
L<span id="id1.p1.1.1.2" class="ltx_text ltx_font_italic">ightly</span>
<span id="id1.p1.1.1.3" class="ltx_text ltx_font_italic">Supervised</span>
L<span id="id1.p1.1.1.4" class="ltx_text ltx_font_italic">earning</span>
<span id="id1.p1.1.1.5" class="ltx_text ltx_font_italic">For</span>
E<span id="id1.p1.1.1.6" class="ltx_text ltx_font_italic">fficient</span>
N<span id="id1.p1.1.1.7" class="ltx_text ltx_font_italic">amed</span>
<span id="id1.p1.1.1.8" class="ltx_text ltx_font_italic">Entity</span>
<span id="id1.p1.1.1.9" class="ltx_text ltx_font_italic">Recognition</span>
<span id="id1.p1.1.1.10" class="ltx_text ltx_font_medium"></span></span></p>
<br class="ltx_break ltx_centering">
<table id="id1.p1.2" class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr id="id1.p1.2.1.1" class="ltx_tr">
<td id="id1.p1.2.1.1.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Haris Riaz, Razvan-Gabriel Dumitru, Mihai Surdeanu</span></td>
</tr>
<tr id="id1.p1.2.2.2" class="ltx_tr">
<td id="id1.p1.2.2.2.1" class="ltx_td ltx_align_center">University of Arizona</td>
</tr>
<tr id="id1.p1.2.3.3" class="ltx_tr">
<td id="id1.p1.2.3.3.1" class="ltx_td ltx_align_center">Tucson, AZ, USA</td>
</tr>
<tr id="id1.p1.2.4.4" class="ltx_tr">
<td id="id1.p1.2.4.4.1" class="ltx_td ltx_align_center">{hriaz, rdumitru, msurdeanu}@arizona.edu</td>
</tr>
</tbody>
</table>
<p id="id1.p1.3" class="ltx_p ltx_align_center"><span id="id1.p1.3.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Named entity recognition (NER), i.e., the task of identifying named (and sometimes numeric) entities such person and organization names, drugs, protein names, diseases, and dates, is one of the earliest formal natural language processing (NLP) tasks <cite class="ltx_cite ltx_citemacro_cite">Grishman and Sundheim (<a href="#bib.bib14" title="" class="ltx_ref">1996</a>)</cite>. NER remains critical to many real-world applications such as question answering and information extraction <cite class="ltx_cite ltx_citemacro_cite">Yadav and Bethard (<a href="#bib.bib41" title="" class="ltx_ref">2019</a>)</cite>. Despite the tremendous progress observed on the NER task in the past almost three decades, we argue that there are several practical limitations in the way this task is generally formalized, which impact our understanding of what methods perform best in practice. In particular:</p>
<p id="S1.p1.2" class="ltx_p ltx_align_left"><span id="S1.p1.2.1" class="ltx_text ltx_font_bold">(1)</span></p>
<p id="S1.p1.3" class="ltx_p">Current settings for the NER task require an amount of annotations that are unrealistic for many real-world applications. For example, a common setting for semi-supervised NER uses 5% of the CoNLL-2003 corpus’ (<cite class="ltx_cite ltx_citemacro_citet">Tjong Kim Sang and De Meulder (<a href="#biba.bib2" title="" class="ltx_ref">2003</a>)</cite>) training data, or over 10K total tokens <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>); Zheng et al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>. In our work <cite class="ltx_cite ltx_citemacro_cite">Vacareanu et al. (<a href="#bib.bib37" title="" class="ltx_ref">2024</a>)</cite>, we have observed that NER annotations take approximately 3.2 seconds per token in practice. Thus, annotating the equivalent amount of data in a new domain would take approximately 9 person hours. This is unrealistic in many scenarios (e.g., intelligence, pandemic surveillance) that require the rapid development of custom models and where domain experts “do not want to come willingly and do not come cheaply.”<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>IARPA program manager, personal communication</span></span></span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p ltx_align_left"><span id="S1.p2.1.1" class="ltx_text ltx_font_bold">(2)</span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While recent directions that use in-context learning (ICL) for NER with autoregressive decoder-based large language models (LLMs) perform well <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, they do not scale as well as encoder-based methods due to the decoder’s high inference overhead; each generated token requires its own forward pass through the model.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p ltx_align_left"><span id="S1.p4.1.1" class="ltx_text ltx_font_bold">(3)</span></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Recent trends rely mostly on neural networks (NNs) to learn the NER task, ignoring linguistic hints such as “one sense per discourse” <cite class="ltx_cite ltx_citemacro_cite">Gale et al. (<a href="#bib.bib12" title="" class="ltx_ref">1992</a>)</cite> that might be present and are likely to be useful in lightly-supervised settings.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To remedy these limitations we propose an <span id="S1.p6.1.1" class="ltx_text ltx_font_italic">extremely lightly supervised</span> scenario for NER, in which the only supervision comes in the form of a lexicon containing 10 examples per entity class. Importantly, the 10 examples are selected by a domain expert that does <span id="S1.p6.1.2" class="ltx_text ltx_font_italic">not</span> have access to any dataset annotations. Further, we propose a simple NER approach for this scenario that is efficient and performs well despite the limited supervision. Our method uses an encoder-only inference strategy, but, at training time, it combines multiple strategies including language models and several linguistic heuristics. We call our method <span id="S1.p6.1.3" class="ltx_text ltx_font_normal">E<span id="S1.p6.1.3.1" class="ltx_text ltx_font_italic">xtremely </span>L<span id="S1.p6.1.3.2" class="ltx_text ltx_font_italic">ightly Supervised </span>L<span id="S1.p6.1.3.3" class="ltx_text ltx_font_italic">earning for </span>E<span id="S1.p6.1.3.4" class="ltx_text ltx_font_italic">fficient </span>N<span id="S1.p6.1.3.5" class="ltx_text ltx_font_italic">amed Entity Recognition</span></span> (ELLEN)<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/hriaz17/ELLEN" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/hriaz17/ELLEN</a></span></span></span>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Our main contributions are as follows:</p>
<p id="S1.p7.2" class="ltx_p ltx_align_left"><span id="S1.p7.2.1" class="ltx_text ltx_font_bold">(1)</span></p>
<p id="S1.p7.3" class="ltx_p">We demonstrate the effectiveness of combining language models with commonsense linguistic rules inspired by <cite class="ltx_cite ltx_citemacro_cite">Liao and Veeramachaneni (<a href="#bib.bib21" title="" class="ltx_ref">2009</a>)</cite> and aggregated under a self-training, modular, neuro-symbolic architecture. Our approach is considerably simpler than other complex statistical methods for semi-supervised NER <cite class="ltx_cite ltx_citemacro_cite">(Nagesh and Surdeanu, <a href="#bib.bib26" title="" class="ltx_ref">2018</a>; Lakshmi Narayan et al., <a href="#bib.bib18" title="" class="ltx_ref">2019</a>; Peng et al., <a href="#bib.bib31" title="" class="ltx_ref">2019</a>; Zhou et al., <a href="#bib.bib44" title="" class="ltx_ref">2022</a>; Chen et al., <a href="#bib.bib9" title="" class="ltx_ref">2019</a>; Clark et al., <a href="#bib.bib10" title="" class="ltx_ref">2018</a>; Chen et al., <a href="#bib.bib7" title="" class="ltx_ref">2020</a>; Zheng et al., <a href="#bib.bib43" title="" class="ltx_ref">2023</a>, inter alia)</cite>.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p ltx_align_left"><span id="S1.p8.1.1" class="ltx_text ltx_font_bold">(2)</span></p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">Our approach includes a novel component called the Masked Language Modeling (MLM) Heuristic, which is a fully unsupervised NER method that achieves over 55% precision on the CoNLL-2003 NER dataset. Further, this component complements other self training as well as linguistic heuristics in the semi-supervised setting.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p ltx_align_left"><span id="S1.p10.1.1" class="ltx_text ltx_font_bold">(3)</span></p>
</div>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">We evaluate our method on CoNLL-2003 (<cite class="ltx_cite ltx_citemacro_citet">Tjong Kim Sang and De Meulder (<a href="#biba.bib2" title="" class="ltx_ref">2003</a>)</cite>) under three different degrees of supervision, and in a zero-shot setting on WNUT-17 (<cite class="ltx_cite ltx_citemacro_citet">Derczynski et al. (<a href="#biba.bib1" title="" class="ltx_ref">2017</a>)</cite>). On CoNLL, under the proposed setting of extremely limited supervision, we show that our method achieves an F1 score of <span id="S1.p11.1.1" class="ltx_text ltx_font_bold">76.87</span>%. Further, when we increase the degree of supervision to match other methods which are state-of-the-art in the semi-supervised NER setting, we find that our method achieves comparable performance. We also show that our method continues to scale, even when using all of the data available for supervision. In a zero-shot evaluation on WNUT-17, we find our method to be comparable to LLMs such as GPT-3.5 <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib29" title="" class="ltx_ref">2023b</a>)</cite> and GPT-4 <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib28" title="" class="ltx_ref">2023a</a>)</cite>, while also obtaining over 75% of the performance of a fully supervised model trained on WNUT-17.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Recently, Large Language Models (LLM’s) have emerged as the dominant approach for a wide variety of NLP tasks, including Named Entity Recognition. <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite> show that LLM’s consistenly achieve SOTA performance on many NER datasets. With In-Context Learning, LLM’s have also proven to be very useful in the FewShot NER setting, as recently shown by <cite class="ltx_cite ltx_citemacro_cite">Ashok and Lipton (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>. However, LLM’s typically have a high inference overhead <cite class="ltx_cite ltx_citemacro_cite">Narayanan et al. (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> and may not always perform well in specialized or low-resource domains. Moreover, there are increasing concerns about data contamination. <cite class="ltx_cite ltx_citemacro_cite">Golchin and Surdeanu (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite> demonstrate that GPT-3.5 and GPT-4 have encountered test data with labels from widely-used NLP benchmark datasets during pre-training. <cite class="ltx_cite ltx_citemacro_cite">Sainz et al. (<a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite> claim that this is true for CoNLL-2003, one of our evaluation datasets.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Focusing on Semi-Supervised NER, and not FewShot NER, current state-of-the-art methods include JointProp <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>, which is a multi-task learning framework that jointly tries to solve relation classification and NER using a heterogeneous graph structure. Semi-LADA <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite> adapts the mixup data augmentation technique to sequence labeling, and then trains on linearly interpolated pairs of samples. Both of these methods use at least 5% of the labeled data as their most minimally supervised setting, which we argue, is an impractical level of supervision for semi-supervised NER.
Another class of statistical
methods <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>); Zhou et al. (<a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite> try to solve the reliance on gold labels by resorting to distant supervision: they construct lexicons based on large dataset independent knowledge bases. These methods then use Positive-Unlabeled (PU) learning to train classifiers using only labeled positive examples and a set of unlabeled data containing both positives and negatives.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Other approaches like <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib22" title="" class="ltx_ref">2019a</a>)</cite> showed the benefits of augmenting neural NER taggers with external gazetteers. However, external gazetteers may not always be available for particular domains. In contrast we propose a semi-supervised method for NER along the lines of the approach taken by <cite class="ltx_cite ltx_citemacro_cite">Liao and Veeramachaneni (<a href="#bib.bib21" title="" class="ltx_ref">2009</a>)</cite>: one that combines the generalizability of contemporary deep learning with intuitive reasoning and linguistic insights, and we demonstrate its strong performance, under a setting of extremely low supervision. We posit that such an approach can rival more multifaceted statistical techniques for semi-supervised NER, such as PU learning &amp; data augmentation, among others.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2403.17385/assets/figures/ELLEN-architecture.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="381" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S2.F1.6.3" class="ltx_text" style="font-size:90%;">The proposed method illustrated. <math id="S2.F1.4.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.F1.4.1.m1.1b"><mi id="S2.F1.4.1.m1.1.1" xref="S2.F1.4.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.F1.4.1.m1.1c"><ci id="S2.F1.4.1.m1.1.1.cmml" xref="S2.F1.4.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.4.1.m1.1d">D</annotation></semantics></math> refers to a subset of the unlabeled data which is added back to the labeled data for retraining in the next iteration.
OSPD refers to the “One Sense Per Discourse” rule; “Global rules” indicate the rules described in Section <a href="#S3.SS3" title="3.3. Global Rules ‣ 3. Proposed Method ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. The colors used in the figure represent the decreasing quality of the generated annotations in the three stages, after the fine-tuning stage: <span id="S2.F1.6.3.1" class="ltx_text" style="color:#00FF00;">green</span> <math id="S2.F1.5.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S2.F1.5.2.m2.1b"><mo stretchy="false" id="S2.F1.5.2.m2.1.1" xref="S2.F1.5.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.F1.5.2.m2.1c"><ci id="S2.F1.5.2.m2.1.1.cmml" xref="S2.F1.5.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.5.2.m2.1d">\rightarrow</annotation></semantics></math> <span id="S2.F1.6.3.2" class="ltx_text" style="color:#FF8000;">orange</span> <math id="S2.F1.6.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S2.F1.6.3.m3.1b"><mo stretchy="false" id="S2.F1.6.3.m3.1.1" xref="S2.F1.6.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.F1.6.3.m3.1c"><ci id="S2.F1.6.3.m3.1.1.cmml" xref="S2.F1.6.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.6.3.m3.1d">\rightarrow</annotation></semantics></math> <span id="S2.F1.6.3.3" class="ltx_text" style="color:#0000FF;">blue</span>.</span></figcaption>
</figure>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S2.T1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Category</span></th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S2.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.2.1.1" class="ltx_p"><span id="S2.T1.1.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Entities</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S2.T1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">ORG</span></th>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S2.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.2.1.1" class="ltx_p"><span id="S2.T1.1.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Reuters, PUK, NATO, Honda, Ajax Amsterdam, Motorola, PSV Eindhoven, PKK, Hansa Rostock, Commonwealth</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S2.T1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">LOC</span></th>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S2.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.2.1.1" class="ltx_p"><span id="S2.T1.1.3.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Germany, Australia, Britain, Spain, Italy, LONDON, Russia, China, Japan, NEW YORK</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<th id="S2.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S2.T1.1.4.3.1.1" class="ltx_text" style="font-size:90%;">MISC</span></th>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S2.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.2.1.1" class="ltx_p"><span id="S2.T1.1.4.3.2.1.1.1" class="ltx_text" style="font-size:90%;">Dutch, British, French, Russian, German, Iraqi, Israeli, English, Australian, American</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<th id="S2.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S2.T1.1.5.4.1.1" class="ltx_text" style="font-size:90%;">PER</span></th>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S2.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.2.1.1" class="ltx_p"><span id="S2.T1.1.5.4.2.1.1.1" class="ltx_text" style="font-size:90%;">Clinton, Yeltsin, Arafat, Lebed, Wasim Akram, Waqar Younis, Mushtaq Ahmed, Netanyahu, Williams, Rubin</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Lexicon generated by the domain expert following the criteria that we outline in Section <a href="#S3.SS6" title="3.6. Minimizing The Dependency On A Lexicon ‣ 3. Proposed Method ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.6</span></a>. This lexicon serves as the seed set of entities for our model and the sole source of “gold” supervision.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Proposed Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.4" class="ltx_p">As our method does not rely on any explicitly labeled texts, we begin by discarding all labels in the NER dataset (CoNLL-2003 in this paper). We then ask a domain expert to generate a small lexicon of 10 example named entities per class, for each of the four classes in CoNLL-2003, i.e., <span id="S3.p1.4.1" class="ltx_text ltx_font_typewriter">PER</span>, <span id="S3.p1.4.2" class="ltx_text ltx_font_typewriter">ORG</span>, <span id="S3.p1.4.3" class="ltx_text ltx_font_typewriter">LOC</span>, and <span id="S3.p1.4.4" class="ltx_text ltx_font_typewriter">MISC</span>. This lexicon:

<span id="S3.I1" class="ltx_inline-enumerate">
<span id="S3.I1.i1" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">a)</span> <span id="S3.I1.i1.1" class="ltx_text">is sourced entirely from the tokens in the dataset;
</span></span>
<span id="S3.I1.i2" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">b)</span> <span id="S3.I1.i2.1" class="ltx_text">is constructed <span id="S3.I1.i2.1.1" class="ltx_text ltx_font_italic">without</span> looking at any of the labels in the dataset;
</span></span>
<span id="S3.I1.i3" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">c)</span> <span id="S3.I1.i3.1" class="ltx_text">does not rely on any external knowledge-base or dictionary; and
</span></span>
<span id="S3.I1.i4" class="ltx_inline-item"><span class="ltx_tag ltx_tag_inline-item">d)</span> <span id="S3.I1.i4.1" class="ltx_text">serves as the sole source of “gold supervision” for our method.
</span></span>
</span>
The domain expert is able to construct the lexicon (refer to Table <a href="#S2.T1" title="Table 1 ‣ 2. Related Works ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) in less than 30 minutes for CoNLL-2003. We believe 10 examples per class is a reasonable number for producing an informative lexicon with the minimum amount of effort required; this is similar to few-shot relation extraction, which focuses on 5 examples/5-shots <cite class="ltx_cite ltx_citemacro_cite">Han et al. (<a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite>. We then use this lexicon to annotate a small portion of the entirely unlabeled data of CoNLL-2003. In terms of “degree of supervision” (refer to table <a href="#S4.T4" title="Table 4 ‣ 4.1. Data &amp; Setup ‣ 4. Experimental Results ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), we annotate about 9.13% of entities present in the CoNLL training data using the lexicon. We refer to this annotated subset as <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">L</annotation></semantics></math> and the remaining unlabeled data as <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">U</annotation></semantics></math>, following the convention of traditional semi-supervised learning literature. At a high level, our approach extends the simple self-training algorithm presented by <cite class="ltx_cite ltx_citemacro_cite">Liao and Veeramachaneni (<a href="#bib.bib21" title="" class="ltx_ref">2009</a>)</cite>, which is shown in Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3. Proposed Method ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
However, we argue that the procedure delineated in this algorithm is abstract, offering no guidance on the precise sequence in which the NER classifier <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="C_{k}" display="inline"><semantics id="S3.p1.3.m3.1a"><msub id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">C</mi><mi id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">𝐶</ci><ci id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">C_{k}</annotation></semantics></math> (and any other linguistic rules) should be applied to extract new data <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">D</annotation></semantics></math>. Such ambiguity could potentially lead to the common pitfalls inherent in classic self-training approaches. In our study, determining the optimal order and manner in which these various approaches should be combined proved to be a nuanced task. Our work is motivated by curriculum learning, which argues that better models are learned when the training data is “presented in a meaningful order which illustrates gradually more complex examples” <cite class="ltx_cite ltx_citemacro_cite">Bengio et al. (<a href="#bib.bib5" title="" class="ltx_ref">2009</a>); Sachan and Xing (<a href="#bib.bib32" title="" class="ltx_ref">2016</a>)</cite>.
In this paper, we adjust this principle to mean <span id="S3.p1.4.5" class="ltx_text ltx_font_italic">gradually noisier examples</span> based on the observation that in self-training it is critical that the initial models be of higher quality to reduce noise in future iterations.
Inspired by this idea, we propose an intuitive, three-stage framework, illustrated in Figure <a href="#S2.F1" title="Figure 1 ‣ 2. Related Works ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, for effectively combining linguistic rules with pre-trained language models such as <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>. We design our framework to simultaneously balance two orthogonal goals:</p>
<ol id="S3.I2" class="ltx_enumerate">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">avoiding the pitfalls of classic self-training (e.g., a model failing to correct its errors and instead amplifying them) to the extent possible, and</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">still being conceptually similar to self-training at a high level.</p>
</div>
</li>
</ol>
<p id="S3.p1.5" class="ltx_p">Our proposed NER method is fully modular, uses the <span id="S3.p1.5.1" class="ltx_text ltx_font_typewriter">deberta-v3-large</span> encoder<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>DeBERTa-v3 is the current state-of-the-art encoder-based model on many benchmark NLP tasks. The key advantage of using DeBERTa is its relative positional encoding, which allows the model to generalize better to longer sequences.</span></span></span> as the neural component, and blends various other linguistic and statistical heuristics in a sieve <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib19" title="" class="ltx_ref">2013</a>)</cite>. We first describe each of these heuristics below, and then describe how they are integrated in the three-stage ELLEN framework.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_float"><span id="alg1.4.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> A simple NER self-training algorithm</figcaption>
<div id="alg1.5" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:90%;">1:</span></span><span id="alg1.l1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Given:</span><span id="alg1.l1.3" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:90%;">2:</span></span><span id="alg1.l2.2" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l2.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="alg1.l2.m1.1a"><mi mathsize="90%" id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">L</annotation></semantics></math><span id="alg1.l2.3" class="ltx_text" style="font-size:90%;"> - a small set of labeled training data
</span>
</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:90%;">3:</span></span><span id="alg1.l3.2" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l3.m1.1" class="ltx_Math" alttext="U" display="inline"><semantics id="alg1.l3.m1.1a"><mi mathsize="90%" id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><ci id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">U</annotation></semantics></math><span id="alg1.l3.3" class="ltx_text" style="font-size:90%;"> - unlabeled data
</span>
</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:90%;">4:</span></span><span id="alg1.l4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="alg1.l4.3" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l4.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="alg1.l4.m1.1a"><mi mathsize="90%" id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><ci id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">k</annotation></semantics></math><span id="alg1.l4.4" class="ltx_text" style="font-size:90%;"> iterations </span><span id="alg1.l4.5" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span><span id="alg1.l4.6" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:90%;">5:</span></span><span id="alg1.l5.2" class="ltx_text" style="font-size:90%;">    </span><span id="alg1.l5.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Step 1:</span><span id="alg1.l5.4" class="ltx_text" style="font-size:90%;"> Train a NER </span><math id="alg1.l5.m1.1" class="ltx_Math" alttext="C_{k}" display="inline"><semantics id="alg1.l5.m1.1a"><msub id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><mi mathsize="90%" id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2.cmml">C</mi><mi mathsize="90%" id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><csymbol cd="ambiguous" id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1">subscript</csymbol><ci id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2">𝐶</ci><ci id="alg1.l5.m1.1.1.3.cmml" xref="alg1.l5.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">C_{k}</annotation></semantics></math><span id="alg1.l5.5" class="ltx_text" style="font-size:90%;"> based on </span><math id="alg1.l5.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="alg1.l5.m2.1a"><mi mathsize="90%" id="alg1.l5.m2.1.1" xref="alg1.l5.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m2.1b"><ci id="alg1.l5.m2.1.1.cmml" xref="alg1.l5.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m2.1c">L</annotation></semantics></math><span id="alg1.l5.6" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:90%;">6:</span></span><span id="alg1.l6.2" class="ltx_text" style="font-size:90%;">    </span><span id="alg1.l6.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Step 2:</span><span id="alg1.l6.4" class="ltx_text" style="font-size:90%;"> Extract new data </span><math id="alg1.l6.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="alg1.l6.m1.1a"><mi mathsize="90%" id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><ci id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">D</annotation></semantics></math><span id="alg1.l6.5" class="ltx_text" style="font-size:90%;"> based on </span><math id="alg1.l6.m2.1" class="ltx_Math" alttext="C_{k}" display="inline"><semantics id="alg1.l6.m2.1a"><msub id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml"><mi mathsize="90%" id="alg1.l6.m2.1.1.2" xref="alg1.l6.m2.1.1.2.cmml">C</mi><mi mathsize="90%" id="alg1.l6.m2.1.1.3" xref="alg1.l6.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l6.m2.1b"><apply id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1"><csymbol cd="ambiguous" id="alg1.l6.m2.1.1.1.cmml" xref="alg1.l6.m2.1.1">subscript</csymbol><ci id="alg1.l6.m2.1.1.2.cmml" xref="alg1.l6.m2.1.1.2">𝐶</ci><ci id="alg1.l6.m2.1.1.3.cmml" xref="alg1.l6.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m2.1c">C_{k}</annotation></semantics></math><span id="alg1.l6.6" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:90%;">7:</span></span><span id="alg1.l7.2" class="ltx_text" style="font-size:90%;">    </span><span id="alg1.l7.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Step 3:</span><span id="alg1.l7.4" class="ltx_text" style="font-size:90%;"> Add </span><math id="alg1.l7.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="alg1.l7.m1.1a"><mi mathsize="90%" id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><ci id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">D</annotation></semantics></math><span id="alg1.l7.5" class="ltx_text" style="font-size:90%;"> to </span><math id="alg1.l7.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="alg1.l7.m2.1a"><mi mathsize="90%" id="alg1.l7.m2.1.1" xref="alg1.l7.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="alg1.l7.m2.1b"><ci id="alg1.l7.m2.1.1.cmml" xref="alg1.l7.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m2.1c">L</annotation></semantics></math><span id="alg1.l7.6" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:90%;">8:</span></span><span id="alg1.l8.2" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg1.l8.3" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l8.4" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span>
</div>
</div>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.1.   Unsupervised Entity Recognition Using A Masked Language Model (MLM)</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Motivated by the observation that any language model, subjected to pretraining via the Masked Language Modeling (MLM) objective, likely acquires semantic, syntactic, and world knowledge, we hypothesize that the capability to discern named entities is also inherently embedded within such models.
We present a novel, fully unsupervised algorithm, implemented as a rule in our neuro-symbolic modular architecture that allows us to gain additional “free” supervision , beyond our small lexicon of ten entities. This algorithm relies on a small pre-trained LM from <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2019b</a>)</cite>, which leverages our lexicon to extract new Named Entities from unlabeled data.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Unlike recent prompt-based approaches for NER, particularly few-shot NER, which involve either prompting LLM’s with in-context examples to inject NER ability <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, or involve constructing dynamic templates based on label aware pivot words, our approach is much simpler and more constrained. We first use a very simple, linguistically inspired regular expression, based on part-of-speech (POS) tags, for detecting named entity spans:</p>
<table id="A4.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S3.Ex1.3.2.1.1" class="ltx_text ltx_markedasmath ltx_font_typewriter">(NNP|NNPS)+(IN(NNP|NNPS)+)?</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.2" class="ltx_p">where <span id="S3.SS1.p2.2.1" class="ltx_text ltx_font_typewriter">NNP</span>/<span id="S3.SS1.p2.2.2" class="ltx_text ltx_font_typewriter">NNPS</span> are the POS tags of singular/plural proper nouns, and <span id="S3.SS1.p2.2.3" class="ltx_text ltx_font_typewriter">IN</span> is the POS tag assigned to prepositions. On the unlabeled portion of the CoNLL training dataset, this rule can detect named entity boundaries with a precision of <span id="S3.SS1.p2.2.4" class="ltx_text ltx_font_bold">85.16%</span>, as shown in Table <a href="#S3.T2" title="Table 2 ‣ 3.1. Unsupervised Entity Recognition Using A Masked Language Model (MLM) ‣ 3. Proposed Method ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Note that this rule is considerably simpler and more efficient than other recent computationally-intensive approaches, e.g., the entity typing and span identification method of <cite class="ltx_cite ltx_citemacro_citet">Shen et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite>, or the span classification approach adopted by <cite class="ltx_cite ltx_citemacro_citet">Arora and Park (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>. Typically, these approaches initially train a model for span classification, followed by a model for entity type classification.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Precision</span></td>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Recall</span></td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T2.1.1.1.3.1" class="ltx_text" style="font-size:90%;">F1 Score</span></td>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<td id="S3.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T2.1.2.2.1.1" class="ltx_text" style="font-size:90%;">85.16%</span></td>
<td id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T2.1.2.2.2.1" class="ltx_text" style="font-size:90%;">90.96%</span></td>
<td id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T2.1.2.2.3.1" class="ltx_text" style="font-size:90%;">87.96%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Micro-F1 scores of the regex rule for detecting named entity boundaries.</figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.9" class="ltx_p">Once entity spans are identified, we label them using a masking heuristic. Intuitively, our method selects the entity label whose exemplars in the lexicon fill in the span with the highest likelihood. More formally, we first mask the span identified by the regex above with a number of <span id="S3.SS1.p3.9.1" class="ltx_text ltx_font_typewriter">[MASK]</span> tokens equal to the tokens included in the span. For example, the span <span id="S3.SS1.p3.9.2" class="ltx_text ltx_font_italic">John Doe</span> in the sentence: <span id="S3.SS1.p3.9.3" class="ltx_text ltx_font_italic">John Doe is happy</span> will be masked as <span id="S3.SS1.p3.9.4" class="ltx_text ltx_font_typewriter">[MASK]</span><span id="S3.SS1.p3.9.5" class="ltx_text ltx_font_italic"> </span><span id="S3.SS1.p3.9.6" class="ltx_text ltx_font_typewriter">[MASK]</span><span id="S3.SS1.p3.9.7" class="ltx_text ltx_font_italic"> is happy</span>. We then iteratively fill in lexicon entries of the same length (across all entity classes) and keep track of all token probabilities. For example, the entry <span id="S3.SS1.p3.9.8" class="ltx_text ltx_font_italic">Dole</span> in the Person lexicon, which is tokenized as <span id="S3.SS1.p3.9.9" class="ltx_text ltx_font_italic">Do</span> and <span id="S3.SS1.p3.9.10" class="ltx_text ltx_font_italic">##le</span>,<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We use the BERT tokenizer convention for multi-token words here for readability.</span></span></span> produces the sentence: <span id="S3.SS1.p3.9.11" class="ltx_text ltx_font_italic">Do ##le is happy</span>. Lastly, we select the entity label based on the following formula:</p>
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.1" class="ltx_Math" alttext="c=\arg\max_{k}\max_{j}\frac{1}{n}\sum_{i}^{n}p(t_{i}|x)" display="block"><semantics id="S3.Ex2.m1.1a"><mrow id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml"><mi id="S3.Ex2.m1.1.1.3" xref="S3.Ex2.m1.1.1.3.cmml">c</mi><mo id="S3.Ex2.m1.1.1.2" xref="S3.Ex2.m1.1.1.2.cmml">=</mo><mrow id="S3.Ex2.m1.1.1.1" xref="S3.Ex2.m1.1.1.1.cmml"><mrow id="S3.Ex2.m1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.3.cmml"><mi id="S3.Ex2.m1.1.1.1.3.1" xref="S3.Ex2.m1.1.1.1.3.1.cmml">arg</mi><mo lspace="0.167em" id="S3.Ex2.m1.1.1.1.3a" xref="S3.Ex2.m1.1.1.1.3.cmml">⁡</mo><mrow id="S3.Ex2.m1.1.1.1.3.2" xref="S3.Ex2.m1.1.1.1.3.2.cmml"><munder id="S3.Ex2.m1.1.1.1.3.2.1" xref="S3.Ex2.m1.1.1.1.3.2.1.cmml"><mi id="S3.Ex2.m1.1.1.1.3.2.1.2" xref="S3.Ex2.m1.1.1.1.3.2.1.2.cmml">max</mi><mi id="S3.Ex2.m1.1.1.1.3.2.1.3" xref="S3.Ex2.m1.1.1.1.3.2.1.3.cmml">k</mi></munder><mo lspace="0.167em" id="S3.Ex2.m1.1.1.1.3.2a" xref="S3.Ex2.m1.1.1.1.3.2.cmml">⁡</mo><mrow id="S3.Ex2.m1.1.1.1.3.2.2" xref="S3.Ex2.m1.1.1.1.3.2.2.cmml"><munder id="S3.Ex2.m1.1.1.1.3.2.2.1" xref="S3.Ex2.m1.1.1.1.3.2.2.1.cmml"><mi id="S3.Ex2.m1.1.1.1.3.2.2.1.2" xref="S3.Ex2.m1.1.1.1.3.2.2.1.2.cmml">max</mi><mi id="S3.Ex2.m1.1.1.1.3.2.2.1.3" xref="S3.Ex2.m1.1.1.1.3.2.2.1.3.cmml">j</mi></munder><mo lspace="0.167em" id="S3.Ex2.m1.1.1.1.3.2.2a" xref="S3.Ex2.m1.1.1.1.3.2.2.cmml">⁡</mo><mfrac id="S3.Ex2.m1.1.1.1.3.2.2.2" xref="S3.Ex2.m1.1.1.1.3.2.2.2.cmml"><mn id="S3.Ex2.m1.1.1.1.3.2.2.2.2" xref="S3.Ex2.m1.1.1.1.3.2.2.2.2.cmml">1</mn><mi id="S3.Ex2.m1.1.1.1.3.2.2.2.3" xref="S3.Ex2.m1.1.1.1.3.2.2.2.3.cmml">n</mi></mfrac></mrow></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.2.cmml">​</mo><mrow id="S3.Ex2.m1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.cmml"><munderover id="S3.Ex2.m1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.Ex2.m1.1.1.1.1.2.2.2" xref="S3.Ex2.m1.1.1.1.1.2.2.2.cmml">∑</mo><mi id="S3.Ex2.m1.1.1.1.1.2.2.3" xref="S3.Ex2.m1.1.1.1.1.2.2.3.cmml">i</mi><mi id="S3.Ex2.m1.1.1.1.1.2.3" xref="S3.Ex2.m1.1.1.1.1.2.3.cmml">n</mi></munderover><mrow id="S3.Ex2.m1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.Ex2.m1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex2.m1.1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.Ex2.m1.1.1.1.1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex2.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.2.2.cmml">t</mi><mi id="S3.Ex2.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S3.Ex2.m1.1.1.1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.Ex2.m1.1.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.Ex2.m1.1.1.1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.1b"><apply id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1"><eq id="S3.Ex2.m1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.2"></eq><ci id="S3.Ex2.m1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.3">𝑐</ci><apply id="S3.Ex2.m1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1"><times id="S3.Ex2.m1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.2"></times><apply id="S3.Ex2.m1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.3"><arg id="S3.Ex2.m1.1.1.1.3.1.cmml" xref="S3.Ex2.m1.1.1.1.3.1"></arg><apply id="S3.Ex2.m1.1.1.1.3.2.cmml" xref="S3.Ex2.m1.1.1.1.3.2"><apply id="S3.Ex2.m1.1.1.1.3.2.1.cmml" xref="S3.Ex2.m1.1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.3.2.1.1.cmml" xref="S3.Ex2.m1.1.1.1.3.2.1">subscript</csymbol><max id="S3.Ex2.m1.1.1.1.3.2.1.2.cmml" xref="S3.Ex2.m1.1.1.1.3.2.1.2"></max><ci id="S3.Ex2.m1.1.1.1.3.2.1.3.cmml" xref="S3.Ex2.m1.1.1.1.3.2.1.3">𝑘</ci></apply><apply id="S3.Ex2.m1.1.1.1.3.2.2.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2"><apply id="S3.Ex2.m1.1.1.1.3.2.2.1.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.3.2.2.1.1.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.1">subscript</csymbol><max id="S3.Ex2.m1.1.1.1.3.2.2.1.2.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.1.2"></max><ci id="S3.Ex2.m1.1.1.1.3.2.2.1.3.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.1.3">𝑗</ci></apply><apply id="S3.Ex2.m1.1.1.1.3.2.2.2.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.2"><divide id="S3.Ex2.m1.1.1.1.3.2.2.2.1.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.2"></divide><cn type="integer" id="S3.Ex2.m1.1.1.1.3.2.2.2.2.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.2.2">1</cn><ci id="S3.Ex2.m1.1.1.1.3.2.2.2.3.cmml" xref="S3.Ex2.m1.1.1.1.3.2.2.2.3">𝑛</ci></apply></apply></apply></apply><apply id="S3.Ex2.m1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1"><apply id="S3.Ex2.m1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.Ex2.m1.1.1.1.1.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.2.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.Ex2.m1.1.1.1.1.2.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.2"></sum><ci id="S3.Ex2.m1.1.1.1.1.2.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S3.Ex2.m1.1.1.1.1.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.2.3">𝑛</ci></apply><apply id="S3.Ex2.m1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1"><times id="S3.Ex2.m1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.2"></times><ci id="S3.Ex2.m1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.3">𝑝</ci><apply id="S3.Ex2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.Ex2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.2.2">𝑡</ci><ci id="S3.Ex2.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.Ex2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1.1.1.3">𝑥</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.1c">c=\arg\max_{k}\max_{j}\frac{1}{n}\sum_{i}^{n}p(t_{i}|x)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.8" class="ltx_p">where <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="t_{i}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">t</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝑡</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">t_{i}</annotation></semantics></math> is the <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">i</annotation></semantics></math>th masked token (e.g., <span id="S3.SS1.p3.8.1" class="ltx_text ltx_font_italic">Do</span> is <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="t_{0}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">t</mi><mn id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">𝑡</ci><cn type="integer" id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">t_{0}</annotation></semantics></math> in the example above); <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">x</annotation></semantics></math> is the sentence with the masked tokens; <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mi id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><ci id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">n</annotation></semantics></math> is the total number of masks for the current example (e.g., 2 for the example above); the <math id="S3.SS1.p3.6.m6.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS1.p3.6.m6.1a"><mi id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><ci id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">j</annotation></semantics></math> index iterates over all exemplars for the current entity label; and <math id="S3.SS1.p3.7.m7.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p3.7.m7.1a"><mi id="S3.SS1.p3.7.m7.1.1" xref="S3.SS1.p3.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.m7.1b"><ci id="S3.SS1.p3.7.m7.1.1.cmml" xref="S3.SS1.p3.7.m7.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.m7.1c">k</annotation></semantics></math> iterates over all entity class lexicons. That is, for each exemplar, we first compute the average probability of all its tokens. Then we pick the exemplar with the highest probability in a given lexicon as the probability of the corresponding entity label. Lastly, we select the entity label <math id="S3.SS1.p3.8.m8.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS1.p3.8.m8.1a"><mi id="S3.SS1.p3.8.m8.1.1" xref="S3.SS1.p3.8.m8.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.8.m8.1b"><ci id="S3.SS1.p3.8.m8.1.1.cmml" xref="S3.SS1.p3.8.m8.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.8.m8.1c">c</annotation></semantics></math> with the highest probability.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.2" class="ltx_p">We denote this rule as the Masked Language Modeling (MLM) Heuristic. We demonstrate its NER effectiveness on the development set of CoNLL-2003, in a fully unsupervised fashion in Table <a href="#S3.T3" title="Table 3 ‣ 3.1. Unsupervised Entity Recognition Using A Masked Language Model (MLM) ‣ 3. Proposed Method ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. As shown in the table, the F1 score of the MLM heuristic is over 56% on the development set of CoNLL-2003. In each iteration of the procedure shown in figure <a href="#alg1" title="Algorithm 1 ‣ 3. Proposed Method ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the MLM is used in the burn-in and intermediate stages to annotate a subset of the unlabeled data <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">U</annotation></semantics></math>, which is eventually added back to <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">L</annotation></semantics></math>.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Entity Type</span></th>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Precision</span></th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Recall</span></th>
<th id="S3.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.1.1.4.1" class="ltx_text" style="font-size:90%;">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.2.1" class="ltx_tr">
<th id="S3.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Overall</span></th>
<td id="S3.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.2.1.2.1" class="ltx_text" style="font-size:90%;">61.78%</span></td>
<td id="S3.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.2.1.3.1" class="ltx_text" style="font-size:90%;">51.90%</span></td>
<td id="S3.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.2.1.4.1" class="ltx_text" style="font-size:90%;">56.41%</span></td>
</tr>
<tr id="S3.T3.1.3.2" class="ltx_tr">
<th id="S3.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.3.2.1.1" class="ltx_text" style="font-size:90%;">LOC</span></th>
<td id="S3.T3.1.3.2.2" class="ltx_td ltx_align_center" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.3.2.2.1" class="ltx_text" style="font-size:90%;">69.72%</span></td>
<td id="S3.T3.1.3.2.3" class="ltx_td ltx_align_center" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.3.2.3.1" class="ltx_text" style="font-size:90%;">41.53%</span></td>
<td id="S3.T3.1.3.2.4" class="ltx_td ltx_align_center" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.3.2.4.1" class="ltx_text" style="font-size:90%;">52.05%</span></td>
</tr>
<tr id="S3.T3.1.4.3" class="ltx_tr">
<th id="S3.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.4.3.1.1" class="ltx_text" style="font-size:90%;">MISC</span></th>
<td id="S3.T3.1.4.3.2" class="ltx_td ltx_align_center" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.4.3.2.1" class="ltx_text" style="font-size:90%;">45.18%</span></td>
<td id="S3.T3.1.4.3.3" class="ltx_td ltx_align_center" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.4.3.3.1" class="ltx_text" style="font-size:90%;">55.15%</span></td>
<td id="S3.T3.1.4.3.4" class="ltx_td ltx_align_center" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.4.3.4.1" class="ltx_text" style="font-size:90%;">49.67%</span></td>
</tr>
<tr id="S3.T3.1.5.4" class="ltx_tr">
<th id="S3.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.5.4.1.1" class="ltx_text" style="font-size:90%;">ORG</span></th>
<td id="S3.T3.1.5.4.2" class="ltx_td ltx_align_center" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.5.4.2.1" class="ltx_text" style="font-size:90%;">44.85%</span></td>
<td id="S3.T3.1.5.4.3" class="ltx_td ltx_align_center" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.5.4.3.1" class="ltx_text" style="font-size:90%;">40.88%</span></td>
<td id="S3.T3.1.5.4.4" class="ltx_td ltx_align_center" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.5.4.4.1" class="ltx_text" style="font-size:90%;">42.77%</span></td>
</tr>
<tr id="S3.T3.1.6.5" class="ltx_tr">
<th id="S3.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.6.5.1.1" class="ltx_text" style="font-size:90%;">PER</span></th>
<td id="S3.T3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.6.5.2.1" class="ltx_text" style="font-size:90%;">85.07%</span></td>
<td id="S3.T3.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.6.5.3.1" class="ltx_text" style="font-size:90%;">65.02%</span></td>
<td id="S3.T3.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S3.T3.1.6.5.4.1" class="ltx_text" style="font-size:90%;">73.71%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>P/R/F1 of the Masked Language Modeling Heuristic as a fully unsupervised NER algorithm on the CoNLL-2003 development set. It obtains an entity-level F1 score of 56.41% with over 60% overall precision.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.2.   Dynamic Window Filtering</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">In most lightly supervised settings, NER models tend to suffer from the “unlabeled entity problem” as described in <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>, where the entities of a sentence may not be fully annotated. This tends to seriously degrade model performance, since the model treats unlabeled entities as negative or <span id="S3.SS2.p1.2.1" class="ltx_text ltx_font_typewriter">O</span>/Outside instances. Even self-training methods may not be sufficient to completely alleviate the false negative problem since they are susceptible to confirmation bias <cite class="ltx_cite ltx_citemacro_cite">Arazo et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, i.e., erroneously predicted pseudo-labels are likely to deteriorate the model’s performance in subsequent rounds of training. In contrast to <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>’s method which uses negative sampling to avoid training the NER on unlabeled entities, we propose a very simple and run-time efficient linguistically inspired algorithm for controlling the effect of false negatives in sparsely annotated data settings, such as ours. We refer to our algorithm as “Dynamic Window Filtering.” Using part-of-speech (POS) tag information<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>POS tags are now available for many languages (see <a target="_blank" href="https://universaldependencies.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://universaldependencies.org/</a>) and can be obtained from various off-the-shelf models or language processing tools.</span></span></span> and an intuition that tokens which are labeled as <span id="S3.SS2.p1.2.2" class="ltx_text ltx_font_typewriter">O</span>/Outside and which possess a POS tag of <span id="S3.SS2.p1.2.3" class="ltx_text ltx_font_typewriter">NNP</span> (singular proper noun) are highly likely to be unlabeled named entities and thus should be discarded from the NER’s training data. We implement this algorithm as the following rule: we slide a contextual window across each sentence in the labeled subset of the data, <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">L</annotation></semantics></math>, and for each named entity segment we encounter whose label is known, we create a window of size <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">W</annotation></semantics></math>, which dynamically expands in both directions around the labeled entity until an <span id="S3.SS2.p1.2.4" class="ltx_text ltx_font_typewriter">O</span> token that is also tagged with the POS tag of <span id="S3.SS2.p1.2.5" class="ltx_text ltx_font_typewriter">NNP</span> (singular proper noun), is encountered<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We note that alternatives to Dynamic Windowing exist for managing unlabeled entities. For instance, as demonstrated in <cite class="ltx_cite ltx_citemacro_cite">Vacareanu et al. (<a href="#bib.bib37" title="" class="ltx_ref">2024</a>)</cite>, unlabeled entities can remain in the NER’s training data with their impact mitigated by excluding them from the loss calculation, i.e., by backpropagating only over gold-annotated tokens.</span></span></span>. We also emphasize that our POS tags are inherently noisy, since they are obtained from an external LSTM-CRF based POS tagger that was trained exclusively on the Penn Treebank corpus <cite class="ltx_cite ltx_citemacro_cite">Marcus et al. (<a href="#bib.bib25" title="" class="ltx_ref">1993</a>)</cite>. We <span id="S3.SS2.p1.2.6" class="ltx_text ltx_font_italic">do not</span> use the gold POS tags from the CoNLL data. The example below illustrates this rule:
<br class="ltx_break">
<br class="ltx_break"><span id="S3.SS2.p1.2.7" class="ltx_text ltx_font_bold">Example 1:</span> 
<br class="ltx_break"><span id="S3.SS2.p1.2.8" class="ltx_text ltx_font_bold ltx_font_italic">EU<span id="S3.SS2.p1.2.8.1" class="ltx_text ltx_font_medium"> rejects </span>German<span id="S3.SS2.p1.2.8.2" class="ltx_text ltx_font_medium"> call to boycott </span>British<span id="S3.SS2.p1.2.8.3" class="ltx_text ltx_font_medium"> Lamb.
<br class="ltx_break"></span></span></p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">Suppose “EU” and “British” are both Named Entities with known labels and are also proper nouns. Suppose “German” is also a proper noun but with an unknown named entity label (and, thus, it is currently labeled as <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">O</span>). Dynamic Window Filtering creates a contextual window around “EU” and “British”, expanding in size until it encounters the token “German”. This algorithm would thus break the original example into two new segments:</p>
<ol id="S3.I3" class="ltx_enumerate">
<li id="S3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I3.i1.p1" class="ltx_para">
<p id="S3.I3.i1.p1.1" class="ltx_p">“<span id="S3.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">EU</span> rejects”</p>
</div>
</li>
<li id="S3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I3.i2.p1" class="ltx_para">
<p id="S3.I3.i2.p1.1" class="ltx_p">“call to boycott <span id="S3.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">British</span> lamb.”</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p">In every stage of our method, we apply dynamic window filtering on the data that the NER is trained on. This includes both the initial set of sparsely annotated gold data and its augmentations with the pseudo-labeled data (<math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">D</annotation></semantics></math>) that is extracted from the unlabeled data (<math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">U</annotation></semantics></math>) in each iteration. We find that this algorithm achieves the same goal as the method presented in <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite> i.e. discarding Named Entities with unknown labels, while being much simpler and computationally cheaper.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.3.   Global Rules</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Lightly supervised NER models may confuse named entities between Organizations and Persons, Organizations and Locations (and vice versa) due to their shared context. To remedy this, we apply a series of commonsense linguistic rules on the aggregated predictions of the NER model and the Masked Language Modeling (MLM) heuristic. We apply rules for disambiguating named entity segments that have been tagged as Persons (<span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">PER</span>) but end in a company suffix. We update the labels of such segments, including the company suffix to <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_typewriter">ORG</span>. For example, if the entity segment “Walt Disney” is tagged as <span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_typewriter">PER</span>, but it is immediately followed by “Inc.” (a company suffix), the rule would force the whole segment “Walt Disney Inc.” to be an <span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_typewriter">ORG</span>. Similarly, if any named entity segment is tagged as a Location, but it is followed or follows a segment tagged as an Organization, we update the labels of the both the Location segment and the Organization segment to be <span id="S3.SS3.p1.1.5" class="ltx_text ltx_font_typewriter">ORG</span>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Additionally, we observe that many instances of the CoNLL-2003 validation set consist of terse reports of scores of games between sports teams (which are Organization entities), but which also semantically overlap with Location names. For example, the name “Somerset” could refer to a county in England (<span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">LOC</span>) or a cricket club (<span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_typewriter">ORG</span>). It is common for a lightly supervised model to confuse the labels to be assigned to such examples. To remedy this, we propose an additional heuristic, which identifies segments labeled as Locations (<span id="S3.SS3.p2.1.3" class="ltx_text ltx_font_typewriter">LOC</span>) and if these segments are followed a score token or at least two integer numbers resembling a score<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>We use regular expressions to detect score tokens, integer patterns, and hyphens respectively.</span></span></span>, we force their labels to be <span id="S3.SS3.p2.1.4" class="ltx_text ltx_font_typewriter">ORG</span>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.4.   One Sense Per Discourse</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Amalvy et al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> demonstrated the significance of both local and global document-level context in enhancing the efficacy of pre-trained transformer-based models for NER. In our work, we harness the document-level metadata provided in CoNLL-03 to integrate the “One Sense Per Discourse” (OSPD) principle <cite class="ltx_cite ltx_citemacro_cite">Gale et al. (<a href="#bib.bib12" title="" class="ltx_ref">1992</a>)</cite> into our neuro-symbolic approach. Primarily conceived for word sense disambiguation, OSPD posits that a term’s sense remains consistent when repeatedly used within a cohesive discourse. We operationalize this idea by asserting that if a named entity’s predominant classification within a CoNLL discourse leans towards a particular label, then all instances of that entity within the discourse should adopt this dominant label. For instance, should “IBM” appear five times in a document—thrice as an <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_typewriter">ORG</span> and twice as a <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_typewriter">LOC</span>—our method dictates that all mentions of “IBM” be labeled as <span id="S3.SS4.p1.1.3" class="ltx_text ltx_font_typewriter">ORG</span> due to its majority occurrence.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.5.   Confidence-Based Rules</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.2" class="ltx_p">In semi-supervised learning, classifier confidence can effectively guide the inclusion of unlabeled data. Nonetheless, contemporary deep neural networks often produce overconfident predictions. To harness the confidence-based heuristics outlined in <cite class="ltx_cite ltx_citemacro_cite">Liao and Veeramachaneni (<a href="#bib.bib21" title="" class="ltx_ref">2009</a>)</cite>, we adopt the “Smoothed Generalized Cross Entropy” loss from <cite class="ltx_cite ltx_citemacro_cite">Zhang and Sabuncu (<a href="#bib.bib42" title="" class="ltx_ref">2018</a>)</cite> &amp; <cite class="ltx_cite ltx_citemacro_cite">Dimachkie (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>, which has been shown to regulate and calibrate model predictions. We then include the following rules in our method: For any segment of tokens classified as an <span id="S3.SS5.p1.2.1" class="ltx_text ltx_font_typewriter">ORG</span>, <span id="S3.SS5.p1.2.2" class="ltx_text ltx_font_typewriter">LOC</span>, or <span id="S3.SS5.p1.2.3" class="ltx_text ltx_font_typewriter">PER</span> with a classifier confidence score <math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="&gt;T" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><mrow id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mi id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml"></mi><mo id="S3.SS5.p1.1.m1.1.1.1" xref="S3.SS5.p1.1.m1.1.1.1.cmml">&gt;</mo><mi id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><gt id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S3.SS5.p1.1.m1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1.2">absent</csymbol><ci id="S3.SS5.p1.1.m1.1.1.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">&gt;T</annotation></semantics></math><span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>In our experiments, we empirically set <math id="footnote8.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="footnote8.m1.1b"><mi id="footnote8.m1.1.1" xref="footnote8.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="footnote8.m1.1c"><ci id="footnote8.m1.1.1.cmml" xref="footnote8.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote8.m1.1d">T</annotation></semantics></math> to 0.9.</span></span></span>, we find other mentions of the same segment within the same CoNLL document and force their label(s) to be the same as the high confidence segment. This is known as the <span id="S3.SS5.p1.2.4" class="ltx_text ltx_font_italic">multi-mention</span> heuristic. In addition, if the high confidence segment ends in a company suffix, we remove the company suffix and apply the multi-mention property on the remaining segment. We apply the same rule for a high confidence segment that begins with a Person title (from a list of common English honorifics). Furthermore, for each segment ending in a company suffix or starting with a Person prefix, we remove the affix, while retaining the context, to form a new, previously unseen sentence which we then reclassify. For example, suppose we have a sentence in the training data with a <span id="S3.SS5.p1.2.5" class="ltx_text ltx_font_typewriter">PER</span> segment tagged with high confidence as follows: “<span id="S3.SS5.p1.2.6" class="ltx_text ltx_font_italic">The meeting was led by <span id="S3.SS5.p1.2.6.1" class="ltx_text ltx_font_bold">Ms. Taylor</span></span>.” Then, removing the Person title would yield the new sentence: “<span id="S3.SS5.p1.2.7" class="ltx_text ltx_font_italic">The meeting was led by <span id="S3.SS5.p1.2.7.1" class="ltx_text ltx_font_bold">Taylor</span></span>.” Should the predicted labels of this altered segment in the new sentence differ from those before the affix removal, especially if classified without high confidence, we designate such sentences for inclusion in subset <math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><mi id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><ci id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">D</annotation></semantics></math>. This subset is reintroduced to the training data in the subsequent semi-supervised learning iteration, as illustrated in figure <a href="#S2.F1" title="Figure 1 ‣ 2. Related Works ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We apply these confidence-based heuristics in a sieve i.e. in order of decreasing precision.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.6.   Minimizing The Dependency On A Lexicon</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">We minimize the dependency of our self-training algorithm on the lexicon chosen by the domain expert by outlining a process that they must follow for picking the lexicon. Using the simple regular expression defined in Section <a href="#S3.SS1" title="3.1. Unsupervised Entity Recognition Using A Masked Language Model (MLM) ‣ 3. Proposed Method ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> (that is able to detect named entity boundaries with high precision), we harvest named entity candidates from the CoNLL-2003 training data in a fully unsupervised manner. These candidates, ranked by their frequency of occurrence in the data, are presented to the domain expert who is tasked to select the most frequent and <span id="S3.SS6.p1.1.1" class="ltx_text ltx_font_italic">unambiguous</span> ones for each class (i.e. for each class, the lexicon should not contain entities that overlap with another class). By enforcing this criteria of objectivity, we implicitly minimize the chances of multiple domain experts picking vastly different lexicons, thereby minimizing the effects of lexicon variability on our method.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.7.   ELLEN: Integrating Neural And Symbolic Components</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.5" class="ltx_p">In Figure <a href="#S2.F1" title="Figure 1 ‣ 2. Related Works ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we depict a three-stage framework for amalgamating the heuristics and determining the subset <math id="S3.SS7.p1.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS7.p1.1.m1.1a"><mi id="S3.SS7.p1.1.m1.1.1" xref="S3.SS7.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.1.m1.1b"><ci id="S3.SS7.p1.1.m1.1.1.cmml" xref="S3.SS7.p1.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.1.m1.1c">D</annotation></semantics></math> from unlabeled data <math id="S3.SS7.p1.2.m2.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.SS7.p1.2.m2.1a"><mi id="S3.SS7.p1.2.m2.1.1" xref="S3.SS7.p1.2.m2.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.2.m2.1b"><ci id="S3.SS7.p1.2.m2.1.1.cmml" xref="S3.SS7.p1.2.m2.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.2.m2.1c">U</annotation></semantics></math> to augment the labeled set <math id="S3.SS7.p1.3.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS7.p1.3.m3.1a"><mi id="S3.SS7.p1.3.m3.1.1" xref="S3.SS7.p1.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.3.m3.1b"><ci id="S3.SS7.p1.3.m3.1.1.cmml" xref="S3.SS7.p1.3.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.3.m3.1c">L</annotation></semantics></math> in each semi-supervised learning iteration. This procedure involves three stages: initial burn-in, subsequent intermediate stage, and a concluding burn-out stage. Our selection criterion for <math id="S3.SS7.p1.4.m4.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS7.p1.4.m4.1a"><mi id="S3.SS7.p1.4.m4.1.1" xref="S3.SS7.p1.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.4.m4.1b"><ci id="S3.SS7.p1.4.m4.1.1.cmml" xref="S3.SS7.p1.4.m4.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.4.m4.1c">D</annotation></semantics></math> is straightforward: only sentences with entity label updates due to heuristics are considered. This approach aims to expand model knowledge within each cycle while curtailing classic self-training pitfalls. During burn-in, predictions from the Masked Language Modeling Heuristic (MLM) are combined with those from the NER, favoring the MLM due to the NER’s initial weakness. Confidence-based heuristics, reliant on the NER’s outputs, are deferred. Global rules and OSPD are applied solely on sentences modified by the MLM. As the NER matures through training on MLM outputs, the intermediate phase gives it precedence over the MLM; here, confidence-based heuristics solely target NER predictions, while global rules and OSPD extend to any NER or model-updated sentence. In the burn-out phase, we relax constraints, allowing full self-training. With the model now robust, it gleans any residual data from <math id="S3.SS7.p1.5.m5.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S3.SS7.p1.5.m5.1a"><mi id="S3.SS7.p1.5.m5.1.1" xref="S3.SS7.p1.5.m5.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p1.5.m5.1b"><ci id="S3.SS7.p1.5.m5.1.1.cmml" xref="S3.SS7.p1.5.m5.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p1.5.m5.1c">U</annotation></semantics></math> for a final training iteration.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Experimental Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.1.   Data &amp; Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We evaluate our method on CoNLL-2003 using three different degrees of supervision (see Table <a href="#S4.T4" title="Table 4 ‣ 4.1. Data &amp; Setup ‣ 4. Experimental Results ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). We define the “<span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">degree of supervision</span>” to be the percentage of named entities annotated, relative to the total number of entities present in the data. The first setting is the proposed extremely lightly supervised setting, equivalent to 9.13% in terms of degree of supervision or about “1%” in terms of the number of labeled sentences. We borrow the second “5%” data setting (which corresponds to the first 700 sentences in CoNLL-03’s training split) from the current state-of-the-art approaches on semi-supervised NER <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>); Zheng et al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>. However, unlike <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>, which uses <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">Fairseq</span> <cite class="ltx_cite ltx_citemacro_cite">Ott et al. (<a href="#bib.bib30" title="" class="ltx_ref">2019</a>)</cite> for augmenting the unlabeled data with equivalent back-translations from German, we sample 10,000 unlabeled sentences at random<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>We do 3 random augmentations for choosing the 10,000 unlabeled sentences. The first 700 sentences are chosen without randomization, to keep the data setting exactly the same as Semi-LADA <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite> &amp; JointProp <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>.</span></span></span>, without any augmentation. The third setting is the fully supervised setting, where we evaluate the effectiveness of ELLEN against ACE <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite>, the current SOTA method on CoNLL-03, and a DeBERTa V3 <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> classifier<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a target="_blank" href="https://huggingface.co/tner/deberta-v3-large-conll2003" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/tner/deberta-v3-large-conll2003</a></span></span></span> finetuned on CoNLL-03.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">To summarize, the three different sources of supervision in our experiments, are as follows:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">1% data setting:</span> We use the unambiguous lexicon produced by the domain expert consisting of 10 examples for each of the four CoNLL-03 classes: <span id="S4.I1.i1.p1.1.2" class="ltx_text ltx_font_typewriter">MISC</span>, <span id="S4.I1.i1.p1.1.3" class="ltx_text ltx_font_typewriter">ORG</span>, <span id="S4.I1.i1.p1.1.4" class="ltx_text ltx_font_typewriter">LOC</span> and <span id="S4.I1.i1.p1.1.5" class="ltx_text ltx_font_typewriter">PER</span>.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">5% data setting:</span> In this setting, we also extract an unambiguous lexicon from the entities within the first 700 sentences of the CoNLL-03 training split, adhering to the consistent definition of ‘unambiguous’ as described in Section <a href="#S3.SS6" title="3.6. Minimizing The Dependency On A Lexicon ‣ 3. Proposed Method ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.6</span></a>—entities within each class must not overlap with those from other classes. This lexicon, comprising 98 examples for the <span id="S4.I1.i2.p1.1.2" class="ltx_text ltx_font_typewriter">MISC</span> class, 174 for <span id="S4.I1.i2.p1.1.3" class="ltx_text ltx_font_typewriter">ORG</span>, 189 for <span id="S4.I1.i2.p1.1.4" class="ltx_text ltx_font_typewriter">LOC</span>, and 274 for <span id="S4.I1.i2.p1.1.5" class="ltx_text ltx_font_typewriter">PER</span>, is then used for annotating the unlabeled data and for the MLM.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Fully supervised setting:</span> In this setting, we <span id="S4.I1.i3.p1.1.2" class="ltx_text ltx_font_italic">do not</span> use a lexicon for annotating the data since all gold labels are available. However, since the MLM heuristic (section <a href="#S3.SS1" title="3.1. Unsupervised Entity Recognition Using A Masked Language Model (MLM) ‣ 3. Proposed Method ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>) requires a lexicon, we extract an unambiguous one just for the MLM, from all of the labeled sentences in CoNLL-03’s training split. This lexicon contains 868 examples for the <span id="S4.I1.i3.p1.1.3" class="ltx_text ltx_font_typewriter">MISC</span> class, 2329 for <span id="S4.I1.i3.p1.1.4" class="ltx_text ltx_font_typewriter">ORG</span>, 1245 for <span id="S4.I1.i3.p1.1.5" class="ltx_text ltx_font_typewriter">LOC</span>, and 3598 for <span id="S4.I1.i3.p1.1.6" class="ltx_text ltx_font_typewriter">PER</span> (refer to Appendix <a href="#A3" title="Appendix C Masked Language Model (MLM): Inverse Breaking Ties ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>).</p>
</div>
</li>
</ol>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T4.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Setting</span></th>
<td id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T4.1.1.1.2.1" class="ltx_text" style="font-size:90%;">1%</span></td>
<td id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T4.1.1.1.3.1" class="ltx_text" style="font-size:90%;">5%</span></td>
<td id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T4.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Supervised</span></td>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<th id="S4.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T4.1.2.2.1.1" class="ltx_text" style="font-size:90%;">Supervision degree</span></th>
<td id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T4.1.2.2.2.1" class="ltx_text" style="font-size:90%;">9.13%</span></td>
<td id="S4.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T4.1.2.2.3.1" class="ltx_text" style="font-size:90%;">28.5%</span></td>
<td id="S4.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T4.1.2.2.4.1" class="ltx_text" style="font-size:90%;">100%</span></td>
</tr>
<tr id="S4.T4.1.3.3" class="ltx_tr">
<th id="S4.T4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T4.1.3.3.1.1" class="ltx_text" style="font-size:90%;"># Labeled tokens</span></th>
<td id="S4.T4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T4.1.3.3.2.1" class="ltx_text" style="font-size:90%;">2569</span></td>
<td id="S4.T4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T4.1.3.3.3.1" class="ltx_text" style="font-size:90%;">6971</span></td>
<td id="S4.T4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T4.1.3.3.4.1" class="ltx_text" style="font-size:90%;">34043</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Statistics on degrees of supervision used in this work. 5% (in terms of number of sentences) is a common setting for semi-supervised NER. For the 1% and 5% settings, we calculate the supervision degree based on unambiguous lexicons.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.2.   Results Using 1% Labeled Data</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">As shown in table <a href="#S4.T5" title="Table 5 ‣ 4.2. Results Using 1% Labeled Data ‣ 4. Experimental Results ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, in the extremely lightly supervised setting, which is more restrictive than typical semi-supervised NER approaches, we find that our method achieves an F1 score of <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">76.87%</span> on the CoNLL-2003 test set. The only supervision here comes from a domain expert’s lexicon which itself does not use any gold labels from CoNLL-2003. This result indicates our method’s real-world effectiveness, where annotations are scarce and lexicons like ours are the only source(s) of supervision.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.15" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.15.16.1" class="ltx_tr">
<th id="S4.T5.15.16.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.15.16.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.15.16.1.1.1.1" class="ltx_p" style="width:24.2pt;"><span id="S4.T5.15.16.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Type</span></span>
</span>
</th>
<th id="S4.T5.15.16.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.15.16.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.15.16.1.2.1.1" class="ltx_p"><span id="S4.T5.15.16.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Precision</span></span>
</span>
</th>
<th id="S4.T5.15.16.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.15.16.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.15.16.1.3.1.1" class="ltx_p"><span id="S4.T5.15.16.1.3.1.1.1" class="ltx_text" style="font-size:90%;">Recall</span></span>
</span>
</th>
<th id="S4.T5.15.16.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.15.16.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.15.16.1.4.1.1" class="ltx_p"><span id="S4.T5.15.16.1.4.1.1.1" class="ltx_text" style="font-size:90%;">F1</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.3.3" class="ltx_tr">
<td id="S4.T5.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.4.1.1" class="ltx_p" style="width:24.2pt;"><span id="S4.T5.3.3.4.1.1.1" class="ltx_text" style="font-size:90%;">Overall</span></span>
</span>
</td>
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.1.1.1.1" class="ltx_p"><span id="S4.T5.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">74.63 </span><math id="S4.T5.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm 0.33\%" display="inline"><semantics id="S4.T5.1.1.1.1.1.m1.1a"><mrow id="S4.T5.1.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.1.1.1.1.1.m1.1.1a" xref="S4.T5.1.1.1.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.1.1.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.1.1.1.1.1.m1.1.1.2.2" xref="S4.T5.1.1.1.1.1.m1.1.1.2.2.cmml">0.33</mn><mo mathsize="70%" id="S4.T5.1.1.1.1.1.m1.1.1.2.1" xref="S4.T5.1.1.1.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1.2.2">0.33</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.1.m1.1c">\pm 0.33\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T5.2.2.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.2.2.2.1.1" class="ltx_p"><span id="S4.T5.2.2.2.1.1.1" class="ltx_text" style="font-size:90%;">79.26 </span><math id="S4.T5.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\pm 0.92\%" display="inline"><semantics id="S4.T5.2.2.2.1.1.m1.1a"><mrow id="S4.T5.2.2.2.1.1.m1.1.1" xref="S4.T5.2.2.2.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.2.2.2.1.1.m1.1.1a" xref="S4.T5.2.2.2.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.2.2.2.1.1.m1.1.1.2" xref="S4.T5.2.2.2.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.2.2.2.1.1.m1.1.1.2.2" xref="S4.T5.2.2.2.1.1.m1.1.1.2.2.cmml">0.92</mn><mo mathsize="70%" id="S4.T5.2.2.2.1.1.m1.1.1.2.1" xref="S4.T5.2.2.2.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.1.1.m1.1b"><apply id="S4.T5.2.2.2.1.1.m1.1.1.cmml" xref="S4.T5.2.2.2.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.2.2.2.1.1.m1.1.1.1.cmml" xref="S4.T5.2.2.2.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.2.2.2.1.1.m1.1.1.2.cmml" xref="S4.T5.2.2.2.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.2.2.2.1.1.m1.1.1.2.1.cmml" xref="S4.T5.2.2.2.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.2.2.2.1.1.m1.1.1.2.2.cmml" xref="S4.T5.2.2.2.1.1.m1.1.1.2.2">0.92</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.1.1.m1.1c">\pm 0.92\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T5.3.3.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.3.1.1" class="ltx_p"><span id="S4.T5.3.3.3.1.1.1" class="ltx_text" style="font-size:90%;">76.87 </span><math id="S4.T5.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\pm 0.48\%" display="inline"><semantics id="S4.T5.3.3.3.1.1.m1.1a"><mrow id="S4.T5.3.3.3.1.1.m1.1.1" xref="S4.T5.3.3.3.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.3.3.3.1.1.m1.1.1a" xref="S4.T5.3.3.3.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.3.3.3.1.1.m1.1.1.2" xref="S4.T5.3.3.3.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.3.3.3.1.1.m1.1.1.2.2" xref="S4.T5.3.3.3.1.1.m1.1.1.2.2.cmml">0.48</mn><mo mathsize="70%" id="S4.T5.3.3.3.1.1.m1.1.1.2.1" xref="S4.T5.3.3.3.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.1.1.m1.1b"><apply id="S4.T5.3.3.3.1.1.m1.1.1.cmml" xref="S4.T5.3.3.3.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.3.3.3.1.1.m1.1.1.1.cmml" xref="S4.T5.3.3.3.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.3.3.3.1.1.m1.1.1.2.cmml" xref="S4.T5.3.3.3.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.3.3.3.1.1.m1.1.1.2.1.cmml" xref="S4.T5.3.3.3.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.3.3.3.1.1.m1.1.1.2.2.cmml" xref="S4.T5.3.3.3.1.1.m1.1.1.2.2">0.48</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.1.1.m1.1c">\pm 0.48\%</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.6.6" class="ltx_tr">
<td id="S4.T5.6.6.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.6.6.4.1.1" class="ltx_p" style="width:24.2pt;"><span id="S4.T5.6.6.4.1.1.1" class="ltx_text" style="font-size:90%;">LOC</span></span>
</span>
</td>
<td id="S4.T5.4.4.1" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.4.4.1.1.1" class="ltx_p"><span id="S4.T5.4.4.1.1.1.1" class="ltx_text" style="font-size:90%;">87.92 </span><math id="S4.T5.4.4.1.1.1.m1.1" class="ltx_Math" alttext="\pm 1.21\%" display="inline"><semantics id="S4.T5.4.4.1.1.1.m1.1a"><mrow id="S4.T5.4.4.1.1.1.m1.1.1" xref="S4.T5.4.4.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.4.4.1.1.1.m1.1.1a" xref="S4.T5.4.4.1.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.4.4.1.1.1.m1.1.1.2" xref="S4.T5.4.4.1.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.4.4.1.1.1.m1.1.1.2.2" xref="S4.T5.4.4.1.1.1.m1.1.1.2.2.cmml">1.21</mn><mo mathsize="70%" id="S4.T5.4.4.1.1.1.m1.1.1.2.1" xref="S4.T5.4.4.1.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.1.1.1.m1.1b"><apply id="S4.T5.4.4.1.1.1.m1.1.1.cmml" xref="S4.T5.4.4.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.4.4.1.1.1.m1.1.1.1.cmml" xref="S4.T5.4.4.1.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.4.4.1.1.1.m1.1.1.2.cmml" xref="S4.T5.4.4.1.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.4.4.1.1.1.m1.1.1.2.1.cmml" xref="S4.T5.4.4.1.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.4.4.1.1.1.m1.1.1.2.2.cmml" xref="S4.T5.4.4.1.1.1.m1.1.1.2.2">1.21</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.1.1.1.m1.1c">\pm 1.21\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T5.5.5.2" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.5.5.2.1.1" class="ltx_p"><span id="S4.T5.5.5.2.1.1.1" class="ltx_text" style="font-size:90%;">78.68 </span><math id="S4.T5.5.5.2.1.1.m1.1" class="ltx_Math" alttext="\pm 4.76\%" display="inline"><semantics id="S4.T5.5.5.2.1.1.m1.1a"><mrow id="S4.T5.5.5.2.1.1.m1.1.1" xref="S4.T5.5.5.2.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.5.5.2.1.1.m1.1.1a" xref="S4.T5.5.5.2.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.5.5.2.1.1.m1.1.1.2" xref="S4.T5.5.5.2.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.5.5.2.1.1.m1.1.1.2.2" xref="S4.T5.5.5.2.1.1.m1.1.1.2.2.cmml">4.76</mn><mo mathsize="70%" id="S4.T5.5.5.2.1.1.m1.1.1.2.1" xref="S4.T5.5.5.2.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.2.1.1.m1.1b"><apply id="S4.T5.5.5.2.1.1.m1.1.1.cmml" xref="S4.T5.5.5.2.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.5.5.2.1.1.m1.1.1.1.cmml" xref="S4.T5.5.5.2.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.5.5.2.1.1.m1.1.1.2.cmml" xref="S4.T5.5.5.2.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.5.5.2.1.1.m1.1.1.2.1.cmml" xref="S4.T5.5.5.2.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.5.5.2.1.1.m1.1.1.2.2.cmml" xref="S4.T5.5.5.2.1.1.m1.1.1.2.2">4.76</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.2.1.1.m1.1c">\pm 4.76\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T5.6.6.3" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.6.6.3.1.1" class="ltx_p"><span id="S4.T5.6.6.3.1.1.1" class="ltx_text" style="font-size:90%;">83.04 </span><math id="S4.T5.6.6.3.1.1.m1.1" class="ltx_Math" alttext="\pm 2.36\%" display="inline"><semantics id="S4.T5.6.6.3.1.1.m1.1a"><mrow id="S4.T5.6.6.3.1.1.m1.1.1" xref="S4.T5.6.6.3.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.6.6.3.1.1.m1.1.1a" xref="S4.T5.6.6.3.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.6.6.3.1.1.m1.1.1.2" xref="S4.T5.6.6.3.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.6.6.3.1.1.m1.1.1.2.2" xref="S4.T5.6.6.3.1.1.m1.1.1.2.2.cmml">2.36</mn><mo mathsize="70%" id="S4.T5.6.6.3.1.1.m1.1.1.2.1" xref="S4.T5.6.6.3.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.6.6.3.1.1.m1.1b"><apply id="S4.T5.6.6.3.1.1.m1.1.1.cmml" xref="S4.T5.6.6.3.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.6.6.3.1.1.m1.1.1.1.cmml" xref="S4.T5.6.6.3.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.6.6.3.1.1.m1.1.1.2.cmml" xref="S4.T5.6.6.3.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.6.6.3.1.1.m1.1.1.2.1.cmml" xref="S4.T5.6.6.3.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.6.6.3.1.1.m1.1.1.2.2.cmml" xref="S4.T5.6.6.3.1.1.m1.1.1.2.2">2.36</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.6.3.1.1.m1.1c">\pm 2.36\%</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.9.9" class="ltx_tr">
<td id="S4.T5.9.9.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.9.9.4.1.1" class="ltx_p" style="width:24.2pt;"><span id="S4.T5.9.9.4.1.1.1" class="ltx_text" style="font-size:90%;">MISC</span></span>
</span>
</td>
<td id="S4.T5.7.7.1" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.7.7.1.1.1" class="ltx_p"><span id="S4.T5.7.7.1.1.1.1" class="ltx_text" style="font-size:90%;">56.32 </span><math id="S4.T5.7.7.1.1.1.m1.1" class="ltx_Math" alttext="\pm 1.82\%" display="inline"><semantics id="S4.T5.7.7.1.1.1.m1.1a"><mrow id="S4.T5.7.7.1.1.1.m1.1.1" xref="S4.T5.7.7.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.7.7.1.1.1.m1.1.1a" xref="S4.T5.7.7.1.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.7.7.1.1.1.m1.1.1.2" xref="S4.T5.7.7.1.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.7.7.1.1.1.m1.1.1.2.2" xref="S4.T5.7.7.1.1.1.m1.1.1.2.2.cmml">1.82</mn><mo mathsize="70%" id="S4.T5.7.7.1.1.1.m1.1.1.2.1" xref="S4.T5.7.7.1.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.7.7.1.1.1.m1.1b"><apply id="S4.T5.7.7.1.1.1.m1.1.1.cmml" xref="S4.T5.7.7.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.7.7.1.1.1.m1.1.1.1.cmml" xref="S4.T5.7.7.1.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.7.7.1.1.1.m1.1.1.2.cmml" xref="S4.T5.7.7.1.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.7.7.1.1.1.m1.1.1.2.1.cmml" xref="S4.T5.7.7.1.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.7.7.1.1.1.m1.1.1.2.2.cmml" xref="S4.T5.7.7.1.1.1.m1.1.1.2.2">1.82</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.7.1.1.1.m1.1c">\pm 1.82\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T5.8.8.2" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.8.8.2.1.1" class="ltx_p"><span id="S4.T5.8.8.2.1.1.1" class="ltx_text" style="font-size:90%;">61.00 </span><math id="S4.T5.8.8.2.1.1.m1.1" class="ltx_Math" alttext="\pm 1.25\%" display="inline"><semantics id="S4.T5.8.8.2.1.1.m1.1a"><mrow id="S4.T5.8.8.2.1.1.m1.1.1" xref="S4.T5.8.8.2.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.8.8.2.1.1.m1.1.1a" xref="S4.T5.8.8.2.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.8.8.2.1.1.m1.1.1.2" xref="S4.T5.8.8.2.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.8.8.2.1.1.m1.1.1.2.2" xref="S4.T5.8.8.2.1.1.m1.1.1.2.2.cmml">1.25</mn><mo mathsize="70%" id="S4.T5.8.8.2.1.1.m1.1.1.2.1" xref="S4.T5.8.8.2.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.8.8.2.1.1.m1.1b"><apply id="S4.T5.8.8.2.1.1.m1.1.1.cmml" xref="S4.T5.8.8.2.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.8.8.2.1.1.m1.1.1.1.cmml" xref="S4.T5.8.8.2.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.8.8.2.1.1.m1.1.1.2.cmml" xref="S4.T5.8.8.2.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.8.8.2.1.1.m1.1.1.2.1.cmml" xref="S4.T5.8.8.2.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.8.8.2.1.1.m1.1.1.2.2.cmml" xref="S4.T5.8.8.2.1.1.m1.1.1.2.2">1.25</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.8.8.2.1.1.m1.1c">\pm 1.25\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T5.9.9.3" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.9.9.3.1.1" class="ltx_p"><span id="S4.T5.9.9.3.1.1.1" class="ltx_text" style="font-size:90%;">58.57 </span><math id="S4.T5.9.9.3.1.1.m1.1" class="ltx_Math" alttext="\pm 0.53\%" display="inline"><semantics id="S4.T5.9.9.3.1.1.m1.1a"><mrow id="S4.T5.9.9.3.1.1.m1.1.1" xref="S4.T5.9.9.3.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.9.9.3.1.1.m1.1.1a" xref="S4.T5.9.9.3.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.9.9.3.1.1.m1.1.1.2" xref="S4.T5.9.9.3.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.9.9.3.1.1.m1.1.1.2.2" xref="S4.T5.9.9.3.1.1.m1.1.1.2.2.cmml">0.53</mn><mo mathsize="70%" id="S4.T5.9.9.3.1.1.m1.1.1.2.1" xref="S4.T5.9.9.3.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.9.9.3.1.1.m1.1b"><apply id="S4.T5.9.9.3.1.1.m1.1.1.cmml" xref="S4.T5.9.9.3.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.9.9.3.1.1.m1.1.1.1.cmml" xref="S4.T5.9.9.3.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.9.9.3.1.1.m1.1.1.2.cmml" xref="S4.T5.9.9.3.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.9.9.3.1.1.m1.1.1.2.1.cmml" xref="S4.T5.9.9.3.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.9.9.3.1.1.m1.1.1.2.2.cmml" xref="S4.T5.9.9.3.1.1.m1.1.1.2.2">0.53</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.9.9.3.1.1.m1.1c">\pm 0.53\%</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.12.12" class="ltx_tr">
<td id="S4.T5.12.12.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.12.12.4.1.1" class="ltx_p" style="width:24.2pt;"><span id="S4.T5.12.12.4.1.1.1" class="ltx_text" style="font-size:90%;">ORG</span></span>
</span>
</td>
<td id="S4.T5.10.10.1" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.10.10.1.1.1" class="ltx_p"><span id="S4.T5.10.10.1.1.1.1" class="ltx_text" style="font-size:90%;">62.29 </span><math id="S4.T5.10.10.1.1.1.m1.1" class="ltx_Math" alttext="\pm 0.39\%" display="inline"><semantics id="S4.T5.10.10.1.1.1.m1.1a"><mrow id="S4.T5.10.10.1.1.1.m1.1.1" xref="S4.T5.10.10.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.10.10.1.1.1.m1.1.1a" xref="S4.T5.10.10.1.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.10.10.1.1.1.m1.1.1.2" xref="S4.T5.10.10.1.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.10.10.1.1.1.m1.1.1.2.2" xref="S4.T5.10.10.1.1.1.m1.1.1.2.2.cmml">0.39</mn><mo mathsize="70%" id="S4.T5.10.10.1.1.1.m1.1.1.2.1" xref="S4.T5.10.10.1.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.10.10.1.1.1.m1.1b"><apply id="S4.T5.10.10.1.1.1.m1.1.1.cmml" xref="S4.T5.10.10.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.10.10.1.1.1.m1.1.1.1.cmml" xref="S4.T5.10.10.1.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.10.10.1.1.1.m1.1.1.2.cmml" xref="S4.T5.10.10.1.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.10.10.1.1.1.m1.1.1.2.1.cmml" xref="S4.T5.10.10.1.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.10.10.1.1.1.m1.1.1.2.2.cmml" xref="S4.T5.10.10.1.1.1.m1.1.1.2.2">0.39</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.10.10.1.1.1.m1.1c">\pm 0.39\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T5.11.11.2" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.11.11.2.1.1" class="ltx_p"><span id="S4.T5.11.11.2.1.1.1" class="ltx_text" style="font-size:90%;">77.26 </span><math id="S4.T5.11.11.2.1.1.m1.1" class="ltx_Math" alttext="\pm 1.64\%" display="inline"><semantics id="S4.T5.11.11.2.1.1.m1.1a"><mrow id="S4.T5.11.11.2.1.1.m1.1.1" xref="S4.T5.11.11.2.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.11.11.2.1.1.m1.1.1a" xref="S4.T5.11.11.2.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.11.11.2.1.1.m1.1.1.2" xref="S4.T5.11.11.2.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.11.11.2.1.1.m1.1.1.2.2" xref="S4.T5.11.11.2.1.1.m1.1.1.2.2.cmml">1.64</mn><mo mathsize="70%" id="S4.T5.11.11.2.1.1.m1.1.1.2.1" xref="S4.T5.11.11.2.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.11.11.2.1.1.m1.1b"><apply id="S4.T5.11.11.2.1.1.m1.1.1.cmml" xref="S4.T5.11.11.2.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.11.11.2.1.1.m1.1.1.1.cmml" xref="S4.T5.11.11.2.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.11.11.2.1.1.m1.1.1.2.cmml" xref="S4.T5.11.11.2.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.11.11.2.1.1.m1.1.1.2.1.cmml" xref="S4.T5.11.11.2.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.11.11.2.1.1.m1.1.1.2.2.cmml" xref="S4.T5.11.11.2.1.1.m1.1.1.2.2">1.64</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.11.11.2.1.1.m1.1c">\pm 1.64\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T5.12.12.3" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.12.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.12.12.3.1.1" class="ltx_p"><span id="S4.T5.12.12.3.1.1.1" class="ltx_text" style="font-size:90%;">68.97 </span><math id="S4.T5.12.12.3.1.1.m1.1" class="ltx_Math" alttext="\pm 0.51\%" display="inline"><semantics id="S4.T5.12.12.3.1.1.m1.1a"><mrow id="S4.T5.12.12.3.1.1.m1.1.1" xref="S4.T5.12.12.3.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.12.12.3.1.1.m1.1.1a" xref="S4.T5.12.12.3.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.12.12.3.1.1.m1.1.1.2" xref="S4.T5.12.12.3.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.12.12.3.1.1.m1.1.1.2.2" xref="S4.T5.12.12.3.1.1.m1.1.1.2.2.cmml">0.51</mn><mo mathsize="70%" id="S4.T5.12.12.3.1.1.m1.1.1.2.1" xref="S4.T5.12.12.3.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.12.12.3.1.1.m1.1b"><apply id="S4.T5.12.12.3.1.1.m1.1.1.cmml" xref="S4.T5.12.12.3.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.12.12.3.1.1.m1.1.1.1.cmml" xref="S4.T5.12.12.3.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.12.12.3.1.1.m1.1.1.2.cmml" xref="S4.T5.12.12.3.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.12.12.3.1.1.m1.1.1.2.1.cmml" xref="S4.T5.12.12.3.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.12.12.3.1.1.m1.1.1.2.2.cmml" xref="S4.T5.12.12.3.1.1.m1.1.1.2.2">0.51</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.12.12.3.1.1.m1.1c">\pm 0.51\%</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T5.15.15" class="ltx_tr">
<td id="S4.T5.15.15.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.15.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.15.15.4.1.1" class="ltx_p" style="width:24.2pt;"><span id="S4.T5.15.15.4.1.1.1" class="ltx_text" style="font-size:90%;">PER</span></span>
</span>
</td>
<td id="S4.T5.13.13.1" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.13.13.1.1.1" class="ltx_p"><span id="S4.T5.13.13.1.1.1.1" class="ltx_text" style="font-size:90%;">87.98 </span><math id="S4.T5.13.13.1.1.1.m1.1" class="ltx_Math" alttext="\pm 0.92\%" display="inline"><semantics id="S4.T5.13.13.1.1.1.m1.1a"><mrow id="S4.T5.13.13.1.1.1.m1.1.1" xref="S4.T5.13.13.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.13.13.1.1.1.m1.1.1a" xref="S4.T5.13.13.1.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.13.13.1.1.1.m1.1.1.2" xref="S4.T5.13.13.1.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.13.13.1.1.1.m1.1.1.2.2" xref="S4.T5.13.13.1.1.1.m1.1.1.2.2.cmml">0.92</mn><mo mathsize="70%" id="S4.T5.13.13.1.1.1.m1.1.1.2.1" xref="S4.T5.13.13.1.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.13.13.1.1.1.m1.1b"><apply id="S4.T5.13.13.1.1.1.m1.1.1.cmml" xref="S4.T5.13.13.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.13.13.1.1.1.m1.1.1.1.cmml" xref="S4.T5.13.13.1.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.13.13.1.1.1.m1.1.1.2.cmml" xref="S4.T5.13.13.1.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.13.13.1.1.1.m1.1.1.2.1.cmml" xref="S4.T5.13.13.1.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.13.13.1.1.1.m1.1.1.2.2.cmml" xref="S4.T5.13.13.1.1.1.m1.1.1.2.2">0.92</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.13.13.1.1.1.m1.1c">\pm 0.92\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T5.14.14.2" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.14.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.14.14.2.1.1" class="ltx_p"><span id="S4.T5.14.14.2.1.1.1" class="ltx_text" style="font-size:90%;">92.71 </span><math id="S4.T5.14.14.2.1.1.m1.1" class="ltx_Math" alttext="\pm 0.27\%" display="inline"><semantics id="S4.T5.14.14.2.1.1.m1.1a"><mrow id="S4.T5.14.14.2.1.1.m1.1.1" xref="S4.T5.14.14.2.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.14.14.2.1.1.m1.1.1a" xref="S4.T5.14.14.2.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.14.14.2.1.1.m1.1.1.2" xref="S4.T5.14.14.2.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.14.14.2.1.1.m1.1.1.2.2" xref="S4.T5.14.14.2.1.1.m1.1.1.2.2.cmml">0.27</mn><mo mathsize="70%" id="S4.T5.14.14.2.1.1.m1.1.1.2.1" xref="S4.T5.14.14.2.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.14.14.2.1.1.m1.1b"><apply id="S4.T5.14.14.2.1.1.m1.1.1.cmml" xref="S4.T5.14.14.2.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.14.14.2.1.1.m1.1.1.1.cmml" xref="S4.T5.14.14.2.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.14.14.2.1.1.m1.1.1.2.cmml" xref="S4.T5.14.14.2.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.14.14.2.1.1.m1.1.1.2.1.cmml" xref="S4.T5.14.14.2.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.14.14.2.1.1.m1.1.1.2.2.cmml" xref="S4.T5.14.14.2.1.1.m1.1.1.2.2">0.27</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.14.14.2.1.1.m1.1c">\pm 0.27\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T5.15.15.3" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T5.15.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.15.15.3.1.1" class="ltx_p"><span id="S4.T5.15.15.3.1.1.1" class="ltx_text" style="font-size:90%;">90.28 </span><math id="S4.T5.15.15.3.1.1.m1.1" class="ltx_Math" alttext="\pm 0.43\%" display="inline"><semantics id="S4.T5.15.15.3.1.1.m1.1a"><mrow id="S4.T5.15.15.3.1.1.m1.1.1" xref="S4.T5.15.15.3.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T5.15.15.3.1.1.m1.1.1a" xref="S4.T5.15.15.3.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T5.15.15.3.1.1.m1.1.1.2" xref="S4.T5.15.15.3.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T5.15.15.3.1.1.m1.1.1.2.2" xref="S4.T5.15.15.3.1.1.m1.1.1.2.2.cmml">0.43</mn><mo mathsize="70%" id="S4.T5.15.15.3.1.1.m1.1.1.2.1" xref="S4.T5.15.15.3.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.15.15.3.1.1.m1.1b"><apply id="S4.T5.15.15.3.1.1.m1.1.1.cmml" xref="S4.T5.15.15.3.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T5.15.15.3.1.1.m1.1.1.1.cmml" xref="S4.T5.15.15.3.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T5.15.15.3.1.1.m1.1.1.2.cmml" xref="S4.T5.15.15.3.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T5.15.15.3.1.1.m1.1.1.2.1.cmml" xref="S4.T5.15.15.3.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T5.15.15.3.1.1.m1.1.1.2.2.cmml" xref="S4.T5.15.15.3.1.1.m1.1.1.2.2">0.43</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.15.15.3.1.1.m1.1c">\pm 0.43\%</annotation></semantics></math></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Precision/Recall/F1 scores for ELLEN on CoNLL-2003 under the extremely lightly supervised setting. All of our runs are averaged over 3 random seeds. We present entity-level metrics using the official CoNLL-scoring script.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.3.   Results Using 5% Labeled Data</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Under the 5% data setting (or 28.5% in terms of “degree of supervision”), we show that our method achieves an F1 score of <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">84.87</span>% (Table <a href="#S4.T6" title="Table 6 ‣ 4.3. Results Using 5% Labeled Data ‣ 4. Experimental Results ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), outperforming more complex methods like PU learning <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite>, models based on hierarchical latent variables <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> &amp; those employing noise strategies <cite class="ltx_cite ltx_citemacro_cite">Lakshmi Narayan et al. (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>. We find that it also performs favorably compared to Semi-LADA <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite> without using any back-translations for the unlabeled data. More importantly, we highlight that our method outperforms PU-Learning approaches whilst using much fewer exemplars per class (the PU-Learning methods of <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> &amp; <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite> rely on a lexicon that contains “2,000 person names, 748 location names, 353 organization names, and 104 MISC entities”). We include Table <a href="#A1.T10" title="Table 10 ‣ Appendix A Statistics of Labeling With PU Learning Lexicon ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (Appendix <a href="#A1" title="Appendix A Statistics of Labeling With PU Learning Lexicon ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>), which is directly taken from <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite>, to illustrate this.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Furthermore, we also highlight Figure 4 from GPT-NER <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite> which shows the performance of ACE <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite> in a low-resource context. Specifically, when ACE is trained on a 1% subset (in terms of the number of sentences) of CoNLL-03’s training data, it’s F1 score falls below 20% and below 70% when trained on a 5% subset. Although a fair comparison with our method cannot be made due to differing definitions of “low resource,” it is noteworthy that ELLEN attains F1 scores of 76.87% and 84.87% under our equivalent “1%” and “5%” settings respectively, suggesting that our method significantly outperforms ACE in resource-constrained scenarios.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.3.4.1" class="ltx_tr">
<th id="S4.T6.3.4.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.4.1.1.1.1" class="ltx_p"><span id="S4.T6.3.4.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Methods</span></span>
</span>
</th>
<th id="S4.T6.3.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.4.1.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.4.1.2.1.1.1" class="ltx_text" style="font-size:90%;">P</span></span>
</span>
</th>
<th id="S4.T6.3.4.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.4.1.3.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.4.1.3.1.1.1" class="ltx_text" style="font-size:90%;">R</span></span>
</span>
</th>
<th id="S4.T6.3.4.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.4.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.4.1.4.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.4.1.4.1.1.1" class="ltx_text" style="font-size:90%;">F1</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.3.5.1" class="ltx_tr">
<td id="S4.T6.3.5.1.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.5.1.1.1.1" class="ltx_p"><span id="S4.T6.3.5.1.1.1.1.1" class="ltx_text" style="font-size:90%;">VSL-GG-Hier</span></span>
</span>
</td>
<td id="S4.T6.3.5.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.5.1.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.5.1.2.1.1.1" class="ltx_text" style="font-size:90%;">84.13%</span></span>
</span>
</td>
<td id="S4.T6.3.5.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.5.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.5.1.3.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.5.1.3.1.1.1" class="ltx_text" style="font-size:90%;">82.64%</span></span>
</span>
</td>
<td id="S4.T6.3.5.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.5.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.5.1.4.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.5.1.4.1.1.1" class="ltx_text" style="font-size:90%;">83.38%</span></span>
</span>
</td>
</tr>
<tr id="S4.T6.3.6.2" class="ltx_tr">
<td id="S4.T6.3.6.2.1" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.6.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.6.2.1.1.1" class="ltx_p"><span id="S4.T6.3.6.2.1.1.1.1" class="ltx_text" style="font-size:90%;">MT + Noise</span></span>
</span>
</td>
<td id="S4.T6.3.6.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.6.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.6.2.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.6.2.2.1.1.1" class="ltx_text" style="font-size:90%;">83.74%</span></span>
</span>
</td>
<td id="S4.T6.3.6.2.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.6.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.6.2.3.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.6.2.3.1.1.1" class="ltx_text" style="font-size:90%;">81.49%</span></span>
</span>
</td>
<td id="S4.T6.3.6.2.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.6.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.6.2.4.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.6.2.4.1.1.1" class="ltx_text" style="font-size:90%;">82.60%</span></span>
</span>
</td>
</tr>
<tr id="S4.T6.3.7.3" class="ltx_tr">
<td id="S4.T6.3.7.3.1" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.7.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.7.3.1.1.1" class="ltx_p"><span id="S4.T6.3.7.3.1.1.1.1" class="ltx_text" style="font-size:90%;">Semi-LADA</span></span>
</span>
</td>
<td id="S4.T6.3.7.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.7.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.7.3.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.7.3.2.1.1.1" class="ltx_text" style="font-size:90%;">86.93%</span></span>
</span>
</td>
<td id="S4.T6.3.7.3.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.7.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.7.3.3.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.7.3.3.1.1.1" class="ltx_text" style="font-size:90%;">85.74%</span></span>
</span>
</td>
<td id="S4.T6.3.7.3.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.7.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.7.3.4.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.7.3.4.1.1.1" class="ltx_text" style="font-size:90%;">86.33%</span></span>
</span>
</td>
</tr>
<tr id="S4.T6.3.8.4" class="ltx_tr">
<td id="S4.T6.3.8.4.1" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.8.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.8.4.1.1.1" class="ltx_p"><span id="S4.T6.3.8.4.1.1.1.1" class="ltx_text" style="font-size:90%;">Jointprop</span></span>
</span>
</td>
<td id="S4.T6.3.8.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.8.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.8.4.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.8.4.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">89.88%</span></span>
</span>
</td>
<td id="S4.T6.3.8.4.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.8.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.8.4.3.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.8.4.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">85.98%</span></span>
</span>
</td>
<td id="S4.T6.3.8.4.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.8.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.8.4.4.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.8.4.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">87.68%</span></span>
</span>
</td>
</tr>
<tr id="S4.T6.3.9.5" class="ltx_tr">
<td id="S4.T6.3.9.5.1" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.9.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.9.5.1.1.1" class="ltx_p"><span id="S4.T6.3.9.5.1.1.1.1" class="ltx_text" style="font-size:90%;">PU-Learning</span></span>
</span>
</td>
<td id="S4.T6.3.9.5.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.9.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.9.5.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.9.5.2.1.1.1" class="ltx_text" style="font-size:90%;">85.79%</span></span>
</span>
</td>
<td id="S4.T6.3.9.5.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.9.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.9.5.3.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.9.5.3.1.1.1" class="ltx_text" style="font-size:90%;">81.03%</span></span>
</span>
</td>
<td id="S4.T6.3.9.5.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.9.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.9.5.4.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.9.5.4.1.1.1" class="ltx_text" style="font-size:90%;">83.34%</span></span>
</span>
</td>
</tr>
<tr id="S4.T6.3.3" class="ltx_tr">
<td id="S4.T6.3.3.4" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.3.4.1.1" class="ltx_p"><span id="S4.T6.3.3.4.1.1.1" class="ltx_text" style="font-size:90%;">ELLEN†</span></span>
</span>
</td>
<td id="S4.T6.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.1.1.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.1.1.1.1.1.2" class="ltx_text" style="font-size:90%;">81.88 </span><span id="S4.T6.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;"> <math id="S4.T6.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm 1.18\%" display="inline"><semantics id="S4.T6.1.1.1.1.1.1.m1.1a"><mrow id="S4.T6.1.1.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.1.1.m1.1.1.cmml"><mo id="S4.T6.1.1.1.1.1.1.m1.1.1a" xref="S4.T6.1.1.1.1.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T6.1.1.1.1.1.1.m1.1.1.2" xref="S4.T6.1.1.1.1.1.1.m1.1.1.2.cmml"><mn id="S4.T6.1.1.1.1.1.1.m1.1.1.2.2" xref="S4.T6.1.1.1.1.1.1.m1.1.1.2.2.cmml">1.18</mn><mo id="S4.T6.1.1.1.1.1.1.m1.1.1.2.1" xref="S4.T6.1.1.1.1.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.1.1.m1.1b"><apply id="S4.T6.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T6.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T6.1.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T6.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T6.1.1.1.1.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T6.1.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T6.1.1.1.1.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T6.1.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T6.1.1.1.1.1.1.m1.1.1.2.2">1.18</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.1.1.m1.1c">\pm 1.18\%</annotation></semantics></math></span></span>
</span>
</td>
<td id="S4.T6.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.2.2.2.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.2.2.2.1.1.1" class="ltx_text" style="font-size:90%;">88.01 </span><math id="S4.T6.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\pm 0.19\%" display="inline"><semantics id="S4.T6.2.2.2.1.1.m1.1a"><mrow id="S4.T6.2.2.2.1.1.m1.1.1" xref="S4.T6.2.2.2.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T6.2.2.2.1.1.m1.1.1a" xref="S4.T6.2.2.2.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T6.2.2.2.1.1.m1.1.1.2" xref="S4.T6.2.2.2.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T6.2.2.2.1.1.m1.1.1.2.2" xref="S4.T6.2.2.2.1.1.m1.1.1.2.2.cmml">0.19</mn><mo mathsize="70%" id="S4.T6.2.2.2.1.1.m1.1.1.2.1" xref="S4.T6.2.2.2.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.1.1.m1.1b"><apply id="S4.T6.2.2.2.1.1.m1.1.1.cmml" xref="S4.T6.2.2.2.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T6.2.2.2.1.1.m1.1.1.1.cmml" xref="S4.T6.2.2.2.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T6.2.2.2.1.1.m1.1.1.2.cmml" xref="S4.T6.2.2.2.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T6.2.2.2.1.1.m1.1.1.2.1.cmml" xref="S4.T6.2.2.2.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T6.2.2.2.1.1.m1.1.1.2.2.cmml" xref="S4.T6.2.2.2.1.1.m1.1.1.2.2">0.19</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.1.1.m1.1c">\pm 0.19\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T6.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T6.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.3.3.1.1" class="ltx_p" style="width:37.0pt;"><span id="S4.T6.3.3.3.1.1.1" class="ltx_text" style="font-size:90%;">84.87 </span><math id="S4.T6.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\pm 0.62\%" display="inline"><semantics id="S4.T6.3.3.3.1.1.m1.1a"><mrow id="S4.T6.3.3.3.1.1.m1.1.1" xref="S4.T6.3.3.3.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T6.3.3.3.1.1.m1.1.1a" xref="S4.T6.3.3.3.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T6.3.3.3.1.1.m1.1.1.2" xref="S4.T6.3.3.3.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T6.3.3.3.1.1.m1.1.1.2.2" xref="S4.T6.3.3.3.1.1.m1.1.1.2.2.cmml">0.62</mn><mo mathsize="70%" id="S4.T6.3.3.3.1.1.m1.1.1.2.1" xref="S4.T6.3.3.3.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.1.1.m1.1b"><apply id="S4.T6.3.3.3.1.1.m1.1.1.cmml" xref="S4.T6.3.3.3.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T6.3.3.3.1.1.m1.1.1.1.cmml" xref="S4.T6.3.3.3.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T6.3.3.3.1.1.m1.1.1.2.cmml" xref="S4.T6.3.3.3.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T6.3.3.3.1.1.m1.1.1.2.1.cmml" xref="S4.T6.3.3.3.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T6.3.3.3.1.1.m1.1.1.2.2.cmml" xref="S4.T6.3.3.3.1.1.m1.1.1.2.2">0.62</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.1.1.m1.1c">\pm 0.62\%</annotation></semantics></math></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance on CoNLL 2003 with 5% labeled data. It should be noted that JointProp <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite> is a multi-task learning framework. All of our runs are averaged over 3 random seeds.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.4.   Zero-Shot Evaluation</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We apply our extremely lightly supervised (“1%”) method in a zero-shot manner on WNUT-17, a dataset from the social media domain, characterized by noisy text.
That is, using the model that was trained on CoNLL-03 with only a lexicon of 10 samples per class (provided by the domain expert), we proceed to evaluate this model on the WNUT-17 test dataset.
After aligning the predictions of each model with the label space of CoNLL-03 (see Appendix <a href="#A2" title="Appendix B WNUT-17: Zero-Shot Evaluation ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> for details), we observe that ELLEN achieves comparable zero-shot performance to GPT-3.5 and GPT-4, and also achieves relatively strong zero-shot performance against a <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_italic">fully supervised</span> model<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>We use the RoBERTa large model, available here: <a target="_blank" href="https://huggingface.co/tner/roberta-large-wnut2017" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/tner/roberta-large-wnut2017</a></span></span></span> from the the T-NER library <cite class="ltx_cite ltx_citemacro_cite">Ushio and Camacho-Collados (<a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite> that was actually trained on WNUT-17 gold data (see Table <a href="#S4.T7" title="Table 7 ‣ 4.4. Zero-Shot Evaluation ‣ 4. Experimental Results ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). This result is exciting because it indicates the potential for our method to be used across domains, given the relatively small size of our model and its extremely light supervision.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.5" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.5.6.1" class="ltx_tr">
<th id="S4.T7.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T7.5.6.1.1.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<th id="S4.T7.5.6.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T7.5.6.1.2.1" class="ltx_text" style="font-size:90%;">LOC</span></th>
<th id="S4.T7.5.6.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T7.5.6.1.3.1" class="ltx_text" style="font-size:90%;">MISC</span></th>
<th id="S4.T7.5.6.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T7.5.6.1.4.1" class="ltx_text" style="font-size:90%;">ORG</span></th>
<th id="S4.T7.5.6.1.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T7.5.6.1.5.1" class="ltx_text" style="font-size:90%;">PER</span></th>
<th id="S4.T7.5.6.1.6" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T7.5.6.1.6.1" class="ltx_text" style="font-size:90%;">AVG</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.5.7.1" class="ltx_tr">
<th id="S4.T7.5.7.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T7.5.7.1.1.1" class="ltx_text" style="font-size:90%;">T-NER</span></th>
<td id="S4.T7.5.7.1.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.7.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.7.1.2.1.1" class="ltx_p"><span id="S4.T7.5.7.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">64.21%</span></span>
</span>
</td>
<td id="S4.T7.5.7.1.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.7.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.7.1.3.1.1" class="ltx_p"><span id="S4.T7.5.7.1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">42.04%</span></span>
</span>
</td>
<td id="S4.T7.5.7.1.4" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.7.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.7.1.4.1.1" class="ltx_p"><span id="S4.T7.5.7.1.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">42.98%</span></span>
</span>
</td>
<td id="S4.T7.5.7.1.5" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.7.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.7.1.5.1.1" class="ltx_p"><span id="S4.T7.5.7.1.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">66.11%</span></span>
</span>
</td>
<td id="S4.T7.5.7.1.6" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.7.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.7.1.6.1.1" class="ltx_p"><span id="S4.T7.5.7.1.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">55.11%</span></span>
</span>
</td>
</tr>
<tr id="S4.T7.5.8.2" class="ltx_tr">
<th id="S4.T7.5.8.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T7.5.8.2.1.1" class="ltx_text" style="font-size:90%;">GPT-3.5</span></th>
<td id="S4.T7.5.8.2.2" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.8.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.8.2.2.1.1" class="ltx_p"><span id="S4.T7.5.8.2.2.1.1.1" class="ltx_text" style="font-size:90%;">49.17%</span></span>
</span>
</td>
<td id="S4.T7.5.8.2.3" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.8.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.8.2.3.1.1" class="ltx_p"><span id="S4.T7.5.8.2.3.1.1.1" class="ltx_text" style="font-size:90%;">8.06%</span></span>
</span>
</td>
<td id="S4.T7.5.8.2.4" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.8.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.8.2.4.1.1" class="ltx_p"><span id="S4.T7.5.8.2.4.1.1.1" class="ltx_text" style="font-size:90%;">29.71%</span></span>
</span>
</td>
<td id="S4.T7.5.8.2.5" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.8.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.8.2.5.1.1" class="ltx_p"><span id="S4.T7.5.8.2.5.1.1.1" class="ltx_text" style="font-size:90%;">59.84%</span></span>
</span>
</td>
<td id="S4.T7.5.8.2.6" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.8.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.8.2.6.1.1" class="ltx_p"><span id="S4.T7.5.8.2.6.1.1.1" class="ltx_text" style="font-size:90%;">39.96%</span></span>
</span>
</td>
</tr>
<tr id="S4.T7.5.9.3" class="ltx_tr">
<th id="S4.T7.5.9.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T7.5.9.3.1.1" class="ltx_text" style="font-size:90%;">GPT-4</span></th>
<td id="S4.T7.5.9.3.2" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.9.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.9.3.2.1.1" class="ltx_p"><span id="S4.T7.5.9.3.2.1.1.1" class="ltx_text" style="font-size:90%;">58.70%</span></span>
</span>
</td>
<td id="S4.T7.5.9.3.3" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.9.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.9.3.3.1.1" class="ltx_p"><span id="S4.T7.5.9.3.3.1.1.1" class="ltx_text" style="font-size:90%;">25.40%</span></span>
</span>
</td>
<td id="S4.T7.5.9.3.4" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.9.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.9.3.4.1.1" class="ltx_p"><span id="S4.T7.5.9.3.4.1.1.1" class="ltx_text" style="font-size:90%;">38.05%</span></span>
</span>
</td>
<td id="S4.T7.5.9.3.5" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.9.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.9.3.5.1.1" class="ltx_p"><span id="S4.T7.5.9.3.5.1.1.1" class="ltx_text" style="font-size:90%;">56.87%</span></span>
</span>
</td>
<td id="S4.T7.5.9.3.6" class="ltx_td ltx_align_justify" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.9.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.9.3.6.1.1" class="ltx_p"><span id="S4.T7.5.9.3.6.1.1.1" class="ltx_text" style="font-size:90%;">43.72%</span></span>
</span>
</td>
</tr>
<tr id="S4.T7.5.5" class="ltx_tr">
<th id="S4.T7.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T7.5.5.6.1" class="ltx_text" style="font-size:90%;">ELLEN†</span></th>
<td id="S4.T7.1.1.1" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.1.1.1.1" class="ltx_p"><span id="S4.T7.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">44.82 </span><math id="S4.T7.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm 3.84\%" display="inline"><semantics id="S4.T7.1.1.1.1.1.m1.1a"><mrow id="S4.T7.1.1.1.1.1.m1.1.1" xref="S4.T7.1.1.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T7.1.1.1.1.1.m1.1.1a" xref="S4.T7.1.1.1.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T7.1.1.1.1.1.m1.1.1.2" xref="S4.T7.1.1.1.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T7.1.1.1.1.1.m1.1.1.2.2" xref="S4.T7.1.1.1.1.1.m1.1.1.2.2.cmml">3.84</mn><mo mathsize="70%" id="S4.T7.1.1.1.1.1.m1.1.1.2.1" xref="S4.T7.1.1.1.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.1.1.m1.1b"><apply id="S4.T7.1.1.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T7.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T7.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T7.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T7.1.1.1.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T7.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T7.1.1.1.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T7.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T7.1.1.1.1.1.m1.1.1.2.2">3.84</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.1.1.m1.1c">\pm 3.84\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T7.2.2.2" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.2.2.2.1.1" class="ltx_p"><span id="S4.T7.2.2.2.1.1.1" class="ltx_text" style="font-size:90%;">6.21 </span><math id="S4.T7.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\pm 1.25\%" display="inline"><semantics id="S4.T7.2.2.2.1.1.m1.1a"><mrow id="S4.T7.2.2.2.1.1.m1.1.1" xref="S4.T7.2.2.2.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T7.2.2.2.1.1.m1.1.1a" xref="S4.T7.2.2.2.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T7.2.2.2.1.1.m1.1.1.2" xref="S4.T7.2.2.2.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T7.2.2.2.1.1.m1.1.1.2.2" xref="S4.T7.2.2.2.1.1.m1.1.1.2.2.cmml">1.25</mn><mo mathsize="70%" id="S4.T7.2.2.2.1.1.m1.1.1.2.1" xref="S4.T7.2.2.2.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.2.1.1.m1.1b"><apply id="S4.T7.2.2.2.1.1.m1.1.1.cmml" xref="S4.T7.2.2.2.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T7.2.2.2.1.1.m1.1.1.1.cmml" xref="S4.T7.2.2.2.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T7.2.2.2.1.1.m1.1.1.2.cmml" xref="S4.T7.2.2.2.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T7.2.2.2.1.1.m1.1.1.2.1.cmml" xref="S4.T7.2.2.2.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T7.2.2.2.1.1.m1.1.1.2.2.cmml" xref="S4.T7.2.2.2.1.1.m1.1.1.2.2">1.25</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.2.1.1.m1.1c">\pm 1.25\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T7.3.3.3" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.3.3.3.1.1" class="ltx_p"><span id="S4.T7.3.3.3.1.1.1" class="ltx_text" style="font-size:90%;">26.49 </span><math id="S4.T7.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\pm 5.01\%" display="inline"><semantics id="S4.T7.3.3.3.1.1.m1.1a"><mrow id="S4.T7.3.3.3.1.1.m1.1.1" xref="S4.T7.3.3.3.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T7.3.3.3.1.1.m1.1.1a" xref="S4.T7.3.3.3.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T7.3.3.3.1.1.m1.1.1.2" xref="S4.T7.3.3.3.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T7.3.3.3.1.1.m1.1.1.2.2" xref="S4.T7.3.3.3.1.1.m1.1.1.2.2.cmml">5.01</mn><mo mathsize="70%" id="S4.T7.3.3.3.1.1.m1.1.1.2.1" xref="S4.T7.3.3.3.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.3.1.1.m1.1b"><apply id="S4.T7.3.3.3.1.1.m1.1.1.cmml" xref="S4.T7.3.3.3.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T7.3.3.3.1.1.m1.1.1.1.cmml" xref="S4.T7.3.3.3.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T7.3.3.3.1.1.m1.1.1.2.cmml" xref="S4.T7.3.3.3.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T7.3.3.3.1.1.m1.1.1.2.1.cmml" xref="S4.T7.3.3.3.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T7.3.3.3.1.1.m1.1.1.2.2.cmml" xref="S4.T7.3.3.3.1.1.m1.1.1.2.2">5.01</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.3.1.1.m1.1c">\pm 5.01\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T7.4.4.4" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.4.4.4.1.1" class="ltx_p"><span id="S4.T7.4.4.4.1.1.1" class="ltx_text" style="font-size:90%;">67.00 </span><math id="S4.T7.4.4.4.1.1.m1.1" class="ltx_Math" alttext="\pm 3.54\%" display="inline"><semantics id="S4.T7.4.4.4.1.1.m1.1a"><mrow id="S4.T7.4.4.4.1.1.m1.1.1" xref="S4.T7.4.4.4.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T7.4.4.4.1.1.m1.1.1a" xref="S4.T7.4.4.4.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T7.4.4.4.1.1.m1.1.1.2" xref="S4.T7.4.4.4.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T7.4.4.4.1.1.m1.1.1.2.2" xref="S4.T7.4.4.4.1.1.m1.1.1.2.2.cmml">3.54</mn><mo mathsize="70%" id="S4.T7.4.4.4.1.1.m1.1.1.2.1" xref="S4.T7.4.4.4.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.4.4.4.1.1.m1.1b"><apply id="S4.T7.4.4.4.1.1.m1.1.1.cmml" xref="S4.T7.4.4.4.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T7.4.4.4.1.1.m1.1.1.1.cmml" xref="S4.T7.4.4.4.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T7.4.4.4.1.1.m1.1.1.2.cmml" xref="S4.T7.4.4.4.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T7.4.4.4.1.1.m1.1.1.2.1.cmml" xref="S4.T7.4.4.4.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T7.4.4.4.1.1.m1.1.1.2.2.cmml" xref="S4.T7.4.4.4.1.1.m1.1.1.2.2">3.54</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.4.4.4.1.1.m1.1c">\pm 3.54\%</annotation></semantics></math></span>
</span>
</td>
<td id="S4.T7.5.5.5" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T7.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.5.5.5.1.1" class="ltx_p"><span id="S4.T7.5.5.5.1.1.1" class="ltx_text" style="font-size:90%;">41.56 </span><math id="S4.T7.5.5.5.1.1.m1.1" class="ltx_Math" alttext="\pm 0.92\%" display="inline"><semantics id="S4.T7.5.5.5.1.1.m1.1a"><mrow id="S4.T7.5.5.5.1.1.m1.1.1" xref="S4.T7.5.5.5.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T7.5.5.5.1.1.m1.1.1a" xref="S4.T7.5.5.5.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T7.5.5.5.1.1.m1.1.1.2" xref="S4.T7.5.5.5.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T7.5.5.5.1.1.m1.1.1.2.2" xref="S4.T7.5.5.5.1.1.m1.1.1.2.2.cmml">0.92</mn><mo mathsize="70%" id="S4.T7.5.5.5.1.1.m1.1.1.2.1" xref="S4.T7.5.5.5.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.5.5.5.1.1.m1.1b"><apply id="S4.T7.5.5.5.1.1.m1.1.1.cmml" xref="S4.T7.5.5.5.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T7.5.5.5.1.1.m1.1.1.1.cmml" xref="S4.T7.5.5.5.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T7.5.5.5.1.1.m1.1.1.2.cmml" xref="S4.T7.5.5.5.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T7.5.5.5.1.1.m1.1.1.2.1.cmml" xref="S4.T7.5.5.5.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T7.5.5.5.1.1.m1.1.1.2.2.cmml" xref="S4.T7.5.5.5.1.1.m1.1.1.2.2">0.92</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.5.5.5.1.1.m1.1c">\pm 0.92\%</annotation></semantics></math></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>Comparing F1 scores: ELLEN, GPT-3.5, and GPT-4 are evaluated in zero-shot mode against T-NER’s fully supervised model on the WNUT-17 test set, after label alignment with CoNLL-03 († indicates our framework). For ELLEN, we report the average zero-shot score of 3 different random initializations and training runs of the models under extremely light supervision.</figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.5.   Results Using Full Supervision</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Lastly, we show that our method can also be adapted to a fully-supervised setting. Table <a href="#S4.T8" title="Table 8 ‣ 4.5. Results Using Full Supervision ‣ 4. Experimental Results ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows that we obtain a respectable F1 score of <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_bold">90.98%</span> relative to ACE (<span id="S4.SS5.p1.1.2" class="ltx_text ltx_font_bold">94.6%</span>) and a standard supervised classifier (<span id="S4.SS5.p1.1.3" class="ltx_text ltx_font_bold">92.2%</span>). We note that, while our neuro-symbolic approach is effective in low resource settings, when full supervision is available, other methods may outperform ours. This is primarily due to the noise introduced by the various heuristics we propose in Section <a href="#S3" title="3. Proposed Method ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (MLM, One Sense Per Discourse, confidence-based rules), which may erroneously annotate <span id="S4.SS5.p1.1.4" class="ltx_text ltx_font_typewriter">O</span>/Outside entities as belonging to a non-<span id="S4.SS5.p1.1.5" class="ltx_text ltx_font_typewriter">O</span> class, leading to our model being iteratively retrained on some noisy data (as shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2. Related Works ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="S4.T8" class="ltx_table">
<table id="S4.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T8.1.2.1" class="ltx_tr">
<th id="S4.T8.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T8.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="S4.T8.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T8.1.2.1.2.1" class="ltx_text" style="font-size:90%;">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T8.1.1" class="ltx_tr">
<th id="S4.T8.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T8.1.1.2.1" class="ltx_text" style="font-size:90%;">ELLEN</span></th>
<td id="S4.T8.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T8.1.1.1.1" class="ltx_text" style="font-size:90%;">90.98 </span><math id="S4.T8.1.1.1.m1.1" class="ltx_Math" alttext="\pm 0.54\%" display="inline"><semantics id="S4.T8.1.1.1.m1.1a"><mrow id="S4.T8.1.1.1.m1.1.1" xref="S4.T8.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T8.1.1.1.m1.1.1a" xref="S4.T8.1.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T8.1.1.1.m1.1.1.2" xref="S4.T8.1.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T8.1.1.1.m1.1.1.2.2" xref="S4.T8.1.1.1.m1.1.1.2.2.cmml">0.54</mn><mo mathsize="70%" id="S4.T8.1.1.1.m1.1.1.2.1" xref="S4.T8.1.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.1.1.1.m1.1b"><apply id="S4.T8.1.1.1.m1.1.1.cmml" xref="S4.T8.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T8.1.1.1.m1.1.1.1.cmml" xref="S4.T8.1.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T8.1.1.1.m1.1.1.2.cmml" xref="S4.T8.1.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T8.1.1.1.m1.1.1.2.1.cmml" xref="S4.T8.1.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T8.1.1.1.m1.1.1.2.2.cmml" xref="S4.T8.1.1.1.m1.1.1.2.2">0.54</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.1.1.1.m1.1c">\pm 0.54\%</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T8.1.3.1" class="ltx_tr">
<th id="S4.T8.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T8.1.3.1.1.1" class="ltx_text" style="font-size:90%;">DeBERTa V3</span></th>
<td id="S4.T8.1.3.1.2" class="ltx_td ltx_align_center" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T8.1.3.1.2.1" class="ltx_text" style="font-size:90%;">92.2%</span></td>
</tr>
<tr id="S4.T8.1.4.2" class="ltx_tr">
<th id="S4.T8.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S4.T8.1.4.2.1.1" class="ltx_text" style="font-size:90%;">ACE </span><cite class="ltx_cite ltx_citemacro_cite">Wang et al. <span id="S4.T8.1.4.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">(</span><a href="#bib.bib39" title="" class="ltx_ref">2021</a><span id="S4.T8.1.4.2.1.3.2.2.1" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S4.T8.1.4.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="S4.T8.1.4.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">94.6%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 8: </span>Performance of ELLEN on CoNLL-2003 test when using all available annotations from the CoNLL-2003 training data (fully supervised setting). For ELLEN, we report an average of training runs over 3 random seeds.</figcaption>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.6.   Error Analysis And Ablation Experiment</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">Focusing on the extremely lightly supervised setting for CoNLL-03, we observed that over 30% of model errors on validation data involve confusing <span id="S4.SS6.p1.1.1" class="ltx_text ltx_font_typewriter">ORG</span> and <span id="S4.SS6.p1.1.2" class="ltx_text ltx_font_typewriter">LOC</span> entities. These errors can be attributed to a combination of factors: a) a bias in CoNLL-03’s validation and test data towards sporting events not sufficiently reflected in the training data, b) the inadequacy of a global rule (refer to Section <a href="#S3.SS3" title="3.3. Global Rules ‣ 3. Proposed Method ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>) to differentiate between sports teams (<span id="S4.SS6.p1.1.3" class="ltx_text ltx_font_typewriter">ORG</span>) and locations (<span id="S4.SS6.p1.1.4" class="ltx_text ltx_font_typewriter">LOC</span>) in nuanced contexts, e.g., both ‘YORKSHIRE’ and ‘HEADINGLEY’ would be labeled as <span id="S4.SS6.p1.1.5" class="ltx_text ltx_font_typewriter">ORG</span>’s in the sentence: “YORKSHIRE AT HEADINGLEY” even though ‘HEADINGLEY’ is a <span id="S4.SS6.p1.1.6" class="ltx_text ltx_font_typewriter">LOC</span>. c) errors arising from noisy POS tags and incorrectly identified entity spans, e.g., “Dhaka Stock Exchange” would be identified as two separate entities “Dhaka” and “Stock Exchange” by the regular expression, leading to incorrect labels by the MLM Heuristic during training; and d) confusion between <span id="S4.SS6.p1.1.7" class="ltx_text ltx_font_typewriter">ORG</span> and <span id="S4.SS6.p1.1.8" class="ltx_text ltx_font_typewriter">MISC</span> classes, partly because the <span id="S4.SS6.p1.1.9" class="ltx_text ltx_font_typewriter">MISC</span> class lexicon primarily includes nationalities, which does not fully represent its broader scope (e.g., events, products, works of art).</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">In an ablation on the CoNLL-2003 dev set (Table <a href="#S4.T9" title="Table 9 ‣ 4.6. Error Analysis And Ablation Experiment ‣ 4. Experimental Results ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>), we found the MLM to be the most impactful component. This was followed by dynamic window filtering, which allows our method to achieve a <span id="S4.SS6.p2.1.1" class="ltx_text ltx_font_bold">64.7%</span> F1 score on its own. Importantly, reintegrating other components—OSPD, confidence-based heuristics, and global rules—each further enhances performance, underscoring their collective contribution to the method’s effectiveness.</p>
</div>
<figure id="S4.T9" class="ltx_table">
<table id="S4.T9.12" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T9.12.13.1" class="ltx_tr">
<th id="S4.T9.12.13.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding:2.25pt 4.0pt;"><span id="S4.T9.12.13.1.1.1" class="ltx_text" style="font-size:90%;">Ablations</span></th>
<th id="S4.T9.12.13.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:2.25pt 4.0pt;"><span id="S4.T9.12.13.1.2.1" class="ltx_text" style="font-size:90%;">P</span></th>
<th id="S4.T9.12.13.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:2.25pt 4.0pt;"><span id="S4.T9.12.13.1.3.1" class="ltx_text" style="font-size:90%;">R</span></th>
<th id="S4.T9.12.13.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:2.25pt 4.0pt;"><span id="S4.T9.12.13.1.4.1" class="ltx_text" style="font-size:90%;">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T9.3.3" class="ltx_tr">
<th id="S4.T9.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:2.25pt 4.0pt;"><span id="S4.T9.3.3.4.1" class="ltx_text" style="font-size:90%;">Full system</span></th>
<td id="S4.T9.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.25pt 4.0pt;">
<span id="S4.T9.1.1.1.1" class="ltx_text" style="font-size:90%;">71.31 </span><math id="S4.T9.1.1.1.m1.1" class="ltx_Math" alttext="\pm 2.4\%" display="inline"><semantics id="S4.T9.1.1.1.m1.1a"><mrow id="S4.T9.1.1.1.m1.1.1" xref="S4.T9.1.1.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T9.1.1.1.m1.1.1a" xref="S4.T9.1.1.1.m1.1.1.cmml">±</mo><mrow id="S4.T9.1.1.1.m1.1.1.2" xref="S4.T9.1.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T9.1.1.1.m1.1.1.2.2" xref="S4.T9.1.1.1.m1.1.1.2.2.cmml">2.4</mn><mo mathsize="70%" id="S4.T9.1.1.1.m1.1.1.2.1" xref="S4.T9.1.1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.1.1.1.m1.1b"><apply id="S4.T9.1.1.1.m1.1.1.cmml" xref="S4.T9.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T9.1.1.1.m1.1.1.1.cmml" xref="S4.T9.1.1.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T9.1.1.1.m1.1.1.2.cmml" xref="S4.T9.1.1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T9.1.1.1.m1.1.1.2.1.cmml" xref="S4.T9.1.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T9.1.1.1.m1.1.1.2.2.cmml" xref="S4.T9.1.1.1.m1.1.1.2.2">2.4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.1.1.1.m1.1c">\pm 2.4\%</annotation></semantics></math>
</td>
<td id="S4.T9.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.25pt 4.0pt;">
<span id="S4.T9.2.2.2.1" class="ltx_text" style="font-size:90%;">75.90 </span><math id="S4.T9.2.2.2.m1.1" class="ltx_Math" alttext="\pm 0.9\%" display="inline"><semantics id="S4.T9.2.2.2.m1.1a"><mrow id="S4.T9.2.2.2.m1.1.1" xref="S4.T9.2.2.2.m1.1.1.cmml"><mo mathsize="70%" id="S4.T9.2.2.2.m1.1.1a" xref="S4.T9.2.2.2.m1.1.1.cmml">±</mo><mrow id="S4.T9.2.2.2.m1.1.1.2" xref="S4.T9.2.2.2.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T9.2.2.2.m1.1.1.2.2" xref="S4.T9.2.2.2.m1.1.1.2.2.cmml">0.9</mn><mo mathsize="70%" id="S4.T9.2.2.2.m1.1.1.2.1" xref="S4.T9.2.2.2.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.2.2.2.m1.1b"><apply id="S4.T9.2.2.2.m1.1.1.cmml" xref="S4.T9.2.2.2.m1.1.1"><csymbol cd="latexml" id="S4.T9.2.2.2.m1.1.1.1.cmml" xref="S4.T9.2.2.2.m1.1.1">plus-or-minus</csymbol><apply id="S4.T9.2.2.2.m1.1.1.2.cmml" xref="S4.T9.2.2.2.m1.1.1.2"><csymbol cd="latexml" id="S4.T9.2.2.2.m1.1.1.2.1.cmml" xref="S4.T9.2.2.2.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T9.2.2.2.m1.1.1.2.2.cmml" xref="S4.T9.2.2.2.m1.1.1.2.2">0.9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.2.2.2.m1.1c">\pm 0.9\%</annotation></semantics></math>
</td>
<td id="S4.T9.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:2.25pt 4.0pt;">
<span id="S4.T9.3.3.3.1" class="ltx_text" style="font-size:90%;">73.52 </span><math id="S4.T9.3.3.3.m1.1" class="ltx_Math" alttext="\pm 1.6\%" display="inline"><semantics id="S4.T9.3.3.3.m1.1a"><mrow id="S4.T9.3.3.3.m1.1.1" xref="S4.T9.3.3.3.m1.1.1.cmml"><mo mathsize="70%" id="S4.T9.3.3.3.m1.1.1a" xref="S4.T9.3.3.3.m1.1.1.cmml">±</mo><mrow id="S4.T9.3.3.3.m1.1.1.2" xref="S4.T9.3.3.3.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T9.3.3.3.m1.1.1.2.2" xref="S4.T9.3.3.3.m1.1.1.2.2.cmml">1.6</mn><mo mathsize="70%" id="S4.T9.3.3.3.m1.1.1.2.1" xref="S4.T9.3.3.3.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.3.3.3.m1.1b"><apply id="S4.T9.3.3.3.m1.1.1.cmml" xref="S4.T9.3.3.3.m1.1.1"><csymbol cd="latexml" id="S4.T9.3.3.3.m1.1.1.1.cmml" xref="S4.T9.3.3.3.m1.1.1">plus-or-minus</csymbol><apply id="S4.T9.3.3.3.m1.1.1.2.cmml" xref="S4.T9.3.3.3.m1.1.1.2"><csymbol cd="latexml" id="S4.T9.3.3.3.m1.1.1.2.1.cmml" xref="S4.T9.3.3.3.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T9.3.3.3.m1.1.1.2.2.cmml" xref="S4.T9.3.3.3.m1.1.1.2.2">1.6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.3.3.3.m1.1c">\pm 1.6\%</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T9.6.6" class="ltx_tr">
<th id="S4.T9.6.6.4" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:2.25pt 4.0pt;"><span id="S4.T9.6.6.4.1" class="ltx_text" style="font-size:90%;">MLM</span></th>
<td id="S4.T9.4.4.1" class="ltx_td ltx_align_center" style="padding:2.25pt 4.0pt;">
<span id="S4.T9.4.4.1.1" class="ltx_text" style="font-size:90%;">61.96 </span><math id="S4.T9.4.4.1.m1.1" class="ltx_Math" alttext="\pm 1.1\%" display="inline"><semantics id="S4.T9.4.4.1.m1.1a"><mrow id="S4.T9.4.4.1.m1.1.1" xref="S4.T9.4.4.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T9.4.4.1.m1.1.1a" xref="S4.T9.4.4.1.m1.1.1.cmml">±</mo><mrow id="S4.T9.4.4.1.m1.1.1.2" xref="S4.T9.4.4.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T9.4.4.1.m1.1.1.2.2" xref="S4.T9.4.4.1.m1.1.1.2.2.cmml">1.1</mn><mo mathsize="70%" id="S4.T9.4.4.1.m1.1.1.2.1" xref="S4.T9.4.4.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.4.4.1.m1.1b"><apply id="S4.T9.4.4.1.m1.1.1.cmml" xref="S4.T9.4.4.1.m1.1.1"><csymbol cd="latexml" id="S4.T9.4.4.1.m1.1.1.1.cmml" xref="S4.T9.4.4.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T9.4.4.1.m1.1.1.2.cmml" xref="S4.T9.4.4.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T9.4.4.1.m1.1.1.2.1.cmml" xref="S4.T9.4.4.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T9.4.4.1.m1.1.1.2.2.cmml" xref="S4.T9.4.4.1.m1.1.1.2.2">1.1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.4.4.1.m1.1c">\pm 1.1\%</annotation></semantics></math>
</td>
<td id="S4.T9.5.5.2" class="ltx_td ltx_align_center" style="padding:2.25pt 4.0pt;">
<span id="S4.T9.5.5.2.1" class="ltx_text" style="font-size:90%;">74.00 </span><math id="S4.T9.5.5.2.m1.1" class="ltx_Math" alttext="\pm 1.0\%" display="inline"><semantics id="S4.T9.5.5.2.m1.1a"><mrow id="S4.T9.5.5.2.m1.1.1" xref="S4.T9.5.5.2.m1.1.1.cmml"><mo mathsize="70%" id="S4.T9.5.5.2.m1.1.1a" xref="S4.T9.5.5.2.m1.1.1.cmml">±</mo><mrow id="S4.T9.5.5.2.m1.1.1.2" xref="S4.T9.5.5.2.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T9.5.5.2.m1.1.1.2.2" xref="S4.T9.5.5.2.m1.1.1.2.2.cmml">1.0</mn><mo mathsize="70%" id="S4.T9.5.5.2.m1.1.1.2.1" xref="S4.T9.5.5.2.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.5.5.2.m1.1b"><apply id="S4.T9.5.5.2.m1.1.1.cmml" xref="S4.T9.5.5.2.m1.1.1"><csymbol cd="latexml" id="S4.T9.5.5.2.m1.1.1.1.cmml" xref="S4.T9.5.5.2.m1.1.1">plus-or-minus</csymbol><apply id="S4.T9.5.5.2.m1.1.1.2.cmml" xref="S4.T9.5.5.2.m1.1.1.2"><csymbol cd="latexml" id="S4.T9.5.5.2.m1.1.1.2.1.cmml" xref="S4.T9.5.5.2.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T9.5.5.2.m1.1.1.2.2.cmml" xref="S4.T9.5.5.2.m1.1.1.2.2">1.0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.5.5.2.m1.1c">\pm 1.0\%</annotation></semantics></math>
</td>
<td id="S4.T9.6.6.3" class="ltx_td ltx_align_center" style="padding:2.25pt 4.0pt;">
<span id="S4.T9.6.6.3.1" class="ltx_text" style="font-size:90%;">67.40 </span><math id="S4.T9.6.6.3.m1.1" class="ltx_Math" alttext="\pm 0.2\%" display="inline"><semantics id="S4.T9.6.6.3.m1.1a"><mrow id="S4.T9.6.6.3.m1.1.1" xref="S4.T9.6.6.3.m1.1.1.cmml"><mo mathsize="70%" id="S4.T9.6.6.3.m1.1.1a" xref="S4.T9.6.6.3.m1.1.1.cmml">±</mo><mrow id="S4.T9.6.6.3.m1.1.1.2" xref="S4.T9.6.6.3.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T9.6.6.3.m1.1.1.2.2" xref="S4.T9.6.6.3.m1.1.1.2.2.cmml">0.2</mn><mo mathsize="70%" id="S4.T9.6.6.3.m1.1.1.2.1" xref="S4.T9.6.6.3.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.6.6.3.m1.1b"><apply id="S4.T9.6.6.3.m1.1.1.cmml" xref="S4.T9.6.6.3.m1.1.1"><csymbol cd="latexml" id="S4.T9.6.6.3.m1.1.1.1.cmml" xref="S4.T9.6.6.3.m1.1.1">plus-or-minus</csymbol><apply id="S4.T9.6.6.3.m1.1.1.2.cmml" xref="S4.T9.6.6.3.m1.1.1.2"><csymbol cd="latexml" id="S4.T9.6.6.3.m1.1.1.2.1.cmml" xref="S4.T9.6.6.3.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T9.6.6.3.m1.1.1.2.2.cmml" xref="S4.T9.6.6.3.m1.1.1.2.2">0.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.6.6.3.m1.1c">\pm 0.2\%</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T9.9.9" class="ltx_tr">
<th id="S4.T9.9.9.4" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:2.25pt 4.0pt;"><span id="S4.T9.9.9.4.1" class="ltx_text" style="font-size:90%;">CR, GR,</span></th>
<td id="S4.T9.7.7.1" class="ltx_td ltx_align_center" style="padding:2.25pt 4.0pt;">
<span id="S4.T9.7.7.1.1" class="ltx_text" style="font-size:90%;">70.30 </span><math id="S4.T9.7.7.1.m1.1" class="ltx_Math" alttext="\pm 3.5\%" display="inline"><semantics id="S4.T9.7.7.1.m1.1a"><mrow id="S4.T9.7.7.1.m1.1.1" xref="S4.T9.7.7.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T9.7.7.1.m1.1.1a" xref="S4.T9.7.7.1.m1.1.1.cmml">±</mo><mrow id="S4.T9.7.7.1.m1.1.1.2" xref="S4.T9.7.7.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T9.7.7.1.m1.1.1.2.2" xref="S4.T9.7.7.1.m1.1.1.2.2.cmml">3.5</mn><mo mathsize="70%" id="S4.T9.7.7.1.m1.1.1.2.1" xref="S4.T9.7.7.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.7.7.1.m1.1b"><apply id="S4.T9.7.7.1.m1.1.1.cmml" xref="S4.T9.7.7.1.m1.1.1"><csymbol cd="latexml" id="S4.T9.7.7.1.m1.1.1.1.cmml" xref="S4.T9.7.7.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T9.7.7.1.m1.1.1.2.cmml" xref="S4.T9.7.7.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T9.7.7.1.m1.1.1.2.1.cmml" xref="S4.T9.7.7.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T9.7.7.1.m1.1.1.2.2.cmml" xref="S4.T9.7.7.1.m1.1.1.2.2">3.5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.7.7.1.m1.1c">\pm 3.5\%</annotation></semantics></math>
</td>
<td id="S4.T9.8.8.2" class="ltx_td ltx_align_center" style="padding:2.25pt 4.0pt;">
<span id="S4.T9.8.8.2.1" class="ltx_text" style="font-size:90%;">74.36 </span><math id="S4.T9.8.8.2.m1.1" class="ltx_Math" alttext="\pm 0.8\%" display="inline"><semantics id="S4.T9.8.8.2.m1.1a"><mrow id="S4.T9.8.8.2.m1.1.1" xref="S4.T9.8.8.2.m1.1.1.cmml"><mo mathsize="70%" id="S4.T9.8.8.2.m1.1.1a" xref="S4.T9.8.8.2.m1.1.1.cmml">±</mo><mrow id="S4.T9.8.8.2.m1.1.1.2" xref="S4.T9.8.8.2.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T9.8.8.2.m1.1.1.2.2" xref="S4.T9.8.8.2.m1.1.1.2.2.cmml">0.8</mn><mo mathsize="70%" id="S4.T9.8.8.2.m1.1.1.2.1" xref="S4.T9.8.8.2.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.8.8.2.m1.1b"><apply id="S4.T9.8.8.2.m1.1.1.cmml" xref="S4.T9.8.8.2.m1.1.1"><csymbol cd="latexml" id="S4.T9.8.8.2.m1.1.1.1.cmml" xref="S4.T9.8.8.2.m1.1.1">plus-or-minus</csymbol><apply id="S4.T9.8.8.2.m1.1.1.2.cmml" xref="S4.T9.8.8.2.m1.1.1.2"><csymbol cd="latexml" id="S4.T9.8.8.2.m1.1.1.2.1.cmml" xref="S4.T9.8.8.2.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T9.8.8.2.m1.1.1.2.2.cmml" xref="S4.T9.8.8.2.m1.1.1.2.2">0.8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.8.8.2.m1.1c">\pm 0.8\%</annotation></semantics></math>
</td>
<td id="S4.T9.9.9.3" class="ltx_td ltx_align_center" style="padding:2.25pt 4.0pt;">
<span id="S4.T9.9.9.3.1" class="ltx_text" style="font-size:90%;">72.23 </span><math id="S4.T9.9.9.3.m1.1" class="ltx_Math" alttext="\pm 2.1\%" display="inline"><semantics id="S4.T9.9.9.3.m1.1a"><mrow id="S4.T9.9.9.3.m1.1.1" xref="S4.T9.9.9.3.m1.1.1.cmml"><mo mathsize="70%" id="S4.T9.9.9.3.m1.1.1a" xref="S4.T9.9.9.3.m1.1.1.cmml">±</mo><mrow id="S4.T9.9.9.3.m1.1.1.2" xref="S4.T9.9.9.3.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T9.9.9.3.m1.1.1.2.2" xref="S4.T9.9.9.3.m1.1.1.2.2.cmml">2.1</mn><mo mathsize="70%" id="S4.T9.9.9.3.m1.1.1.2.1" xref="S4.T9.9.9.3.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.9.9.3.m1.1b"><apply id="S4.T9.9.9.3.m1.1.1.cmml" xref="S4.T9.9.9.3.m1.1.1"><csymbol cd="latexml" id="S4.T9.9.9.3.m1.1.1.1.cmml" xref="S4.T9.9.9.3.m1.1.1">plus-or-minus</csymbol><apply id="S4.T9.9.9.3.m1.1.1.2.cmml" xref="S4.T9.9.9.3.m1.1.1.2"><csymbol cd="latexml" id="S4.T9.9.9.3.m1.1.1.2.1.cmml" xref="S4.T9.9.9.3.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T9.9.9.3.m1.1.1.2.2.cmml" xref="S4.T9.9.9.3.m1.1.1.2.2">2.1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.9.9.3.m1.1c">\pm 2.1\%</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T9.12.14.1" class="ltx_tr">
<th id="S4.T9.12.14.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:2.25pt 4.0pt;"><span id="S4.T9.12.14.1.1.1" class="ltx_text" style="font-size:90%;">OSPD</span></th>
<td id="S4.T9.12.14.1.2" class="ltx_td" style="padding:2.25pt 4.0pt;"></td>
<td id="S4.T9.12.14.1.3" class="ltx_td" style="padding:2.25pt 4.0pt;"></td>
<td id="S4.T9.12.14.1.4" class="ltx_td" style="padding:2.25pt 4.0pt;"></td>
</tr>
<tr id="S4.T9.12.12" class="ltx_tr">
<th id="S4.T9.12.12.4" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:2.25pt 4.0pt;"><span id="S4.T9.12.12.4.1" class="ltx_text" style="font-size:90%;">MLM, CR,</span></th>
<td id="S4.T9.10.10.1" class="ltx_td ltx_align_center" style="padding:2.25pt 4.0pt;">
<span id="S4.T9.10.10.1.1" class="ltx_text" style="font-size:90%;">59.20 </span><math id="S4.T9.10.10.1.m1.1" class="ltx_Math" alttext="\pm 1.8\%" display="inline"><semantics id="S4.T9.10.10.1.m1.1a"><mrow id="S4.T9.10.10.1.m1.1.1" xref="S4.T9.10.10.1.m1.1.1.cmml"><mo mathsize="70%" id="S4.T9.10.10.1.m1.1.1a" xref="S4.T9.10.10.1.m1.1.1.cmml">±</mo><mrow id="S4.T9.10.10.1.m1.1.1.2" xref="S4.T9.10.10.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T9.10.10.1.m1.1.1.2.2" xref="S4.T9.10.10.1.m1.1.1.2.2.cmml">1.8</mn><mo mathsize="70%" id="S4.T9.10.10.1.m1.1.1.2.1" xref="S4.T9.10.10.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.10.10.1.m1.1b"><apply id="S4.T9.10.10.1.m1.1.1.cmml" xref="S4.T9.10.10.1.m1.1.1"><csymbol cd="latexml" id="S4.T9.10.10.1.m1.1.1.1.cmml" xref="S4.T9.10.10.1.m1.1.1">plus-or-minus</csymbol><apply id="S4.T9.10.10.1.m1.1.1.2.cmml" xref="S4.T9.10.10.1.m1.1.1.2"><csymbol cd="latexml" id="S4.T9.10.10.1.m1.1.1.2.1.cmml" xref="S4.T9.10.10.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T9.10.10.1.m1.1.1.2.2.cmml" xref="S4.T9.10.10.1.m1.1.1.2.2">1.8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.10.10.1.m1.1c">\pm 1.8\%</annotation></semantics></math>
</td>
<td id="S4.T9.11.11.2" class="ltx_td ltx_align_center" style="padding:2.25pt 4.0pt;">
<span id="S4.T9.11.11.2.1" class="ltx_text" style="font-size:90%;">71.30 </span><math id="S4.T9.11.11.2.m1.1" class="ltx_Math" alttext="\pm 1.0\%" display="inline"><semantics id="S4.T9.11.11.2.m1.1a"><mrow id="S4.T9.11.11.2.m1.1.1" xref="S4.T9.11.11.2.m1.1.1.cmml"><mo mathsize="70%" id="S4.T9.11.11.2.m1.1.1a" xref="S4.T9.11.11.2.m1.1.1.cmml">±</mo><mrow id="S4.T9.11.11.2.m1.1.1.2" xref="S4.T9.11.11.2.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T9.11.11.2.m1.1.1.2.2" xref="S4.T9.11.11.2.m1.1.1.2.2.cmml">1.0</mn><mo mathsize="70%" id="S4.T9.11.11.2.m1.1.1.2.1" xref="S4.T9.11.11.2.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.11.11.2.m1.1b"><apply id="S4.T9.11.11.2.m1.1.1.cmml" xref="S4.T9.11.11.2.m1.1.1"><csymbol cd="latexml" id="S4.T9.11.11.2.m1.1.1.1.cmml" xref="S4.T9.11.11.2.m1.1.1">plus-or-minus</csymbol><apply id="S4.T9.11.11.2.m1.1.1.2.cmml" xref="S4.T9.11.11.2.m1.1.1.2"><csymbol cd="latexml" id="S4.T9.11.11.2.m1.1.1.2.1.cmml" xref="S4.T9.11.11.2.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T9.11.11.2.m1.1.1.2.2.cmml" xref="S4.T9.11.11.2.m1.1.1.2.2">1.0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.11.11.2.m1.1c">\pm 1.0\%</annotation></semantics></math>
</td>
<td id="S4.T9.12.12.3" class="ltx_td ltx_align_center" style="padding:2.25pt 4.0pt;">
<span id="S4.T9.12.12.3.1" class="ltx_text" style="font-size:90%;">64.70 </span><math id="S4.T9.12.12.3.m1.1" class="ltx_Math" alttext="\pm 1.3\%" display="inline"><semantics id="S4.T9.12.12.3.m1.1a"><mrow id="S4.T9.12.12.3.m1.1.1" xref="S4.T9.12.12.3.m1.1.1.cmml"><mo mathsize="70%" id="S4.T9.12.12.3.m1.1.1a" xref="S4.T9.12.12.3.m1.1.1.cmml">±</mo><mrow id="S4.T9.12.12.3.m1.1.1.2" xref="S4.T9.12.12.3.m1.1.1.2.cmml"><mn mathsize="70%" id="S4.T9.12.12.3.m1.1.1.2.2" xref="S4.T9.12.12.3.m1.1.1.2.2.cmml">1.3</mn><mo mathsize="70%" id="S4.T9.12.12.3.m1.1.1.2.1" xref="S4.T9.12.12.3.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.12.12.3.m1.1b"><apply id="S4.T9.12.12.3.m1.1.1.cmml" xref="S4.T9.12.12.3.m1.1.1"><csymbol cd="latexml" id="S4.T9.12.12.3.m1.1.1.1.cmml" xref="S4.T9.12.12.3.m1.1.1">plus-or-minus</csymbol><apply id="S4.T9.12.12.3.m1.1.1.2.cmml" xref="S4.T9.12.12.3.m1.1.1.2"><csymbol cd="latexml" id="S4.T9.12.12.3.m1.1.1.2.1.cmml" xref="S4.T9.12.12.3.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.T9.12.12.3.m1.1.1.2.2.cmml" xref="S4.T9.12.12.3.m1.1.1.2.2">1.3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.12.12.3.m1.1c">\pm 1.3\%</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T9.12.15.2" class="ltx_tr">
<th id="S4.T9.12.15.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:2.25pt 4.0pt;"><span id="S4.T9.12.15.2.1.1" class="ltx_text" style="font-size:90%;">GR, OSPD</span></th>
<td id="S4.T9.12.15.2.2" class="ltx_td ltx_border_bb" style="padding:2.25pt 4.0pt;"></td>
<td id="S4.T9.12.15.2.3" class="ltx_td ltx_border_bb" style="padding:2.25pt 4.0pt;"></td>
<td id="S4.T9.12.15.2.4" class="ltx_td ltx_border_bb" style="padding:2.25pt 4.0pt;"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 9: </span>Ablation of major components in our system, measured by P/R/F1 on CoNLL-03 validation data. <span id="S4.T9.23.1" class="ltx_text ltx_font_bold">‘CR’</span> refers to “Confidence-Based Rules.” <span id="S4.T9.24.2" class="ltx_text ltx_font_bold">‘GR’</span> refers to “Global Rules.” <span id="S4.T9.25.3" class="ltx_text ltx_font_bold">‘OSPD’</span> refers to “One Sense Per Discourse”. <span id="S4.T9.26.4" class="ltx_text ltx_font_bold">‘MLM’</span> refers to the “Masked Language Model”.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we present a framework that harmoniously blends linguistics and deep learning to overcome the paucity of labeled data for NER, requiring significantly less supervision than previous methods. Real-world entity extraction is often hindered by the lack of annotated data, especially in low-resource domains. While LLMs offer potential remedies, they are not without limitations. Our solution, ELLEN, introduces an efficient, encoder-only method that enables the assembly of an NER system in as little as “half a day”, requiring only a single expert-provided lexicon. We show ELLEN’s strong performance in the extremely low resource setting, showing that it scales well under varying supervision levels, while also outperforming other, more complex approaches.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was partially supported by the
Defense Advanced Research Projects Agency
(DARPA) under the Habitus
program and by the National Science Foundation (NSF) under grant #2006583. Mihai Surdeanu declares a
financial interest in lum.ai. This interest has
been properly disclosed to the University of
Arizona Institutional Review Committee and
is managed in accordance with its conflict of
interest policies.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">Limitations</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">Our proposed method, while showcasing promising results in settings of lightly supervised named entity recognition (NER), faces certain limitations that warrant discussion. Primarily, our evaluation was conducted only on two flat NER datasets. Adapting our method across a broader spectrum of datasets, especially those that may feature more complex, fine-grained, or nested entity structures, needs further exploration. Consequently, our current approach does not explicitly address the challenges associated with more intricate NER tasks, such as nested, fine-grained, hierarchical or intersectional NER, which require the identification of entities within entities or the recognition of novel entity types beyond traditional categories.</p>
</div>
<div id="Sx2.p2" class="ltx_para">
<p id="Sx2.p2.1" class="ltx_p">Some of the rules employed by our method are domain and language-specific, which could limit their wider applicability. However, we also highlight that four out of the eleven total rules in our method are domain and language-independent (assuming the existence of a language model for that domain/language). These include the Masked Language Model (MLM), a heuristic for “free” supervision from exemplars (which can come from any domain or language), Dynamic Window Filtering, which assumes that POS tags are available for a given language/domain and that the language distinguishes between common nouns and proper nouns, One Sense Per Discourse (OSPD) which simply propagates the majority label within a document, and the basic multi-mention heuristic for label propagation which only uses classifier confidence scores. MLM and dynamic window filtering, both language and domain-independent, are the two components that contribute the most to the performance of our NER method (as shown in Table <a href="#S4.T9" title="Table 9 ‣ 4.6. Error Analysis And Ablation Experiment ‣ 4. Experimental Results ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>).</p>
</div>
<div id="Sx2.p3" class="ltx_para">
<p id="Sx2.p3.1" class="ltx_p">Certain rules, such as those dependent on company suffixes and person honorifics, are not domain-independent but are transferable across languages with the adaptation of language-specific affixes. This adaptability suggests a pathway to applying our method to new languages, provided a list of relevant suffixes and honorifics is used. However, there may be challenges in directly applying some of these rules to languages which use vastly different conventions for naming entities. Nevertheless, in presenting our findings, we have not claimed our method to be universally applicable across all languages and domains. Instead, we aimed to demonstrate how the integration of linguistic insights with neural networks can mitigate the scarcity of labeled data in NER. Our framework, which harmoniously blends these elements, points to a significant step forward, while highlighting the necessity for further research to extend its applicability to more diverse and complex NER scenarios.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">Ethical Considerations</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">This work utilizes two public, commonly-used datasets for Named Entity Recognition (NER). One dataset, WNUT-17, derived from the social media domain, mainly consists of user-generated comments, of which a very small portion may include language that some might find inappropriate or offensive. Furthermore, our approach incorporates open-source pre-trained language models. Thus, any biases inherent in these models due to their pre-training data would also apply to our work. Regarding the selection of named entity seeds (see Section <a href="#S3.SS6" title="3.6. Minimizing The Dependency On A Lexicon ‣ 3. Proposed Method ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.6</span></a>), while efforts were made to minimize subjectivity in the creation of the lexicon, it is theoretically possible for the proposed method to be used to intentionally introduce biases into an NER model. However, we believe that, apart from the potential issues already mentioned, this work does not raise any significant ethical concerns.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Bibliographical References</h2>

<div id="S6.p1" class="ltx_para">
<span id="S6.p1.1" class="ltx_ERROR undefined">\c@NAT@ctr</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography"></h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amalvy et al. (2023)</span>
<span class="ltx_bibblock">
Arthur Amalvy, Vincent Labatut, and Richard Dufour. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-short.62" title="" class="ltx_ref ltx_href">The role of global and local context in named entity recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pages 714–722, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arazo et al. (2020)</span>
<span class="ltx_bibblock">
Eric Arazo, Diego Ortego, Paul Albert, Noel E. O’Connor, and Kevin McGuinness. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/IJCNN48605.2020.9207304" title="" class="ltx_ref ltx_href">Pseudo-labeling and confirmation bias in deep semi-supervised learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2020 International Joint Conference on Neural Networks (IJCNN)</em>, pages 1–8.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora and Park (2023)</span>
<span class="ltx_bibblock">
Jatin Arora and Youngja Park. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-short.36" title="" class="ltx_ref ltx_href">Split-NER: Named entity recognition via two question-answering-based classifications</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pages 416–426, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ashok and Lipton (2023)</span>
<span class="ltx_bibblock">
Dhananjay Ashok and Zachary C. Lipton. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.15444" title="" class="ltx_ref ltx_href">Promptner: Prompting for named entity recognition</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et al. (2009)</span>
<span class="ltx_bibblock">
Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009.

</span>
<span class="ltx_bibblock">Curriculum learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 26th annual international conference on machine learning</em>, pages 41–48.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Haiyan Chen, Shuwei Yuan, and Xiang Zhang. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3502223.3502228" title="" class="ltx_ref ltx_href">Rose-ner: Robust semi-supervised named entity recognition on insufficient labeled data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 10th International Joint Conference on Knowledge Graphs</em>, IJCKG ’21, page 38–44, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Jiaao Chen, Zhenghui Wang, Ran Tian, Zichao Yang, and Diyi Yang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.95" title="" class="ltx_ref ltx_href">Local additivity based data augmentation for semi-supervised NER</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 1241–1251, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Jiawei Chen, Yaojie Lu, Hongyu Lin, Jie Lou, Wei Jia, Dai Dai, Hua Wu, Boxi Cao, Xianpei Han, and Le Sun. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.11038" title="" class="ltx_ref ltx_href">Learning in-context learning for named entity recognition</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Mingda Chen, Qingming Tang, Karen Livescu, and Kevin Gimpel. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1906.09535" title="" class="ltx_ref ltx_href">Variational sequential labelers for semi-supervised learning</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2018)</span>
<span class="ltx_bibblock">
Kevin Clark, Thang Luong, Christopher D. Manning, and Quoc V. Le. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/D18-1217.pdf" title="" class="ltx_ref ltx_href">Semi-supervised sequence modeling with cross-view training</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dimachkie (2023)</span>
<span class="ltx_bibblock">
Chady Dimachkie. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://medium.com/ntropy-network/is-cross-entropy-all-you-need-lets-discuss-an-alternative-ac0df6ff5691" title="" class="ltx_ref ltx_href">Cross-entropy is all you need… or is it?</a>

</span>
<span class="ltx_bibblock">Medium.

</span>
<span class="ltx_bibblock">Accessed: 2023-10-17.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gale et al. (1992)</span>
<span class="ltx_bibblock">
William A Gale, Kenneth Church, and David Yarowsky. 1992.

</span>
<span class="ltx_bibblock">One sense per discourse.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Golchin and Surdeanu (2023)</span>
<span class="ltx_bibblock">
Shahriar Golchin and Mihai Surdeanu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2308.08493" title="" class="ltx_ref ltx_href">Time travel in llms: Tracing data contamination in large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.08493.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grishman and Sundheim (1996)</span>
<span class="ltx_bibblock">
Ralph Grishman and Beth M Sundheim. 1996.

</span>
<span class="ltx_bibblock">Message understanding conference-6: A brief history.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">COLING 1996 Volume 1: The 16th International Conference on Computational Linguistics</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2018)</span>
<span class="ltx_bibblock">
Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-1514" title="" class="ltx_ref ltx_href">FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 4803–4809, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2023)</span>
<span class="ltx_bibblock">
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2111.09543" title="" class="ltx_ref ltx_href">Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kauf and Ivanova (2023)</span>
<span class="ltx_bibblock">
Carina Kauf and Anna A. Ivanova. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:258762317" title="" class="ltx_ref ltx_href">A better way to do masked language model scoring</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2305.10588.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lakshmi Narayan et al. (2019)</span>
<span class="ltx_bibblock">
Pooja Lakshmi Narayan, Ajay Nagesh, and Mihai Surdeanu. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/S19-1020" title="" class="ltx_ref ltx_href">Exploration of noise strategies in semi-supervised named entity classification</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)</em>, pages 186–191, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2013)</span>
<span class="ltx_bibblock">
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/COLI_a_00152" title="" class="ltx_ref ltx_href">Deterministic coreference resolution based on entity-centric, precision-ranked rules</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Computational Linguistics</em>, 39(4):885–916.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Yangming Li, Lemao Liu, and Shuming Shi. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2012.05426" title="" class="ltx_ref ltx_href">Empirical analysis of unlabeled entity problem in named entity recognition</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao and Veeramachaneni (2009)</span>
<span class="ltx_bibblock">
Wenhui Liao and Sriharsha Veeramachaneni. 2009.

</span>
<span class="ltx_bibblock">A simple semi-supervised algorithm for named entity recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing</em>, SemiSupLearn ’09, page 58–65, USA. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019a)</span>
<span class="ltx_bibblock">
Tianyu Liu, Jin-Ge Yao, and Chin-Yew Lin. 2019a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1524" title="" class="ltx_ref ltx_href">Towards improving neural named entity recognition with gazetteers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 5301–5307, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019b)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1907.11692" title="" class="ltx_ref ltx_href">Roberta: A robustly optimized BERT pretraining approach</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1907.11692.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2005)</span>
<span class="ltx_bibblock">
Tong Luo, Kurt Kramer, Dmitry B. Goldgof, Lawrence O. Hall, Scott Samson, Andrew Remsen, and Thomas Hopkins. 2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://jmlr.org/papers/v6/luo05a.html" title="" class="ltx_ref ltx_href">Active learning to recognize multiple types of plankton</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 6(20):589–613.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marcus et al. (1993)</span>
<span class="ltx_bibblock">
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/J93-2004" title="" class="ltx_ref ltx_href">Building a large annotated corpus of English: The Penn Treebank</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Computational Linguistics</em>, 19(2):313–330.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nagesh and Surdeanu (2018)</span>
<span class="ltx_bibblock">
Ajay Nagesh and Mihai Surdeanu. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N18-2057" title="" class="ltx_ref ltx_href">Keep your bearings: Lightly-supervised information extraction with ladder networks that avoids semantic drift</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, pages 352–358, New Orleans, Louisiana. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan et al. (2023)</span>
<span class="ltx_bibblock">
Deepak Narayanan, Keshav Santhanam, Peter Henderson, Rishi Bommasani, Tony Lee, and Percy Liang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.02440" title="" class="ltx_ref ltx_href">Cheaply evaluating inference efficiency metrics for autoregressive transformer apis</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023a)</span>
<span class="ltx_bibblock">
OpenAI. 2023a.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2303.08774.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023b)</span>
<span class="ltx_bibblock">
OpenAI. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_href">Introducing chatgpt</a>.

</span>
<span class="ltx_bibblock">Accessed: 10-20-2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott et al. (2019)</span>
<span class="ltx_bibblock">
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1904.01038" title="" class="ltx_ref ltx_href">fairseq: A fast, extensible toolkit for sequence modeling</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2019)</span>
<span class="ltx_bibblock">
Minlong Peng, Xiaoyu Xing, Qi Zhang, Jinlan Fu, and Xuanjing Huang. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1231" title="" class="ltx_ref ltx_href">Distantly supervised named entity recognition using positive-unlabeled learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 2409–2419, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sachan and Xing (2016)</span>
<span class="ltx_bibblock">
Mrinmaya Sachan and Eric Xing. 2016.

</span>
<span class="ltx_bibblock">Easy questions first? a case study on curriculum learning for question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 453–463.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sainz et al. (2023)</span>
<span class="ltx_bibblock">
Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen Etxaniz, and Eneko Agirre. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://hitz-zentroa.github.io/lm-contamination/" title="" class="ltx_ref ltx_href">Lm contamination index</a>.

</span>
<span class="ltx_bibblock">Accessed: 2023-10-12.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scheffer et al. (2001)</span>
<span class="ltx_bibblock">
Tobias Scheffer, Christian Decomain, and Stefan Wrobel. 2001.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:38833768" title="" class="ltx_ref ltx_href">Active hidden markov models for information extraction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">International Symposium on Intelligent Data Analysis</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2023)</span>
<span class="ltx_bibblock">
Yongliang Shen, Zeqi Tan, Shuhui Wu, Wenqi Zhang, Rongsheng Zhang, Yadong Xi, Weiming Lu, and Yueting Zhuang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.17104" title="" class="ltx_ref ltx_href">Promptner: Prompt locating and typing for named entity recognition</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ushio and Camacho-Collados (2021)</span>
<span class="ltx_bibblock">
Asahi Ushio and Jose Camacho-Collados. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.eacl-demos.7" title="" class="ltx_ref ltx_href">T-NER: An all-round python library for transformer-based named entity recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</em>, pages 53–62, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vacareanu et al. (2024)</span>
<span class="ltx_bibblock">
Robert Vacareanu, Enrique Noriega-Atala, Gus Hahn-Powell, Marco A. Valenzuela-Escárcega, and Mihai Surdeanu. 2024.

</span>
<span class="ltx_bibblock">Active learning design choices for NER with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation</em>, Torino, Italy. European Language Resources Association.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.10428" title="" class="ltx_ref ltx_href">Gpt-ner: Named entity recognition via large language models</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, and Kewei Tu. 2021.

</span>
<span class="ltx_bibblock">Automated Concatenation of Embeddings for Structured Prediction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (<span id="bib.bib39.1.1.1" class="ltx_text ltx_font_bold">ACL-IJCNLP 2021</span>)</em>. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2023)</span>
<span class="ltx_bibblock">
Tingyu Xie, Qi Li, Jian Zhang, Yan Zhang, Zuozhu Liu, and Hongwei Wang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.10035" title="" class="ltx_ref ltx_href">Empirical study of zero-shot ner with chatgpt</a>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yadav and Bethard (2019)</span>
<span class="ltx_bibblock">
Vikas Yadav and Steven Bethard. 2019.

</span>
<span class="ltx_bibblock">A survey on recent advances in named entity recognition from deep learning models.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.11470</em>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Sabuncu (2018)</span>
<span class="ltx_bibblock">
Zhilu Zhang and Mert R. Sabuncu. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1805.07836" title="" class="ltx_ref ltx_href">Generalized cross entropy loss for training deep neural networks with noisy labels</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Yandan Zheng, Anran Hao, and Anh Tuan Luu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.813" title="" class="ltx_ref ltx_href">Jointprop: Joint semi-supervised learning for entity and relation extraction with heterogeneous graph-based propagation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 14541–14555, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2022)</span>
<span class="ltx_bibblock">
Kang Zhou, Yuepei Li, and Qi Li. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.498" title="" class="ltx_ref ltx_href">Distantly supervised named entity recognition via confidence-based multi-class positive and unlabeled learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 7198–7211, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, and Hoifung Poon. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.03279" title="" class="ltx_ref ltx_href">Universalner: Targeted distillation from large language models for open named entity recognition</a>.

</span>
</li>
</ul>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.   Language Resource References</h2>

<div id="S7.p1" class="ltx_para">
<span id="S7.p1.1" class="ltx_ERROR undefined">\c@NAT@ctr</span>
</div>
</section>
<section id="biba" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography"> </h2>

<ul class="ltx_biblist">
<li id="biba.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Derczynski et al. (2017)</span>
<span class="ltx_bibblock">
Derczynski, Leon and Nichols, Eric and van Erp, Marieke and Limsopatham, Nut. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W17-4418" title="" class="ltx_ref ltx_href"><em id="biba.bib1.1.1.1" class="ltx_emph ltx_font_italic">Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition</em></a>.

</span>
<span class="ltx_bibblock">Association for Computational Linguistics.

</span>
</li>
<li id="biba.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tjong Kim Sang and De Meulder (2003)</span>
<span class="ltx_bibblock">
Tjong Kim Sang, Erik F. and De Meulder, Fien. 2003.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/W03-0419" title="" class="ltx_ref ltx_href"><em id="biba.bib2.1.1.1" class="ltx_emph ltx_font_italic">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</em></a>.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Statistics of Labeling With PU Learning Lexicon</h2>

<figure id="A1.T10" class="ltx_table">
<table id="A1.T10.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T10.1.1.1" class="ltx_tr">
<th id="A1.T10.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Type</span></th>
<th id="A1.T10.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.1.1.2.1" class="ltx_text" style="font-size:90%;"># of 1.w.</span></th>
<th id="A1.T10.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Precision</span></th>
<th id="A1.T10.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.1.1.4.1" class="ltx_text" style="font-size:90%;">Recall</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T10.1.2.1" class="ltx_tr">
<th id="A1.T10.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.2.1.1.1" class="ltx_text" style="font-size:90%;">PER</span></th>
<th id="A1.T10.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.2.1.2.1" class="ltx_text" style="font-size:90%;">2,507</span></th>
<td id="A1.T10.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.2.1.3.1" class="ltx_text" style="font-size:90%;">89.26%</span></td>
<td id="A1.T10.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.2.1.4.1" class="ltx_text" style="font-size:90%;">17.38%</span></td>
</tr>
<tr id="A1.T10.1.3.2" class="ltx_tr">
<th id="A1.T10.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.3.2.1.1" class="ltx_text" style="font-size:90%;">LOC</span></th>
<th id="A1.T10.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.3.2.2.1" class="ltx_text" style="font-size:90%;">4,384</span></th>
<td id="A1.T10.1.3.2.3" class="ltx_td ltx_align_right" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.3.2.3.1" class="ltx_text" style="font-size:90%;">85.07%</span></td>
<td id="A1.T10.1.3.2.4" class="ltx_td ltx_align_right" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.3.2.4.1" class="ltx_text" style="font-size:90%;">50.03%</span></td>
</tr>
<tr id="A1.T10.1.4.3" class="ltx_tr">
<th id="A1.T10.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.4.3.1.1" class="ltx_text" style="font-size:90%;">ORG</span></th>
<th id="A1.T10.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.4.3.2.1" class="ltx_text" style="font-size:90%;">3,198</span></th>
<td id="A1.T10.1.4.3.3" class="ltx_td ltx_align_right" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.4.3.3.1" class="ltx_text" style="font-size:90%;">86.17%</span></td>
<td id="A1.T10.1.4.3.4" class="ltx_td ltx_align_right" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.4.3.4.1" class="ltx_text" style="font-size:90%;">29.45%</span></td>
</tr>
<tr id="A1.T10.1.5.4" class="ltx_tr">
<th id="A1.T10.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.5.4.1.1" class="ltx_text" style="font-size:90%;">MISC</span></th>
<th id="A1.T10.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.5.4.2.1" class="ltx_text" style="font-size:90%;">1,464</span></th>
<td id="A1.T10.1.5.4.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.5.4.3.1" class="ltx_text" style="font-size:90%;">92.13%</span></td>
<td id="A1.T10.1.5.4.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding:2.25pt 4.6pt;"><span id="A1.T10.1.5.4.4.1" class="ltx_text" style="font-size:90%;">30.59%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 10: </span>Data labeling results using the lexicon used by PU Learning methods: the number of labeled words (# of l.w.), the word-level precision (# of true labeled words/# of total labeled words) and recall, on CoNLL-2003.</figcaption>
</figure>
<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Table <a href="#A1.T10" title="Table 10 ‣ Appendix A Statistics of Labeling With PU Learning Lexicon ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, which is directly taken from the work of <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite>, illustrates the data labeling statistics of the large lexicons (sourced from external dictionaries) used by PU-learning methods. This is in stark contrast to our method, where the lexicon contains only 10 exemplars per class in the ‘1%’ data setting or at most, a few hundred for the <span id="A1.p1.1.1" class="ltx_text ltx_font_typewriter">PER</span> class in the ‘5%’ data setting.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>WNUT-17: Zero-Shot Evaluation</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">To allow our model, which was trained on CoNLL-2003, to be fairly compared in a zero-shot setting against the fully supervised <span id="A2.p1.1.1" class="ltx_text ltx_font_typewriter">roberta-large</span> model from the T-NER <cite class="ltx_cite ltx_citemacro_cite">Ushio and Camacho-Collados (<a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite> library (which is trained on WNUT-17 gold data), we mapped the generated labels from the fully supervised model onto the label space of CoNLL-2003. We aligned the classes from WNUT-17 with CoNLL classes (<span id="A2.p1.1.2" class="ltx_text ltx_font_typewriter">ORG, LOC, PER, MISC</span>) based on semantic overlap. We aligned the ‘products’ and ‘creative-work’ classes with the MISC class from CoNLL. This is because the MISC class from CoNLL also contains many product names and ‘works of art,’ e.g., “Ain’t No Telling” by Jimi Hendrix.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">We aligned the ‘group’ class from WNUT-17 with ‘ORG’ from CoNLL because many ‘group‘ names in the WNUT-17 test data have a semantic overlap with organizations, e.g., ‘Nirvana”, ‘San Diego Padres.’ Based upon our inspection of the data, the ‘group’ class also includes entities like musical bands, sports teams, non-profit organizations, political groups, etc. Such entities fit well within the typical CoNLL understanding of an ‘organization.’ For the remaining classes of WNUT-17 (<span id="A2.p2.1.1" class="ltx_text ltx_font_typewriter">corporation, location, person</span>), we mapped them directly to their corresponding CoNLL-03 equivalents. We also applied this mapping to the zero-shot predictions of GPT-3.5 and GPT-4, to allow all models to be fairly compared against each other. We evaluated all models shown in Table <a href="#S4.T7" title="Table 7 ‣ 4.4. Zero-Shot Evaluation ‣ 4. Experimental Results ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> on the full test set (1287 samples) of WNUT-17.</p>
</div>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.1" class="ltx_p">We accessed GPT-3.5 and GPT-4 through the Azure OpenAI service, using the <span id="A2.p3.1.1" class="ltx_text ltx_font_typewriter">gpt-35-turbo-0613</span> and <span id="A2.p3.1.2" class="ltx_text ltx_font_typewriter">gpt-4-0613</span> models with <span id="A2.p3.1.3" class="ltx_text ltx_font_typewriter">temperature=0</span> for deterministic results. We borrow the prompt format from the vanilla zero-shot prompt used by <cite class="ltx_cite ltx_citemacro_cite">Xie et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite> (shown in the figure below). In the zero-shot evaluation with GPT-3.5 and GPT-4, we observed issues similar to those observed by <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite>, i.e., both LLMs often fail to match the output length with the input sentence’s token count in sequence labeling tasks like NER, a challenge amplified in longer sentences. This is documented in Table <a href="#A2.T11" title="Table 11 ‣ Appendix B WNUT-17: Zero-Shot Evaluation ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, distinguishing “Misalignment errors”—the discrepancy in the number of LLM generated NER tags versus sentence tokens—and “Parsing errors,” where the LLM generation does not form a valid sequence of NER labels and hence, cannot be parsed, with GPT-3.5 showing more pronounced issues.</p>
</div>
<div id="A2.p4" class="ltx_para">
<p id="A2.p4.1" class="ltx_p">To mitigate these alignment problems, we used a simple approach:</p>
<ol id="A2.I1" class="ltx_enumerate">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p">For outputs with fewer NER tags than input tokens, we padded the sequence on the right with ‘<span id="A2.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter">O</span>’ tags to equalize the lengths.</p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.p1.1" class="ltx_p">For outputs with excess NER tags, we truncated the surplus from the right to match the input token sequence length.</p>
</div>
</li>
</ol>
</div>
<div id="A2.p5" class="ltx_para">
<p id="A2.p5.1" class="ltx_p">We then evaluated the aligned and corrected predictions of both LLM’s on the WNUT-17 test set using the official CoNLL-scoring script (results reported in Table <a href="#S4.T7" title="Table 7 ‣ 4.4. Zero-Shot Evaluation ‣ 4. Experimental Results ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<figure id="A2.T11" class="ltx_table">
<table id="A2.T11.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T11.1.1.1" class="ltx_tr">
<th id="A2.T11.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="A2.T11.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="A2.T11.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="A2.T11.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Misalignment Errors</span></th>
<th id="A2.T11.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="A2.T11.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Parsing Errors</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T11.1.2.1" class="ltx_tr">
<th id="A2.T11.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="A2.T11.1.2.1.1.1" class="ltx_text" style="font-size:90%;">GPT-3.5</span></th>
<td id="A2.T11.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="A2.T11.1.2.1.2.1" class="ltx_text" style="font-size:90%;">426</span></td>
<td id="A2.T11.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="A2.T11.1.2.1.3.1" class="ltx_text" style="font-size:90%;">80</span></td>
</tr>
<tr id="A2.T11.1.3.2" class="ltx_tr">
<th id="A2.T11.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="A2.T11.1.3.2.1.1" class="ltx_text" style="font-size:90%;">GPT-4</span></th>
<td id="A2.T11.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="A2.T11.1.3.2.2.1" class="ltx_text" style="font-size:90%;">195</span></td>
<td id="A2.T11.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;"><span id="A2.T11.1.3.2.3.1" class="ltx_text" style="font-size:90%;">44</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 11: </span>Comparison of error counts between GPT-3.5 and GPT-4.</figcaption>
</figure>
<div id="A2.p6" class="ltx_para ltx_noindent">
<svg id="A2.p6.pic1" class="ltx_picture" height="156.4" overflow="visible" version="1.1" width="600"><g transform="translate(0,156.4) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#0000BF" fill-opacity="1.0"><path d="M 0 5.91 L 0 150.49 C 0 153.76 2.64 156.4 5.91 156.4 L 594.09 156.4 C 597.36 156.4 600 153.76 600 150.49 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 130.75 L 598.03 130.75 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 136.66)"><foreignObject width="556.69" height="13.84" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible" color="#FFFFFF">
<span id="A2.p6.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="A2.p6.pic1.1.1.1.1.1.1" class="ltx_p">Prompt Used For Zero-Shot Evaluation of GPT-3.5/4</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="105.16" transform="matrix(1 0 0 -1 0 15.22)" overflow="visible" color="#000000">
<span id="A2.p6.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="A2.p6.pic1.2.2.2.1.1.1" class="ltx_p">Given entity label set: [‘B-PER’, ‘I-PER’, ‘B-ORG’, ‘I-ORG’, ‘B-LOC’, ‘I-LOC’, ‘B-MISC’, ‘I-MISC’, ‘O’]</span>
<span id="A2.p6.pic1.2.2.2.1.1.2" class="ltx_p">Based on the given entity label set, please recognize the named entities for each token in the given text, and return the answer as a list of named entity tags.</span>
<span id="A2.p6.pic1.2.2.2.1.1.3" class="ltx_p">Text: {input text}</span>
<span id="A2.p6.pic1.2.2.2.1.1.4" class="ltx_p">Answer: {ChatGPT response}</span>
</span></foreignObject></g></g></svg>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Masked Language Model (MLM): Inverse Breaking Ties</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.2" class="ltx_p">In order to obtain more robust annotations from the Masked Language Model (MLM), we only consider the entity span <math id="A3.p1.1.m1.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="A3.p1.1.m1.1a"><msub id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml"><mi id="A3.p1.1.m1.1.1.2" xref="A3.p1.1.m1.1.1.2.cmml">x</mi><mi id="A3.p1.1.m1.1.1.3" xref="A3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><apply id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A3.p1.1.m1.1.1.1.cmml" xref="A3.p1.1.m1.1.1">subscript</csymbol><ci id="A3.p1.1.m1.1.1.2.cmml" xref="A3.p1.1.m1.1.1.2">𝑥</ci><ci id="A3.p1.1.m1.1.1.3.cmml" xref="A3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">x_{i}</annotation></semantics></math> labeled by the MLM where the difference between the score of the class that is predicted with the highest probability and the score of the class that is predicted with the second highest probability is greater than some threshold <math id="A3.p1.2.m2.1" class="ltx_Math" alttext="t_{class}" display="inline"><semantics id="A3.p1.2.m2.1a"><msub id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml"><mi id="A3.p1.2.m2.1.1.2" xref="A3.p1.2.m2.1.1.2.cmml">t</mi><mrow id="A3.p1.2.m2.1.1.3" xref="A3.p1.2.m2.1.1.3.cmml"><mi id="A3.p1.2.m2.1.1.3.2" xref="A3.p1.2.m2.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="A3.p1.2.m2.1.1.3.1" xref="A3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="A3.p1.2.m2.1.1.3.3" xref="A3.p1.2.m2.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="A3.p1.2.m2.1.1.3.1a" xref="A3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="A3.p1.2.m2.1.1.3.4" xref="A3.p1.2.m2.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="A3.p1.2.m2.1.1.3.1b" xref="A3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="A3.p1.2.m2.1.1.3.5" xref="A3.p1.2.m2.1.1.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="A3.p1.2.m2.1.1.3.1c" xref="A3.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="A3.p1.2.m2.1.1.3.6" xref="A3.p1.2.m2.1.1.3.6.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><apply id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A3.p1.2.m2.1.1.1.cmml" xref="A3.p1.2.m2.1.1">subscript</csymbol><ci id="A3.p1.2.m2.1.1.2.cmml" xref="A3.p1.2.m2.1.1.2">𝑡</ci><apply id="A3.p1.2.m2.1.1.3.cmml" xref="A3.p1.2.m2.1.1.3"><times id="A3.p1.2.m2.1.1.3.1.cmml" xref="A3.p1.2.m2.1.1.3.1"></times><ci id="A3.p1.2.m2.1.1.3.2.cmml" xref="A3.p1.2.m2.1.1.3.2">𝑐</ci><ci id="A3.p1.2.m2.1.1.3.3.cmml" xref="A3.p1.2.m2.1.1.3.3">𝑙</ci><ci id="A3.p1.2.m2.1.1.3.4.cmml" xref="A3.p1.2.m2.1.1.3.4">𝑎</ci><ci id="A3.p1.2.m2.1.1.3.5.cmml" xref="A3.p1.2.m2.1.1.3.5">𝑠</ci><ci id="A3.p1.2.m2.1.1.3.6.cmml" xref="A3.p1.2.m2.1.1.3.6">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">t_{class}</annotation></semantics></math>:</p>
</div>
<div id="A3.p2" class="ltx_para">
<table id="A3.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A3.Ex3.m1.1" class="ltx_math_unparsed" alttext="x_{i}\mid P(y_{i}=l_{1}|x_{i})-P(y_{i}=l_{2}|x_{i})&gt;t_{class}" display="block"><semantics id="A3.Ex3.m1.1a"><mrow id="A3.Ex3.m1.1b"><msub id="A3.Ex3.m1.1.1"><mi id="A3.Ex3.m1.1.1.2">x</mi><mi id="A3.Ex3.m1.1.1.3">i</mi></msub><mo lspace="0em" rspace="0.167em" id="A3.Ex3.m1.1.2">∣</mo><mi id="A3.Ex3.m1.1.3">P</mi><mrow id="A3.Ex3.m1.1.4"><mo stretchy="false" id="A3.Ex3.m1.1.4.1">(</mo><msub id="A3.Ex3.m1.1.4.2"><mi id="A3.Ex3.m1.1.4.2.2">y</mi><mi id="A3.Ex3.m1.1.4.2.3">i</mi></msub><mo id="A3.Ex3.m1.1.4.3">=</mo><msub id="A3.Ex3.m1.1.4.4"><mi id="A3.Ex3.m1.1.4.4.2">l</mi><mn id="A3.Ex3.m1.1.4.4.3">1</mn></msub><mo fence="false" rspace="0.167em" stretchy="false" id="A3.Ex3.m1.1.4.5">|</mo><msub id="A3.Ex3.m1.1.4.6"><mi id="A3.Ex3.m1.1.4.6.2">x</mi><mi id="A3.Ex3.m1.1.4.6.3">i</mi></msub><mo stretchy="false" id="A3.Ex3.m1.1.4.7">)</mo></mrow><mo id="A3.Ex3.m1.1.5">−</mo><mi id="A3.Ex3.m1.1.6">P</mi><mrow id="A3.Ex3.m1.1.7"><mo stretchy="false" id="A3.Ex3.m1.1.7.1">(</mo><msub id="A3.Ex3.m1.1.7.2"><mi id="A3.Ex3.m1.1.7.2.2">y</mi><mi id="A3.Ex3.m1.1.7.2.3">i</mi></msub><mo id="A3.Ex3.m1.1.7.3">=</mo><msub id="A3.Ex3.m1.1.7.4"><mi id="A3.Ex3.m1.1.7.4.2">l</mi><mn id="A3.Ex3.m1.1.7.4.3">2</mn></msub><mo fence="false" rspace="0.167em" stretchy="false" id="A3.Ex3.m1.1.7.5">|</mo><msub id="A3.Ex3.m1.1.7.6"><mi id="A3.Ex3.m1.1.7.6.2">x</mi><mi id="A3.Ex3.m1.1.7.6.3">i</mi></msub><mo stretchy="false" id="A3.Ex3.m1.1.7.7">)</mo></mrow><mo id="A3.Ex3.m1.1.8">&gt;</mo><msub id="A3.Ex3.m1.1.9"><mi id="A3.Ex3.m1.1.9.2">t</mi><mrow id="A3.Ex3.m1.1.9.3"><mi id="A3.Ex3.m1.1.9.3.2">c</mi><mo lspace="0em" rspace="0em" id="A3.Ex3.m1.1.9.3.1">​</mo><mi id="A3.Ex3.m1.1.9.3.3">l</mi><mo lspace="0em" rspace="0em" id="A3.Ex3.m1.1.9.3.1a">​</mo><mi id="A3.Ex3.m1.1.9.3.4">a</mi><mo lspace="0em" rspace="0em" id="A3.Ex3.m1.1.9.3.1b">​</mo><mi id="A3.Ex3.m1.1.9.3.5">s</mi><mo lspace="0em" rspace="0em" id="A3.Ex3.m1.1.9.3.1c">​</mo><mi id="A3.Ex3.m1.1.9.3.6">s</mi></mrow></msub></mrow><annotation encoding="application/x-tex" id="A3.Ex3.m1.1c">x_{i}\mid P(y_{i}=l_{1}|x_{i})-P(y_{i}=l_{2}|x_{i})&gt;t_{class}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.2" class="ltx_p">Here <math id="A3.p3.1.m1.1" class="ltx_Math" alttext="l_{1}" display="inline"><semantics id="A3.p3.1.m1.1a"><msub id="A3.p3.1.m1.1.1" xref="A3.p3.1.m1.1.1.cmml"><mi id="A3.p3.1.m1.1.1.2" xref="A3.p3.1.m1.1.1.2.cmml">l</mi><mn id="A3.p3.1.m1.1.1.3" xref="A3.p3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A3.p3.1.m1.1b"><apply id="A3.p3.1.m1.1.1.cmml" xref="A3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A3.p3.1.m1.1.1.1.cmml" xref="A3.p3.1.m1.1.1">subscript</csymbol><ci id="A3.p3.1.m1.1.1.2.cmml" xref="A3.p3.1.m1.1.1.2">𝑙</ci><cn type="integer" id="A3.p3.1.m1.1.1.3.cmml" xref="A3.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p3.1.m1.1c">l_{1}</annotation></semantics></math> is the most likely class label and <math id="A3.p3.2.m2.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="A3.p3.2.m2.1a"><msub id="A3.p3.2.m2.1.1" xref="A3.p3.2.m2.1.1.cmml"><mi id="A3.p3.2.m2.1.1.2" xref="A3.p3.2.m2.1.1.2.cmml">l</mi><mn id="A3.p3.2.m2.1.1.3" xref="A3.p3.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A3.p3.2.m2.1b"><apply id="A3.p3.2.m2.1.1.cmml" xref="A3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="A3.p3.2.m2.1.1.1.cmml" xref="A3.p3.2.m2.1.1">subscript</csymbol><ci id="A3.p3.2.m2.1.1.2.cmml" xref="A3.p3.2.m2.1.1.2">𝑙</ci><cn type="integer" id="A3.p3.2.m2.1.1.3.cmml" xref="A3.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p3.2.m2.1c">l_{2}</annotation></semantics></math> is the second most likely class label, according to the MLM. This is motivated by the <span id="A3.p3.2.1" class="ltx_text ltx_font_italic">Breaking Ties</span> active learning method of <cite class="ltx_cite ltx_citemacro_citet">Scheffer et al. (<a href="#bib.bib34" title="" class="ltx_ref">2001</a>); Luo et al. (<a href="#bib.bib24" title="" class="ltx_ref">2005</a>)</cite>, which aims to select token samples where the difference between the top two predictions is the smallest, in order to increase the likelihood of confident classifications. However, for the MLM, we adopt the <span id="A3.p3.2.2" class="ltx_text ltx_font_italic">inverse</span> of breaking ties, where we maximize the difference between the top two predictions, based on a threshold. We use different thresholds for each class as shown in Table <a href="#A3.T12" title="Table 12 ‣ Appendix C Masked Language Model (MLM): Inverse Breaking Ties ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. We empirically observe that we obtain a slightly higher F1 score with the MLM as an unsupervised NER on the CoNLL-03 development set when using different thresholds for each class instead of a single threshold value for all classes.</p>
</div>
<div id="A3.p4" class="ltx_para">
<p id="A3.p4.1" class="ltx_p">In our experiments, we also empirically observe that the MLM tends to produce more robust probabilities when the lexicon entities filling the <span id="A3.p4.1.1" class="ltx_text ltx_font_typewriter">[MASK]</span> slot(s) are segmented into fewer subwords by the model’s tokenizer. This is supported by the findings of <cite class="ltx_cite ltx_citemacro_citet">Kauf and Ivanova (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>, who observe that methods that estimate the psuedo-log-likelihood of a sentence yield inflated scores for out-of-vocabulary
words. Hence we employ an additional heuristic where we filter the lexicon entities to only single subword entities. We believe that better methods for estimating and aggregating probabilities for sentences that contain out-of-vocabulary words can be explored in future work. <cite class="ltx_cite ltx_citemacro_citet">Kauf and Ivanova (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> introduce one such method, which has been shown to address the issue of attributing uneven likelihoods to multi-token words. Specifically, it proves beneficial to mask not only the current token but also all subsequent tokens that are part of the same word.</p>
</div>
<div id="A3.p5" class="ltx_para">
<p id="A3.p5.1" class="ltx_p">Furthermore, given the large size of the lexicons extracted for the MLM in both the “5%” and fully supervised setting, we filter our lexicon to keep only the top 20 entities for each class, sorted by their frequency of occurrence in the training data.</p>
</div>
<figure id="A3.T12" class="ltx_table">
<table id="A3.T12.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T12.1.1.1" class="ltx_tr">
<th id="A3.T12.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Class</th>
<th id="A3.T12.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Threshold</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T12.1.2.1" class="ltx_tr">
<td id="A3.T12.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">ORG</td>
<td id="A3.T12.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.28</td>
</tr>
<tr id="A3.T12.1.3.2" class="ltx_tr">
<td id="A3.T12.1.3.2.1" class="ltx_td ltx_align_center">PER</td>
<td id="A3.T12.1.3.2.2" class="ltx_td ltx_align_center">0.2</td>
</tr>
<tr id="A3.T12.1.4.3" class="ltx_tr">
<td id="A3.T12.1.4.3.1" class="ltx_td ltx_align_center">LOC</td>
<td id="A3.T12.1.4.3.2" class="ltx_td ltx_align_center">0.1</td>
</tr>
<tr id="A3.T12.1.5.4" class="ltx_tr">
<td id="A3.T12.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb">MISC</td>
<td id="A3.T12.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">0.05</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span><span id="A3.T12.3.1" class="ltx_text" style="font-size:90%;">Per class thresholds used by the Masked Language Model (MLM) to implement “inverse breaking ties”.</span></figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Hyperparameters and Hardware</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.6" class="ltx_p">Instead of the regular cross-entropy loss, we use a Generalized Cross Entropy Loss function with label smoothing <cite class="ltx_cite ltx_citemacro_cite">Dimachkie (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> for training our models, which offers a better trade-off between the noise-robustness of mean absolute error and the noise sensitivity of cross entropy loss. This trade-off can be controlled by a hyperparameter <math id="A4.p1.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="A4.p1.1.m1.1a"><mi id="A4.p1.1.m1.1.1" xref="A4.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><ci id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">q</annotation></semantics></math>. We experiment with multiple settings where we vary <math id="A4.p1.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="A4.p1.2.m2.1a"><mi id="A4.p1.2.m2.1.1" xref="A4.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="A4.p1.2.m2.1b"><ci id="A4.p1.2.m2.1.1.cmml" xref="A4.p1.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.2.m2.1c">q</annotation></semantics></math>, along with the learning rate, the dynamic window size, the number of burn-in, intermediate and burn-out stages and the total number of self-training iterations. This search involved under 20 runs, based on the development partition of CoNLL-2003. We use a dynamic window size of <math id="A4.p1.3.m3.1" class="ltx_Math" alttext="5" display="inline"><semantics id="A4.p1.3.m3.1a"><mn id="A4.p1.3.m3.1.1" xref="A4.p1.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="A4.p1.3.m3.1b"><cn type="integer" id="A4.p1.3.m3.1.1.cmml" xref="A4.p1.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.3.m3.1c">5</annotation></semantics></math>, a batch size of <math id="A4.p1.4.m4.1" class="ltx_Math" alttext="16" display="inline"><semantics id="A4.p1.4.m4.1a"><mn id="A4.p1.4.m4.1.1" xref="A4.p1.4.m4.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="A4.p1.4.m4.1b"><cn type="integer" id="A4.p1.4.m4.1.1.cmml" xref="A4.p1.4.m4.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.4.m4.1c">16</annotation></semantics></math> for training, a learning rate of <math id="A4.p1.5.m5.1" class="ltx_Math" alttext="1e{-5}" display="inline"><semantics id="A4.p1.5.m5.1a"><mrow id="A4.p1.5.m5.1.1" xref="A4.p1.5.m5.1.1.cmml"><mrow id="A4.p1.5.m5.1.1.2" xref="A4.p1.5.m5.1.1.2.cmml"><mn id="A4.p1.5.m5.1.1.2.2" xref="A4.p1.5.m5.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="A4.p1.5.m5.1.1.2.1" xref="A4.p1.5.m5.1.1.2.1.cmml">​</mo><mi id="A4.p1.5.m5.1.1.2.3" xref="A4.p1.5.m5.1.1.2.3.cmml">e</mi></mrow><mo id="A4.p1.5.m5.1.1.1" xref="A4.p1.5.m5.1.1.1.cmml">−</mo><mn id="A4.p1.5.m5.1.1.3" xref="A4.p1.5.m5.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.5.m5.1b"><apply id="A4.p1.5.m5.1.1.cmml" xref="A4.p1.5.m5.1.1"><minus id="A4.p1.5.m5.1.1.1.cmml" xref="A4.p1.5.m5.1.1.1"></minus><apply id="A4.p1.5.m5.1.1.2.cmml" xref="A4.p1.5.m5.1.1.2"><times id="A4.p1.5.m5.1.1.2.1.cmml" xref="A4.p1.5.m5.1.1.2.1"></times><cn type="integer" id="A4.p1.5.m5.1.1.2.2.cmml" xref="A4.p1.5.m5.1.1.2.2">1</cn><ci id="A4.p1.5.m5.1.1.2.3.cmml" xref="A4.p1.5.m5.1.1.2.3">𝑒</ci></apply><cn type="integer" id="A4.p1.5.m5.1.1.3.cmml" xref="A4.p1.5.m5.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.5.m5.1c">1e{-5}</annotation></semantics></math>, and a confidence-threshold of <math id="A4.p1.6.m6.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="A4.p1.6.m6.1a"><mn id="A4.p1.6.m6.1.1" xref="A4.p1.6.m6.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="A4.p1.6.m6.1b"><cn type="float" id="A4.p1.6.m6.1.1.cmml" xref="A4.p1.6.m6.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.6.m6.1c">0.9</annotation></semantics></math> across all of our data settings. We show the other hyperparameters in Table <a href="#A4.T13" title="Table 13 ‣ Appendix D Hyperparameters and Hardware ‣ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>. All experiments were carried out on a system with 2 Nvidia RTX 3090 GPUs.</p>
</div>
<figure id="A4.T13" class="ltx_table">
<table id="A4.T13.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A4.T13.1.1.1" class="ltx_tr">
<td id="A4.T13.1.1.1.1" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.1.1.1.1.1" class="ltx_p"><span id="A4.T13.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Setting</span></span>
</span>
</td>
<td id="A4.T13.1.1.1.2" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.1.1.2.1.1" class="ltx_p"><span id="A4.T13.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Burn-in stages</span></span>
</span>
</td>
<td id="A4.T13.1.1.1.3" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.1.1.3.1.1" class="ltx_p"><span id="A4.T13.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Intermediate stages</span></span>
</span>
</td>
<td id="A4.T13.1.1.1.4" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.1.1.4.1.1" class="ltx_p"><span id="A4.T13.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Burn-out stages</span></span>
</span>
</td>
<td id="A4.T13.1.1.1.5" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.1.1.5.1.1" class="ltx_p"><span id="A4.T13.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Noise-level (<span id="A4.T13.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_typewriter">q</span>)</span></span>
</span>
</td>
<td id="A4.T13.1.1.1.6" class="ltx_td ltx_align_justify ltx_border_tt" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.1.1.6.1.1" class="ltx_p"><span id="A4.T13.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Self-training iterations</span></span>
</span>
</td>
<td id="A4.T13.1.1.1.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_tt" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.1.1.7.1.1" class="ltx_p"><span id="A4.T13.1.1.1.7.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Label Smoothing</span></span>
</span>
</td>
</tr>
<tr id="A4.T13.1.2.2" class="ltx_tr">
<td id="A4.T13.1.2.2.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.2.2.1.1.1" class="ltx_p"><span id="A4.T13.1.2.2.1.1.1.1" class="ltx_text" style="font-size:90%;">1% data</span></span>
</span>
</td>
<td id="A4.T13.1.2.2.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.2.2.2.1.1" class="ltx_p"><span id="A4.T13.1.2.2.2.1.1.1" class="ltx_text" style="font-size:90%;">1</span></span>
</span>
</td>
<td id="A4.T13.1.2.2.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.2.2.3.1.1" class="ltx_p"><span id="A4.T13.1.2.2.3.1.1.1" class="ltx_text" style="font-size:90%;">2</span></span>
</span>
</td>
<td id="A4.T13.1.2.2.4" class="ltx_td ltx_align_justify ltx_border_t" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.2.2.4.1.1" class="ltx_p"><span id="A4.T13.1.2.2.4.1.1.1" class="ltx_text" style="font-size:90%;">1</span></span>
</span>
</td>
<td id="A4.T13.1.2.2.5" class="ltx_td ltx_align_justify ltx_border_t" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.2.2.5.1.1" class="ltx_p"><span id="A4.T13.1.2.2.5.1.1.1" class="ltx_text" style="font-size:90%;">0.9</span></span>
</span>
</td>
<td id="A4.T13.1.2.2.6" class="ltx_td ltx_align_justify ltx_border_t" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.2.2.6.1.1" class="ltx_p"><span id="A4.T13.1.2.2.6.1.1.1" class="ltx_text" style="font-size:90%;">4</span></span>
</span>
</td>
<td id="A4.T13.1.2.2.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.2.2.7.1.1" class="ltx_p"><span id="A4.T13.1.2.2.7.1.1.1" class="ltx_text" style="font-size:90%;">0.1</span></span>
</span>
</td>
</tr>
<tr id="A4.T13.1.3.3" class="ltx_tr">
<td id="A4.T13.1.3.3.1" class="ltx_td ltx_align_justify" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.3.3.1.1.1" class="ltx_p"><span id="A4.T13.1.3.3.1.1.1.1" class="ltx_text" style="font-size:90%;">5% data</span></span>
</span>
</td>
<td id="A4.T13.1.3.3.2" class="ltx_td ltx_align_justify" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.3.3.2.1.1" class="ltx_p"><span id="A4.T13.1.3.3.2.1.1.1" class="ltx_text" style="font-size:90%;">1</span></span>
</span>
</td>
<td id="A4.T13.1.3.3.3" class="ltx_td ltx_align_justify" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.3.3.3.1.1" class="ltx_p"><span id="A4.T13.1.3.3.3.1.1.1" class="ltx_text" style="font-size:90%;">2</span></span>
</span>
</td>
<td id="A4.T13.1.3.3.4" class="ltx_td ltx_align_justify" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.3.3.4.1.1" class="ltx_p"><span id="A4.T13.1.3.3.4.1.1.1" class="ltx_text" style="font-size:90%;">0</span></span>
</span>
</td>
<td id="A4.T13.1.3.3.5" class="ltx_td ltx_align_justify" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.3.3.5.1.1" class="ltx_p"><span id="A4.T13.1.3.3.5.1.1.1" class="ltx_text" style="font-size:90%;">0.7</span></span>
</span>
</td>
<td id="A4.T13.1.3.3.6" class="ltx_td ltx_align_justify" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.3.3.6.1.1" class="ltx_p"><span id="A4.T13.1.3.3.6.1.1.1" class="ltx_text" style="font-size:90%;">3</span></span>
</span>
</td>
<td id="A4.T13.1.3.3.7" class="ltx_td ltx_nopad_r ltx_align_justify" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.3.3.7.1.1" class="ltx_p"><span id="A4.T13.1.3.3.7.1.1.1" class="ltx_text" style="font-size:90%;">0.1</span></span>
</span>
</td>
</tr>
<tr id="A4.T13.1.4.4" class="ltx_tr">
<td id="A4.T13.1.4.4.1" class="ltx_td ltx_align_justify ltx_border_bb" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.4.4.1.1.1" class="ltx_p"><span id="A4.T13.1.4.4.1.1.1.1" class="ltx_text" style="font-size:90%;">100% data</span></span>
</span>
</td>
<td id="A4.T13.1.4.4.2" class="ltx_td ltx_align_justify ltx_border_bb" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.4.4.2.1.1" class="ltx_p"><span id="A4.T13.1.4.4.2.1.1.1" class="ltx_text" style="font-size:90%;">1</span></span>
</span>
</td>
<td id="A4.T13.1.4.4.3" class="ltx_td ltx_align_justify ltx_border_bb" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.4.4.3.1.1" class="ltx_p"><span id="A4.T13.1.4.4.3.1.1.1" class="ltx_text" style="font-size:90%;">1</span></span>
</span>
</td>
<td id="A4.T13.1.4.4.4" class="ltx_td ltx_align_justify ltx_border_bb" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.4.4.4.1.1" class="ltx_p"><span id="A4.T13.1.4.4.4.1.1.1" class="ltx_text" style="font-size:90%;">0</span></span>
</span>
</td>
<td id="A4.T13.1.4.4.5" class="ltx_td ltx_align_justify ltx_border_bb" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.4.4.5.1.1" class="ltx_p"><span id="A4.T13.1.4.4.5.1.1.1" class="ltx_text" style="font-size:90%;">0.7</span></span>
</span>
</td>
<td id="A4.T13.1.4.4.6" class="ltx_td ltx_align_justify ltx_border_bb" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.4.4.6.1.1" class="ltx_p"><span id="A4.T13.1.4.4.6.1.1.1" class="ltx_text" style="font-size:90%;">2</span></span>
</span>
</td>
<td id="A4.T13.1.4.4.7" class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb" style="padding:9pt 4.0pt;">
<span id="A4.T13.1.4.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.4.4.7.1.1" class="ltx_p"><span id="A4.T13.1.4.4.7.1.1.1" class="ltx_text" style="font-size:90%;">0.2</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 13: </span>The Hyperparameters we use for training ELLEN under various supervision settings.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.17384" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.17385" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.17385">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.17385" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.17387" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 17:27:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
