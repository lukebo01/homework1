<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.18739] Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification</title><meta property="og:description" content="Similar to humans, animals make extensive use of verbal and non-verbal forms of communication, including a large range of audio signals. In this paper, we address dog vocalizations and explore the use of self-supervise…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.18739">

<!--Generated on Sun May  5 20:56:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">Similar to humans, animals make extensive use of verbal and non-verbal forms of communication, including a large range of audio signals. In this paper, we address dog vocalizations and explore the use of self-supervised speech representation models pre-trained on human speech to address dog bark classification tasks that find parallels in human-centered tasks in speech recognition. We specifically address four tasks: dog recognition, breed identification, gender classification, and context grounding. We show that using speech embedding representations significantly improves over simpler classification baselines. Further, we also find that models pre-trained on large human speech acoustics can provide additional performance boosts on several tasks.

<br class="ltx_break">
<br class="ltx_break">
<span id="id7.id1.1" class="ltx_text ltx_font_bold">Keywords: </span>animal vocalizations, semi-supervised learning, audio processing</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\NAT@set@cites</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id6" class="ltx_logical-block">
<div id="id6.p1" class="ltx_para">
<p id="id6.p1.1" class="ltx_p ltx_align_center"><span id="id6.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification</span></p>
<br class="ltx_break ltx_centering">
<table id="id5.5" class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr id="id3.3.3" class="ltx_tr">
<td id="id3.3.3.3" class="ltx_td ltx_align_center"><span id="id3.3.3.3.3" class="ltx_text ltx_font_bold" style="font-size:120%;">Artem Abzaliev<sup id="id3.3.3.3.3.1" class="ltx_sup"><span id="id3.3.3.3.3.1.1" class="ltx_text ltx_font_medium">1</span></sup>, Humberto Pérez Espinosa<sup id="id3.3.3.3.3.2" class="ltx_sup"><span id="id3.3.3.3.3.2.1" class="ltx_text ltx_font_medium">2</span></sup>, Rada Mihalcea<sup id="id3.3.3.3.3.3" class="ltx_sup"><span id="id3.3.3.3.3.3.1" class="ltx_text ltx_font_medium">1</span></sup></span></td>
</tr>
<tr id="id5.5.5" class="ltx_tr">
<td id="id5.5.5.2" class="ltx_td ltx_align_center">
<sup id="id5.5.5.2.1" class="ltx_sup"><span id="id5.5.5.2.1.1" class="ltx_text ltx_font_italic">1,3</span></sup>University of Michigan, <sup id="id5.5.5.2.2" class="ltx_sup">2</sup>National Institute of Astrophysics, Optics and Electronics (INAOE)</td>
</tr>
<tr id="id5.5.6.1" class="ltx_tr">
<td id="id5.5.6.1.1" class="ltx_td ltx_align_center">abzaliev@umich.edu, humbertop@inaoep.mx, mihalcea@umich.edu.com</td>
</tr>
</tbody>
</table>
<p id="id6.p1.2" class="ltx_p ltx_align_center"><span id="id6.p1.2.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Until recently, “what humans do” has been considered the most widely accepted definition of intelligence <cite class="ltx_cite ltx_citemacro_cite">Tomasello (<a href="#bib.bib47" title="" class="ltx_ref">2019</a>)</cite>, but a large body of recent work has demonstrated that there are numerous other forms of non-human intelligence <cite class="ltx_cite ltx_citemacro_cite">Bridle (<a href="#bib.bib7" title="" class="ltx_ref">2022</a>); Call and Carpenter (<a href="#bib.bib8" title="" class="ltx_ref">2001</a>); Biro et al. (<a href="#bib.bib6" title="" class="ltx_ref">2003</a>)</cite>. While there are several new studies demonstrating plant intelligence <cite class="ltx_cite ltx_citemacro_cite">Wohlleben (<a href="#bib.bib49" title="" class="ltx_ref">2016</a>); dos Santos et al. (<a href="#bib.bib13" title="" class="ltx_ref">2024</a>)</cite> most of the research to date has focused on the intelligence of animals <cite class="ltx_cite ltx_citemacro_cite">De Waal (<a href="#bib.bib10" title="" class="ltx_ref">2016</a>); Grandin and Johnson (<a href="#bib.bib14" title="" class="ltx_ref">2009</a>)</cite>. Forms of animal intelligence range from memory <cite class="ltx_cite ltx_citemacro_cite">Matzel and Kolata (<a href="#bib.bib25" title="" class="ltx_ref">2010</a>)</cite> and problem-solving <cite class="ltx_cite ltx_citemacro_cite">Seed and Call (<a href="#bib.bib41" title="" class="ltx_ref">2010</a>)</cite>, all the way to the use of tools <cite class="ltx_cite ltx_citemacro_cite">St Amant and Horton (<a href="#bib.bib44" title="" class="ltx_ref">2008</a>)</cite> and communication <cite class="ltx_cite ltx_citemacro_cite">Seyfarth and Cheney (<a href="#bib.bib42" title="" class="ltx_ref">2003</a>); López (<a href="#bib.bib23" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Like humans, animals use both verbal and non-verbal forms of communication, including audio signals such as calls, songs, or hisses; visual signals such as facial expressions, tail moves, or postural gestures; chemical cues; tactile cues; and bioluminescence. In general, the study of animal communication has been mainly addressed in fields such as biology, ecology, and anthropology, including, for instance, prairie dogs <cite class="ltx_cite ltx_citemacro_cite">Slobodchikoff et al. (<a href="#bib.bib43" title="" class="ltx_ref">2009</a>)</cite>, birds <cite class="ltx_cite ltx_citemacro_cite">Thorpe (<a href="#bib.bib46" title="" class="ltx_ref">1961</a>)</cite> or body movement in bees <cite class="ltx_cite ltx_citemacro_cite">Al Toufailia et al. (<a href="#bib.bib1" title="" class="ltx_ref">2013</a>)</cite>. Only recently we have started to see research that leverages advances in machine learning <cite class="ltx_cite ltx_citemacro_cite">Bergler et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>); Jasim et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>); Maegawa et al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Focusing specifically on animal vocal communication, a recent study <cite class="ltx_cite ltx_citemacro_cite">Andreas et al. (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite> highlighted three main questions to be answered to increase our understanding of how animals communicate: (1) What are the phonetic and perceptual units used by animals? (phonemes); (2) What are the composition rules used to combine those units? (morphology,  syntax); and (3) Do those units carry meaning and, if so, how do we map the sound units to their meaning? (semantics,  pragmatics).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we explore the third question and specifically attempt to understand the semantics of dog vocalizations. We use a state-of-the-art human speech representation learning model and show that such models can predict the context of a bark.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This paper makes three main contributions. First, we introduce a dataset and a set of tasks for dog bark classification. We draw parallels between human speech classification tasks and dog bark classification tasks, including dog recognition, breed recognition, gender identification, and context grounding.
Second, through several experiments, we show that we can leverage models developed for human speech processing to explore dog vocalizations and demonstrate that these can be used to significantly enhance performance on several dog bark classification tasks. Finally, through this work, we hope to open new opportunities for research in the area of animal communication, which can leverage the extensive expertise available in the NLP community.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<div id="S1.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:336.6pt;height:211.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-42.1pt,26.4pt) scale(0.8,0.8) ;">
<table id="S1.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Context</th>
<th id="S1.T1.1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t"># segments</th>
<th id="S1.T1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">Duration (sec)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.2.1" class="ltx_tr">
<td id="S1.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Very aggressive barking at a stranger (L-S2)</td>
<td id="S1.T1.1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">2,843</td>
<td id="S1.T1.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">2,778.66</td>
</tr>
<tr id="S1.T1.1.1.3.2" class="ltx_tr">
<td id="S1.T1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">Normal barking at a stranger (L-S1)</td>
<td id="S1.T1.1.1.3.2.2" class="ltx_td ltx_align_right">2,772</td>
<td id="S1.T1.1.1.3.2.3" class="ltx_td ltx_align_right">2,512.92</td>
</tr>
<tr id="S1.T1.1.1.4.3" class="ltx_tr">
<td id="S1.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r">Barking due to assault on the owner (L-A)</td>
<td id="S1.T1.1.1.4.3.2" class="ltx_td ltx_align_right">829</td>
<td id="S1.T1.1.1.4.3.3" class="ltx_td ltx_align_right">956.58</td>
</tr>
<tr id="S1.T1.1.1.5.4" class="ltx_tr">
<td id="S1.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r">Negative grunt (during the presence of a stranger) (GR-N)</td>
<td id="S1.T1.1.1.5.4.2" class="ltx_td ltx_align_right">637</td>
<td id="S1.T1.1.1.5.4.3" class="ltx_td ltx_align_right">746.60</td>
</tr>
<tr id="S1.T1.1.1.6.5" class="ltx_tr">
<td id="S1.T1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r">Negative squeal (during the presence of a stranger) (CH-N)</td>
<td id="S1.T1.1.1.6.5.2" class="ltx_td ltx_align_right">298</td>
<td id="S1.T1.1.1.6.5.3" class="ltx_td ltx_align_right">546.72</td>
</tr>
<tr id="S1.T1.1.1.7.6" class="ltx_tr">
<td id="S1.T1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r">Sadness/anxiety barking (L-TA)</td>
<td id="S1.T1.1.1.7.6.2" class="ltx_td ltx_align_right">288</td>
<td id="S1.T1.1.1.7.6.3" class="ltx_td ltx_align_right">200.27</td>
</tr>
<tr id="S1.T1.1.1.8.7" class="ltx_tr">
<td id="S1.T1.1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_r">Positive squeal (during gameplay) (CH-P)</td>
<td id="S1.T1.1.1.8.7.2" class="ltx_td ltx_align_right">91</td>
<td id="S1.T1.1.1.8.7.3" class="ltx_td ltx_align_right">150.49</td>
</tr>
<tr id="S1.T1.1.1.9.8" class="ltx_tr">
<td id="S1.T1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_r">Barking during play (L-P)</td>
<td id="S1.T1.1.1.9.8.2" class="ltx_td ltx_align_right">76</td>
<td id="S1.T1.1.1.9.8.3" class="ltx_td ltx_align_right">51.21</td>
</tr>
<tr id="S1.T1.1.1.10.9" class="ltx_tr">
<td id="S1.T1.1.1.10.9.1" class="ltx_td ltx_align_left ltx_border_r">Barking due to stimulation when walking (L-PA)</td>
<td id="S1.T1.1.1.10.9.2" class="ltx_td ltx_align_right">62</td>
<td id="S1.T1.1.1.10.9.3" class="ltx_td ltx_align_right">84.06</td>
</tr>
<tr id="S1.T1.1.1.11.10" class="ltx_tr">
<td id="S1.T1.1.1.11.10.1" class="ltx_td ltx_align_left ltx_border_r">Barking in fear at a stranger (L-S3)</td>
<td id="S1.T1.1.1.11.10.2" class="ltx_td ltx_align_right">54</td>
<td id="S1.T1.1.1.11.10.3" class="ltx_td ltx_align_right">45.08</td>
</tr>
<tr id="S1.T1.1.1.12.11" class="ltx_tr">
<td id="S1.T1.1.1.12.11.1" class="ltx_td ltx_align_left ltx_border_r">Positive grunt (during gameplay) (GR-P)</td>
<td id="S1.T1.1.1.12.11.2" class="ltx_td ltx_align_right">51</td>
<td id="S1.T1.1.1.12.11.3" class="ltx_td ltx_align_right">79.56</td>
</tr>
<tr id="S1.T1.1.1.13.12" class="ltx_tr">
<td id="S1.T1.1.1.13.12.1" class="ltx_td ltx_align_left ltx_border_r">Barking arrival of the owner at home (L-H)</td>
<td id="S1.T1.1.1.13.12.2" class="ltx_td ltx_align_right">24</td>
<td id="S1.T1.1.1.13.12.3" class="ltx_td ltx_align_right">26.20</td>
</tr>
<tr id="S1.T1.1.1.14.13" class="ltx_tr">
<td id="S1.T1.1.1.14.13.1" class="ltx_td ltx_align_left ltx_border_r">Barking that is neither playful nor strange (L-O)</td>
<td id="S1.T1.1.1.14.13.2" class="ltx_td ltx_align_right">9</td>
<td id="S1.T1.1.1.14.13.3" class="ltx_td ltx_align_right">9.50</td>
</tr>
<tr id="S1.T1.1.1.15.14" class="ltx_tr">
<td id="S1.T1.1.1.15.14.1" class="ltx_td ltx_align_left ltx_border_r">Non-dog sounds (voices, TV, cars, appliances, etc.) (S)</td>
<td id="S1.T1.1.1.15.14.2" class="ltx_td ltx_align_right">8,755</td>
<td id="S1.T1.1.1.15.14.3" class="ltx_td ltx_align_right">14,304.05</td>
</tr>
<tr id="S1.T1.1.1.16.15" class="ltx_tr">
<td id="S1.T1.1.1.16.15.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span id="S1.T1.1.1.16.15.1.1" class="ltx_text ltx_font_smallcaps">Total</span></td>
<td id="S1.T1.1.1.16.15.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_t">16,789</td>
<td id="S1.T1.1.1.16.15.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_t">22,491</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>14 types of dog vocalizations together with the corresponding number of segments and duration.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Animal Communication Datasets.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Compared to human languages, there are significantly fewer datasets available for animal communication. The largest library of animal vocalizations is the Macaulay Library at the Cornell Lab of Ornithology,<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.macaulaylibrary.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.macaulaylibrary.org</a></span></span></span> which includes audio, photos, and videos of 2,674 species of amphibians, fish, mammals, and more, with the main focus of the library on birds. Another large library of animal vocalizations is the Animal Sound Archive<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.tierstimmenarchiv.de/webinterface/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tierstimmenarchiv.de/webinterface/</a></span></span></span>, which covers 1,800 bird species and 580 mammal species.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">There are also several datasets related to marine mammals. <cite class="ltx_cite ltx_citemacro_citet">Ness et al. (<a href="#bib.bib29" title="" class="ltx_ref">2013</a>)</cite> presented a large dataset of over 20,000 recordings of Orca vocalizations. The Watkins Marine Mammal Sound Database<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://cis.whoi.edu/science/B/whalesounds/index.cfm" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cis.whoi.edu/science/B/whalesounds/index.cfm</a></span></span></span> contains 15,000 annotated sound clips for more than 60 species of marine mammals.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p3.1" class="ltx_p">Specifically for dog vocalizations, one of the most popular datasets was introduced by <cite class="ltx_cite ltx_citemacro_citet">Pongrácz et al. (<a href="#bib.bib38" title="" class="ltx_ref">2005</a>)</cite>. It includes twelve Mudi dogs and consists of 244 recordings. Another dataset is the UT3 database <cite class="ltx_cite ltx_citemacro_citet">Gutiérrez-Serafín et al. (<a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>, with 74 dogs and  6,000 individual audios. Neither of these datasets is publicly available.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Computational Approaches to Animal Communication Analysis.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Several studies have applied machine learning to animal communication, most of which used Convolutional Neural Networks (CNNs) to classify bird calls <cite class="ltx_cite ltx_citemacro_citet">Maegawa et al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>); Jasim et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>, primate species <cite class="ltx_cite ltx_citemacro_citet">Pellegrini (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>); Oikarinen et al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite>, multi-species classification of birds and frogs <cite class="ltx_cite ltx_citemacro_citet">LeBien et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>, or orca sounds <cite class="ltx_cite ltx_citemacro_cite">Bergler et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Ntalampiras (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite> used various methods to transfer the signal from music genre identification to bird species identification.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">Specifically for dogs, there have been several studies studying dog vocalizations <cite class="ltx_cite ltx_citemacro_cite">Pérez-Espinosa and
Torres-García (<a href="#bib.bib37" title="" class="ltx_ref">2019</a>); Pérez-Espinosa et al. (<a href="#bib.bib36" title="" class="ltx_ref">2015</a>); Gutiérrez-Serafín et al. (<a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>. Our work is more closely related to <cite class="ltx_cite ltx_citemacro_cite">Yin and McCowan (<a href="#bib.bib51" title="" class="ltx_ref">2004</a>)</cite>, where the contexts in which barking occurs are predicted, along with individual dog recognition. The results of the experiments of <cite class="ltx_cite ltx_citemacro_citet">Hantke et al. (<a href="#bib.bib17" title="" class="ltx_ref">2018</a>)</cite> confirmed that one can predict the context of the bark. <cite class="ltx_cite ltx_citemacro_citet">Molnár et al. (<a href="#bib.bib27" title="" class="ltx_ref">2009</a>)</cite> also finds that the barks include information about the individual dog, as well as information about the context. However, no pre-trained models for dog vocalizations are currently available. To our knowledge, we are the first to use neural acoustic representations for tasks on dog vocalizations, and we are also the first to explore the use of human speech pre-training.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We use a dataset consisting of recordings of 74 dogs, collected in Tepic (Mexico) and Puebla (Mexico), at the homes of the dogs’ owners. A subset of this dataset was previously used by <cite class="ltx_cite ltx_citemacro_citet">Pérez-Espinosa et al. (<a href="#bib.bib35" title="" class="ltx_ref">2018</a>)</cite>. The dog vocalizations were recorded while being exposed to different stimuli (e.g., stranger, play, see Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The recordings were conducted using a video camera Sony CX405 Handycam; in this work, we only use the audio recordings, obtained using the built-in microphone on the camera. The audio codec is A52 stereo with a sampling rate of 48,000 Hz and a bit rate of 256 kbps. The protocol for obtaining the dog vocalizations used in this study was designed and validated by experts in animal behavior from the Tlaxcala Center for Behavioral Biology in Mexico.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The dataset includes recordings of 48 female and 26 male dogs, mostly of three breeds: 42 Chihuahua, 21 French Poodles, and 11 Schnauzer. For mixed breeds, we first selected the breed mentioned. We focused on these breeds since they are among the most common domestic breeds in Mexican households. Given time and resource constraints during the data collection process, these breeds allowed for a broader choice of participants. The dog’s average age is 35 months, ranging between 5 to 84 months old.</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Stimuli.</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">Dog vocalizations were induced by exposing them to several stimuli, with the participation of the owner and/or an experimenter. To illustrate, the following represent examples of situations used during the data collection: the experimenter repeatedly rings the home doorbell and knocks the door hard; the experimenter simulates an attack on the owner; the owner speaks affectionately to the dog; the owner stimulates the dog using the objects or toys with which the dog normally plays; the owner performs the normal routine that precedes a walk; the owner ties the dog on a leash to a tree and walks out of sight (see Figure <a href="#S3.F1" title="Figure 1 ‣ Stimuli. ‣ 3. Dataset ‣ Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> as an example); and others. The dogs are recorded while reacting to these stimuli, resulting in recordings lasting between 10 sec to 60 min.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2404.18739/assets/figures/screenshot.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Data collection for the stimulus “playing with toy”; the owner stimulates the dog using the toys with which the dog normally plays.</figcaption>
</figure>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Processing and Annotation.</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">The recordings are automatically segmented into shorter segments ranging between 0.3 to 5 sec in length. The segmentation is performed using a threshold to separate between sound and silence or background noise; the threshold was identified using the short-time energy and spectral centroid aspects of the acoustic signal. Only the sound segments are used for the experiments.
Each of the resulting segments was manually annotated using the information associated with the stimulus. One of the fourteen contexts was assigned to each segment; if the audio did not have any dog-related sounds, it was assigned a Non-dog sound label.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p2.1" class="ltx_p">Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the fourteen labels used in the annotation, along with the corresponding statistics for the number of segments and total duration.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Dog Bark Classification Tasks</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Using acoustic representations of dog barks, we explore several fundamental tasks, including the recognition of individual dogs; the identification of the breed of a dog; the identification of a dog’s gender; and the grounding of a dog bark to its context. These tasks have counterparts in human speech analysis, such as speaker identification or grounded language analysis.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Leveraging human speech for acoustic dog bark representations</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">To create acoustic representations of the dog vocalizations in the dataset, we fine-tune a pre-trained state-of-the-art self-supervised speech representation model. We use Wav2Vec2 <cite class="ltx_cite ltx_citemacro_cite">Baevski et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, which uses a self-supervised training objective to predict masked latent representations, pre-trained on the Librispeech corpus <cite class="ltx_cite ltx_citemacro_cite">Panayotov et al. (<a href="#bib.bib32" title="" class="ltx_ref">2015</a>)</cite>. Wav2vec2 uses 960
hours of unlabeled human-speech data to learn how to represent audio signals as a sequence of discrete tokens. Some of those discrete tokens are masked, similar to the process used to train the BERT contextual embedding model <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>. In Wav2Vec2, the learning of discrete units and unmasking are happening simultaneously.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p2.1" class="ltx_p">We use an open-source implementation of Wav2Vec2 from HuggingFace <cite class="ltx_cite ltx_citemacro_cite">Wolf et al. (<a href="#bib.bib50" title="" class="ltx_ref">2019</a>)</cite>. We experiment with two model versions: (1) a model trained from scratch, using the dog vocalizations dataset in Section <a href="#S3" title="3. Dataset ‣ Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>; (2) a model pre-trained on 960 hours of unlabeled human speech data, and fine-tuned on dog vocalizations.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Experimental Setup.</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">All the experiments use a ten-fold cross-validation setup. Specifically, for the tasks of breed identification, gender identification, and grounding, we use <span id="S4.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">grouped</span> ten-fold validation, with individual dogs being the group variable. That is, we leave 7-8 dogs as a test dataset and train on the remaining dogs’ vocalizations, to control for any confounding information. For the dog recognition task, given the goal to recognize individual dogs, the model has to see each class (i.e., each dog) during the training, and thus all 74 individual dogs have to be present in both the training and test datasets. We note that this particular way of cross-validating might enable easier learning for the model and does not prevent shortcut learning, which is a common drawback for all author identification tasks. Therefore even Wav2Vec2 pretrained from scratch performs relatively well, and the performance boost is more pronounced than for other tasks.</p>
</div>
</section>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.1.   Dog Recognition</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We formulate this task as classifying a single audio segment as belonging to one of the 74 dogs in the dataset. According to <cite class="ltx_cite ltx_citemacro_cite">Molnár et al. (<a href="#bib.bib26" title="" class="ltx_ref">2006</a>)</cite> humans struggle to discriminate between individual dog barks, but machine learning methods, even unsupervised, can perform rather well <cite class="ltx_cite ltx_citemacro_cite">Yin and McCowan (<a href="#bib.bib51" title="" class="ltx_ref">2004</a>)</cite>. This task is similar to identifying speakers, where many datasets <cite class="ltx_cite ltx_citemacro_cite">Nagrani et al. (<a href="#bib.bib28" title="" class="ltx_ref">2017</a>); Chung et al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> and methods <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>); Ding et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite> already exist.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4.1. Dog Recognition ‣ 4. Dog Bark Classification Tasks ‣ Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results, where we apply the Wav2Vec2 model to dog identification. Our results are in line with the results from <cite class="ltx_cite ltx_citemacro_cite">Pérez-Espinosa et al. (<a href="#bib.bib35" title="" class="ltx_ref">2018</a>); Molnár et al. (<a href="#bib.bib27" title="" class="ltx_ref">2009</a>)</cite>, and demonstrate that effectiveness of acoustic representations to discriminate between individual dogs. Further, we find that a model pre-trained on human speech significantly outperforms the model trained from scratch.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Given the differences between human speech and animal vocalizations, we still need more work to understand how pre-training on human speech improves the performance on dog vocalization tasks. We believe that the pre-training on human speech enables the model to learn abstract vocalization structures, which in turn are beneficial for understanding animal vocalizations. This hypothesis is supported by previous studies showing that pre-training on seemingly unrelated tasks can be beneficial, for instance, pretraining on symbolic music data and applying it to natural language data provides significant performance improvements due to the ability of the neural networks used by the model to represent abstract syntactic structure <cite class="ltx_cite ltx_citemacro_cite">Papadimitriou and Jurafsky (<a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>. Similarly, in computer vision, pre-training on ImageNet (object recognition) data is found to improve radiography processing <cite class="ltx_cite ltx_citemacro_cite">Jabbour et al. (<a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:145.5pt;height:52.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.2pt,6.6pt) scale(0.8,0.8) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Method</th>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Accuracy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.2.1" class="ltx_tr">
<th id="S4.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Majority</th>
<td id="S4.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">5.03%</td>
</tr>
<tr id="S4.T2.1.1.3.2" class="ltx_tr">
<th id="S4.T2.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wav2Vec2 (from scratch)</th>
<td id="S4.T2.1.1.3.2.2" class="ltx_td ltx_align_center">23.74%</td>
</tr>
<tr id="S4.T2.1.1.4.3" class="ltx_tr">
<th id="S4.T2.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Wav2Vec2 (pre-trained)</th>
<td id="S4.T2.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.4.3.2.1" class="ltx_text ltx_font_bold">49.95%</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Accuracy for the dog recognition task.</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:289.9pt;height:66pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.2pt,8.2pt) scale(0.8,0.8) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
<th id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3">F-1 measure</th>
</tr>
<tr id="S4.T3.1.1.2.2" class="ltx_tr">
<th id="S4.T3.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Method</th>
<th id="S4.T3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Acc.</th>
<th id="S4.T3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Chihuahua</th>
<th id="S4.T3.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">French Poodle</th>
<th id="S4.T3.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Schnauzer</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.3.1" class="ltx_tr">
<th id="S4.T3.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Majority</th>
<td id="S4.T3.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">58.76%</td>
<td id="S4.T3.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">61.49%</td>
<td id="S4.T3.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">6.59%</td>
<td id="S4.T3.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">6.78%</td>
</tr>
<tr id="S4.T3.1.1.4.2" class="ltx_tr">
<th id="S4.T3.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wav2Vec2 (from scratch)</th>
<td id="S4.T3.1.1.4.2.2" class="ltx_td ltx_align_center">60.18%</td>
<td id="S4.T3.1.1.4.2.3" class="ltx_td ltx_align_center">74.42%</td>
<td id="S4.T3.1.1.4.2.4" class="ltx_td ltx_align_center">14.96%</td>
<td id="S4.T3.1.1.4.2.5" class="ltx_td ltx_align_center">5.79%</td>
</tr>
<tr id="S4.T3.1.1.5.3" class="ltx_tr">
<th id="S4.T3.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Wav2Vec2 (pre-trained)</th>
<td id="S4.T3.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.5.3.2.1" class="ltx_text ltx_font_bold">62.28%</span></td>
<td id="S4.T3.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.5.3.3.1" class="ltx_text ltx_font_bold">74.47%</span></td>
<td id="S4.T3.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.5.3.4.1" class="ltx_text ltx_font_bold">36.11%</span></td>
<td id="S4.T3.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.5.3.5.1" class="ltx_text ltx_font_bold">14.88%</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy and F-1 measure for dog breed identification.</figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:275.9pt;height:66pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.5pt,8.2pt) scale(0.8,0.8) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T4.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
<th id="S4.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4">F-1 measure</th>
</tr>
<tr id="S4.T4.1.1.2.2" class="ltx_tr">
<th id="S4.T4.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Method</th>
<th id="S4.T4.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Acc.</th>
<th id="S4.T4.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">L-S2</th>
<th id="S4.T4.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CH-N</th>
<th id="S4.T4.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">GR-N</th>
<th id="S4.T4.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">L-S1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.3.1" class="ltx_tr">
<th id="S4.T4.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Majority</th>
<td id="S4.T4.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">56.37%</td>
<td id="S4.T4.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">41.31%</td>
<td id="S4.T4.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0.00%</td>
<td id="S4.T4.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">0.00%</td>
<td id="S4.T4.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">30.39%</td>
</tr>
<tr id="S4.T4.1.1.4.2" class="ltx_tr">
<th id="S4.T4.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wav2Vec2 (from scratch)</th>
<td id="S4.T4.1.1.4.2.2" class="ltx_td ltx_align_center">58.45%</td>
<td id="S4.T4.1.1.4.2.3" class="ltx_td ltx_align_center">49.26%</td>
<td id="S4.T4.1.1.4.2.4" class="ltx_td ltx_align_center">21.26%</td>
<td id="S4.T4.1.1.4.2.5" class="ltx_td ltx_align_center">78.20%</td>
<td id="S4.T4.1.1.4.2.6" class="ltx_td ltx_align_center">48.64%</td>
</tr>
<tr id="S4.T4.1.1.5.3" class="ltx_tr">
<th id="S4.T4.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Wav2Vec2 (pre-trained)</th>
<td id="S4.T4.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.1.5.3.2.1" class="ltx_text ltx_font_bold">62.18%</span></td>
<td id="S4.T4.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.1.5.3.3.1" class="ltx_text ltx_font_bold">49.66%</span></td>
<td id="S4.T4.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.1.5.3.4.1" class="ltx_text ltx_font_bold">45.26%</span></td>
<td id="S4.T4.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.1.5.3.5.1" class="ltx_text ltx_font_bold">90.70%</span></td>
<td id="S4.T4.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.1.5.3.6.1" class="ltx_text ltx_font_bold">51.13%</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Accuracy and F-1 measure for context grounding.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.2.   Breed Identification</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In this task, we aim to predict the breed of a dog. Our dataset contains mostly three breeds: Chihuahua, French Poodle, and Schnauzer. We hypothesize that different breeds have different pitches so the acoustic model should be able to identify those differences, independent of the context. This experiment is related to previous work <cite class="ltx_cite ltx_citemacro_cite">LeBien et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>); Oikarinen et al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite>. The task is similar to human accent recognition, where given audio files in a single language (i.e., English) the goal is to classify the accent of a speaker (e.g., USA vs. UK vs. India), with several approaches proposed in previous work <cite class="ltx_cite ltx_citemacro_cite">Ayrancı et al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>); Honnavalli and Shylaja (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>); Sun (<a href="#bib.bib45" title="" class="ltx_ref">2002</a>)</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The results are shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.1. Dog Recognition ‣ 4. Dog Bark Classification Tasks ‣ Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Wav2Vec2 trained from scratch outperforms most baselines. As before, we obtain an additional significant boost in performance when pre-training on human speech data. The variation in individual breeds can be explained by the unbalanced number of observations per breed, with Chihuahua being the most common breed in our dataset (57%), followed by French Poodle (28%) and Schnauzer (15%).</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.3.   Gender Identification</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The goal of this task is to probe whether it is possible to predict the gender of a dog from vocalizations. This is a task analogous to the prediction of demographics (e.g., age, gender) from language or speech, with many previous studies conducted on this topic <cite class="ltx_cite ltx_citemacro_cite">Qawaqneh et al. (<a href="#bib.bib39" title="" class="ltx_ref">2017</a>); Saraf et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023</a>); Gupta et al. (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>); Welch et al. (<a href="#bib.bib48" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<div id="S4.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:206.9pt;height:66pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.9pt,8.2pt) scale(0.8,0.8) ;">
<table id="S4.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T5.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
<th id="S4.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">F-1 measure</th>
</tr>
<tr id="S4.T5.1.1.2.2" class="ltx_tr">
<th id="S4.T5.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Method</th>
<th id="S4.T5.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Acc.</th>
<th id="S4.T5.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Female</th>
<th id="S4.T5.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Male</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.3.1" class="ltx_tr">
<th id="S4.T5.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Majority</th>
<td id="S4.T5.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">68.70%</td>
<td id="S4.T5.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">74.73%</td>
<td id="S4.T5.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">7.76%</td>
</tr>
<tr id="S4.T5.1.1.4.2" class="ltx_tr">
<th id="S4.T5.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wav2Vec2 (from scratch)</th>
<td id="S4.T5.1.1.4.2.2" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.4.2.2.1" class="ltx_text ltx_font_bold">70.07%</span></td>
<td id="S4.T5.1.1.4.2.3" class="ltx_td ltx_align_center">76.54%</td>
<td id="S4.T5.1.1.4.2.4" class="ltx_td ltx_align_center"><span id="S4.T5.1.1.4.2.4.1" class="ltx_text ltx_font_bold">19.29%</span></td>
</tr>
<tr id="S4.T5.1.1.5.3" class="ltx_tr">
<th id="S4.T5.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Wav2Vec2 (pre-trained)</th>
<td id="S4.T5.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_b">68.90%</td>
<td id="S4.T5.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.1.1.5.3.3.1" class="ltx_text ltx_font_bold">79.31%</span></td>
<td id="S4.T5.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_b">7.10%</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Accuracy and F-1 measure for dog gender identification.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Table <a href="#S4.T5" title="Table 5 ‣ 4.3. Gender Identification ‣ 4. Dog Bark Classification Tasks ‣ Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the results. The Wav2Vec2 model trained from scratch performs better than the baseline model, with no further improvements obtained with Wav2Vec2 pre-trained on human speech. Interestingly, we do see an improvement brought by human speech pre-training on the female class, for which we have significantly more data in our dataset (67.95% female vs 32.04% male by total duration). We found that gender identification is the most difficult task among all the tasks we propose. We hypothesize that the model trained from scratch focuses on learning acoustic features, while the pre-trained wav2vec attempts to learn shortcuts and overfits quickly. We noticed that it often predicts just the majority class (female) so that F1 increases for female and decreases for male, while the overall accuracy is almost the same as for the majority baseline.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.4.   Grounding</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In this task, we predict the context of the bark; i.e., we determine the association between a dog vocalization and its surrounding. Because of the highly skewed label distribution (see Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), we focus on the contexts for which more examples are available: very aggressive barking at a stranger (L-S2); normal barking at a stranger (L-S1); negative squeal (in the presence of a stranger) (CH-N); negative grunt (in the presence of a stranger) (GR-N). We do not include barking due to assault on the owner (L-A) because in early experiments we found that the model cannot distinguish it from the very aggressive barking at a stranger (L-S2).</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Human language grounding is the mapping of language symbols such as words to their corresponding objects in the real world. There have been several works showing that animals ground their vocalizations as well. For instance, the vocalizations of prairie dogs are grounded and used to transmit the characteristics of the predators (e.g., color or size) <cite class="ltx_cite ltx_citemacro_cite">Slobodchikoff et al. (<a href="#bib.bib43" title="" class="ltx_ref">2009</a>)</cite>. Other work has also demonstrated that it is possible to predict call types for marmoset monkeys <cite class="ltx_cite ltx_citemacro_cite">Oikarinen et al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> also shows. We hypothesize that dog vocalizations are related to their context, and therefore can be grounded.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">Table <a href="#S4.T4" title="Table 4 ‣ 4.1. Dog Recognition ‣ 4. Dog Bark Classification Tasks ‣ Towards Dog Bark Decoding: Leveraging Human Speech Processing for Automated Bark Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the results. Similar to the previous experiments, both Wav2Vec2 models outperform the majority baseline, with the Wav2Vec2 pre-trained on human speech leading to the most accurate results.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we explored the use of pre-trained self-supervised speech representation models to address dog barking classification tasks. We specifically addressed four tasks that find parallels in human-centered speech recognition tasks: dog recognition, breed recognition, gender identification, and context grounding. We showed that acoustic representation models using Wav2Vec2 can significantly improve over simpler classification baselines. Additionally, we found that a model pre-trained on human speech can further improve the results. We hope our work will encourage others in the NLP community to start addressing the many research opportunities that exist in the area of animal communication. The dataset used in this work, along with the baselines that we introduced, are publicly available by request from <span id="S5.p1.1.1" class="ltx_text ltx_font_typewriter">humbertop@ccc.inaoep.mx</span>.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Limitations</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we focused on only one species, domestic dogs, and only three breeds. More species are required to understand how modern computational methods can be used for studying animal vocalization. In the future, we are planning to extend our work to birds and marine mammals, since those species have a large amount of data available.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">We also focused on only one neural network architecture, Wav2Vec2. While it is a popular architecture for human speech processing, other architectures might be more suitable for studying animal vocalizations. Also, we used supervised learning in this work, since the dataset was manually annotated. The majority of the datasets are not annotated and thus would require semi-supervised or unsupervised learning, which is more challenging.</p>
</div>
<div id="S6.p3" class="ltx_para">
<span id="S6.p3.1" class="ltx_ERROR undefined">\c@NAT@ctr</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography"></h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Al Toufailia et al. (2013)</span>
<span class="ltx_bibblock">
Hasan Al Toufailia, Margaret J Couvillon, Francis LW Ratnieks, and Christoph
Grüter. 2013.

</span>
<span class="ltx_bibblock">Honey bee waggle dance communication: signal meaning and signal noise
affect dance follower behaviour.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Behavioral Ecology and Sociobiology</em>, 67:549–556.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andreas et al. (2022)</span>
<span class="ltx_bibblock">
Jacob Andreas, Gašper Beguš, Michael M. Bronstein, Roee Diamant, Denley
Delaney, Shane Gero, Shafi Goldwasser, David F. Gruber, Sarah de Haas,
Peter Malkin, Nikolay Pavlov, Roger Payne, Giovanni Petri, Daniela Rus,
Pratyusha Sharma, Dan Tchernov, Pernille Tønnesen, Antonio Torralba, Daniel
Vogt, and Robert J. Wood. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.isci.2022.104393" title="" class="ltx_ref ltx_href">Toward understanding the communication in sperm whales</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">iScience</em>, 25(6):104393.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ayrancı et al. (2020)</span>
<span class="ltx_bibblock">
Ahmet Aytuğ Ayrancı, Sergen Atay, and Tülay Yıldırım.
2020.

</span>
<span class="ltx_bibblock">Speaker accent recognition using machine learning algorithms.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">2020 Innovations in Intelligent Systems and Applications
Conference (ASYU)</em>, pages 1–6. IEEE.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2006.11477" title="" class="ltx_ref ltx_href">wav2vec 2.0: A framework for
self-supervised learning of speech representations</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bergler et al. (2019)</span>
<span class="ltx_bibblock">
Christian Bergler, Manuel Schmitt, Rachael Xi Cheng, Andreas K Maier, Volker
Barth, and Elmar Nöth. 2019.

</span>
<span class="ltx_bibblock">Deep learning for orca call type Identification-A fully
unsupervised approach.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, pages 3357–3361. isca-speech.org.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biro et al. (2003)</span>
<span class="ltx_bibblock">
Dora Biro, Noriko Inoue-Nakamura, Rikako Tonooka, Gen Yamakoshi, Cláudia
Sousa, and Tetsuro Matsuzawa. 2003.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:651756" title="" class="ltx_ref ltx_href">Cultural
innovation and transmission of tool use in wild chimpanzees: evidence from
field experiments</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Animal Cognition</em>, 6:213–223.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bridle (2022)</span>
<span class="ltx_bibblock">
James Bridle. 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Ways of being: Beyond human intelligence</em>.

</span>
<span class="ltx_bibblock">Penguin UK.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Call and Carpenter (2001)</span>
<span class="ltx_bibblock">
Josep Call and Malinda Carpenter. 2001.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:3894710" title="" class="ltx_ref ltx_href">Do apes and
children know what they have seen?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Animal Cognition</em>, 3:207–220.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2018)</span>
<span class="ltx_bibblock">
Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. 2018.

</span>
<span class="ltx_bibblock">Voxceleb2: Deep speaker recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De Waal (2016)</span>
<span class="ltx_bibblock">
Frans De Waal. 2016.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Are we smart enough to know how smart animals are?</em>

</span>
<span class="ltx_bibblock">WW Norton &amp; Company.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. (2020)</span>
<span class="ltx_bibblock">
Shaojin Ding, Tianlong Chen, Xinyu Gong, Weiwei Zha, and Zhangyang Wang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2005.03215" title="" class="ltx_ref ltx_href">Autospeech: Neural
architecture search for speaker recognition</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">dos Santos et al. (2024)</span>
<span class="ltx_bibblock">
Luana Silva dos Santos, Victor Hugo Silva dos Santos, and Fabio Rubio Scarano.
2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:267564291" title="" class="ltx_ref ltx_href">Plant
intelligence: history and current trends</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Theoretical and Experimental Plant Physiology</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grandin and Johnson (2009)</span>
<span class="ltx_bibblock">
Temple Grandin and Catherine Johnson. 2009.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Animals in translation: Using the mysteries of autism to decode
animal behavior</em>.

</span>
<span class="ltx_bibblock">SUNY Press.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2022)</span>
<span class="ltx_bibblock">
Tarun Gupta, Duc-Tuan Truong, Tran The Anh, and Chng Eng Siong. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2203.11774" title="" class="ltx_ref ltx_href">Estimation of speaker age
and height from speech signal using bi-encoder transformer mixture model</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gutiérrez-Serafín et al. (2019)</span>
<span class="ltx_bibblock">
Benjamín Gutiérrez-Serafín, Humberto Pérez Espinosa, Juan
Martínez-Miranda, and Ismael Edrein Espinosa-Curiel. 2019.

</span>
<span class="ltx_bibblock">Classification of barking context of domestic dog using high-level
descriptors.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Res. Comput. Sci.</em>, 148(3):23–35.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hantke et al. (2018)</span>
<span class="ltx_bibblock">
Simone Hantke, Nicholas Cummins, and Bjorn Schuller. 2018.

</span>
<span class="ltx_bibblock">What is my dog trying to tell me? the automatic recognition of the
context and perceived emotion of dog barks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</em>, pages 5134–5138. IEEE.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honnavalli and Shylaja (2021)</span>
<span class="ltx_bibblock">
Dweepa Honnavalli and SS Shylaja. 2021.

</span>
<span class="ltx_bibblock">Supervised machine learning model for accent recognition in english
speech using sequential mfcc features.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in Artificial Intelligence and Data Engineering:
Select Proceedings of AIDE 2019</em>, pages 55–66. Springer.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023)</span>
<span class="ltx_bibblock">
Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech
Galuba, Florian Metze, and Christoph Feichtenhofer. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2207.06405" title="" class="ltx_ref ltx_href">Masked autoencoders that
listen</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jabbour et al. (2020)</span>
<span class="ltx_bibblock">
Sarah Jabbour, David Fouhey, Ella Kazerooni, Michael W. Sjoding, and Jenna
Wiens. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2009.10132" title="" class="ltx_ref ltx_href">Deep learning applied to
chest x-rays: Exploiting and preventing shortcuts</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jasim et al. (2022)</span>
<span class="ltx_bibblock">
Hasan Abdullah Jasim, Saadaldeen R Ahmed, Abdullahi Abdu Ibrahim, and
Adil Deniz Duru. 2022.

</span>
<span class="ltx_bibblock">Classify bird species audio by augment convolutional neural network.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2022 International Congress on Human-Computer Interaction,
Optimization and Robotic Applications (HORA)</em>, pages 1–6.
ieeexplore.ieee.org.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeBien et al. (2020)</span>
<span class="ltx_bibblock">
Jack LeBien, Ming Zhong, Marconi Campos-Cerqueira, Julian P Velev, Rahul
Dodhia, Juan Lavista Ferres, and T Mitchell Aide. 2020.

</span>
<span class="ltx_bibblock">A pipeline for identification of bird and frog species in tropical
soundscape recordings using a convolutional neural network.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Ecol. Inform.</em>, 59:101113.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">López (2020)</span>
<span class="ltx_bibblock">
Bruno Díaz López. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:214699954" title="" class="ltx_ref ltx_href">When
personality matters: personality and social structure in wild bottlenose
dolphins, tursiops truncatus</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Animal Behaviour</em>, 163:73–84.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maegawa et al. (2021)</span>
<span class="ltx_bibblock">
Yuko Maegawa, Yuji Ushigome, Masato Suzuki, Karen Taguchi, Keigo Kobayashi,
Chihiro Haga, and Takanori Matsui. 2021.

</span>
<span class="ltx_bibblock">A new survey method using convolutional neural networks for automatic
classification of bird calls.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Ecol. Inform.</em>, 61:101164.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matzel and Kolata (2010)</span>
<span class="ltx_bibblock">
Louis D Matzel and Stefan Kolata. 2010.

</span>
<span class="ltx_bibblock">Selective attention, working memory, and animal intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Neuroscience &amp; Biobehavioral Reviews</em>, 34(1):23–30.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Molnár et al. (2006)</span>
<span class="ltx_bibblock">
Csaba Molnár, Péter Pongrácz, Antal Dóka, and Ádám
Miklósi. 2006.

</span>
<span class="ltx_bibblock">Can humans discriminate between dogs on the base of the acoustic
parameters of barks?

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Behavioural processes</em>, 73(1):76–83.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Molnár et al. (2009)</span>
<span class="ltx_bibblock">
Csaba Molnár, Péter Pongrácz, Tamás Faragó, Antal Dóka, and Ádám
Miklósi. 2009.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.beproc.2009.06.011" title="" class="ltx_ref ltx_href">Dogs discriminate between barks: The effect of context and identity of the
caller</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Behavioural Processes</em>, 82(2):198–201.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nagrani et al. (2017)</span>
<span class="ltx_bibblock">
Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. 2017.

</span>
<span class="ltx_bibblock">Voxceleb: A large-scale speaker identification dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ness et al. (2013)</span>
<span class="ltx_bibblock">
Steven Ness, Helena Symonds, Paul Spong, and George Tzanetakis. 2013.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1307.0589" title="" class="ltx_ref ltx_href">The orchive : Data mining a
massive bioacoustic archive</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ntalampiras (2018)</span>
<span class="ltx_bibblock">
Stavros Ntalampiras. 2018.

</span>
<span class="ltx_bibblock">Bird species identification via transfer learning from music genres.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Ecol. Inform.</em>, 44:76–81.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oikarinen et al. (2019)</span>
<span class="ltx_bibblock">
Tuomas Oikarinen, Karthik Srinivasan, Olivia Meisner, Julia B Hyman, Shivangi
Parmar, Adrian Fanucci-Kiss, Robert Desimone, Rogier Landman, and Guoping
Feng. 2019.

</span>
<span class="ltx_bibblock">Deep convolutional network for animal sound classification and source
attribution using dual audio recordings.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">J. Acoust. Soc. Am.</em>, 145(2):654.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panayotov et al. (2015)</span>
<span class="ltx_bibblock">
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015.

</span>
<span class="ltx_bibblock">Librispeech: an asr corpus based on public domain audio books.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">2015 IEEE international conference on acoustics, speech and
signal processing (ICASSP)</em>, pages 5206–5210. IEEE.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papadimitriou and Jurafsky (2020)</span>
<span class="ltx_bibblock">
Isabel Papadimitriou and Dan Jurafsky. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2004.14601" title="" class="ltx_ref ltx_href">Learning music helps you
read: Using transfer to study linguistic structure in language models</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pellegrini (2021)</span>
<span class="ltx_bibblock">
Thomas Pellegrini. 2021.

</span>
<span class="ltx_bibblock">Deep-learning-based central african primate species classification
with MixUp and SpecAugment.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Interspeech 2021</em>. ut3-toulouseinp.hal.science.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pérez-Espinosa et al. (2018)</span>
<span class="ltx_bibblock">
H. Pérez-Espinosa, V. Reyes-Meza, E. Aguilar-Benitez, and Y. M. &amp;;
Sanzón-Rosas. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/322234.322243" title="" class="ltx_ref ltx_href">Automatic individual
dog recognition based on the acoustic properties of its barks.</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Journal of Intelligent &amp; Fuzzy Systems</em>, 34(5):3273–3280.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pérez-Espinosa et al. (2015)</span>
<span class="ltx_bibblock">
Humberto Pérez-Espinosa, José Martın Pérez-Martınez,
José Ángel Durán-Reynoso, and Verónica Reyes-Meza. 2015.

</span>
<span class="ltx_bibblock">Automatic classification of context in induced barking.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Research in Computing Science</em>, 100:63–74.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pérez-Espinosa and
Torres-García (2019)</span>
<span class="ltx_bibblock">
Humberto Pérez-Espinosa and Alejandro Antonio Torres-García. 2019.

</span>
<span class="ltx_bibblock">Evaluation of quantitative and qualitative features for the acoustic
analysis of domestic dogs’ vocalizations.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Journal of Intelligent &amp; Fuzzy Systems</em>, 36(5):5051–5061.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pongrácz et al. (2005)</span>
<span class="ltx_bibblock">
Péter Pongrácz, Csaba Molnár, Adám Miklósi, and Vilmos
Csányi. 2005.

</span>
<span class="ltx_bibblock">Human listeners are able to classify dog (canis familiaris) barks
recorded in different situations.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Journal of comparative psychology</em>, 119(2):136.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qawaqneh et al. (2017)</span>
<span class="ltx_bibblock">
Zakariya Qawaqneh, Arafat Abu Mallouh, and Buket D Barkana. 2017.

</span>
<span class="ltx_bibblock">Deep neural network framework and transformed mfccs for speaker’s age
and gender classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em>, 115:5–14.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saraf et al. (2023)</span>
<span class="ltx_bibblock">
Amruta Saraf, Ganesh Sivaraman, and Elie Khoury. 2023.

</span>
<span class="ltx_bibblock">A zero-shot approach to identifying children’s speech in automatic
gender classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">2022 IEEE Spoken Language Technology Workshop (SLT)</em>, pages
853–859. IEEE.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seed and Call (2010)</span>
<span class="ltx_bibblock">
AM Seed and Josep Call. 2010.

</span>
<span class="ltx_bibblock">Problem-solving in tool-using and non-tool-using animals.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Encyclopedia of animal behavior</em>, 2:778–785.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seyfarth and Cheney (2003)</span>
<span class="ltx_bibblock">
Robert M Seyfarth and Dorothy L Cheney. 2003.

</span>
<span class="ltx_bibblock">Signalers and receivers in animal communication.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Annual review of psychology</em>, 54(1):145–173.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Slobodchikoff et al. (2009)</span>
<span class="ltx_bibblock">
Con N Slobodchikoff, Andrea Paseka, and Jennifer L Verdolin. 2009.

</span>
<span class="ltx_bibblock">Prairie dog alarm calls encode labels about predator colors.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Animal cognition</em>, 12:435–439.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">St Amant and Horton (2008)</span>
<span class="ltx_bibblock">
Robert St Amant and Thomas E Horton. 2008.

</span>
<span class="ltx_bibblock">Revisiting the definition of animal tool use.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Animal Behaviour</em>, 75(4):1199–1208.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun (2002)</span>
<span class="ltx_bibblock">
Xuejing Sun. 2002.

</span>
<span class="ltx_bibblock">Pitch accent prediction using ensemble machine learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Seventh international conference on spoken language
processing</em>. Citeseer.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorpe (1961)</span>
<span class="ltx_bibblock">
William Homan Thorpe. 1961.

</span>
<span class="ltx_bibblock">Bird-song: the biology of vocal communication and expression in
birds.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tomasello (2019)</span>
<span class="ltx_bibblock">
Michael Tomasello. 2019.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Becoming human: A theory of ontogeny</em>.

</span>
<span class="ltx_bibblock">Harvard University Press.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welch et al. (2020)</span>
<span class="ltx_bibblock">
Charles Welch, Jonathan K. Kummerfeld, Verónica Pérez-Rosas, and Rada
Mihalcea. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.334" title="" class="ltx_ref ltx_href">Compositional demographic word embeddings</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 4076–4089, Online. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wohlleben (2016)</span>
<span class="ltx_bibblock">
Peter Wohlleben. 2016.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">The hidden life of trees: What they feel, how they
communicate—Discoveries from a secret world</em>, volume 1.

</span>
<span class="ltx_bibblock">Greystone Books.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2019)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al. 2019.

</span>
<span class="ltx_bibblock">Huggingface’s transformers: State-of-the-art natural language
processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03771</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin and McCowan (2004)</span>
<span class="ltx_bibblock">
Sophia Yin and Brenda McCowan. 2004.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.anbehav.2003.07.016" title="" class="ltx_ref ltx_href">Barking in domestic dogs: context specificity and individual
identification</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Animal Behaviour</em>, 68(2):343–355.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.18738" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.18739" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.18739">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.18739" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.18740" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 20:56:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
