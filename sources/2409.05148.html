<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.05148] Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis</title><meta property="og:description" content="Within the context of creating new Socially Assistive Robots, emotion recognition has become a key development factor, as it allows the robot to adapt to the user’s emotional state in the wild. In this work, we focused…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.05148">

<!--Generated on Sat Oct  5 19:10:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Emotion recognition ">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>eHealth Center, Faculty of Computer Science, Multimedia and Telecommunicactions, Universitat Oberta de Catalunya, 08016 Barcelona, Spain
<span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>ibenitoal@uoc.edu</span></span></span>, <span id="id1.2" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>cventuraroy@uoc.edu</span></span></span> 
<br class="ltx_break"></span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>MIND/IN2UB, Department of Electronic and Biomedical Engineeering, Universitat de Barcelona, 08028 Barcelona, Spain 
<br class="ltx_break"></span></span></span>
<h1 class="ltx_title ltx_title_document">Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elena Ortega-Beltrán
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Josep Cabacas-Maso
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ismael Benito-Altamirano
</span><span class="ltx_author_notes">1122
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-2504-6123" title="ORCID identifier" class="ltx_ref">0000-0002-2504-6123</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Carles Ventura
</span><span class="ltx_author_notes">11</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Within the context of creating new Socially Assistive Robots, emotion recognition has become a key development factor, as it allows the robot to adapt to the user’s emotional state in the wild. In this work, we focused on the analysis of two voice recording Spanish datasets: ELRA-S0329 and EmoMatchSpanishDB. Specifically, we centered our work in the paralanguage, e. g. the vocal characteristics that go along with the message and clarifies the meaning. We proposed the use of the DeepSpectrum method, which consists of extracting a visual representation of the audio tracks and feeding them to a pretrained CNN model. For the classification task, DeepSpectrum is often paired with a Support Vector Classifier –DS-SVC–, or a Fully-Connected deep-learning classifier –DS-FC–. We compared the results of the DS-SVC and DS-FC architectures with the state-of-the-art (SOTA) for ELRA-S0329 and EmoMatchSpanishDB. Moreover, we proposed our own classifier based upon Attention Mechanisms, namely DS-AM. We trained all models against both datasets, and we found that our DS-AM model outperforms the SOTA models for the datasets and the SOTA DeepSpectrum architectures. Finally, we trained our DS-AM model in one dataset and tested it in the other, to simulate real-world conditions on how biased is the model to the dataset.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Emotion recognition <math id="id1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="id1.m1.1a"><mo id="id1.m1.1.1" xref="id1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id1.m1.1b"><ci id="id1.m1.1.1.cmml" xref="id1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.m1.1c">\cdot</annotation></semantics></math> Paralinguistic <math id="id2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="id2.m2.1a"><mo id="id2.m2.1.1" xref="id2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id2.m2.1b"><ci id="id2.m2.1.1.cmml" xref="id2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.m2.1c">\cdot</annotation></semantics></math> Spanish <math id="id3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="id3.m3.1a"><mo id="id3.m3.1.1" xref="id3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id3.m3.1b"><ci id="id3.m3.1.1.cmml" xref="id3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id3.m3.1c">\cdot</annotation></semantics></math> Deep Spectrum <math id="id4.m4.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="id4.m4.1a"><mo id="id4.m4.1.1" xref="id4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="id4.m4.1b"><ci id="id4.m4.1.1.cmml" xref="id4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="id4.m4.1c">\cdot</annotation></semantics></math> Attention mechanisms
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">During the last decades, an increasing of age population has been observed in many countries, i. e. the USA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> or Western Europe countries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, this has let to the proposal for Socially Assistive Robots (SAR) to help elder people with day-to-day problems, but also aiming to help in with mental struggles related to age <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Within this context, human emotion recognition becomes a key factor in the successful adoption of such technologies, and proposals have came around using artificial intelligence to solve this problem  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this work, we focused on the analysis of voice-recorded data from Spanish speakers, as Spanish is one of the most spoken languages in Europe, South and North America and often does not receive the same attention from researchers than English. We selected two databases: (1) ELRA-S0329 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, a stock dataset from the European Language Resources Association which contains recordings of professional speakers in six emotions (anger, disgust, fear, joy, sadness, surprise) plus a neutral style in fast, slow, soft, loud and normal style, and (2) EmoMatchSpanishDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, a dataset from the <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">Universidad Europea de Madrid</em> which contains recordings of 50 individuals expressing six different emotions (anger, disgust, fear, happiness, sadness, surprise) and a neutral one.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We proposed to follow the work of Amiriparian et al., using DeepSpectrum toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to extract paralinguistic features from the audio data using a pipeline that consisted of: (1) converting audio data into a spectrum representation of this data –a Mel spectrogram–; (2) using pretrained a VGG16 CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> as an image feature extractor; and, (3) using a Support Vector Classifier (SVC) to classify the emotions for each of the datasets. We named this architecture DeepSpectrum-SVC or, simply, DS-SVC (see <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>). We trained this architecture under a 10-fold cross-validation process and compared the results with the state-of-the-art models for each dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div id="S1.F1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:325.2pt;height:96.9pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-80.0pt,23.6pt) scale(0.670308712699587,0.670308712699587) ;"><svg id="S1.F1.1.pic1" class="ltx_picture" height="198.51" overflow="visible" version="1.1" width="662.11"><g transform="translate(0,198.51) matrix(1 0 0 -1 0 0) translate(34.11,0) translate(0,99.26)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -29.5 -17.5)" fill="#000000" stroke="#000000"><foreignObject width="59" height="35" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2409.05148/assets/figures/audio_waveform.png" id="S1.F1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="59" height="35" alt="Refer to caption"></foreignObject></g><g stroke-width="0.8pt"><path d="M 39.37 0 L 77.76 0" style="fill:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 77.76 0)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 88.61 -29.5)" fill="#000000" stroke="#000000"><foreignObject width="59" height="59" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2409.05148/assets/figures/mel_spectrogram.png" id="S1.F1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="59" height="59" alt="Refer to caption"></foreignObject></g><g stroke-width="0.8pt"><path d="M 157.48 0 L 195.87 0" style="fill:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 195.87 0)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g fill="#FFFFB3"><path d="M 212.79 29.54 L 259.65 11.85 L 259.65 -11.85 L 212.79 -29.54 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 217.4 -6.81)" fill="#000000" stroke="#000000"><foreignObject width="37.64" height="13.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.1.pic1.3.3.3.1.1" class="ltx_text ltx_font_sansserif" style="font-size:144%;">F.E.</span></foreignObject></g><g stroke-width="0.8pt"><path d="M 265.75 0 L 284.45 0" style="fill:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 284.45 0)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g fill="#FFFFFF"><path d="M 299.57 -19.69 h 83.23 v 39.37 h -83.23 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 304.19 -6.81)" fill="#000000" stroke="#000000"><foreignObject width="73.5" height="13.62" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.1.pic1.4.4.4.1.1" class="ltx_text ltx_font_sansserif" style="font-size:144%;">Features</span></foreignObject></g><g stroke-width="0.8pt"><path d="M 383.86 0 L 402.56 0" style="fill:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 402.56 0)" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt"><path d="M -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g fill="#FFFFB3"><path d="M 422.73 -39.37 h 88.65 v 78.74 h -88.65 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 427.34 -6.92)" fill="#000000" stroke="#000000"><foreignObject width="79.42" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.1.pic1.5.5.5.1.1" class="ltx_text ltx_font_sansserif" style="font-size:144%;">Classifier</span></foreignObject></g><g fill="#FFFFFF"><path d="M 551.3 59.61 h 71.44 v 39.37 h -71.44 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 555.91 72.38)" fill="#000000" stroke="#000000"><foreignObject width="62.21" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.1.pic1.6.6.6.1.1" class="ltx_text ltx_font_sansserif" style="font-size:144%;">Class 1</span></foreignObject></g><g fill="#FFFFFF"><path d="M 551.3 -19.69 h 71.44 v 39.37 h -71.44 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 555.91 -6.92)" fill="#000000" stroke="#000000"><foreignObject width="62.21" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.1.pic1.7.7.7.1.1" class="ltx_text ltx_font_sansserif" style="font-size:144%;">Class 2</span></foreignObject></g><g fill="#FFFFFF"><path d="M 551.3 -98.98 h 76.42 v 39.37 h -76.42 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 555.91 -86.21)" fill="#000000" stroke="#000000"><foreignObject width="67.19" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.1.pic1.8.8.8.1.1" class="ltx_text ltx_font_sansserif" style="font-size:144%;">Class N</span></foreignObject></g><path d="M 511.66 29.47 L 552.68 56.59" style="fill:none"></path><g transform="matrix(0.83427 0.55136 -0.55136 0.83427 552.68 56.59)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path d="M 511.66 0 L 546.04 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 546.04 0)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path d="M 511.66 -28.86 L 554.52 -56.62" style="fill:none"></path><g transform="matrix(0.83932 -0.54364 0.54364 0.83932 554.52 -56.62)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g></g></svg>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text" style="font-size:90%;">Schematic of a DeepSpectrum pipeline for audio classification. The audio signal is converted into a Mel spectrogram, which is then fed to a pretrained CNN backbone to extract features. The features are then classified using an available classifier.</span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Additionally, we proposed to use a well-known technique to solve the classification task, using a Fully-Connected deep-learning classifier, which we named DeepSpectrum-FC or, simply, DS-FC. Moreover, we proposed our own classifier based on Attention Mechanisms, following the work of Gorriz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. We named this architecture DeepSpectrum-AM or, simply, DS-AM. We trained all models against both datasets and we found that our DS-AM model outperforms the SOTA models for the datasets and the SOTA DeepSpectrum architectures.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Finally, we trained our DS-AM model in one dataset and tested it in the other, in order to simulate in-the-wild conditions where the audio samples are not only from unknown speakers but also with other acoustic conditions and different input texts.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">All in all, the DS-SVC architecture performed better than its counterparts for both datasets; the DS-FC only outperformed SOTA models that used an SVC to solve the classification task; and, the DS-AM model outperformed all SOTA models for both datasets and the DS-SVC and DS-FC architectures that we also studied. The DS-AM model was trained in one dataset and tested in the other showed that the model is biased to the dataset, as it performed worse than the DS-AM model trained and tested in the same dataset. And as expected, the EmoMatchSpanishDB dataset, having 50 speakers instead of the 2 in ELRA-S0329, is more appropriate for in-the-wild emotion recognition.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Speech databases</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Emotion recognition is a widely studied field that comprises several disciplines, such as psychology or linguistics, and in recent decades, also computer science. Normally, speech databases introduce labels to classify or quantify the emotions expressed in the recordings. These labels can be discrete, such as the ones proposed by Ekman <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, which consists on several emotions, i. e. “anger” or “joy”; or continuous, such as the valance-arousal system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, which consists on two dimensions that describe the emotional state of the speaker.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In addition to this, speech databases distinguish themselves by the way the recordings are obtained. We can divide them in three categories:</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Acted or Simulated:</span> These databases are recorded by professional actors in a recording studio. Although they usually have very good audio quality, they are not so realistic or useful as real ones. Examples are: EmoDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, IEMOCAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> Spanish Expressive Voices Corpus Description <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, ELRA-S0329 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> or EmoMatchSpanishDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Induced:</span> There are created placing the actors in simulated situations to produce the requested emotion. They are more useful than the acted ones, as they are more close to real situations. An example is eNTERFACE’05 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Natural:</span> They are obtained from real situations as talk-shows, social media or call centers. Although the more useful of all, they have legal and ethical issues to use them. Examples are RECOLA Speech Database. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> or CMU-MOSEAS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Acoustic characteristics</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">From a phonological point-of-view, we can distinguish between segmental and suprasegmental characteristics in the study of audio tracks. On one hand, segmental characteristics are tied to a limited window of time within an utterance (such as a phoneme, approximately 20-30ms). The most common characteristics are those associated with the <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">cepstrum</em>, which consists of a transformation of the spectrum that allows us to discover periodic features in the frequency domain. For example, the MFCC (Mel Frequency Cepstral Coefficients) or the LPCC (Linear Prediction Cepstral Coefficients) belong to this category, and their definition can be found in the common literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">On the other hand, the suprasegmentals characteristics are tied to the entire utterance as a whole, like speech rate, shimmer or jitter. Often, these characteristics are evaluated in much larger windows of time, these characteristics are called Low-Level Descriptors (LLD), a widely spread standard for these features is the ComParE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> feature set, composed of 6373 static features derived from low-level descriptors (LLD). Also, another widespread feature set is the MSFs (Modulation Spectral Features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>), which are processed through a set of filters that simulate the human auditory system.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">The usual approach from authors to machine learning solutions is: first, to implement any sort of these above-mentioned classical feature extraction method and, then, use a classifier to solve the classification task. The classifier can be as simple as a Support Vector Classifier (SVC), or more complex, like a Recurrent Neural Network (RNN).</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">For the ELRA-S0329 dataset, Kerkeni et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> used two different feature extractors: MSFs and MFCC; and they combined both, obtaining three different feature sets (MSF, MFCC and MSF+MFCC). Regarding the classifier task, they introduced three different classifiers, first a Multi-Linear Regression (MLR), then a Support Vector Machine classifier (SVC) and finally a Recurrent Neural Network (RNN). <a href="#S2.T1" title="Table 1 ‣ 2.2 Acoustic characteristics ‣ 2 Related Work ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a> shows the results obtained by these authors.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.3.2" class="ltx_text" style="font-size:90%;">Accuracy results obtained for MLR, SVC, and RNN at ELRA-S0329 (MSF, MFCC and MSF+MFCC features sets) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</span></figcaption>
<table id="S2.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.4.1.1" class="ltx_tr">
<th id="S2.T1.4.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S2.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.4.1.1.2.1" class="ltx_text ltx_font_bold">MLR</span></th>
<th id="S2.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.4.1.1.3.1" class="ltx_text ltx_font_bold">SVC</span></th>
<th id="S2.T1.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.4.1.1.4.1" class="ltx_text ltx_font_bold">RNN</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.4.2.1" class="ltx_tr">
<th id="S2.T1.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MSF</th>
<td id="S2.T1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.706</td>
<td id="S2.T1.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.776</td>
<td id="S2.T1.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.823</td>
</tr>
<tr id="S2.T1.4.3.2" class="ltx_tr">
<th id="S2.T1.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MFCC</th>
<td id="S2.T1.4.3.2.2" class="ltx_td ltx_align_center">0.761</td>
<td id="S2.T1.4.3.2.3" class="ltx_td ltx_align_center">0.707</td>
<td id="S2.T1.4.3.2.4" class="ltx_td ltx_align_center">0.866</td>
</tr>
<tr id="S2.T1.4.4.3" class="ltx_tr">
<th id="S2.T1.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">MSF+MFCC</th>
<td id="S2.T1.4.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">0.824</td>
<td id="S2.T1.4.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">0.681</td>
<td id="S2.T1.4.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">0.905</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">For the EmoMatchSpanishDB dataset, García-Cuesta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> used the well-known ComParE feature set for audio preprocessing, they also introduced the EgeMaps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> feature set. Regarding the classifier task, they three algorithms: a Support Vector Machine Classifier (SVC), a XGBOOST classifier and a Feed-Foward Neural Network (FFNN). <a href="#S2.T2" title="Table 2 ‣ 2.2 Acoustic characteristics ‣ 2 Related Work ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a> shows the results obtained by these authors.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S2.T2.3.2" class="ltx_text" style="font-size:90%;">F1-Score and accuracy results obtained for SVC, XGBOOST, and FFNN at EmoMatchSpanishDB (eGeMAPS and CompaRE features sets) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</span></figcaption>
<table id="S2.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.4.1.1" class="ltx_tr">
<th id="S2.T2.4.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S2.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S2.T2.4.1.1.2.1" class="ltx_text ltx_font_bold">SVC</span></th>
<th id="S2.T2.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S2.T2.4.1.1.3.1" class="ltx_text ltx_font_bold">XGBOOST</span></th>
<th id="S2.T2.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S2.T2.4.1.1.4.1" class="ltx_text ltx_font_bold">FFNN</span></th>
</tr>
<tr id="S2.T2.4.2.2" class="ltx_tr">
<th id="S2.T2.4.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S2.T2.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S2.T2.4.2.2.2.1" class="ltx_text ltx_font_bold">F1</span></th>
<th id="S2.T2.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T2.4.2.2.3.1" class="ltx_text ltx_font_bold">Acc</span></th>
<th id="S2.T2.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S2.T2.4.2.2.4.1" class="ltx_text ltx_font_bold">F1</span></th>
<th id="S2.T2.4.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T2.4.2.2.5.1" class="ltx_text ltx_font_bold">Acc</span></th>
<th id="S2.T2.4.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S2.T2.4.2.2.6.1" class="ltx_text ltx_font_bold">F1</span></th>
<th id="S2.T2.4.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S2.T2.4.2.2.7.1" class="ltx_text ltx_font_bold">Acc</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.4.3.1" class="ltx_tr">
<th id="S2.T2.4.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">eGeMAPS</th>
<td id="S2.T2.4.3.1.2" class="ltx_td ltx_align_center ltx_border_t">0.522</td>
<td id="S2.T2.4.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.542</td>
<td id="S2.T2.4.3.1.4" class="ltx_td ltx_align_center ltx_border_t">0.503</td>
<td id="S2.T2.4.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.562</td>
<td id="S2.T2.4.3.1.6" class="ltx_td ltx_align_center ltx_border_t">0.511</td>
<td id="S2.T2.4.3.1.7" class="ltx_td ltx_align_center ltx_border_t">0.554</td>
</tr>
<tr id="S2.T2.4.4.2" class="ltx_tr">
<th id="S2.T2.4.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">CompaRE</th>
<td id="S2.T2.4.4.2.2" class="ltx_td ltx_align_center ltx_border_bb">0.589</td>
<td id="S2.T2.4.4.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.642</td>
<td id="S2.T2.4.4.2.4" class="ltx_td ltx_align_center ltx_border_bb">0.588</td>
<td id="S2.T2.4.4.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.642</td>
<td id="S2.T2.4.4.2.6" class="ltx_td ltx_align_center ltx_border_bb">0.567</td>
<td id="S2.T2.4.4.2.7" class="ltx_td ltx_align_center ltx_border_bb">0.596</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>DeepSpectrum</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">DeepSpectrum arose as full deep-learning alternative to the classical feature extraction methods. Originally intended to extract features from snore human sounds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, currently, it presents itself as a Python toolkit that produces visual representation of audio tracks –spectrogram or chromagram– and feeds them to a pretrained CNN for image feature extraction. Insted of extracting classical characteristics like the Mel Frequency Cepstral Coefficients (MFCC) or the Modulation Spectral Features (MSFs), DeepSpectrum uses the CNN to extract the features from the Mel spectrogram themselves.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">It supports widely more than 10 different backbone models, such as VGG16, ResNet, DenseNet, Inception, etc. Plus, it allows to select from which convolutional layer the features will be extracted. Moreover, other parameters can be selected, like: window and hopsize for feature extraction, length of fft window used for creating the spectograms, frequency scales and limits, color map of the plots, etc. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">In the original deep spectrum analysis, Amiriparian et al. used a Supported Vector Machine classifier to solve the classification task after extracting the features with the CNN, this is depicted at <a href="#S2.F2" title="Figure 2 ‣ 2.3 DeepSpectrum ‣ 2 Related Work ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2409.05148/assets/x1.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="276" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">A typical DeepSpectrum-SVC (DS-SVC) pipeline for audio classification. The audio signal is converted into a Mel spectrogram, which is then fed to a pretrained CNN backbone to extract features. The features are then classified using a Support Vector Classifier.</span></figcaption>
</figure>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">In other related works, such as in JAafar and Lachiri  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, were authors used a Fully-Connected deep-learning classifier to solve the classification task, this is depicted at <a href="#S2.F3" title="Figure 3 ‣ 2.3 DeepSpectrum ‣ 2 Related Work ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>. Note this was also considered in the original DeepSpectrum work but only as a future work possibility <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2409.05148/assets/x2.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="276" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Another configuration of DeepSpectrum, DeepSpectrum-FC (DS-FN) where the features are extracted from the Mel spectrogram using a pretrained CNN backbone and then classified using a Fully-Connected deep-learning classifier.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Materials and Methods</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Two datasets were considered in this study, both of them are in Spanish language:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">ELRA-S0329 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite></span> was presented on 2018 and consists on the recordings of one female and one male professional speakers in six emotions (anger, disgust, fear, joy, sadness, surprise) plus a neutral style in fast, slow, soft, loud and normal style. It contains 6041 archives with a total recording time of 7 hours and 52 minutes. In the original work, authors trained the dataset using a 10-cross validation scheme, first they splitted up the folds and later, they created a 70% split for training and 30% for testing. In order to compare to the state-of-the-art, we will use the same methodology, for this dataset. The best baseline obtained by Kerkeni et al. was a RNN with MFCC and MS features, with an accuracy of 90.05% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, as is described in <a href="#S2.T1" title="Table 1 ‣ 2.2 Acoustic characteristics ‣ 2 Related Work ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">EmoMatchSpanishDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span> was presented on 2023 and contains the recordings of 50 individuals (31 male and 19 female) with a total of 2050 archives expressing six different emotions (anger, disgust, fear, happiness, sadness, surprise) and a neutral one. In the original work, authors introduced a cross validation scheme named “Leave-One-Speaker-Out” (LOSO), where they used 45 speakers for training and 5 speakers for testing. The best baseline obtained by García-Cuesta et al. was a SVC (or XGBOOST) classifier with the ComParE feature set, with an accuracy of 64.2% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, as is described in <a href="#S2.T2" title="Table 2 ‣ 2.2 Acoustic characteristics ‣ 2 Related Work ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">There exists a noticeable difference in the baseline accuracy despite the similarities in the feature extraction pipelines. This is somehow to be expected, as the EmoMatchSpanishDB dataset has 50 speakers instead of the 2 in ELRA-S0329. Nevertheless, we understood this distinction as an opportunity to test the robustness of our models in different conditions.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Proposed method</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We proposed to use our own variant of a DeepSpectrum pipeline, this approximation consists on bringing attention mechanisms to the DeepSpectrum architecture. Instead of using the entire back-bone CNN to extract the image features from the Mel spectrogram, we modified a VGG-16 CNN to include two attention mechanisms, following the work of Gorriz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, who used this architecture to evaluate the severity of knee osteoarthritis processing X-Ray images. We named this architecture DeepSpectrum-AM or, simply, DS-AM. The attention mechanisms were added to the last two convolutional blocks of the VGG-16, as shown in <a href="#S3.F4" title="Figure 4 ‣ 3.2 Proposed method ‣ 3 Materials and Methods ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F4.1" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption">a)</figcaption><img src="/html/2409.05148/assets/figures/xarxa_att.png" id="S3.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="261" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F4.2" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption ltx_centering">b)</figcaption><img src="/html/2409.05148/assets/figures/deepspectrum_vgg16-vgg16_att_drawio.png" id="S3.F4.2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="444" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.4.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.5.2" class="ltx_text" style="font-size:90%;">The DeepSpectrum-Attention Mechanisms (DS-AM) architecture. The figure shows: a) the general model which includes a modified DeepSpectrum-AM; and, b) the attention mechanism module.</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Experimental design</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">As the first experiment, we proposed a transfer learning solution to the classification task by using the pretrained VGG-16 CNN in the DeepSpectrum toolkit (from Keras stock library –where VGG-16 was trained over ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>–) to extract the features from the Mel spectrograms. Then we used the well-known SVM classifier, as proposed in the original work of DeepSpectrum <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. We named the architecture for this experiment DeepSpectrum-SVC or, simply, DS-SVC (see <a href="#S2.F2" title="Figure 2 ‣ 2.3 DeepSpectrum ‣ 2 Related Work ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>). Note that we used a VGG-16 CNN as the backbone, as it is the same architecture used in the further experiments, specially in the case of the addition of attention mechanisms.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">As the second experiment, we proposed a fine-tuning solution for the feature extraction step paired with a fully-connected deep-learning. For the fine-tuning, we obtained the same pretrained stock network –VGG-16 trained on ImageNet from Keras– and we unfroze all layers with a low-learning rate. We expected this approximation to better handle the specificity of our datasets, as our images are Mel spectograms, not general images. We named this architecture DeepSpectrum-FC or, simply, DS-FC (see <a href="#S2.F3" title="Figure 3 ‣ 2.3 DeepSpectrum ‣ 2 Related Work ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>).</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">As third experiment, we proposed a step further in network configuration, by taking a modified version of the VGG-16 network with two attention mechanisms, following the work of Gorriz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. As we took the VGG-16 from the one pretrained in Keras, we used all the pretrained weights for the firsts layers of the extractor, but start using new untrained values for the attention heads. We named this architecture DeepSpectrum-Attention Mechanisms or, simply, DS-AM.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">For experiments 1, 2 and 3, for ELRA-S0329, we have grouped all the five neutral labelled emotions in only one category, as we consider that the neutral emotion contained in EmoMatchSpanishDB only represents the normal category and doesn’t contain the wide range that ELRA-S0329 has. We trained over both datasets, ELRA-S0329 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and EmoMatchSpanishDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, following the partitions that most resembled the original works. This means that for both datasets we used a 10-fold cross-validation process, as described in the original works. Despite this, notice that this means that the folds for the ELRA-S0329 dataset are constituted by different instances from the two same speakers, and the folds for the EmoMatchSpanishDB dataset are constituted by different instances from the 50 speakers. In this case, as we have a total of 50 speakers, we used 45 speakers for training and 5 speakers for validation.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">As the fourth experiment, we trained the best model obtained with each of the datasets and we tested its performance in the other dataset, we did not use the cross-validation here. Doing so, we simulated an in-the-wild scenario where our task should be applied, being the audio samples not only from unknown speakers but also with other acoustic conditions and different input texts. For this experiment, for ELRA-S0329, we used only the normal-neutral labeled samples for testing purposes, although we maintained the whole set of five neutral emotions for training. We considered that the neutral emotion contained in EmoMatchSpanishDB only represents the normal category and doesn’t contain the wide range that ELRA-S0329 has.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><a href="#S4.T3" title="Table 3 ‣ 4 Experimental Results ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a> shows the results of training DeepSpectrum-SVC over both datasets, ELRA-S0329 and EmoMatchSpanishDB, and compares them with the state-of-the-art models for each dataset regarding machine-learning classic classifiers. It can be shown that our model performed similarly to the SOTA models for the ELRA-S0329 dataset, but it was outperformed by the SOTA models for the EmoMatchSpanishDB dataset.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.4.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.5.2" class="ltx_text" style="font-size:90%;">Comparison between the SOTA models for machine-learning classification solvers: MRL and SVC for ELRA-S0329 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>; and SVC and XGBOOST for EmoMatchSpanishDB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Our model using a pretrained VGG-16 CNN as feature extractor and a SVC as classifier is also shown, which scores similar to SOTA for the ELRA-S0329 dataset.</span></figcaption>
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.2.2" class="ltx_tr">
<th id="S4.T3.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T3.2.2.3.1" class="ltx_text ltx_font_bold">Feature Set</span></th>
<td id="S4.T3.2.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.2.2.4.1" class="ltx_text ltx_font_bold">Classifier</span></td>
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.1" class="ltx_text ltx_font_bold">Acc.<sub id="S4.T3.1.1.1.1.1" class="ltx_sub"><span id="S4.T3.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">ELRA-S0329</span></sub></span></td>
<td id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.2.2.2.1" class="ltx_text ltx_font_bold">Acc.<sub id="S4.T3.2.2.2.1.1" class="ltx_sub"><span id="S4.T3.2.2.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">EmoMatchSpanishDB</span></sub></span></td>
</tr>
<tr id="S4.T3.2.3.1" class="ltx_tr">
<th id="S4.T3.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T3.2.3.1.1.1" class="ltx_text">MSF</span></th>
<td id="S4.T3.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">MLR</td>
<td id="S4.T3.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.706</td>
<td id="S4.T3.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T3.2.4.2" class="ltx_tr">
<td id="S4.T3.2.4.2.1" class="ltx_td ltx_align_center">SVC</td>
<td id="S4.T3.2.4.2.2" class="ltx_td ltx_align_center">0.776</td>
<td id="S4.T3.2.4.2.3" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.2.5.3" class="ltx_tr">
<th id="S4.T3.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T3.2.5.3.1.1" class="ltx_text">MFCC</span></th>
<td id="S4.T3.2.5.3.2" class="ltx_td ltx_align_center">MLR</td>
<td id="S4.T3.2.5.3.3" class="ltx_td ltx_align_center">0.761</td>
<td id="S4.T3.2.5.3.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.2.6.4" class="ltx_tr">
<td id="S4.T3.2.6.4.1" class="ltx_td ltx_align_center">SVC</td>
<td id="S4.T3.2.6.4.2" class="ltx_td ltx_align_center">0.707</td>
<td id="S4.T3.2.6.4.3" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.2.7.5" class="ltx_tr">
<th id="S4.T3.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T3.2.7.5.1.1" class="ltx_text">MSF+MFCC</span></th>
<td id="S4.T3.2.7.5.2" class="ltx_td ltx_align_center">MLR</td>
<td id="S4.T3.2.7.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.7.5.3.1" class="ltx_text ltx_font_bold">0.824</span></td>
<td id="S4.T3.2.7.5.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.2.8.6" class="ltx_tr">
<td id="S4.T3.2.8.6.1" class="ltx_td ltx_align_center">SVC</td>
<td id="S4.T3.2.8.6.2" class="ltx_td ltx_align_center">0.681</td>
<td id="S4.T3.2.8.6.3" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T3.2.9.7" class="ltx_tr">
<th id="S4.T3.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T3.2.9.7.1.1" class="ltx_text">EgeMaps</span></th>
<td id="S4.T3.2.9.7.2" class="ltx_td ltx_align_center ltx_border_t">SVC</td>
<td id="S4.T3.2.9.7.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T3.2.9.7.4" class="ltx_td ltx_align_center ltx_border_t">0.542</td>
</tr>
<tr id="S4.T3.2.10.8" class="ltx_tr">
<td id="S4.T3.2.10.8.1" class="ltx_td ltx_align_center">XGBOOST</td>
<td id="S4.T3.2.10.8.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.10.8.3" class="ltx_td ltx_align_center">0.562</td>
</tr>
<tr id="S4.T3.2.11.9" class="ltx_tr">
<th id="S4.T3.2.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2"><span id="S4.T3.2.11.9.1.1" class="ltx_text">ComParE</span></th>
<td id="S4.T3.2.11.9.2" class="ltx_td ltx_align_center">SVC</td>
<td id="S4.T3.2.11.9.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.11.9.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.11.9.4.1" class="ltx_text ltx_font_bold">0.642</span></td>
</tr>
<tr id="S4.T3.2.12.10" class="ltx_tr">
<td id="S4.T3.2.12.10.1" class="ltx_td ltx_align_center">XGBOOST</td>
<td id="S4.T3.2.12.10.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T3.2.12.10.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.12.10.3.1" class="ltx_text ltx_font_bold">0.642</span></td>
</tr>
<tr id="S4.T3.2.13.11" class="ltx_tr">
<th id="S4.T3.2.13.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" colspan="2">DeepSpectrum-SVC (DS-SVC)</th>
<td id="S4.T3.2.13.11.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.2.13.11.2.1" class="ltx_text ltx_font_bold">0.821</span></td>
<td id="S4.T3.2.13.11.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.525</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><a href="#S4.T4" title="Table 4 ‣ 4 Experimental Results ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 4</span></a> show the results of training DeepSpectrum-FC and DeepSpectrum-AM over both datasets, ELRA-S0329 and EmoMatchSpanishDB –experiments 2 and 3–, and compares them with the state-of-the-art models for each dataset regarding deep-learning classifiers. It can be shown that our model outperformed the SOTA models for both datasets and the SOTA DeepSpectrum architectures. Specially for the EmoMatchSpanishDB dataset, can be seen that our deep-learning models surpassed the XGBOOST classifier, which was the best model in the original work, see also <a href="#S4.T3" title="Table 3 ‣ 4 Experimental Results ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.4.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.5.2" class="ltx_text" style="font-size:90%;">Comparison between the SOTA models for deep-leaning classification solvers: RNN for ELRA-S0329 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>; FFNN for EmoMatchSpanishDB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>; and our model using a pretrained VGG-16 CNN as feature extractor and: a Fully-Connected deep-learning classifier (DS-FC) and a CNN with two attention mechanisms (DS-AM).</span></figcaption>
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.2.2" class="ltx_tr">
<th id="S4.T4.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T4.2.2.3.1" class="ltx_text ltx_font_bold">Feature Set</span></th>
<td id="S4.T4.2.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.2.2.4.1" class="ltx_text ltx_font_bold">Classifier</span></td>
<td id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.1.1.1.1" class="ltx_text ltx_font_bold">Acc.<sub id="S4.T4.1.1.1.1.1" class="ltx_sub"><span id="S4.T4.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">ELRA-S0329</span></sub></span></td>
<td id="S4.T4.2.2.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.2.2.2.1" class="ltx_text ltx_font_bold">Acc.<sub id="S4.T4.2.2.2.1.1" class="ltx_sub"><span id="S4.T4.2.2.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">EmoMatchSpanishDB</span></sub></span></td>
</tr>
<tr id="S4.T4.2.3.1" class="ltx_tr">
<th id="S4.T4.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MSF</th>
<td id="S4.T4.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">RNN</td>
<td id="S4.T4.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">0.823</td>
<td id="S4.T4.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T4.2.4.2" class="ltx_tr">
<th id="S4.T4.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MFCC</th>
<td id="S4.T4.2.4.2.2" class="ltx_td ltx_align_center">RNN</td>
<td id="S4.T4.2.4.2.3" class="ltx_td ltx_align_center">0.866</td>
<td id="S4.T4.2.4.2.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T4.2.5.3" class="ltx_tr">
<th id="S4.T4.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MSF+MFCC</th>
<td id="S4.T4.2.5.3.2" class="ltx_td ltx_align_center">RNN</td>
<td id="S4.T4.2.5.3.3" class="ltx_td ltx_align_center">0.905</td>
<td id="S4.T4.2.5.3.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T4.2.6.4" class="ltx_tr">
<th id="S4.T4.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">EgeMaps</th>
<td id="S4.T4.2.6.4.2" class="ltx_td ltx_align_center ltx_border_t">FFNN</td>
<td id="S4.T4.2.6.4.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T4.2.6.4.4" class="ltx_td ltx_align_center ltx_border_t">0.554</td>
</tr>
<tr id="S4.T4.2.7.5" class="ltx_tr">
<th id="S4.T4.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ComParE</th>
<td id="S4.T4.2.7.5.2" class="ltx_td ltx_align_center">FFNN</td>
<td id="S4.T4.2.7.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S4.T4.2.7.5.4" class="ltx_td ltx_align_center">0.554</td>
</tr>
<tr id="S4.T4.2.8.6" class="ltx_tr">
<th id="S4.T4.2.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="2">DeepSpectrum-FC (DS-FC)</th>
<td id="S4.T4.2.8.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.8.6.2.1" class="ltx_text ltx_font_bold">0.975</span></td>
<td id="S4.T4.2.8.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.2.8.6.3.1" class="ltx_text ltx_font_bold">0.659</span></td>
</tr>
<tr id="S4.T4.2.9.7" class="ltx_tr">
<th id="S4.T4.2.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" colspan="2">DeepSpectrum-AM (DS-AM)</th>
<td id="S4.T4.2.9.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.9.7.2.1" class="ltx_text ltx_font_bold">0.984</span></td>
<td id="S4.T4.2.9.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.2.9.7.3.1" class="ltx_text ltx_font_bold">0.683</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Additional results are shown in <a href="#S4.T5" title="Table 5 ‣ 4 Experimental Results ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 5</span></a> where a detailed classification report is shown for the ELRA-S0329 dataset and DeepSpectrum-AM model. <a href="#S4.T6" title="Table 6 ‣ 4 Experimental Results ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 6</span></a> shows the same for the EmoMatchSpanishDB dataset and DeepSpectrum-AM model. For this architecture, also a detailed view of the confusion matrix for both datasets is shown in <a href="#S4.F5" title="Figure 5 ‣ 4 Experimental Results ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.3.2" class="ltx_text" style="font-size:90%;">Classification report for DeepSpectrum-AM model trained over the ELRA-S0329 dataset.</span></figcaption>
<table id="S4.T5.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.4.1.1" class="ltx_tr">
<th id="S4.T5.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T5.4.1.1.1.1" class="ltx_text ltx_font_bold">Label</span></th>
<th id="S4.T5.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.4.1.1.2.1" class="ltx_text ltx_font_bold">Precision</span></th>
<th id="S4.T5.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.4.1.1.3.1" class="ltx_text ltx_font_bold">Recall</span></th>
<th id="S4.T5.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.4.1.1.4.1" class="ltx_text ltx_font_bold">F1-score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.4.2.1" class="ltx_tr">
<th id="S4.T5.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ANGER</th>
<td id="S4.T5.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.986</td>
<td id="S4.T5.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.992</td>
<td id="S4.T5.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.989</td>
</tr>
<tr id="S4.T5.4.3.2" class="ltx_tr">
<th id="S4.T5.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DISGUST</th>
<td id="S4.T5.4.3.2.2" class="ltx_td ltx_align_center">0.927</td>
<td id="S4.T5.4.3.2.3" class="ltx_td ltx_align_center">0.991</td>
<td id="S4.T5.4.3.2.4" class="ltx_td ltx_align_center">0.958</td>
</tr>
<tr id="S4.T5.4.4.3" class="ltx_tr">
<th id="S4.T5.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FEAR</th>
<td id="S4.T5.4.4.3.2" class="ltx_td ltx_align_center">0.980</td>
<td id="S4.T5.4.4.3.3" class="ltx_td ltx_align_center">0.978</td>
<td id="S4.T5.4.4.3.4" class="ltx_td ltx_align_center">0.979</td>
</tr>
<tr id="S4.T5.4.5.4" class="ltx_tr">
<th id="S4.T5.4.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">JOY</th>
<td id="S4.T5.4.5.4.2" class="ltx_td ltx_align_center">0.996</td>
<td id="S4.T5.4.5.4.3" class="ltx_td ltx_align_center">0.985</td>
<td id="S4.T5.4.5.4.4" class="ltx_td ltx_align_center">0.990</td>
</tr>
<tr id="S4.T5.4.6.5" class="ltx_tr">
<th id="S4.T5.4.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NEUTRAL</th>
<td id="S4.T5.4.6.5.2" class="ltx_td ltx_align_center">0.996</td>
<td id="S4.T5.4.6.5.3" class="ltx_td ltx_align_center">0.977</td>
<td id="S4.T5.4.6.5.4" class="ltx_td ltx_align_center">0.986</td>
</tr>
<tr id="S4.T5.4.7.6" class="ltx_tr">
<th id="S4.T5.4.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SADNESS</th>
<td id="S4.T5.4.7.6.2" class="ltx_td ltx_align_center">0.994</td>
<td id="S4.T5.4.7.6.3" class="ltx_td ltx_align_center">0.981</td>
<td id="S4.T5.4.7.6.4" class="ltx_td ltx_align_center">0.987</td>
</tr>
<tr id="S4.T5.4.8.7" class="ltx_tr">
<th id="S4.T5.4.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">SURPRISE</th>
<td id="S4.T5.4.8.7.2" class="ltx_td ltx_align_center ltx_border_bb">0.994</td>
<td id="S4.T5.4.8.7.3" class="ltx_td ltx_align_center ltx_border_bb">0.989</td>
<td id="S4.T5.4.8.7.4" class="ltx_td ltx_align_center ltx_border_bb">0.991</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.2.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.3.2" class="ltx_text" style="font-size:90%;">Classification report for DeepSpectrum-AM model trained over the EmoMatchSpanishDB dataset.</span></figcaption>
<table id="S4.T6.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.4.1.1" class="ltx_tr">
<th id="S4.T6.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T6.4.1.1.1.1" class="ltx_text ltx_font_bold">Label</span></th>
<th id="S4.T6.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T6.4.1.1.2.1" class="ltx_text ltx_font_bold">Precision</span></th>
<th id="S4.T6.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T6.4.1.1.3.1" class="ltx_text ltx_font_bold">Recall</span></th>
<th id="S4.T6.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T6.4.1.1.4.1" class="ltx_text ltx_font_bold">F1-score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.4.2.1" class="ltx_tr">
<th id="S4.T6.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ANGER</th>
<td id="S4.T6.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.732</td>
<td id="S4.T6.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.809</td>
<td id="S4.T6.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.769</td>
</tr>
<tr id="S4.T6.4.3.2" class="ltx_tr">
<th id="S4.T6.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DISGUST</th>
<td id="S4.T6.4.3.2.2" class="ltx_td ltx_align_center">0.624</td>
<td id="S4.T6.4.3.2.3" class="ltx_td ltx_align_center">0.443</td>
<td id="S4.T6.4.3.2.4" class="ltx_td ltx_align_center">0.518</td>
</tr>
<tr id="S4.T6.4.4.3" class="ltx_tr">
<th id="S4.T6.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FEAR</th>
<td id="S4.T6.4.4.3.2" class="ltx_td ltx_align_center">0.711</td>
<td id="S4.T6.4.4.3.3" class="ltx_td ltx_align_center">0.738</td>
<td id="S4.T6.4.4.3.4" class="ltx_td ltx_align_center">0.724</td>
</tr>
<tr id="S4.T6.4.5.4" class="ltx_tr">
<th id="S4.T6.4.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">JOY</th>
<td id="S4.T6.4.5.4.2" class="ltx_td ltx_align_center">0.530</td>
<td id="S4.T6.4.5.4.3" class="ltx_td ltx_align_center">0.488</td>
<td id="S4.T6.4.5.4.4" class="ltx_td ltx_align_center">0.508</td>
</tr>
<tr id="S4.T6.4.6.5" class="ltx_tr">
<th id="S4.T6.4.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NEUTRAL</th>
<td id="S4.T6.4.6.5.2" class="ltx_td ltx_align_center">0.710</td>
<td id="S4.T6.4.6.5.3" class="ltx_td ltx_align_center">0.841</td>
<td id="S4.T6.4.6.5.4" class="ltx_td ltx_align_center">0.770</td>
</tr>
<tr id="S4.T6.4.7.6" class="ltx_tr">
<th id="S4.T6.4.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SADNESS</th>
<td id="S4.T6.4.7.6.2" class="ltx_td ltx_align_center">0.643</td>
<td id="S4.T6.4.7.6.3" class="ltx_td ltx_align_center">0.580</td>
<td id="S4.T6.4.7.6.4" class="ltx_td ltx_align_center">0.610</td>
</tr>
<tr id="S4.T6.4.8.7" class="ltx_tr">
<th id="S4.T6.4.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">SURPRISE</th>
<td id="S4.T6.4.8.7.2" class="ltx_td ltx_align_center ltx_border_bb">0.727</td>
<td id="S4.T6.4.8.7.3" class="ltx_td ltx_align_center ltx_border_bb">0.618</td>
<td id="S4.T6.4.8.7.4" class="ltx_td ltx_align_center ltx_border_bb">0.668</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.1" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption">a)</figcaption><img src="/html/2409.05148/assets/figures/s0329_cm2.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="508" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.2" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption">b)</figcaption><img src="/html/2409.05148/assets/figures/emomatch_cm2.png" id="S4.F5.2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="518" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.4.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.5.2" class="ltx_text" style="font-size:90%;">Confusion matrix for DeepSpectrum-AM over a) ELRA-S0329 dataset; and, b) EmoMatchSpanishDB dataset.</span></figcaption>
</figure>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Results for experiment 4 are shown in <a href="#S4.T7" title="Table 7 ‣ 4 Experimental Results ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 7</span></a>. It can be seen that the model trained in ELRA-S0329 and tested in EmoMatchSpanishDB performed poorly, while the model trained in EmoMatchSpanishDB and tested in ELRA-S0329 performed better. This is also shown in the confusion matrices in <a href="#S4.F6" title="Figure 6 ‣ 4 Experimental Results ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>. This is considered a good result, and to be expected. As training in EmoMatchSpanishDB, the model has to learn to generalize the features of the emotions, and testing on in-the-wild scenario –ELRA-S0329– the model has to learn to generalize the features of the speakers.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Additional results for the in-the-wild experiment can be found in the <a href="#S4.T8" title="Table 8 ‣ 4 Experimental Results ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 8</span></a> and <a href="#S4.T9" title="Table 9 ‣ 4 Experimental Results ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 9</span></a>, where the confusion matrices for both crossed experiments can be found in <a href="#S4.F6" title="Figure 6 ‣ 4 Experimental Results ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>. Finally, a detailed report for precision, recall and F1-score for both experiments can be found in <a href="#S4.T8" title="Table 8 ‣ 4 Experimental Results ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 8</span></a> and <a href="#S4.T9" title="Table 9 ‣ 4 Experimental Results ‣ Better Spanish Emotion Recognition In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 9</span></a>.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.2.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S4.T7.3.2" class="ltx_text" style="font-size:90%;">Accuracy comparison between training and testing in ELRA-S0329 and EmoMatchSpanishDB with VGG16 + 2 attention mechanisms</span></figcaption>
<table id="S4.T7.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.4.1.1" class="ltx_tr">
<th id="S4.T7.4.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T7.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T7.4.1.1.2.1" class="ltx_text ltx_font_bold">Test</span></th>
</tr>
<tr id="S4.T7.4.2.2" class="ltx_tr">
<th id="S4.T7.4.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S4.T7.4.2.2.1.1" class="ltx_text ltx_font_bold">Train</span></th>
<th id="S4.T7.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T7.4.2.2.2.1" class="ltx_text ltx_font_bold">ELRA-S0329</span></th>
<th id="S4.T7.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T7.4.2.2.3.1" class="ltx_text ltx_font_bold">EmoMatchSpanishDB</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.4.3.1" class="ltx_tr">
<th id="S4.T7.4.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ELRA-S0329</th>
<td id="S4.T7.4.3.1.2" class="ltx_td ltx_align_center ltx_border_t">98.38%</td>
<td id="S4.T7.4.3.1.3" class="ltx_td ltx_align_center ltx_border_t">35.66%</td>
</tr>
<tr id="S4.T7.4.4.2" class="ltx_tr">
<th id="S4.T7.4.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">EmoMatchSpanishDB</th>
<td id="S4.T7.4.4.2.2" class="ltx_td ltx_align_center ltx_border_bb">41.54%</td>
<td id="S4.T7.4.4.2.3" class="ltx_td ltx_align_center ltx_border_bb">64.20%</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.1" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption">a)</figcaption><img src="/html/2409.05148/assets/figures/trainemotestelra2.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="514" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.2" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption">b)</figcaption><img src="/html/2409.05148/assets/figures/trainelratestemo.png" id="S4.F6.2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="514" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.4.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.5.2" class="ltx_text" style="font-size:90%;">Confusion matrix for DeepSpectrum-AM model: a) training in EmoMatchSpanishDB and testing in ELRA-S0329; and, b) training in ELRA-S0329 and testing in EmoMatchSpanishDB.</span></figcaption>
</figure>
<figure id="S4.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T8.2.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="S4.T8.3.2" class="ltx_text" style="font-size:90%;">Classification report training ELRA-S0329 and testing EmoMatchSpanishDB</span></figcaption>
<table id="S4.T8.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T8.4.1.1" class="ltx_tr">
<th id="S4.T8.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T8.4.1.1.1.1" class="ltx_text ltx_font_bold">Label</span></th>
<th id="S4.T8.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T8.4.1.1.2.1" class="ltx_text ltx_font_bold">Precision</span></th>
<th id="S4.T8.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T8.4.1.1.3.1" class="ltx_text ltx_font_bold">Recall</span></th>
<th id="S4.T8.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T8.4.1.1.4.1" class="ltx_text ltx_font_bold">F1-score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T8.4.2.1" class="ltx_tr">
<th id="S4.T8.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ANGER</th>
<td id="S4.T8.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">1.000</td>
<td id="S4.T8.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.010</td>
<td id="S4.T8.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.010</td>
</tr>
<tr id="S4.T8.4.3.2" class="ltx_tr">
<th id="S4.T8.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DISGUST</th>
<td id="S4.T8.4.3.2.2" class="ltx_td ltx_align_center">0.350</td>
<td id="S4.T8.4.3.2.3" class="ltx_td ltx_align_center">0.050</td>
<td id="S4.T8.4.3.2.4" class="ltx_td ltx_align_center">0.080</td>
</tr>
<tr id="S4.T8.4.4.3" class="ltx_tr">
<th id="S4.T8.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FEAR</th>
<td id="S4.T8.4.4.3.2" class="ltx_td ltx_align_center">0.380</td>
<td id="S4.T8.4.4.3.3" class="ltx_td ltx_align_center">0.510</td>
<td id="S4.T8.4.4.3.4" class="ltx_td ltx_align_center">0.440</td>
</tr>
<tr id="S4.T8.4.5.4" class="ltx_tr">
<th id="S4.T8.4.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">JOY</th>
<td id="S4.T8.4.5.4.2" class="ltx_td ltx_align_center">0.200</td>
<td id="S4.T8.4.5.4.3" class="ltx_td ltx_align_center">0.220</td>
<td id="S4.T8.4.5.4.4" class="ltx_td ltx_align_center">0.210</td>
</tr>
<tr id="S4.T8.4.6.5" class="ltx_tr">
<th id="S4.T8.4.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NEUTRAL</th>
<td id="S4.T8.4.6.5.2" class="ltx_td ltx_align_center">0.350</td>
<td id="S4.T8.4.6.5.3" class="ltx_td ltx_align_center">0.870</td>
<td id="S4.T8.4.6.5.4" class="ltx_td ltx_align_center">0.500</td>
</tr>
<tr id="S4.T8.4.7.6" class="ltx_tr">
<th id="S4.T8.4.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SADNESS</th>
<td id="S4.T8.4.7.6.2" class="ltx_td ltx_align_center">0.110</td>
<td id="S4.T8.4.7.6.3" class="ltx_td ltx_align_center">0.010</td>
<td id="S4.T8.4.7.6.4" class="ltx_td ltx_align_center">0.010</td>
</tr>
<tr id="S4.T8.4.8.7" class="ltx_tr">
<th id="S4.T8.4.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">SURPRISE</th>
<td id="S4.T8.4.8.7.2" class="ltx_td ltx_align_center ltx_border_bb">0.610</td>
<td id="S4.T8.4.8.7.3" class="ltx_td ltx_align_center ltx_border_bb">0.350</td>
<td id="S4.T8.4.8.7.4" class="ltx_td ltx_align_center ltx_border_bb">0.450</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T9.2.1.1" class="ltx_text" style="font-size:90%;">Table 9</span>: </span><span id="S4.T9.3.2" class="ltx_text" style="font-size:90%;">Classification report training EmoMatchSpanishDB and testing ELRA-S0329</span></figcaption>
<table id="S4.T9.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T9.4.1.1" class="ltx_tr">
<th id="S4.T9.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T9.4.1.1.1.1" class="ltx_text ltx_font_bold">Label</span></th>
<th id="S4.T9.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T9.4.1.1.2.1" class="ltx_text ltx_font_bold">Precision</span></th>
<th id="S4.T9.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T9.4.1.1.3.1" class="ltx_text ltx_font_bold">Recall</span></th>
<th id="S4.T9.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T9.4.1.1.4.1" class="ltx_text ltx_font_bold">F1-score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T9.4.2.1" class="ltx_tr">
<th id="S4.T9.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ANGER</th>
<td id="S4.T9.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.480</td>
<td id="S4.T9.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.510</td>
<td id="S4.T9.4.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.490</td>
</tr>
<tr id="S4.T9.4.3.2" class="ltx_tr">
<th id="S4.T9.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DISGUST</th>
<td id="S4.T9.4.3.2.2" class="ltx_td ltx_align_center">0.350</td>
<td id="S4.T9.4.3.2.3" class="ltx_td ltx_align_center">0.420</td>
<td id="S4.T9.4.3.2.4" class="ltx_td ltx_align_center">0.380</td>
</tr>
<tr id="S4.T9.4.4.3" class="ltx_tr">
<th id="S4.T9.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FEAR</th>
<td id="S4.T9.4.4.3.2" class="ltx_td ltx_align_center">0.420</td>
<td id="S4.T9.4.4.3.3" class="ltx_td ltx_align_center">0.540</td>
<td id="S4.T9.4.4.3.4" class="ltx_td ltx_align_center">0.470</td>
</tr>
<tr id="S4.T9.4.5.4" class="ltx_tr">
<th id="S4.T9.4.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">JOY</th>
<td id="S4.T9.4.5.4.2" class="ltx_td ltx_align_center">1.000</td>
<td id="S4.T9.4.5.4.3" class="ltx_td ltx_align_center">0.000</td>
<td id="S4.T9.4.5.4.4" class="ltx_td ltx_align_center">0.010</td>
</tr>
<tr id="S4.T9.4.6.5" class="ltx_tr">
<th id="S4.T9.4.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NEUTRAL</th>
<td id="S4.T9.4.6.5.2" class="ltx_td ltx_align_center">0.610</td>
<td id="S4.T9.4.6.5.3" class="ltx_td ltx_align_center">0.150</td>
<td id="S4.T9.4.6.5.4" class="ltx_td ltx_align_center">0.240</td>
</tr>
<tr id="S4.T9.4.7.6" class="ltx_tr">
<th id="S4.T9.4.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SADNESS</th>
<td id="S4.T9.4.7.6.2" class="ltx_td ltx_align_center">0.330</td>
<td id="S4.T9.4.7.6.3" class="ltx_td ltx_align_center">0.680</td>
<td id="S4.T9.4.7.6.4" class="ltx_td ltx_align_center">0.440</td>
</tr>
<tr id="S4.T9.4.8.7" class="ltx_tr">
<th id="S4.T9.4.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">SURPRISE</th>
<td id="S4.T9.4.8.7.2" class="ltx_td ltx_align_center ltx_border_bb">0.550</td>
<td id="S4.T9.4.8.7.3" class="ltx_td ltx_align_center ltx_border_bb">0.620</td>
<td id="S4.T9.4.8.7.4" class="ltx_td ltx_align_center ltx_border_bb">0.580</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">For ELRA-S0329 the three models proposed outperform the current state-of-the-art models. If we compare DeepSpectrum-VGG16 MSF-SVC, we obtain an accuracy increase of 4,51%. and DeepSpectrum-FN and DeepSpectrum-AM obtain an accuracy increase of 7,43% and 8,33% versus RNN (MFCC + MS). As the performance is so outstanding, the classification report gives us not any new information.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">For EmoMatchSpanishDB, only two of the models outperform the current state-of-the-art. DeepSpectrum-FN and DeepSpectrum-AM obtain an accuracy increase of 1,7% and 4,1% versus SVC. In the confusion matrix and the classification report we can observe that for emotions like anger or fear the system has both a high precision and recall, while for emotions like joy the performance is not so good.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">In the cross-datasets experiments, we see that the model trained in EmoMatchSpanishDB outperforms the model trained in ELRA-S0329 in a 5,88%. This was expected as EmoMatchSpanishDB, having 50 speakers instead of the 2 in ELRA-S0329, is more appropriate for in-the-wild emotion recognition.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">If we analyze the confusion matrix in both cases, we see the model trained in ELRA-S039 has the highest performance for neutral labelled samples, not surprising considering the five subcategories present in the dataset. Instead, in the model trained in EmoMatchSpanishDB, this happens with emotions like Surprise and Anger.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">Finally, we can affirm that DeepSpectrum has proven its usefulness in the field of emotion recognition. It allows us to apply image recognition developments and to do transfer learning to an audio recognition problem, furthering the available ways of improvement in the research field.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">Future lines of research should be directed to, in one hand, increasing the number of available datasets to do research, taking into account that languages like Spanish have a large number of speakers but a not so large number of datasets, compared to English. On the other hand, Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and the addition of other audio parameters like MSFs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> to the system could increase the performance of it.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work is part of the project PLEC2021-007868, funded by MCIN and by the EU, and the project PID2022-138721NB-I00, funded by MCIN.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Abdollahi, H., Mahoor, M.H., Zandie, R., Siewierski, J., Qualls, S.H.:
Artificial emotional intelligence in socially assistive robots for older
adults: A pilot study. IEEE Transactions on Affective Computing
<span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">14</span>(3), 2020–2032 (2022). https://doi.org/10.1109/TAFFC.2022.3143803

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Amiriparian, S., Gerczuk, M., Ottl, S., Cummins, N., Freitag, M., Pugachevskiy,
S., Baird, A., Schuller, B.: Snore Sound Classification Using
Image-Based Deep Spectrum Features. In: Interspeech 2017. pp.
3512–3516. ISCA (Aug 2017). https://doi.org/10.21437/Interspeech.2017-434

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Amiriparian, S., Gerczuk, M., Ottl, S., Schuller, B.: Deepspectrum.
<a target="_blank" href="https://github.com/DeepSpectrum/DeepSpectrum" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/DeepSpectrum/DeepSpectrum</a> (2020), accessed:
2024-07-30

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Barra Chicote, R., Montero Martínez, J.M., Macías Guarasa, J., Lutfi,
S.L., Lucas Cuesta, J.M., Fernández Martínez, F.,
D’Haro Enríquez, L.F., San Segundo Hernández, R.,
Ferreiros López, J., Córdoba Herralde, R.d., Pardo Muñoz, J.M.:
Spanish expressive voices: corpus for emotion research in spanish pp. 60–70
(Mayo 2008), <a target="_blank" href="http://www.lrec-conf.org/proceedings/lrec2008/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.lrec-conf.org/proceedings/lrec2008/</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Bestelmeyer, P.E.G., Kotz, S.A., Belin, P.: Effects of emotional valence and
arousal on the voice perception network. Social Cognitive and Affective
Neuroscience <span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">12</span>, 1351–1358 (8 2017). https://doi.org/10.1093/scan/nsx059

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Burkhardt, F., Paeschke, A., Rolfes, M., Sendlmeier, W.F., Weiss, B., et al.: A
database of german emotional speech. In: Interspeech. vol. 5, pp. 1517–1520
(2005)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Busso, C., Bulut, M., Lee, C.C., Kazemzadeh, A., Mower, E., Kim, S., Chang,
J.N., Lee, S., Narayanan, S.S.: Iemocap: Interactive emotional dyadic motion
capture database. Language resources and evaluation <span id="bib.bib7.1.1" class="ltx_text ltx_font_bold">42</span>, 335–359
(2008)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Ekman, P.: Emotional and conversational nonverbal signals. In: Language,
knowledge, and representation: Proceedings of the sixth international
colloquium on cognitive science (ICCS-99). pp. 39–50. Springer (2004)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Garcia-Cuesta, E., Salvador, A.B., Pãez, D.G.: EmoMatchSpanishDB: study of
speech emotion recognition machine learning models in a new spanish elicited
database <span id="bib.bib9.1.1" class="ltx_text ltx_font_bold">83</span>(5), 13093–13112. https://doi.org/10.1007/s11042-023-15959-w,
<a target="_blank" href="https://link.springer.com/10.1007/s11042-023-15959-w" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.springer.com/10.1007/s11042-023-15959-w</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Górriz, M., Antony, J., McGuinness, K., Giró-i Nieto, X., O’Connor,
N.E.: Assessing knee oa severity with cnn attention-based end-to-end
architectures. In: International Conference on Medical Imaging with Deep
Learning. pp. 197–214 (2019)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hortal, E., Brechard Alarcia, R.: Gantron: Emotional speech synthesis with
generative adversarial networks (10 2021)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Jaafar, N., Lachiri, Z.: Stress recognition from speech by combining
image-based deep spectrum and text-based features. In: 2022 IEEE Information
Technologies &amp; Smart Industrial Systems (ITSIS). pp. 1–6. IEEE (2022)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Kerkeni, L., Serrestou, Y., Mbarki, M., Raoof, K., Mahjoub, M.A.: Speech
emotion recognition: Methods and cases study:. In: Proceedings of the 10th
International Conference on Agents and Artificial Intelligence. pp. 175–182.
SCITEPRESS - Science and Technology Publications.
https://doi.org/10.5220/0006611601750182,
<a target="_blank" href="http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006611601750182" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006611601750182</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Lin, C.H., Liao, W.K., Hsieh, W.C., Liao, W.J., Wang, J.C.: Emotion
identification using extremely low frequency components of speech feature
contours. The Scientific World Journal <span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">2014</span>(1), 757121 (2014)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Martin, O., Kotsia, I., Macq, B., Pitas, I.: The enterface’05 audio-visual
emotion database. pp. 8–8. IEEE (2006). https://doi.org/10.1109/ICDEW.2006.145

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Preston, S.H., Vierboom, Y.C.: The changing age distribution of the united
states. Population and Development Review <span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">47</span>(2), 527–539 (2021)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Ringeval, F., Sonderegger, A., Sauer, J., Lalanne, D.: Introducing the recola
multimodal corpus of remote collaborative and affective interactions. In:
2013 10th IEEE international conference and workshops on automatic face and
gesture recognition (FG). pp. 1–8. IEEE (2013)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Schuller, B.W., Batliner, A., Amiriparian, S., Barnhill, A., Gerczuk, M.,
Triantafyllopoulos, A., Baird, A.E., Tzirakis, P., Gagne, C., Cowen, A.S.,
et al.: The acm multimedia 2023 computational paralinguistics challenge:
Emotion share &amp; requests. In: Proceedings of the 31st ACM International
Conference on Multimedia. pp. 9635–9639 (2023)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Šídlo, L., Šprocha, B., Ďurček, P.: A
retrospective and prospective view of current and future population ageing in
the european union 28 countries. Moravian geographical reports
<span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">28</span>(3), 187–207 (2020)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Vary, P., Martin, R.: Digital Speech Transmission and Enhancement. IEEE Press,
Wiley (2023), <a target="_blank" href="https://books.google.es/books?id=3rDiEAAAQBAJ" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://books.google.es/books?id=3rDiEAAAQBAJ</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Wu, S., Falk, T.H., Chan, W.Y.: Automatic speech emotion recognition using
modulation spectral features <span id="bib.bib22.1.1" class="ltx_text ltx_font_bold">53</span>(5), 768–785.
https://doi.org/10.1016/j.specom.2010.08.013,
<a target="_blank" href="https://linkinghub.elsevier.com/retrieve/pii/S0167639310001470" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://linkinghub.elsevier.com/retrieve/pii/S0167639310001470</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Yu, C., Sommerlad, A., Sakure, L., Livingston, G.: Socially assistive robots
for people with dementia: systematic review and meta-analysis of feasibility,
acceptability and the effect on cognition, neuropsychiatric symptoms and
quality of life. Ageing research reviews <span id="bib.bib23.1.1" class="ltx_text ltx_font_bold">78</span>, 101633 (2022)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Zadeh, A.B., Cao, Y., Hessner, S., Liang, P.P., Poria, S., Morency, L.P.:
Cmu-moseas: A multimodal language dataset for spanish, portuguese, german and
french. pp. 1801–1812. Association for Computational Linguistics (2020).
https://doi.org/10.18653/v1/2020.emnlp-main.141

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.05147" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.05148" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.05148">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.05148" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.05149" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 19:10:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
