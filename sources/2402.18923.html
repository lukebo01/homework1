<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.18923] Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition</title><meta property="og:description" content="Dysarthria, a common issue among stroke patients, severely impacts speech intelligibility. Inappropriate pauses are crucial indicators in severity assessment and speech-language therapy. We propose to extend a large-sc…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.18923">

<!--Generated on Tue Mar  5 15:42:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Dysarthria, a common issue among stroke patients, severely impacts speech intelligibility. Inappropriate pauses are crucial indicators in severity assessment and speech-language therapy. We propose to extend a large-scale speech recognition model for inappropriate pause detection in dysarthric speech. To this end, we propose task design, labeling strategy, and a speech recognition model with an inappropriate pause prediction layer. First, we treat pause detection as speech recognition, using an automatic speech recognition (ASR) model to convert speech into text with pause tags. According to the newly designed task, we label pause locations at the text level and their appropriateness. We collaborate with speech-language pathologists to establish labeling criteria, ensuring high-quality annotated data. Finally, we extend the ASR model with an inappropriate pause prediction layer for end-to-end inappropriate pause detection. Moreover, we propose a task-tailored metric for evaluating inappropriate pause detection independent of ASR performance. Our experiments show that the proposed method better detects inappropriate pauses in dysarthric speech than baselines. (Inappropriate Pause Error Rate: 14.47%)</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
Dysarthric Speech, Inappropriate Pause Detection, Pause Detection, Speech Recognition</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Post-stroke dysarthria, a speech disorder from stroke-induced muscle issues, impairs speech control. Stroke is a widely recognized public health issue, marked by a substantial occurrence and fatality rate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Dysarthria affects half of stroke patients, making communication difficult <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Symptoms vary by stroke location and size, necessitating personalized therapy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Current assessment relies on time-consuming auditory evaluation by healthcare professionals, highlighting the need for efficient automatic methods to improve stroke patient speech-language therapy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
We explore dysarthria assessment using the <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Autumn paragraph</span> in Korea <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We focus on identifying Inappropriate Pauses (IPs), which are essential for patient feedback, highlighting areas requiring further training. Traditionally, speech-language pathologists handle this in their face-to-face sessions, but we propose automating pause detection and appropriateness assessment with an artificial intelligence model to support their work.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Inappropriate pause (IP) refers to delays that occur in untypical locations. In dysarthric speech, pauses come at unexpected locations, such as in the middle of a noun phrase, resulting in reduced speech intelligibility <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
During the reading task, it is observed whether the patient cannot read a word in a single breath or stutters too much. Traditional assessment by pathologists relies on auditory analysis, lacking automatic inappropriate pause detection research. Detecting and assessing pauses within sentences using speech signals is necessary to evaluate pause appropriateness automatically. This is because the location of a pause within a sentence determines its appropriateness. Therefore, we first use the pause detection method to assess IPs.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Some traditional dysarthric speech pause detection approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> rely on algorithmic methods using amplitude thresholds. While these approaches effectively detect pause intervals, they only provide temporal information about pauses, not the location of pauses within the sentence. Some researchers use forced alignment to get pause locations in speech signals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Forced alignment detects pauses using speech and its transcription. However, its performance worsens because of the characteristics of dysarthric speech (i.e., inaccurate articulations, disfluencies, slow speaking rate, etc.) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Moreover, accurate transcription is necessary for forced alignment, resulting in a two-stage process: obtainment transcription and forced alignment.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To tackle these challenges, we approach pause detection as a speech recognition problem. We treat pauses as a distinct token in the automatic speech recognition (ASR) model, which inputs speech and produces text with pause tags.
Additionally, we enhance the ASR model with an inappropriate pause prediction layer, creating an end-to-end solution for detecting inappropriate pauses in dysarthric speech.
Our approach simplifies pause labeling at the text level compared to previous conventional temporal labeling.
Furthermore, in collaboration with speech-language pathologists, we establish criteria for inappropriate pauses, focusing on within-word insertions for precise sentence pause identification.
We also introduce a tailored evaluation metric specific to this task, allowing separate assessment of pause detection performance from ASR performance. From our experiment, incorporating pause detection into the ASR model also improves ASR performance.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Labelling Pause Location and Inappropriate Pause in Dysarthric Speech</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We utilize the Korean dysarthric speech corpus, which comprises 2,251 utterances from the <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">Autumn paragraph</span>, recorded by stroke patients. <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">Autumn paragraph</span> contains all the necessary consonants and vowels for evaluation. Although recorded speeches are for paragraph reading, we experiment on the sentence level. This is because pauses within sentences carry more importance than those between sentences. The severity scale for this datset is the NIH Stoke Scale: without dysarthria, mild to moderate, and severe cases. Table <a href="#S2.T1" title="Table 1 ‣ 2 Labelling Pause Location and Inappropriate Pause in Dysarthric Speech ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows Korean dysarthric speech corpus statistics. We first label pause location at a text level to detect IP in dysarthric speech and then annotate each pause appropriateness.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Dataset statistics by severity of dysarthric speech corpus.</figcaption>
<div id="S2.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:48.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(54.9pt,-6.1pt) scale(1.33890517109985,1.33890517109985) ;">
<table id="S2.T1.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.3.1.1.1" class="ltx_tr">
<th id="S2.T1.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Severity</th>
<th id="S2.T1.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">w/o dysarthria</th>
<th id="S2.T1.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Mild-to-Moderate</th>
<th id="S2.T1.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Severe</th>
<th id="S2.T1.3.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Total</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.3.1.2.1" class="ltx_tr">
<td id="S2.T1.3.1.2.1.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_tt"># of Utterances</td>
<td id="S2.T1.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">72</td>
<td id="S2.T1.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">1985</td>
<td id="S2.T1.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt">194</td>
<td id="S2.T1.3.1.2.1.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_tt">2251</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Labeling Pause Location</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Following previous research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, we define a pause as an uninterrupted silence portion of an utterance lasting at least 150 msec. We add <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">&lt;SIL&gt;</span> tags to indicate pause locations in the text level for the pause detection task, as shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Labeling Pause Location ‣ 2 Labelling Pause Location and Inappropriate Pause in Dysarthric Speech ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Labeling Pause Location ‣ 2 Labelling Pause Location and Inappropriate Pause in Dysarthric Speech ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we compare the proposed method to the TIMIT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>-style method, which labels the time information of each phoneme and pauses. TIMIT-style labeled corpus is usually utilized in phoneme segmentation. This approach is costly in labeling, but a substantial amount of labeled corpus is required to achieve reliable performance for clinical purposes. Therefore, we propose a simplified labeling method that reduces costs and enables more efficient analysis of the position of pauses on the paragraph-reading task with a smaller labeled corpus.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2402.18923/assets/labelling.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="204" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>Examples of TIMIT-style labeling (left) and the proposed labeling (right). The samples are from the Korean dysarthric speech corpus.</figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">With forced alignment, the pause location labeling is not necessary. However, forced alignment requires a transcription of a speech. In other words, forced alignment still needs transcription labeling. Our labeling strategy, which annotates pause location at the text level, allows for pause location labeling at a cost similar to transcription labeling.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2402.18923/assets/model_architecture_re.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="187" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>The proposed inappropriate pause detection model architecture. Above the whisper decoder layers, there are two task-specific layers: The inappropriate Pause Prediction layer (IP Prediction layer) and the Transcript Prediction Layer.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Labeling Inappropriate Pauses in Dysarthric Speech</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Based on the annotated locations of pauses, we further label their appropriateness. The labeling criteria were established in collaboration with speech-language pathologists based on their professional expertise in speech therapy. IP was annotated at the word level, where non-pauses are marked as 0, appropriate pauses as 1, and inappropriate pauses as 2. (<span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">IPSeq</span> in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.)</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">We define inappropriate pauses into three categories: Inappropriate pauses within a word unit, Pauses following vocalic surplus expressions, and Pauses resulting from attempts to correct inaccurate pronunciation. First, Inappropriate pauses within a word unit are inappropriate pauses. If pauses occur in unexpected places, such as within a noun phrase, they can detract from readability and intelligibility. Therefore, the pause is considered inappropriate if a patient cannot say a word in a single breath. The IPs corresponding to the first category pauses that occur at unrelated syntax boundaries are our primary concern <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Second, pauses following vocalic surplus expressions such as “uh” or “um” are inappropriate. Vocalic surplus expressions due to hesitation are not the focus of speech therapy, so only pauses accompanying unclear wording or incorrect pronunciation and excessively long pauses are deemed inappropriate. The excessively long pauses are pauses longer than 3 seconds. Lastly, pauses that occur to rectify mispronunciation are marked as inappropriate rather than simple repetitions. Patients tend to repeat the syllables because they are hard to pronounce words correctly at once. This criterion was also applied only when the pause intervals were excessively long. (longer than 3 seconds)</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Inappropriate Pause Detection as ASR</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We define pause detection as a speech recognition task and employ sequence-to-sequence (Seq2Seq) architecture to generate transformed text (text with pause tags) for a given speech, as shown in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.1 Labeling Pause Location ‣ 2 Labelling Pause Location and Inappropriate Pause in Dysarthric Speech ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We use OpenAI’s whisper-small <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> as our speech Seq2Seq model. Furthermore, we extend the Whisper model to train inappropriate pause detection and speech recognition tasks simultaneously. The detailed process of the proposed method is shown in Equation <a href="#S3.E1" title="In 3 Inappropriate Pause Detection as ASR ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. First, the Seq2Seq model processes an input speech X. From the latent representation Z of the Seq2Seq model, we incorporate two separate layers: the projection layer to its vocabulary size (Transcript prediction layer) and the inappropriate pause prediction layer (IP prediction layer). The transcript prediction layer projects the last hidden state to the vocabulary size for input speech transcription. Its output V includes pause tags if there is a pause interval in X. The IP prediction layer determines whether each token is an appropriate pause, inappropriate pause, or non-pause (word). To train the IP prediction layer, we employ the soft-dtw loss. The final loss <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">L</annotation></semantics></math> is shown in Equation <a href="#S3.E2" title="In 3 Inappropriate Pause Detection as ASR ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our model detects pauses and determines their appropriateness in an end-to-end manner, eliminating the need for post-processing.</p>
</div>
<div id="S3.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center">
<div id="S3.E1.m1.1.1.1" class="ltx_inline-block ltx_markedasmath ltx_transformed_outer" style="width:216.8pt;height:53.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(44.3pt,-11.0pt) scale(1.69141557367575,1.69141557367575) ;">
<p id="S3.E1.m1.1.1.1.1" class="ltx_p"><math id="S3.E1.m1.1.1.1.1.m1.3" class="ltx_Math" alttext="\begin{aligned} Z&amp;=\text{Seq2Seq}(X)\\
V^{\prime}&amp;=\text{TranscriptPrediction}(Z)\\
IP^{\prime}&amp;=\text{IPPrediction}(Z)\end{aligned}" display="inline"><semantics id="S3.E1.m1.1.1.1.1.m1.3a"><mtable columnspacing="0pt" rowspacing="0pt" id="S3.E1.m1.1.1.1.1.m1.3.3" xref="S3.E1.m1.1.1.1.1.m1.3.3.cmml"><mtr id="S3.E1.m1.1.1.1.1.m1.3.3a" xref="S3.E1.m1.1.1.1.1.m1.3.3.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.1.1.1.1.m1.3.3b" xref="S3.E1.m1.1.1.1.1.m1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.m1.1.1.1.2.1" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.2.1.cmml">Z</mi></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.1.1.1.1.m1.3.3c" xref="S3.E1.m1.1.1.1.1.m1.3.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.3.cmml"></mi><mo id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.cmml"><mtext id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.2a.cmml">Seq2Seq</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.1" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.1.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.3.2" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.3.2.1" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.1.cmml">X</mi><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.3.2.2" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1.m1.3.3d" xref="S3.E1.m1.1.1.1.1.m1.3.3.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.1.1.1.1.m1.3.3e" xref="S3.E1.m1.1.1.1.1.m1.3.3.cmml"><msup id="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1.cmml"><mi id="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1.2" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1.2.cmml">V</mi><mo id="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1.3" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1.3.cmml">′</mo></msup></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.1.1.1.1.m1.3.3f" xref="S3.E1.m1.1.1.1.1.m1.3.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.3" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.3.cmml"></mi><mo id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.2" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.cmml"><mtext id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.2" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.2a.cmml">TranscriptPrediction</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.1" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.1.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.3.2" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.3.2.1" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.1" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.1.cmml">Z</mi><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.3.2.2" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E1.m1.1.1.1.1.m1.3.3g" xref="S3.E1.m1.1.1.1.1.m1.3.3.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.1.1.1.1.m1.3.3h" xref="S3.E1.m1.1.1.1.1.m1.3.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.cmml"><mi id="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.2" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.1" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.1.cmml">​</mo><msup id="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3.2" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3.2.cmml">P</mi><mo id="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3.3" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3.3.cmml">′</mo></msup></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.1.1.1.1.m1.3.3i" xref="S3.E1.m1.1.1.1.1.m1.3.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.3" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.3.cmml"></mi><mo id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.2" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.cmml"><mtext id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.2" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.2a.cmml">IPPrediction</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.1" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.1.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.3.2" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.3.2.1" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.1" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.1.cmml">Z</mi><mo stretchy="false" id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.3.2.2" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.cmml">)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1.1.1.1.m1.3b"><matrix id="S3.E1.m1.1.1.1.1.m1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3"><matrixrow id="S3.E1.m1.1.1.1.1.m1.3.3a.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3"><ci id="S3.E1.m1.1.1.1.1.m1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.2.1">𝑍</ci><apply id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.2"></eq><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.3">absent</csymbol><apply id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4"><times id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.1"></times><ci id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.2a.cmml" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.2"><mtext id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.4.2">Seq2Seq</mtext></ci><ci id="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.1.1.1.1.1.1">𝑋</ci></apply></apply></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1.m1.3.3b.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3"><apply id="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1.2.cmml" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1.2">𝑉</ci><ci id="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1.3.cmml" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.2.1.3">′</ci></apply><apply id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1"><eq id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.2"></eq><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.3">absent</csymbol><apply id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4"><times id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.1"></times><ci id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.2a.cmml" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.2"><mtext id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.4.2">TranscriptPrediction</mtext></ci><ci id="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.2.2.2.1.1.1">𝑍</ci></apply></apply></matrixrow><matrixrow id="S3.E1.m1.1.1.1.1.m1.3.3c.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3"><apply id="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1"><times id="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.1"></times><ci id="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.2.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.2">𝐼</ci><apply id="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3.2">𝑃</ci><ci id="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.2.1.3.3">′</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1"><eq id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.2"></eq><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.3">absent</csymbol><apply id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4"><times id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.1"></times><ci id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.2a.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.2"><mtext id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.4.2">IPPrediction</mtext></ci><ci id="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.m1.3.3.3.1.1.1">𝑍</ci></apply></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1.1.1.1.m1.3c">\begin{aligned} Z&amp;=\text{Seq2Seq}(X)\\
V^{\prime}&amp;=\text{TranscriptPrediction}(Z)\\
IP^{\prime}&amp;=\text{IPPrediction}(Z)\end{aligned}</annotation></semantics></math></p>
</span></div>
</td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="0" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p3" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="L=\text{Cross-entropy}(V^{\prime})+\text{Soft-dtw}(IP^{\prime})" display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mi id="S3.E2.m1.2.2.4" xref="S3.E2.m1.2.2.4.cmml">L</mi><mo id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml">=</mo><mrow id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mtext id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3a.cmml">Cross-entropy</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">V</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.2.3" xref="S3.E2.m1.2.2.2.3.cmml">+</mo><mrow id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml"><mtext id="S3.E2.m1.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.3a.cmml">Soft-dtw</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.cmml">​</mo><mrow id="S3.E2.m1.2.2.2.2.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.2.2.1.1.2" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.2.2.1.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml"><mi id="S3.E2.m1.2.2.2.2.1.1.1.2" xref="S3.E2.m1.2.2.2.2.1.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.1.1.1.1" xref="S3.E2.m1.2.2.2.2.1.1.1.1.cmml">​</mo><msup id="S3.E2.m1.2.2.2.2.1.1.1.3" xref="S3.E2.m1.2.2.2.2.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.2.2.1.1.1.3.2" xref="S3.E2.m1.2.2.2.2.1.1.1.3.2.cmml">P</mi><mo id="S3.E2.m1.2.2.2.2.1.1.1.3.3" xref="S3.E2.m1.2.2.2.2.1.1.1.3.3.cmml">′</mo></msup></mrow><mo stretchy="false" id="S3.E2.m1.2.2.2.2.1.1.3" xref="S3.E2.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><eq id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3"></eq><ci id="S3.E2.m1.2.2.4.cmml" xref="S3.E2.m1.2.2.4">𝐿</ci><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><plus id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.3"></plus><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.3a.cmml" xref="S3.E2.m1.1.1.1.1.3"><mtext id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3">Cross-entropy</mtext></ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">𝑉</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">′</ci></apply></apply><apply id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2"><times id="S3.E2.m1.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2"></times><ci id="S3.E2.m1.2.2.2.2.3a.cmml" xref="S3.E2.m1.2.2.2.2.3"><mtext id="S3.E2.m1.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.3">Soft-dtw</mtext></ci><apply id="S3.E2.m1.2.2.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1"><times id="S3.E2.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.1"></times><ci id="S3.E2.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.2">𝐼</ci><apply id="S3.E2.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3">superscript</csymbol><ci id="S3.E2.m1.2.2.2.2.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3.2">𝑃</ci><ci id="S3.E2.m1.2.2.2.2.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.2.2.1.1.1.3.3">′</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">L=\text{Cross-entropy}(V^{\prime})+\text{Soft-dtw}(IP^{\prime})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The corpus used for training is divided into training, validation, and test sets in the ratio of 8:1:1. The samples for each set are 1800, 225, and 226, respectively. The division of the dataset is done while maintaining the ratio of data samples for each severity level.
The hyper-parameters used for training are as follows. For the AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> optimizer, we set the learning rate to 5e-4 with linear decaying. We set the batch size as 8.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We compare the proposed method with Montreal Forced Alignment (MFA)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
For baseline, three types of MFA were used. <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">MFA-GT</span> aligns the ground truth transcription and speech. <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">MFA-Whisper</span> and <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_italic">MFA-Dysarthric-Whisper</span> utilize ASR transcription from open-source Whisper and fine-tuned Whisper with dysarthric speech, respectively. (Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Results ‣ 4 Experiments ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) Whisper fine-tuning was done with the same Korean dysarthric speech corpus, and the training details were the same.
Moreover, we also train the proposed method, except with a different ASR model, Conformer-RNNT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. We used a Korean pre-trained Conformer model.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://huggingface.co/eesungkim/stt_kr_conformer_transducer_large</span></span></span></span>
The training details were maintained for the Conformer-RNNT model. Note that <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_italic">Conformer-RNNT</span> does not have an IP prediction layer. We further discuss this in Section <a href="#S5" title="5 Discussion ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Note that we only compare the pause detection performance with other methods because, to our best knowledge, we are the first to detect IPs in dysarthric speech automatically. Hence, there is no baseline.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2402.18923/assets/table_3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="223" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text ltx_font_bold">Fig. 3</span>: </span>Example of pause sequences for calculation. Above is an example of a sequence with pauses to see if the model measures pauses well, and below is an example of a sequence to see if the model measures inappropriate pauses well.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Metrics</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We separately evaluate ASR and pause detection. We use word error rate (WER) and character error rate (CER) for ASR. To measure WER and CER, we remove pause tags from a generated transcript. In the case of pause detection, we propose new metrics to evaluate the accuracy of pause detection and inappropriate pause detection tasks independently of the speech recognition results. To this end, we first construct <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">PauseSeq</span> and <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">IPSeq</span> as shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In the case of <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">PauseSeq</span>, the ground truth and predicted transcriptions are marked as 1 if a word is a pause tag and 0 otherwise. For <span id="S4.SS2.p1.1.4" class="ltx_text ltx_font_italic">IPSeq</span>, which stands for Inappropriate Pause Sequence, each transcription is converted to the sequence with 0, 1, and 2, where 0 is non-pause, 1 is an appropriate pause, and 2 is IP. From the asr-independent sequence, we measure the performance by error rate. We calculate CER with the <span id="S4.SS2.p1.1.5" class="ltx_text ltx_font_italic">PauseSeq</span> and <span id="S4.SS2.p1.1.6" class="ltx_text ltx_font_italic">IPSeq</span> and refer to these <span id="S4.SS2.p1.1.7" class="ltx_text ltx_font_italic">PauER</span> and <span id="S4.SS2.p1.1.8" class="ltx_text ltx_font_italic">IPER</span>, respectively.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Results ‣ 4 Experiments ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the ASR and pause detection performance. Our method performs better than other methods in ASR and pause detection. Specifically, <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">PauER</span> is lower in our method than <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_italic">MFA-GT</span>. When comparing three types of MFA, pause detection performance was influenced by transcription performance. The best results among MFAs were obtained using GT transcription, followed by <span id="S4.SS3.p1.1.3" class="ltx_text ltx_font_italic">Dysarthric-Whisper</span> and <span id="S4.SS3.p1.1.4" class="ltx_text ltx_font_italic">Whisper</span>. This trend persisted even for pause detection fine-tuned <span id="S4.SS3.p1.1.5" class="ltx_text ltx_font_italic">Conformer-RNNT</span> model. Higher ASR performance correlated with improved pause detection performance. Additionally, comparing ASR performance, fine-tuning with dysarthric speech for ASR-only (<span id="S4.SS3.p1.1.6" class="ltx_text ltx_font_italic">MFA-Dysarthric-Whisper</span>) versus jointly training for pause detection and ASR (<span id="S4.SS3.p1.1.7" class="ltx_text ltx_font_italic">Ours</span>) showed enhanced ASR performance. This suggests a complementary relationship between pause detection and ASR in dysarthric speech, where prolonged pause intervals are prominent characteristics.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.5.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Pause detection and ASR evaluation. <span id="S4.T2.6.2" class="ltx_text ltx_font_italic">WER</span> and <span id="S4.T2.7.3" class="ltx_text ltx_font_italic">CER</span> measure ASR performance, and <span id="S4.T2.8.4" class="ltx_text ltx_font_italic">PauER</span> measures pause detection performance.</figcaption>
<div id="S4.T2.9" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:164.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(74.2pt,-28.1pt) scale(1.51998954642537,1.51998954642537) ;">
<table id="S4.T2.9.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.9.1.1.1" class="ltx_tr">
<th id="S4.T2.9.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S4.T2.9.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">WER(%)</td>
<td id="S4.T2.9.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CER(%)</td>
<td id="S4.T2.9.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">PauER(%)</td>
</tr>
<tr id="S4.T2.9.1.2.2" class="ltx_tr">
<th id="S4.T2.9.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">MFA-GT</th>
<td id="S4.T2.9.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
<td id="S4.T2.9.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
<td id="S4.T2.9.1.2.2.4" class="ltx_td ltx_align_center ltx_border_tt">11.14</td>
</tr>
<tr id="S4.T2.9.1.3.3" class="ltx_tr">
<th id="S4.T2.9.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MFA-Whisper</th>
<td id="S4.T2.9.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">54.89</td>
<td id="S4.T2.9.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.35</td>
<td id="S4.T2.9.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">22.49</td>
</tr>
<tr id="S4.T2.9.1.4.4" class="ltx_tr">
<th id="S4.T2.9.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MFA-Dysarthric-Whisper</th>
<td id="S4.T2.9.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.21</td>
<td id="S4.T2.9.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.38</td>
<td id="S4.T2.9.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">17.27</td>
</tr>
<tr id="S4.T2.9.1.5.5" class="ltx_tr">
<th id="S4.T2.9.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Conformer-RNNT</th>
<td id="S4.T2.9.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">64.52</td>
<td id="S4.T2.9.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">49.99</td>
<td id="S4.T2.9.1.5.5.4" class="ltx_td ltx_align_center ltx_border_tt">22.81</td>
</tr>
<tr id="S4.T2.9.1.6.6" class="ltx_tr">
<th id="S4.T2.9.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Ours</th>
<td id="S4.T2.9.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.9.1.6.6.2.1" class="ltx_text ltx_font_bold">25.31</span></td>
<td id="S4.T2.9.1.6.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.9.1.6.6.3.1" class="ltx_text ltx_font_bold">11.96</span></td>
<td id="S4.T2.9.1.6.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.9.1.6.6.4.1" class="ltx_text ltx_font_bold">3.077</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>IP detection, Pause detection, and ASR performance according to severity level</figcaption>
<div id="S4.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:128.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(65.0pt,-19.3pt) scale(1.42807543154882,1.42807543154882) ;">
<table id="S4.T3.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.1.1.1" class="ltx_tr">
<th id="S4.T3.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Severity</th>
<th id="S4.T3.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">WER(%)</th>
<th id="S4.T3.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">CER(%)</th>
<th id="S4.T3.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">PauER(%)</th>
<th id="S4.T3.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">IPER(%)</th>
</tr>
<tr id="S4.T3.3.1.2.2" class="ltx_tr">
<th id="S4.T3.3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Total</th>
<th id="S4.T3.3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">25.31</th>
<th id="S4.T3.3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">11.96</th>
<th id="S4.T3.3.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">3.07</th>
<th id="S4.T3.3.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">14.47</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.1.3.1" class="ltx_tr">
<th id="S4.T3.3.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">w/o dysarthria</th>
<td id="S4.T3.3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">6.93</td>
<td id="S4.T3.3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">2.89</td>
<td id="S4.T3.3.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">2.48</td>
<td id="S4.T3.3.1.3.1.5" class="ltx_td ltx_align_center ltx_border_tt">20.69</td>
</tr>
<tr id="S4.T3.3.1.4.2" class="ltx_tr">
<th id="S4.T3.3.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Mild-to-Moderate</th>
<td id="S4.T3.3.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.38</td>
<td id="S4.T3.3.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.20</td>
<td id="S4.T3.3.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.03</td>
<td id="S4.T3.3.1.4.2.5" class="ltx_td ltx_align_center ltx_border_t">15.53</td>
</tr>
<tr id="S4.T3.3.1.5.3" class="ltx_tr">
<th id="S4.T3.3.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Severe</th>
<td id="S4.T3.3.1.5.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">57.44</td>
<td id="S4.T3.3.1.5.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">30.47</td>
<td id="S4.T3.3.1.5.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">3.60</td>
<td id="S4.T3.3.1.5.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">13.40</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Results ‣ 4 Experiments ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the IP detection performance of the proposed method. <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">Total</span> is the same model with <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">Ours</span> in Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Results ‣ 4 Experiments ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We dissect the performance according to the dysarthria severity. Dysarthric speech becomes more slurred and challenging to understand as its severity increases. An effective IP detection method operates robustly across different levels of severity. This is crucial to ensure that effective diagnosis and feedback can be provided even for patients with severe dysarthria. In Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Results ‣ 4 Experiments ‣ Inappropriate Pause Detection in Dysarthric Speech Using Large-Scale Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we can observe that IP and pause detection performance remains relatively consistent across different severity levels. In the case of <span id="S4.SS3.p2.1.3" class="ltx_text ltx_font_italic">IPER</span>, performance deteriorates as severity decreases. This is because lower severity levels result in fewer instances of IPs. However, ASR performance decreases as the severity level increases. This suggests that the data used for training was sufficient for IP detection but insufficient for training ASR, leading to this discrepancy in performance.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We experimented with our proposed architecture across various ASR models, including wav2vec 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>-CTC, but the IP layer was not trained effectively with decoding methods such as CTC and RNN-Transducer. It’s possible that our proposed IP detection method may not apply to ASR models with different decoding strategies. However, the approach using Whisper, as we suggest, offers scalability. With simple text-level labeling, it can be extended to languages beyond Korean. Moreover, incorporating recent advancements in Whisper, such as word-boundary extraction using cross-attention, it becomes possible to extract pause durations as well.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We address IP detection in dysarthric speech by treating pauses as distinct tokens in the ASR model. Furthermore, we extend the ASR model with the IP prediction layer, constructing an end-to-end process in IP detection. In this way, we simplify pause labeling and enhance ASR performance in dysarthric speech. We also establish pause appropriateness criteria in the collaboration of medical professionals.
The detailed evaluation showed that the proposed method performs better pause detection than baseline and severity-robust IP detection performance. The proposed model’s consistency in detecting inappropriate pauses across different dysarthria severity levels highlights its potential for providing effective diagnosis and feedback, particularly for patients with severe dysarthria.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Valery L Feigin, Michael Brainin, Bo Norrving, Sheila Martins, Ralph L Sacco,
Werner Hacke, Marc Fisher, Jeyaraj Pandian, and Patrice Lindsay,

</span>
<span class="ltx_bibblock">“World stroke organization (wso): global stroke fact sheet 2022,”

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">International Journal of Stroke</span>, vol. 17, no. 1, pp. 18–29,
2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Heather L Flowers, Frank L Silver, Jiming Fang, Elizabeth Rochon, and Rosemary
Martino,

</span>
<span class="ltx_bibblock">“The incidence, co-occurrence, and predictors of dysphagia,
dysarthria, and aphasia after first-ever acute ischemic stroke,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Journal of communication disorders</span>, vol. 46, no. 3, pp.
238–248, 2013.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Pamela Comrie, Catherine MacKenzie, and James McCall,

</span>
<span class="ltx_bibblock">“The influence of acquired dysarthria on conversational
turn-taking,”

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Clinical linguistics &amp; phonetics</span>, vol. 15, no. 5, pp.
383–398, 2001.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Claire Mitchell, Audrey Bowen, Sarah Tyson, and Paul Conroy,

</span>
<span class="ltx_bibblock">“A feasibility randomized controlled trial of readyspeech for people
with dysarthria after stroke,”

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Clinical Rehabilitation</span>, vol. 32, no. 8, pp. 1037–1046, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Imed Laaridh, Christine Meunier, and Corinne Fredouille,

</span>
<span class="ltx_bibblock">“Perceptual evaluation for automatic anomaly detection in disordered
speech: Focus on ambiguous cases,”

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Speech Communication</span>, vol. 105, pp. 23–33, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
HyangHee Kim,

</span>
<span class="ltx_bibblock">“Dysarthria evaluation,”

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Communication Sciences &amp; Disorders</span>, pp. 23–28, 2005.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Leeseul Shim Jiyeon Han, Okbun Lee,

</span>
<span class="ltx_bibblock">“The study of breath group based on oral airflow in reading by
healthy speakers,”

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Speech Sciences</span>, vol. 15, no. 4, pp. 135–146, 2008.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Vicki L. Hammen and Kathryn M. Yorkston,

</span>
<span class="ltx_bibblock">“Speech and pause characteristics following speech rate reduction in
hypokinetic dysarthria,”

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Journal of Communication Disorders</span>, vol. 29, no. 6, pp.
429–445, 1996.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Kristin Rosen, Bruce Murdoch, Joanne Folker, Adam Vogel, Louise Cahill, Martin
Delatycki, and Louise Corben,

</span>
<span class="ltx_bibblock">“Automatic method of pause measurement for normal and dysarthric
speech,”

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Clinical Linguistics &amp; Phonetics</span>, vol. 24, no. 2, pp.
141–154, 2010.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Jordan R Green, David R Beukelman, and Laura J Ball,

</span>
<span class="ltx_bibblock">“Algorithmic estimation of pauses in extended speech samples of
dysarthric and typical speech,”

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">J. Med. Speech. Lang. Pathol.</span>, vol. 12, no. 4, pp. 149–154,
2004.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Yu Ting Yeung, Ka Ho Wong, and Helen Meng,

</span>
<span class="ltx_bibblock">“Improving automatic forced alignment for dysarthric speech
transcription,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2015</span>, 2015, pp. 2991–2995.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
G. Diwakar and Veena Karjigi,

</span>
<span class="ltx_bibblock">“Improving speech to text alignment based on repetition detection
for dysarthric speech,”

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Circuits, Systems, and Signal Processing</span>, vol. 39, no. 11, pp.
5543–5567, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Sim Hyun Sub Kim Ki Eun,

</span>
<span class="ltx_bibblock">“The reading rate characteristics of adults with cerebral palsy,”

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Journal of Special Education</span>, vol. 34, no. 4, pp. 49–72, 2001.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Ko Yolmae, Kim Deog Young, Choi Yaelin, and Kim HyangHee,

</span>
<span class="ltx_bibblock">“Speech rate and pause characteristics in patients with parkinson’s
disease,”

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Phonetics and Speech Sciences</span>, vol. 2, no. 4, pp. 173–184,
2010.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
John S Garofolo,

</span>
<span class="ltx_bibblock">“Timit acoustic phonetic continuous speech corpus,”

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Linguistic Data Consortium, 1993</span>, 1993.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Meghan Darling-White and Jessica Huber,

</span>
<span class="ltx_bibblock">“The impact of parkinson’s disease on breath pauses and their
relationship to speech impairment: A longitudinal study,”

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">American Journal of Speech-Language Pathology</span>, vol. 29, pp.
1–13, 07 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
Ilya Sutskever,

</span>
<span class="ltx_bibblock">“Robust speech recognition via large-scale weak supervision,”

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2212.04356</span>, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter,

</span>
<span class="ltx_bibblock">“Decoupled weight decay regularization,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan
Sonderegger,

</span>
<span class="ltx_bibblock">“Montreal Forced Aligner: Trainable Text-Speech Alignment Using
Kaldi,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2017</span>, 2017, pp. 498–502.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu,
Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang,

</span>
<span class="ltx_bibblock">“Conformer: Convolution-augmented transformer for speech
recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2020</span>, 2020, pp. 5036–5040.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli,

</span>
<span class="ltx_bibblock">“wav2vec 2.0: A framework for self-supervised learning of speech
representations,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 2020,
vol. 33, pp. 12449–12460.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.18922" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.18923" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.18923">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.18923" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.18924" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 15:42:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
