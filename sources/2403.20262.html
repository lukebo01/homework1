<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.20262] ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models</title><meta property="og:description" content="Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models’ context size to better capture dependencies within long documents. While benchmarks have been proposed to asse…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.20262">

<!--Generated on Fri Apr  5 17:44:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\affiliations</span>
<p id="p1.2" class="ltx_p">NAVER LABS Europe
<span id="p1.2.1" class="ltx_ERROR undefined">\contributions</span>
<span id="p1.2.2" class="ltx_ERROR undefined">\website</span>
<span id="p1.2.3" class="ltx_ERROR undefined">\websiteref</span>



<span id="p1.2.4" class="ltx_ERROR undefined">\newfloatcommand</span>capbtabboxtable[][<span id="p1.2.5" class="ltx_ERROR undefined">\FBwidth</span>]</p>
</div>
<h1 class="ltx_title ltx_title_document">ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models’ context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, our work proposes a new benchmark for long-context LLMs focused on a practical meeting assistant scenario. In this scenario, the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, named ELITR-Bench, augments the existing ELITR corpus’ transcripts with 271 manually crafted questions and their ground-truth answers. Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, especially when questions are asked sequentially within a conversation. We also provide a thorough analysis of our GPT-4-based evaluation method, encompassing insights from a crowdsourcing study. Our findings suggest that while GPT-4’s evaluation scores are correlated with human judges’, its ability to differentiate among more than three score levels may be limited.</p>
</div>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The context window of Large Language Models (LLMs) has recently undergone a significant expansion, scaling from a few thousand tokens to tens or even hundreds of thousands <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. As a consequence, benchmarks have emerged to assess LLMs’ long-range abilities, tackling the specific challenges of Question Answering (QA) on long documents <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>. However, while previous datasets focusing on long-context models offer longitudinal evaluations across different tasks, they often provide only superficial analyses of each task. The covered tasks are also often generic – e.g., questions on Wikipedia <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> – and thus not particularly suitable for realistic, focused applications.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In contrast, our work advocates for a situated evaluation of long-context LLM performance within specific, real-world scenarios. As a practical illustration, consider a meeting assistant that allows users to inquire about meetings they did not attend. Given that hour-long meeting transcripts must fit within the agent’s context window, proficient handling of long contexts is a prerequisite.
This paper then introduces the first benchmark – to the best of our knowledge – for evaluating long-context LLMs on a realistic meeting assistant task. Our benchmark, named ELITR-Bench,<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We release the data for our benchmark at <a target="_blank" href="https://github.com/utter-project/UTTER-MS9-meetingdata/tree/master/ELITR-Bench" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/utter-project/UTTER-MS9-meetingdata/tree/master/ELITR-Bench</a>.</span></span></span> is built upon the meeting transcripts of the past ELITR project <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>. These transcripts have been obtained by Automatic Speech Recognition (ASR) with minimal human corrections – yielding long, noisy documents of oral nature that present unique challenges for LLMs. Our extensive experiments on ELITR-Bench with 9 recent long-context LLMs showed a gap between proprietary and open-source models that is emphasized when questions are asked sequentially within a conversation rather in a QA mode. We also provide an in-depth validation of our GPT-4-based evaluation through a crowdsourcing study, showing a high correlation with human judges’ scores. However, we also point out GPT-4’s limited ability to distinguish beyond three score levels.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The remainder of the paper is structured as follows. We provide a review of related literature in Section <a href="#S2" title="2 Related work ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, before introducing the proposed ELITR-Bench in Section <a href="#S3" title="3 ELITR-Bench ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We then describe our experimental setup and results in Sections <a href="#S4" title="4 Experimental setup ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S5" title="5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, respectively. Section <a href="#S6" title="6 LLM-based evaluation assessment ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> provides an in-depth assessment of our LLM-based evaluation methodology. Finally, Section <a href="#S7" title="7 Conclusion ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> concludes the paper and provides some perspectives for future work.</p>
</div>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Long-context LLMs and techniques.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Numerous techniques have emerged to address the challenge of long-context modeling.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>For a comprehensive collection of resources on this subject, we let the reader refer to <a target="_blank" href="https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling</a>.</span></span></span> While an exhaustive survey of these methods is beyond the scope of this paper, they can generally be categorized into three main groups (excluding other distinct approaches such as retrieval-augmented generation <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> and context compression <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>): (a) the development of efficient transformer architectures to address the quadratic attention challenge, including sparse transformers <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, linear transformers <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, and hierarchical transformers <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>; (b) approaches like recurrent attention networks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> and state-space models <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>; (c) length extrapolation or position embedding interpolation, where LLMs are fine-tuned or adapted at inference time to adjust tokens’ positions to match the new context length <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>.
These techniques also contributed to the context length expansion in proprietary models like GPT-4 (32K-128K), Claude-3 (200k), and Gemini-1.5 (128K-1M).</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Long-context benchmarks.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Several benchmarks have recently emerged with the growing interest in evaluating techniques that extend the context length of LLMs. Long Range Arena <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> was proposed to assess the quality of efficient transformer models in long-context scenarios, covering 1K-16K tokens sequences through different data types and modalities. L-Eval<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> offers a comprehensive evaluation suite with 20 sub-tasks and over 2,000 human-labeled query-response pairs, aggregating pre-existing datasets like NarrativeQA <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>.
LongEval <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> proposes synthetic tasks of varying difficulty, while
LongBench <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> and LongBench-Chat <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> aggregate several datasets in English and Chinese.
Other recent benchmarks appeared, such as: LongAlpaca <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, Loogle <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, LoCoMo <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, BAMBOO <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>, FLenQA <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>, and <math id="S2.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\infty" display="inline"><semantics id="S2.SS0.SSS0.Px2.p1.1.m1.1a"><mi mathvariant="normal" id="S2.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">∞</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.1.m1.1b"><infinity id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.1.m1.1c">\infty</annotation></semantics></math>Bench <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> that proposes an average data length over 100K tokens.
Our contribution, ELITR-Bench, distinguishes itself from existing benchmarks in several ways: (a) it focuses on a real use-case – meeting assistants, (b) it challenges models by requiring them to make inferences from noisy ASR-based documents, and (c) it offers both question answering and conversation versions (see Section <a href="#S3" title="3 ELITR-Bench ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), enabling the analysis of different prompt modes.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation with LLMs.</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Recent works explored the use of LLMs such as GPT-4 as judges to evaluate responses on open-ended questions. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Zheng et al.</span> [<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite> measured agreement between LLM and human evaluators while introducing two datasets (MT-bench and Chatbot Arena).
They showed that LLM judges like GPT-4 can match both controlled and crowdsourced human annotations, achieving over 80% agreement – the same level of agreement between humans.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">He et al.</span> [<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> evaluated the performance of GPT-4 against 415 crowdsourcing human labelers.
Despite employing best quality control practices, the highest labeling accuracy achieved through crowdsourcing was 81.5% whereas GPT-4 obtained 83.6%.
As in certain scenarios, employing proprietary LLMs as evaluators can pose challenges due to their closed-source nature, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Kim et al.</span> [<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> introduced Prometheus, an open-source LLM fine-tuned for evaluation. In this work, we compare LLMs-as-a-judge (GPT-4 and Prometheus) with expert and crowdsourcing-based human evaluators to assess responses generated by several long-context models on ELITR-Bench.
</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>ELITR-Bench</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We build our benchmark on top of the ELITR Minuting Corpus <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Accessible at: <a target="_blank" href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4692" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4692</a></span></span></span> This corpus contains transcripts of meetings conducted in both Czech and English, along with manually crafted summaries referred to as ‘minutes’. The meeting durations range from 10 minutes to over 2 hours, with the majority lasting around one hour. Although transcripts have been corrected from ASR outputs, they still contain noise and reflect various oral language phenomena such as interjections. Each transcript is de-identified<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The authors ensured the removal or masking of any personally identifiable information (PII), such as names, addresses, or other details from the transcripts. Moreover, they de-identified any project or organization-related information, as its inclusion could indirectly reveal the individuals involved.</span></span></span> and accompanied by one or multiple corresponding minutes files. However, in the benchmark described here, we only use the verbatim transcripts and exclude the minutes. In the current version of ELITR-Bench, our focus is on English meetings. Specifically, we utilized the official <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">dev</span> and <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">test2</span> sets, consisting of 10 and 8 meetings respectively, both sourced from ELITR-English. These meetings focus on discussions related to the computer science domain, with a particular emphasis on Natural Language Processing (NLP) topics. For every meeting within this corpus, we meticulously formulated a series of questions that can be directly addressed using the corresponding transcript, and provided their corresponding ground-truth answers. We present in Appendix <a href="#A4" title="Appendix D ELITR-Bench excerpt ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> (Table <a href="#A4.T12" title="Table 12 ‣ Appendix D ELITR-Bench excerpt ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>) a snippet of an ELITR meeting transcript, and showcase examples of questions and answers introduced in ELITR-Bench.</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Question type and answer position.</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">The questions we defined span various types, including:
<span id="S3.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_bold">Who</span> questions,
<span id="S3.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_bold">What</span> questions (that also cover <span id="S3.SS0.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_italic">Why</span> questions),
<span id="S3.SS0.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_bold">When</span> questions,
and <span id="S3.SS0.SSS0.Px1.p1.1.5" class="ltx_text ltx_font_bold">How many</span> questions.
Additionally, we annotated the position of the answer within the meeting transcript, categorizing it as either in the <span id="S3.SS0.SSS0.Px1.p1.1.6" class="ltx_text ltx_font_bold">Beginning</span> (first third), <span id="S3.SS0.SSS0.Px1.p1.1.7" class="ltx_text ltx_font_bold">Middle</span> (second third), <span id="S3.SS0.SSS0.Px1.p1.1.8" class="ltx_text ltx_font_bold">End</span> (final third), or spanning <span id="S3.SS0.SSS0.Px1.p1.1.9" class="ltx_text ltx_font_bold">Several</span> passages throughout the transcript. This annotation was conducted to verify the findings of <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Liu et al.</span> [<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>, which suggest that LLMs may face challenges in processing information located in the middle of long contexts, potentially leading to performance degradation.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">QA and Conversation settings.</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">The proposed ELITR-Bench is available in two settings. In <span id="S3.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_bold">ELITR-Bench-QA</span>, we designed for each meeting a set of stand-alone questions (along with their answers) that can be addressed solely based on the meeting transcript, without additional context. We also designed a modified <span id="S3.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_bold">ELITR-Bench-Conv</span> version where questions are to be asked in sequence, in a pre-defined order within a conversation. In this setting, some of the questions contain pronominal references or ellipses, for which previous conversational context (i.e., previous questions and answers) must be used to answer properly. For example, the question “<span id="S3.SS0.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">What is challenging about testing the demo system at the students firm fair?</span>” from the QA setting is replaced in the Conv setting with “<span id="S3.SS0.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_italic">What is challenging about this event?</span>”, where the answer to the previous question in the conversation was “<span id="S3.SS0.SSS0.Px2.p1.1.5" class="ltx_text ltx_font_italic">The students firm fair</span>”. Such questions have been obtained by manually re-writing the Conv questions into QA questions by resolving coreferences. The number of QA/Conv differentiating questions is 16 (out of 141) for the dev set and 17 (out of 130) for the test set.</p>
</div>
<div id="S3.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p2.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ QA and Conversation settings. ‣ 3 ELITR-Bench ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides a summary of the statistics for our benchmark. In the upcoming sections, we will showcase the performance of long-context LLMs on ELITR-Bench, particularly in their ability to handle hour-long meetings – which requires processing extended contexts of more than 10K tokens on average.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.10.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.11.2" class="ltx_text" style="font-size:90%;">Statistics for the ELITR-Bench dataset: all questions and answers are annotated by question type (<span id="S3.T1.11.2.1" class="ltx_text ltx_font_italic">What</span>, <span id="S3.T1.11.2.2" class="ltx_text ltx_font_italic">Who</span>, <span id="S3.T1.11.2.3" class="ltx_text ltx_font_italic">When</span>, <span id="S3.T1.11.2.4" class="ltx_text ltx_font_italic">How many</span>) and by the position of the answer within the meeting transcript (<span id="S3.T1.11.2.5" class="ltx_text ltx_font_italic">Beginning</span>, <span id="S3.T1.11.2.6" class="ltx_text ltx_font_italic">Middle</span>, <span id="S3.T1.11.2.7" class="ltx_text ltx_font_italic">End</span>, or spanning <span id="S3.T1.11.2.8" class="ltx_text ltx_font_italic">Several</span> sections). The number of tokens per meeting is counted using a LLaMA-2 tokenizer.</span></figcaption>
<div id="S3.T1.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:389.3pt;height:166.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.6pt,9.3pt) scale(0.9,0.9) ;">
<table id="S3.T1.12.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.12.1.1.1" class="ltx_tr">
<th id="S3.T1.12.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.12.1.1.1.1.1" class="ltx_text ltx_font_bold">Split</span></th>
<th id="S3.T1.12.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.12.1.1.1.2.1" class="ltx_text ltx_font_bold">#Meetings</span></th>
<th id="S3.T1.12.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.12.1.1.1.3.1" class="ltx_text ltx_font_bold">#Questions</span></th>
<th id="S3.T1.12.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" colspan="2">
<table id="S3.T1.12.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.12.1.1.1.4.1.1" class="ltx_tr">
<td id="S3.T1.12.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T1.12.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">#Questions by</span></td>
</tr>
<tr id="S3.T1.12.1.1.1.4.1.2" class="ltx_tr">
<td id="S3.T1.12.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T1.12.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">question type</span></td>
</tr>
</table>
</th>
<th id="S3.T1.12.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" colspan="2">
<table id="S3.T1.12.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.12.1.1.1.5.1.1" class="ltx_tr">
<td id="S3.T1.12.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T1.12.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">#Questions by</span></td>
</tr>
<tr id="S3.T1.12.1.1.1.5.1.2" class="ltx_tr">
<td id="S3.T1.12.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T1.12.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">answer position</span></td>
</tr>
</table>
</th>
<th id="S3.T1.12.1.1.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<table id="S3.T1.12.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.12.1.1.1.6.1.1" class="ltx_tr">
<td id="S3.T1.12.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T1.12.1.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">#Tokens per meeting:</span></td>
</tr>
<tr id="S3.T1.12.1.1.1.6.1.2" class="ltx_tr">
<td id="S3.T1.12.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S3.T1.12.1.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold">average [min; max]</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.12.1.2.1" class="ltx_tr">
<td id="S3.T1.12.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Dev</td>
<td id="S3.T1.12.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">10</td>
<td id="S3.T1.12.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">141</td>
<td id="S3.T1.12.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">What</td>
<td id="S3.T1.12.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t">59</td>
<td id="S3.T1.12.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t">Begin</td>
<td id="S3.T1.12.1.2.1.7" class="ltx_td ltx_align_right ltx_border_t">45</td>
<td id="S3.T1.12.1.2.1.8" class="ltx_td ltx_align_right ltx_border_t">11,339 [5,152; 17,410]</td>
</tr>
<tr id="S3.T1.12.1.3.2" class="ltx_tr">
<td id="S3.T1.12.1.3.2.1" class="ltx_td"></td>
<td id="S3.T1.12.1.3.2.2" class="ltx_td"></td>
<td id="S3.T1.12.1.3.2.3" class="ltx_td"></td>
<td id="S3.T1.12.1.3.2.4" class="ltx_td ltx_align_left">Who</td>
<td id="S3.T1.12.1.3.2.5" class="ltx_td ltx_align_right">51</td>
<td id="S3.T1.12.1.3.2.6" class="ltx_td ltx_align_left">Middle</td>
<td id="S3.T1.12.1.3.2.7" class="ltx_td ltx_align_right">29</td>
<td id="S3.T1.12.1.3.2.8" class="ltx_td"></td>
</tr>
<tr id="S3.T1.12.1.4.3" class="ltx_tr">
<td id="S3.T1.12.1.4.3.1" class="ltx_td"></td>
<td id="S3.T1.12.1.4.3.2" class="ltx_td"></td>
<td id="S3.T1.12.1.4.3.3" class="ltx_td"></td>
<td id="S3.T1.12.1.4.3.4" class="ltx_td ltx_align_left">When</td>
<td id="S3.T1.12.1.4.3.5" class="ltx_td ltx_align_right">21</td>
<td id="S3.T1.12.1.4.3.6" class="ltx_td ltx_align_left">End</td>
<td id="S3.T1.12.1.4.3.7" class="ltx_td ltx_align_right">32</td>
<td id="S3.T1.12.1.4.3.8" class="ltx_td"></td>
</tr>
<tr id="S3.T1.12.1.5.4" class="ltx_tr">
<td id="S3.T1.12.1.5.4.1" class="ltx_td"></td>
<td id="S3.T1.12.1.5.4.2" class="ltx_td"></td>
<td id="S3.T1.12.1.5.4.3" class="ltx_td"></td>
<td id="S3.T1.12.1.5.4.4" class="ltx_td ltx_align_left">How many</td>
<td id="S3.T1.12.1.5.4.5" class="ltx_td ltx_align_right">10</td>
<td id="S3.T1.12.1.5.4.6" class="ltx_td ltx_align_left">Several</td>
<td id="S3.T1.12.1.5.4.7" class="ltx_td ltx_align_right">35</td>
<td id="S3.T1.12.1.5.4.8" class="ltx_td"></td>
</tr>
<tr id="S3.T1.12.1.6.5" class="ltx_tr">
<td id="S3.T1.12.1.6.5.1" class="ltx_td ltx_align_left ltx_border_t">Test</td>
<td id="S3.T1.12.1.6.5.2" class="ltx_td ltx_align_right ltx_border_t">8</td>
<td id="S3.T1.12.1.6.5.3" class="ltx_td ltx_align_right ltx_border_t">130</td>
<td id="S3.T1.12.1.6.5.4" class="ltx_td ltx_align_left ltx_border_t">What</td>
<td id="S3.T1.12.1.6.5.5" class="ltx_td ltx_align_right ltx_border_t">57</td>
<td id="S3.T1.12.1.6.5.6" class="ltx_td ltx_align_left ltx_border_t">Begin</td>
<td id="S3.T1.12.1.6.5.7" class="ltx_td ltx_align_right ltx_border_t">43</td>
<td id="S3.T1.12.1.6.5.8" class="ltx_td ltx_align_right ltx_border_t">12,562 [4,779; 17,615]</td>
</tr>
<tr id="S3.T1.12.1.7.6" class="ltx_tr">
<td id="S3.T1.12.1.7.6.1" class="ltx_td"></td>
<td id="S3.T1.12.1.7.6.2" class="ltx_td"></td>
<td id="S3.T1.12.1.7.6.3" class="ltx_td"></td>
<td id="S3.T1.12.1.7.6.4" class="ltx_td ltx_align_left">Who</td>
<td id="S3.T1.12.1.7.6.5" class="ltx_td ltx_align_right">45</td>
<td id="S3.T1.12.1.7.6.6" class="ltx_td ltx_align_left">Middle</td>
<td id="S3.T1.12.1.7.6.7" class="ltx_td ltx_align_right">34</td>
<td id="S3.T1.12.1.7.6.8" class="ltx_td"></td>
</tr>
<tr id="S3.T1.12.1.8.7" class="ltx_tr">
<td id="S3.T1.12.1.8.7.1" class="ltx_td"></td>
<td id="S3.T1.12.1.8.7.2" class="ltx_td"></td>
<td id="S3.T1.12.1.8.7.3" class="ltx_td"></td>
<td id="S3.T1.12.1.8.7.4" class="ltx_td ltx_align_left">When</td>
<td id="S3.T1.12.1.8.7.5" class="ltx_td ltx_align_right">20</td>
<td id="S3.T1.12.1.8.7.6" class="ltx_td ltx_align_left">End</td>
<td id="S3.T1.12.1.8.7.7" class="ltx_td ltx_align_right">22</td>
<td id="S3.T1.12.1.8.7.8" class="ltx_td"></td>
</tr>
<tr id="S3.T1.12.1.9.8" class="ltx_tr">
<td id="S3.T1.12.1.9.8.1" class="ltx_td ltx_border_bb"></td>
<td id="S3.T1.12.1.9.8.2" class="ltx_td ltx_border_bb"></td>
<td id="S3.T1.12.1.9.8.3" class="ltx_td ltx_border_bb"></td>
<td id="S3.T1.12.1.9.8.4" class="ltx_td ltx_align_left ltx_border_bb">How many</td>
<td id="S3.T1.12.1.9.8.5" class="ltx_td ltx_align_right ltx_border_bb">8</td>
<td id="S3.T1.12.1.9.8.6" class="ltx_td ltx_align_left ltx_border_bb">Several</td>
<td id="S3.T1.12.1.9.8.7" class="ltx_td ltx_align_right ltx_border_bb">31</td>
<td id="S3.T1.12.1.9.8.8" class="ltx_td ltx_border_bb"></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental setup</h2>

<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation protocol.</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">The evaluation on ELITR-Bench is conducted as follows. For each meeting, a prompt containing the transcript and detailing the assistant’s task is formed. Then, questions are appended to the initial prompt to drive the conversation about the corresponding meeting. We consider two ways to do this: (i) the <span id="S4.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">single-turn mode</span>, where only a single question is tackled in the conversation (i.e., the prompt is re-initialized for each new question), or (ii) the <span id="S4.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">multi-turn mode</span>, where all the questions related to a meeting are asked successively within a single conversation. Given the stand-alone nature of questions in ELITR-Bench-QA, one can adopt either the single-turn or multi-turn modes for this setting, whereas for ELITR-Bench-Conv it only makes sense to use the multi-turn mode as some questions are inter-dependent. In our evaluation methodology, given a question integrated in the aforementioned prompt, the response generated by an LLM is evaluated automatically using a GPT-4 judge,<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Our GPT-4 judge is based on the gpt-4-0613 checkpoint, for its cheaper cost compared to gpt-4-turbo models. Pilot experiments with different GPT-4 judges led to similar evaluation scores.</span></span></span> following the standard practice in LLM evaluation (as discussed in Section <a href="#S2" title="2 Related work ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Specifically, we adopted a score rubric-based evaluation methodology <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> in which a generated response is evaluated on its proximity to the ground-truth answer, given the associated question and a score rubric that details the quality criteria expected at each score level (ranging from the lowest score of 1 to the perfect score of 10). The prompt used for the evaluation as well as our manually defined score rubric are provided in Appendix <a href="#A5" title="Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a> (Figs. <a href="#A5.F5" title="Figure 5 ‣ Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#A5.F6" title="Figure 6 ‣ Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, respectively). Although our core experiment results rely on automatic LLM-based evaluation (Section <a href="#S5" title="5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), we further confirm the validity of this methodology against human judgement in Section <a href="#S6" title="6 LLM-based evaluation assessment ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Compared models.</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">In our experiments on ELITR-Bench, we compared responses generated by 9 recent LLMs with long-context capabilities. We included both commercial models and open-source long-context models based on LLaMA-2 in our benchmarking:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">GPT-3.5</span> and <span id="S4.I1.i1.p1.1.2" class="ltx_text ltx_font_bold">GPT-4</span> <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> are powerful commercial LLMs from OpenAI that have obtained state-of-the-art performance on a wide range of LLM benchmarks. We used the gpt-3.5-turbo-16k-0613 and gpt-4-1106-preview checkpoints,<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://platform.openai.com/docs/models/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://platform.openai.com/docs/models/</a></span></span></span> which respectively enable a context length of 16K and 128K tokens.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">LongAlpaca-7B</span> and <span id="S4.I1.i2.p1.1.2" class="ltx_text ltx_font_bold">LongAlpaca-13B</span> were obtained by fine-tuning LLaMA-2 models using the LongLoRA technique on the LongAlpaca dataset, both introduced in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Chen et al.</span> [<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>. Their context size limit is 32K.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">LongChat-7B-v1.5</span> is the LLaMa-2 version of the original LongChat-7B model <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, trained on curated conversation data with rotary position embeddings (RoPE) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>. It enables a context of at most 32K tokens.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Vicuna-7B-v1.5</span> and <span id="S4.I1.i4.p1.1.2" class="ltx_text ltx_font_bold">Vicuna-13B-v1.5</span> were obtained by fine-tuning LLaMA-2 on the user-shared ShareGPT conversations, similarly to the original Vicuna model <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>. Their context length is 16K – which we extrapolate to 32K at inference time using RoPE <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>, to enable processing the longer meeting transcripts.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p"><span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">LongAlign-7B</span> and <span id="S4.I1.i5.p1.1.2" class="ltx_text ltx_font_bold">LongAlign-13B</span> are based on the LongAlign recipe <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> by fine-tuning LLaMA-2 models on synthetic long sequences using a compact batching strategy. Their maximum context size is 64K tokens.</p>
</div>
</li>
</ul>
<p id="S4.SS0.SSS0.Px2.p1.2" class="ltx_p">We provide more details on the compared models in Appendix <a href="#A1.SS1" title="A.1 Compared models and hardware details ‣ Appendix A Experimental setup details ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>. Additionally, we described the search conducted to select the best configuration (including inference hyperparameters and prompt formatting) for each model in Appendix <a href="#A1.SS2" title="A.2 Configuration search on ELITR-Bench-QA’s dev set ‣ Appendix A Experimental setup details ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section describes the results of the experiments conducted on ELITR-Bench. In Section <a href="#S5.SS1" title="5.1 Main results ‣ 5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>, we summarize the benchmarking results of the compared models on ELITR-Bench-QA and ELITR-Bench-Conv. Then, in Section <a href="#S5.SS2" title="5.2 Impact of question type and answer position ‣ 5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>, we analyze the impact of the question types and answer positions on the models’ performance. Finally, Section <a href="#S5.SS3" title="5.3 Results on QA/Conv differentiating questions ‣ 5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a> discusses the results for questions that differ between the QA and Conv settings.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Main results</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The main results of the benchmarking on ELITR-Bench are reported in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Main results ‣ 5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The compared models are evaluated in three settings that combine the ELITR-Bench-QA or ELITR-Bench-Conv question set with the single-turn mode (i.e., one question asked per conversation) or multi-turn mode (i.e., all questions related to one meeting asked in a single conversation).<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Single-turn ELITR-Bench-Conv is omitted as some questions in the Conv setting are context-dependent (i.e., rely on previous questions or answers) and thus could not be asked independently.</span></span></span> For each of the three considered settings, we report the results on the dev set, the results on the test set, and their mean. Given the extensive cost of GPT4-based evaluation (detailed in Section <a href="#S4" title="4 Experimental setup ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), we performed a single seeded run for the dev set and three seeded runs for the test set. For the latter, we report the average score over the three runs. In Appendix <a href="#A2.SS1" title="B.1 Variance over seeded run results ‣ Appendix B Additional experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>, we provide more details about the seeded runs as well as their standard deviations.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Looking at the three settings in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Main results ‣ 5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we observe that GPT-4 clearly dominates over all other approaches with an average score that is always above 8.<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>While one might argue that GPT-4 is unfairly advantaged due to the use of a GPT-4 judge, we show in Section <a href="#S6.SS2" title="6.2 Evaluator comparison results ‣ 6 LLM-based evaluation assessment ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a> that the dominance of this model is observed for other evaluators as well.</span></span></span> GPT-3.5 obtained a slightly lower average score  – around 7 – that still outperforms open-source LLMs. Among these, differences are smaller with scores close to 6 on the single-turn setting, and ranging from 4 to 6 on the multi-turn settings. Nonetheless, we can note that Vicuna-13B-v1.5 is the open-source approach that performed the most favorably overall on the three settings. Interestingly, the results in the single-turn and multi-turn modes show large discrepancies for open-source models – even when the question set is exactly the same, for ELITR-Bench-QA. This seems to indicate that open-source long-context LLMs get distracted by the previous questions and answers, which affects their performance. In contrast, GPT-4 is instead able to increase its performance between the single-turn mode and the multi-turn mode. Comparing the results of the QA and Conv settings in the multi-turn mode, we found only minimal differences. This can be explained by the small number of questions that differ between QA and Conv (16 for the dev set and 17 for the test set). In Section <a href="#S5.SS3" title="5.3 Results on QA/Conv differentiating questions ‣ 5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>, we analyze the results on this subset of differentiating questions to better understand the impact of the benchmark setting (QA vs Conv) and inference mode (single-turn vs multi-turn).</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.3.2" class="ltx_text" style="font-size:90%;">Results on different ELITR-Bench settings. The reported numbers correspond to the average scores from 1 to 10 (higher is better) obtained by a GPT-4 evaluator, on a single seeded run for the dev set and 3 seeded runs for the test set. Boldface numbers correspond to the best performance among proprietary or open-source models. The results for GPT-3.5 are omitted in the multi-turn setting as the context length exceeded the 16K limit of this model.</span></figcaption>
<div id="S5.T2.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:349.9pt;height:194.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-19.4pt,10.8pt) scale(0.9,0.9) ;">
<table id="S5.T2.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.4.1.1.1" class="ltx_tr">
<th id="S5.T2.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="3"><span id="S5.T2.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S5.T2.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S5.T2.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Single-turn</span></td>
<td id="S5.T2.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="6"><span id="S5.T2.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Multi-turn</span></td>
</tr>
<tr id="S5.T2.4.1.2.2" class="ltx_tr">
<td id="S5.T2.4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span id="S5.T2.4.1.2.2.1.1" class="ltx_text ltx_font_bold ltx_inline-block" style="width:85.4pt;">ELITR-Bench-QA</span></td>
<td id="S5.T2.4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span id="S5.T2.4.1.2.2.2.1" class="ltx_text ltx_font_bold ltx_inline-block" style="width:85.4pt;">ELITR-Bench-QA</span></td>
<td id="S5.T2.4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span id="S5.T2.4.1.2.2.3.1" class="ltx_text ltx_font_bold ltx_inline-block" style="width:85.4pt;">ELITR-Bench-Conv</span></td>
</tr>
<tr id="S5.T2.4.1.3.3" class="ltx_tr">
<td id="S5.T2.4.1.3.3.1" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.4.1.3.3.1.1" class="ltx_text ltx_font_bold">Dev</span></td>
<td id="S5.T2.4.1.3.3.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.4.1.3.3.2.1" class="ltx_text ltx_font_bold">Test</span></td>
<td id="S5.T2.4.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.4.1.3.3.3.1" class="ltx_text ltx_font_bold">Mean</span></td>
<td id="S5.T2.4.1.3.3.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.4.1.3.3.4.1" class="ltx_text ltx_font_bold">Dev</span></td>
<td id="S5.T2.4.1.3.3.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.4.1.3.3.5.1" class="ltx_text ltx_font_bold">Test</span></td>
<td id="S5.T2.4.1.3.3.6" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.4.1.3.3.6.1" class="ltx_text ltx_font_bold">Mean</span></td>
<td id="S5.T2.4.1.3.3.7" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.4.1.3.3.7.1" class="ltx_text ltx_font_bold">Dev</span></td>
<td id="S5.T2.4.1.3.3.8" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.4.1.3.3.8.1" class="ltx_text ltx_font_bold">Test</span></td>
<td id="S5.T2.4.1.3.3.9" class="ltx_td ltx_align_right ltx_border_t"><span id="S5.T2.4.1.3.3.9.1" class="ltx_text ltx_font_bold">Mean</span></td>
</tr>
<tr id="S5.T2.4.1.4.4" class="ltx_tr">
<th id="S5.T2.4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GPT-3.5</th>
<td id="S5.T2.4.1.4.4.2" class="ltx_td ltx_align_right ltx_border_t">7.04</td>
<td id="S5.T2.4.1.4.4.3" class="ltx_td ltx_align_right ltx_border_t">7.44</td>
<td id="S5.T2.4.1.4.4.4" class="ltx_td ltx_align_right ltx_border_t">7.24</td>
<td id="S5.T2.4.1.4.4.5" class="ltx_td ltx_align_right ltx_border_t">-</td>
<td id="S5.T2.4.1.4.4.6" class="ltx_td ltx_align_right ltx_border_t">-</td>
<td id="S5.T2.4.1.4.4.7" class="ltx_td ltx_align_right ltx_border_t">-</td>
<td id="S5.T2.4.1.4.4.8" class="ltx_td ltx_align_right ltx_border_t">-</td>
<td id="S5.T2.4.1.4.4.9" class="ltx_td ltx_align_right ltx_border_t">-</td>
<td id="S5.T2.4.1.4.4.10" class="ltx_td ltx_align_right ltx_border_t">-</td>
</tr>
<tr id="S5.T2.4.1.5.5" class="ltx_tr">
<th id="S5.T2.4.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-4</th>
<td id="S5.T2.4.1.5.5.2" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.5.5.2.1" class="ltx_text ltx_font_bold">8.21</span></td>
<td id="S5.T2.4.1.5.5.3" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.5.5.3.1" class="ltx_text ltx_font_bold">8.39</span></td>
<td id="S5.T2.4.1.5.5.4" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.5.5.4.1" class="ltx_text ltx_font_bold">8.30</span></td>
<td id="S5.T2.4.1.5.5.5" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.5.5.5.1" class="ltx_text ltx_font_bold">8.53</span></td>
<td id="S5.T2.4.1.5.5.6" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.5.5.6.1" class="ltx_text ltx_font_bold">8.42</span></td>
<td id="S5.T2.4.1.5.5.7" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.5.5.7.1" class="ltx_text ltx_font_bold">8.47</span></td>
<td id="S5.T2.4.1.5.5.8" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.5.5.8.1" class="ltx_text ltx_font_bold">8.53</span></td>
<td id="S5.T2.4.1.5.5.9" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.5.5.9.1" class="ltx_text ltx_font_bold">8.36</span></td>
<td id="S5.T2.4.1.5.5.10" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.5.5.10.1" class="ltx_text ltx_font_bold">8.45</span></td>
</tr>
<tr id="S5.T2.4.1.6.6" class="ltx_tr">
<th id="S5.T2.4.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LongAlpaca-7B</th>
<td id="S5.T2.4.1.6.6.2" class="ltx_td ltx_align_right ltx_border_t">5.89</td>
<td id="S5.T2.4.1.6.6.3" class="ltx_td ltx_align_right ltx_border_t">5.60</td>
<td id="S5.T2.4.1.6.6.4" class="ltx_td ltx_align_right ltx_border_t">5.75</td>
<td id="S5.T2.4.1.6.6.5" class="ltx_td ltx_align_right ltx_border_t">4.53</td>
<td id="S5.T2.4.1.6.6.6" class="ltx_td ltx_align_right ltx_border_t">4.84</td>
<td id="S5.T2.4.1.6.6.7" class="ltx_td ltx_align_right ltx_border_t">4.68</td>
<td id="S5.T2.4.1.6.6.8" class="ltx_td ltx_align_right ltx_border_t">4.70</td>
<td id="S5.T2.4.1.6.6.9" class="ltx_td ltx_align_right ltx_border_t">4.58</td>
<td id="S5.T2.4.1.6.6.10" class="ltx_td ltx_align_right ltx_border_t">4.64</td>
</tr>
<tr id="S5.T2.4.1.7.7" class="ltx_tr">
<th id="S5.T2.4.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LongAlpaca-13B</th>
<td id="S5.T2.4.1.7.7.2" class="ltx_td ltx_align_right">6.17</td>
<td id="S5.T2.4.1.7.7.3" class="ltx_td ltx_align_right">6.25</td>
<td id="S5.T2.4.1.7.7.4" class="ltx_td ltx_align_right">6.21</td>
<td id="S5.T2.4.1.7.7.5" class="ltx_td ltx_align_right">4.76</td>
<td id="S5.T2.4.1.7.7.6" class="ltx_td ltx_align_right">4.71</td>
<td id="S5.T2.4.1.7.7.7" class="ltx_td ltx_align_right">4.73</td>
<td id="S5.T2.4.1.7.7.8" class="ltx_td ltx_align_right">4.74</td>
<td id="S5.T2.4.1.7.7.9" class="ltx_td ltx_align_right">4.74</td>
<td id="S5.T2.4.1.7.7.10" class="ltx_td ltx_align_right">4.74</td>
</tr>
<tr id="S5.T2.4.1.8.8" class="ltx_tr">
<th id="S5.T2.4.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LongChat-7B-v1.5</th>
<td id="S5.T2.4.1.8.8.2" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.8.8.2.1" class="ltx_text ltx_font_bold">6.60</span></td>
<td id="S5.T2.4.1.8.8.3" class="ltx_td ltx_align_right">5.78</td>
<td id="S5.T2.4.1.8.8.4" class="ltx_td ltx_align_right">6.19</td>
<td id="S5.T2.4.1.8.8.5" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.8.8.5.1" class="ltx_text ltx_font_bold">5.85</span></td>
<td id="S5.T2.4.1.8.8.6" class="ltx_td ltx_align_right">4.17</td>
<td id="S5.T2.4.1.8.8.7" class="ltx_td ltx_align_right">5.01</td>
<td id="S5.T2.4.1.8.8.8" class="ltx_td ltx_align_right">5.21</td>
<td id="S5.T2.4.1.8.8.9" class="ltx_td ltx_align_right">4.31</td>
<td id="S5.T2.4.1.8.8.10" class="ltx_td ltx_align_right">4.76</td>
</tr>
<tr id="S5.T2.4.1.9.9" class="ltx_tr">
<th id="S5.T2.4.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vicuna-7B-v1.5</th>
<td id="S5.T2.4.1.9.9.2" class="ltx_td ltx_align_right">5.42</td>
<td id="S5.T2.4.1.9.9.3" class="ltx_td ltx_align_right">5.61</td>
<td id="S5.T2.4.1.9.9.4" class="ltx_td ltx_align_right">5.51</td>
<td id="S5.T2.4.1.9.9.5" class="ltx_td ltx_align_right">4.68</td>
<td id="S5.T2.4.1.9.9.6" class="ltx_td ltx_align_right">4.61</td>
<td id="S5.T2.4.1.9.9.7" class="ltx_td ltx_align_right">4.65</td>
<td id="S5.T2.4.1.9.9.8" class="ltx_td ltx_align_right">4.67</td>
<td id="S5.T2.4.1.9.9.9" class="ltx_td ltx_align_right">4.69</td>
<td id="S5.T2.4.1.9.9.10" class="ltx_td ltx_align_right">4.68</td>
</tr>
<tr id="S5.T2.4.1.10.10" class="ltx_tr">
<th id="S5.T2.4.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vicuna-13B-v1.5</th>
<td id="S5.T2.4.1.10.10.2" class="ltx_td ltx_align_right">5.92</td>
<td id="S5.T2.4.1.10.10.3" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.10.10.3.1" class="ltx_text ltx_font_bold">6.52</span></td>
<td id="S5.T2.4.1.10.10.4" class="ltx_td ltx_align_right">6.22</td>
<td id="S5.T2.4.1.10.10.5" class="ltx_td ltx_align_right">5.52</td>
<td id="S5.T2.4.1.10.10.6" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.10.10.6.1" class="ltx_text ltx_font_bold">5.67</span></td>
<td id="S5.T2.4.1.10.10.7" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.10.10.7.1" class="ltx_text ltx_font_bold">5.60</span></td>
<td id="S5.T2.4.1.10.10.8" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.10.10.8.1" class="ltx_text ltx_font_bold">5.42</span></td>
<td id="S5.T2.4.1.10.10.9" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.10.10.9.1" class="ltx_text ltx_font_bold">5.78</span></td>
<td id="S5.T2.4.1.10.10.10" class="ltx_td ltx_align_right"><span id="S5.T2.4.1.10.10.10.1" class="ltx_text ltx_font_bold">5.60</span></td>
</tr>
<tr id="S5.T2.4.1.11.11" class="ltx_tr">
<th id="S5.T2.4.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LongAlign-7B</th>
<td id="S5.T2.4.1.11.11.2" class="ltx_td ltx_align_right">6.11</td>
<td id="S5.T2.4.1.11.11.3" class="ltx_td ltx_align_right">6.46</td>
<td id="S5.T2.4.1.11.11.4" class="ltx_td ltx_align_right">6.28</td>
<td id="S5.T2.4.1.11.11.5" class="ltx_td ltx_align_right">5.43</td>
<td id="S5.T2.4.1.11.11.6" class="ltx_td ltx_align_right">4.47</td>
<td id="S5.T2.4.1.11.11.7" class="ltx_td ltx_align_right">4.95</td>
<td id="S5.T2.4.1.11.11.8" class="ltx_td ltx_align_right">5.04</td>
<td id="S5.T2.4.1.11.11.9" class="ltx_td ltx_align_right">5.06</td>
<td id="S5.T2.4.1.11.11.10" class="ltx_td ltx_align_right">5.05</td>
</tr>
<tr id="S5.T2.4.1.12.12" class="ltx_tr">
<th id="S5.T2.4.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">LongAlign-13B</th>
<td id="S5.T2.4.1.12.12.2" class="ltx_td ltx_align_right ltx_border_bb">6.27</td>
<td id="S5.T2.4.1.12.12.3" class="ltx_td ltx_align_right ltx_border_bb">6.33</td>
<td id="S5.T2.4.1.12.12.4" class="ltx_td ltx_align_right ltx_border_bb"><span id="S5.T2.4.1.12.12.4.1" class="ltx_text ltx_font_bold">6.30</span></td>
<td id="S5.T2.4.1.12.12.5" class="ltx_td ltx_align_right ltx_border_bb">4.65</td>
<td id="S5.T2.4.1.12.12.6" class="ltx_td ltx_align_right ltx_border_bb">5.33</td>
<td id="S5.T2.4.1.12.12.7" class="ltx_td ltx_align_right ltx_border_bb">4.99</td>
<td id="S5.T2.4.1.12.12.8" class="ltx_td ltx_align_right ltx_border_bb">4.81</td>
<td id="S5.T2.4.1.12.12.9" class="ltx_td ltx_align_right ltx_border_bb">4.95</td>
<td id="S5.T2.4.1.12.12.10" class="ltx_td ltx_align_right ltx_border_bb">4.88</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Impact of question type and answer position</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">As each question in ELITR-Bench is characterized by its type (<span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">Who</span>, <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">What</span>, <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_italic">When</span>, <span id="S5.SS2.p1.1.4" class="ltx_text ltx_font_italic">How many</span>) and answer location (<span id="S5.SS2.p1.1.5" class="ltx_text ltx_font_italic">Beginning</span>, <span id="S5.SS2.p1.1.6" class="ltx_text ltx_font_italic">Middle</span>, <span id="S5.SS2.p1.1.7" class="ltx_text ltx_font_italic">End</span>, <span id="S5.SS2.p1.1.8" class="ltx_text ltx_font_italic">Several</span>), we sought to identify whether these impact the models’ ability to answer correctly. We show in Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Impact of question type and answer position ‣ 5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> the results restricted to each question type and answer location, obtained on the test set of ELITR-Bench-QA in single-turn mode. Due to space limitations, we aggregate the results by family of models (GPT models or LLaMA-2 models) to look for general trends among comparable models. The detailed, per-model results are available in Appendix <a href="#A2.SS2" title="B.2 Additional results on question type and answer position ‣ Appendix B Additional experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a> (Table <a href="#A2.T9" title="Table 9 ‣ B.2 Additional results on question type and answer position ‣ Appendix B Additional experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>).</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Looking at the question type results, we find that for both model families the <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_italic">Who</span> questions are the easiest to answer. In contrast, the <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_italic">What</span> questions were the most challenging for GPT models and the second most challenging for LLaMA-2 models. This is not surprising as <span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_italic">What</span> questions sometimes require complex answers that go beyond simply listing entities, dates or numbers. Interestingly, LLaMA-2 models struggled the most with the <span id="S5.SS2.p2.1.4" class="ltx_text ltx_font_italic">How many</span> questions. Although the amount of such questions is very limited (8 in the test set) which calls for caution on tentative interpretations, this seems to suggest that LLaMA-2 models are notably less proficient at dealing with quantities and numbers than GPT models.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">In contrast, the results by answer location in Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Impact of question type and answer position ‣ 5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> do not seem to show any general trend. In particular, we do not notice at first glance any “lost in the middle” effect <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> which posits that information located in the middle section of long contexts is harder to access for LLMs. To further verify this, we conducted a one-tailed Welch’s t-test <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> for each model to investigate the hypothesis stating that the model’s average score for questions with middle-position answers is lower than that of other questions. We found that this hypothesis is only verified for two models: LongChat-7B-v1.5 (p-value = 0.032) and Vicuna-7B-v1.5 (p-value = 0.046) – the full results are available in Appendix <a href="#A2.SS2" title="B.2 Additional results on question type and answer position ‣ Appendix B Additional experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a> (Table <a href="#A2.T10" title="Table 10 ‣ B.2 Additional results on question type and answer position ‣ Appendix B Additional experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). This suggests that all models may not be affected in the same way by the location of information in the context.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.3.2" class="ltx_text" style="font-size:90%;">Results by question type and answer location for the GPT family (2 models) and the LLaMA-2 family (7 models) on the test set of ELITR-Bench-QA in single-turn mode. The number N below a subset indicates the corresponding subset size.</span></figcaption>
<div id="S5.T3.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:431.9pt;height:86.8pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.0pt,4.8pt) scale(0.9,0.9) ;">
<table id="S5.T3.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.4.1.1.1" class="ltx_tr">
<th id="S5.T3.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S5.T3.4.1.1.1.1.1" class="ltx_text">
<span id="S5.T3.4.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.4.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T3.4.1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T3.4.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></span></span>
<span id="S5.T3.4.1.1.1.1.1.1.2" class="ltx_tr">
<span id="S5.T3.4.1.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T3.4.1.1.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">family</span></span></span>
</span></span></th>
<th id="S5.T3.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S5.T3.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Question type</span></th>
<th id="S5.T3.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S5.T3.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Answer location</span></th>
</tr>
<tr id="S5.T3.4.1.2.2" class="ltx_tr">
<th id="S5.T3.4.1.2.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">
<table id="S5.T3.4.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.4.1.2.2.1.1.1" class="ltx_tr">
<td id="S5.T3.4.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Who</span></td>
</tr>
<tr id="S5.T3.4.1.2.2.1.1.2" class="ltx_tr">
<td id="S5.T3.4.1.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.1.1.2.1.1" class="ltx_text ltx_font_bold">(N=45)</span></td>
</tr>
</table>
</th>
<th id="S5.T3.4.1.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">
<table id="S5.T3.4.1.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.4.1.2.2.2.1.1" class="ltx_tr">
<td id="S5.T3.4.1.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">What</span></td>
</tr>
<tr id="S5.T3.4.1.2.2.2.1.2" class="ltx_tr">
<td id="S5.T3.4.1.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.2.1.2.1.1" class="ltx_text ltx_font_bold">(N=57)</span></td>
</tr>
</table>
</th>
<th id="S5.T3.4.1.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">
<table id="S5.T3.4.1.2.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.4.1.2.2.3.1.1" class="ltx_tr">
<td id="S5.T3.4.1.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.3.1.1.1.1" class="ltx_text ltx_font_bold">When</span></td>
</tr>
<tr id="S5.T3.4.1.2.2.3.1.2" class="ltx_tr">
<td id="S5.T3.4.1.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.3.1.2.1.1" class="ltx_text ltx_font_bold">(N=20)</span></td>
</tr>
</table>
</th>
<th id="S5.T3.4.1.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">
<table id="S5.T3.4.1.2.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.4.1.2.2.4.1.1" class="ltx_tr">
<td id="S5.T3.4.1.2.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.4.1.1.1.1" class="ltx_text ltx_font_bold">How many</span></td>
</tr>
<tr id="S5.T3.4.1.2.2.4.1.2" class="ltx_tr">
<td id="S5.T3.4.1.2.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.4.1.2.1.1" class="ltx_text ltx_font_bold">(N=8)</span></td>
</tr>
</table>
</th>
<th id="S5.T3.4.1.2.2.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">
<table id="S5.T3.4.1.2.2.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.4.1.2.2.5.1.1" class="ltx_tr">
<td id="S5.T3.4.1.2.2.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.5.1.1.1.1" class="ltx_text ltx_font_bold">Begin</span></td>
</tr>
<tr id="S5.T3.4.1.2.2.5.1.2" class="ltx_tr">
<td id="S5.T3.4.1.2.2.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.5.1.2.1.1" class="ltx_text ltx_font_bold">(N=43)</span></td>
</tr>
</table>
</th>
<th id="S5.T3.4.1.2.2.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">
<table id="S5.T3.4.1.2.2.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.4.1.2.2.6.1.1" class="ltx_tr">
<td id="S5.T3.4.1.2.2.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.6.1.1.1.1" class="ltx_text ltx_font_bold">Middle</span></td>
</tr>
<tr id="S5.T3.4.1.2.2.6.1.2" class="ltx_tr">
<td id="S5.T3.4.1.2.2.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.6.1.2.1.1" class="ltx_text ltx_font_bold">(N=34)</span></td>
</tr>
</table>
</th>
<th id="S5.T3.4.1.2.2.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">
<table id="S5.T3.4.1.2.2.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.4.1.2.2.7.1.1" class="ltx_tr">
<td id="S5.T3.4.1.2.2.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.7.1.1.1.1" class="ltx_text ltx_font_bold">End</span></td>
</tr>
<tr id="S5.T3.4.1.2.2.7.1.2" class="ltx_tr">
<td id="S5.T3.4.1.2.2.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.7.1.2.1.1" class="ltx_text ltx_font_bold">(N=22)</span></td>
</tr>
</table>
</th>
<th id="S5.T3.4.1.2.2.8" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">
<table id="S5.T3.4.1.2.2.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.4.1.2.2.8.1.1" class="ltx_tr">
<td id="S5.T3.4.1.2.2.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.8.1.1.1.1" class="ltx_text ltx_font_bold">Several</span></td>
</tr>
<tr id="S5.T3.4.1.2.2.8.1.2" class="ltx_tr">
<td id="S5.T3.4.1.2.2.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="S5.T3.4.1.2.2.8.1.2.1.1" class="ltx_text ltx_font_bold">(N=31)</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.4.1.3.1" class="ltx_tr">
<th id="S5.T3.4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GPT</th>
<td id="S5.T3.4.1.3.1.2" class="ltx_td ltx_align_right ltx_border_t">8.24</td>
<td id="S5.T3.4.1.3.1.3" class="ltx_td ltx_align_right ltx_border_t">7.62</td>
<td id="S5.T3.4.1.3.1.4" class="ltx_td ltx_align_right ltx_border_t">7.98</td>
<td id="S5.T3.4.1.3.1.5" class="ltx_td ltx_align_right ltx_border_t">8.04</td>
<td id="S5.T3.4.1.3.1.6" class="ltx_td ltx_align_right ltx_border_t">7.85</td>
<td id="S5.T3.4.1.3.1.7" class="ltx_td ltx_align_right ltx_border_t">7.87</td>
<td id="S5.T3.4.1.3.1.8" class="ltx_td ltx_align_right ltx_border_t">8.04</td>
<td id="S5.T3.4.1.3.1.9" class="ltx_td ltx_align_right ltx_border_t">7.97</td>
</tr>
<tr id="S5.T3.4.1.4.2" class="ltx_tr">
<th id="S5.T3.4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">LLaMA-2</th>
<td id="S5.T3.4.1.4.2.2" class="ltx_td ltx_align_right ltx_border_bb">6.50</td>
<td id="S5.T3.4.1.4.2.3" class="ltx_td ltx_align_right ltx_border_bb">5.88</td>
<td id="S5.T3.4.1.4.2.4" class="ltx_td ltx_align_right ltx_border_bb">6.00</td>
<td id="S5.T3.4.1.4.2.5" class="ltx_td ltx_align_right ltx_border_bb">5.29</td>
<td id="S5.T3.4.1.4.2.6" class="ltx_td ltx_align_right ltx_border_bb">6.31</td>
<td id="S5.T3.4.1.4.2.7" class="ltx_td ltx_align_right ltx_border_bb">5.84</td>
<td id="S5.T3.4.1.4.2.8" class="ltx_td ltx_align_right ltx_border_bb">6.00</td>
<td id="S5.T3.4.1.4.2.9" class="ltx_td ltx_align_right ltx_border_bb">6.07</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Results on QA/Conv differentiating questions</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">As introduced in Section <a href="#S3" title="3 ELITR-Bench ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, some of the questions differ between ELITR-Bench-QA and ELITR-Bench-Conv and typically contain pronominal references or ellipses in the Conv setting, which makes them particularly challenging to tackle. In this section, we look at the results on this subset of differentiating questions – both in their QA and Conv versions – and study the impact of using the single-turn or multi-turn mode. The results are provided in Fig. <a href="#S5.F1" title="Figure 1 ‣ 5.3 Results on QA/Conv differentiating questions ‣ 5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, which compares 3 settings: single-turn mode with QA questions, multi-turn mode with QA questions, and multi-turn mode with Conv questions. The reported scores are averaged over the dev and test sets’ differentiating questions (respectively, 16 and 17 questions) to make up for the limited size of these subsets.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Similarly with what we observed in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Main results ‣ 5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we notice again a clear difference between GPT-4 and open-source models: the performance of the former improves (slightly) from single-turn to multi-turn, whereas the performance of the latter notably degrades. In contrast with our previous findings that showed little to no difference between the results on ELITR-Bench-QA and ELITR-Bench-Conv for the multi-turn mode, we observe this time that the average score decreases from QA to Conv for open-source models. While the difference is small, this trend is present for all open-source models except LongChat-7B-v1.5. This trend was expected as the Conv questions in this subset are more challenging to answer. However, interestingly, GPT-4 results did not show the same trend. We hypothesize that the opposite trends identified for GPT-4 and open-source models might be explained by a ‘snowballing’ effect that causes an error propagation in lower-performing open-source models and instead provides additional helpful context for GPT-4.</p>
</div>
<figure id="S5.F1" class="ltx_figure"><img src="/html/2403.20262/assets/x1.png" id="S5.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="141" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S5.F1.3.2" class="ltx_text" style="font-size:90%;">Results restricted to QA/Conv differentiating questions. The score reported for each model and evaluation setting corresponds to the average of the scores obtained on the dev subset (16 questions) and the test subset (17 questions). Best viewed in color.</span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>LLM-based evaluation assessment</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we seek to verify the validity of the LLM-based (namely, GPT-4-based) evaluation methodology introduced in Section <a href="#S4" title="4 Experimental setup ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and applied in Section <a href="#S5" title="5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. In Section <a href="#S6.SS1" title="6.1 Compared evaluators ‣ 6 LLM-based evaluation assessment ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>, we define the LLM-based and human-based evaluators that we considered for comparison. The details of the crowdsourcing study we conducted to collect human score annotations are provided in Appendix <a href="#A3" title="Appendix C Crowdsourcing study details ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>. Then, Section <a href="#S6.SS2" title="6.2 Evaluator comparison results ‣ 6 LLM-based evaluation assessment ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a> presents our results and findings on the evaluator comparison.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Compared evaluators</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Our evaluation assessment experiment consists in checking the validity of the numeric scores (from 1 to 10) assigned for each tuple composed of a question, its ground-truth answer, and an LLM response to evaluate. For that purpose, we compared the score annotations obtained through two LLM-based evaluators and two human-based evaluators:</p>
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p"><span id="S6.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">GPT-4</span> <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>: This evaluator corresponds to the one detailed in the evaluation protocol in Section <a href="#S4" title="4 Experimental setup ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and is based on the gpt-4-0613 model.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p"><span id="S6.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Prometheus</span> <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>: This fine-tuned model was originally proposed to provide an open-source alternative to using GPT-4 for score rubric-based evaluation. We used the Prometheus-13B-v1.0<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a target="_blank" href="https://huggingface.co/kaist-ai/prometheus-13b-v1.0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/kaist-ai/prometheus-13b-v1.0</a></span></span></span> model, with a prompt similar to the one adopted for GPT-4 – the only difference is that the score rubric is re-scaled to a 1-5 range to fit Prometheus’ expected format and multiplied by 2 in post-processing to be comparable to other scores. The prompt is available in Appendix <a href="#A5" title="Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a> (Figs. <a href="#A5.F7" title="Figure 7 ‣ Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#A5.F8" title="Figure 8 ‣ Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>).</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p"><span id="S6.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Gold Human</span>: This expert human annotation was done by one of the authors. The scores were assigned following the same 10-point score rubric as the one used for the GPT-4 evaluator (given in Appendix <a href="#A5" title="Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>, Fig. <a href="#A5.F6" title="Figure 6 ‣ Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), to enforce consistency across questions.</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p"><span id="S6.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Silver Human</span>: This evaluator is based on a crowdsourcing study with the Prolific<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a target="_blank" href="https://www.prolific.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.prolific.com/</a></span></span></span> platform where we averaged the scores assigned by 10 human annotators for each question. The annotators were provided with the same score rubric as for GPT-4 and Gold Human. We give more details on this evaluation in Appendix <a href="#A3" title="Appendix C Crowdsourcing study details ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</li>
</ul>
<p id="S6.SS1.p1.2" class="ltx_p">Given the costly nature of human annotations, we performed our evaluation assessment on a small subset of the experiments described in Section <a href="#S5.SS1" title="5.1 Main results ‣ 5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>. We specifically focused on the results of ELITR-Bench-QA’s test set in the single-turn mode. We looked at the results of 3 models that performed diversely in this setting: the best proprietary model (GPT-4), the best open-source model (Vicuna-13B-v1.5), and the worst open-source model (LongAlpaca-7B).</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Evaluator comparison results</h3>

<figure id="S6.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S6.T4.3.2" class="ltx_text" style="font-size:90%;">Comparison of the scores obtained by different evaluators for the responses generated by GPT-4, Vicuna-13B-v1.5, or LongAlpaca-7B. The evaluation was performed on ELITR-Bench-QA’s test set in the single-turn mode, and for a single seeded run.</span></figcaption>
<div id="S6.T4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.9pt;height:81pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.9pt,4.5pt) scale(0.9,0.9) ;">
<table id="S6.T4.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T4.4.1.1.1" class="ltx_tr">
<th id="S6.T4.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S6.T4.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S6.T4.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S6.T4.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Evaluator</span></th>
</tr>
<tr id="S6.T4.4.1.2.2" class="ltx_tr">
<th id="S6.T4.4.1.2.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t"><span id="S6.T4.4.1.2.2.1.1" class="ltx_text ltx_font_bold">GPT-4</span></th>
<th id="S6.T4.4.1.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t"><span id="S6.T4.4.1.2.2.2.1" class="ltx_text ltx_font_bold">Prometheus</span></th>
<th id="S6.T4.4.1.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t"><span id="S6.T4.4.1.2.2.3.1" class="ltx_text ltx_font_bold">Gold Human</span></th>
<th id="S6.T4.4.1.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t"><span id="S6.T4.4.1.2.2.4.1" class="ltx_text ltx_font_bold">Silver Human</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T4.4.1.3.1" class="ltx_tr">
<th id="S6.T4.4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GPT-4</th>
<td id="S6.T4.4.1.3.1.2" class="ltx_td ltx_align_right ltx_border_t">8.33</td>
<td id="S6.T4.4.1.3.1.3" class="ltx_td ltx_align_right ltx_border_t">5.68</td>
<td id="S6.T4.4.1.3.1.4" class="ltx_td ltx_align_right ltx_border_t">7.93</td>
<td id="S6.T4.4.1.3.1.5" class="ltx_td ltx_align_right ltx_border_t">7.21</td>
</tr>
<tr id="S6.T4.4.1.4.2" class="ltx_tr">
<th id="S6.T4.4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vicuna-13B-v1.5</th>
<td id="S6.T4.4.1.4.2.2" class="ltx_td ltx_align_right">6.69</td>
<td id="S6.T4.4.1.4.2.3" class="ltx_td ltx_align_right">4.80</td>
<td id="S6.T4.4.1.4.2.4" class="ltx_td ltx_align_right">6.19</td>
<td id="S6.T4.4.1.4.2.5" class="ltx_td ltx_align_right">5.80</td>
</tr>
<tr id="S6.T4.4.1.5.3" class="ltx_tr">
<th id="S6.T4.4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">LongAlpaca-7B</th>
<td id="S6.T4.4.1.5.3.2" class="ltx_td ltx_align_right ltx_border_bb">5.57</td>
<td id="S6.T4.4.1.5.3.3" class="ltx_td ltx_align_right ltx_border_bb">4.46</td>
<td id="S6.T4.4.1.5.3.4" class="ltx_td ltx_align_right ltx_border_bb">4.55</td>
<td id="S6.T4.4.1.5.3.5" class="ltx_td ltx_align_right ltx_border_bb">4.72</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S6.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.20262/assets/x2.png" id="S6.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="248" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S6.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.20262/assets/x3.png" id="S6.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F2.4.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S6.F2.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">(a)<span id="S6.F2.5.2.1" class="ltx_text ltx_font_medium"> Distribution of GPT-4 and Silver Human scores with respect to each Gold Human score bin (1-10); the N below a score bin indicates the bin size. </span>(b)<span id="S6.F2.5.2.2" class="ltx_text ltx_font_medium"> Pearson correlation between evaluators.</span></span></figcaption>
</figure>
<section id="S6.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Model-level comparison.</h4>

<div id="S6.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS2.SSS0.Px1.p1.1" class="ltx_p">To get a high-level, coarse-grained comparison of the different evaluators introduced above, we applied each of them to the responses generated by GPT-4, Vicuna-13B-v1.5, and LongAlpaca-7B. The results of the corresponding evaluations are presented in Table <a href="#S6.T4" title="Table 4 ‣ 6.2 Evaluator comparison results ‣ 6 LLM-based evaluation assessment ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We can first observe that the ranking of the three models to evaluate is the same for all the evaluators: GPT-4, Vicuna-13B-v1.5, and LongAlpaca-7B (from the most highly rated model to the most poorly rated one). However, we found that the range of scores was more diverse: Prometheus’ scores were overall fairly low (from 4 to 6), while GPT-4’s scores are much higher (from 5 to 9). In comparison, the human scores from the Gold Human and Silver Human evaluators were more similar to GPT-4 with scores between 4 and 8.</p>
</div>
</section>
<section id="S6.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Correlation analysis.</h4>

<div id="S6.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS2.SSS0.Px2.p1.1" class="ltx_p">To get a deeper understanding of how evaluators compare to one another, we calculated the Pearson correlation for every evaluator pair on the responses aggregated over the 8 meetings of the test set and generated by the three retained models. The results are displayed in Fig <a href="#S6.F2.sf2" title="In Figure 2 ‣ 6.2 Evaluator comparison results ‣ 6 LLM-based evaluation assessment ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>. GPT-4 shows a strong correlation with the two human-based evaluators (0.82 with Gold Human and 0.78 with Silver Human), which is in agreement with the findings from previous studies on GPT-4 judges <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. Prometheus, on the other hand, yielded a weak correlation (between 0.2 and 0.3) with all the other evaluators. We hypothesize that this could be due to a domain shift with respect to what Prometheus was fine-tuned on, caused by the nature of the meeting-related questions and the presence of anonymized entities (e.g., [PERSON3]). Turning to the two human-based evaluators, Gold Human and Silver Human obtained a very strong correlation of 0.89 which confirms the validity of the crowdsourcing study and the feasibility of the annotation task by non-expert judges.</p>
</div>
</section>
<section id="S6.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparison of score distribution across evaluators.</h4>

<div id="S6.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS2.SSS0.Px3.p1.1" class="ltx_p">So far in this section, we have found that GPT-4 and human-based evaluators lead to scores that are highly correlated (see Figure <a href="#S6.F2.sf2" title="In Figure 2 ‣ 6.2 Evaluator comparison results ‣ 6 LLM-based evaluation assessment ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>) but with slightly different score ranges (see Table <a href="#S6.T4" title="Table 4 ‣ 6.2 Evaluator comparison results ‣ 6 LLM-based evaluation assessment ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). This led us to investigate how scores are distributed for different evaluators, and to study to what extent score levels match across evaluators. For that purpose, we considered the pool of (question, response, score) tuples obtained with the Gold Human evaluator on the responses from GPT-4, Vicuna-13B-v1.5, and LongAlpaca-7B for the 130 questions of the test set, i.e., 390 instances in total. We split these instances into 10 bins based on their score value from 1 to 10. Then, for all the instances in a bin, we check the distribution of the scores obtained by other evaluators on the bin’s (question, response) pairs. In practical terms, we seek to highlight through this procedure how Gold Human and alternative evaluators align at the grade level. The results are plotted in Fig. <a href="#S6.F2.sf1" title="In Figure 2 ‣ 6.2 Evaluator comparison results ‣ 6 LLM-based evaluation assessment ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a> where we describe the score distribution of the alternative evaluator through its means and 95% confidence intervals. Interestingly, we observe that the scores for the GPT-4 evaluator seem to fall into 3 distinct clusters, corresponding respectively to the intervals [1, 2], [3, 5] and [6, 10] in the Gold Human scores. This suggests that despite the use of a 10-point score rubric to align the GPT-4 evaluator’s scores with detailed desiderata, this evaluator is only able to distinguish between three levels of response quality. This finding then leads us to question the common practice of using LLM-based evaluator scores on a 5-point or 10-point scale. In contrast, the scores from Silver Human show a more linear relationship with the Gold Human scores, suggesting that implementing the 10-point score rubric in the crowdsourcing study aided in achieving a closer alignment between external human annotators and the evaluation criteria set by the organizers.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This paper introduced ELITR-Bench, a new benchmark for long-context LLMs focused on the meeting assistant task. We augmented the meeting transcripts from the existing ELITR corpus with 271 manually crafted questions and their respective ground-truth answers. Our experiments showcase the performance of recent long-context LLMs on ELITR-Bench, highlighting a gap between proprietary OpenAI models and LLaMA-2-based long-context models – in particular when dealing with multi-turn conversations. We validated our evaluation methodology based on a GPT-4 judge through its comparison against a Prometheus-based evaluator, as well as an expert human evaluator and a crowdsourcing-based evaluator. We demonstrate that the GPT-4 judge displays good correlation with human judgments, but a deeper investigation also reveals that it is unable to provide a very fine-grained evaluation on a 10-point scale, contrary to non-expert humans recruited on a crowdsourcing platform.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">As future work, we are planning to extend ELITR-Bench for the evaluation of retrieval augmented generation (RAG) models. For instance, we could split each transcript into a set of short passages (containing a few utterances) and annotate the relevant passage(s) for each answer. Then, for each question, RAG models would need to first identify the relevant passage(s) and generate the response to the question based on those.
Further studying the impact of de-identification (e.g., named entity anonymization) on QA performance is another interesting direction that could be investigated by reintroducing fake, randomly generated names into the meeting transcripts. In particular, we expect that this might have an impact on <span id="S7.p2.1.1" class="ltx_text ltx_font_italic">Who</span> questions, which heavily depend on correctly generating anonymized entities that often have an important character overlap (e.g., [PERSON1] and [PERSON11]).</p>
</div>
</section>
<section id="Sx1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This paper was partially funded by the European Commission through the UTTER project under grant number 101070631.</p>
</div>
</section>
<section id="Sx2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Ethics statement</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The data collection and evaluation process rigorously adhered to the guidelines established by the UTTER EU project. In accordance with EU project policies, we regularly report to an ethics panel, with the most recent Ethical Review meeting held on November 2nd, 2023.</p>
</div>
<div id="Sx2.p2" class="ltx_para">
<p id="Sx2.p2.1" class="ltx_p">Notably, for the human evaluation of LLMs, we chose Prolific, an alternative crowdsourcing platform tailored for academic research. We meticulously followed Prolific’s guidelines for human experiments, deviating only in terms of compensation for human labelers. While Prolific sets a minimum compensation of $6.50 per hour, we offered a significantly higher rate of £9 per hour (equivalent to $11.5 per hour).</p>
</div>
</section>
<section id="Sx3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Reproducibility statement</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">The complete set of ELITR-Bench (question, ground-truth answer) pairs, along with metadata, is provided in JSON format at <a target="_blank" href="https://github.com/utter-project/UTTER-MS9-meetingdata/tree/master/ELITR-Bench" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/utter-project/UTTER-MS9-meetingdata/tree/master/ELITR-Bench</a>. We also indicate for each question the responses generated by the different long-context LLMs considered in this paper, as well as the evaluation score attributed by the GPT-4 judge and other studied evaluators.</p>
</div>
<div id="Sx3.p2" class="ltx_para">
<p id="Sx3.p2.1" class="ltx_p">Additionally, the code for the response generation and for the evaluation will be released to enable reproducibility of the paper’s results and to foster future benchmarking efforts on ELITR-Bench.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">An et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">L-eval: Instituting standardized evaluation for long context language models, 2023.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Bai et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Longbench: A bilingual, multitask benchmark for long context understanding, 2023.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Bai et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Longalign: A recipe for long context alignment of large language models, 2024.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Beltagy et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Longformer: The long-document transformer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib4.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib4.10.2" class="ltx_text" style="font-size:90%;">, abs/2004.05150, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Bulatov et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Aydar Bulatov, Yuri Kuratov, Yermek Kapushev, and Mikhail S. Burtsev.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Scaling transformer to 1m tokens and beyond with rmt, 2024.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Extending context window of large language models via positional interpolation, 2023.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Longlora: Efficient fine-tuning of long-context large language models, 2024.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Chevalier et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Adapting language models to compress contexts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib8.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em><span id="bib.bib8.11.3" class="ltx_text" style="font-size:90%;">, pages 3829–3846, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Chiang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://lmsys.org/blog/2023-03-30-vicuna/</a><span id="bib.bib9.9.1" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Child et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Generating long sequences with sparse transformers.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;">, abs/1904.10509, 2019.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Choromanski et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Rethinking attention with performers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib11.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span id="bib.bib11.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Dai et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Transformer-XL: Attentive language models beyond a fixed-length context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib12.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em><span id="bib.bib12.11.3" class="ltx_text" style="font-size:90%;">, pages 2978–2988, Florence, Italy, 2019. Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Dong et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models, 2024.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.4.4.1" class="ltx_text" style="font-size:90%;">Gu and Dao [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">
Albert Gu and Tri Dao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">Mamba: Linear-time sequence modeling with selective state spaces, 2023.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Zeyu He, Chieh-Yang Huang, Chien-Kuang Cornelia Ding, Shaurya Rohatgi, and Ting-Hao’Kenneth’ Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">If in a crowdsourced data annotation pipeline, a gpt-4.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib15.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.16795</em><span id="bib.bib15.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Katharopoulos et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Transformers are rnns: Fast autoregressive transformers with linear attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib16.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 37th International Conference on Machine Learning</em><span id="bib.bib16.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Khandve et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Snehal Ishwar Khandve, Vedangi Wagh, Apurva Wani, Isha Joshi, and Raviraj Joshi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Hierarchical neural network approaches for long document classification.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib17.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib17.10.2" class="ltx_text" style="font-size:90%;">, abs/2201.06774, 2022.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Kim et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Prometheus: Inducing evaluation capability in language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Kočiský et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">The NarrativeQA reading comprehension challenge.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Transactions of the Association for Computational Linguistics</em><span id="bib.bib19.10.2" class="ltx_text" style="font-size:90%;">, 6:317–328, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.4.4.1" class="ltx_text" style="font-size:90%;">Koo and Li [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text" style="font-size:90%;">
Terry K Koo and Mae Y Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">A guideline of selecting and reporting intraclass correlation coefficients for reliability research.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib20.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of chiropractic medicine</em><span id="bib.bib20.9.2" class="ltx_text" style="font-size:90%;">, 15(2):155–163, 2016.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Levy et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Mosh Levy, Alon Jacoby, and Yoav Goldberg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Same task, more tokens: the impact of input length on the reasoning performance of large language models, 2024.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">How long can open-source llms truly promise on context length?
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://lmsys.org/blog/2023-06-29-longchat/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://lmsys.org/blog/2023-06-29-longchat/</a><span id="bib.bib22.9.1" class="ltx_text" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Loogle: Can long-context language models understand long contexts?, 2023b.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Hao Liu, Matei Zaharia, and Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Ring attention with blockwise transformers for near-infinite context, 2023a.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Lost in the middle: How language models use long contexts, 2023b.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Yang Liu, Jiaxiang Liu, Li Chen, Yuxiang Lu, Shikun Feng, Zhida Feng, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Ernie-sparse: Learning hierarchical efficient transformer through regularized self-attention, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Maharana et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Evaluating very long-term conversational memory of llm agents, 2024.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Martins et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
André Martins, António Farinhas, Marcos Treviso, Vlad Niculae, Pedro Aguiar, and Mario Figueiredo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Sparse and continuous attention mechanisms.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib28.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib28.11.3" class="ltx_text" style="font-size:90%;">, pages 20989–21001. Curran Associates, Inc., 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Nedoluzhko et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Anna Nedoluzhko, Muskaan Singh, Marie Hledíková, Tirthankar Ghosal, and Ondřej Bojar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">ELITR minuting corpus: A novel dataset for automatic minuting from multi-party meetings in English and Czech.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the Thirteenth Language Resources and Evaluation Conference</em><span id="bib.bib29.11.3" class="ltx_text" style="font-size:90%;">, pages 3174–3182, Marseille, France, 2022. European Language Resources Association.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.4.4.1" class="ltx_text" style="font-size:90%;">OpenAI [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.6.1" class="ltx_text" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">GPT-4 Technical Report.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">pages 1–100, 2023.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Pal et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Giraffe: Adventures in expanding context lengths in llms, 2023.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Peng et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">RWKV: Reinventing RNNs for the transformer era.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib32.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Findings of the Association for Computational Linguistics: EMNLP 2023</em><span id="bib.bib32.11.3" class="ltx_text" style="font-size:90%;">, pages 14048–14077, Singapore, 2023. Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Peng et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">YaRN: Efficient context window extension of large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib33.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span id="bib.bib33.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Su et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Roformer: Enhanced transformer with rotary position embedding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib34.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neurocomputing</em><span id="bib.bib34.10.2" class="ltx_text" style="font-size:90%;">, 568:127063, 2024.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Tay et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Long range arena : A benchmark for efficient transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib35.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span id="bib.bib35.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, and Alexander M Rush.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">Mambabyte: Token-free selective state space model, 2024.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Linformer: Self-attention with linear complexity.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib37.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib37.10.2" class="ltx_text" style="font-size:90%;">, abs/2006.04768, 2020.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.4.4.1" class="ltx_text" style="font-size:90%;">Welch [1947]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.6.1" class="ltx_text" style="font-size:90%;">
Bernard L. Welch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">The generalization of ‘student’s’ problem when several different population variances are involved.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib38.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Biometrika</em><span id="bib.bib38.9.2" class="ltx_text" style="font-size:90%;">, 34(1/2):28–35, 1947.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Wu et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">Hi-transformer: Hierarchical interactive transformer for efficient and effective long document modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib39.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</em><span id="bib.bib39.11.3" class="ltx_text" style="font-size:90%;">, pages 848–853, Online, 2021. Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Xiong et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">Effective long-context scaling of foundation models, 2023.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Xu et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Retrieval meets long context large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib41.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span id="bib.bib41.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Zaheer et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Big bird: Transformers for longer sequences.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib42.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib42.11.3" class="ltx_text" style="font-size:90%;">, pages 17283–17297. Curran Associates, Inc., 2020.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.6.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">
Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun.
</span>
</span>
<span class="ltx_bibblock"><math id="bib.bib43.1.m1.1" class="ltx_Math" alttext="\infty" display="inline"><semantics id="bib.bib43.1.m1.1a"><mi mathsize="90%" mathvariant="normal" id="bib.bib43.1.m1.1.1" xref="bib.bib43.1.m1.1.1.cmml">∞</mi><annotation-xml encoding="MathML-Content" id="bib.bib43.1.m1.1b"><infinity id="bib.bib43.1.m1.1.1.cmml" xref="bib.bib43.1.m1.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="bib.bib43.1.m1.1c">\infty</annotation></semantics></math><span id="bib.bib43.9.1" class="ltx_text" style="font-size:90%;">bench: Extending long context evaluation beyond 100k tokens, 2024.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Zheng et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Judging LLM-as-a-judge with MT-bench and chatbot arena.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib44.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em><span id="bib.bib44.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Experimental setup details</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Compared models and hardware details</h3>

<figure id="A1.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A1.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="A1.T5.3.2" class="ltx_text" style="font-size:90%;">Summary of the long-context models compared in Section <a href="#S5" title="5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. *Vicuna models are provided with a 16K context limit, but it was extended to 32K using RoPE extrapolation <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>.</span></figcaption>
<div id="A1.T5.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:416.8pt;height:153.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-69.5pt,25.4pt) scale(0.75,0.75) ;">
<table id="A1.T5.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T5.4.1.1.1" class="ltx_tr">
<th id="A1.T5.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="A1.T5.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="A1.T5.4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<table id="A1.T5.4.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T5.4.1.1.1.2.1.1" class="ltx_tr">
<td id="A1.T5.4.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="A1.T5.4.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Context</span></td>
</tr>
<tr id="A1.T5.4.1.1.1.2.1.2" class="ltx_tr">
<td id="A1.T5.4.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="A1.T5.4.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">limit</span></td>
</tr>
</table>
</th>
<th id="A1.T5.4.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="A1.T5.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Backbone</span></th>
<th id="A1.T5.4.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="A1.T5.4.1.1.1.4.1" class="ltx_text ltx_font_bold">Link</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T5.4.1.2.1" class="ltx_tr">
<td id="A1.T5.4.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">GPT-3.5 (turbo-16k-0613)</td>
<td id="A1.T5.4.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">16K</td>
<td id="A1.T5.4.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">-</td>
<td id="A1.T5.4.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t"><a target="_blank" href="https://platform.openai.com/docs/models/gpt-3-5-turbo" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://platform.openai.com/docs/models/gpt-3-5-turbo</a></td>
</tr>
<tr id="A1.T5.4.1.3.2" class="ltx_tr">
<td id="A1.T5.4.1.3.2.1" class="ltx_td ltx_align_left">GPT-4 (1106-preview)</td>
<td id="A1.T5.4.1.3.2.2" class="ltx_td ltx_align_left">128K</td>
<td id="A1.T5.4.1.3.2.3" class="ltx_td ltx_align_left">-</td>
<td id="A1.T5.4.1.3.2.4" class="ltx_td ltx_align_left"><a target="_blank" href="https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo</a></td>
</tr>
<tr id="A1.T5.4.1.4.3" class="ltx_tr">
<td id="A1.T5.4.1.4.3.1" class="ltx_td ltx_align_left">LongAlpaca-7B</td>
<td id="A1.T5.4.1.4.3.2" class="ltx_td ltx_align_left">32K</td>
<td id="A1.T5.4.1.4.3.3" class="ltx_td ltx_align_left">LLaMA-2-7B</td>
<td id="A1.T5.4.1.4.3.4" class="ltx_td ltx_align_left"><a target="_blank" href="https://huggingface.co/Yukang/LongAlpaca-7B" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/Yukang/LongAlpaca-7B</a></td>
</tr>
<tr id="A1.T5.4.1.5.4" class="ltx_tr">
<td id="A1.T5.4.1.5.4.1" class="ltx_td ltx_align_left">LongAlpaca-13B</td>
<td id="A1.T5.4.1.5.4.2" class="ltx_td ltx_align_left">32K</td>
<td id="A1.T5.4.1.5.4.3" class="ltx_td ltx_align_left">LLaMA-2-13B</td>
<td id="A1.T5.4.1.5.4.4" class="ltx_td ltx_align_left"><a target="_blank" href="https://huggingface.co/Yukang/LongAlpaca-13B" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/Yukang/LongAlpaca-13B</a></td>
</tr>
<tr id="A1.T5.4.1.6.5" class="ltx_tr">
<td id="A1.T5.4.1.6.5.1" class="ltx_td ltx_align_left">LongChat-7B-v1.5</td>
<td id="A1.T5.4.1.6.5.2" class="ltx_td ltx_align_left">32K</td>
<td id="A1.T5.4.1.6.5.3" class="ltx_td ltx_align_left">LLaMA-2-7B</td>
<td id="A1.T5.4.1.6.5.4" class="ltx_td ltx_align_left"><a target="_blank" href="https://huggingface.co/lmsys/longchat-7b-v1.5-32k" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/lmsys/longchat-7b-v1.5-32k</a></td>
</tr>
<tr id="A1.T5.4.1.7.6" class="ltx_tr">
<td id="A1.T5.4.1.7.6.1" class="ltx_td ltx_align_left">Vicuna-7B-v1.5</td>
<td id="A1.T5.4.1.7.6.2" class="ltx_td ltx_align_left">16K*</td>
<td id="A1.T5.4.1.7.6.3" class="ltx_td ltx_align_left">LLaMA-2-7B</td>
<td id="A1.T5.4.1.7.6.4" class="ltx_td ltx_align_left"><a target="_blank" href="https://huggingface.co/lmsys/vicuna-7b-v1.5-16k" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/lmsys/vicuna-7b-v1.5-16k</a></td>
</tr>
<tr id="A1.T5.4.1.8.7" class="ltx_tr">
<td id="A1.T5.4.1.8.7.1" class="ltx_td ltx_align_left">Vicuna-13B-v1.5</td>
<td id="A1.T5.4.1.8.7.2" class="ltx_td ltx_align_left">16K*</td>
<td id="A1.T5.4.1.8.7.3" class="ltx_td ltx_align_left">LLaMA-2-13B</td>
<td id="A1.T5.4.1.8.7.4" class="ltx_td ltx_align_left"><a target="_blank" href="https://huggingface.co/lmsys/vicuna-13b-v1.5-16k" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/lmsys/vicuna-13b-v1.5-16k</a></td>
</tr>
<tr id="A1.T5.4.1.9.8" class="ltx_tr">
<td id="A1.T5.4.1.9.8.1" class="ltx_td ltx_align_left">LongAlign-7B</td>
<td id="A1.T5.4.1.9.8.2" class="ltx_td ltx_align_left">64K</td>
<td id="A1.T5.4.1.9.8.3" class="ltx_td ltx_align_left">LLaMA-2-7B</td>
<td id="A1.T5.4.1.9.8.4" class="ltx_td ltx_align_left"><a target="_blank" href="https://huggingface.co/THUDM/LongAlign-7B-64k" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/THUDM/LongAlign-7B-64k</a></td>
</tr>
<tr id="A1.T5.4.1.10.9" class="ltx_tr">
<td id="A1.T5.4.1.10.9.1" class="ltx_td ltx_align_left ltx_border_bb">LongAlign-13B</td>
<td id="A1.T5.4.1.10.9.2" class="ltx_td ltx_align_left ltx_border_bb">64K</td>
<td id="A1.T5.4.1.10.9.3" class="ltx_td ltx_align_left ltx_border_bb">LLaMA-2-13B</td>
<td id="A1.T5.4.1.10.9.4" class="ltx_td ltx_align_left ltx_border_bb"><a target="_blank" href="https://huggingface.co/THUDM/LongAlign-13B-64k" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/THUDM/LongAlign-13B-64k</a></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">We summarize the details of the different long-context LLMs compared in our experiments in Table <a href="#A1.T5" title="Table 5 ‣ A.1 Compared models and hardware details ‣ Appendix A Experimental setup details ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We provide for each model its context size limit in tokens, its backbone model (i.e., the pre-trained model used for the fine-tuning), and the link to the model checkpoint on Huggingface for open-source models or the link to the relevant OpenAI documentation for proprietary models.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.1" class="ltx_p">The inference was done on a single A100 GPU with 80GB memory. In preliminary experiments, we also attempted to include the Mistral-7B-Instruct-v0.2<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a target="_blank" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2</a></span></span></span> model in our study, as this model supports a context of up to 32K tokens. However, running this model on ELITR-Bench led to a GPU out-of-memory error on the A100, and thus we discarded it.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Configuration search on ELITR-Bench-QA’s dev set</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">In our pilot experiments, we noted that the open-source models retained for our study tended to be fairly impacted by the choice of the prompt and the inference hyperparameters. Therefore, we conducted a search on the inference configuration space to select appropriate hyperparameters for each open-source model.<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>In comparison to open-source models, we found that GPT-4 and GPT-3.5 were more robust to differences in the inference configuration. Therefore, given the extensive cost of doing a large number of runs for commercial models, we did not conduct a configuration search on these.</span></span></span> The configuration search was carried out in two steps on the dev set of ELITR-Bench-QA, in the single-turn mode. The evaluation was performed using GPT-4 as the evaluator, as described in the evaluation protocol in Section <a href="#S4" title="4 Experimental setup ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p">In the first step of the search – whose results are given in Table <a href="#A1.T6" title="Table 6 ‣ A.2 Configuration search on ELITR-Bench-QA’s dev set ‣ Appendix A Experimental setup details ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> – we varied three dimensions in the inference:</p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">The decoding method, which was either greedy decoding or nucleus sampling with a temperature of 0.6 and top-p of 0.9;</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">The use (or absence) of a chat template,<span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a target="_blank" href="https://huggingface.co/docs/transformers/main/en/chat_templating" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/docs/transformers/main/en/chat_templating</a></span></span></span> which modifies the prompt to integrate the same tags used in the fine-tuning stage – those tags varying across models;</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">The use (or absence) of question-answer markers, which introduces to the prompt the tokens ‘QUESTION:’ and ‘ANSWER:’ before the question and the expected answer, respectively.</p>
</div>
</li>
</ul>
<p id="A1.SS2.p2.2" class="ltx_p">The specific chat template we adopted for each model is based on the one used during the model’s fine-tuning: the LLaMA2 template for LongAlpaca-7B and LongAlpaca-13B; the Vicuna template for LongChat-7B-v1.5, Vicuna-7B-v1.5 and Vicuna-13B-v1.5; and the LongAlign template for LongAlign-7B and LongAlign-13B.</p>
</div>
<div id="A1.SS2.p3" class="ltx_para">
<p id="A1.SS2.p3.1" class="ltx_p">In the second step of the search, we used the configuration that yielded the best results on the first step for each model and tested the impact of setting the repetition penalty hyperparameter to 1.1 (instead of the default 1.0 value) in the inference. The results of step 2 are provided in Table <a href="#A1.T7" title="Table 7 ‣ A.2 Configuration search on ELITR-Bench-QA’s dev set ‣ Appendix A Experimental setup details ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div id="A1.SS2.p4" class="ltx_para">
<p id="A1.SS2.p4.1" class="ltx_p">Ultimately, the following configurations were retained for each model:</p>
<ul id="A1.I2" class="ltx_itemize">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p id="A1.I2.i1.p1.1" class="ltx_p"><span id="A1.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">LongAlpaca-7B:</span> greedy decoding with a chat template and QA markers;</p>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p id="A1.I2.i2.p1.1" class="ltx_p"><span id="A1.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">LongAlpaca-13B:</span> greedy decoding with a chat template;</p>
</div>
</li>
<li id="A1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i3.p1" class="ltx_para">
<p id="A1.I2.i3.p1.1" class="ltx_p"><span id="A1.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">LongChat-7B-v1.5:</span> greedy decoding with a chat template;</p>
</div>
</li>
<li id="A1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i4.p1" class="ltx_para">
<p id="A1.I2.i4.p1.1" class="ltx_p"><span id="A1.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Vicuna-7B-v1.5:</span> nucleus sampling with QA markers;</p>
</div>
</li>
<li id="A1.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i5.p1" class="ltx_para">
<p id="A1.I2.i5.p1.1" class="ltx_p"><span id="A1.I2.i5.p1.1.1" class="ltx_text ltx_font_bold">Vicuna-13B-v1.5:</span> nucleus sampling with a chat template, QA markers, and repetition penalty;</p>
</div>
</li>
<li id="A1.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i6.p1" class="ltx_para">
<p id="A1.I2.i6.p1.1" class="ltx_p"><span id="A1.I2.i6.p1.1.1" class="ltx_text ltx_font_bold">LongAlign-7B:</span> greedy decoding with a chat template;</p>
</div>
</li>
<li id="A1.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i7.p1" class="ltx_para">
<p id="A1.I2.i7.p1.1" class="ltx_p"><span id="A1.I2.i7.p1.1.1" class="ltx_text ltx_font_bold">LongAlign-13B:</span> nucleus sampling with a chat template.</p>
</div>
</li>
</ul>
<p id="A1.SS2.p4.2" class="ltx_p">For the proprietary models, <span id="A1.SS2.p4.2.1" class="ltx_text ltx_font_bold">GPT-3.5</span> and <span id="A1.SS2.p4.2.2" class="ltx_text ltx_font_bold">GPT-4</span>, we used nucleus sampling (temperature = 0.6 and top-p = 0.9) with the standard OpenAI chat template.</p>
</div>
<div id="A1.SS2.p5" class="ltx_para">
<p id="A1.SS2.p5.1" class="ltx_p">The cost of the two-step configuration search amounted to approximately $150.<span id="footnote14" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>We assessed the cost of performing the evaluation of a single model on the 141 dev set questions to $3 approximately. As we evaluated 7 models on 6 configurations in the first step, and 7 models on 1 configuration in the second step, this yields $147.</span></span></span> To limit excessive expenses, we used the same model configuration for the different settings we experimented in (single-turn ELITR-Bench-QA, multi-turn ELITR-Bench-QA, and multi-turn ELITR-Bench-Conv).</p>
</div>
<figure id="A1.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A1.T6.2.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="A1.T6.3.2" class="ltx_text" style="font-size:90%;">Results of step 1 for our configuration search on ELITR-Bench-QA’s dev set, in the single-turn mode. The configuration corresponding to using neither a chat template nor QA markers is not included as this was shown to severely underperform in our preliminary experiments.</span></figcaption>
<div id="A1.T6.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:483.5pt;height:135.4pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.9pt,7.5pt) scale(0.9,0.9) ;">
<table id="A1.T6.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T6.4.1.1.1" class="ltx_tr">
<th id="A1.T6.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A1.T6.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Decoding</span></th>
<th id="A1.T6.4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<table id="A1.T6.4.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T6.4.1.1.1.2.1.1" class="ltx_tr">
<td id="A1.T6.4.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="A1.T6.4.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Chat</span></td>
</tr>
<tr id="A1.T6.4.1.1.1.2.1.2" class="ltx_tr">
<td id="A1.T6.4.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="A1.T6.4.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">templ.</span></td>
</tr>
</table>
</th>
<th id="A1.T6.4.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">
<table id="A1.T6.4.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T6.4.1.1.1.3.1.1" class="ltx_tr">
<td id="A1.T6.4.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="A1.T6.4.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">QA</span></td>
</tr>
<tr id="A1.T6.4.1.1.1.3.1.2" class="ltx_tr">
<td id="A1.T6.4.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="A1.T6.4.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">mark.</span></td>
</tr>
</table>
</th>
<th id="A1.T6.4.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<table id="A1.T6.4.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T6.4.1.1.1.4.1.1" class="ltx_tr">
<td id="A1.T6.4.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">LongAl-</span></td>
</tr>
<tr id="A1.T6.4.1.1.1.4.1.2" class="ltx_tr">
<td id="A1.T6.4.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">paca-7B</span></td>
</tr>
</table>
</th>
<th id="A1.T6.4.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<table id="A1.T6.4.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T6.4.1.1.1.5.1.1" class="ltx_tr">
<td id="A1.T6.4.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">LongAl-</span></td>
</tr>
<tr id="A1.T6.4.1.1.1.5.1.2" class="ltx_tr">
<td id="A1.T6.4.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">paca-13B</span></td>
</tr>
</table>
</th>
<th id="A1.T6.4.1.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<table id="A1.T6.4.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T6.4.1.1.1.6.1.1" class="ltx_tr">
<td id="A1.T6.4.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">LongChat-</span></td>
</tr>
<tr id="A1.T6.4.1.1.1.6.1.2" class="ltx_tr">
<td id="A1.T6.4.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold">7B-v1.5</span></td>
</tr>
</table>
</th>
<th id="A1.T6.4.1.1.1.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<table id="A1.T6.4.1.1.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T6.4.1.1.1.7.1.1" class="ltx_tr">
<td id="A1.T6.4.1.1.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.7.1.1.1.1" class="ltx_text ltx_font_bold">Vicuna-</span></td>
</tr>
<tr id="A1.T6.4.1.1.1.7.1.2" class="ltx_tr">
<td id="A1.T6.4.1.1.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.7.1.2.1.1" class="ltx_text ltx_font_bold">7B-v1.5</span></td>
</tr>
</table>
</th>
<th id="A1.T6.4.1.1.1.8" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<table id="A1.T6.4.1.1.1.8.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T6.4.1.1.1.8.1.1" class="ltx_tr">
<td id="A1.T6.4.1.1.1.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.8.1.1.1.1" class="ltx_text ltx_font_bold">Vicuna-</span></td>
</tr>
<tr id="A1.T6.4.1.1.1.8.1.2" class="ltx_tr">
<td id="A1.T6.4.1.1.1.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.8.1.2.1.1" class="ltx_text ltx_font_bold">13B-v1.5</span></td>
</tr>
</table>
</th>
<th id="A1.T6.4.1.1.1.9" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<table id="A1.T6.4.1.1.1.9.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T6.4.1.1.1.9.1.1" class="ltx_tr">
<td id="A1.T6.4.1.1.1.9.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.9.1.1.1.1" class="ltx_text ltx_font_bold">LongAl-</span></td>
</tr>
<tr id="A1.T6.4.1.1.1.9.1.2" class="ltx_tr">
<td id="A1.T6.4.1.1.1.9.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.9.1.2.1.1" class="ltx_text ltx_font_bold">ign-7B</span></td>
</tr>
</table>
</th>
<th id="A1.T6.4.1.1.1.10" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<table id="A1.T6.4.1.1.1.10.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T6.4.1.1.1.10.1.1" class="ltx_tr">
<td id="A1.T6.4.1.1.1.10.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.10.1.1.1.1" class="ltx_text ltx_font_bold">LongAl-</span></td>
</tr>
<tr id="A1.T6.4.1.1.1.10.1.2" class="ltx_tr">
<td id="A1.T6.4.1.1.1.10.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T6.4.1.1.1.10.1.2.1.1" class="ltx_text ltx_font_bold">ign-13B</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T6.4.1.2.1" class="ltx_tr">
<th id="A1.T6.4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Greedy</th>
<th id="A1.T6.4.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Y</th>
<th id="A1.T6.4.1.2.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Y</th>
<td id="A1.T6.4.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T6.4.1.2.1.4.1" class="ltx_text ltx_font_bold">5.89</span></td>
<td id="A1.T6.4.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t">6.13</td>
<td id="A1.T6.4.1.2.1.6" class="ltx_td ltx_align_right ltx_border_t">6.22</td>
<td id="A1.T6.4.1.2.1.7" class="ltx_td ltx_align_right ltx_border_t">4.94</td>
<td id="A1.T6.4.1.2.1.8" class="ltx_td ltx_align_right ltx_border_t">5.19</td>
<td id="A1.T6.4.1.2.1.9" class="ltx_td ltx_align_right ltx_border_t">6.04</td>
<td id="A1.T6.4.1.2.1.10" class="ltx_td ltx_align_right ltx_border_t">6.16</td>
</tr>
<tr id="A1.T6.4.1.3.2" class="ltx_tr">
<th id="A1.T6.4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Greedy</th>
<th id="A1.T6.4.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Y</th>
<th id="A1.T6.4.1.3.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">N</th>
<td id="A1.T6.4.1.3.2.4" class="ltx_td ltx_align_right">5.55</td>
<td id="A1.T6.4.1.3.2.5" class="ltx_td ltx_align_right"><span id="A1.T6.4.1.3.2.5.1" class="ltx_text ltx_font_bold">6.17</span></td>
<td id="A1.T6.4.1.3.2.6" class="ltx_td ltx_align_right"><span id="A1.T6.4.1.3.2.6.1" class="ltx_text ltx_font_bold">6.60</span></td>
<td id="A1.T6.4.1.3.2.7" class="ltx_td ltx_align_right">5.38</td>
<td id="A1.T6.4.1.3.2.8" class="ltx_td ltx_align_right">5.13</td>
<td id="A1.T6.4.1.3.2.9" class="ltx_td ltx_align_right"><span id="A1.T6.4.1.3.2.9.1" class="ltx_text ltx_font_bold">6.11</span></td>
<td id="A1.T6.4.1.3.2.10" class="ltx_td ltx_align_right">6.16</td>
</tr>
<tr id="A1.T6.4.1.4.3" class="ltx_tr">
<th id="A1.T6.4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Greedy</th>
<th id="A1.T6.4.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">N</th>
<th id="A1.T6.4.1.4.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Y</th>
<td id="A1.T6.4.1.4.3.4" class="ltx_td ltx_align_right">5.89</td>
<td id="A1.T6.4.1.4.3.5" class="ltx_td ltx_align_right">5.87</td>
<td id="A1.T6.4.1.4.3.6" class="ltx_td ltx_align_right">6.23</td>
<td id="A1.T6.4.1.4.3.7" class="ltx_td ltx_align_right">5.05</td>
<td id="A1.T6.4.1.4.3.8" class="ltx_td ltx_align_right">4.71</td>
<td id="A1.T6.4.1.4.3.9" class="ltx_td ltx_align_right">5.43</td>
<td id="A1.T6.4.1.4.3.10" class="ltx_td ltx_align_right">5.94</td>
</tr>
<tr id="A1.T6.4.1.5.4" class="ltx_tr">
<th id="A1.T6.4.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Nucleus</th>
<th id="A1.T6.4.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Y</th>
<th id="A1.T6.4.1.5.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Y</th>
<td id="A1.T6.4.1.5.4.4" class="ltx_td ltx_align_right">5.18</td>
<td id="A1.T6.4.1.5.4.5" class="ltx_td ltx_align_right">5.91</td>
<td id="A1.T6.4.1.5.4.6" class="ltx_td ltx_align_right">6.19</td>
<td id="A1.T6.4.1.5.4.7" class="ltx_td ltx_align_right">4.99</td>
<td id="A1.T6.4.1.5.4.8" class="ltx_td ltx_align_right"><span id="A1.T6.4.1.5.4.8.1" class="ltx_text ltx_font_bold">5.70</span></td>
<td id="A1.T6.4.1.5.4.9" class="ltx_td ltx_align_right">5.67</td>
<td id="A1.T6.4.1.5.4.10" class="ltx_td ltx_align_right">6.25</td>
</tr>
<tr id="A1.T6.4.1.6.5" class="ltx_tr">
<th id="A1.T6.4.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Nucleus</th>
<th id="A1.T6.4.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Y</th>
<th id="A1.T6.4.1.6.5.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">N</th>
<td id="A1.T6.4.1.6.5.4" class="ltx_td ltx_align_right">5.61</td>
<td id="A1.T6.4.1.6.5.5" class="ltx_td ltx_align_right">6.11</td>
<td id="A1.T6.4.1.6.5.6" class="ltx_td ltx_align_right">5.85</td>
<td id="A1.T6.4.1.6.5.7" class="ltx_td ltx_align_right">5.33</td>
<td id="A1.T6.4.1.6.5.8" class="ltx_td ltx_align_right">5.00</td>
<td id="A1.T6.4.1.6.5.9" class="ltx_td ltx_align_right">6.06</td>
<td id="A1.T6.4.1.6.5.10" class="ltx_td ltx_align_right"><span id="A1.T6.4.1.6.5.10.1" class="ltx_text ltx_font_bold">6.27</span></td>
</tr>
<tr id="A1.T6.4.1.7.6" class="ltx_tr">
<th id="A1.T6.4.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Nucleus</th>
<th id="A1.T6.4.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">N</th>
<th id="A1.T6.4.1.7.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Y</th>
<td id="A1.T6.4.1.7.6.4" class="ltx_td ltx_align_right ltx_border_bb">5.58</td>
<td id="A1.T6.4.1.7.6.5" class="ltx_td ltx_align_right ltx_border_bb">5.96</td>
<td id="A1.T6.4.1.7.6.6" class="ltx_td ltx_align_right ltx_border_bb">5.91</td>
<td id="A1.T6.4.1.7.6.7" class="ltx_td ltx_align_right ltx_border_bb"><span id="A1.T6.4.1.7.6.7.1" class="ltx_text ltx_font_bold">5.42</span></td>
<td id="A1.T6.4.1.7.6.8" class="ltx_td ltx_align_right ltx_border_bb">4.89</td>
<td id="A1.T6.4.1.7.6.9" class="ltx_td ltx_align_right ltx_border_bb">5.18</td>
<td id="A1.T6.4.1.7.6.10" class="ltx_td ltx_align_right ltx_border_bb">5.99</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="A1.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A1.T7.2.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="A1.T7.3.2" class="ltx_text" style="font-size:90%;">Results of step 2 for our configuration search on ELITR-Bench-QA’s dev set, in the single-turn mode. In the cases where we include a repetition penalty, we set the corresponding hyperparameter to 1.1 (instead of 1.0, the default value corresponding to no repetition penalty).</span></figcaption>
<div id="A1.T7.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:413.5pt;height:70.6pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.0pt,3.9pt) scale(0.9,0.9) ;">
<table id="A1.T7.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T7.4.1.1.1" class="ltx_tr">
<th id="A1.T7.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">
<table id="A1.T7.4.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.4.1.1.1.1.1.1" class="ltx_tr">
<td id="A1.T7.4.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="A1.T7.4.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Repetition</span></td>
</tr>
<tr id="A1.T7.4.1.1.1.1.1.2" class="ltx_tr">
<td id="A1.T7.4.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="A1.T7.4.1.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">penalty</span></td>
</tr>
</table>
</th>
<td id="A1.T7.4.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt">
<table id="A1.T7.4.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.4.1.1.1.2.1.1" class="ltx_tr">
<td id="A1.T7.4.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">LongAl-</span></td>
</tr>
<tr id="A1.T7.4.1.1.1.2.1.2" class="ltx_tr">
<td id="A1.T7.4.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">paca-7B</span></td>
</tr>
</table>
</td>
<td id="A1.T7.4.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt">
<table id="A1.T7.4.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.4.1.1.1.3.1.1" class="ltx_tr">
<td id="A1.T7.4.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">LongAl-</span></td>
</tr>
<tr id="A1.T7.4.1.1.1.3.1.2" class="ltx_tr">
<td id="A1.T7.4.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">paca-13B</span></td>
</tr>
</table>
</td>
<td id="A1.T7.4.1.1.1.4" class="ltx_td ltx_align_right ltx_border_tt">
<table id="A1.T7.4.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.4.1.1.1.4.1.1" class="ltx_tr">
<td id="A1.T7.4.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">LongChat-</span></td>
</tr>
<tr id="A1.T7.4.1.1.1.4.1.2" class="ltx_tr">
<td id="A1.T7.4.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">7B-v1.5</span></td>
</tr>
</table>
</td>
<td id="A1.T7.4.1.1.1.5" class="ltx_td ltx_align_right ltx_border_tt">
<table id="A1.T7.4.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.4.1.1.1.5.1.1" class="ltx_tr">
<td id="A1.T7.4.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Vicuna-</span></td>
</tr>
<tr id="A1.T7.4.1.1.1.5.1.2" class="ltx_tr">
<td id="A1.T7.4.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">7B-v1.5</span></td>
</tr>
</table>
</td>
<td id="A1.T7.4.1.1.1.6" class="ltx_td ltx_align_right ltx_border_tt">
<table id="A1.T7.4.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.4.1.1.1.6.1.1" class="ltx_tr">
<td id="A1.T7.4.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">Vicuna-</span></td>
</tr>
<tr id="A1.T7.4.1.1.1.6.1.2" class="ltx_tr">
<td id="A1.T7.4.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold">13B-v1.5</span></td>
</tr>
</table>
</td>
<td id="A1.T7.4.1.1.1.7" class="ltx_td ltx_align_right ltx_border_tt">
<table id="A1.T7.4.1.1.1.7.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.4.1.1.1.7.1.1" class="ltx_tr">
<td id="A1.T7.4.1.1.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.7.1.1.1.1" class="ltx_text ltx_font_bold">LongAl-</span></td>
</tr>
<tr id="A1.T7.4.1.1.1.7.1.2" class="ltx_tr">
<td id="A1.T7.4.1.1.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.7.1.2.1.1" class="ltx_text ltx_font_bold">ign-7B</span></td>
</tr>
</table>
</td>
<td id="A1.T7.4.1.1.1.8" class="ltx_td ltx_align_right ltx_border_tt">
<table id="A1.T7.4.1.1.1.8.1" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.4.1.1.1.8.1.1" class="ltx_tr">
<td id="A1.T7.4.1.1.1.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.8.1.1.1.1" class="ltx_text ltx_font_bold">LongAl-</span></td>
</tr>
<tr id="A1.T7.4.1.1.1.8.1.2" class="ltx_tr">
<td id="A1.T7.4.1.1.1.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A1.T7.4.1.1.1.8.1.2.1.1" class="ltx_text ltx_font_bold">ign-13B</span></td>
</tr>
</table>
</td>
</tr>
<tr id="A1.T7.4.1.2.2" class="ltx_tr">
<th id="A1.T7.4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Y</th>
<td id="A1.T7.4.1.2.2.2" class="ltx_td ltx_align_right ltx_border_t">5.80</td>
<td id="A1.T7.4.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t">5.73</td>
<td id="A1.T7.4.1.2.2.4" class="ltx_td ltx_align_right ltx_border_t">6.11</td>
<td id="A1.T7.4.1.2.2.5" class="ltx_td ltx_align_right ltx_border_t">4.90</td>
<td id="A1.T7.4.1.2.2.6" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T7.4.1.2.2.6.1" class="ltx_text ltx_font_bold">5.92</span></td>
<td id="A1.T7.4.1.2.2.7" class="ltx_td ltx_align_right ltx_border_t">5.90</td>
<td id="A1.T7.4.1.2.2.8" class="ltx_td ltx_align_right ltx_border_t">6.21</td>
</tr>
<tr id="A1.T7.4.1.3.3" class="ltx_tr">
<th id="A1.T7.4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">N</th>
<td id="A1.T7.4.1.3.3.2" class="ltx_td ltx_align_right ltx_border_bb"><span id="A1.T7.4.1.3.3.2.1" class="ltx_text ltx_font_bold">5.89</span></td>
<td id="A1.T7.4.1.3.3.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="A1.T7.4.1.3.3.3.1" class="ltx_text ltx_font_bold">6.17</span></td>
<td id="A1.T7.4.1.3.3.4" class="ltx_td ltx_align_right ltx_border_bb"><span id="A1.T7.4.1.3.3.4.1" class="ltx_text ltx_font_bold">6.60</span></td>
<td id="A1.T7.4.1.3.3.5" class="ltx_td ltx_align_right ltx_border_bb"><span id="A1.T7.4.1.3.3.5.1" class="ltx_text ltx_font_bold">5.42</span></td>
<td id="A1.T7.4.1.3.3.6" class="ltx_td ltx_align_right ltx_border_bb">5.70</td>
<td id="A1.T7.4.1.3.3.7" class="ltx_td ltx_align_right ltx_border_bb"><span id="A1.T7.4.1.3.3.7.1" class="ltx_text ltx_font_bold">6.11</span></td>
<td id="A1.T7.4.1.3.3.8" class="ltx_td ltx_align_right ltx_border_bb"><span id="A1.T7.4.1.3.3.8.1" class="ltx_text ltx_font_bold">6.27</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional experimental results</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Variance over seeded run results</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">To account for the seed-dependent variability in the evaluation, we performed 3 seeded runs on the test set. The set of seeds used is {2023, 2024, 2025}. The results are reported in Table <a href="#A2.T8" title="Table 8 ‣ B.1 Variance over seeded run results ‣ Appendix B Additional experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, where we indicate for each (model, setting) pair the mean score over the 3 seeds as well as the sample standard deviation.</p>
</div>
<div id="A2.SS1.p2" class="ltx_para">
<p id="A2.SS1.p2.1" class="ltx_p">Note that the same seed is used both for the response generation part and the GPT-4-based evaluation part, as both can be sources of variance in the reported results. Based on our configuration search (see Appendix <a href="#A1.SS2" title="A.2 Configuration search on ELITR-Bench-QA’s dev set ‣ Appendix A Experimental setup details ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>), some of the response generation models were set to use greedy decoding: LongAlpaca-7B, LongAlpaca-13B, LongChat-7B-v1.5, LongAlign-7B. For such models, the response generation is deterministic and the only source of variance is that of the GPT-4 evaluator.</p>
</div>
<div id="A2.SS1.p3" class="ltx_para">
<p id="A2.SS1.p3.1" class="ltx_p">The results from Table <a href="#A2.T8" title="Table 8 ‣ B.1 Variance over seeded run results ‣ Appendix B Additional experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> show that the variance across settings is fairly different. In the single-turn ELITR-Bench-QA setting, the standard deviation for all models remain relatively low, even for the models that use nucleus sampling (GPT-3.5, GPT-4, Vicuna-7B-v1.5, Vicuna-13B-v1.5, LongAlign-13B). However, in the multi-turn settings, we observe an increased standard deviation for those same models overall, in particular for Vicuna-7B-v1.5 and LongAlign-13B. We hypothesize that the sequence of questions asked in the same conversation in the multi-turn setting causes different seeded runs to cumulate errors and slightly diverge along the course of the conversation.</p>
</div>
<figure id="A2.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T8.2.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="A2.T8.3.2" class="ltx_text" style="font-size:90%;">Results for the seeded runs on the test set for different ELITR-Bench settings. The reported numbers correspond to the mean score ± sample standard deviation computed over 3 seeds. Boldface numbers correspond to the best performance among proprietary or open-source models. The results for GPT-3.5 are omitted in the multi-turn setting as the context length exceeded the 16K limit of this model.</span></figcaption>
<div id="A2.T8.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:363.4pt;height:199.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-20.2pt,11.1pt) scale(0.9,0.9) ;">
<table id="A2.T8.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T8.4.1.1.1" class="ltx_tr">
<th id="A2.T8.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="A2.T8.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="A2.T8.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T8.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Single-turn</span></td>
<td id="A2.T8.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="A2.T8.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Multi-turn</span></td>
</tr>
<tr id="A2.T8.4.1.2.2" class="ltx_tr">
<td id="A2.T8.4.1.2.2.1" class="ltx_td ltx_align_right ltx_border_t">
<table id="A2.T8.4.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T8.4.1.2.2.1.1.1" class="ltx_tr">
<td id="A2.T8.4.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T8.4.1.2.2.1.1.1.1.1" class="ltx_text ltx_font_bold">ELITR-Bench-QA</span></td>
</tr>
<tr id="A2.T8.4.1.2.2.1.1.2" class="ltx_tr">
<td id="A2.T8.4.1.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T8.4.1.2.2.1.1.2.1.1" class="ltx_text ltx_font_bold">(test set)</span></td>
</tr>
</table>
</td>
<td id="A2.T8.4.1.2.2.2" class="ltx_td ltx_align_right ltx_border_t">
<table id="A2.T8.4.1.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T8.4.1.2.2.2.1.1" class="ltx_tr">
<td id="A2.T8.4.1.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T8.4.1.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">ELITR-Bench-QA</span></td>
</tr>
<tr id="A2.T8.4.1.2.2.2.1.2" class="ltx_tr">
<td id="A2.T8.4.1.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T8.4.1.2.2.2.1.2.1.1" class="ltx_text ltx_font_bold">(test set)</span></td>
</tr>
</table>
</td>
<td id="A2.T8.4.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t">
<table id="A2.T8.4.1.2.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T8.4.1.2.2.3.1.1" class="ltx_tr">
<td id="A2.T8.4.1.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T8.4.1.2.2.3.1.1.1.1" class="ltx_text ltx_font_bold">ELITR-Bench-Conv</span></td>
</tr>
<tr id="A2.T8.4.1.2.2.3.1.2" class="ltx_tr">
<td id="A2.T8.4.1.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T8.4.1.2.2.3.1.2.1.1" class="ltx_text ltx_font_bold">(test set)</span></td>
</tr>
</table>
</td>
</tr>
<tr id="A2.T8.4.1.3.3" class="ltx_tr">
<th id="A2.T8.4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GPT-3.5</th>
<td id="A2.T8.4.1.3.3.2" class="ltx_td ltx_align_right ltx_border_t">7.44 ± 0.12</td>
<td id="A2.T8.4.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t">-</td>
<td id="A2.T8.4.1.3.3.4" class="ltx_td ltx_align_right ltx_border_t">-</td>
</tr>
<tr id="A2.T8.4.1.4.4" class="ltx_tr">
<th id="A2.T8.4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-4</th>
<td id="A2.T8.4.1.4.4.2" class="ltx_td ltx_align_right"><span id="A2.T8.4.1.4.4.2.1" class="ltx_text ltx_font_bold">8.38 ± 0.07</span></td>
<td id="A2.T8.4.1.4.4.3" class="ltx_td ltx_align_right"><span id="A2.T8.4.1.4.4.3.1" class="ltx_text ltx_font_bold">8.42 ± 0.09</span></td>
<td id="A2.T8.4.1.4.4.4" class="ltx_td ltx_align_right"><span id="A2.T8.4.1.4.4.4.1" class="ltx_text ltx_font_bold">8.36 ± 0.12</span></td>
</tr>
<tr id="A2.T8.4.1.5.5" class="ltx_tr">
<th id="A2.T8.4.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LongAlpaca-7B</th>
<td id="A2.T8.4.1.5.5.2" class="ltx_td ltx_align_right ltx_border_t">5.60 ± 0.06</td>
<td id="A2.T8.4.1.5.5.3" class="ltx_td ltx_align_right ltx_border_t">4.84 ± 0.02</td>
<td id="A2.T8.4.1.5.5.4" class="ltx_td ltx_align_right ltx_border_t">4.58 ± 0.04</td>
</tr>
<tr id="A2.T8.4.1.6.6" class="ltx_tr">
<th id="A2.T8.4.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LongAlpaca-13B</th>
<td id="A2.T8.4.1.6.6.2" class="ltx_td ltx_align_right">6.25 ± 0.05</td>
<td id="A2.T8.4.1.6.6.3" class="ltx_td ltx_align_right">4.71 ± 0.01</td>
<td id="A2.T8.4.1.6.6.4" class="ltx_td ltx_align_right">4.74 ± 0.06</td>
</tr>
<tr id="A2.T8.4.1.7.7" class="ltx_tr">
<th id="A2.T8.4.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LongChat-7B-v1.5</th>
<td id="A2.T8.4.1.7.7.2" class="ltx_td ltx_align_right">5.78 ± 0.06</td>
<td id="A2.T8.4.1.7.7.3" class="ltx_td ltx_align_right">4.17 ± 0.07</td>
<td id="A2.T8.4.1.7.7.4" class="ltx_td ltx_align_right">4.31 ± 0.07</td>
</tr>
<tr id="A2.T8.4.1.8.8" class="ltx_tr">
<th id="A2.T8.4.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vicuna-7B-v1.5</th>
<td id="A2.T8.4.1.8.8.2" class="ltx_td ltx_align_right">5.61 ± 0.17</td>
<td id="A2.T8.4.1.8.8.3" class="ltx_td ltx_align_right">4.61 ± 0.26</td>
<td id="A2.T8.4.1.8.8.4" class="ltx_td ltx_align_right">4.69 ± 0.34</td>
</tr>
<tr id="A2.T8.4.1.9.9" class="ltx_tr">
<th id="A2.T8.4.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vicuna-13B-v1.5</th>
<td id="A2.T8.4.1.9.9.2" class="ltx_td ltx_align_right"><span id="A2.T8.4.1.9.9.2.1" class="ltx_text ltx_font_bold">6.52 ± 0.16</span></td>
<td id="A2.T8.4.1.9.9.3" class="ltx_td ltx_align_right"><span id="A2.T8.4.1.9.9.3.1" class="ltx_text ltx_font_bold">5.67 ± 0.10</span></td>
<td id="A2.T8.4.1.9.9.4" class="ltx_td ltx_align_right"><span id="A2.T8.4.1.9.9.4.1" class="ltx_text ltx_font_bold">5.78 ± 0.13</span></td>
</tr>
<tr id="A2.T8.4.1.10.10" class="ltx_tr">
<th id="A2.T8.4.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LongAlign-7B</th>
<td id="A2.T8.4.1.10.10.2" class="ltx_td ltx_align_right">6.46 ± 0.07</td>
<td id="A2.T8.4.1.10.10.3" class="ltx_td ltx_align_right">4.47 ± 0.01</td>
<td id="A2.T8.4.1.10.10.4" class="ltx_td ltx_align_right">5.06 ± 0.03</td>
</tr>
<tr id="A2.T8.4.1.11.11" class="ltx_tr">
<th id="A2.T8.4.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">LongAlign-13B</th>
<td id="A2.T8.4.1.11.11.2" class="ltx_td ltx_align_right ltx_border_bb">6.33 ± 0.09</td>
<td id="A2.T8.4.1.11.11.3" class="ltx_td ltx_align_right ltx_border_bb">5.33 ± 0.47</td>
<td id="A2.T8.4.1.11.11.4" class="ltx_td ltx_align_right ltx_border_bb">4.95 ± 0.22</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Additional results on question type and answer position</h3>

<figure id="A2.T9" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T9.2.1.1" class="ltx_text" style="font-size:90%;">Table 9</span>: </span><span id="A2.T9.3.2" class="ltx_text" style="font-size:90%;">Results by question type and answer position on the test set of ELITR-Bench-QA in single-turn mode. The number N below a subset indicates the corresponding subset size.</span></figcaption>
<div id="A2.T9.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:469.7pt;height:200.2pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.1pt,11.1pt) scale(0.9,0.9) ;">
<table id="A2.T9.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T9.4.1.1.1" class="ltx_tr">
<th id="A2.T9.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="A2.T9.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="A2.T9.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="A2.T9.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Question type</span></td>
<td id="A2.T9.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="A2.T9.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Answer position</span></td>
</tr>
<tr id="A2.T9.4.1.2.2" class="ltx_tr">
<td id="A2.T9.4.1.2.2.1" class="ltx_td ltx_align_right ltx_border_t">
<table id="A2.T9.4.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T9.4.1.2.2.1.1.1" class="ltx_tr">
<td id="A2.T9.4.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Who</span></td>
</tr>
<tr id="A2.T9.4.1.2.2.1.1.2" class="ltx_tr">
<td id="A2.T9.4.1.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.1.1.2.1.1" class="ltx_text ltx_font_bold">(N=45)</span></td>
</tr>
</table>
</td>
<td id="A2.T9.4.1.2.2.2" class="ltx_td ltx_align_right ltx_border_t">
<table id="A2.T9.4.1.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T9.4.1.2.2.2.1.1" class="ltx_tr">
<td id="A2.T9.4.1.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">What</span></td>
</tr>
<tr id="A2.T9.4.1.2.2.2.1.2" class="ltx_tr">
<td id="A2.T9.4.1.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.2.1.2.1.1" class="ltx_text ltx_font_bold">(N=57)</span></td>
</tr>
</table>
</td>
<td id="A2.T9.4.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t">
<table id="A2.T9.4.1.2.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T9.4.1.2.2.3.1.1" class="ltx_tr">
<td id="A2.T9.4.1.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.3.1.1.1.1" class="ltx_text ltx_font_bold">When</span></td>
</tr>
<tr id="A2.T9.4.1.2.2.3.1.2" class="ltx_tr">
<td id="A2.T9.4.1.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.3.1.2.1.1" class="ltx_text ltx_font_bold">(N=20)</span></td>
</tr>
</table>
</td>
<td id="A2.T9.4.1.2.2.4" class="ltx_td ltx_align_right ltx_border_t">
<table id="A2.T9.4.1.2.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T9.4.1.2.2.4.1.1" class="ltx_tr">
<td id="A2.T9.4.1.2.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.4.1.1.1.1" class="ltx_text ltx_font_bold">How many</span></td>
</tr>
<tr id="A2.T9.4.1.2.2.4.1.2" class="ltx_tr">
<td id="A2.T9.4.1.2.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.4.1.2.1.1" class="ltx_text ltx_font_bold">(N=8)</span></td>
</tr>
</table>
</td>
<td id="A2.T9.4.1.2.2.5" class="ltx_td ltx_align_right ltx_border_t">
<table id="A2.T9.4.1.2.2.5.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T9.4.1.2.2.5.1.1" class="ltx_tr">
<td id="A2.T9.4.1.2.2.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.5.1.1.1.1" class="ltx_text ltx_font_bold">Begin</span></td>
</tr>
<tr id="A2.T9.4.1.2.2.5.1.2" class="ltx_tr">
<td id="A2.T9.4.1.2.2.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.5.1.2.1.1" class="ltx_text ltx_font_bold">(N=43)</span></td>
</tr>
</table>
</td>
<td id="A2.T9.4.1.2.2.6" class="ltx_td ltx_align_right ltx_border_t">
<table id="A2.T9.4.1.2.2.6.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T9.4.1.2.2.6.1.1" class="ltx_tr">
<td id="A2.T9.4.1.2.2.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.6.1.1.1.1" class="ltx_text ltx_font_bold">Middle</span></td>
</tr>
<tr id="A2.T9.4.1.2.2.6.1.2" class="ltx_tr">
<td id="A2.T9.4.1.2.2.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.6.1.2.1.1" class="ltx_text ltx_font_bold">(N=34)</span></td>
</tr>
</table>
</td>
<td id="A2.T9.4.1.2.2.7" class="ltx_td ltx_align_right ltx_border_t">
<table id="A2.T9.4.1.2.2.7.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T9.4.1.2.2.7.1.1" class="ltx_tr">
<td id="A2.T9.4.1.2.2.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.7.1.1.1.1" class="ltx_text ltx_font_bold">End</span></td>
</tr>
<tr id="A2.T9.4.1.2.2.7.1.2" class="ltx_tr">
<td id="A2.T9.4.1.2.2.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.7.1.2.1.1" class="ltx_text ltx_font_bold">(N=22)</span></td>
</tr>
</table>
</td>
<td id="A2.T9.4.1.2.2.8" class="ltx_td ltx_align_right ltx_border_t">
<table id="A2.T9.4.1.2.2.8.1" class="ltx_tabular ltx_align_middle">
<tr id="A2.T9.4.1.2.2.8.1.1" class="ltx_tr">
<td id="A2.T9.4.1.2.2.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.8.1.1.1.1" class="ltx_text ltx_font_bold">Several</span></td>
</tr>
<tr id="A2.T9.4.1.2.2.8.1.2" class="ltx_tr">
<td id="A2.T9.4.1.2.2.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A2.T9.4.1.2.2.8.1.2.1.1" class="ltx_text ltx_font_bold">(N=31)</span></td>
</tr>
</table>
</td>
</tr>
<tr id="A2.T9.4.1.3.3" class="ltx_tr">
<th id="A2.T9.4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GPT-3.5</th>
<td id="A2.T9.4.1.3.3.2" class="ltx_td ltx_align_right ltx_border_t">7.91</td>
<td id="A2.T9.4.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t">6.94</td>
<td id="A2.T9.4.1.3.3.4" class="ltx_td ltx_align_right ltx_border_t">7.68</td>
<td id="A2.T9.4.1.3.3.5" class="ltx_td ltx_align_right ltx_border_t">7.79</td>
<td id="A2.T9.4.1.3.3.6" class="ltx_td ltx_align_right ltx_border_t">7.33</td>
<td id="A2.T9.4.1.3.3.7" class="ltx_td ltx_align_right ltx_border_t">7.45</td>
<td id="A2.T9.4.1.3.3.8" class="ltx_td ltx_align_right ltx_border_t">7.76</td>
<td id="A2.T9.4.1.3.3.9" class="ltx_td ltx_align_right ltx_border_t">7.37</td>
</tr>
<tr id="A2.T9.4.1.4.4" class="ltx_tr">
<th id="A2.T9.4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-4</th>
<td id="A2.T9.4.1.4.4.2" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.4.4.2.1" class="ltx_text ltx_font_bold">8.56</span></td>
<td id="A2.T9.4.1.4.4.3" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.4.4.3.1" class="ltx_text ltx_font_bold">8.29</span></td>
<td id="A2.T9.4.1.4.4.4" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.4.4.4.1" class="ltx_text ltx_font_bold">8.28</span></td>
<td id="A2.T9.4.1.4.4.5" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.4.4.5.1" class="ltx_text ltx_font_bold">8.29</span></td>
<td id="A2.T9.4.1.4.4.6" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.4.4.6.1" class="ltx_text ltx_font_bold">8.36</span></td>
<td id="A2.T9.4.1.4.4.7" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.4.4.7.1" class="ltx_text ltx_font_bold">8.29</span></td>
<td id="A2.T9.4.1.4.4.8" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.4.4.8.1" class="ltx_text ltx_font_bold">8.32</span></td>
<td id="A2.T9.4.1.4.4.9" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.4.4.9.1" class="ltx_text ltx_font_bold">8.57</span></td>
</tr>
<tr id="A2.T9.4.1.5.5" class="ltx_tr">
<th id="A2.T9.4.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LongAlpaca-7B</th>
<td id="A2.T9.4.1.5.5.2" class="ltx_td ltx_align_right ltx_border_t">5.35</td>
<td id="A2.T9.4.1.5.5.3" class="ltx_td ltx_align_right ltx_border_t">5.37</td>
<td id="A2.T9.4.1.5.5.4" class="ltx_td ltx_align_right ltx_border_t">6.35</td>
<td id="A2.T9.4.1.5.5.5" class="ltx_td ltx_align_right ltx_border_t"><span id="A2.T9.4.1.5.5.5.1" class="ltx_text ltx_font_bold">6.79</span></td>
<td id="A2.T9.4.1.5.5.6" class="ltx_td ltx_align_right ltx_border_t">5.81</td>
<td id="A2.T9.4.1.5.5.7" class="ltx_td ltx_align_right ltx_border_t">5.80</td>
<td id="A2.T9.4.1.5.5.8" class="ltx_td ltx_align_right ltx_border_t">4.97</td>
<td id="A2.T9.4.1.5.5.9" class="ltx_td ltx_align_right ltx_border_t">5.53</td>
</tr>
<tr id="A2.T9.4.1.6.6" class="ltx_tr">
<th id="A2.T9.4.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LongAlpaca-13B</th>
<td id="A2.T9.4.1.6.6.2" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.6.6.2.1" class="ltx_text ltx_font_bold">7.19</span></td>
<td id="A2.T9.4.1.6.6.3" class="ltx_td ltx_align_right">5.47</td>
<td id="A2.T9.4.1.6.6.4" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.6.6.4.1" class="ltx_text ltx_font_bold">6.47</span></td>
<td id="A2.T9.4.1.6.6.5" class="ltx_td ltx_align_right">6.00</td>
<td id="A2.T9.4.1.6.6.6" class="ltx_td ltx_align_right">5.93</td>
<td id="A2.T9.4.1.6.6.7" class="ltx_td ltx_align_right">5.95</td>
<td id="A2.T9.4.1.6.6.8" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.6.6.8.1" class="ltx_text ltx_font_bold">6.85</span></td>
<td id="A2.T9.4.1.6.6.9" class="ltx_td ltx_align_right">6.59</td>
</tr>
<tr id="A2.T9.4.1.7.7" class="ltx_tr">
<th id="A2.T9.4.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LongChat-7B-v1.5</th>
<td id="A2.T9.4.1.7.7.2" class="ltx_td ltx_align_right">6.88</td>
<td id="A2.T9.4.1.7.7.3" class="ltx_td ltx_align_right">4.94</td>
<td id="A2.T9.4.1.7.7.4" class="ltx_td ltx_align_right">6.33</td>
<td id="A2.T9.4.1.7.7.5" class="ltx_td ltx_align_right">4.17</td>
<td id="A2.T9.4.1.7.7.6" class="ltx_td ltx_align_right">6.41</td>
<td id="A2.T9.4.1.7.7.7" class="ltx_td ltx_align_right">4.91</td>
<td id="A2.T9.4.1.7.7.8" class="ltx_td ltx_align_right">5.89</td>
<td id="A2.T9.4.1.7.7.9" class="ltx_td ltx_align_right">5.77</td>
</tr>
<tr id="A2.T9.4.1.8.8" class="ltx_tr">
<th id="A2.T9.4.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vicuna-7B-v1.5</th>
<td id="A2.T9.4.1.8.8.2" class="ltx_td ltx_align_right">6.13</td>
<td id="A2.T9.4.1.8.8.3" class="ltx_td ltx_align_right">5.65</td>
<td id="A2.T9.4.1.8.8.4" class="ltx_td ltx_align_right">5.40</td>
<td id="A2.T9.4.1.8.8.5" class="ltx_td ltx_align_right">2.88</td>
<td id="A2.T9.4.1.8.8.6" class="ltx_td ltx_align_right">5.89</td>
<td id="A2.T9.4.1.8.8.7" class="ltx_td ltx_align_right">5.21</td>
<td id="A2.T9.4.1.8.8.8" class="ltx_td ltx_align_right">4.96</td>
<td id="A2.T9.4.1.8.8.9" class="ltx_td ltx_align_right">6.12</td>
</tr>
<tr id="A2.T9.4.1.9.9" class="ltx_tr">
<th id="A2.T9.4.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vicuna-13B-v1.5</th>
<td id="A2.T9.4.1.9.9.2" class="ltx_td ltx_align_right">6.96</td>
<td id="A2.T9.4.1.9.9.3" class="ltx_td ltx_align_right">6.68</td>
<td id="A2.T9.4.1.9.9.4" class="ltx_td ltx_align_right">5.48</td>
<td id="A2.T9.4.1.9.9.5" class="ltx_td ltx_align_right">5.54</td>
<td id="A2.T9.4.1.9.9.6" class="ltx_td ltx_align_right">6.35</td>
<td id="A2.T9.4.1.9.9.7" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.9.9.7.1" class="ltx_text ltx_font_bold">6.41</span></td>
<td id="A2.T9.4.1.9.9.8" class="ltx_td ltx_align_right">6.55</td>
<td id="A2.T9.4.1.9.9.9" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.9.9.9.1" class="ltx_text ltx_font_bold">6.87</span></td>
</tr>
<tr id="A2.T9.4.1.10.10" class="ltx_tr">
<th id="A2.T9.4.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LongAlign-7B-64k</th>
<td id="A2.T9.4.1.10.10.2" class="ltx_td ltx_align_right">6.93</td>
<td id="A2.T9.4.1.10.10.3" class="ltx_td ltx_align_right">6.33</td>
<td id="A2.T9.4.1.10.10.4" class="ltx_td ltx_align_right">6.00</td>
<td id="A2.T9.4.1.10.10.5" class="ltx_td ltx_align_right">5.88</td>
<td id="A2.T9.4.1.10.10.6" class="ltx_td ltx_align_right"><span id="A2.T9.4.1.10.10.6.1" class="ltx_text ltx_font_bold">7.09</span></td>
<td id="A2.T9.4.1.10.10.7" class="ltx_td ltx_align_right">6.39</td>
<td id="A2.T9.4.1.10.10.8" class="ltx_td ltx_align_right">6.47</td>
<td id="A2.T9.4.1.10.10.9" class="ltx_td ltx_align_right">5.66</td>
</tr>
<tr id="A2.T9.4.1.11.11" class="ltx_tr">
<th id="A2.T9.4.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">LongAlign-13B-64k</th>
<td id="A2.T9.4.1.11.11.2" class="ltx_td ltx_align_right ltx_border_bb">6.08</td>
<td id="A2.T9.4.1.11.11.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="A2.T9.4.1.11.11.3.1" class="ltx_text ltx_font_bold">6.74</span></td>
<td id="A2.T9.4.1.11.11.4" class="ltx_td ltx_align_right ltx_border_bb">5.97</td>
<td id="A2.T9.4.1.11.11.5" class="ltx_td ltx_align_right ltx_border_bb">5.75</td>
<td id="A2.T9.4.1.11.11.6" class="ltx_td ltx_align_right ltx_border_bb">6.71</td>
<td id="A2.T9.4.1.11.11.7" class="ltx_td ltx_align_right ltx_border_bb">6.21</td>
<td id="A2.T9.4.1.11.11.8" class="ltx_td ltx_align_right ltx_border_bb">6.33</td>
<td id="A2.T9.4.1.11.11.9" class="ltx_td ltx_align_right ltx_border_bb">5.95</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="A2.T10" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T10.4.2.1" class="ltx_text" style="font-size:90%;">Table 10</span>: </span><span id="A2.T10.2.1" class="ltx_text" style="font-size:90%;">Results of a one-tailed Welch’s t-test on the alternative hypothesis “The average score for questions with middle-position answers is lower than the average score of other questions”, to verify the presence or absence of a “lost in the middle” effect <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>. Boldface numbers denote statistically significant results (p-value <math id="A2.T10.2.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="A2.T10.2.1.m1.1b"><mo id="A2.T10.2.1.m1.1.1" xref="A2.T10.2.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A2.T10.2.1.m1.1c"><lt id="A2.T10.2.1.m1.1.1.cmml" xref="A2.T10.2.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.2.1.m1.1d">&lt;</annotation></semantics></math> 0.05).</span></figcaption>
<div id="A2.T10.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:128.1pt;height:162pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.1pt,9.0pt) scale(0.9,0.9) ;">
<table id="A2.T10.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T10.5.1.1.1" class="ltx_tr">
<th id="A2.T10.5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A2.T10.5.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="A2.T10.5.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="A2.T10.5.1.1.1.2.1" class="ltx_text ltx_font_bold">p-value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T10.5.1.2.1" class="ltx_tr">
<th id="A2.T10.5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GPT-3.5</th>
<td id="A2.T10.5.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">0.466</td>
</tr>
<tr id="A2.T10.5.1.3.2" class="ltx_tr">
<th id="A2.T10.5.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-4</th>
<td id="A2.T10.5.1.3.2.2" class="ltx_td ltx_align_right">0.372</td>
</tr>
<tr id="A2.T10.5.1.4.3" class="ltx_tr">
<th id="A2.T10.5.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LongAlpaca-7B</th>
<td id="A2.T10.5.1.4.3.2" class="ltx_td ltx_align_right">0.713</td>
</tr>
<tr id="A2.T10.5.1.5.4" class="ltx_tr">
<th id="A2.T10.5.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LongAlpaca-13B</th>
<td id="A2.T10.5.1.5.4.2" class="ltx_td ltx_align_right">0.265</td>
</tr>
<tr id="A2.T10.5.1.6.5" class="ltx_tr">
<th id="A2.T10.5.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LongChat-7B-v1.5</th>
<td id="A2.T10.5.1.6.5.2" class="ltx_td ltx_align_right"><span id="A2.T10.5.1.6.5.2.1" class="ltx_text ltx_font_bold">0.032</span></td>
</tr>
<tr id="A2.T10.5.1.7.6" class="ltx_tr">
<th id="A2.T10.5.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vicuna-7B-v1.5</th>
<td id="A2.T10.5.1.7.6.2" class="ltx_td ltx_align_right"><span id="A2.T10.5.1.7.6.2.1" class="ltx_text ltx_font_bold">0.046</span></td>
</tr>
<tr id="A2.T10.5.1.8.7" class="ltx_tr">
<th id="A2.T10.5.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vicuna-13B-v1.5</th>
<td id="A2.T10.5.1.8.7.2" class="ltx_td ltx_align_right">0.469</td>
</tr>
<tr id="A2.T10.5.1.9.8" class="ltx_tr">
<th id="A2.T10.5.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LongAlign-7B</th>
<td id="A2.T10.5.1.9.8.2" class="ltx_td ltx_align_right">0.409</td>
</tr>
<tr id="A2.T10.5.1.10.9" class="ltx_tr">
<th id="A2.T10.5.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">LongAlign-13B</th>
<td id="A2.T10.5.1.10.9.2" class="ltx_td ltx_align_right ltx_border_bb">0.413</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p">In this section, we provide the full results per question type and answer position to expand the compact results of the GPT and LLaMA-2 model families given in Section <a href="#S5.SS2" title="5.2 Impact of question type and answer position ‣ 5 Experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>. The results are given in Table <a href="#A2.T9" title="Table 9 ‣ B.2 Additional results on question type and answer position ‣ Appendix B Additional experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> and were obtained on ELITR-Bench-QA’s test set in the single-turn setting. Looking at the global model performance over the different question types and answer positions, we do not identify any clear trend highlighting a question type or position answer as notably easier or harder.</p>
</div>
<div id="A2.SS2.p2" class="ltx_para">
<p id="A2.SS2.p2.3" class="ltx_p">In constrast, past work reported a “lost in the middle” effect <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>, stating that the middle of a model’s context tends to be overlooked more often than the beginning or end of the context. To further investigate this phenomenon in our dataset, we conducted a statistical hypothesis test on the scores obtained by each individual model. Specifically, we ran a one-tailed Welch’s t-test <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> with the following alternative hypothesis: “The average score for questions with middle-position answers is lower than the average score of other questions”. The p-values obtained for each model’s set of scores are given in Table <a href="#A2.T10" title="Table 10 ‣ B.2 Additional results on question type and answer position ‣ Appendix B Additional experimental results ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. Interestingly, we observe that the “lost in the middle” hypothesis is statistically verified (p-value <math id="A2.SS2.p2.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="A2.SS2.p2.1.m1.1a"><mo id="A2.SS2.p2.1.m1.1.1" xref="A2.SS2.p2.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.1.m1.1b"><lt id="A2.SS2.p2.1.m1.1.1.cmml" xref="A2.SS2.p2.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.1.m1.1c">&lt;</annotation></semantics></math> 0.05) for only two models: LongChat-7B-v1.5 (p-value <math id="A2.SS2.p2.2.m2.1" class="ltx_Math" alttext="=" display="inline"><semantics id="A2.SS2.p2.2.m2.1a"><mo id="A2.SS2.p2.2.m2.1.1" xref="A2.SS2.p2.2.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.2.m2.1b"><eq id="A2.SS2.p2.2.m2.1.1.cmml" xref="A2.SS2.p2.2.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.2.m2.1c">=</annotation></semantics></math> 0.032) and Vicuna-7B-v1.5 (p-value <math id="A2.SS2.p2.3.m3.1" class="ltx_Math" alttext="=" display="inline"><semantics id="A2.SS2.p2.3.m3.1a"><mo id="A2.SS2.p2.3.m3.1.1" xref="A2.SS2.p2.3.m3.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="A2.SS2.p2.3.m3.1b"><eq id="A2.SS2.p2.3.m3.1.1.cmml" xref="A2.SS2.p2.3.m3.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p2.3.m3.1c">=</annotation></semantics></math> 0.046). While we do not have a clear explanation about which of these two models’ characteristics caused that effect, these models have in common that they are based on LLaMA-2-7B and were trained by the same LMSYS organization <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>. It is then possible – although purely hypothetical – that the specific fine-tuning recipe followed by LMSYS on LLaMA-2-7B for these two models led to the “lost in the middle” effect.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Crowdsourcing study details</h2>

<figure id="A3.F3" class="ltx_figure"><img src="/html/2403.20262/assets/figs/prolific_interface.png" id="A3.F3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="419" height="533" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="A3.F3.3.2" class="ltx_text" style="font-size:90%;">Interface for our Prolific crowdsourcing study to collect Silver Human score annotations.</span></figcaption>
</figure>
<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">Our <span id="A3.p1.1.1" class="ltx_text ltx_font_italic">Silver Human</span> evaluation is based on a crowdsourcing study using the Prolific<span id="footnote15" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a target="_blank" href="https://www.prolific.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.prolific.com/</a></span></span></span> platform. A task in this study consists in scoring the responses of the 3 considered models (GPT-4, Vicuna-13B-v1.5, and LongAlpaca-7B) for all the questions of a single meeting – out of 8 meetings in the test set. For each meeting, we hired 10 annotators, without constraining the 10 annotators to be the same across meetings.
Participants were screened based on their primary language (English) and domain expertise (including Computer Science, Information Technology, Engineering, or Mathematics). Each participant received £9 per hour when completing a task (with each task comprising approximately 40-50 questions for assessment). We estimated the task duration to be around 30 minutes – our post-analysis indicated a median time spent per study ranging between 16 and 29 minutes depending on the meeting.
We discarded the annotations that were flagged as too inconsistent with Gold Human scores, and hired new annotators when needed until we had a satisfactory set of 10 annotators per meeting. In total, the crowdsourced Prolific evaluation cost was £400.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p">The guidelines provided for this study start with general information about the task as well as the 10-point score rubric given in Fig. <a href="#A5.F6" title="Figure 6 ‣ Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, in order to help annotators calibrate their scores with concrete criteria. Then the interface presents a tuple composed of a question, its ground-truth answer, and an LLM response to evaluate. From this tuple, the annotator is asked to grade the LLM response with a score ranging from 1 to 10, following the provided score rubric. A screenshot of our interface is shown in Fig. <a href="#A3.F3" title="Figure 3 ‣ Appendix C Crowdsourcing study details ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.1" class="ltx_p">To measure the inter-annotator agreement, we used the intra-class correlation (ICC) coefficient <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> which assesses how consistent annotators’ scores are for every (question, ground-truth answer, LLM response) tuple. The ICC results are detailed in Table <a href="#A3.T11" title="Table 11 ‣ Appendix C Crowdsourcing study details ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> for each individual meeting and overall. For individual meetings, we report the two-way coefficient ICC(2,k) as the set of hired annotators is the same across all the questions of a given meeting. For the result over all meetings, we used instead the one-way coefficient ICC(1,k) since the set of annotators differs across meetings. Most of the ICC coefficients being above 0.9 suggests an excellent inter-annotator agreement, following the interpretation guidelines from <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>.</p>
</div>
<figure id="A3.T11" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A3.T11.2.1.1" class="ltx_text" style="font-size:90%;">Table 11</span>: </span><span id="A3.T11.3.2" class="ltx_text" style="font-size:90%;">Intra-class correlation (ICC) coefficients across annotators from the Prolific crowdsourcing study, corresponding to the Silver Human evaluator.</span></figcaption>
<div id="A3.T11.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:93.0pt;height:162pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-5.2pt,9.0pt) scale(0.9,0.9) ;">
<table id="A3.T11.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T11.4.1.1.1" class="ltx_tr">
<th id="A3.T11.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A3.T11.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Meeting ID</span></th>
<th id="A3.T11.4.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="A3.T11.4.1.1.1.2.1" class="ltx_text ltx_font_bold">ICC</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T11.4.1.2.1" class="ltx_tr">
<th id="A3.T11.4.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">01</th>
<td id="A3.T11.4.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">0.872</td>
</tr>
<tr id="A3.T11.4.1.3.2" class="ltx_tr">
<th id="A3.T11.4.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">02</th>
<td id="A3.T11.4.1.3.2.2" class="ltx_td ltx_align_right">0.964</td>
</tr>
<tr id="A3.T11.4.1.4.3" class="ltx_tr">
<th id="A3.T11.4.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">03</th>
<td id="A3.T11.4.1.4.3.2" class="ltx_td ltx_align_right">0.912</td>
</tr>
<tr id="A3.T11.4.1.5.4" class="ltx_tr">
<th id="A3.T11.4.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">04</th>
<td id="A3.T11.4.1.5.4.2" class="ltx_td ltx_align_right">0.941</td>
</tr>
<tr id="A3.T11.4.1.6.5" class="ltx_tr">
<th id="A3.T11.4.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">05</th>
<td id="A3.T11.4.1.6.5.2" class="ltx_td ltx_align_right">0.906</td>
</tr>
<tr id="A3.T11.4.1.7.6" class="ltx_tr">
<th id="A3.T11.4.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">06</th>
<td id="A3.T11.4.1.7.6.2" class="ltx_td ltx_align_right">0.940</td>
</tr>
<tr id="A3.T11.4.1.8.7" class="ltx_tr">
<th id="A3.T11.4.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">07</th>
<td id="A3.T11.4.1.8.7.2" class="ltx_td ltx_align_right">0.936</td>
</tr>
<tr id="A3.T11.4.1.9.8" class="ltx_tr">
<th id="A3.T11.4.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">08</th>
<td id="A3.T11.4.1.9.8.2" class="ltx_td ltx_align_right">0.942</td>
</tr>
<tr id="A3.T11.4.1.10.9" class="ltx_tr">
<th id="A3.T11.4.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t">all</th>
<td id="A3.T11.4.1.10.9.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">0.965</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="A4" class="ltx_appendix" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>ELITR-Bench excerpt</h2>

<figure id="A4.T12" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 12: </span>Small excerpt of meeting 010 from ELITR’s dev set, with sample questions and answers related to the same meeting from ELITR-Bench.</figcaption>
<table id="A4.T12.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A4.T12.4.1.1" class="ltx_tr">
<th id="A4.T12.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="A4.T12.4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Transcript excerpt</span></th>
<td id="A4.T12.4.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A4.T12.4.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1" class="ltx_p" style="width:303.5pt;">
<span id="A4.T12.4.1.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A4.T12.4.1.1.2.1.1.1.1" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.1.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">…</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.2" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.2.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.2.1.1.1.1" class="ltx_text" style="font-size:90%;">(PERSON19) Just &lt;unintelligible/&gt; like a virtual machine image.</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.3" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.3.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.3.1.1.1.1" class="ltx_text" style="font-size:90%;">(PERSON10) Yeah, yeah.</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.4" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.4.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.4.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.4.1.1.1.1" class="ltx_text" style="font-size:90%;">(PERSON19) You just fire up, an- anyone can fire up, it’s not like you have to you have to call-</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.5" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.5.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.5.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.5.1.1.1.1" class="ltx_text" style="font-size:90%;">(PERSON10) Yeah.</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.6" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.6.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.6.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.6.1.1.1.1" class="ltx_text" style="font-size:90%;">(PERSON19) Like [ORGANIZATION11], get them to run it.</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.7" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.7.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.7.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.7.1.1.1.1" class="ltx_text" style="font-size:90%;">(PERSON10) Yeah.</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.8" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.8.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.8.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.8.1.1.1.1" class="ltx_text" style="font-size:90%;">(PERSON19) I I don’t know that’s easier, but I mean it it’s more more flexible.</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.9" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.9.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.9.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.9.1.1.1.1" class="ltx_text" style="font-size:90%;">(PERSON10) Yeah, yeah.</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.10" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.10.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.10.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.10.1.1.1.1" class="ltx_text" style="font-size:90%;">I haven’t since I haven’t really done it, it’s uh, it’s hard for me to access, so we-</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.11" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.11.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.11.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.11.1.1.1.1" class="ltx_text" style="font-size:90%;">(PERSON19) I know, I know.</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.12" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.12.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.12.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.12.1.1.1.1" class="ltx_text" style="font-size:90%;">(PERSON10) You know.</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.13" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.13.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.13.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.13.1.1.1.1" class="ltx_text" style="font-size:90%;">Uh, okay, so that’s good, we know what to do. I don’t know whether we’ll manage to have these systems package before the demo, but hopefully uh, there won’t be any power outage an our uh, at our site.</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.14" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.14.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.14.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.14.1.1.1.1" class="ltx_text" style="font-size:90%;">(PERSON19) &lt;laugh/&gt;</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.15" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.15.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.15.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.15.1.1.1.1" class="ltx_text" style="font-size:90%;">(PERSON10) &lt;laugh/&gt;</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.16" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.16.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.16.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.16.1.1.1.1" class="ltx_text" style="font-size:90%;">So that was the 1 thing, that I’ve learnt, that we must not uh, that that we must have uh, rep- replicated uh, components across the site.</span></span>
</span></span></span>
<span id="A4.T12.4.1.1.2.1.1.1.17" class="ltx_tr">
<span id="A4.T12.4.1.1.2.1.1.1.17.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.1.1.2.1.1.1.17.1.1.1" class="ltx_p" style="width:433.6pt;"><span id="A4.T12.4.1.1.2.1.1.1.17.1.1.1.1" class="ltx_text" style="font-size:90%;">…</span></span>
</span></span></span>
</span></span>
</span>
</td>
</tr>
<tr id="A4.T12.4.2.2" class="ltx_tr">
<th id="A4.T12.4.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><span id="A4.T12.4.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Question (What)</span></th>
<td id="A4.T12.4.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="A4.T12.4.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.2.2.2.1.1" class="ltx_p" style="width:303.5pt;"><span id="A4.T12.4.2.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Which risk, related to the demo, was discussed?</span></span>
</span>
</td>
</tr>
<tr id="A4.T12.4.3.3" class="ltx_tr">
<th id="A4.T12.4.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="A4.T12.4.3.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Answer</span></th>
<td id="A4.T12.4.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A4.T12.4.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.3.3.2.1.1" class="ltx_p" style="width:303.5pt;"><span id="A4.T12.4.3.3.2.1.1.1" class="ltx_text" style="font-size:90%;">Power outages at [ORGANIZATION2]</span></span>
</span>
</td>
</tr>
<tr id="A4.T12.4.4.4" class="ltx_tr">
<th id="A4.T12.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="A4.T12.4.4.4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Question (Who)</span></th>
<td id="A4.T12.4.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A4.T12.4.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.4.4.2.1.1" class="ltx_p" style="width:303.5pt;"><span id="A4.T12.4.4.4.2.1.1.1" class="ltx_text" style="font-size:90%;">Which entity is running the translation module?</span></span>
</span>
</td>
</tr>
<tr id="A4.T12.4.5.5" class="ltx_tr">
<th id="A4.T12.4.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="A4.T12.4.5.5.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Answer</span></th>
<td id="A4.T12.4.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A4.T12.4.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.5.5.2.1.1" class="ltx_p" style="width:303.5pt;"><span id="A4.T12.4.5.5.2.1.1.1" class="ltx_text" style="font-size:90%;">[ORGANIZATION11]</span></span>
</span>
</td>
</tr>
<tr id="A4.T12.4.6.6" class="ltx_tr">
<th id="A4.T12.4.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="A4.T12.4.6.6.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Question (What)</span></th>
<td id="A4.T12.4.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A4.T12.4.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.6.6.2.1.1" class="ltx_p" style="width:303.5pt;"><span id="A4.T12.4.6.6.2.1.1.1" class="ltx_text" style="font-size:90%;">What should be frozen 1 or 2 weeks before the demo?</span></span>
</span>
</td>
</tr>
<tr id="A4.T12.4.7.7" class="ltx_tr">
<th id="A4.T12.4.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="A4.T12.4.7.7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Answer</span></th>
<td id="A4.T12.4.7.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A4.T12.4.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.7.7.2.1.1" class="ltx_p" style="width:303.5pt;"><span id="A4.T12.4.7.7.2.1.1.1" class="ltx_text" style="font-size:90%;">The stable components of the systems should be frozen 1-2 weeks before the demo</span></span>
</span>
</td>
</tr>
<tr id="A4.T12.4.8.8" class="ltx_tr">
<th id="A4.T12.4.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="A4.T12.4.8.8.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Question (When)</span></th>
<td id="A4.T12.4.8.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A4.T12.4.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.8.8.2.1.1" class="ltx_p" style="width:303.5pt;"><span id="A4.T12.4.8.8.2.1.1.1" class="ltx_text" style="font-size:90%;">When should the recorded demo be provided?</span></span>
</span>
</td>
</tr>
<tr id="A4.T12.4.9.9" class="ltx_tr">
<th id="A4.T12.4.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="A4.T12.4.9.9.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Answer</span></th>
<td id="A4.T12.4.9.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="A4.T12.4.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.4.9.9.2.1.1" class="ltx_p" style="width:303.5pt;"><span id="A4.T12.4.9.9.2.1.1.1" class="ltx_text" style="font-size:90%;">17th of June</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">We provide in Table <a href="#A4.T12" title="Table 12 ‣ Appendix D ELITR-Bench excerpt ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> an excerpt of meeting 010 from the dev set of the ELITR corpus <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>. Entities, such as (PERSON10), (PERSON19), and [ORGANIZATION11], have been de-identified in the original work for the sake of anonymization. Below the excerpt, we provide 4 questions (and their respective answers) related to the same meeting, which have been added through the proposed ELITR-Bench. For each question, we indicate its type between brackets (i.e., <span id="A4.p1.1.1" class="ltx_text ltx_font_italic">Who</span>, <span id="A4.p1.1.2" class="ltx_text ltx_font_italic">What</span>, <span id="A4.p1.1.3" class="ltx_text ltx_font_italic">When</span>, or <span id="A4.p1.1.4" class="ltx_text ltx_font_italic">How many</span>).</p>
</div>
</section>
<section id="A5" class="ltx_appendix" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Prompts</h2>

<figure id="A5.F4" class="ltx_figure">
<table id="A5.F4.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A5.F4.2.1.1" class="ltx_tr">
<td id="A5.F4.2.1.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="A5.F4.2.1.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:341.4pt;">
<span id="A5.F4.2.1.1.1.1.1" class="ltx_p">The following is the transcript of a meeting with multiple participants, where utterances start with the speaker’s anonymized name (for instance (PERSON4)) and may span over several lines.</span>
<span id="A5.F4.2.1.1.1.1.2" class="ltx_p"><span id="A5.F4.2.1.1.1.1.2.1" class="ltx_text" style="color:#0000FF;">{transcript}
<br class="ltx_break">
<br class="ltx_break"></span>As a professional conversational assistant, your task is to answer questions about the meeting by making inferences from the provided transcript.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="A5.F4.4.2" class="ltx_text" style="font-size:90%;">Answer prompt used to obtain LLMs’ responses. Questions are appended to this prompt as described in Section <a href="#S4" title="4 Experimental setup ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The element in blue and enclosed in curly brackets corresponds to a meeting-specific text span that is dynamically adapted.</span></figcaption>
</figure>
<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">In this section, we list the different prompts used in the paper, both for response generation and evaluation. The prompt for response generation follows the same general template given in Fig. <a href="#A5.F4" title="Figure 4 ‣ Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for every evaluated model – both proprietary and open-source models. Then, questions and answers are appended to the prompt as described in Section <a href="#S4" title="4 Experimental setup ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> – either a single question per conversation in the single-turn mode, or all the questions of a meeting in sequence in the multi-turn mode. As detailed in Appendix <a href="#A1.SS2" title="A.2 Configuration search on ELITR-Bench-QA’s dev set ‣ Appendix A Experimental setup details ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>, we slightly modify this base prompt depending on the model-specific selected configuration. As a reminder, these alterations may take two forms: the use of a chat template (which only adds special tags to the prompt) and the use of question-answer markers (which add ‘QUESTION:’ before a question and ‘ANSWER:’ before an answer).</p>
</div>
<div id="A5.p2" class="ltx_para">
<p id="A5.p2.1" class="ltx_p">The prompts that we used for evaluation are inspired from the prompt originally proposed in <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> and include: the question, the response to evaluate, the ground-truth answer, and a score rubric. Note that the transcript is not included in the evaluation prompt as the question and ground-truth answer should provide sufficient information to assess the correctness of the response to evaluate. The full prompts are given in Fig. <a href="#A5.F5" title="Figure 5 ‣ Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> for the GPT-4 evaluator and in Fig. <a href="#A5.F7" title="Figure 7 ‣ Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> for the Prometheus evaluator. Their score rubrics are shown in Fig. <a href="#A5.F6" title="Figure 6 ‣ Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and Fig. <a href="#A5.F8" title="Figure 8 ‣ Appendix E Prompts ‣ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, respectively. For Prometheus, we had to adapt the 10-point score scale to a 5-point scale to match the format used when this model was fine-tuned <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>. The 5-point rubric was defined to retain the main criteria expressed in the 10-point rubric and minimally alter it to enable a fair comparison between the two evaluators.</p>
</div>
<figure id="A5.F5" class="ltx_figure">
<table id="A5.F5.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A5.F5.2.1.1" class="ltx_tr">
<td id="A5.F5.2.1.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="A5.F5.2.1.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:341.4pt;">
<span id="A5.F5.2.1.1.1.1.1" class="ltx_p">### Task description:</span>
<span id="A5.F5.2.1.1.1.1.2" class="ltx_p">You are provided below with a question, a response to evaluate, a reference answer that gets the maximum score of 10, and a score rubric representing evaluation criteria.</span>
<span id="A5.F5.2.1.1.1.1.3" class="ltx_p">1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.</span>
<span id="A5.F5.2.1.1.1.1.4" class="ltx_p">2. After writing a feedback, write a score that is an integer between 1 and 10. You should refer to the score rubric.</span>
<span id="A5.F5.2.1.1.1.1.5" class="ltx_p">3. The output format should first include the feedback and then indicate the integer score in \boxed{}.</span>
<span id="A5.F5.2.1.1.1.1.6" class="ltx_p">4. Please do not generate any other opening, closing, and explanations.</span>
<span id="A5.F5.2.1.1.1.1.7" class="ltx_p">### Question:</span>
<span id="A5.F5.2.1.1.1.1.8" class="ltx_p"><span id="A5.F5.2.1.1.1.1.8.1" class="ltx_text" style="color:#0000FF;">{question}
<br class="ltx_break">
<br class="ltx_break"></span>### Response to evaluate:</span>
<span id="A5.F5.2.1.1.1.1.9" class="ltx_p"><span id="A5.F5.2.1.1.1.1.9.1" class="ltx_text" style="color:#0000FF;">{response}
<br class="ltx_break">
<br class="ltx_break"></span>### Reference answer (score 10):</span>
<span id="A5.F5.2.1.1.1.1.10" class="ltx_p"><span id="A5.F5.2.1.1.1.1.10.1" class="ltx_text" style="color:#0000FF;">{reference}
<br class="ltx_break">
<br class="ltx_break"></span>### Score rubric:</span>
<span id="A5.F5.2.1.1.1.1.11" class="ltx_p"><span id="A5.F5.2.1.1.1.1.11.1" class="ltx_text" style="color:#0000FF;">{rubric}
<br class="ltx_break">
<br class="ltx_break"></span>### Feedback:</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="A5.F5.4.2" class="ltx_text" style="font-size:90%;">Evaluation prompt for the GPT-4 evaluator, inspired from <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Kim et al.</span> [<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>. The elements in blue and enclosed in curly brackets correspond to question-specific text spans that are dynamically adapted.</span></figcaption>
</figure>
<figure id="A5.F6" class="ltx_figure">
<table id="A5.F6.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A5.F6.2.1.1" class="ltx_tr">
<td id="A5.F6.2.1.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="A5.F6.2.1.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:341.4pt;">
<span id="A5.F6.2.1.1.1.1.1" class="ltx_p">[Does the response to evaluate correctly address the given question based on the elements provided by the reference answer? The response should include the elements of the reference answer and should also avoid adding unnecessary elements or being too verbose.]</span>
<span id="A5.F6.2.1.1.1.1.2" class="ltx_p"><span id="A5.F6.2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Score 1:</span> The response to evaluate is incorrect and misses all the elements of the reference answer.</span>
<span id="A5.F6.2.1.1.1.1.3" class="ltx_p"><span id="A5.F6.2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Score 2:</span> The response to evaluate indicates insufficient knowledge to answer the question even though the reference answer states otherwise.</span>
<span id="A5.F6.2.1.1.1.1.4" class="ltx_p"><span id="A5.F6.2.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Score 3-4:</span> The response to evaluate contains some elements vaguely related to the reference answer.</span>
<span id="A5.F6.2.1.1.1.1.5" class="ltx_p"><span id="A5.F6.2.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Score 5-6:</span> The response to evaluate is partially correct and/or covers only a part of the reference answer.</span>
<span id="A5.F6.2.1.1.1.1.6" class="ltx_p"><span id="A5.F6.2.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Score 7-8:</span> The response to evaluate contains most of the reference answer but delivers it in an indirect and/or overly verbose way.</span>
<span id="A5.F6.2.1.1.1.1.7" class="ltx_p"><span id="A5.F6.2.1.1.1.1.7.1" class="ltx_text ltx_font_bold">Score 9:</span> The response to evaluate includes the reference answer but it is more verbose and adds unnecessary elements.</span>
<span id="A5.F6.2.1.1.1.1.8" class="ltx_p"><span id="A5.F6.2.1.1.1.1.8.1" class="ltx_text ltx_font_bold">Score 10:</span> The response to evaluate is essentially equivalent to the reference answer.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="A5.F6.4.2" class="ltx_text" style="font-size:90%;">Score rubric for the GPT-4 evaluator. Boldface is added for the sake of readability and is not included in the actual prompt.</span></figcaption>
</figure>
<figure id="A5.F7" class="ltx_figure">
<table id="A5.F7.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A5.F7.2.1.1" class="ltx_tr">
<td id="A5.F7.2.1.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="A5.F7.2.1.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:341.4pt;">
<span id="A5.F7.2.1.1.1.1.1" class="ltx_p">### Task description:</span>
<span id="A5.F7.2.1.1.1.1.2" class="ltx_p">You are provided below with a question, a response to evaluate, a reference answer that gets the maximum score of 5, and a score rubric representing evaluation criteria.</span>
<span id="A5.F7.2.1.1.1.1.3" class="ltx_p">1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general.</span>
<span id="A5.F7.2.1.1.1.1.4" class="ltx_p">2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.</span>
<span id="A5.F7.2.1.1.1.1.5" class="ltx_p">3. The output format should look as follows: "Feedback: (write the quality assessment feedback) [RESULT] (an integer number between 1 and 5)".</span>
<span id="A5.F7.2.1.1.1.1.6" class="ltx_p">4. Please do not generate any other opening, closing, and explanations.</span>
<span id="A5.F7.2.1.1.1.1.7" class="ltx_p">### Question:</span>
<span id="A5.F7.2.1.1.1.1.8" class="ltx_p"><span id="A5.F7.2.1.1.1.1.8.1" class="ltx_text" style="color:#0000FF;">{question}
<br class="ltx_break">
<br class="ltx_break"></span></span>
<span id="A5.F7.2.1.1.1.1.9" class="ltx_p">### Response to evaluate:</span>
<span id="A5.F7.2.1.1.1.1.10" class="ltx_p"><span id="A5.F7.2.1.1.1.1.10.1" class="ltx_text" style="color:#0000FF;">{response}
<br class="ltx_break">
<br class="ltx_break"></span></span>
<span id="A5.F7.2.1.1.1.1.11" class="ltx_p">### Reference answer (score 5):</span>
<span id="A5.F7.2.1.1.1.1.12" class="ltx_p"><span id="A5.F7.2.1.1.1.1.12.1" class="ltx_text" style="color:#0000FF;">{reference}
<br class="ltx_break">
<br class="ltx_break"></span></span>
<span id="A5.F7.2.1.1.1.1.13" class="ltx_p">### Score rubric:</span>
<span id="A5.F7.2.1.1.1.1.14" class="ltx_p"><span id="A5.F7.2.1.1.1.1.14.1" class="ltx_text" style="color:#0000FF;">{rubric}
<br class="ltx_break">
<br class="ltx_break"></span></span>
<span id="A5.F7.2.1.1.1.1.15" class="ltx_p">### Feedback:</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="A5.F7.4.2" class="ltx_text" style="font-size:90%;">Evaluation prompt for the Prometheus evaluator, inspired from <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Kim et al.</span> [<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>. The elements in blue and enclosed in curly brackets correspond to question-specific text spans that are dynamically adapted.</span></figcaption>
</figure>
<figure id="A5.F8" class="ltx_figure">
<table id="A5.F8.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A5.F8.2.1.1" class="ltx_tr">
<td id="A5.F8.2.1.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="A5.F8.2.1.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:341.4pt;">
<span id="A5.F8.2.1.1.1.1.1" class="ltx_p">[Does the response to evaluate correctly address the given question based on the elements provided by the reference answer? The response should include the elements of the reference answer and should also avoid adding unnecessary elements or being too verbose.]</span>
<span id="A5.F8.2.1.1.1.1.2" class="ltx_p"><span id="A5.F8.2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Score 1:</span> The response to evaluate is incorrect and misses all the elements of the reference answer.</span>
<span id="A5.F8.2.1.1.1.1.3" class="ltx_p"><span id="A5.F8.2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Score 2:</span> The response to evaluate contains some elements vaguely related to the reference answer.</span>
<span id="A5.F8.2.1.1.1.1.4" class="ltx_p"><span id="A5.F8.2.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Score 3:</span> The response to evaluate is partially correct and/or covers only a part of the reference answer.</span>
<span id="A5.F8.2.1.1.1.1.5" class="ltx_p"><span id="A5.F8.2.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Score 4:</span> The response to evaluate contains most of the reference answer but delivers it in an indirect and/or overly verbose way.</span>
<span id="A5.F8.2.1.1.1.1.6" class="ltx_p"><span id="A5.F8.2.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Score 5:</span> The response to evaluate is essentially equivalent to the reference answer.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="A5.F8.4.2" class="ltx_text" style="font-size:90%;">Score rubric for the Prometheus evaluator. Boldface is added for the sake of readability and is not included in the actual prompt.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.20261" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.20262" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.20262">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.20262" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.20263" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 17:44:37 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
