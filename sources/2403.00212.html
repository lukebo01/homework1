<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.00212] Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.00212">

<!--Generated on Fri Apr  5 14:49:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART 
<br class="ltx_break"><svg version="1.1" width="44.74" height="11.93" overflow="visible"><g transform="translate(0,11.93) scale(1,-1)"><path d="M 0,0 0,0" stroke="#000000" stroke-width="0.4"></path></g></svg>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Aniket Tathe
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Department of Mechanical Engineering
<br class="ltx_break">MES College of Engineering, Pune, India
<br class="ltx_break"><a href="mailto:anikettathe.08@gmail.com" title="" class="ltx_ref ltx_href">anikettathe.08@gmail.com</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anand Kamble
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Department of Scientific Computing
<br class="ltx_break">Florida State University, USA
<br class="ltx_break"><a href="mailto:amk23j@fsu.edu" title="" class="ltx_ref ltx_href">amk23j@fsu.edu</a></span>
</span><span class="ltx_author_notes">Corresponding author. Email: <span id="id4.2.id1" class="ltx_text" style="font-size:70%;">amk23j@fsu.edu</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Suyash Kumbharkar
<br class="ltx_break"><span id="id5.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Department of Electrical Engineering 
<br class="ltx_break">and Information Technology
<br class="ltx_break">Technische Hochschule Ingolstad, Germany
<br class="ltx_break"><a href="mailto:suk9387@thi.de" title="" class="ltx_ref ltx_href">suk9387@thi.de</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Atharva Bhandare
<br class="ltx_break"><span id="id6.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Department of Mechanical Engineering
<br class="ltx_break">MES College of Engineering, Pune, India
<br class="ltx_break"><a href="mailto:atharvabhandare512@gmail.com" title="" class="ltx_ref ltx_href">atharvabhandare512@gmail.com</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Anirban C. Mitra
<br class="ltx_break"><span id="id7.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:70%;">Department of Mechanical Engineering
<br class="ltx_break">MES College of Engineering, Pune, India
<br class="ltx_break"><a href="mailto:amitra@mescoepune.org" title="" class="ltx_ref ltx_href">amitra@mescoepune.org</a></span>
</span></span>
</div>

<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Abstract</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">This research addresses the challenge of training an ASR model for personalized voices with minimal data. Utilizing just 14 minutes of custom audio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to create a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual Self-supervised Representations (XLSR) Wav2Vec2 model is fine-tuned on this dataset. The developed web-based GUI efficiently transcribes and translates input Hindi videos. By integrating XLSR Wav2Vec2 and mBART, the system aligns the translated text with the video timeline, delivering an accessible solution for multilingual video content transcription and translation for personalized voice.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Keywords</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">XLSR Wav2Vec2, mBART, RVC, Personalized Speech Conversion, Speaker Diarization.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Introduction</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Training an Automatic Speech Recognition (ASR) model is a formidable task, with the efficacy of the model intricately tied to the quality of the dataset used during training. The challenge becomes even more pronounced when endeavoring to train an ASR model for a low-resource language like Hindi, where the availability of online data, compared to languages such as English, is notably limited. The complexity is further compounded when the aim is to train a personalized ASR model for Hindi, one that not only transcribes but also translates an individual’s unique voice. The generation of an appropriate dataset emerges as a significant hurdle in achieving this objective.
This paper confronts the complexities inherent in training personalized ASR models for Hindi by introducing an innovative methodology. Leveraging just 14 minutes of personalized custom audio, a Retrieval-Based Voice Conversion (RVC)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite> model is meticulously trained. This RVC model serves as the cornerstone for the creation of a tailored Common Voice 16 corpus, specifically tailored for Hindi. Subsequently, a Cross-lingual Self-supervised Representations (XLSR) Wav2Vec2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite> model is trained on this bespoke dataset, showcasing adaptability even with limited resources.
To augment the practical utility of the trained model, a user-friendly web-based Graphical User Interface (GUI) is developed. This GUI, powered by Gradio, simplifies the process of transcribing Hindi audio to Hindi text using XLSR Wav2Vec2 and subsequently translating this Hindi text to English using mBART<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">3</a>]</cite>. The input for this system is a video containing Hindi audio, and the output is a video enriched with accurate English subtitles. This research thus contributes to the advancement of ASR technology for low-resource languages, offering a streamlined pipeline for personalized voice transcription and translation from Hindi audio to English(subtitle), specifically tailored for video content.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Common voice 16.0 data augmentation using Ozen toolkit</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The data augmentation is performed on the Common voice 16.0 dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>]</cite>. In the most recent release, Common Voice 16.1, English language data spans approximately 3,438 hours of audio, with 2,586 hours validated by a community of 90,474 contributors and Hindi language section comprises about 21 hours of recorded audio, of which 14 hours have undergone meticulous validation
A 14-minute segment of custom, personalized audio was meticulously extracted from a YouTube video and subsequently converted into the WAV format. Employing the Ozen toolkit<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">5</a>]</cite>, a comprehensive processing pipeline ensued, involving speech extraction and transcription using the Whisper module. The transcribed results were then saved in the LJ format. It is noteworthy that the audio files are systematically stored in the designated ’wavs’ folder. Simultaneously, the transcribed Hindi texts are organized into separate ’train’ and ’valid’ text files, facilitating effective dataset management.
It is crucial to emphasize that Whisper primarily defaults to English transcription. To adapt this framework for Hindi transcription, necessary modifications were made within the utils.py file, specifically altering the ”task” to ”transcribe” and setting the ”language” to ”hi”. These adjustments ensure accurate and contextually relevant transcription for the Hindi language.
Additionally, the Ozen toolkit incorporates the pyannote<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite> framework for speaker diarization, further enhancing its utility in speech and audio processing applications. This integration enables precise identification and separation of speakers within the audio, contributing to the overall robustness of the system.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>RVC model training and inference</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The Retrieval-based-Voice-Conversion-WebUI by RVC-Project is an open-source tool enabling users to convert voice data using a retrieval-based voice conversion approach. It is freely accessible on GitHub [14] under the MIT software license. Recommended for audio recordings of a minimum 10-minute length for optimal model training, the project offers noise removal capabilities through Vocals/Accompaniment Separation and Reverbation, utilizing the ’HP2-all-vocals’ model. The target sample rate is set at 32,000, and the training utilizes the base v2 pre-trained model (f0G32K and f032K) with a batch size of 40 for 200 epochs on the NVIDIA A5000 GPU. Post 200 epochs, the KL Divergence Loss reaches 0.9292. This can be viewed in <a href="#S4.F1" title="Figure 1 ‣ 4.2 RVC model training and inference ‣ 4 Methodology ‣ Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART" class="ltx_ref">Fig.1</a> KL Divergence Loss (loss_kl) below.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2403.00212/assets/Graphs/Graph_KL_loss.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>KL Divergence Loss (loss_kl)</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">After completing the training phase, the model underwent inference to reproduce the acquired voice characteristics. Utilizing Common Voice audio inputs alongside the trained model’s inferencing, coupled with specified index paths, a bespoke common voice dataset was generated, showcasing the distinctive custom voice. Critical parameters such as volume envelope scaling (0.25), filter radius (3), and search feature ratios (0.75) and 0.33 for safeguarding voiceless consonants and breath sounds were implemented. It is imperative to note that these parameter values may necessitate adjustment based on the audio utilized during training, and users are encouraged to experiment with different settings to identify the optimal configuration. The resulting customized Common Voice 16.0 dataset, featuring the unique custom voice, is accessible under
” Aniket-Tathe-08/Custom_Common_Voice_16.0_dataset_using_RVC_14min_data” on Hugging Face. The pipeline can be viewed in <a href="#S4.F2" title="Figure 2 ‣ 4.2 RVC model training and inference ‣ 4 Methodology ‣ Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART" class="ltx_ref">Fig.2</a> Data Augmentation below.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2403.00212/assets/4th_paper.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Data Augmentation</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">At the end of this process, we have generated a custom common voice 16.0 dataset with input audio which only comprises the personalized voice with the help of RVC which will be further used to fine tune the XLSR model.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Fine-tuning XLSR Wav2Vec2 model</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">XLSR-Wav2Vec2 is a framework for self-supervised learning of speech representations proposed by Facebook AI Research. It builds on the original Wav2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>]</cite> but is trained on much larger speech recognition datasets to learn more robust representations. Specifically, XLSR-Wav2Vec2 leverages the self-supervised objective of masked reconstruction to pre-train a deep convolutional neural network model that encodes speech audio inputs into latent speech embeddings. More than 56,000 hours of unlabeled speech data across 50 languages are utilized to pre-train the XLSR model. This allows it to learn universal speech representations capturing various linguistic properties. Fine-tuning the pre-trained model on downstream speech tasks requires only a few minutes of labeled data to achieve state-of-the-art performance on benchmarks ranging from speech recognition to speech translation. The encoder model architecture and self-supervised pre-training technique allow XLSR-Wav2Vec2 representations to generalize very effectively across languages and domains.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.2" class="ltx_p">”facebook/wav2vec2-large-xlsr-53” was the model used for fine-tuning and the training process occurred on an NVIDIA A5000 GPU for 40 epochs with a learning rate of <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="1\times 10^{-4}" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml"><mn id="S4.SS3.p2.1.m1.1.1.3.2" xref="S4.SS3.p2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS3.p2.1.m1.1.1.3.3" xref="S4.SS3.p2.1.m1.1.1.3.3.cmml"><mo id="S4.SS3.p2.1.m1.1.1.3.3a" xref="S4.SS3.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS3.p2.1.m1.1.1.3.3.2" xref="S4.SS3.p2.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">1</cn><apply id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.3.1.cmml" xref="S4.SS3.p2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS3.p2.1.m1.1.1.3.2.cmml" xref="S4.SS3.p2.1.m1.1.1.3.2">10</cn><apply id="S4.SS3.p2.1.m1.1.1.3.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3.3"><minus id="S4.SS3.p2.1.m1.1.1.3.3.1.cmml" xref="S4.SS3.p2.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS3.p2.1.m1.1.1.3.3.2.cmml" xref="S4.SS3.p2.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">1\times 10^{-4}</annotation></semantics></math> and a weight decay of <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="2.5\times 10^{-6}" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mn id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">2.5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">×</mo><msup id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml"><mn id="S4.SS3.p2.2.m2.1.1.3.2" xref="S4.SS3.p2.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS3.p2.2.m2.1.1.3.3" xref="S4.SS3.p2.2.m2.1.1.3.3.cmml"><mo id="S4.SS3.p2.2.m2.1.1.3.3a" xref="S4.SS3.p2.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS3.p2.2.m2.1.1.3.3.2" xref="S4.SS3.p2.2.m2.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><times id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1"></times><cn type="float" id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">2.5</cn><apply id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.3.1.cmml" xref="S4.SS3.p2.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS3.p2.2.m2.1.1.3.2.cmml" xref="S4.SS3.p2.2.m2.1.1.3.2">10</cn><apply id="S4.SS3.p2.2.m2.1.1.3.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3.3"><minus id="S4.SS3.p2.2.m2.1.1.3.3.1.cmml" xref="S4.SS3.p2.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS3.p2.2.m2.1.1.3.3.2.cmml" xref="S4.SS3.p2.2.m2.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">2.5\times 10^{-6}</annotation></semantics></math> Although there wasn’t a significant difference, the model exhibited slightly better performance with weight decay. The model gave a training accuracy of 0.80 and WER of 0.53 The WER graph and training loss can be seen below in <a href="#S4.SS3" title="4.3 Fine-tuning XLSR Wav2Vec2 model ‣ 4 Methodology ‣ Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART" class="ltx_ref">Fig.3</a>. It was seen that the model was overfitting on the training data. This model trained on a personalized custom Common Voice 16.0 dataset can be found at ” Aniket-Tathe-08/XLSR-Wav2Vec2-Finetuned-14min-dataset”, which accepts Hindi audio input and generates corresponding Hindi text. 
<br class="ltx_break"></p>
</div>
<figure id="S4.SS3.2" class="ltx_figure">
<table id="S4.SS3.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.SS3.2.2.2" class="ltx_tr">
<td id="S4.SS3.1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2403.00212/assets/Graphs/tensorboard_1.png" id="S4.SS3.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="240" height="150" alt="[Uncaptioned image]"></td>
<td id="S4.SS3.2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2403.00212/assets/Graphs/tensorboard_2.png" id="S4.SS3.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="240" height="158" alt="[Uncaptioned image]"></td>
</tr>
<tr id="S4.SS3.2.2.3.1" class="ltx_tr">
<td id="S4.SS3.2.2.3.1.1" class="ltx_td ltx_align_center">(a) WER</td>
<td id="S4.SS3.2.2.3.1.2" class="ltx_td ltx_align_center">(b) Training Loss</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Neural machine translation using mBART</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">mBART (multilingual Bidirectional and Auto-Regressive Transformer) (Liu et al., 2020) is a multilingual sequence-to-sequence model pre-trained using a denoising autoencoding approach on large-scale monolingual corpora spanning 25 languages. It employs a standard Transformer-based encoder-decoder architecture (Vaswani et al. 2017) and is pre-trained to reconstruct input text fragments that have been corrupted through an arbitrary noising function. This self-supervised pre-training enables mBART to learn universal linguistic representations that transfer across multiple languages and downstream tasks. After pre-training on around 200GB of text data, mBART can be fine-tuned on target sequence generation tasks by simply adding task-specific decoder heads. Requiring only a few thousand labelled examples in a low-resource setting, fined-tuned mBART has achieved state-of-the-art performance on various multilingual benchmarks including translation, summarization, and question answering. The pre-trained representations and adapted parameters effectively encapsulate multilingual sequences, allowing mBART to generalize well even for low-resource language generation tasks.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Speaker Diarization using pyannote/speaker-diarization-3.1</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Pyannote is an open-source speech processing toolkit designed for constructing speaker diarization systems. Diarization addresses the question of ’who spoke when?’ in an audio recording. In the end-to-end pipeline, Pyannote determines the start and end times of each speaker, leading to the segmentation of the input audio into multiple segments with the format (start, end, speaker). This information is then used to sequentially process the cropped audio segments through XLSR Wav2Vec2 and mBART for subtitle generation.
The output is stored in an ’output.vtt’ file, containing both the start and end times based on the diarization and the output generated by XLSR Wav2Vec2 and mBART. An example of the ’output.vtt’ file is illustrated in the <a href="#LST1" title="Listing 1 ‣ 4.5 Speaker Diarization using pyannote/speaker-diarization-3.1 ‣ 4 Methodology ‣ Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART" class="ltx_ref">Listing 1</a> Diarization output below.</p>
</div>
<figure id="LST1" class="ltx_float ltx_lstlisting">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float">Listing 1: </span>Diarization output</figcaption>
<div id="LST1.1" class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" style="background-color:#F9F9F9;">
<div class="ltx_listing_data"><a href="data:text/plain;base64,MDA6MDAuMDAwIC0tPiAwMDowNi40MDAKU28gbm93IGxldCdzIGNvbWUgdG8gSmFwYW4uIFdoZW4gSSBjYW1lIHRvIEphcGFuLCBJIHdhcyBhYm91dCAyMiB5ZWFycyBvbGQgYW5kIEkgaGF2ZSBiZWVuIGxpdmluZyBoZXJlIGZvciA4IHllYXJzLgoKMDA6MDYuNDAwIC0tPiAwMDoxMC40MDAKQW5kIGhlcmUgc29tZSBwZW9wbGUgYXJlIGN1cmlvdXMgYWJvdXQgdGhlIHNhbGFyeSwgaG93IG11Y2ggc2FsYXJ5IGlzIHRoZXJlIGluIEphcGFuLgoKMDA6MTAuNDAwIC0tPiAwMDozMi40MDAKU28gcGVyc29uYWxseSwgSSBmZWVsIHRoYXQgYWNjb3JkaW5nIHRvIG15IHNhbGFyeSwgdGhlcmUgaXMgbm8gcHJvYmxlbSBpbiBzb2NpYWwgbWVkaWEu" download="">⬇</a></div>
<div id="lstnumberx1" class="ltx_listingline">
<span id="lstnumberx1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">00:00.000</span><span id="lstnumberx1.2" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx1.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">--&gt;</span><span id="lstnumberx1.4" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx1.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">00:06.400</span>
</div>
<div id="lstnumberx2" class="ltx_listingline">
<span id="lstnumberx2.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">So</span><span id="lstnumberx2.2" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.3" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">now</span><span id="lstnumberx2.4" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">let</span><span id="lstnumberx2.6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">’</span><span id="lstnumberx2.7" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">s</span><span id="lstnumberx2.8" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.9" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">come</span><span id="lstnumberx2.10" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.11" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">to</span><span id="lstnumberx2.12" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.13" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">Japan</span><span id="lstnumberx2.14" class="ltx_text ltx_font_typewriter" style="font-size:90%;">.</span><span id="lstnumberx2.15" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.16" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">When</span><span id="lstnumberx2.17" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.18" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">I</span><span id="lstnumberx2.19" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.20" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">came</span><span id="lstnumberx2.21" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.22" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">to</span><span id="lstnumberx2.23" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.24" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">Japan</span><span id="lstnumberx2.25" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,</span><span id="lstnumberx2.26" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.27" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">I</span><span id="lstnumberx2.28" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.29" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">was</span><span id="lstnumberx2.30" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.31" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">about</span><span id="lstnumberx2.32" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.33" class="ltx_text ltx_font_typewriter" style="font-size:90%;">22</span><span id="lstnumberx2.34" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.35" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">years</span><span id="lstnumberx2.36" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.37" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">old</span><span id="lstnumberx2.38" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.39" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">and</span><span id="lstnumberx2.40" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.41" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">I</span><span id="lstnumberx2.42" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.43" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">have</span><span id="lstnumberx2.44" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.45" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">been</span><span id="lstnumberx2.46" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.47" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">living</span><span id="lstnumberx2.48" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.49" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">here</span><span id="lstnumberx2.50" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.51" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">for</span><span id="lstnumberx2.52" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.53" class="ltx_text ltx_font_typewriter" style="font-size:90%;">8</span><span id="lstnumberx2.54" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx2.55" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">years</span><span id="lstnumberx2.56" class="ltx_text ltx_font_typewriter" style="font-size:90%;">.</span>
</div>
<div id="lstnumberx3" class="ltx_listingline">
</div>
<div id="lstnumberx4" class="ltx_listingline">
<span id="lstnumberx4.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">00:06.400</span><span id="lstnumberx4.2" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx4.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">--&gt;</span><span id="lstnumberx4.4" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx4.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">00:10.400</span>
</div>
<div id="lstnumberx5" class="ltx_listingline">
<span id="lstnumberx5.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">And</span><span id="lstnumberx5.2" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.3" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">here</span><span id="lstnumberx5.4" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">some</span><span id="lstnumberx5.6" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.7" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">people</span><span id="lstnumberx5.8" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.9" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">are</span><span id="lstnumberx5.10" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.11" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">curious</span><span id="lstnumberx5.12" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.13" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">about</span><span id="lstnumberx5.14" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.15" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">the</span><span id="lstnumberx5.16" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.17" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">salary</span><span id="lstnumberx5.18" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,</span><span id="lstnumberx5.19" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.20" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">how</span><span id="lstnumberx5.21" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.22" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">much</span><span id="lstnumberx5.23" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.24" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">salary</span><span id="lstnumberx5.25" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.26" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">is</span><span id="lstnumberx5.27" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.28" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">there</span><span id="lstnumberx5.29" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.30" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">in</span><span id="lstnumberx5.31" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx5.32" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">Japan</span><span id="lstnumberx5.33" class="ltx_text ltx_font_typewriter" style="font-size:90%;">.</span>
</div>
<div id="lstnumberx6" class="ltx_listingline">
</div>
<div id="lstnumberx7" class="ltx_listingline">
<span id="lstnumberx7.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">00:10.400</span><span id="lstnumberx7.2" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx7.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">--&gt;</span><span id="lstnumberx7.4" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx7.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">00:32.400</span>
</div>
<div id="lstnumberx8" class="ltx_listingline">
<span id="lstnumberx8.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">So</span><span id="lstnumberx8.2" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.3" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">personally</span><span id="lstnumberx8.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,</span><span id="lstnumberx8.5" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.6" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">I</span><span id="lstnumberx8.7" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">feel</span><span id="lstnumberx8.9" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.10" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">that</span><span id="lstnumberx8.11" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.12" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">according</span><span id="lstnumberx8.13" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.14" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">to</span><span id="lstnumberx8.15" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.16" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">my</span><span id="lstnumberx8.17" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.18" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">salary</span><span id="lstnumberx8.19" class="ltx_text ltx_font_typewriter" style="font-size:90%;">,</span><span id="lstnumberx8.20" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.21" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">there</span><span id="lstnumberx8.22" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.23" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">is</span><span id="lstnumberx8.24" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.25" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">no</span><span id="lstnumberx8.26" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.27" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">problem</span><span id="lstnumberx8.28" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.29" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">in</span><span id="lstnumberx8.30" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.31" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">social</span><span id="lstnumberx8.32" class="ltx_text ltx_lst_space ltx_font_typewriter" style="font-size:90%;"> </span><span id="lstnumberx8.33" class="ltx_text ltx_lst_identifier ltx_font_typewriter" style="font-size:90%;">media</span><span id="lstnumberx8.34" class="ltx_text ltx_font_typewriter" style="font-size:90%;">.</span>
</div>
</div>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Web GUI using Gradio</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">After uploading the input video with Hindi audio to the GUI, an ’mp3’ file is generated, containing the extracted Hindi audio from the video. This ’mp3’ file is then input into Pyannote for speaker diarization. The output of Pyannote, in the format (start, end, speaker), is used to crop the audio accordingly using this ‘start’ and ‘end’. The cropped audio segments are then processed through the XLSR Wav2Vec2 model for Hindi audio to Hindi text conversion and mBART for translating Hindi text to English. Subsequently, subtitles are generated. Using the ’start’, ’end’, and subtitle results, an ’output.vtt’ file is created. This ’output.vtt’ file is then overlaid onto the input video, producing the final output video with subtitles, as illustrated in the pipeline below in <a href="#S4.SS6" title="4.6 Web GUI using Gradio ‣ 4 Methodology ‣ Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART" class="ltx_ref">Fig. 4</a>.</p>
</div>
<figure id="S4.SS6.1" class="ltx_figure"><img src="/html/2403.00212/assets/Pipeline_part_1.png" id="S4.SS6.1.g1" class="ltx_graphics ltx_img_landscape" width="538" height="123" alt="[Uncaptioned image]">
</figure>
<figure id="S4.SS6.2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.00212/assets/Pipeline_Part_2.png" id="S4.SS6.2.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="153" alt="[Uncaptioned image]"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.SS6.2.1" class="ltx_p ltx_figure_panel ltx_align_center">Figure 4: End-to-end pipeline</p>
</div>
</div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Training a custom ASR model for personalized audio in a low-resource language like Hindi remains a significant challenge, requiring further research in this domain. The XLSR Wav2Vec2 model can be fine-tuned to achieve better accuracy, enabling more precise transcriptions for personalized audio. Using just 14 minutes of personalized custom audio, an RVC model was trained to generate a custom Common Voice 16.0 corpus.
This corpus was then employed to train an XLSR Wav2Vec2 model, resulting in approximately 0.80 accuracy and 0.53 WER. Alternatively, one can explore using other datasets like LibriSpeech<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite>. This method not only facilitates data augmentation but also enables the training of a personalized custom ASR model with high accuracy, even with a very limited dataset. 
<br class="ltx_break">
<span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">, <a target="_blank" href="https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI</a>
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Arun Babu et al.
</span>
<span class="ltx_bibblock">“XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2022</em>, 2022, pp. 2278–2282
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.21437/Interspeech.2022-143" title="" class="ltx_ref ltx_href">10.21437/Interspeech.2022-143</a>
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Yinhan Liu et al.
</span>
<span class="ltx_bibblock">“Multilingual Denoising Pre-training for Neural Machine Translation”, 2020
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2001.08210" title="" class="ltx_ref ltx_href">2001.08210 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Rosana Ardila et al.
</span>
<span class="ltx_bibblock">“Common Voice: A Massively-Multilingual Speech Corpus”, 2020
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/1912.06670" title="" class="ltx_ref ltx_href">1912.06670 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">, <a target="_blank" href="https://github.com/devilismyfriend/ozen-toolkit" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/devilismyfriend/ozen-toolkit</a>
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Hervé Bredin and Antoine Laurent
</span>
<span class="ltx_bibblock">“End-to-end speaker segmentation for overlap-aware resegmentation”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2021</em>, 2021
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Steffen Schneider, Alexei Baevski, Ronan Collobert and Michael Auli
</span>
<span class="ltx_bibblock">“wav2vec: Unsupervised Pre-training for Speech Recognition”, 2019
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/1904.05862" title="" class="ltx_ref ltx_href">1904.05862 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Vassil Panayotov, Guoguo Chen, Daniel Povey and Sanjeev Khudanpur
</span>
<span class="ltx_bibblock">“Librispeech: An ASR corpus based on public domain audio books”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2015, pp. 5206–5210
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/ICASSP.2015.7178964" title="" class="ltx_ref ltx_href">10.1109/ICASSP.2015.7178964</a>
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Isham Mohamed and Uthayasanker Thayasivam
</span>
<span class="ltx_bibblock">“Low Resource Multi-ASR Speech Command Recognition”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">2022 Moratuwa Engineering Research Conference (MERCon)</em>, 2022, pp. 1–6
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/MERCon55799.2022.9906230" title="" class="ltx_ref ltx_href">10.1109/MERCon55799.2022.9906230</a>
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Cheng Yi et al.
</span>
<span class="ltx_bibblock">“Applying Wav2vec2.0 to Speech Recognition in Various Low-resource Languages”, 2021
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2012.12121" title="" class="ltx_ref ltx_href">2012.12121 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Kak Soky, Sheng Li, Chenhui Chu and Tatsuya Kawahara
</span>
<span class="ltx_bibblock">“Domain and Language Adaptation Using Heterogeneous Datasets for Wav2vec2.0-Based Speech Recognition of Low-Resource Language”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx11.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023, pp. 1–5
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/ICASSP49357.2023.10095644" title="" class="ltx_ref ltx_href">10.1109/ICASSP49357.2023.10095644</a>
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">H… Shahgir, Khondker Salman Sayeed and Tanjeem Azwad Zaman
</span>
<span class="ltx_bibblock">“Applying wav2vec2 for Speech Recognition on Bengali Common Voices Dataset”, 2022
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2209.06581" title="" class="ltx_ref ltx_href">2209.06581 [eess.AS]</a>
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Linkai Peng et al.
</span>
<span class="ltx_bibblock">“A Study on Fine-Tuning wav2vec2.0 Model for the Task of Mispronunciation Detection and Diagnosis”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2021</em>, 2021, pp. 4448–4452
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.21437/Interspeech.2021-1344" title="" class="ltx_ref ltx_href">10.21437/Interspeech.2021-1344</a>
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Li-Wei Chen and Alexander Rudnicky
</span>
<span class="ltx_bibblock">“Exploring Wav2vec 2.0 Fine Tuning for Improved Speech Emotion Recognition”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023, pp. 1–5
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/ICASSP49357.2023.10095036" title="" class="ltx_ref ltx_href">10.1109/ICASSP49357.2023.10095036</a>
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Leslie N. Smith
</span>
<span class="ltx_bibblock">“A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay”, 2018
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/1803.09820" title="" class="ltx_ref ltx_href">1803.09820 [cs.LG]</a>
</span>
</li>
</ul>
</section>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.00211" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.00212" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.00212">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.00212" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.00213" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 14:49:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
