<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.15667] Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers</title><meta property="og:description" content="Recent advancements in deep learning techniques have sparked performance boosts in various real-world applications including disease diagnosis based on multi-modal medical data. Cough sound data-based respiratory disea‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.15667">

<!--Generated on Thu Sep  5 17:39:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Cough sound,  COPD,  Chronic Obstructive Pulmonary Disease,  COVID-19,  Deep learning,  Respiratory disease screening
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\floatsetup</span>
<p id="p1.2" class="ltx_p">[table]capposition=top


</p>
</div>
<h1 class="ltx_title ltx_title_document">Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qian¬†Wang, Zhaoyang¬†Bu,
Jiaxuan¬†Mao,
Wenyu¬†Zhu,
Jingya¬†Zhao,
Wei¬†Du,
Guochao¬†Shi,
Min¬†Zhou,
<br class="ltx_break">Si¬†Chen*,
Jieming¬†Qu
</span><span class="ltx_author_notes">Q. Wang, Z. Bu, J. Mao, W. Zhu and S. Chen are with Luca Healthcare, Shanghai, China. (E-mail: qian.wang173@hotmail.com, {buzhaoyang, mary.mao, wendy.zhu, echo.chen}@lucahealthcare.com)J. Zhao, W. Du, G. Shi, M. Zhou and J. Qu are with the Department of Pulmonary and Critical Care Medicine, Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, 200025, China; Institute of Respiratory Disease, Shanghai Jiaotong University School of Medicine, Shanghai, China; Shanghai Key Laboratory of Emergency Prevention, Diagnosis and Treatment of Respiratory Infectious Disease, Shanghai, China. (E-mail: jingya2010@126.com, duweiwilson@qq.com, shiguochao@hotmail.com, doctor_zhou_99@163.com, jmqu0906@163.com)* Corresponding authors.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Recent advancements in deep learning techniques have sparked performance boosts in various real-world applications including disease diagnosis based on multi-modal medical data. Cough sound data-based respiratory disease (e.g., COVID-19 and Chronic Obstructive Pulmonary Disease) diagnosis has also attracted much attention. However, existing works usually utilise traditional machine learning or deep models of moderate scales. On the other hand, the developed approaches are trained and evaluated on small-scale data due to the difficulty of curating and annotating clinical data on scale. To address these issues in prior works, we create a unified framework to evaluate various deep models from lightweight Convolutional Neural Networks (e.g., ResNet18) to modern vision transformers and compare their performance in respiratory disease classification. Based on the observations from such an extensive empirical study, we propose a novel algorithm <span id="id1.id1.1" class="ltx_text ltx_font_italic">Cough Search</span> for cough-based disease classification based on both self-supervised and supervised learning on a large-scale cough data set. Experimental results demonstrate our proposed approach outperforms prior arts consistently on two benchmark datasets for COVID-19 diagnosis and a proprietary dataset for COPD/non-COPD classification with an AUROC of 92.5%.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Cough sound, COPD, Chronic Obstructive Pulmonary Disease, COVID-19, Deep learning, Respiratory disease screening

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Respiratory diseases such as COVID-19 and COPD are widespread in populations and early diagnosis is imperative for disease control and human well-being. According to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, COPD was the third fatal disease in the world in 2022 and is expected to remain the third
cause of death through 2050 at the global level. The current diagnosis of COPD requires measurements of lung functions using clinical equipment in hospitals which is expensive and inconvenient for mass screening. As a result, a significant fraction of COPD patients are undiagnosed in China and are exposed to risks of developing into fatal cancers.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The advancement of machine learning and deep learning techniques enables digital diagnosis for various diseases with satisfying accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. These data-driven techniques can discover hidden patterns in patient‚Äôs medical data and connect them with disease labels of interest. In general, machines are expected to understand medical data at an equivalent level as experts for disease diagnosis after being exposed to large-scale high-quality data for training.
Nowadays, various medical data modalities have been investigated for disease diagnosis. Comprehensive surveys have been made for deep learning techniques used in medical applications using medical imagery <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, physiological data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, electronic health records <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and audio data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Among many respiratory sounds such as wheezing, crackles, and breathing, cough sounds have been used to diagnose respiratory diseases in a long history as cough sounds contain indicative patterns closely related to pathomorphological alterations in the respiratory system. Researchers have tried to develop various digital methods for cough sound-based respiratory disease diagnosis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. However, most existing studies use traditional machine learning methods and handcrafted features extracted by classic audio signal processing techniques or relatively lightweight deep models. This is mainly caused by the lack of large-scale cough data sets to pre-train or even fine-tune modern deep models like wav2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. For the same reason, the performance reported on small datasets may be biased and unreliable.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Recently, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> released a series of respiratory acoustic foundation models which are based on the latest self-supervised learning and are pre-trained on large-scale unlabeled respiratory sound data. In addition, the pre-trained foundation models are evaluated on a curated list of downstream tasks for respiratory sound classification or regression. The benchmarking in this work compares several modern deep models originally designed for audio event classification and demonstrates the superiority of their proposed models which are pre-trained specifically on respiratory sound data. However, their released models are based on backbone models of moderate sizes compared with the most advanced ones for audio event or image classification.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this work, we focus on cough sound data and investigate the best practices for improving the accuracy of cough-based respiratory disease diagnosis. Motivated by existing works in general audio and image classification, we raise the following research questions in this study:
(1) Which pre-trained models are most appropriate for cough-based respiratory disease classification in terms of accuracy and efficiency?
(2) What are the best practices for fine-tuning a pre-trained model for enhanced cough-based respiratory disease classification?
(3) What are the best practices of model ensemble and selection for optimal classification performance?</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.15667/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="194" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overview pre-training and fine-tuning pipelines using various datasets, backbone models and downstream tasks in this study.</figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We conduct extensive empirical studies to answer these research questions and propose a novel cough-based respiratory disease classification approach <span id="S1.p6.1.1" class="ltx_text ltx_font_italic">Cough Search</span>. The contributions of our work can be summarised as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We create a unified framework to investigate the cough-based disease classification performance of various pre-trained models belonging to three categories: ImageNet-pre-trained models, audio data pre-trained models and respiratory sound data pre-trained models.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose an approach to cough-based respiratory disease classification based on both self-supervised learning and supervised learning on a large-scale cough data set.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We conduct extensive experiments on three datasets (two public datasets for COVID-19 classification and one proprietary dataset for COPD/non-COPD classification) and the experimental results demonstrate our proposed approach outperforms all others consistently on three tasks.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we review existing works closely related to this study. Specifically, we first review the latest deep learning approaches to audio event classification as our employed method is based on and adapted from models for general audio event classification tasks. Subsequently, we review recent works on cough sound classification and discuss the limitations of commonly used approaches to cough sound classification for respiratory disease diagnosis.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Audio event classification</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Audio event classification is a well-formulated research task that has attracted significant attention in the community. Deep learning models have dominated state-of-the-art approaches to this task in recent years. These approaches follow a similar framework in which the raw audio data are converted to log-mel-spectrogram images. Hence, innovations in image classification models can also benefit the audio event classification tasks by proper transfer learning. Such transfer learning is enabled by fine-tuning the pre-trained image classification models on large-scale audio datasets like AudioSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">According to the model architectures, existing approaches to audio event classification in the literature can be categorized into two groups: Convolutional Neural Networks (CNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Whilst early works employing ImageNet-pretrained CNN models on audio spectrograms achieved competitive accuracy, they have been outperformed by transformer-based models. Audio Spectrogram Transformer (AST) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> was one of the first to apply vision transformers to audio event classification and performed superior to their CNN counterparts. Koutini et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> proposed the PaSST series by evaluating different variants of tailored ViT models and efficient training strategies for audio event classification. These works vary from one another in their employed backbone architectures and audio data preprocessing but share a similar transfer learning framework in which the ImageNet-pre-trained models are fine-tuned on audio spectrogram data for downstream tasks.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">With access to large-scale audio datasets like AudioSet-2M, more recent works turn to self-supervised learning for enhanced audio representation learning. A framework consisting of self-supervised learning and supervised fine-tuning has been employed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and leads to state-of-the-art methods for audio event classification in various downstream tasks.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">The aforementioned models are usually evaluated on several benchmark datasets for audio event classification. Those performing well in general audio classification tasks are expected to perform well in a specific downstream task like cough sound classification. We believe a thorough evaluation is imperative for choosing the most appropriate solutions to the specific cough sound classification task in practice. More recently, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> made a thorough evaluation of self-supervised learning models across several respiratory sound classification tasks, however, their work aims at a wider scope than cough data classification and their released models were only pre-trained on respiratory sound data which lead to sub-optimal performance as observed in our comparative study.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Cough sound classification</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Cough sound-based respiratory disease classification has attracted much attention. Researchers have explored the possibility and efficacy of using cough data to diagnose various respiratory diseases including tuberculosis (TB) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, Asthma <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, pneumonic infections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and COVID-19 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">However, most of these prior works use traditional machine learning approaches such as logistic regression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, support vector machine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, tree-based models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, multi-layer perceptrons <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and classical convolutional neural networks like ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. The evaluations were usually made on small private datasets, preventing a direct fair comparison across different works and making the practical use of proposed methods difficult.
For example, Pahar et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> evaluated various traditional machine learning methods for cough-based TB classification and the best logistic regression achieves an area under the receiver operator curve (AUROC) of 0.94 using 23 features selected from a set of 78 high-resolution mel-frequency cepstral coefficients. Later, the same group employed the Resnet50 classifier, and discriminated between the COVID-19 positive and the healthy coughs with an area under the ROC curve (AUC) of 0.98 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Along with the release of large-scale cough data sets including COUGHVID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> which contains over 25,000 crowdsourced cough recordings and Coswara <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> which contains more than 7,000 audio samples from around 1,000 participants for COVID-19 diagnosis. Attempts have also been made using modern deep-learning models for cough classification. Approaches falling into this category preprocess the raw audio data into log-mel spectrogram image data so that they can be fed into deep models originally designed for image classification. Xue et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> propose a novel self-supervised learning framework for COVID-19 cough classification. A vision transformer (ViT) is firstly trained on unlabeled cough data in a self-supervised learning manner and the pre-trained model is subsequently fine-tuned on the downstream classification task for COVID-19 screening. Valdes et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> also employ a ViT-based model Audio Spectrogram Transformer (AST) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> for cough signal feature extraction towards the classification cough types (e.g., dry, wet, whooping, etc.). The employed AST was pre-trained on a large-scale image dataset ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and a large-scale audio dataset AudioSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> subsequently. However, features extracted from the pre-trained models are directly used in the downstream task. We believe the data distribution gap between general audio data (e.g., those in AudioSet) and cough data will restrict the capabilities of deep models without proper transfer learning. To address this limitation, in this study, we further fine-tune the ViT-based deep models on cough data to enhance their representation learning from cough data to discover disease signatures underlying cough sound data.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Although most existing works use the off-the-shelf deep models, Dentamaro et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> proposed a novel deep model, dubbed AUCO ResNet, by designing a trainable Mel-like spectrogram layer able to finetune the Mel-like-Spectrogram for capturing relevant time-frequency information. However, it is unclear if such a layer can still benefit ViT-based models when they are pre-trained and fine-tuned on large-scale audio datasets (e.g., AudioSet).</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Distinct from prior works on cough-based respiratory disease classification, in this work, we follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and use modern deep learning techniques adapted from the state-of-the-art deep models in audio event classification (c.f. <a href="#S2.SS1" title="II-A Audio event classification ‚Ä£ II Related Work ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>). The conclusions drawn from our experiments are expected to provide insights into the practical use of cough data for respiratory disease diagnosis.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Method</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we describe the process involved in developing a deep model for cough sound classification. Three steps are illustrated in Figure <a href="#S3.F2" title="Figure 2 ‚Ä£ III Method ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>: audio data preprocessing (Figure <a href="#S3.F2" title="Figure 2 ‚Ä£ III Method ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>a), self-supervised learning (Figure <a href="#S3.F2" title="Figure 2 ‚Ä£ III Method ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>b) and supervised fine-tuning (Figure <a href="#S3.F2" title="Figure 2 ‚Ä£ III Method ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>c). Whilst the audio data preprocessing step and the supervised fine-tuning step are required by all methods investigated in this study, the self-supervised learning step is only involved in the methods belonging to the respiratory sound data-based pre-training approaches.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2408.15667/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="205" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustrations of the proposed approach for cough sound-based respiratory disease classification: (a) cough data preprocessing, (b) self-supervised training and (c) supervised fine-tuning on downstream tasks.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Cough sound segmentation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Collecting multiple cough sounds from one subject is practically useful for robust classification results. For this reason, a cough sound segmentation algorithm is needed to segment individual cough sounds from an audio recording. We propose a cough segmentation algorithm based on signal processing and hand-crafted rules.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">A cough typically consists of two or three phases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. In the first phase, a rapid burst of air breaks through the glottis. In the second phase, the glottis is fully open, allowing air to be steadily expelled from the lungs. Finally, there is possibly a phase where the airflow decreases as the glottis gradually closes.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Our pilot experimental results demonstrate that precise cough onset and offset localisation are not necessary for the classification task. Therefore the cough sound segmentation algorithm boils down to the cough onset detection. A cough event is segmented starting from its detected onset and continues until the segment reaches a predetermined duration.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_italic">Cough onset detection</span>:
Initially, the audio signal undergoes a Short-Time Fourier Transform (STFT) with a hop length of 0.016 seconds and a window length of 0.021 seconds, utilizing the Hanning window function. Subsequently, the magnitude spectrum is derived by taking the absolute value of the STFT coefficients. Next, the magnitude spectrum is integrated across the 120Hz-8000Hz frequency band to yield a univariate energy sequence. The ratio of the current frame‚Äôs energy to that of the preceding frame is computed, and its logarithm is taken to generate a sequence representing the rate of energy change. This sequence is then smoothed using a Butterworth low-pass filter. Peaks in the energy change rate are identified by applying a threshold of 100 to this sequence. For each detected peak in energy change rate, the corresponding energy peak is located within a window of 5 frames centred on the peak. Ultimately, the cough onset is identified as occurring two frames before the located energy peak positions.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Spectrogram generation</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">It is <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">de facto</span> standard to convert raw audio data into 2D spectrogram images for audio event classification using image classification deep models. The generation of spectrogram images is based on STFT and the choice of optimal hyper-parameters is coupled with the type of deep models employed for the classification.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Pre-trained models</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In the deep learning era, a common practice is to use a pre-trained deep model and fine-tune it on the downstream task. It is crucial to choose an appropriate pre-trained model for good performance in the downstream tasks. For cough sound spectrogram-based disease classification problems, the candidate pre-trained models can be categorised into three groups according to the pre-training data modalities. In this study, three types of pre-trained models are considered: ImageNet-pre-trained models, audio data pre-trained models and respiratory sound data pre-trained models</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">ImageNet-pre-trained models have been widely used in a variety of computer vision tasks due to the fact they are extensively studied, rapidly evolved and easily accessible. These models are pre-trained on ImageNet and/or other large-scale natural image datasets of this kind. Prior works have proved the benefit of using such pre-trained models on audio classification tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> provided the models are properly fine-tuned.
As ImageNet-pre-trained models require 3-channel images as inputs, we duplicate the generated 1-channel spectrogram three times to form 3-channel images. For models requiring specified input image sizes (e.g., <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mn id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">√ó</mo><mn id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">224</cn><cn type="integer" id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">224\times 224</annotation></semantics></math>), we resize the images as per requirement.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Audio data pre-trained models close the gap between natural images and audio spectrogram images by pre-training deep neural networks on large-scale audio data. During pre-training, the models take audio spectrogram images as the input and hence can be directly applied to downstream tasks for respiratory sound classification. As the pre-trained models may have employed different parameters to generate the spectrogram, the same process for spectrogram generation as that used during pre-training must be employed during fine-tuning.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Respiratory sound data pre-trained models further close the gap between the general audio spectrogram and the respiratory sound spectrogram. Similar to the audio data pre-trained models, these models can be directly fine-tuned on downstream tasks once the classification head is adapted to fit the number of classes.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">A comparative study will be presented in the section on experiments with typical pre-trained models selected from three categories for empirical evaluations.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Model fine-tuning on cough data</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">To adapt the pre-trained foundation models to a specific downstream task (i.e. cough sound classification in our case) which usually has relatively less labelled data for training, we employ different training paradigms: supervised training and self-supervised training. One individual or a hybrid training strategy can be chosen for the best performance based on the type of training data available in a specific application scenario.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS4.SSS1.5.1.1" class="ltx_text">III-D</span>1 </span>Supervised training</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">In most cases, a supervised training strategy should be used to fine-tune the pre-trained models and adapt them to the downstream task. For this purpose, labelled data are required and the label space is usually different from that in the pre-training phase. As a result, one needs to replace the classification head in the model for fine-tuning.</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.1" class="ltx_p">The classification heads in modern deep learning architectures are usually implemented with a series of fully connected layers together with non-linear activation and normalisation layers. Whilst the classification heads can vary in the number of layers and the dimension of the input layer (determined by the dimension of the features from the backbone of the pre-trained model), the last layer should always consist of output neurons equalling the number of labels in specific downstream tasks.</p>
</div>
<div id="S3.SS4.SSS1.p3" class="ltx_para">
<p id="S3.SS4.SSS1.p3.1" class="ltx_p"><span id="S3.SS4.SSS1.p3.1.1" class="ltx_text ltx_font_bold">SAM</span>: Sharpness-Aware Minimization (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> has been proven as an effective strategy for improving the model generalisation and training efficiency. It seeks parameters that lie in neighbourhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. Used together with optimizers such as Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> during fine-tuning, it is expected to improve the performance of pre-trained models on the downstream tasks.</p>
</div>
<div id="S3.SS4.SSS1.p4" class="ltx_para">
<p id="S3.SS4.SSS1.p4.1" class="ltx_p"><span id="S3.SS4.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Data augmentation</span>: Data augmentation is a commonly used strategy during supervised training to improve the model generalisation and performance. A two-stage data augmentation strategy is employed on the fly during supervised training. In the first stage, we use the Python library <span id="S3.SS4.SSS1.p4.1.2" class="ltx_text ltx_font_italic">audiomentations</span> <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/iver56/audiomentations</span></span></span> and apply three types of data augmentation to the raw audio data: adding Gaussian noises, multiplying the audio by a random gain factor and shifting the pitch up or down without changing the tempo.
In the second stage, the data augmentation is applied to the spectrogram images. We use <span id="S3.SS4.SSS1.p4.1.3" class="ltx_text ltx_font_italic">specAugment</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and <span id="S3.SS4.SSS1.p4.1.4" class="ltx_text ltx_font_italic">mixup</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> methods to augment the training data in each mini-batch on the fly during training. Specifically, three types of augmentation are applied to the log mel spectrogram images: time warping, frequency masking and time masking.</p>
</div>
<div id="S3.SS4.SSS1.p5" class="ltx_para">
<p id="S3.SS4.SSS1.p5.1" class="ltx_p"><span id="S3.SS4.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Data imbalance</span>
It is common to have unbalanced data in medical applications where positive samples (with diseases) are far less than negative ones (healthy). To combat such an issue, we use a simple yet effective strategy which assigns label-dependent weights when computing the loss. The loss for samples belonging to the lower-represented class (positive) is multiplied by a higher weight. The weight can be calculated as the ratio between negative and positive samples in the training set.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS4.SSS2.5.1.1" class="ltx_text">III-D</span>2 </span>Self-supervised training</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">Adapting pre-trained foundation models to a specific downstream task suffers from out-of-distribution issues when the supervised training data are limited. In such cases, self-supervised training on large-scale unlabeled data before the supervised training can bridge the data distribution gap between those used in pre-training and fine-tuning. In our targeted use cases, the models pre-trained either on ImageNet or AudioSet may suffer the out-of-distribution issue when applied to the cough data. To mitigate the performance degradation caused by the distribution gap, we use a two-stage fine-tuning pipeline (c.f. Fig. <a href="#S3.F2" title="Figure 2 ‚Ä£ III Method ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) consisting of a self-supervised training stage followed by supervised training on the downstream tasks.</p>
</div>
<div id="S3.SS4.SSS2.p2" class="ltx_para">
<p id="S3.SS4.SSS2.p2.1" class="ltx_p">We follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and employ the teacher-student framework (c.f. Fig. <a href="#S3.F2" title="Figure 2 ‚Ä£ III Method ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>b) for self-supervised training on large-scale unlabeled cough data. The teacher and the student models share the same architecture of a vision transformer. The student model weights are updated normally by gradient descent whilst the teach model weights are updated by the Exponential Moving Average (EMA) strategy. The objective of training the student model is composed of a global loss and a local loss. Both are implemented by the Mean Squared Error (MSE) loss. The global loss aims to align the final representations output by the student and teacher models.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="L_{local}=||\bm{X}_{s}-\bm{F}_{t}||_{2}^{2}" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">L</mi><mrow id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1" xref="S3.E1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1a" xref="S3.E1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.3.3.4" xref="S3.E1.m1.1.1.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1b" xref="S3.E1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.3.3.5" xref="S3.E1.m1.1.1.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1c" xref="S3.E1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.3.3.6" xref="S3.E1.m1.1.1.3.3.6.cmml">l</mi></mrow></msub><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><msubsup id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.1.cmml">‚Äñ</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml">ùëø</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">s</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml">ùë≠</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">2</mn><mn id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml">2</mn></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">ùêø</ci><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><times id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.1"></times><ci id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2">ùëô</ci><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">ùëú</ci><ci id="S3.E1.m1.1.1.3.3.4.cmml" xref="S3.E1.m1.1.1.3.3.4">ùëê</ci><ci id="S3.E1.m1.1.1.3.3.5.cmml" xref="S3.E1.m1.1.1.3.3.5">ùëé</ci><ci id="S3.E1.m1.1.1.3.3.6.cmml" xref="S3.E1.m1.1.1.3.3.6">ùëô</ci></apply></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2">ùëø</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3">ùë†</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2">ùë≠</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3">ùë°</ci></apply></apply></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">L_{local}=||\bm{X}_{s}-\bm{F}_{t}||_{2}^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="L_{global}=||\bm{c}_{t}-\bm{f}_{s}||_{2}^{2}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">L</mi><mrow id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.1" xref="S3.E2.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.1a" xref="S3.E2.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E2.m1.1.1.3.3.4" xref="S3.E2.m1.1.1.3.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.1b" xref="S3.E2.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E2.m1.1.1.3.3.5" xref="S3.E2.m1.1.1.3.3.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.1c" xref="S3.E2.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E2.m1.1.1.3.3.6" xref="S3.E2.m1.1.1.3.3.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.1d" xref="S3.E2.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E2.m1.1.1.3.3.7" xref="S3.E2.m1.1.1.3.3.7.cmml">l</mi></mrow></msub><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><msubsup id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.1.cmml">‚Äñ</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2.cmml">ùíÑ</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">‚àí</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml">ùíá</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.cmml">s</mi></msub></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml">2</mn><mn id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">2</mn></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">ùêø</ci><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><times id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2">ùëî</ci><ci id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3">ùëô</ci><ci id="S3.E2.m1.1.1.3.3.4.cmml" xref="S3.E2.m1.1.1.3.3.4">ùëú</ci><ci id="S3.E2.m1.1.1.3.3.5.cmml" xref="S3.E2.m1.1.1.3.3.5">ùëè</ci><ci id="S3.E2.m1.1.1.3.3.6.cmml" xref="S3.E2.m1.1.1.3.3.6">ùëé</ci><ci id="S3.E2.m1.1.1.3.3.7.cmml" xref="S3.E2.m1.1.1.3.3.7">ùëô</ci></apply></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.2">ùíÑ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2.3">ùë°</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2">ùíá</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3">ùë†</ci></apply></apply></apply><cn type="integer" id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">L_{global}=||\bm{c}_{t}-\bm{f}_{s}||_{2}^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.SSS2.p3" class="ltx_para">
<p id="S3.SS4.SSS2.p3.8" class="ltx_p">where <math id="S3.SS4.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\bm{F}_{t}\in\mathbb{R}^{P^{\prime}\times M\times E}" display="inline"><semantics id="S3.SS4.SSS2.p3.1.m1.1a"><mrow id="S3.SS4.SSS2.p3.1.m1.1.1" xref="S3.SS4.SSS2.p3.1.m1.1.1.cmml"><msub id="S3.SS4.SSS2.p3.1.m1.1.1.2" xref="S3.SS4.SSS2.p3.1.m1.1.1.2.cmml"><mi id="S3.SS4.SSS2.p3.1.m1.1.1.2.2" xref="S3.SS4.SSS2.p3.1.m1.1.1.2.2.cmml">ùë≠</mi><mi id="S3.SS4.SSS2.p3.1.m1.1.1.2.3" xref="S3.SS4.SSS2.p3.1.m1.1.1.2.3.cmml">t</mi></msub><mo id="S3.SS4.SSS2.p3.1.m1.1.1.1" xref="S3.SS4.SSS2.p3.1.m1.1.1.1.cmml">‚àà</mo><msup id="S3.SS4.SSS2.p3.1.m1.1.1.3" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.cmml"><mi id="S3.SS4.SSS2.p3.1.m1.1.1.3.2" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.SS4.SSS2.p3.1.m1.1.1.3.3" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.cmml"><msup id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2.cmml"><mi id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2.2" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2.2.cmml">P</mi><mo id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2.3" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2.3.cmml">‚Ä≤</mo></msup><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.1" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.3" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.3.cmml">M</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.1a" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.1.cmml">√ó</mo><mi id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.4" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.4.cmml">E</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p3.1.m1.1b"><apply id="S3.SS4.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1"><in id="S3.SS4.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.1"></in><apply id="S3.SS4.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p3.1.m1.1.1.2.1.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS4.SSS2.p3.1.m1.1.1.2.2.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.2.2">ùë≠</ci><ci id="S3.SS4.SSS2.p3.1.m1.1.1.2.3.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.2.3">ùë°</ci></apply><apply id="S3.SS4.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p3.1.m1.1.1.3.1.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS4.SSS2.p3.1.m1.1.1.3.2.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.2">‚Ñù</ci><apply id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3"><times id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.1"></times><apply id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2.1.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2">superscript</csymbol><ci id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2.2.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2.2">ùëÉ</ci><ci id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2.3.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.2.3">‚Ä≤</ci></apply><ci id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.3">ùëÄ</ci><ci id="S3.SS4.SSS2.p3.1.m1.1.1.3.3.4.cmml" xref="S3.SS4.SSS2.p3.1.m1.1.1.3.3.4">ùê∏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p3.1.m1.1c">\bm{F}_{t}\in\mathbb{R}^{P^{\prime}\times M\times E}</annotation></semantics></math> are the local patch representations output by all the transformer blocks in the teacher model and <math id="S3.SS4.SSS2.p3.2.m2.1" class="ltx_Math" alttext="\bm{X}_{s}\in\mathbb{R}^{P^{\prime}\times M\times E}" display="inline"><semantics id="S3.SS4.SSS2.p3.2.m2.1a"><mrow id="S3.SS4.SSS2.p3.2.m2.1.1" xref="S3.SS4.SSS2.p3.2.m2.1.1.cmml"><msub id="S3.SS4.SSS2.p3.2.m2.1.1.2" xref="S3.SS4.SSS2.p3.2.m2.1.1.2.cmml"><mi id="S3.SS4.SSS2.p3.2.m2.1.1.2.2" xref="S3.SS4.SSS2.p3.2.m2.1.1.2.2.cmml">ùëø</mi><mi id="S3.SS4.SSS2.p3.2.m2.1.1.2.3" xref="S3.SS4.SSS2.p3.2.m2.1.1.2.3.cmml">s</mi></msub><mo id="S3.SS4.SSS2.p3.2.m2.1.1.1" xref="S3.SS4.SSS2.p3.2.m2.1.1.1.cmml">‚àà</mo><msup id="S3.SS4.SSS2.p3.2.m2.1.1.3" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.cmml"><mi id="S3.SS4.SSS2.p3.2.m2.1.1.3.2" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.SS4.SSS2.p3.2.m2.1.1.3.3" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.cmml"><msup id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2.cmml"><mi id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2.2" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2.2.cmml">P</mi><mo id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2.3" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2.3.cmml">‚Ä≤</mo></msup><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.1" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.1.cmml">√ó</mo><mi id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.3" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.3.cmml">M</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.1a" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.1.cmml">√ó</mo><mi id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.4" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.4.cmml">E</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p3.2.m2.1b"><apply id="S3.SS4.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1"><in id="S3.SS4.SSS2.p3.2.m2.1.1.1.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.1"></in><apply id="S3.SS4.SSS2.p3.2.m2.1.1.2.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p3.2.m2.1.1.2.1.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS4.SSS2.p3.2.m2.1.1.2.2.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.2.2">ùëø</ci><ci id="S3.SS4.SSS2.p3.2.m2.1.1.2.3.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.2.3">ùë†</ci></apply><apply id="S3.SS4.SSS2.p3.2.m2.1.1.3.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p3.2.m2.1.1.3.1.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS4.SSS2.p3.2.m2.1.1.3.2.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.2">‚Ñù</ci><apply id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3"><times id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.1.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.1"></times><apply id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2.1.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2">superscript</csymbol><ci id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2.2.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2.2">ùëÉ</ci><ci id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2.3.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.2.3">‚Ä≤</ci></apply><ci id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.3.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.3">ùëÄ</ci><ci id="S3.SS4.SSS2.p3.2.m2.1.1.3.3.4.cmml" xref="S3.SS4.SSS2.p3.2.m2.1.1.3.3.4">ùê∏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p3.2.m2.1c">\bm{X}_{s}\in\mathbb{R}^{P^{\prime}\times M\times E}</annotation></semantics></math> is the decoder output which aims to approximate the local representations of the masked patches based on the representations of unmasked patches from the student model; <math id="S3.SS4.SSS2.p3.3.m3.1" class="ltx_Math" alttext="P^{\prime}" display="inline"><semantics id="S3.SS4.SSS2.p3.3.m3.1a"><msup id="S3.SS4.SSS2.p3.3.m3.1.1" xref="S3.SS4.SSS2.p3.3.m3.1.1.cmml"><mi id="S3.SS4.SSS2.p3.3.m3.1.1.2" xref="S3.SS4.SSS2.p3.3.m3.1.1.2.cmml">P</mi><mo id="S3.SS4.SSS2.p3.3.m3.1.1.3" xref="S3.SS4.SSS2.p3.3.m3.1.1.3.cmml">‚Ä≤</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p3.3.m3.1b"><apply id="S3.SS4.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS4.SSS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p3.3.m3.1.1.1.cmml" xref="S3.SS4.SSS2.p3.3.m3.1.1">superscript</csymbol><ci id="S3.SS4.SSS2.p3.3.m3.1.1.2.cmml" xref="S3.SS4.SSS2.p3.3.m3.1.1.2">ùëÉ</ci><ci id="S3.SS4.SSS2.p3.3.m3.1.1.3.cmml" xref="S3.SS4.SSS2.p3.3.m3.1.1.3">‚Ä≤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p3.3.m3.1c">P^{\prime}</annotation></semantics></math> is the number masked patches in the input spectrogram to the student model and <math id="S3.SS4.SSS2.p3.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS4.SSS2.p3.4.m4.1a"><mi id="S3.SS4.SSS2.p3.4.m4.1.1" xref="S3.SS4.SSS2.p3.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p3.4.m4.1b"><ci id="S3.SS4.SSS2.p3.4.m4.1.1.cmml" xref="S3.SS4.SSS2.p3.4.m4.1.1">ùëÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p3.4.m4.1c">M</annotation></semantics></math> is the number of transformer blocks; <math id="S3.SS4.SSS2.p3.5.m5.1" class="ltx_Math" alttext="\bm{c}_{s}\in\mathbb{R}^{1\times E}" display="inline"><semantics id="S3.SS4.SSS2.p3.5.m5.1a"><mrow id="S3.SS4.SSS2.p3.5.m5.1.1" xref="S3.SS4.SSS2.p3.5.m5.1.1.cmml"><msub id="S3.SS4.SSS2.p3.5.m5.1.1.2" xref="S3.SS4.SSS2.p3.5.m5.1.1.2.cmml"><mi id="S3.SS4.SSS2.p3.5.m5.1.1.2.2" xref="S3.SS4.SSS2.p3.5.m5.1.1.2.2.cmml">ùíÑ</mi><mi id="S3.SS4.SSS2.p3.5.m5.1.1.2.3" xref="S3.SS4.SSS2.p3.5.m5.1.1.2.3.cmml">s</mi></msub><mo id="S3.SS4.SSS2.p3.5.m5.1.1.1" xref="S3.SS4.SSS2.p3.5.m5.1.1.1.cmml">‚àà</mo><msup id="S3.SS4.SSS2.p3.5.m5.1.1.3" xref="S3.SS4.SSS2.p3.5.m5.1.1.3.cmml"><mi id="S3.SS4.SSS2.p3.5.m5.1.1.3.2" xref="S3.SS4.SSS2.p3.5.m5.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.SS4.SSS2.p3.5.m5.1.1.3.3" xref="S3.SS4.SSS2.p3.5.m5.1.1.3.3.cmml"><mn id="S3.SS4.SSS2.p3.5.m5.1.1.3.3.2" xref="S3.SS4.SSS2.p3.5.m5.1.1.3.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.SSS2.p3.5.m5.1.1.3.3.1" xref="S3.SS4.SSS2.p3.5.m5.1.1.3.3.1.cmml">√ó</mo><mi id="S3.SS4.SSS2.p3.5.m5.1.1.3.3.3" xref="S3.SS4.SSS2.p3.5.m5.1.1.3.3.3.cmml">E</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p3.5.m5.1b"><apply id="S3.SS4.SSS2.p3.5.m5.1.1.cmml" xref="S3.SS4.SSS2.p3.5.m5.1.1"><in id="S3.SS4.SSS2.p3.5.m5.1.1.1.cmml" xref="S3.SS4.SSS2.p3.5.m5.1.1.1"></in><apply id="S3.SS4.SSS2.p3.5.m5.1.1.2.cmml" xref="S3.SS4.SSS2.p3.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p3.5.m5.1.1.2.1.cmml" xref="S3.SS4.SSS2.p3.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS4.SSS2.p3.5.m5.1.1.2.2.cmml" xref="S3.SS4.SSS2.p3.5.m5.1.1.2.2">ùíÑ</ci><ci id="S3.SS4.SSS2.p3.5.m5.1.1.2.3.cmml" xref="S3.SS4.SSS2.p3.5.m5.1.1.2.3">ùë†</ci></apply><apply id="S3.SS4.SSS2.p3.5.m5.1.1.3.cmml" xref="S3.SS4.SSS2.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p3.5.m5.1.1.3.1.cmml" xref="S3.SS4.SSS2.p3.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS4.SSS2.p3.5.m5.1.1.3.2.cmml" xref="S3.SS4.SSS2.p3.5.m5.1.1.3.2">‚Ñù</ci><apply id="S3.SS4.SSS2.p3.5.m5.1.1.3.3.cmml" xref="S3.SS4.SSS2.p3.5.m5.1.1.3.3"><times id="S3.SS4.SSS2.p3.5.m5.1.1.3.3.1.cmml" xref="S3.SS4.SSS2.p3.5.m5.1.1.3.3.1"></times><cn type="integer" id="S3.SS4.SSS2.p3.5.m5.1.1.3.3.2.cmml" xref="S3.SS4.SSS2.p3.5.m5.1.1.3.3.2">1</cn><ci id="S3.SS4.SSS2.p3.5.m5.1.1.3.3.3.cmml" xref="S3.SS4.SSS2.p3.5.m5.1.1.3.3.3">ùê∏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p3.5.m5.1c">\bm{c}_{s}\in\mathbb{R}^{1\times E}</annotation></semantics></math> is the output representation from the student model and <math id="S3.SS4.SSS2.p3.6.m6.1" class="ltx_Math" alttext="\bm{f}_{t}\in\mathbb{R}^{1\times E}" display="inline"><semantics id="S3.SS4.SSS2.p3.6.m6.1a"><mrow id="S3.SS4.SSS2.p3.6.m6.1.1" xref="S3.SS4.SSS2.p3.6.m6.1.1.cmml"><msub id="S3.SS4.SSS2.p3.6.m6.1.1.2" xref="S3.SS4.SSS2.p3.6.m6.1.1.2.cmml"><mi id="S3.SS4.SSS2.p3.6.m6.1.1.2.2" xref="S3.SS4.SSS2.p3.6.m6.1.1.2.2.cmml">ùíá</mi><mi id="S3.SS4.SSS2.p3.6.m6.1.1.2.3" xref="S3.SS4.SSS2.p3.6.m6.1.1.2.3.cmml">t</mi></msub><mo id="S3.SS4.SSS2.p3.6.m6.1.1.1" xref="S3.SS4.SSS2.p3.6.m6.1.1.1.cmml">‚àà</mo><msup id="S3.SS4.SSS2.p3.6.m6.1.1.3" xref="S3.SS4.SSS2.p3.6.m6.1.1.3.cmml"><mi id="S3.SS4.SSS2.p3.6.m6.1.1.3.2" xref="S3.SS4.SSS2.p3.6.m6.1.1.3.2.cmml">‚Ñù</mi><mrow id="S3.SS4.SSS2.p3.6.m6.1.1.3.3" xref="S3.SS4.SSS2.p3.6.m6.1.1.3.3.cmml"><mn id="S3.SS4.SSS2.p3.6.m6.1.1.3.3.2" xref="S3.SS4.SSS2.p3.6.m6.1.1.3.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.SSS2.p3.6.m6.1.1.3.3.1" xref="S3.SS4.SSS2.p3.6.m6.1.1.3.3.1.cmml">√ó</mo><mi id="S3.SS4.SSS2.p3.6.m6.1.1.3.3.3" xref="S3.SS4.SSS2.p3.6.m6.1.1.3.3.3.cmml">E</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p3.6.m6.1b"><apply id="S3.SS4.SSS2.p3.6.m6.1.1.cmml" xref="S3.SS4.SSS2.p3.6.m6.1.1"><in id="S3.SS4.SSS2.p3.6.m6.1.1.1.cmml" xref="S3.SS4.SSS2.p3.6.m6.1.1.1"></in><apply id="S3.SS4.SSS2.p3.6.m6.1.1.2.cmml" xref="S3.SS4.SSS2.p3.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p3.6.m6.1.1.2.1.cmml" xref="S3.SS4.SSS2.p3.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS4.SSS2.p3.6.m6.1.1.2.2.cmml" xref="S3.SS4.SSS2.p3.6.m6.1.1.2.2">ùíá</ci><ci id="S3.SS4.SSS2.p3.6.m6.1.1.2.3.cmml" xref="S3.SS4.SSS2.p3.6.m6.1.1.2.3">ùë°</ci></apply><apply id="S3.SS4.SSS2.p3.6.m6.1.1.3.cmml" xref="S3.SS4.SSS2.p3.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p3.6.m6.1.1.3.1.cmml" xref="S3.SS4.SSS2.p3.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS4.SSS2.p3.6.m6.1.1.3.2.cmml" xref="S3.SS4.SSS2.p3.6.m6.1.1.3.2">‚Ñù</ci><apply id="S3.SS4.SSS2.p3.6.m6.1.1.3.3.cmml" xref="S3.SS4.SSS2.p3.6.m6.1.1.3.3"><times id="S3.SS4.SSS2.p3.6.m6.1.1.3.3.1.cmml" xref="S3.SS4.SSS2.p3.6.m6.1.1.3.3.1"></times><cn type="integer" id="S3.SS4.SSS2.p3.6.m6.1.1.3.3.2.cmml" xref="S3.SS4.SSS2.p3.6.m6.1.1.3.3.2">1</cn><ci id="S3.SS4.SSS2.p3.6.m6.1.1.3.3.3.cmml" xref="S3.SS4.SSS2.p3.6.m6.1.1.3.3.3">ùê∏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p3.6.m6.1c">\bm{f}_{t}\in\mathbb{R}^{1\times E}</annotation></semantics></math> is the average pooling result of <math id="S3.SS4.SSS2.p3.7.m7.1" class="ltx_Math" alttext="\bm{Y}_{t}" display="inline"><semantics id="S3.SS4.SSS2.p3.7.m7.1a"><msub id="S3.SS4.SSS2.p3.7.m7.1.1" xref="S3.SS4.SSS2.p3.7.m7.1.1.cmml"><mi id="S3.SS4.SSS2.p3.7.m7.1.1.2" xref="S3.SS4.SSS2.p3.7.m7.1.1.2.cmml">ùíÄ</mi><mi id="S3.SS4.SSS2.p3.7.m7.1.1.3" xref="S3.SS4.SSS2.p3.7.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p3.7.m7.1b"><apply id="S3.SS4.SSS2.p3.7.m7.1.1.cmml" xref="S3.SS4.SSS2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p3.7.m7.1.1.1.cmml" xref="S3.SS4.SSS2.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS4.SSS2.p3.7.m7.1.1.2.cmml" xref="S3.SS4.SSS2.p3.7.m7.1.1.2">ùíÄ</ci><ci id="S3.SS4.SSS2.p3.7.m7.1.1.3.cmml" xref="S3.SS4.SSS2.p3.7.m7.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p3.7.m7.1c">\bm{Y}_{t}</annotation></semantics></math>; <math id="S3.SS4.SSS2.p3.8.m8.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS4.SSS2.p3.8.m8.1a"><mi id="S3.SS4.SSS2.p3.8.m8.1.1" xref="S3.SS4.SSS2.p3.8.m8.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p3.8.m8.1b"><ci id="S3.SS4.SSS2.p3.8.m8.1.1.cmml" xref="S3.SS4.SSS2.p3.8.m8.1.1">ùê∏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p3.8.m8.1c">E</annotation></semantics></math> is the dimension of the representations. The final loss is a combination of global and local losses.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments and Results</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we conduct thorough experiments to compare several deep models and experimental settings on classifying respiratory diseases based on cough sounds. We demonstrate the effectiveness of modern deep models in such tasks in practical use cases and provide insights into the choice of best practices in different stages of model training and inference.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Evaluation results (AUROC) of ImageNet pre-trained models</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Dataset</span></td>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.1.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<td id="S4.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.2.2.2.1" class="ltx_text" style="font-size:90%;">LucaCough</span></td>
<td id="S4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.2.2.3.1" class="ltx_text" style="font-size:90%;">UK COVID-19</span></td>
<td id="S4.T1.1.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T1.1.2.2.4.1" class="ltx_text" style="font-size:90%;">COUGHVID</span></td>
</tr>
<tr id="S4.T1.1.3.3" class="ltx_tr">
<th id="S4.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.3.3.1.1" class="ltx_text" style="font-size:90%;">resnet10t.c3_in1k</span></th>
<td id="S4.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.3.2.1" class="ltx_text" style="font-size:90%;">84.0</span></td>
<td id="S4.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.3.3.1" class="ltx_text" style="font-size:90%;">64.2</span></td>
<td id="S4.T1.1.3.3.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T1.1.3.3.4.1" class="ltx_text" style="font-size:90%;">56.5</span></td>
</tr>
<tr id="S4.T1.1.4.4" class="ltx_tr">
<th id="S4.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.4.4.1.1" class="ltx_text" style="font-size:90%;">resnet14t.c3_in1k</span></th>
<td id="S4.T1.1.4.4.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.4.2.1" class="ltx_text" style="font-size:90%;">83.9</span></td>
<td id="S4.T1.1.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.4.3.1" class="ltx_text" style="font-size:90%;">58.7</span></td>
<td id="S4.T1.1.4.4.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.1.4.4.4.1" class="ltx_text" style="font-size:90%;">57.8</span></td>
</tr>
<tr id="S4.T1.1.5.5" class="ltx_tr">
<th id="S4.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.5.5.1.1" class="ltx_text" style="font-size:90%;">resnet18.a3_in1k</span></th>
<td id="S4.T1.1.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.5.5.2.1" class="ltx_text" style="font-size:90%;">76.8</span></td>
<td id="S4.T1.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.5.5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">65.6</span></td>
<td id="S4.T1.1.5.5.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.1.5.5.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">58.1</span></td>
</tr>
<tr id="S4.T1.1.6.6" class="ltx_tr">
<th id="S4.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.6.6.1.1" class="ltx_text" style="font-size:90%;">resnet34.a3_in1k</span></th>
<td id="S4.T1.1.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.6.6.2.1" class="ltx_text" style="font-size:90%;">71.2</span></td>
<td id="S4.T1.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.6.6.3.1" class="ltx_text" style="font-size:90%;">62.2</span></td>
<td id="S4.T1.1.6.6.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.1.6.6.4.1" class="ltx_text" style="font-size:90%;">52.8</span></td>
</tr>
<tr id="S4.T1.1.7.7" class="ltx_tr">
<th id="S4.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.7.7.1.1" class="ltx_text" style="font-size:90%;">resnet50.a3_in1k</span></th>
<td id="S4.T1.1.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.7.7.2.1" class="ltx_text" style="font-size:90%;">65.8</span></td>
<td id="S4.T1.1.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.7.7.3.1" class="ltx_text" style="font-size:90%;">60.5</span></td>
<td id="S4.T1.1.7.7.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.1.7.7.4.1" class="ltx_text" style="font-size:90%;">52.8</span></td>
</tr>
<tr id="S4.T1.1.8.8" class="ltx_tr">
<th id="S4.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.8.8.1.1" class="ltx_text" style="font-size:90%;">resnet101.a3_in1k</span></th>
<td id="S4.T1.1.8.8.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.8.8.2.1" class="ltx_text" style="font-size:90%;">66.4</span></td>
<td id="S4.T1.1.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.8.8.3.1" class="ltx_text" style="font-size:90%;">62.9</span></td>
<td id="S4.T1.1.8.8.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.1.8.8.4.1" class="ltx_text" style="font-size:90%;">53.8</span></td>
</tr>
<tr id="S4.T1.1.9.9" class="ltx_tr">
<th id="S4.T1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.9.9.1.1" class="ltx_text" style="font-size:90%;">tiny_vit_5m_224</span></th>
<td id="S4.T1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.9.9.2.1" class="ltx_text" style="font-size:90%;">84.5</span></td>
<td id="S4.T1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.9.9.3.1" class="ltx_text" style="font-size:90%;">62.4</span></td>
<td id="S4.T1.1.9.9.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T1.1.9.9.4.1" class="ltx_text" style="font-size:90%;">52.8</span></td>
</tr>
<tr id="S4.T1.1.10.10" class="ltx_tr">
<th id="S4.T1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.10.10.1.1" class="ltx_text" style="font-size:90%;">tiny_vit_21m_224</span></th>
<td id="S4.T1.1.10.10.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.10.10.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">85.3</span></td>
<td id="S4.T1.1.10.10.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.10.10.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">66.9</span></td>
<td id="S4.T1.1.10.10.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.1.10.10.4.1" class="ltx_text" style="font-size:90%;">57.1</span></td>
</tr>
<tr id="S4.T1.1.11.11" class="ltx_tr">
<th id="S4.T1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.11.11.1.1" class="ltx_text" style="font-size:90%;">xcit_nano_12_p8_224.fb_dist_in1k</span></th>
<td id="S4.T1.1.11.11.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.11.11.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">85.2</span></td>
<td id="S4.T1.1.11.11.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.11.11.3.1" class="ltx_text" style="font-size:90%;">61.7</span></td>
<td id="S4.T1.1.11.11.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.1.11.11.4.1" class="ltx_text" style="font-size:90%;">55.3</span></td>
</tr>
<tr id="S4.T1.1.12.12" class="ltx_tr">
<th id="S4.T1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T1.1.12.12.1.1" class="ltx_text" style="font-size:90%;">xcit_tiny_12_p8_224.fb_in1k</span></th>
<td id="S4.T1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.12.12.2.1" class="ltx_text" style="font-size:90%;">83.4</span></td>
<td id="S4.T1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.12.12.3.1" class="ltx_text" style="font-size:90%;">62.6</span></td>
<td id="S4.T1.1.12.12.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T1.1.12.12.4.1" class="ltx_text" style="font-size:90%;">53.2</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Three datasets are employed in the experiments for a thorough evaluation across different tasks and data distribution.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">UK COVID-19</span> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> is collected to evaluate machine learning models classifying COVID-19 status using multi-modal data including cough sounds. In this study, we use the ‚Äúmatched" training-test split released along with the dataset <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://zenodo.org/records/11167750</span></span></span>. As a result, there are 2,599 COVID+ and 2,599 COVID- participants in the training set and 907 COVID+ and 907 COVID- participants in the test set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. We follow the same data split of task 2 in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and formulate a binary classification task classifying the status of COVID-19 on this dataset.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">The <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">CoughVID</span> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> contains over 25,000 cough sounds labelled with COVID-19 statuses. Following the definition of task 5 in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, we use this binary classification task (COVID-19 vs healthy) as one of the test beds in our study.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Our private <span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">LucaCough</span> dataset is challenging yet more clinically useful as the negative samples cover not only the coughs of healthy subjects but also the coughs of subjects diagnosed with various respiratory diseases which may share similar cough signatures to COPD cough sounds. Specifically, the negative samples in our dataset include those collected from patients diagnosed with asthma, upper/lower respiratory tract infection (URI/LRI), Bronchiectasis and other non-COPD respiratory diseases. The LucaCough dataset consists of cough data from 3000 subjects (272 COPD and 2728 non-COPD subjects) in the training subset and cough data from 651 subjects (66 COPD and 586 non-COPD) in the test set. The cough sounds are recorded via several different mobile phones to make the data more diverse and enable the generalisation capabilities of the developed models. The data are manually annotated by at least three clinical experts with the help of medical records and lung function test readings. Informed written consent was obtained from all participants. The study was approved by the Ruijin Hospital Shanghai Jiaotong University School of Medicine Ethics Committee (No. 2023-199), Ruijin Hospital Luwan Branch Ethics Committee (2023-HXK-V1), Shanghai Jing‚Äôan District Central Hospital Ethics Committee (No. 2023-33), Shanghai Zhabei Central Hospital Ethics Committee (ZBLL2024030401001) and registered at ClinicalTrials.gov (NCT06082791).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Implementation details</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We implement the proposed method in PyTorch and all the experiments are run on a GeForce RTX 4090 GPU. In the self-supervised pre-training stage, we use the default experimental settings used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. For supervised fine-tuning, we use a learning rate of 2e-6 and a batch size of 24 throughout our study. The Adam optimizer is employed if not specified otherwise.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Since all downstream tasks investigated in our experiments are binary classification problems, we use the area under the receiver operator curve (AUROC) as the evaluation metric if not otherwise specified. Multiple runs with different random seeds are conducted for each experiment to get the mean values as reliable results.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Evaluation of ImageNet pre-trained models</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">There exist plenty of deep models pre-trained on ImageNet. In this study, we use the <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">timm</span> library <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/huggingface/pytorch-image-models</span></span></span> as a convenient tool to access various pre-trained vision models in a unified framework. Firstly, we choose the ResNet series from ResNet10 to ResNet101 to investigate how the model complexity of Convolutional Neural Networks (CNN) affects the performance on three downstream tasks. The results are shown in Table <a href="#S4.T1" title="TABLE I ‚Ä£ IV Experiments and Results ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. It is interesting to see models with fewer layers (e.g., resnet10t, resnet14t and resnet18) generally outperform those with more layers consistently on three datasets. The possible reason could be deeper models require more training data to achieve good performance than their shallow counterparts.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Based on the observations of ResNet performance, we conduct follow-up experiments on four small vision transformer models as listed in the bottom part of Table <a href="#S4.T1" title="TABLE I ‚Ä£ IV Experiments and Results ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. These transformer-based models perform comparably well with their CNN counterparts and the best one achieves the highest AUROC on two tasks (i.e. LucaCough and UK COVID-19), the third highest on the COUGHVID dataset.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Experimental results shown in Table <a href="#S4.T1" title="TABLE I ‚Ä£ IV Experiments and Results ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> demonstrate deeper neural networks with more model capacities may not always be better choices for downstream tasks. Our empirical study provides insight into how to choose ImageNet-pre-trained models for downstream tasks when the data distribution gap is large (e.g., natural images and cough audio spectrogram) and the amount of data for fine-tuning is limited.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Evaluation of AudioSet pre-trained models</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In this section, we evaluate the performance of models pre-trained on audio data (e.g., AudioSet-2M). Specifically, we evaluate PaSST-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, EAT-base and EAT-large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> on the benchmark datasets. PaSST-S is based on DeiT-B384 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> which has the same architecture as ViT-B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. EAT-base is based on ViT-B hence both PaSST-S and EAT-base have 86M parameters, 12 Multi-head self-attention (MSA) layers, 12 attention heads and the embedding dimension is 768. EAT-large is based on ViT-L and has 307M parameters, 24 MSA layers, 16 attention heads and the embedding dimension is 1024.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">We use the code and checkpoints released by the authors of these two works and follow their instructions for fine-tuning on the three downstream tasks. All the checkpoints utilised in our experiments were trained on the large-scale AudioSet-2M dataset in a supervised learning way. In addition, the EAT models were also pre-trained on the AudioSet-2M dataset using a self-supervised learning strategy before fine-tuning on the same dataset. To further fine-tune the models on our downstream tasks, the classification head is replaced with a linear layer with two output neurons for the binary classification problems.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">The experimental results are shown in Table <a href="#S4.T2" title="TABLE II ‚Ä£ IV-D Evaluation of AudioSet pre-trained models ‚Ä£ IV Experiments and Results ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. The investigated models perform comparably well on three downstream tasks. On the COUGHVID dataset, EAT-large achieves higher AUROC than the lighter version EAT-base which again outperforms PaSST-S. However, compared with the experimental results shown in Table <a href="#S4.T1" title="TABLE I ‚Ä£ IV Experiments and Results ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, all three audio data pre-trained models perform significantly better than most of the ImageNet-pre-trained ones on three downstream tasks consistently. The comparison between experimental results in Tables <a href="#S4.T1" title="TABLE I ‚Ä£ IV Experiments and Results ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> and <a href="#S4.T2" title="TABLE II ‚Ä£ IV-D Evaluation of AudioSet pre-trained models ‚Ä£ IV Experiments and Results ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> provides clear evidence that pre-training on audio data benefits the classification of respiratory data including cough sounds.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Evaluation results (AUROC) of models pre-trained one audio data (upper part) and respiratory sound data (bottom part)</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Dataset</span></td>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<th id="S4.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.1.2.2.1.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<td id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.2.2.1" class="ltx_text" style="font-size:90%;">LucaCough</span></td>
<td id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.2.3.1" class="ltx_text" style="font-size:90%;">UK COVID-19</span></td>
<td id="S4.T2.1.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T2.1.2.2.4.1" class="ltx_text" style="font-size:90%;">COUGHVID</span></td>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<th id="S4.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T2.1.3.3.1.1" class="ltx_text" style="font-size:90%;">CLAP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.3.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib46" title="" class="ltx_ref">46</a><span id="S4.T2.1.3.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.3.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.3.3.1" class="ltx_text" style="font-size:90%;">64.8</span></td>
<td id="S4.T2.1.3.3.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T2.1.3.3.4.1" class="ltx_text" style="font-size:90%;">59.9</span></td>
</tr>
<tr id="S4.T2.1.4.4" class="ltx_tr">
<th id="S4.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.4.4.1.1" class="ltx_text" style="font-size:90%;">OPERA-CT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.4.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T2.1.4.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T2.1.4.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.4.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.1.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.4.3.1" class="ltx_text" style="font-size:90%;">70.1</span></td>
<td id="S4.T2.1.4.4.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.4.4.4.1" class="ltx_text" style="font-size:90%;">57.8</span></td>
</tr>
<tr id="S4.T2.1.5.5" class="ltx_tr">
<th id="S4.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.5.5.1.1" class="ltx_text" style="font-size:90%;">OPERA-CE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.5.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T2.1.5.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T2.1.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.3.1" class="ltx_text" style="font-size:90%;">62.9</span></td>
<td id="S4.T2.1.5.5.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.5.5.4.1" class="ltx_text" style="font-size:90%;">56.6</span></td>
</tr>
<tr id="S4.T2.1.6.6" class="ltx_tr">
<th id="S4.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.6.6.1.1" class="ltx_text" style="font-size:90%;">OPERA-GT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.6.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T2.1.6.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T2.1.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.6.2.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S4.T2.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.6.3.1" class="ltx_text" style="font-size:90%;">67.7</span></td>
<td id="S4.T2.1.6.6.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.6.6.4.1" class="ltx_text" style="font-size:90%;">55.2</span></td>
</tr>
<tr id="S4.T2.1.7.7" class="ltx_tr">
<th id="S4.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T2.1.7.7.1.1" class="ltx_text" style="font-size:90%;">PaSST-S </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.7.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S4.T2.1.7.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.7.7.2.1" class="ltx_text" style="font-size:90%;">88.2</span></td>
<td id="S4.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.7.7.3.1" class="ltx_text" style="font-size:90%;">70.5</span></td>
<td id="S4.T2.1.7.7.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T2.1.7.7.4.1" class="ltx_text" style="font-size:90%;">56.5</span></td>
</tr>
<tr id="S4.T2.1.8.8" class="ltx_tr">
<th id="S4.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.8.8.1.1" class="ltx_text" style="font-size:90%;">EAT-base </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.8.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S4.T2.1.8.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T2.1.8.8.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.8.2.1" class="ltx_text" style="font-size:90%;">88.2</span></td>
<td id="S4.T2.1.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.8.3.1" class="ltx_text" style="font-size:90%;">70.3</span></td>
<td id="S4.T2.1.8.8.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.8.8.4.1" class="ltx_text" style="font-size:90%;">57.7</span></td>
</tr>
<tr id="S4.T2.1.9.9" class="ltx_tr">
<th id="S4.T2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.9.9.1.1" class="ltx_text" style="font-size:90%;">EAT-large </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.9.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S4.T2.1.9.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T2.1.9.9.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.9.2.1" class="ltx_text" style="font-size:90%;">88.4</span></td>
<td id="S4.T2.1.9.9.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.9.3.1" class="ltx_text" style="font-size:90%;">70.4</span></td>
<td id="S4.T2.1.9.9.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.9.9.4.1" class="ltx_text" style="font-size:90%;">58.9</span></td>
</tr>
<tr id="S4.T2.1.10.10" class="ltx_tr">
<th id="S4.T2.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T2.1.10.10.1.1" class="ltx_text" style="font-size:90%;">OPERA-CT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.10.10.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T2.1.10.10.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T2.1.10.10.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.10.10.2.1" class="ltx_text" style="font-size:90%;">85.2</span></td>
<td id="S4.T2.1.10.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.10.10.3.1" class="ltx_text" style="font-size:90%;">69.4</span></td>
<td id="S4.T2.1.10.10.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T2.1.10.10.4.1" class="ltx_text" style="font-size:90%;">58.4</span></td>
</tr>
<tr id="S4.T2.1.11.11" class="ltx_tr">
<th id="S4.T2.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.11.11.1.1" class="ltx_text" style="font-size:90%;">OPERA-CE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.11.11.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T2.1.11.11.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T2.1.11.11.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.11.2.1" class="ltx_text" style="font-size:90%;">83.0</span></td>
<td id="S4.T2.1.11.11.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.11.3.1" class="ltx_text" style="font-size:90%;">65.5</span></td>
<td id="S4.T2.1.11.11.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.11.11.4.1" class="ltx_text" style="font-size:90%;">57.5</span></td>
</tr>
<tr id="S4.T2.1.12.12" class="ltx_tr">
<th id="S4.T2.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.12.12.1.1" class="ltx_text" style="font-size:90%;">OPERA-GT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.12.12.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T2.1.12.12.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S4.T2.1.12.12.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.12.12.2.1" class="ltx_text" style="font-size:90%;">86.7</span></td>
<td id="S4.T2.1.12.12.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.12.12.3.1" class="ltx_text" style="font-size:90%;">69.2</span></td>
<td id="S4.T2.1.12.12.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.1.12.12.4.1" class="ltx_text" style="font-size:90%;">57.7</span></td>
</tr>
<tr id="S4.T2.1.13.13" class="ltx_tr">
<th id="S4.T2.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">
<span id="S4.T2.1.13.13.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Cough Search</span><span id="S4.T2.1.13.13.1.2" class="ltx_text" style="font-size:90%;"> (Ours)</span>
</th>
<td id="S4.T2.1.13.13.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.13.13.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">91.7</span></td>
<td id="S4.T2.1.13.13.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.13.13.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">71.3</span></td>
<td id="S4.T2.1.13.13.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T2.1.13.13.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">60.7</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.5.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.6.2" class="ltx_text ltx_font_italic">Evaluation of Respiratory sound data pretrained models</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">We conduct experiments to evaluate models pre-trained on respiratory sound data. The models employed in these experiments include those released by Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and EAT models further trained on cough data by ourselves. Firstly, we apply the released pre-trained models to the LucaCough datasets and present the experimental results together with those from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> on the other two datasets. Subsequently, we continue the pertaining of EAT models on a large-scale cough sound dataset. As a result, we have a variant of the pre-trained EAT-large model dubbed <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_italic">Cough Search</span> as it was further trained on cough data. The <span id="S4.SS5.p1.1.2" class="ltx_text ltx_font_italic">Cough Search</span> model is subsequently fine-tuned on downstream tasks to obtain the classification results and compare them with other respiratory sound data pre-trained models (i.e. the OPERA series). To make a fair comparison, we use the official code released by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> to fine-tune the OPERA models on downstream tasks whilst only linear probing results were reported in the original paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">The comparison results are shown in Table <a href="#S4.T2" title="TABLE II ‚Ä£ IV-D Evaluation of AudioSet pre-trained models ‚Ä£ IV Experiments and Results ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> from which several conclusions can be drawn. Firstly, <span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_italic">Cough Search</span> performs the best on three tasks consistently. Particularly, it outperforms EAT-large on three tasks with the AUROC margins ranging from 0.9 to 3.3 percentage points. This proves that continuous pre-training of the model on cough data can further strengthen its capabilities of classifying diseases based on cough sound data. Secondly, the OPERA models, though pre-trained on respiratory sound data in a self-supervised learning manner, perform inferior not only to <span id="S4.SS5.p2.1.2" class="ltx_text ltx_font_italic">Cough Search</span> but also to the models pre-trained on general audio data (i.e. PaSST-S, EAT-base and EAT-large). This may be attributed to the fact of low model capacities or the lack of pre-training on large-scale audio data like AudioSet-2M.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS6.5.1.1" class="ltx_text">IV-F</span> </span><span id="S4.SS6.6.2" class="ltx_text ltx_font_italic">On the effectiveness of SAM</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">In this subsection, we focus on exploring the effectiveness of the SAM optimizer in our particular cough classification tasks. Specifically, we compare experimental results without and with the use of SAM optimizer during fine-tuning whilst keeping all other experimental settings the same. The comparative study is conducted on the LucaCough dataset with three representative pre-trained models: PaSST-S, EAT-base, EAT-large and <span id="S4.SS6.p1.1.1" class="ltx_text ltx_font_italic">Cough Search</span>. The empirical results are shown in Table <a href="#S4.T3" title="TABLE III ‚Ä£ IV-F On the effectiveness of SAM ‚Ä£ IV Experiments and Results ‚Ä£ Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. The experimental results demonstrate the use of SAM improves the AUROC values consistently for all four models.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Experimental results on LucaCough without and with the use of SAM</figcaption>
<table id="S4.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.4.5.1" class="ltx_tr">
<th id="S4.T3.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Model</th>
<td id="S4.T3.4.5.1.2" class="ltx_td ltx_align_center ltx_border_tt">w/o SAM</td>
<td id="S4.T3.4.5.1.3" class="ltx_td ltx_align_center ltx_border_tt">w/ SAM</td>
</tr>
<tr id="S4.T3.1.1" class="ltx_tr">
<th id="S4.T3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">PaSST-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</th>
<td id="S4.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_t">88.2</td>
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_t">89.1 <math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><mo stretchy="false" id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T3.2.2" class="ltx_tr">
<th id="S4.T3.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">EAT-base <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</th>
<td id="S4.T3.2.2.3" class="ltx_td ltx_align_center">88.2</td>
<td id="S4.T3.2.2.1" class="ltx_td ltx_align_center">89.3 <math id="S4.T3.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.2.2.1.m1.1a"><mo stretchy="false" id="S4.T3.2.2.1.m1.1.1" xref="S4.T3.2.2.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.1.m1.1b"><ci id="S4.T3.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T3.3.3" class="ltx_tr">
<th id="S4.T3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">EAT-large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</th>
<td id="S4.T3.3.3.3" class="ltx_td ltx_align_center">88.4</td>
<td id="S4.T3.3.3.1" class="ltx_td ltx_align_center">89.8 <math id="S4.T3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.3.3.1.m1.1a"><mo stretchy="false" id="S4.T3.3.3.1.m1.1.1" xref="S4.T3.3.3.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.1.m1.1b"><ci id="S4.T3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T3.4.4" class="ltx_tr">
<th id="S4.T3.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">
<span id="S4.T3.4.4.2.1" class="ltx_text ltx_font_italic">Cough Search</span> (Ours)</th>
<td id="S4.T3.4.4.3" class="ltx_td ltx_align_center ltx_border_bb">91.7</td>
<td id="S4.T3.4.4.1" class="ltx_td ltx_align_center ltx_border_bb">92.5 <math id="S4.T3.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.4.4.1.m1.1a"><mo stretchy="false" id="S4.T3.4.4.1.m1.1.1" xref="S4.T3.4.4.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.1.m1.1b"><ci id="S4.T3.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusions and Discussion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This study presents a comprehensive evaluation of various deep learning models and their performance in classifying respiratory diseases such as COVID-19 and Chronic Obstructive Pulmonary Disease (COPD) using cough sound data. Our proposed approach, which leverages both self-supervised and supervised learning on a large-scale cough dataset, has demonstrated superior performance on three datasets.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The use of pre-trained models, particularly those pre-trained on respiratory sound data, has shown significant benefits in enhancing the classification of cough sounds. The continuous pre-training of models on cough data, as evidenced by the performance of the <span id="S5.p2.1.1" class="ltx_text ltx_font_italic">Cough Search</span> model, further strengthens the model‚Äôs ability to discern disease signatures from cough sound data.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">While our approach has shown promising results, there is room for further improvement and exploration. Future work could involve the integration of additional data modalities, such as clinical and demographic information, to enhance the predictive power of the models. The development of more sophisticated self-supervised learning techniques tailored to the unique characteristics of cough sounds could potentially uncover more nuanced patterns in the data.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">In conclusion, our study represents a significant step towards reliable and accurate respiratory disease diagnosis using cough sounds. The insights gained from this research have the potential to inform the design of future diagnostic systems, ultimately contributing to improved disease control and human well-being.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K.¬†Mettananda and S.¬†Mettananda, ‚ÄúBurden of disease scenarios for 204 countries and territories, 2022-2050: a forecasting analysis for the global burden of disease study 2021,‚Äù 2024.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
F.¬†Piccialli, V.¬†Di¬†Somma, F.¬†Giampaolo, S.¬†Cuomo, and G.¬†Fortino, ‚ÄúA survey on deep learning in medicine: Why, how and when?‚Äù <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Information Fusion</em>, vol.¬†66, pp. 111‚Äì137, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S.¬†K. Zhou, H.¬†Greenspan, and D.¬†Shen, <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Deep learning for medical image analysis</em>.¬†¬†¬†Academic Press, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
B.¬†Rim, N.-J. Sung, S.¬†Min, and M.¬†Hong, ‚ÄúDeep learning in physiological signal data: A survey,‚Äù <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol.¬†20, no.¬†4, p. 969, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J.¬†R.¬†A. Solares, F.¬†E.¬†D. Raimondi, Y.¬†Zhu, F.¬†Rahimian, D.¬†Canoy, J.¬†Tran, A.¬†C.¬†P. Gomes, A.¬†H. Payberah, M.¬†Zottoli, M.¬†Nazarzadeh <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúDeep learning for electronic health records: A comparative review of multiple deep neural architectures,‚Äù <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">Journal of biomedical informatics</em>, vol. 101, p. 103337, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A.¬†H. Sfayyih, N.¬†Sulaiman, and A.¬†H. Sabry, ‚ÄúA review on lung disease recognition by acoustic signal analysis with deep learning networks,‚Äù <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Journal of big Data</em>, vol.¬†10, no.¬†1, p. 101, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
R.¬†V. Sharan and H.¬†Rahimi-Ardabili, ‚ÄúDetecting acute respiratory diseases in the pediatric population using cough sound features and machine learning: a systematic review,‚Äù <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Journal of Medical Informatics</em>, vol. 176, p. 105093, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A.¬†Ijaz, M.¬†Nabeel, U.¬†Masood, T.¬†Mahmood, M.¬†S. Hashmi, I.¬†Posokhova, A.¬†Rizwan, and A.¬†Imran, ‚ÄúTowards using cough for respiratory disease diagnosis by leveraging artificial intelligence: A survey,‚Äù <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Informatics in Medicine Unlocked</em>, vol.¬†29, p. 100832, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A.¬†Baevski, A.¬†Babu, W.-N. Hsu, and M.¬†Auli, ‚ÄúEfficient self-supervised learning with contextualized target representations for vision, speech and language,‚Äù in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.¬†¬†¬†PMLR, 2023, pp. 1416‚Äì1429.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T.¬†Lin, Y.¬†Wang, X.¬†Liu, and X.¬†Qiu, ‚ÄúA survey of transformers,‚Äù <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">AI open</em>, vol.¬†3, pp. 111‚Äì132, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y.¬†Zhang, T.¬†Xia, J.¬†Han, Y.¬†Wu, G.¬†Rizos, Y.¬†Liu, M.¬†Mosuily, J.¬†Chauhan, and C.¬†Mascolo, ‚ÄúTowards open respiratory acoustic foundation models: Pretraining and benchmarking,‚Äù 2024. [Online]. Available: <a target="_blank" href="https://arxiv.org/abs/2406.16148" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2406.16148</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J.¬†F. Gemmeke, D.¬†P. Ellis, D.¬†Freedman, A.¬†Jansen, W.¬†Lawrence, R.¬†C. Moore, M.¬†Plakal, and M.¬†Ritter, ‚ÄúAudio set: An ontology and human-labeled dataset for audio events,‚Äù in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.¬†¬†¬†IEEE, 2017, pp. 776‚Äì780.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K.¬†Palanisamy, D.¬†Singhania, and A.¬†Yao, ‚ÄúRethinking cnn models for audio classification,‚Äù <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.11154</em>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Q.¬†Kong, Y.¬†Cao, T.¬†Iqbal, Y.¬†Wang, W.¬†Wang, and M.¬†D. Plumbley, ‚ÄúPanns: Large-scale pretrained audio neural networks for audio pattern recognition,‚Äù <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.¬†28, pp. 2880‚Äì2894, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
K.¬†Drossos, S.¬†I. Mimilakis, S.¬†Gharib, Y.¬†Li, and T.¬†Virtanen, ‚ÄúSound event detection with depthwise separable and dilated convolutions,‚Äù in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">2020 International Joint Conference on Neural Networks (IJCNN)</em>.¬†¬†¬†IEEE, 2020, pp. 1‚Äì7.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
V.¬†Abrol and P.¬†Sharma, ‚ÄúLearning hierarchy aware embedding from raw audio for acoustic scene classification,‚Äù <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.¬†28, pp. 1964‚Äì1973, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Y.¬†Gong, Y.-A. Chung, and J.¬†Glass, ‚ÄúAst: Audio spectrogram transformer,‚Äù <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.01778</em>, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
K.¬†Koutini, J.¬†Schl√ºter, H.¬†Eghbal-Zadeh, and G.¬†Widmer, ‚ÄúEfficient training of audio transformers with patchout,‚Äù <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.05069</em>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
X.¬†Li, N.¬†Shao, and X.¬†Li, ‚ÄúSelf-supervised audio teacher-student transformer for both clip-level and frame-level tasks,‚Äù <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 2024.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
W.¬†Chen, Y.¬†Liang, Z.¬†Ma, Z.¬†Zheng, and X.¬†Chen, ‚ÄúEat: Self-supervised pre-training with efficient audio transformer,‚Äù <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.03497</em>, 2024.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
T.¬†Alex, S.¬†Ahmed, A.¬†Mustafa, M.¬†Awais, and P.¬†J. Jackson, ‚ÄúDtf-at: Decoupled time-frequency audio transformer for event classification,‚Äù in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol.¬†38, no.¬†16, 2024, pp. 17‚Äâ647‚Äì17‚Äâ655.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
M.¬†Pahar, M.¬†Klopper, B.¬†Reeve, R.¬†Warren, G.¬†Theron, and T.¬†Niesler, ‚ÄúAutomatic cough classification for tuberculosis screening in a real-world environment,‚Äù <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Physiological measurement</em>, vol.¬†42, no.¬†10, p. 105014, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
M.¬†Sharma, V.¬†Nduba, L.¬†N. Njagi, W.¬†Murithi, Z.¬†Mwongera, T.¬†R. Hawn, S.¬†N. Patel, and D.¬†J. Horne, ‚ÄúTbscreen: A passive cough classifier for tuberculosis screening with a controlled dataset,‚Äù <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Science Advances</em>, vol.¬†10, no.¬†1, p. eadi0282, 2024.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
F.¬†Barata, P.¬†Tinschert, F.¬†Rassouli, C.¬†Steurer-Stey, E.¬†Fleisch, M.¬†A. Puhan, M.¬†Brutsche, D.¬†Kotz, and T.¬†Kowatsch, ‚ÄúAutomatic recognition, segmentation, and sex assignment of nocturnal asthmatic coughs and cough epochs in smartphone audio recordings: observational field study,‚Äù <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Journal of medical Internet research</em>, vol.¬†22, no.¬†7, p. e18082, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
X.¬†Xu, E.¬†Nemati, K.¬†Vatanparvar, V.¬†Nathan, T.¬†Ahmed, M.¬†M. Rahman, D.¬†McCaffrey, J.¬†Kuang, and J.¬†A. Gao, ‚ÄúListen2cough: Leveraging end-to-end deep learning cough detection model to enhance lung health assessment using passively sensed audio,‚Äù <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>, vol.¬†5, no.¬†1, pp. 1‚Äì22, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
P.¬†D. Barua, T.¬†Keles, M.¬†Kuluozturk, M.¬†A. Kobat, S.¬†Dogan, M.¬†Baygin, T.¬†Tuncer, R.-S. Tan, and U.¬†R. Acharya, ‚ÄúAutomated asthma detection in a 1326-subject cohort using a one-dimensional attractive-and-repulsive center-symmetric local binary pattern technique with cough sounds,‚Äù <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Neural Computing and Applications</em>, pp. 1‚Äì15, 2024.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A.¬†Kumar, K.¬†Abhishek, C.¬†Chakraborty, and N.¬†Kryvinska, ‚ÄúDeep learning and internet of things based lung ailment recognition through coughing spectrograms,‚Äù <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol.¬†9, pp. 95‚Äâ938‚Äì95‚Äâ948, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J.¬†Vrindavanam, R.¬†Srinath, H.¬†H. Shankar, and G.¬†Nagesh, ‚ÄúMachine learning based covid-19 cough classification models-a comparative analysis,‚Äù in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">2021 5th International Conference on Computing Methodologies and Communication (ICCMC)</em>.¬†¬†¬†IEEE, 2021, pp. 420‚Äì426.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
N.¬†Melek¬†Manshouri, ‚ÄúIdentifying covid-19 by using spectral analysis of cough recordings: a distinctive classification study,‚Äù <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Cognitive neurodynamics</em>, vol.¬†16, no.¬†1, pp. 239‚Äì253, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
H.¬†Xue and F.¬†D. Salim, ‚ÄúExploring self-supervised representation ensembles for covid-19 cough classification,‚Äù in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em>, 2021, pp. 1944‚Äì1952.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
F.¬†Manzella, G.¬†Pagliarini, G.¬†Sciavicco, and I.¬†E. Stan, ‚ÄúThe voice of covid-19: Breath and cough recording classification with temporal decision trees and random forests,‚Äù <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence in Medicine</em>, vol. 137, p. 102486, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M.¬†Pahar, M.¬†Klopper, R.¬†Warren, and T.¬†Niesler, ‚ÄúCovid-19 cough classification using machine learning and global smartphone recordings,‚Äù <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Computers in Biology and Medicine</em>, vol. 135, p. 104572, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
L.¬†Orlandic, T.¬†Teijeiro, and D.¬†Atienza, ‚ÄúThe coughvid crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms,‚Äù <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Scientific Data</em>, vol.¬†8, no.¬†1, p. 156, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
N.¬†Sharma, P.¬†Krishnan, R.¬†Kumar, S.¬†Ramoji, S.¬†Chetupalli, R.¬†Nirmala, P.¬†Kumar¬†Ghosh, and S.¬†Ganapathy, ‚ÄúCoswara-a database of breathing, cough, and voice sounds for covid-19 diagnosis,‚Äù in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</em>, vol. 2020.¬†¬†¬†International Speech Communication Association, 2020, pp. 4811‚Äì4815.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J.¬†Vald√©s, K.¬†Habashy, P.¬†Xi, M.¬†Cohen-McFarlane, B.¬†Wallace, R.¬†Goubran, and F.¬†Knoefel, ‚ÄúCough classification with deep derived features using audio spectrogram transformer,‚Äù in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">2022 IEEE International Conference on Big Data (Big Data)</em>.¬†¬†¬†IEEE, 2022, pp. 1729‚Äì1739.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
J.¬†Deng, W.¬†Dong, R.¬†Socher, L.-J. Li, K.¬†Li, and L.¬†Fei-Fei, ‚ÄúImagenet: A large-scale hierarchical image database,‚Äù in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on computer vision and pattern recognition</em>.¬†¬†¬†Ieee, 2009, pp. 248‚Äì255.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
V.¬†Dentamaro, P.¬†Giglio, D.¬†Impedovo, L.¬†Moretti, and G.¬†Pirlo, ‚ÄúAuco resnet: an end-to-end network for covid-19 pre-screening from cough and breath,‚Äù <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition</em>, vol. 127, p. 108656, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
W.¬†Thorpe, M.¬†Kurver, G.¬†King, and C.¬†Salome, ‚ÄúAcoustic analysis of cough,‚Äù in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">The Seventh Australian and New Zealand Intelligent Information Systems Conference, 2001</em>.¬†¬†¬†IEEE, 2001, pp. 391‚Äì394.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
P.¬†Foret, A.¬†Kleiner, H.¬†Mobahi, and B.¬†Neyshabur, ‚ÄúSharpness-aware minimization for efficiently improving generalization,‚Äù in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
D.¬†P. Kingma and J.¬†Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>, 2014.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
D.¬†S. Park, W.¬†Chan, Y.¬†Zhang, C.-C. Chiu, B.¬†Zoph, E.¬†D. Cubuk, and Q.¬†V. Le, ‚ÄúSpecaugment: A simple data augmentation method for automatic speech recognition,‚Äù <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Interspeech 2019</em>, p. 2613, 2019.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
H.¬†Zhang, M.¬†Cisse, Y.¬†N. Dauphin, and D.¬†Lopez-Paz, ‚Äúmixup: Beyond empirical risk minimization,‚Äù in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2018.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
H.¬†Coppock, G.¬†Nicholson, I.¬†Kiskin, V.¬†Koutra, K.¬†Baker, J.¬†Budd, R.¬†Payne, E.¬†Karoune, D.¬†Hurley, A.¬†Titcomb <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúAudio-based ai classifiers show no evidence of improved covid-19 screening over simple symptoms checkers,‚Äù <em id="bib.bib43.2.2" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, vol.¬†6, no.¬†2, pp. 229‚Äì242, 2024.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
H.¬†Touvron, M.¬†Cord, M.¬†Douze, F.¬†Massa, A.¬†Sablayrolles, and H.¬†J√©gou, ‚ÄúTraining data-efficient image transformers &amp; distillation through attention,‚Äù in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.¬†¬†¬†PMLR, 2021, pp. 10‚Äâ347‚Äì10‚Äâ357.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
A.¬†Dosovitskiy, L.¬†Beyer, A.¬†Kolesnikov, D.¬†Weissenborn, X.¬†Zhai, T.¬†Unterthiner, M.¬†Dehghani, M.¬†Minderer, G.¬†Heigold, S.¬†Gelly <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ‚ÄúAn image is worth 16x16 words: Transformers for image recognition at scale,‚Äù in <em id="bib.bib45.2.2" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2020.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
B.¬†Elizalde, S.¬†Deshmukh, M.¬†Al¬†Ismail, and H.¬†Wang, ‚ÄúClap learning audio concepts from natural language supervision,‚Äù in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.¬†¬†¬†IEEE, 2023, pp. 1‚Äì5.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.15666" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.15667" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.15667">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.15667" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.15668" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 17:39:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
