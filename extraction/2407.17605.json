{
    "S3.T1": {
        "caption": "Table 1:  Word Error Rate of RNN-T and reduced CTC models built by fine-tuning USM models on En-De MuST-C data.",
        "table": "<figure id=\"S3.T1\" class=\"ltx_table\">\n<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\">WER (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.2.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<td id=\"S3.T1.1.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_r\">dev</td>\n<td id=\"S3.T1.1.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_r\">tst-COMMON</td>\n<td id=\"S3.T1.1.2.1.4\" class=\"ltx_td ltx_align_right ltx_border_r\">tst-HE</td>\n</tr>\n<tr id=\"S3.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><code id=\"S3.T1.1.3.2.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">RNN-T</code></th>\n<td id=\"S3.T1.1.3.2.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">9.2</td>\n<td id=\"S3.T1.1.3.2.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">9.6</td>\n<td id=\"S3.T1.1.3.2.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">7.4</td>\n</tr>\n<tr id=\"S3.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><code id=\"S3.T1.1.4.3.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">Reduced CTC</code></th>\n<td id=\"S3.T1.1.4.3.2\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t\">10.2</td>\n<td id=\"S3.T1.1.4.3.3\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t\">10.2</td>\n<td id=\"S3.T1.1.4.3.4\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t\">8.0</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Word Error Rate of RNN-T and reduced CTC models built by fine-tuning USM models on En-De MuST-C data.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We carried out AST experiments on the En-De subset of the MuST-C (Di Gangi et al., 2019) V2 data. We did not pre-process the transcriptions or translations in any way, thus working in the \"written text\" domain. This choice was mostly due to the fact that MT models are trained on such text data; it also makes our work easily reproducible. The ASR model thus needs to generate all the extra information that may not be available in the speech signal such as punctuation, capitalization, correct formatting of numbers, dates, etc. As a result, the word error rate (WER) results listed in Table 1 may seem higher than expected.",
            "The ASR models used in our experiments are trained incrementally from USM (Zhang et al., 2023) models. We started with a 600M parameter model and finetuned both RNN-T and Reduced CTC ASR models on the MuST-C data. The results are presented in Table 1."
        ]
    },
    "S3.T2": {
        "caption": "Table 2:  SacreBleu of WMT baseline and incrementally trained WMT-MuST-C MT model on MuST-C data.",
        "table": "<figure id=\"S3.T2\" class=\"ltx_table\">\n<table id=\"S3.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S3.T2.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\">Source</th>\n<td id=\"S3.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">SacreBleu (%)</td>\n</tr>\n<tr id=\"S3.T2.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T2.1.2.2.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">Transcript</th>\n<td id=\"S3.T2.1.2.2.3\" class=\"ltx_td ltx_align_right ltx_border_r\">dev</td>\n<td id=\"S3.T2.1.2.2.4\" class=\"ltx_td ltx_align_right ltx_border_r\">tst-COMMON</td>\n<td id=\"S3.T2.1.2.2.5\" class=\"ltx_td ltx_align_right ltx_border_r\">tst-HE</td>\n</tr>\n<tr id=\"S3.T2.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><code id=\"S3.T2.1.3.3.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">WMT</code></th>\n<th id=\"S3.T2.1.3.3.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\">correct</th>\n<td id=\"S3.T2.1.3.3.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">32.7</td>\n<td id=\"S3.T2.1.3.3.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">33.8</td>\n<td id=\"S3.T2.1.3.3.5\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">31.6</td>\n</tr>\n<tr id=\"S3.T2.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><code id=\"S3.T2.1.4.4.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">WMT-MuST-C</code></th>\n<th id=\"S3.T2.1.4.4.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">correct</th>\n<td id=\"S3.T2.1.4.4.3\" class=\"ltx_td ltx_align_right ltx_border_r\">35.9</td>\n<td id=\"S3.T2.1.4.4.4\" class=\"ltx_td ltx_align_right ltx_border_r\">36.4</td>\n<td id=\"S3.T2.1.4.4.5\" class=\"ltx_td ltx_align_right ltx_border_r\">36.2</td>\n</tr>\n<tr id=\"S3.T2.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><code id=\"S3.T2.1.5.5.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">WMT</code></th>\n<th id=\"S3.T2.1.5.5.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<code id=\"S3.T2.1.5.5.2.1\" class=\"ltx_verbatim ltx_font_typewriter\">RNN-T</code> 1-best</th>\n<td id=\"S3.T2.1.5.5.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">31.1</td>\n<td id=\"S3.T2.1.5.5.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">30.8</td>\n<td id=\"S3.T2.1.5.5.5\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">29.9</td>\n</tr>\n<tr id=\"S3.T2.1.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"><code id=\"S3.T2.1.6.6.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">WMT-MuST-C</code></th>\n<th id=\"S3.T2.1.6.6.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r\">\n<code id=\"S3.T2.1.6.6.2.1\" class=\"ltx_verbatim ltx_font_typewriter\">RNN-T</code> 1-best</th>\n<td id=\"S3.T2.1.6.6.3\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">33.8</td>\n<td id=\"S3.T2.1.6.6.4\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">33.0</td>\n<td id=\"S3.T2.1.6.6.5\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">33.7</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>SacreBleu of WMT baseline and incrementally trained WMT-MuST-C MT model on MuST-C data.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "A first set of MT experiments evaluated the performance of the WMT model on MuST-C before and after incremental training. In addition to using the correct ASR transcript we also experimented with training and/or evaluating on the 1-best transcript produced by the RNN-T ASR model. The results are presented in Table 2. Replacing correct transcription for the source sentence with the RNN-T 1-best degrades performance by 2-3 SacreBleu, depending on the evaluation data set."
        ]
    },
    "S3.T3": {
        "caption": "Table 3:  Comparison of direct AST model with cascade architecture using either WMT baseline or incrementally trained WMT-MuST-C MT model on MuST-C data together with RNN-T ASR model also incrementally trained on MuST-C data.",
        "table": "<figure id=\"S3.T3\" class=\"ltx_table\">\n<table id=\"S3.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S3.T3.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\">Source Transcript</th>\n<td id=\"S3.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">SacreBleu (%)</td>\n</tr>\n<tr id=\"S3.T3.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T3.1.2.2.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<td id=\"S3.T3.1.2.2.3\" class=\"ltx_td ltx_align_right ltx_border_r\">dev</td>\n<td id=\"S3.T3.1.2.2.4\" class=\"ltx_td ltx_align_right ltx_border_r\">tst-COMMON</td>\n<td id=\"S3.T3.1.2.2.5\" class=\"ltx_td ltx_align_right ltx_border_r\">tst-HE</td>\n</tr>\n<tr id=\"S3.T3.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\">Direct model: USM encoder + LAS decoder</th>\n<td id=\"S3.T3.1.3.3.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">28.2</td>\n<td id=\"S3.T3.1.3.3.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">27.2</td>\n<td id=\"S3.T3.1.3.3.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">27.3</td>\n</tr>\n<tr id=\"S3.T3.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><code id=\"S3.T3.1.4.4.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">WMT</code></th>\n<th id=\"S3.T3.1.4.4.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<code id=\"S3.T3.1.4.4.2.1\" class=\"ltx_verbatim ltx_font_typewriter\">RNN-T</code> 1-best</th>\n<td id=\"S3.T3.1.4.4.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">31.1</td>\n<td id=\"S3.T3.1.4.4.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">30.8</td>\n<td id=\"S3.T3.1.4.4.5\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">29.9</td>\n</tr>\n<tr id=\"S3.T3.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"><code id=\"S3.T3.1.5.5.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">WMT-MuST-C</code></th>\n<th id=\"S3.T3.1.5.5.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_r\">\n<code id=\"S3.T3.1.5.5.2.1\" class=\"ltx_verbatim ltx_font_typewriter\">RNN-T</code> 1-best</th>\n<td id=\"S3.T3.1.5.5.3\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">33.8</td>\n<td id=\"S3.T3.1.5.5.4\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">33.0</td>\n<td id=\"S3.T3.1.5.5.5\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">33.7</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Comparison of direct AST model with cascade architecture using either WMT baseline or incrementally trained WMT-MuST-C MT model on MuST-C data together with RNN-T ASR model also incrementally trained on MuST-C data.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "The very first AST experiment was to compare direct model performance with that of the cascade model. In earlier experiments with written text ASR carried out on LibriTTS Zen et al. (2019) data we observed slightly better performance of LAS (Chan et al., 2015) versus RNN-T models (Rao et al., 2018) built by fine-tuning a USM encoder: 8.5% and 10.9% on dev-clean and test-other partitions versus 8.8% and 11.2%. Given that observation and the fact that the MT decoder relies heavily on cross-attention to reorder the target side tokens relative to the source, we build a direct AST model using the LAS architecture. Table 3 compares SacreBleu for the direct AST model with the cascade model built by feeding RNN-T 1-best transcriptions to the MT model. The RNN-T model is built by fine-tuning a USM encoder (600M parameters) on MuST-C data. The same approach is used to build the LAS direct AST model."
        ]
    },
    "S3.T4": {
        "caption": "Table 4:  L2 loss per token summed across 1024 dimensions.",
        "table": "<figure id=\"S3.T4\" class=\"ltx_table\">\n<table id=\"S3.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">MT Model</th>\n<th id=\"S3.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\">L2-loss/token</th>\n</tr>\n<tr id=\"S3.T4.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T4.1.2.2.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\">dev</th>\n<th id=\"S3.T4.1.2.2.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\">tst-COMMON</th>\n<th id=\"S3.T4.1.2.2.4\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r\">tst-HE</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T4.1.3.1\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><code id=\"S3.T4.1.3.1.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">WMT</code></th>\n<td id=\"S3.T4.1.3.1.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">9.0</td>\n<td id=\"S3.T4.1.3.1.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">9.2</td>\n<td id=\"S3.T4.1.3.1.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">8.9</td>\n</tr>\n<tr id=\"S3.T4.1.4.2\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"><code id=\"S3.T4.1.4.2.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">WMT-MuST-C</code></th>\n<td id=\"S3.T4.1.4.2.2\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">9.5</td>\n<td id=\"S3.T4.1.4.2.3\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">9.8</td>\n<td id=\"S3.T4.1.4.2.4\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">9.4</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>L2 loss per token summed across 1024 dimensions.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "A first experiment trained the exporter component (3 layer conformer) to ensure L2 match with the MT token embeddings. This uses the MuST-C parallel data (speech, transcription). Table 4 shows the loss values after 25,000 training steps using a global batch size of 128. A very good fit between the encodings exported after CTC reduction and the MT token embeddings is achieved, within 0.001 (10/1024) absolute diff per dimension."
        ]
    },
    "S3.T5": {
        "caption": "Table 5:  AST performance for cascade architecture using L2 loss matcher for reduced CTC ASR, WMT MT model.",
        "table": "<figure id=\"S3.T5\" class=\"ltx_table\">\n<table id=\"S3.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T5.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Cascade AST Model</th>\n<td id=\"S3.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">SacreBleu</td>\n</tr>\n<tr id=\"S3.T5.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T5.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<td id=\"S3.T5.1.2.2.2\" class=\"ltx_td ltx_align_right ltx_border_r\">dev</td>\n<td id=\"S3.T5.1.2.2.3\" class=\"ltx_td ltx_align_right ltx_border_r\">tst-COMMON</td>\n<td id=\"S3.T5.1.2.2.4\" class=\"ltx_td ltx_align_right ltx_border_r\">tst-HE</td>\n</tr>\n<tr id=\"S3.T5.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T5.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">transcript</th>\n<td id=\"S3.T5.1.3.3.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">32.7</td>\n<td id=\"S3.T5.1.3.3.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">34.2</td>\n<td id=\"S3.T5.1.3.3.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">30.9</td>\n</tr>\n<tr id=\"S3.T5.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T5.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">\n<code id=\"S3.T5.1.4.4.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">RNN-T</code> 1-best</th>\n<td id=\"S3.T5.1.4.4.2\" class=\"ltx_td ltx_align_right ltx_border_r\">31.1</td>\n<td id=\"S3.T5.1.4.4.3\" class=\"ltx_td ltx_align_right ltx_border_r\">30.8</td>\n<td id=\"S3.T5.1.4.4.4\" class=\"ltx_td ltx_align_right ltx_border_r\">29.9</td>\n</tr>\n<tr id=\"S3.T5.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T5.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<code id=\"S3.T5.1.5.5.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">Reduced CTC</code> 1-best</th>\n<td id=\"S3.T5.1.5.5.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">30.6</td>\n<td id=\"S3.T5.1.5.5.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">30.8</td>\n<td id=\"S3.T5.1.5.5.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">28.5</td>\n</tr>\n<tr id=\"S3.T5.1.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T5.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">\n<code id=\"S3.T5.1.6.6.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">Reduced CTC</code> L2 loss, step 0</th>\n<td id=\"S3.T5.1.6.6.2\" class=\"ltx_td ltx_align_right ltx_border_r\">30.7</td>\n<td id=\"S3.T5.1.6.6.3\" class=\"ltx_td ltx_align_right ltx_border_r\">30.6</td>\n<td id=\"S3.T5.1.6.6.4\" class=\"ltx_td ltx_align_right ltx_border_r\">28.7</td>\n</tr>\n<tr id=\"S3.T5.1.7.7\" class=\"ltx_tr\">\n<th id=\"S3.T5.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">\n<code id=\"S3.T5.1.7.7.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">Reduced CTC</code> L2 loss, step 8k</th>\n<td id=\"S3.T5.1.7.7.2\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">32.5</td>\n<td id=\"S3.T5.1.7.7.3\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">32.5</td>\n<td id=\"S3.T5.1.7.7.4\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">31.8</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>AST performance for cascade architecture using L2 loss matcher for reduced CTC ASR, WMT MT model.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We then proceeded to train the exporter from this initial point on parallel (speech, translation) MuST-C data in an attempt to improve AST performance beyond the initial one (step 0). We also verified that the AST performance at step 0 matches the cascade 1-best one as shown in Table 5.",
            "We wish to note that when training the exporter on MuST-C AST data, there is task specific \"leakage\" into the model despite the fact that ASR/MT parameters are frozen. That should explain the ability to beat the WMT translation model performance when feeding it correct transcripts for the MuST-C data, as observed on the tst-HE test set (last row in Table 5)."
        ]
    },
    "S3.T6": {
        "caption": "Table 6:  AST performance for cascade architecture using L2 loss matcher for Reduced CTC ASR, incrementally trained WMT-MuST-C MT model.",
        "table": "<figure id=\"S3.T6\" class=\"ltx_table\">\n<table id=\"S3.T6.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T6.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T6.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Cascade AST Model</th>\n<td id=\"S3.T6.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">SacreBleu</td>\n</tr>\n<tr id=\"S3.T6.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T6.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<td id=\"S3.T6.1.2.2.2\" class=\"ltx_td ltx_align_right ltx_border_r\">dev</td>\n<td id=\"S3.T6.1.2.2.3\" class=\"ltx_td ltx_align_right ltx_border_r\">tst-COMMON</td>\n<td id=\"S3.T6.1.2.2.4\" class=\"ltx_td ltx_align_right ltx_border_r\">tst-HE</td>\n</tr>\n<tr id=\"S3.T6.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T6.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">transcript</th>\n<td id=\"S3.T6.1.3.3.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">35.3</td>\n<td id=\"S3.T6.1.3.3.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">35.8</td>\n<td id=\"S3.T6.1.3.3.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">35.3</td>\n</tr>\n<tr id=\"S3.T6.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T6.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">\n<code id=\"S3.T6.1.4.4.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">RNN-T</code> 1-best</th>\n<td id=\"S3.T6.1.4.4.2\" class=\"ltx_td ltx_align_right ltx_border_r\">33.8</td>\n<td id=\"S3.T6.1.4.4.3\" class=\"ltx_td ltx_align_right ltx_border_r\">33.0</td>\n<td id=\"S3.T6.1.4.4.4\" class=\"ltx_td ltx_align_right ltx_border_r\">33.7</td>\n</tr>\n<tr id=\"S3.T6.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T6.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<code id=\"S3.T6.1.5.5.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">Reduced CTC</code> 1-best</th>\n<td id=\"S3.T6.1.5.5.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">32.8</td>\n<td id=\"S3.T6.1.5.5.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">32.0</td>\n<td id=\"S3.T6.1.5.5.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">32.4</td>\n</tr>\n<tr id=\"S3.T6.1.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T6.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">\n<code id=\"S3.T6.1.6.6.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">Reduced CTC</code> L2 loss, step 0</th>\n<td id=\"S3.T6.1.6.6.2\" class=\"ltx_td ltx_align_right ltx_border_r\">32.4</td>\n<td id=\"S3.T6.1.6.6.3\" class=\"ltx_td ltx_align_right ltx_border_r\">32.3</td>\n<td id=\"S3.T6.1.6.6.4\" class=\"ltx_td ltx_align_right ltx_border_r\">33.1</td>\n</tr>\n<tr id=\"S3.T6.1.7.7\" class=\"ltx_tr\">\n<th id=\"S3.T6.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">\n<code id=\"S3.T6.1.7.7.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">Reduced CTC</code> L2 loss, step 11k</th>\n<td id=\"S3.T6.1.7.7.2\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">32.7</td>\n<td id=\"S3.T6.1.7.7.3\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">32.3</td>\n<td id=\"S3.T6.1.7.7.4\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">32.5</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>AST performance for cascade architecture using L2 loss matcher for Reduced CTC ASR, incrementally trained WMT-MuST-C MT model.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "A second set of experiments confirmed that this was indeed the main reason for the improvements above: when using the WMT-MuST-C translation back-end (a WMT model adapted on MuST-C data) and an exporter matched to the token embeddings of this model there is no longer a significant gain from further training of the Reduced CTC L2 loss model, as shown in Table 6."
        ]
    },
    "S3.T7": {
        "caption": "Table 7:  Ablation experiments comparing AST performance for cascade architecture using exporter initialized randomly versus L2 loss matcher with both WMT and incrementally trained WMT-MuST-C MT model.",
        "table": "<figure id=\"S3.T7\" class=\"ltx_table\">\n<table id=\"S3.T7.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T7.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">MT Model</th>\n<th id=\"S3.T7.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Exporter Initialization</th>\n<td id=\"S3.T7.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">SacreBleu</td>\n</tr>\n<tr id=\"S3.T7.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T7.1.2.2.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<td id=\"S3.T7.1.2.2.3\" class=\"ltx_td ltx_align_right ltx_border_r\">dev</td>\n<td id=\"S3.T7.1.2.2.4\" class=\"ltx_td ltx_align_right ltx_border_r\">tst-COMMON</td>\n<td id=\"S3.T7.1.2.2.5\" class=\"ltx_td ltx_align_right ltx_border_r\">tst-HE</td>\n</tr>\n<tr id=\"S3.T7.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><code id=\"S3.T7.1.3.3.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">WMT</code></th>\n<th id=\"S3.T7.1.3.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">random</th>\n<td id=\"S3.T7.1.3.3.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">31.2</td>\n<td id=\"S3.T7.1.3.3.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">30.3</td>\n<td id=\"S3.T7.1.3.3.5\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">30.8</td>\n</tr>\n<tr id=\"S3.T7.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\"><code id=\"S3.T7.1.4.4.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">WMT</code></th>\n<th id=\"S3.T7.1.4.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">L2 loss matcher</th>\n<td id=\"S3.T7.1.4.4.3\" class=\"ltx_td ltx_align_right ltx_border_r\">32.5</td>\n<td id=\"S3.T7.1.4.4.4\" class=\"ltx_td ltx_align_right ltx_border_r\">32.5</td>\n<td id=\"S3.T7.1.4.4.5\" class=\"ltx_td ltx_align_right ltx_border_r\">31.8</td>\n</tr>\n<tr id=\"S3.T7.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><code id=\"S3.T7.1.5.5.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">WMT-MuST-C</code></th>\n<th id=\"S3.T7.1.5.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">random</th>\n<td id=\"S3.T7.1.5.5.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">31.0</td>\n<td id=\"S3.T7.1.5.5.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">30.4</td>\n<td id=\"S3.T7.1.5.5.5\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">31.3</td>\n</tr>\n<tr id=\"S3.T7.1.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"><code id=\"S3.T7.1.6.6.1.1\" class=\"ltx_verbatim ltx_font_typewriter\">WMT-MuST-C</code></th>\n<th id=\"S3.T7.1.6.6.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">L2 loss matcher</th>\n<td id=\"S3.T7.1.6.6.3\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">32.7</td>\n<td id=\"S3.T7.1.6.6.4\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">32.3</td>\n<td id=\"S3.T7.1.6.6.5\" class=\"ltx_td ltx_align_right ltx_border_b ltx_border_r\">32.5</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Ablation experiments comparing AST performance for cascade architecture using exporter initialized randomly versus L2 loss matcher with both WMT and incrementally trained WMT-MuST-C MT model.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "A last set of experiments aim at understanding to what extent the L2 loss initialization of the exporter module is useful. Table 7 shows the results of ablation experiments using both WMT and WMT-MuST-C translation models: we use random initialization for the exporter module instead of the L2 loss one described above."
        ]
    }
}