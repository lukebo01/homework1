{
    "S3.T1": {
        "caption": "Table 1 :  Accuracies (in %,  \u03c3 \ud835\udf0e \\sigma =0.15-0.32%) and differences in train time and inference time ( \u03c3 \ud835\udf0e \\sigma =0.9-1.1%) for word classification and speaker identification for full and pruned fine-tuned models. For words classification, the base models are pruned to 8 layers and the large models are pruned to 15 layers. For speaker identification, the base models are pruned to 2 layers and the large models are pruned to 4 layers. On average, the accuracy for word classification decreased by only 0.25% ( \u224a \u03c3 approximately-equals-or-equals absent \ud835\udf0e \\approxeq\\sigma ), while the accuracy for speaker identification increased by 1.22% (*\u00a0excluded). The training time and inference time was significantly reduced in both cases.",
        "table": "<figure id=\"S3.T1\" class=\"ltx_table\">\n<table id=\"S3.T1.8\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.8.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.8.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_rr\"></th>\n<td id=\"S3.T1.8.1.1.2\" class=\"ltx_td\"></td>\n<td id=\"S3.T1.8.1.1.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.1.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">word</span></td>\n<td id=\"S3.T1.8.1.1.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.1.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">class.</span></td>\n<td id=\"S3.T1.8.1.1.5\" class=\"ltx_td\"></td>\n<td id=\"S3.T1.8.1.1.6\" class=\"ltx_td ltx_border_rr\"></td>\n<td id=\"S3.T1.8.1.1.7\" class=\"ltx_td\"></td>\n<td id=\"S3.T1.8.1.1.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.1.1.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">speaker</span></td>\n<td id=\"S3.T1.8.1.1.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.1.1.9.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ident.</span></td>\n<td id=\"S3.T1.8.1.1.10\" class=\"ltx_td\"></td>\n<td id=\"S3.T1.8.1.1.11\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.8.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.8.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_rr ltx_border_tt\"></th>\n<td id=\"S3.T1.8.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Full</span></td>\n<td id=\"S3.T1.8.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Pruned</span></td>\n<td id=\"S3.T1.8.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.2.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Acc</span></td>\n<td id=\"S3.T1.8.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.2.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Train time</span></td>\n<td id=\"S3.T1.8.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\"><span id=\"S3.T1.8.2.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Inference</span></td>\n<td id=\"S3.T1.8.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.2.2.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">Full</span></td>\n<td id=\"S3.T1.8.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.2.2.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Pruned</span></td>\n<td id=\"S3.T1.8.2.2.9\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.2.2.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">Acc</span></td>\n<td id=\"S3.T1.8.2.2.10\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.2.2.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">Train time</span></td>\n<td id=\"S3.T1.8.2.2.11\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.2.2.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">Inference</span></td>\n</tr>\n<tr id=\"S3.T1.8.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T1.8.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\"><span id=\"S3.T1.8.3.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<td id=\"S3.T1.8.3.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.3.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Model</span></td>\n<td id=\"S3.T1.8.3.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.3.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Model</span></td>\n<td id=\"S3.T1.8.3.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.3.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">diff. (%)</span></td>\n<td id=\"S3.T1.8.3.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.3.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">diff. (%)</span></td>\n<td id=\"S3.T1.8.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S3.T1.8.3.3.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">diff. (%)</span></td>\n<td id=\"S3.T1.8.3.3.7\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.3.3.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">Model</span></td>\n<td id=\"S3.T1.8.3.3.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.3.3.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Model</span></td>\n<td id=\"S3.T1.8.3.3.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.3.3.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">diff. (%)</span></td>\n<td id=\"S3.T1.8.3.3.10\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.3.3.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">diff. (%)</span></td>\n<td id=\"S3.T1.8.3.3.11\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.3.3.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">diff. (%)</span></td>\n</tr>\n<tr id=\"S3.T1.8.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T1.8.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_tt\"><span id=\"S3.T1.8.4.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">wav2vec2</span></th>\n<td id=\"S3.T1.8.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.4.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">98.32</span></td>\n<td id=\"S3.T1.8.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.4.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">98.43</span></td>\n<td id=\"S3.T1.8.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.4.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">+ 0.11</span></td>\n<td id=\"S3.T1.8.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.4.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 15.88</span></td>\n<td id=\"S3.T1.8.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\"><span id=\"S3.T1.8.4.4.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 22. 73</span></td>\n<td id=\"S3.T1.8.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.4.4.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">95.17</span></td>\n<td id=\"S3.T1.8.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.4.4.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">97.79</span></td>\n<td id=\"S3.T1.8.4.4.9\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.4.4.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">+ 2.75</span></td>\n<td id=\"S3.T1.8.4.4.10\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.4.4.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 38.33</span></td>\n<td id=\"S3.T1.8.4.4.11\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.4.4.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 53.25</span></td>\n</tr>\n<tr id=\"S3.T1.8.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T1.8.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\"><span id=\"S3.T1.8.5.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">large</span></th>\n<td id=\"S3.T1.8.5.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.5.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">97.21</span></td>\n<td id=\"S3.T1.8.5.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.5.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">96.92</span></td>\n<td id=\"S3.T1.8.5.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.5.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 0.20</span></td>\n<td id=\"S3.T1.8.5.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.5.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 37.48</span></td>\n<td id=\"S3.T1.8.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S3.T1.8.5.5.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 35.42</span></td>\n<td id=\"S3.T1.8.5.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.5.5.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">98.52</span></td>\n<td id=\"S3.T1.8.5.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.5.5.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">98.68</span></td>\n<td id=\"S3.T1.8.5.5.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.5.5.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">+ 0.16</span></td>\n<td id=\"S3.T1.8.5.5.10\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.5.5.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 59.08</span></td>\n<td id=\"S3.T1.8.5.5.11\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.5.5.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 66.64</span></td>\n</tr>\n<tr id=\"S3.T1.8.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T1.8.6.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\"><span id=\"S3.T1.8.6.6.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">wavLM</span></th>\n<td id=\"S3.T1.8.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.6.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">97.22</span></td>\n<td id=\"S3.T1.8.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.6.6.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">96.96</span></td>\n<td id=\"S3.T1.8.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.6.6.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 0.25</span></td>\n<td id=\"S3.T1.8.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.6.6.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 17.33</span></td>\n<td id=\"S3.T1.8.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span id=\"S3.T1.8.6.6.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 22.32</span></td>\n<td id=\"S3.T1.8.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.6.6.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">93.18</span></td>\n<td id=\"S3.T1.8.6.6.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.6.6.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">96.27</span></td>\n<td id=\"S3.T1.8.6.6.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.6.6.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">+ 3.38</span></td>\n<td id=\"S3.T1.8.6.6.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.6.6.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 45.30</span></td>\n<td id=\"S3.T1.8.6.6.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.6.6.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 55.10</span></td>\n</tr>\n<tr id=\"S3.T1.8.7.7\" class=\"ltx_tr\">\n<th id=\"S3.T1.8.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\"><span id=\"S3.T1.8.7.7.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">large</span></th>\n<td id=\"S3.T1.8.7.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.7.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">98.86</span></td>\n<td id=\"S3.T1.8.7.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.7.7.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">98.66</span></td>\n<td id=\"S3.T1.8.7.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.7.7.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 0.20</span></td>\n<td id=\"S3.T1.8.7.7.5\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.7.7.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 25.64</span></td>\n<td id=\"S3.T1.8.7.7.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S3.T1.8.7.7.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 30.36</span></td>\n<td id=\"S3.T1.8.7.7.7\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.7.7.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">97.14</span></td>\n<td id=\"S3.T1.8.7.7.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.7.7.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">97.53</span></td>\n<td id=\"S3.T1.8.7.7.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.7.7.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">+ 0.40</span></td>\n<td id=\"S3.T1.8.7.7.10\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.7.7.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 66.93</span></td>\n<td id=\"S3.T1.8.7.7.11\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.7.7.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 64.73</span></td>\n</tr>\n<tr id=\"S3.T1.8.8.8\" class=\"ltx_tr\">\n<th id=\"S3.T1.8.8.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\"><span id=\"S3.T1.8.8.8.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">HuBERT</span></th>\n<td id=\"S3.T1.8.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.8.8.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">97.43</span></td>\n<td id=\"S3.T1.8.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.8.8.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">96.65</span></td>\n<td id=\"S3.T1.8.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.8.8.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 0.80</span></td>\n<td id=\"S3.T1.8.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.8.8.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 5.08</span></td>\n<td id=\"S3.T1.8.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span id=\"S3.T1.8.8.8.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 18.64</span></td>\n<td id=\"S3.T1.8.8.8.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.8.8.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">94.36</span></td>\n<td id=\"S3.T1.8.8.8.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.8.8.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">92.43</span></td>\n<td id=\"S3.T1.8.8.8.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.8.8.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 1.80</span></td>\n<td id=\"S3.T1.8.8.8.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.8.8.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 44.98</span></td>\n<td id=\"S3.T1.8.8.8.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.8.8.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 52.99</span></td>\n</tr>\n<tr id=\"S3.T1.8.9.9\" class=\"ltx_tr\">\n<th id=\"S3.T1.8.9.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\"><span id=\"S3.T1.8.9.9.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">large</span></th>\n<td id=\"S3.T1.8.9.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.9.9.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">98.39</span></td>\n<td id=\"S3.T1.8.9.9.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.9.9.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">98.83</span></td>\n<td id=\"S3.T1.8.9.9.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.9.9.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">+ 0.45</span></td>\n<td id=\"S3.T1.8.9.9.5\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.9.9.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">-17.49</span></td>\n<td id=\"S3.T1.8.9.9.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S3.T1.8.9.9.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">-30.35</span></td>\n<td id=\"S3.T1.8.9.9.7\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.9.9.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">69.64</span></td>\n<td id=\"S3.T1.8.9.9.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.9.9.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">96.36</span></td>\n<td id=\"S3.T1.8.9.9.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.9.9.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">+ 38.37*</span></td>\n<td id=\"S3.T1.8.9.9.10\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.9.9.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 58.89</span></td>\n<td id=\"S3.T1.8.9.9.11\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.8.9.9.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 65.94</span></td>\n</tr>\n<tr id=\"S3.T1.8.10.10\" class=\"ltx_tr\">\n<th id=\"S3.T1.8.10.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\"><span id=\"S3.T1.8.10.10.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">ccc-wav2vec2</span></th>\n<td id=\"S3.T1.8.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.10.10.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">97.61</span></td>\n<td id=\"S3.T1.8.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.10.10.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">96.65</span></td>\n<td id=\"S3.T1.8.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.10.10.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 0.98</span></td>\n<td id=\"S3.T1.8.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.10.10.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 45.09</span></td>\n<td id=\"S3.T1.8.10.10.6\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span id=\"S3.T1.8.10.10.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">-21.81</span></td>\n<td id=\"S3.T1.8.10.10.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.10.10.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">96.20</span></td>\n<td id=\"S3.T1.8.10.10.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.10.10.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">98.55</span></td>\n<td id=\"S3.T1.8.10.10.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.10.10.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">+ 2.43</span></td>\n<td id=\"S3.T1.8.10.10.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.10.10.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">- 21.38</span></td>\n<td id=\"S3.T1.8.10.10.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.8.10.10.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">-52.64</span></td>\n</tr>\n<tr id=\"S3.T1.8.11.11\" class=\"ltx_tr\">\n<th id=\"S3.T1.8.11.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_tt\"><span id=\"S3.T1.8.11.11.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Average</span></th>\n<td id=\"S3.T1.8.11.11.2\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S3.T1.8.11.11.3\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S3.T1.8.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.11.11.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">-0.25</span></td>\n<td id=\"S3.T1.8.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.11.11.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">-23.42</span></td>\n<td id=\"S3.T1.8.11.11.6\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\"><span id=\"S3.T1.8.11.11.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">-25.9</span></td>\n<td id=\"S3.T1.8.11.11.7\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S3.T1.8.11.11.8\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S3.T1.8.11.11.9\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.11.11.9.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">+1.22</span></td>\n<td id=\"S3.T1.8.11.11.10\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.11.11.10.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">-47.84</span></td>\n<td id=\"S3.T1.8.11.11.11\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.8.11.11.11.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">-58.75</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S3.T1.14.1.1\" class=\"ltx_text ltx_font_bold\">Table 1</span>: </span>Accuracies (in %, <math id=\"S3.T1.4.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma\" display=\"inline\"><semantics id=\"S3.T1.4.m1.1b\"><mi id=\"S3.T1.4.m1.1.1\" xref=\"S3.T1.4.m1.1.1.cmml\">\u03c3</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.4.m1.1c\"><ci id=\"S3.T1.4.m1.1.1.cmml\" xref=\"S3.T1.4.m1.1.1\">\ud835\udf0e</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.4.m1.1d\">\\sigma</annotation></semantics></math>=0.15-0.32%) and differences in train time and inference time (<math id=\"S3.T1.5.m2.1\" class=\"ltx_Math\" alttext=\"\\sigma\" display=\"inline\"><semantics id=\"S3.T1.5.m2.1b\"><mi id=\"S3.T1.5.m2.1.1\" xref=\"S3.T1.5.m2.1.1.cmml\">\u03c3</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.5.m2.1c\"><ci id=\"S3.T1.5.m2.1.1.cmml\" xref=\"S3.T1.5.m2.1.1\">\ud835\udf0e</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.5.m2.1d\">\\sigma</annotation></semantics></math>=0.9-1.1%) for word classification and speaker identification for full and pruned fine-tuned models. For words classification, the base models are pruned to 8 layers and the large models are pruned to 15 layers. For speaker identification, the base models are pruned to 2 layers and the large models are pruned to 4 layers. On average, the accuracy for word classification decreased by only 0.25% (<math id=\"S3.T1.6.m3.1\" class=\"ltx_Math\" alttext=\"\\approxeq\\sigma\" display=\"inline\"><semantics id=\"S3.T1.6.m3.1b\"><mrow id=\"S3.T1.6.m3.1.1\" xref=\"S3.T1.6.m3.1.1.cmml\"><mi id=\"S3.T1.6.m3.1.1.2\" xref=\"S3.T1.6.m3.1.1.2.cmml\"></mi><mo id=\"S3.T1.6.m3.1.1.1\" xref=\"S3.T1.6.m3.1.1.1.cmml\">\u224a</mo><mi id=\"S3.T1.6.m3.1.1.3\" xref=\"S3.T1.6.m3.1.1.3.cmml\">\u03c3</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.6.m3.1c\"><apply id=\"S3.T1.6.m3.1.1.cmml\" xref=\"S3.T1.6.m3.1.1\"><csymbol cd=\"latexml\" id=\"S3.T1.6.m3.1.1.1.cmml\" xref=\"S3.T1.6.m3.1.1.1\">approximately-equals-or-equals</csymbol><csymbol cd=\"latexml\" id=\"S3.T1.6.m3.1.1.2.cmml\" xref=\"S3.T1.6.m3.1.1.2\">absent</csymbol><ci id=\"S3.T1.6.m3.1.1.3.cmml\" xref=\"S3.T1.6.m3.1.1.3\">\ud835\udf0e</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.6.m3.1d\">\\approxeq\\sigma</annotation></semantics></math>), while the accuracy for speaker identification increased by 1.22% (*\u00a0excluded). The training time and inference time was significantly reduced in both cases.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We fine-tuned models for word classification and speaker identification, all results can be seen in the exact accuracies for all models can be seen in Table 1. The fine-tuned models for word classification all achieve test accuracy higher than 97%percent9797\\% and very similar convexity scores in the later layers. The fine-tuned models for speaker identification all achieve test accuracies of more than 93%percent9393\\%, except for HuBERT large which only has 69.64%. This is also reflected in the convexity scores which are lower for HuBERT large (see Figure 1). We generally observe that the higher the convexity score in the last layer is, the higher the performance. A similar relation between accuracy and convexity was also observed by T\u011btkov\u00e1 et al. in [8].",
            "For the word classification task, the test set accuracies of the pruned model are very comparable to the full model, the exact accuracies are shown in Table 1. Only a slight decrease of 0.25% was observed on average, while the training time could be reduced by on average 20% and inference time by 25%. For the speaker identification task, the pruned models even outperformed the full models on average by 1.2%percent1.21.2\\%. The training time was reduced to almost half (by 48% on average) and inference time was reduced by almost 60%. This clearly shows, how a better understanding of the latent space representations can help with making informed decisions on which layers are useful for certain tasks and lead not only to smaller and more efficient models but even to better performance.",
            "We also prune the base models at every second layer and train it for the word classification task to investigate if the best layer based on the convexity score is actually the best layer to prune the model. The results in Figure 2 show a clear relation between the convexity score of a certain layer and accuracy after pruning to that layer. For three models the best performing pruned model is in fact the one pruned to 8 layers, where the convexity score is also the highest or not significantly increasing for later layers anymore. Eventhough layer 10 is the highest convexity layer for wavLM, HuBERT and ccc-wav2vec2, only for ccc-wav2vec2 the model pruned to 10 layers performs slightly better than the one pruned to 8 layers. This shows, that using the layer after which the convexity doesn\u2019t increase substantially anymore instead of using the actually highest convexity layer can be a good strategy as the smaller models have a better/comparable performance. The models pruned to less layers show a slight decrease in performance, however, the accuracy is still 96-98% for pruning at layers 4, 6 and is still above 92% for pruning at layer 2 and 0. Even removing all transformer layers and only using the feature extractor still leads to a performance of 70-80% for all models. This also shows that most of the model can easily be pruned without much of a performance decrease. We omit this experiment for other model and task combinations to save computational resources. Yet, the results for speaker identification in Table 1 support the conclusion that pruning at layers with higher convexity scores can actually improve performance, as the accuracy for speaker identification for all except one model significantly increased when using a high convexity layer instead of the full model."
        ]
    }
}