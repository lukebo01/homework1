{
    "S3.T1": {
        "caption": "Table 1.  Statistics of the MER 2024 dataset",
        "table": "<figure id=\"S3.T1\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1. </span>Statistics of the MER 2024 dataset</figcaption>\n<div id=\"S3.T1.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:390.3pt;height:95.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(48.4pt,-11.9pt) scale(1.32953208696743,1.32953208696743) ;\">\n<table id=\"S3.T1.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Dataset</th>\n<th id=\"S3.T1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Labeled</th>\n<th id=\"S3.T1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Unlabeled</th>\n<th id=\"S3.T1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Duration (hr:min:sec)</th>\n</tr>\n<tr id=\"S3.T1.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Train&amp;Val</th>\n<th id=\"S3.T1.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">5030</th>\n<th id=\"S3.T1.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0</th>\n<th id=\"S3.T1.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">05:56:39</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MER-SEMI</th>\n<td id=\"S3.T1.1.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"S3.T1.1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1169/115595</td>\n<td id=\"S3.T1.1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">100:38:49</td>\n</tr>\n<tr id=\"S3.T1.1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">MER-NOISE</th>\n<td id=\"S3.T1.1.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"S3.T1.1.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_b\">1170/115595</td>\n<td id=\"S3.T1.1.1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_b\">100:38:49</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Dataset: We conduct experiments using the MER-SEMI dataset, as detailed in Table 1. The Train&amp;Val set consists of 5,030 video clips with both discrete and dimensional emotion labels. Given the absence of a predefined training/validation split, we utilize five-fold cross-validation on the Train&amp;Val set (Lian et al., 2024). To evaluate model generalization, the MER-SEMI track includes 1,169 unlabeled video clips in a test set, drawn from a total of 115,595 unlabeled data. Participants must predict discrete emotion labels across all unlabeled data, not just the test set. The set of discrete emotion labels includes six categories: neutral, anger, happiness, sadness, worry, and surprise. We use a weighted average F1 score as the evaluation metric, aligning with the official baseline."
        ]
    },
    "S3.T2": {
        "caption": "Table 2.  Evaluation of Unimodal and Multimodal Feature Performance.",
        "table": "<figure id=\"S3.T2\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2. </span>Evaluation of Unimodal and Multimodal Feature Performance.</figcaption>\n<div id=\"S3.T2.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:411.9pt;height:337.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(41.1pt,-33.6pt) scale(1.24903001133053,1.24903001133053) ;\">\n<table id=\"S3.T2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">Features</td>\n<td id=\"S3.T2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S3.T2.1.1.1.1.2.1\" class=\"ltx_text\">\n<span id=\"S3.T2.1.1.1.1.2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S3.T2.1.1.1.1.2.1.1.1\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Train&amp;Val</span></span>\n<span id=\"S3.T2.1.1.1.1.2.1.1.2\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.2.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">F1-Score(\u2191)</span></span>\n</span></span></td>\n<td id=\"S3.T2.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S3.T2.1.1.1.1.3.1\" class=\"ltx_text\">\n<span id=\"S3.T2.1.1.1.1.3.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S3.T2.1.1.1.1.3.1.1.1\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.3.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">MER-SEMI</span></span>\n<span id=\"S3.T2.1.1.1.1.3.1.1.2\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.3.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">F1-Score(\u2191)</span></span>\n</span></span></td>\n</tr>\n<tr id=\"S3.T2.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.2.2.1\" class=\"ltx_td ltx_align_center\">A</td>\n<td id=\"S3.T2.1.1.2.2.2\" class=\"ltx_td ltx_align_center\">V</td>\n<td id=\"S3.T2.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">T</td>\n</tr>\n<tr id=\"S3.T2.1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\">Unimodal Results</td>\n</tr>\n<tr id=\"S3.T2.1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">HL</td>\n<td id=\"S3.T2.1.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td id=\"S3.T2.1.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n<td id=\"S3.T2.1.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">72.82\u00b10.30</td>\n<td id=\"S3.T2.1.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">83.49</td>\n</tr>\n<tr id=\"S3.T2.1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.5.5.1\" class=\"ltx_td ltx_align_left\">HL(18)</td>\n<td id=\"S3.T2.1.1.5.5.2\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S3.T2.1.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td id=\"S3.T2.1.1.5.5.4\" class=\"ltx_td ltx_align_center\">74.40\u00b10.46</td>\n<td id=\"S3.T2.1.1.5.5.5\" class=\"ltx_td ltx_align_center\">84.78</td>\n</tr>\n<tr id=\"S3.T2.1.1.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.6.6.1\" class=\"ltx_td ltx_align_left\">HL(16-21)</td>\n<td id=\"S3.T2.1.1.6.6.2\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S3.T2.1.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td id=\"S3.T2.1.1.6.6.4\" class=\"ltx_td ltx_align_center\">73.98\u00b10.43</td>\n<td id=\"S3.T2.1.1.6.6.5\" class=\"ltx_td ltx_align_center\">84.79</td>\n</tr>\n<tr id=\"S3.T2.1.1.7.7\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.7.7.1\" class=\"ltx_td ltx_align_left\">HLFT(18)</td>\n<td id=\"S3.T2.1.1.7.7.2\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S3.T2.1.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td id=\"S3.T2.1.1.7.7.4\" class=\"ltx_td ltx_align_center\">76.30\u00b10.33</td>\n<td id=\"S3.T2.1.1.7.7.5\" class=\"ltx_td ltx_align_center\">84.69</td>\n</tr>\n<tr id=\"S3.T2.1.1.8.8\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.8.8.1\" class=\"ltx_td ltx_align_left\">HLFT(16-21)</td>\n<td id=\"S3.T2.1.1.8.8.2\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S3.T2.1.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td id=\"S3.T2.1.1.8.8.4\" class=\"ltx_td ltx_align_center\">80.24\u00b10.21</td>\n<td id=\"S3.T2.1.1.8.8.5\" class=\"ltx_td ltx_align_center\">84.88</td>\n</tr>\n<tr id=\"S3.T2.1.1.9.9\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.9.9.1\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S3.T2.1.1.9.9.2\" class=\"ltx_td ltx_align_left\">CLIPL</td>\n<td id=\"S3.T2.1.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td id=\"S3.T2.1.1.9.9.4\" class=\"ltx_td ltx_align_center\">66.22\u00b10.43</td>\n<td id=\"S3.T2.1.1.9.9.5\" class=\"ltx_td ltx_align_center\">60.95</td>\n</tr>\n<tr id=\"S3.T2.1.1.10.10\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.10.10.1\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S3.T2.1.1.10.10.2\" class=\"ltx_td ltx_align_left\">CLIPL-A</td>\n<td id=\"S3.T2.1.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td id=\"S3.T2.1.1.10.10.4\" class=\"ltx_td ltx_align_center\">66.05\u00b10.37</td>\n<td id=\"S3.T2.1.1.10.10.5\" class=\"ltx_td ltx_align_center\">64.59</td>\n</tr>\n<tr id=\"S3.T2.1.1.11.11\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\">Multimodal Results</td>\n</tr>\n<tr id=\"S3.T2.1.1.12.12\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.12.12.1\" class=\"ltx_td ltx_align_left ltx_border_t\">HLFT(16-21)</td>\n<td id=\"S3.T2.1.1.12.12.2\" class=\"ltx_td ltx_align_left ltx_border_t\">CLIPL</td>\n<td id=\"S3.T2.1.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n<td id=\"S3.T2.1.1.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_t\">84.34\u00b10.25</td>\n<td id=\"S3.T2.1.1.12.12.5\" class=\"ltx_td ltx_align_center ltx_border_t\">86.78</td>\n</tr>\n<tr id=\"S3.T2.1.1.13.13\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.13.13.1\" class=\"ltx_td ltx_align_left\">HLFT(16-21)</td>\n<td id=\"S3.T2.1.1.13.13.2\" class=\"ltx_td ltx_align_left\">CLIPL-A</td>\n<td id=\"S3.T2.1.1.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td id=\"S3.T2.1.1.13.13.4\" class=\"ltx_td ltx_align_center\">84.70\u00b10.19</td>\n<td id=\"S3.T2.1.1.13.13.5\" class=\"ltx_td ltx_align_center\">87.01</td>\n</tr>\n<tr id=\"S3.T2.1.1.14.14\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.14.14.1\" class=\"ltx_td ltx_align_left\">HLFT(16-21)</td>\n<td id=\"S3.T2.1.1.14.14.2\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S3.T2.1.1.14.14.3\" class=\"ltx_td ltx_align_left ltx_border_r\">Baichuan2</td>\n<td id=\"S3.T2.1.1.14.14.4\" class=\"ltx_td ltx_align_center\">79.66\u00b10.13</td>\n<td id=\"S3.T2.1.1.14.14.5\" class=\"ltx_td ltx_align_center\">85.78</td>\n</tr>\n<tr id=\"S3.T2.1.1.15.15\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.15.15.1\" class=\"ltx_td ltx_align_left ltx_border_b\">HLFT(16-21)</td>\n<td id=\"S3.T2.1.1.15.15.2\" class=\"ltx_td ltx_align_left ltx_border_b\">CLIPL-A</td>\n<td id=\"S3.T2.1.1.15.15.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\">Baichuan2</td>\n<td id=\"S3.T2.1.1.15.15.4\" class=\"ltx_td ltx_align_center ltx_border_b\">83.85\u00b10.35</td>\n<td id=\"S3.T2.1.1.15.15.5\" class=\"ltx_td ltx_align_center ltx_border_b\">88.90</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We present the experimental results for both unimodal and multimodal approaches in Table 2. For the acoustic modality, HL represents features extracted using the baseline method (Lian et al., 2024) from the HuBERT-large model, with HL(i\ud835\udc56i) indicating the feature output from the i\ud835\udc56i-th layer. HLFT(i\ud835\udc56i) indicates the output from the i\ud835\udc56i-th layer after fine-tuning the HuBERT-large model with adapters. As shown in the table, the performance of the fine-tuned features demonstrates a notable improvement over the baseline model. The proposed parameter-efficient fine-tuning method achieves superior performance with multi-layer fused features compared to single-layer features. Specifically, HL(16-21) surpasses HL(18), and HLFT(16-21) outperforms HLFT(18), suggesting that complementary information exists among features from different layers, resulting in more robust results. Moreover, the multi-layer fused features obtained through the proposed parameter-efficient fine-tuning method achieve the highest F1 score of 84.88% on the test set. This method also demonstrates performance improvements of 7.42% on the Train&amp;Val set and 1.39% on the test set compared to the baseline model, validating its effectiveness.",
            "For the visual modality, CLIPL represents the features extracted using the CLIP-Large model, whereas CLIPL-A denotes the features aligned through the proposed visual feature alignment strategy. As shown in Table 2, in comparison to the CLIPL features, the CLIPL-A features exhibit comparable performance on the Train&amp;Val set and show a 3.64% improvement on the test set. These results affirm the efficacy of the visual feature alignment strategy in enhancing performance within multimodal emotion recognition tasks.",
            "We conduct a comprehensive comparison of the multimodal fusion effects, with specific results presented in Table 2. Initially, we fuse the best-performing acoustic modality features, HLFT (16-21), with the visual modality features extracted by CLIP-large. This fusion improves the recognition accuracy from 84.88% to 86.78%. Furthermore, using the aligned features extracted by the pre-trained vision MLP, the recognition accuracy further increases to 87.01%, which provides additional validation for the effectiveness of the feature alignment pre-training method. Similarly, we evaluate the fusion of lexical modality and acoustic modality. When fusing Baichuan2 features with HLFT (16-21) features, the performance on the test set reaches 85.78%. Ultimately, the fusion of all three modality features results in the highest performance of 88.90% on the test set. Additionally, the table illustrates that the effect of multimodal fusion on the test set surpasses that of any single modality."
        ]
    }
}