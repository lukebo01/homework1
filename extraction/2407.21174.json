{
    "S3.T1": {
        "caption": "Table 1.  Examples of adversarial perturbations from the Flickr dataset (top three rows) and COCO (bottom two rows).",
        "table": "<figure id=\"S3.T1\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1. </span>Examples of adversarial perturbations from the Flickr dataset (top three rows) and COCO (bottom two rows).</figcaption>\n<table id=\"S3.T1.15\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.15.16.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.15.16.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S3.T1.15.16.1.1.1\" class=\"ltx_text ltx_font_bold\">Original</span></th>\n<th id=\"S3.T1.15.16.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S3.T1.15.16.1.2.1\" class=\"ltx_text ltx_font_bold\">Perturbed Example</span></th>\n<th id=\"S3.T1.15.16.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S3.T1.15.16.1.3.1\" class=\"ltx_text ltx_font_bold\">Difference</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/clean_image_new1.png\" id=\"S3.T1.1.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"S3.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/perturbed_image_new1.png\" id=\"S3.T1.2.2.2.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"S3.T1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/diff_image_new1.png\" id=\"S3.T1.3.3.3.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n</tr>\n<tr id=\"S3.T1.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/clean_image_new2.png\" id=\"S3.T1.4.4.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"S3.T1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/perturbed_image_new2.png\" id=\"S3.T1.5.5.2.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"S3.T1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/diff_image_new2.png\" id=\"S3.T1.6.6.3.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n</tr>\n<tr id=\"S3.T1.9.9\" class=\"ltx_tr\">\n<td id=\"S3.T1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/clean_image_new3.png\" id=\"S3.T1.7.7.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"S3.T1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/perturbed_image_new3.png\" id=\"S3.T1.8.8.2.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"S3.T1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/diff_image_new3.png\" id=\"S3.T1.9.9.3.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n</tr>\n<tr id=\"S3.T1.12.12\" class=\"ltx_tr\">\n<td id=\"S3.T1.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/clean_image_coco1.png\" id=\"S3.T1.10.10.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"S3.T1.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/perturbed_image_coco1.png\" id=\"S3.T1.11.11.2.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"S3.T1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/diff_image_coco1.png\" id=\"S3.T1.12.12.3.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n</tr>\n<tr id=\"S3.T1.15.15\" class=\"ltx_tr\">\n<td id=\"S3.T1.13.13.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/clean_image_coco.png\" id=\"S3.T1.13.13.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"S3.T1.14.14.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/perturbed_image_coco.png\" id=\"S3.T1.14.14.2.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"S3.T1.15.15.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><img src=\"/html/2407.21174/assets/saved_images/diff_image_coco.png\" id=\"S3.T1.15.15.3.g1\" class=\"ltx_graphics ltx_img_square\" width=\"132\" height=\"132\" alt=\"[Uncaptioned image]\"></td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Table 1 shows examples of the adversarial perturbations generated using FGSM on the Flickr8k and COCO datasets. Despite its simplicity, FGSM is effective in generating adversarial examples that are close to the original inputs but cause the model to make incorrect predictions. This method is computationally efficient and serves as a useful tool for testing and improving the robustness of machine learning models. This table shows how adversarial attacks are imperceptible to human eyes, as shown in the second column. The third column demonstrates the perturbations by showing the difference between the original and perturbed images. In the \u2019Difference\u2019 column, we can see square or patch-like patterns. This happens because of ViT\u2019s architecture, which processes input images by dividing them into patches. FGSM leverages the gradients computed for each patch, generating perturbations that exploit the vulnerabilities within these regions. Table 1 underscores the necessity for robust models capable of defying such attacks. By incorporating adversarial examples into the training process, we can enhance the model\u2019s ability to defend against such attacks and improve its overall robustness."
        ]
    },
    "S4.T2": {
        "caption": "Table 2.  Model performance on training and testing datasets using Flickr8k dataset.",
        "table": "<figure id=\"S4.T2\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2. </span>Model performance on training and testing datasets using Flickr8k dataset.</figcaption>\n<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Model</th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\">Train Set</th>\n<th id=\"S4.T2.1.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\">Test Set</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Baseline BLEU Score</th>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">0.285</td>\n<td id=\"S4.T2.1.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\">0.232</td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Adversarial Example</th>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_right\">0.164</td>\n<td id=\"S4.T2.1.3.2.3\" class=\"ltx_td ltx_align_right\">0.143</td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Adversarial Training</th>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_right\">0.217</td>\n<td id=\"S4.T2.1.4.3.3\" class=\"ltx_td ltx_align_right\">0.215</td>\n</tr>\n<tr id=\"S4.T2.1.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Adversarial Training by freezing ViT</th>\n<td id=\"S4.T2.1.5.4.2\" class=\"ltx_td ltx_align_right\">0.200</td>\n<td id=\"S4.T2.1.5.4.3\" class=\"ltx_td ltx_align_right\">0.181</td>\n</tr>\n<tr id=\"S4.T2.1.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">Adversarial Training by freezing GPT</th>\n<td id=\"S4.T2.1.6.5.2\" class=\"ltx_td ltx_align_right ltx_border_b\">0.060</td>\n<td id=\"S4.T2.1.6.5.3\" class=\"ltx_td ltx_align_right ltx_border_b\">0.050</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "After conducting experiments on both the Flickr8k and COCO datasets, we evaluated the performance of our multimodal machine learning models using the BLEU score metric. The goal of these experiments was to find an efficient way to increase the robustness of these models against adversarial attacks. As shown in Table 2, our baseline model, trained on the original data, initially achieved the best performance. However, when we generated adversarial examples using the FGSM and trained the model with these samples, its performance decreased significantly. This suggests that our multimodal model is not robust against adversarial attacks, as generating a single adversarial example for each image in the dataset was sufficient to degrade its performance.",
            "Aiming to find an efficient way to increase the robustness of our multimodal model, we investigated whether training only one of its components (either the image model or the text model) would be sufficient. First, we froze the image model (ViT) and trained the text model with both the original data and the adversarial examples. As shown in Table 2, the model\u2019s performance in this case was lower than with full adversarial training, although the difference was not significant. Alternatively, we froze the text model (GPT) and trained the image model; in this case, we can see the performance is much lower than the full adversarial training\u2019s performance."
        ]
    },
    "S4.T3": {
        "caption": "Table 3.  Model performance on training and testing datasets using COCO dataset.",
        "table": "<figure id=\"S4.T3\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3. </span>Model performance on training and testing datasets using COCO dataset.</figcaption>\n<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Model</th>\n<th id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\">Train Set</th>\n<th id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\">Test Set</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Baseline BLEU Score</th>\n<td id=\"S4.T3.1.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">0.3000</td>\n<td id=\"S4.T3.1.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\">0.2812</td>\n</tr>\n<tr id=\"S4.T3.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Adversarial Example</th>\n<td id=\"S4.T3.1.3.2.2\" class=\"ltx_td ltx_align_right\">0.1464</td>\n<td id=\"S4.T3.1.3.2.3\" class=\"ltx_td ltx_align_right\">0.1430</td>\n</tr>\n<tr id=\"S4.T3.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Adversarial Training</th>\n<td id=\"S4.T3.1.4.3.2\" class=\"ltx_td ltx_align_right\">0.2910</td>\n<td id=\"S4.T3.1.4.3.3\" class=\"ltx_td ltx_align_right\">0.2890</td>\n</tr>\n<tr id=\"S4.T3.1.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Adversarial Training by freezing ViT</th>\n<td id=\"S4.T3.1.5.4.2\" class=\"ltx_td ltx_align_right\">0.2601</td>\n<td id=\"S4.T3.1.5.4.3\" class=\"ltx_td ltx_align_right\">0.2500</td>\n</tr>\n<tr id=\"S4.T3.1.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">Adversarial Training by freezing GPT</th>\n<td id=\"S4.T3.1.6.5.2\" class=\"ltx_td ltx_align_right ltx_border_b\">0.0900</td>\n<td id=\"S4.T3.1.6.5.3\" class=\"ltx_td ltx_align_right ltx_border_b\">0.0920</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We conducted similar experiments using the COCO dataset, and the results are presented in Table 3. The baseline performance on the COCO dataset was higher than on Flickr8k. However, when the model was trained solely on adversarial examples generated from the COCO dataset, the performance decreased significantly, as expected. In the third phase, where we trained the model on both the original data and the adversarial samples, the result was comparable to the baseline model\u2019s performance."
        ]
    }
}