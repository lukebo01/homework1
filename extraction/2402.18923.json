{
    "S2.T1": {
        "caption": "Table 1 :  Dataset statistics by severity of dysarthric speech corpus.",
        "table": "<figure id=\"S2.T1\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S2.T1.2.1.1\" class=\"ltx_text ltx_font_bold\">Table 1</span>: </span>Dataset statistics by severity of dysarthric speech corpus.</figcaption>\n<div id=\"S2.T1.3\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:48.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(54.9pt,-6.1pt) scale(1.33890517109985,1.33890517109985) ;\">\n<table id=\"S2.T1.3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">Severity</th>\n<th id=\"S2.T1.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">w/o dysarthria</th>\n<th id=\"S2.T1.3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Mild-to-Moderate</th>\n<th id=\"S2.T1.3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Severe</th>\n<th id=\"S2.T1.3.1.1.1.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Total</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_tt\"># of Utterances</td>\n<td id=\"S2.T1.3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt\">72</td>\n<td id=\"S2.T1.3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt\">1985</td>\n<td id=\"S2.T1.3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt\">194</td>\n<td id=\"S2.T1.3.1.2.1.5\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_tt\">2251</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We utilize the Korean dysarthric speech corpus, which comprises 2,251 utterances from the Autumn paragraph, recorded by stroke patients. Autumn paragraph contains all the necessary consonants and vowels for evaluation. Although recorded speeches are for paragraph reading, we experiment on the sentence level. This is because pauses within sentences carry more importance than those between sentences. The severity scale for this datset is the NIH Stoke Scale: without dysarthria, mild to moderate, and severe cases. Table 1 shows Korean dysarthric speech corpus statistics. We first label pause location at a text level to detect IP in dysarthric speech and then annotate each pause appropriateness."
        ]
    },
    "S4.T2": {
        "caption": "Table 2 :  Pause detection and ASR evaluation.  WER  and  CER  measure ASR performance, and  PauER  measures pause detection performance.",
        "table": "<figure id=\"S4.T2\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S4.T2.5.1.1\" class=\"ltx_text ltx_font_bold\">Table 2</span>: </span>Pause detection and ASR evaluation. <span id=\"S4.T2.6.2\" class=\"ltx_text ltx_font_italic\">WER</span> and <span id=\"S4.T2.7.3\" class=\"ltx_text ltx_font_italic\">CER</span> measure ASR performance, and <span id=\"S4.T2.8.4\" class=\"ltx_text ltx_font_italic\">PauER</span> measures pause detection performance.</figcaption>\n<div id=\"S4.T2.9\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:164.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(74.2pt,-28.1pt) scale(1.51998954642537,1.51998954642537) ;\">\n<table id=\"S4.T2.9.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.9.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.9.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"S4.T2.9.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">WER(%)</td>\n<td id=\"S4.T2.9.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">CER(%)</td>\n<td id=\"S4.T2.9.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">PauER(%)</td>\n</tr>\n<tr id=\"S4.T2.9.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.9.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\">MFA-GT</th>\n<td id=\"S4.T2.9.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">-</td>\n<td id=\"S4.T2.9.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">-</td>\n<td id=\"S4.T2.9.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">11.14</td>\n</tr>\n<tr id=\"S4.T2.9.1.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.9.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">MFA-Whisper</th>\n<td id=\"S4.T2.9.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">54.89</td>\n<td id=\"S4.T2.9.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">27.35</td>\n<td id=\"S4.T2.9.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">22.49</td>\n</tr>\n<tr id=\"S4.T2.9.1.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T2.9.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">MFA-Dysarthric-Whisper</th>\n<td id=\"S4.T2.9.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32.21</td>\n<td id=\"S4.T2.9.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">22.38</td>\n<td id=\"S4.T2.9.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">17.27</td>\n</tr>\n<tr id=\"S4.T2.9.1.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T2.9.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\">Conformer-RNNT</th>\n<td id=\"S4.T2.9.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">64.52</td>\n<td id=\"S4.T2.9.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">49.99</td>\n<td id=\"S4.T2.9.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">22.81</td>\n</tr>\n<tr id=\"S4.T2.9.1.6.6\" class=\"ltx_tr\">\n<th id=\"S4.T2.9.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">Ours</th>\n<td id=\"S4.T2.9.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T2.9.1.6.6.2.1\" class=\"ltx_text ltx_font_bold\">25.31</span></td>\n<td id=\"S4.T2.9.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T2.9.1.6.6.3.1\" class=\"ltx_text ltx_font_bold\">11.96</span></td>\n<td id=\"S4.T2.9.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S4.T2.9.1.6.6.4.1\" class=\"ltx_text ltx_font_bold\">3.077</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We compare the proposed method with Montreal Forced Alignment (MFA)[19].\nFor baseline, three types of MFA were used. MFA-GT aligns the ground truth transcription and speech. MFA-Whisper and MFA-Dysarthric-Whisper utilize ASR transcription from open-source Whisper and fine-tuned Whisper with dysarthric speech, respectively. (Table 2) Whisper fine-tuning was done with the same Korean dysarthric speech corpus, and the training details were the same.\nMoreover, we also train the proposed method, except with a different ASR model, Conformer-RNNT [20]. We used a Korean pre-trained Conformer model.111https://huggingface.co/eesungkim/stt_kr_conformer_transducer_large\nThe training details were maintained for the Conformer-RNNT model. Note that Conformer-RNNT does not have an IP prediction layer. We further discuss this in Section 5. Note that we only compare the pause detection performance with other methods because, to our best knowledge, we are the first to detect IPs in dysarthric speech automatically. Hence, there is no baseline.",
            "Table 2 shows the ASR and pause detection performance. Our method performs better than other methods in ASR and pause detection. Specifically, PauER is lower in our method than MFA-GT. When comparing three types of MFA, pause detection performance was influenced by transcription performance. The best results among MFAs were obtained using GT transcription, followed by Dysarthric-Whisper and Whisper. This trend persisted even for pause detection fine-tuned Conformer-RNNT model. Higher ASR performance correlated with improved pause detection performance. Additionally, comparing ASR performance, fine-tuning with dysarthric speech for ASR-only (MFA-Dysarthric-Whisper) versus jointly training for pause detection and ASR (Ours) showed enhanced ASR performance. This suggests a complementary relationship between pause detection and ASR in dysarthric speech, where prolonged pause intervals are prominent characteristics.",
            "Table 3 shows the IP detection performance of the proposed method. Total is the same model with Ours in Table 2. We dissect the performance according to the dysarthria severity. Dysarthric speech becomes more slurred and challenging to understand as its severity increases. An effective IP detection method operates robustly across different levels of severity. This is crucial to ensure that effective diagnosis and feedback can be provided even for patients with severe dysarthria. In Table 3, we can observe that IP and pause detection performance remains relatively consistent across different severity levels. In the case of IPER, performance deteriorates as severity decreases. This is because lower severity levels result in fewer instances of IPs. However, ASR performance decreases as the severity level increases. This suggests that the data used for training was sufficient for IP detection but insufficient for training ASR, leading to this discrepancy in performance."
        ]
    },
    "S4.T3": {
        "caption": "Table 3 :  IP detection, Pause detection, and ASR performance according to severity level",
        "table": "<figure id=\"S4.T3\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S4.T3.2.1.1\" class=\"ltx_text ltx_font_bold\">Table 3</span>: </span>IP detection, Pause detection, and ASR performance according to severity level</figcaption>\n<div id=\"S4.T3.3\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:128.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(65.0pt,-19.3pt) scale(1.42807543154882,1.42807543154882) ;\">\n<table id=\"S4.T3.3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Severity</th>\n<th id=\"S4.T3.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">WER(%)</th>\n<th id=\"S4.T3.3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">CER(%)</th>\n<th id=\"S4.T3.3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">PauER(%)</th>\n<th id=\"S4.T3.3.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">IPER(%)</th>\n</tr>\n<tr id=\"S4.T3.3.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">Total</th>\n<th id=\"S4.T3.3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">25.31</th>\n<th id=\"S4.T3.3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">11.96</th>\n<th id=\"S4.T3.3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">3.07</th>\n<th id=\"S4.T3.3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">14.47</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.3.1.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\">w/o dysarthria</th>\n<td id=\"S4.T3.3.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">6.93</td>\n<td id=\"S4.T3.3.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">2.89</td>\n<td id=\"S4.T3.3.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">2.48</td>\n<td id=\"S4.T3.3.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">20.69</td>\n</tr>\n<tr id=\"S4.T3.3.1.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Mild-to-Moderate</th>\n<td id=\"S4.T3.3.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">22.38</td>\n<td id=\"S4.T3.3.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10.20</td>\n<td id=\"S4.T3.3.1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.03</td>\n<td id=\"S4.T3.3.1.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">15.53</td>\n</tr>\n<tr id=\"S4.T3.3.1.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">Severe</th>\n<td id=\"S4.T3.3.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">57.44</td>\n<td id=\"S4.T3.3.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">30.47</td>\n<td id=\"S4.T3.3.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">3.60</td>\n<td id=\"S4.T3.3.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">13.40</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Table 3 shows the IP detection performance of the proposed method. Total is the same model with Ours in Table 2. We dissect the performance according to the dysarthria severity. Dysarthric speech becomes more slurred and challenging to understand as its severity increases. An effective IP detection method operates robustly across different levels of severity. This is crucial to ensure that effective diagnosis and feedback can be provided even for patients with severe dysarthria. In Table 3, we can observe that IP and pause detection performance remains relatively consistent across different severity levels. In the case of IPER, performance deteriorates as severity decreases. This is because lower severity levels result in fewer instances of IPs. However, ASR performance decreases as the severity level increases. This suggests that the data used for training was sufficient for IP detection but insufficient for training ASR, leading to this discrepancy in performance.",
            "Table 3 shows the IP detection performance of the proposed method. Total is the same model with Ours in Table 2. We dissect the performance according to the dysarthria severity. Dysarthric speech becomes more slurred and challenging to understand as its severity increases. An effective IP detection method operates robustly across different levels of severity. This is crucial to ensure that effective diagnosis and feedback can be provided even for patients with severe dysarthria. In Table 3, we can observe that IP and pause detection performance remains relatively consistent across different severity levels. In the case of IPER, performance deteriorates as severity decreases. This is because lower severity levels result in fewer instances of IPs. However, ASR performance decreases as the severity level increases. This suggests that the data used for training was sufficient for IP detection but insufficient for training ASR, leading to this discrepancy in performance."
        ]
    }
}