{
    "S5.T1": {
        "caption": "Table 1:  WERs (%) for HAT with neural contextual biasing. 3 3 3 Without contextual biasing, HAT with JOIST and text-injected MWER achieves 4.0%, 22.1%, 10.5% and 1.6% WERs on VS, NO_PRE, PRE and ANTI, respectively. \u2003The models are trained from scratch using supervised data (Sup), JOIST or contextual text injection (CTI).",
        "table": "<figure id=\"S5.T1\" class=\"ltx_table\">\n<table id=\"S5.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T1.1.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S5.T1.1.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T1.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T1.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Method</span></span>\n</span></span></th>\n<th id=\"S5.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T1.1.1.1.2.1\" class=\"ltx_text\">\n<span id=\"S5.T1.1.1.1.2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T1.1.1.1.2.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T1.1.1.1.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">\u00a0\u00a0\u00a0 VS</span></span>\n</span></span></th>\n<th id=\"S5.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">In-Context</th>\n<th id=\"S5.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T1.1.1.1.4.1\" class=\"ltx_text\">\n<span id=\"S5.T1.1.1.1.4.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T1.1.1.1.4.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T1.1.1.1.4.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">\u2005 ANTI</span></span>\n</span></span></th>\n</tr>\n<tr id=\"S5.T1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">NO_PRE</th>\n<th id=\"S5.T1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\u00a0\u00a0 PRE</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Sup</th>\n<th id=\"S5.T1.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\">4.1</th>\n<td id=\"S5.T1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.0</td>\n<td id=\"S5.T1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.3</td>\n<td id=\"S5.T1.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">1.8</td>\n</tr>\n<tr id=\"S5.T1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">JOIST</th>\n<th id=\"S5.T1.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\">4.0</th>\n<td id=\"S5.T1.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">2.2</td>\n<td id=\"S5.T1.1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">2.1</td>\n<td id=\"S5.T1.1.4.2.5\" class=\"ltx_td ltx_align_center\">1.9</td>\n</tr>\n<tr id=\"S5.T1.1.5.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">CTI</th>\n<th id=\"S5.T1.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_rr\"><span id=\"S5.T1.1.5.3.2.1\" class=\"ltx_text ltx_font_bold\">3.9</span></th>\n<td id=\"S5.T1.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T1.1.5.3.3.1\" class=\"ltx_text ltx_font_bold\">1.7</span></td>\n<td id=\"S5.T1.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T1.1.5.3.4.1\" class=\"ltx_text ltx_font_bold\">1.7</span></td>\n<td id=\"S5.T1.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.1.5.3.5.1\" class=\"ltx_text ltx_font_bold\">1.6</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>WERs (%) for HAT with neural contextual biasing.<span id=\"footnote3\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Without contextual biasing, HAT with JOIST and text-injected MWER achieves 4.0%, 22.1%, 10.5% and 1.6% WERs on VS, NO_PRE, PRE and ANTI, respectively.</span></span></span>\u2003The models are trained from scratch using supervised data (Sup), JOIST or contextual text injection (CTI).</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": []
    },
    "S5.T2": {
        "caption": "Table 2:  WERs (%) for HAT with neural contextual biasing. The models are trained in two stages: first with contextual text injection (CTI), and then fine-tuned using MWER, JOIST-based MWER, or contextual text-injected (CTI) MWER training.",
        "table": "<figure id=\"S5.T2\" class=\"ltx_table\">\n<table id=\"S5.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" rowspan=\"2\"><span id=\"S5.T2.1.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S5.T2.1.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\u00a0\u00a0\u00a0\u00a0 Method</span></span>\n</span></span></th>\n<th id=\"S5.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" rowspan=\"2\"><span id=\"S5.T2.1.1.1.2.1\" class=\"ltx_text\">\n<span id=\"S5.T2.1.1.1.2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.1.1.1.2.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.1.1.1.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\u00a0\u00a0\u00a0 VS</span></span>\n</span></span></th>\n<th id=\"S5.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" colspan=\"2\">In-Context</th>\n<th id=\"S5.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" rowspan=\"2\"><span id=\"S5.T2.1.1.1.4.1\" class=\"ltx_text\">\n<span id=\"S5.T2.1.1.1.4.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.1.1.1.4.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.1.1.1.4.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\u2005 ANTI</span></span>\n</span></span></th>\n</tr>\n<tr id=\"S5.T2.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">NO_PRE</th>\n<th id=\"S5.T2.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">\u00a0\u00a0 PRE</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">CTI</th>\n<th id=\"S5.T2.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">3.9</th>\n<td id=\"S5.T2.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">1.7</td>\n<td id=\"S5.T2.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">1.7</td>\n<td id=\"S5.T2.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S5.T2.1.3.1.5.1\" class=\"ltx_text ltx_font_bold\">1.6</span></td>\n</tr>\n<tr id=\"S5.T2.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">+ MWER</th>\n<th id=\"S5.T2.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">3.9</th>\n<td id=\"S5.T2.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">1.7</td>\n<td id=\"S5.T2.1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">1.5</td>\n<td id=\"S5.T2.1.4.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">1.9</td>\n</tr>\n<tr id=\"S5.T2.1.5.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">+ JOIST MWER</th>\n<th id=\"S5.T2.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S5.T2.1.5.3.2.1\" class=\"ltx_text ltx_font_bold\">3.8</span></th>\n<td id=\"S5.T2.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">1.5</td>\n<td id=\"S5.T2.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">1.4</td>\n<td id=\"S5.T2.1.5.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">1.9</td>\n</tr>\n<tr id=\"S5.T2.1.6.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">+ CTI MWER</th>\n<th id=\"S5.T2.1.6.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_rr\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S5.T2.1.6.4.2.1\" class=\"ltx_text ltx_font_bold\">3.8</span></th>\n<td id=\"S5.T2.1.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S5.T2.1.6.4.3.1\" class=\"ltx_text ltx_font_bold\">1.3</span></td>\n<td id=\"S5.T2.1.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S5.T2.1.6.4.4.1\" class=\"ltx_text ltx_font_bold\">1.3</span></td>\n<td id=\"S5.T2.1.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">1.8</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>WERs (%) for HAT with neural contextual biasing. The models are trained in two stages: first with contextual text injection (CTI), and then fine-tuned using MWER, JOIST-based MWER, or contextual text-injected (CTI) MWER training.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "As in Table 2, MWER significantly improves over CTI on the in-context PRE and the head VS, but degrades on the anti-context test set. We then perform JOIST-based MWER [33] by injecting text into only the HAT encoder. During this process, the biasing modules are updated with only the supervised data. Compared to the MWER training, JOIST-based MWER dramatically reduces WERs on NO_PRE. Finally, we perform contextual text-injected MWER (Section 4), updating both HAT and biasing modules with supervised data and unpaired text. CTI MWER significantly outperforms JOIST-based MWER across VS and all biasing sets.\nMoreover, CTI MWER offers remarkable improvement over CTI on all test sets, achieving total relative WER reductions of a 56.7%, 43.5% on in-context biasing sets, and 7.3% on head VS from the supervised baseline.\nNote that, to reduce the WER also on head VS, we selectively drop out the neural biasing component for 30% of the training mini-batches."
        ]
    },
    "S5.T3": {
        "caption": "Table 3:  WERs (%) for supervised training, contextual text injection (CTI) and contextual text-injected (CTI) MWER training with respect to the number of bias phrases (0-3000) assigned to each test utterance.",
        "table": "<figure id=\"S5.T3\" class=\"ltx_table\">\n<table id=\"S5.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:3.8pt;padding-right:3.8pt;\" rowspan=\"2\"><span id=\"S5.T3.1.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S5.T3.1.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">\u00a0\u00a0\u00a0 Method</span></span>\n</span></span></th>\n<th id=\"S5.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_tt\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">VS</th>\n<td id=\"S5.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.8pt;padding-right:3.8pt;\" colspan=\"6\">In-Context: NO_PRE</td>\n</tr>\n<tr id=\"S5.T3.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">\u00a0\u00a00</th>\n<td id=\"S5.T3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">0</td>\n<td id=\"S5.T3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">\u2006150</td>\n<td id=\"S5.T3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">\u2006300</td>\n<td id=\"S5.T3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">\u2006600</td>\n<td id=\"S5.T3.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">1500</td>\n<td id=\"S5.T3.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">3000</td>\n</tr>\n<tr id=\"S5.T3.1.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">Sup</th>\n<th id=\"S5.T3.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">4.1</th>\n<td id=\"S5.T3.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">22.2</td>\n<td id=\"S5.T3.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">2.7</td>\n<td id=\"S5.T3.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">2.7</td>\n<td id=\"S5.T3.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">2.8</td>\n<td id=\"S5.T3.1.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">3.4</td>\n<td id=\"S5.T3.1.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">3.5</td>\n</tr>\n<tr id=\"S5.T3.1.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">CTI</th>\n<th id=\"S5.T3.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">3.9</th>\n<td id=\"S5.T3.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\"><span id=\"S5.T3.1.4.4.3.1\" class=\"ltx_text ltx_font_bold\">21.8</span></td>\n<td id=\"S5.T3.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">1.3</td>\n<td id=\"S5.T3.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">1.4</td>\n<td id=\"S5.T3.1.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">1.7</td>\n<td id=\"S5.T3.1.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">1.8</td>\n<td id=\"S5.T3.1.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">2.1</td>\n</tr>\n<tr id=\"S5.T3.1.5.5\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:3.8pt;padding-right:3.8pt;\">+ CTI MWER</th>\n<th id=\"S5.T3.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_rr\" style=\"padding-left:3.8pt;padding-right:3.8pt;\"><span id=\"S5.T3.1.5.5.2.1\" class=\"ltx_text ltx_font_bold\">3.8</span></th>\n<td id=\"S5.T3.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:3.8pt;padding-right:3.8pt;\"><span id=\"S5.T3.1.5.5.3.1\" class=\"ltx_text ltx_font_bold\">21.8</span></td>\n<td id=\"S5.T3.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:3.8pt;padding-right:3.8pt;\"><span id=\"S5.T3.1.5.5.4.1\" class=\"ltx_text ltx_font_bold\">1.1</span></td>\n<td id=\"S5.T3.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:3.8pt;padding-right:3.8pt;\"><span id=\"S5.T3.1.5.5.5.1\" class=\"ltx_text ltx_font_bold\">1.2</span></td>\n<td id=\"S5.T3.1.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:3.8pt;padding-right:3.8pt;\"><span id=\"S5.T3.1.5.5.6.1\" class=\"ltx_text ltx_font_bold\">1.2</span></td>\n<td id=\"S5.T3.1.5.5.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:3.8pt;padding-right:3.8pt;\"><span id=\"S5.T3.1.5.5.7.1\" class=\"ltx_text ltx_font_bold\">1.4</span></td>\n<td id=\"S5.T3.1.5.5.8\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.8pt;padding-right:3.8pt;\"><span id=\"S5.T3.1.5.5.8.1\" class=\"ltx_text ltx_font_bold\">1.8</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>WERs (%) for supervised training, contextual text injection (CTI) and contextual text-injected (CTI) MWER training with respect to the number of bias phrases (0-3000) assigned to each test utterance.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We first train a baseline HAT with deferred NAM [18] using only the supervised data, and show its results in Table 3.\n\u03bba=0.9,\u03bbr=0.1,K=32formulae-sequencesubscript\ud835\udf06\ud835\udc4e0.9formulae-sequencesubscript\ud835\udf06\ud835\udc5f0.1\ud835\udc3e32\\lambda_{a}=0.9,\\lambda_{r}=0.1,K=32 for all the experiments. Note that, for each biasing test set, the reported WER is the average across 5 different scenarios where the model receives varying number of bias entities.",
            "We gradually increase the number of bias phrases assigned to each test utterance (0-3,000) when evaluating HAT with deferred NAM on the NO_PRE in-context biasing set in Table 3.\nDespite the consistent increase in WER with more bias phrases, CTI and CTI MWER remarkably maintain WER below 1.8% even at the maximum 3000 bias phrases. Importantly, the performance trends observed in Sections 5.3, 5.4 persist across all bias phrase counts: CTI and CTI MWER outperform the baseline supervised model, with their advantage increasing as the number of phrases decreases. These trends hold consistently for PRE and ANTI biasing sets as well."
        ]
    }
}