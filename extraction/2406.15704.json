{
    "S4.T1": {
        "caption": "Table 1:  SAVE benchmark details, including the number of samples used for evaluation and metrics reported. Since TextVQA, GQA, NExT-QA and VGGSS test sets are large, randomly sampled subsets with enough samples for statistical significance were used for efficient evaluation. Zero-shot refers to both instruction and audio-visual inputs that are unseen in the training set. Note that Presentation-QA\nis newly proposed AVQA test sets focusing on speech-audio-visual joint information.",
        "table": "<figure id=\"S4.T1\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>SAVE benchmark details, including the number of samples used for evaluation and metrics reported. Since TextVQA, GQA, NExT-QA and VGGSS test sets are large, randomly sampled subsets with enough samples for statistical significance were used for efficient evaluation. Zero-shot refers to both instruction and audio-visual inputs that are unseen in the training set. Note that Presentation-QA\nis newly proposed AVQA test sets focusing on speech-audio-visual joint information.</figcaption>\n<table id=\"S4.T1.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S4.T1.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Task</span></td>\n<td id=\"S4.T1.3.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S4.T1.3.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Test set</span></td>\n<td id=\"S4.T1.3.1.1.3\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S4.T1.3.1.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">#samples</span></td>\n<td id=\"S4.T1.3.1.1.4\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S4.T1.3.1.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Metrics</span></td>\n<td id=\"S4.T1.3.1.1.5\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S4.T1.3.1.1.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Zero-shot</span></td>\n</tr>\n<tr id=\"S4.T1.3.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T1.3.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ASR</span></td>\n<td id=\"S4.T1.3.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T1.3.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">LibriSpeech test-clean </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T1.3.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Panayotov et\u00a0al.<span id=\"S4.T1.3.2.2.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2015</a><span id=\"S4.T1.3.2.2.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S4.T1.3.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T1.3.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">2620</span></td>\n<td id=\"S4.T1.3.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T1.3.2.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">WER</span></td>\n<td id=\"S4.T1.3.2.2.5\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T1.3.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">No</span></td>\n</tr>\n<tr id=\"S4.T1.3.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.3.3.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.3.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">AAC</span></td>\n<td id=\"S4.T1.3.3.3.2\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T1.3.3.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">AudioCaps test </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T1.3.3.3.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Kim et\u00a0al.<span id=\"S4.T1.3.3.3.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S4.T1.3.3.3.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S4.T1.3.3.3.3\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">938</span></td>\n<td id=\"S4.T1.3.3.3.4\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.3.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">SPIDEr</span></td>\n<td id=\"S4.T1.3.3.3.5\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.3.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">No</span></td>\n</tr>\n<tr id=\"S4.T1.3.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.4.4.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.4.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">IC</span></td>\n<td id=\"S4.T1.3.4.4.2\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T1.3.4.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Flickr30k test </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T1.3.4.4.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Young et\u00a0al.<span id=\"S4.T1.3.4.4.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">2014</a><span id=\"S4.T1.3.4.4.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S4.T1.3.4.4.3\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.4.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">1000</span></td>\n<td id=\"S4.T1.3.4.4.4\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.4.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIDEr</span></td>\n<td id=\"S4.T1.3.4.4.5\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.4.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n</tr>\n<tr id=\"S4.T1.3.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.5.5.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">OCR</span></td>\n<td id=\"S4.T1.3.5.5.2\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T1.3.5.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">TextVQA test </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T1.3.5.5.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Singh et\u00a0al.<span id=\"S4.T1.3.5.5.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S4.T1.3.5.5.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S4.T1.3.5.5.3\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.5.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">1000</span></td>\n<td id=\"S4.T1.3.5.5.4\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.5.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">Accuracy</span></td>\n<td id=\"S4.T1.3.5.5.5\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.5.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n</tr>\n<tr id=\"S4.T1.3.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.6.6.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.6.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">VQA</span></td>\n<td id=\"S4.T1.3.6.6.2\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T1.3.6.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">GQA test dev balanced </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T1.3.6.6.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Hudson &amp; Manning<span id=\"S4.T1.3.6.6.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S4.T1.3.6.6.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S4.T1.3.6.6.3\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.6.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">1000</span></td>\n<td id=\"S4.T1.3.6.6.4\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.6.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">Accuracy</span></td>\n<td id=\"S4.T1.3.6.6.5\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.6.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n</tr>\n<tr id=\"S4.T1.3.7.7\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.7.7.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.7.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Video QA</span></td>\n<td id=\"S4.T1.3.7.7.2\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T1.3.7.7.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">NExT-QA test </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T1.3.7.7.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Xiao et\u00a0al.<span id=\"S4.T1.3.7.7.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">2021</a><span id=\"S4.T1.3.7.7.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S4.T1.3.7.7.3\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.7.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">1000</span></td>\n<td id=\"S4.T1.3.7.7.4\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.7.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">Accuracy</span></td>\n<td id=\"S4.T1.3.7.7.5\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.7.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n</tr>\n<tr id=\"S4.T1.3.8.8\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.8.8.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T1.3.8.8.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">AVSR</span></td>\n<td id=\"S4.T1.3.8.8.2\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T1.3.8.8.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">How2 dev5 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T1.3.8.8.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sanabria et\u00a0al.<span id=\"S4.T1.3.8.8.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S4.T1.3.8.8.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S4.T1.3.8.8.3\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T1.3.8.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">500</span></td>\n<td id=\"S4.T1.3.8.8.4\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T1.3.8.8.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">WER</span></td>\n<td id=\"S4.T1.3.8.8.5\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T1.3.8.8.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">No</span></td>\n</tr>\n<tr id=\"S4.T1.3.9.9\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.9.9.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.9.9.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">AVQA</span></td>\n<td id=\"S4.T1.3.9.9.2\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T1.3.9.9.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Ego4D </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T1.3.9.9.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Grauman et\u00a0al.<span id=\"S4.T1.3.9.9.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2022</a><span id=\"S4.T1.3.9.9.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S4.T1.3.9.9.2.5\" class=\"ltx_text\" style=\"font-size:80%;\"> + Presentation-QA</span>\n</td>\n<td id=\"S4.T1.3.9.9.3\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.9.9.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">2000</span></td>\n<td id=\"S4.T1.3.9.9.4\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.9.9.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">Accuracy</span></td>\n<td id=\"S4.T1.3.9.9.5\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.9.9.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n</tr>\n<tr id=\"S4.T1.3.10.10\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.10.10.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.10.10.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">AVSSD</span></td>\n<td id=\"S4.T1.3.10.10.2\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T1.3.10.10.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">VGGSS </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T1.3.10.10.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Chen et\u00a0al.<span id=\"S4.T1.3.10.10.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2020</a>; Zhao et\u00a0al.<span id=\"S4.T1.3.10.10.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib57\" title=\"\" class=\"ltx_ref\">2023</a><span id=\"S4.T1.3.10.10.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S4.T1.3.10.10.3\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.10.10.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">850</span></td>\n<td id=\"S4.T1.3.10.10.4\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.10.10.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">Accuracy</span></td>\n<td id=\"S4.T1.3.10.10.5\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.3.10.10.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n</tr>\n<tr id=\"S4.T1.3.11.11\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.11.11.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S4.T1.3.11.11.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">AVM</span></td>\n<td id=\"S4.T1.3.11.11.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span id=\"S4.T1.3.11.11.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">SpokenCOCO </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T1.3.11.11.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Hsu et\u00a0al.<span id=\"S4.T1.3.11.11.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S4.T1.3.11.11.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S4.T1.3.11.11.2.5\" class=\"ltx_text\" style=\"font-size:80%;\"> + VGGSS</span>\n</td>\n<td id=\"S4.T1.3.11.11.3\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S4.T1.3.11.11.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">1000</span></td>\n<td id=\"S4.T1.3.11.11.4\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S4.T1.3.11.11.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">Accuracy</span></td>\n<td id=\"S4.T1.3.11.11.5\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S4.T1.3.11.11.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [
            "SAVE benchmark details, including the number of samples used for evaluation and metrics reported. Since TextVQA, GQA, NExT-QA and VGGSS test sets are large, randomly sampled subsets with enough samples for statistical significance were used for efficient evaluation. Zero-shot refers to both instruction and audio-visual inputs that are unseen in the training set. Note that Presentation-QA\nis newly proposed AVQA test sets focusing on speech-audio-visual joint information."
        ],
        "references": [
            "In particular, we curate Ego4D-QA and Presentation-QA test sets to evaluate accuracy in audio-visual understanding with speech. The questions for the two sets are generated by prompting GPT-4 with video descriptions and ASR transcriptions for each video clip. Detailed examples for AVQA datasets are in Appendix B. The SAVE benchmark is summarised in Table 1, and details about evaluation metrics can be found in Appendix C."
        ]
    },
    "S4.T2": {
        "caption": "Table 2:  The SAVE benchmark single-modal task results.\nIf specified, InstructBLIP is fine-tuned on the training data of video-SALMONN (\u201cInstructBLIP fine-tuned\u201d). Evaluation metrics can be found in Appendix  C . When using visual-only inputs, the other modality is masked during training and inference. Tasks unable to be performed are marked with \u201c-\u201d.",
        "table": "<figure id=\"S4.T2\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>The SAVE benchmark single-modal task results.\nIf specified, InstructBLIP is fine-tuned on the training data of video-SALMONN (\u201cInstructBLIP fine-tuned\u201d). Evaluation metrics can be found in Appendix <a href=\"#A3\" title=\"Appendix C Evaluation Details \u2023 video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. When using visual-only inputs, the other modality is masked during training and inference. Tasks unable to be performed are marked with \u201c-\u201d.</figcaption>\n<table id=\"S4.T2.6\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.6.6.7\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S4.T2.6.6.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Systems</span></td>\n<td id=\"S4.T2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T2.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">ASR</span><span id=\"S4.T2.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T2.1.1.1.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T2.1.1.1.m1.1.1\" xref=\"S4.T2.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.m1.1b\"><ci id=\"S4.T2.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>\n</td>\n<td id=\"S4.T2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T2.2.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">AC</span><span id=\"S4.T2.2.2.2.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T2.2.2.2.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T2.2.2.2.m1.1.1\" xref=\"S4.T2.2.2.2.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.2.2.2.m1.1b\"><ci id=\"S4.T2.2.2.2.m1.1.1.cmml\" xref=\"S4.T2.2.2.2.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.2.2.2.m1.1c\">\\uparrow</annotation></semantics></math>\n</td>\n<td id=\"S4.T2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T2.3.3.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Video QA</span><span id=\"S4.T2.3.3.3.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T2.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T2.3.3.3.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T2.3.3.3.m1.1.1\" xref=\"S4.T2.3.3.3.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.3.3.3.m1.1b\"><ci id=\"S4.T2.3.3.3.m1.1.1.cmml\" xref=\"S4.T2.3.3.3.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.3.3.3.m1.1c\">\\uparrow</annotation></semantics></math>\n</td>\n<td id=\"S4.T2.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T2.4.4.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">IC</span><span id=\"S4.T2.4.4.4.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T2.4.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T2.4.4.4.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T2.4.4.4.m1.1.1\" xref=\"S4.T2.4.4.4.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.4.4.4.m1.1b\"><ci id=\"S4.T2.4.4.4.m1.1.1.cmml\" xref=\"S4.T2.4.4.4.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.4.4.4.m1.1c\">\\uparrow</annotation></semantics></math>\n</td>\n<td id=\"S4.T2.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T2.5.5.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">OCR</span><span id=\"S4.T2.5.5.5.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T2.5.5.5.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T2.5.5.5.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T2.5.5.5.m1.1.1\" xref=\"S4.T2.5.5.5.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.5.5.5.m1.1b\"><ci id=\"S4.T2.5.5.5.m1.1.1.cmml\" xref=\"S4.T2.5.5.5.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.5.5.5.m1.1c\">\\uparrow</annotation></semantics></math>\n</td>\n<td id=\"S4.T2.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T2.6.6.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">VQA</span><span id=\"S4.T2.6.6.6.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T2.6.6.6.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T2.6.6.6.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T2.6.6.6.m1.1.1\" xref=\"S4.T2.6.6.6.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.6.6.6.m1.1b\"><ci id=\"S4.T2.6.6.6.m1.1.1.cmml\" xref=\"S4.T2.6.6.6.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.6.6.6.m1.1c\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S4.T2.6.7.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.6.7.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T2.6.7.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Whisper large-v2</span></td>\n<td id=\"S4.T2.6.7.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.6.7.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">2.9%</span></td>\n<td id=\"S4.T2.6.7.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.6.7.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T2.6.7.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.6.7.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T2.6.7.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.6.7.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T2.6.7.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.6.7.1.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T2.6.7.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.6.7.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr id=\"S4.T2.6.8.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.6.8.2.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T2.6.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">InstructBLIP 13B </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T2.6.8.2.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Dai et\u00a0al.<span id=\"S4.T2.6.8.2.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2023</a><span id=\"S4.T2.6.8.2.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S4.T2.6.8.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.8.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T2.6.8.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.8.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T2.6.8.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.8.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">21.0%</span></td>\n<td id=\"S4.T2.6.8.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.8.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.5</span></td>\n<td id=\"S4.T2.6.8.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.8.2.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">36.5%</span></td>\n<td id=\"S4.T2.6.8.2.7\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.T2.6.8.2.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">48.9</span><span id=\"S4.T2.6.8.2.7.2\" class=\"ltx_text\" style=\"font-size:80%;\">%</span>\n</td>\n</tr>\n<tr id=\"S4.T2.6.9.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.6.9.3.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T2.6.9.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">InstructBLIP 13B fine-tuned</span></td>\n<td id=\"S4.T2.6.9.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.9.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T2.6.9.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.9.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T2.6.9.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.9.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.7%</span></td>\n<td id=\"S4.T2.6.9.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.9.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">78.9</span></td>\n<td id=\"S4.T2.6.9.3.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.9.3.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">36.7%</span></td>\n<td id=\"S4.T2.6.9.3.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.9.3.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">45.6%</span></td>\n</tr>\n<tr id=\"S4.T2.6.10.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.6.10.4.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T2.6.10.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Video-LLaMA 7B </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T2.6.10.4.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Zhang et\u00a0al.<span id=\"S4.T2.6.10.4.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">2023b</a><span id=\"S4.T2.6.10.4.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S4.T2.6.10.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.10.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">100%+</span></td>\n<td id=\"S4.T2.6.10.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.10.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">3.5</span></td>\n<td id=\"S4.T2.6.10.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.10.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">22.5%</span></td>\n<td id=\"S4.T2.6.10.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.10.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">22.0</span></td>\n<td id=\"S4.T2.6.10.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.10.4.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">16.4%</span></td>\n<td id=\"S4.T2.6.10.4.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.10.4.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">15.1%</span></td>\n</tr>\n<tr id=\"S4.T2.6.11.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.6.11.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T2.6.11.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">video-SALMONN 13B (ours, visual-only)</span></td>\n<td id=\"S4.T2.6.11.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.6.11.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T2.6.11.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.6.11.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T2.6.11.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.6.11.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">44.8%</span></td>\n<td id=\"S4.T2.6.11.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.6.11.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">74.0</span></td>\n<td id=\"S4.T2.6.11.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.6.11.5.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">34.2%</span></td>\n<td id=\"S4.T2.6.11.5.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.6.11.5.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">45.6%</span></td>\n</tr>\n<tr id=\"S4.T2.6.12.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.6.12.6.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T2.6.12.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">video-SALMONN 7B (ours)</span></td>\n<td id=\"S4.T2.6.12.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.12.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">4.1%</span></td>\n<td id=\"S4.T2.6.12.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.12.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">39.1</span></td>\n<td id=\"S4.T2.6.12.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.12.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">42.5%</span></td>\n<td id=\"S4.T2.6.12.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.12.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">78.1</span></td>\n<td id=\"S4.T2.6.12.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.12.6.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">34.6%</span></td>\n<td id=\"S4.T2.6.12.6.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.6.12.6.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">45.3%</span></td>\n</tr>\n<tr id=\"S4.T2.6.13.7\" class=\"ltx_tr\">\n<td id=\"S4.T2.6.13.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S4.T2.6.13.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">video-SALMONN 13B (ours)</span></td>\n<td id=\"S4.T2.6.13.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S4.T2.6.13.7.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.6</span><span id=\"S4.T2.6.13.7.2.2\" class=\"ltx_text\" style=\"font-size:80%;\">%</span>\n</td>\n<td id=\"S4.T2.6.13.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.6.13.7.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">49.7</span></td>\n<td id=\"S4.T2.6.13.7.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S4.T2.6.13.7.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">49.6</span><span id=\"S4.T2.6.13.7.4.2\" class=\"ltx_text\" style=\"font-size:80%;\">%</span>\n</td>\n<td id=\"S4.T2.6.13.7.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.6.13.7.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">89.6</span></td>\n<td id=\"S4.T2.6.13.7.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S4.T2.6.13.7.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">37.8</span><span id=\"S4.T2.6.13.7.6.2\" class=\"ltx_text\" style=\"font-size:80%;\">%</span>\n</td>\n<td id=\"S4.T2.6.13.7.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.6.13.7.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">44.8%</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "The results of video-SALMONN on the SAVE benchmark tasks are summarised in Table 2 and Table 3 for single-modal and audio-visual tasks respectively. While other models can only perform a subset of SAVE tasks, video-SALMONN is the first single model that achieves competitive performance on all tasks with remarkably better performance on audio-visual tasks. In particular, video-SALMONN effectively achieves zero-shot audio-visual co-reasoning as an emergent ability, which is reflected by the performance on the two AVQA datasets, the AVSSD and AVM tasks.",
            "On audio-based tasks in Table 2, video-SALMONN obtains both the lowest WER and the highest SPIDEr scores compared to Whisper large-v2 and Video-LLaMA respectively. We do not report WER for Video-LLaMA as that is over 100% due to a very high insertion rate.\nOn visual tasks, video-SALMONN demonstrates the best results on IC, OCR and Video QA, and on-par results on VQA with InstructBLIP fine-tuned on the same training set. In particular, the multi-resolution causal modelling in video-SALMONN yields over 25% improvements compared to InstructBLIP even though the latter is fine-tuned on the same set of video data. This directly reflects the benefit of the MRC Q-Former."
        ]
    },
    "S4.T3": {
        "caption": "Table 3:  The SAVE benchmark audio-visual task results. If specified, InstructBLIP is fine-tuned on the training data of video-SALMONN (\u201cInstructBLIP \u2020 \u2020 {\\dagger} \u201d). The other modality is masked in both training and testing when using visual-only inputs. Tasks unable to be performed are marked with \u201c-\u201d. We split AVQA into Ego4D-QA (E) and Presentation-QA (P).",
        "table": "<figure id=\"S4.T3\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>The SAVE benchmark audio-visual task results. If specified, InstructBLIP is fine-tuned on the training data of video-SALMONN (\u201cInstructBLIP<math id=\"S4.T3.2.m1.1\" class=\"ltx_Math\" alttext=\"{\\dagger}\" display=\"inline\"><semantics id=\"S4.T3.2.m1.1b\"><mo id=\"S4.T3.2.m1.1.1\" xref=\"S4.T3.2.m1.1.1.cmml\">\u2020</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.2.m1.1c\"><ci id=\"S4.T3.2.m1.1.1.cmml\" xref=\"S4.T3.2.m1.1.1\">\u2020</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.2.m1.1d\">{\\dagger}</annotation></semantics></math>\u201d). The other modality is masked in both training and testing when using visual-only inputs. Tasks unable to be performed are marked with \u201c-\u201d. We split AVQA into Ego4D-QA (E) and Presentation-QA (P).</figcaption>\n<table id=\"S4.T3.9\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.8.6.7\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S4.T3.8.6.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Systems</span></td>\n<td id=\"S4.T3.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.4.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">AVSR</span><span id=\"S4.T3.4.2.2.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T3.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T3.3.1.1.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T3.3.1.1.m1.1.1\" xref=\"S4.T3.3.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.3.1.1.m1.1b\"><ci id=\"S4.T3.3.1.1.m1.1.1.cmml\" xref=\"S4.T3.3.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.3.1.1.m1.1c\">\\downarrow</annotation></semantics></math><span id=\"S4.T3.4.2.2.3\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T3.4.2.2.m2.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T3.4.2.2.m2.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T3.4.2.2.m2.1.1\" xref=\"S4.T3.4.2.2.m2.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.4.2.2.m2.1b\"><ci id=\"S4.T3.4.2.2.m2.1.1.cmml\" xref=\"S4.T3.4.2.2.m2.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.4.2.2.m2.1c\">\\uparrow</annotation></semantics></math>\n</td>\n<td id=\"S4.T3.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.5.3.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">AVQA (E)</span><span id=\"S4.T3.5.3.3.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T3.5.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T3.5.3.3.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T3.5.3.3.m1.1.1\" xref=\"S4.T3.5.3.3.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.5.3.3.m1.1b\"><ci id=\"S4.T3.5.3.3.m1.1.1.cmml\" xref=\"S4.T3.5.3.3.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.5.3.3.m1.1c\">\\uparrow</annotation></semantics></math>\n</td>\n<td id=\"S4.T3.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.6.4.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">AVQA (P)</span><span id=\"S4.T3.6.4.4.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T3.6.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T3.6.4.4.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T3.6.4.4.m1.1.1\" xref=\"S4.T3.6.4.4.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.6.4.4.m1.1b\"><ci id=\"S4.T3.6.4.4.m1.1.1.cmml\" xref=\"S4.T3.6.4.4.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.6.4.4.m1.1c\">\\uparrow</annotation></semantics></math>\n</td>\n<td id=\"S4.T3.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.7.5.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">AVSSD</span><span id=\"S4.T3.7.5.5.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T3.7.5.5.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T3.7.5.5.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T3.7.5.5.m1.1.1\" xref=\"S4.T3.7.5.5.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.7.5.5.m1.1b\"><ci id=\"S4.T3.7.5.5.m1.1.1.cmml\" xref=\"S4.T3.7.5.5.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.7.5.5.m1.1c\">\\uparrow</annotation></semantics></math>\n</td>\n<td id=\"S4.T3.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.8.6.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">AVM</span><span id=\"S4.T3.8.6.6.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T3.8.6.6.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T3.8.6.6.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T3.8.6.6.m1.1.1\" xref=\"S4.T3.8.6.6.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.8.6.6.m1.1b\"><ci id=\"S4.T3.8.6.6.m1.1.1.cmml\" xref=\"S4.T3.8.6.6.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.8.6.6.m1.1c\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S4.T3.9.8.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.9.8.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T3.9.8.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Whisper large-v2</span></td>\n<td id=\"S4.T3.9.8.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.9.8.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">8.3%</span></td>\n<td id=\"S4.T3.9.8.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.9.8.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T3.9.8.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.9.8.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T3.9.8.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.9.8.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T3.9.8.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.9.8.1.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr id=\"S4.T3.9.9.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.9.9.2.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T3.9.9.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">InstructBLIP 13B </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.9.9.2.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Dai et\u00a0al.<span id=\"S4.T3.9.9.2.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2023</a><span id=\"S4.T3.9.9.2.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.9.9.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.9.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T3.9.9.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.9.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T3.9.9.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.9.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T3.9.9.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.9.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">1.1%</span></td>\n<td id=\"S4.T3.9.9.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.9.2.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr id=\"S4.T3.9.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.9.7.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T3.9.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">InstructBLIP</span><math id=\"S4.T3.9.7.1.m1.1\" class=\"ltx_Math\" alttext=\"{\\dagger}\" display=\"inline\"><semantics id=\"S4.T3.9.7.1.m1.1a\"><mo mathsize=\"80%\" id=\"S4.T3.9.7.1.m1.1.1\" xref=\"S4.T3.9.7.1.m1.1.1.cmml\">\u2020</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.9.7.1.m1.1b\"><ci id=\"S4.T3.9.7.1.m1.1.1.cmml\" xref=\"S4.T3.9.7.1.m1.1.1\">\u2020</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.9.7.1.m1.1c\">{\\dagger}</annotation></semantics></math><span id=\"S4.T3.9.7.1.2\" class=\"ltx_text\" style=\"font-size:80%;\"> 13B</span>\n</td>\n<td id=\"S4.T3.9.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.7.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T3.9.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T3.9.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T3.9.7.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">20.3%</span></td>\n<td id=\"S4.T3.9.7.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.7.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr id=\"S4.T3.9.10.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.9.10.3.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T3.9.10.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Video-LLaMA 7B </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.9.10.3.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Zhang et\u00a0al.<span id=\"S4.T3.9.10.3.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">2023b</a><span id=\"S4.T3.9.10.3.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.9.10.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.10.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T3.9.10.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.10.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">18.2%</span></td>\n<td id=\"S4.T3.9.10.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.10.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">21.3%</span></td>\n<td id=\"S4.T3.9.10.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.10.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">41.9%</span></td>\n<td id=\"S4.T3.9.10.3.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.10.3.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">52.3%</span></td>\n</tr>\n<tr id=\"S4.T3.9.11.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.9.11.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T3.9.11.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">video-SALMONN 13B (ours, visual-only)</span></td>\n<td id=\"S4.T3.9.11.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.9.11.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td id=\"S4.T3.9.11.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.9.11.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">35.0%</span></td>\n<td id=\"S4.T3.9.11.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.9.11.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">46.5%</span></td>\n<td id=\"S4.T3.9.11.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.9.11.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">23.5%</span></td>\n<td id=\"S4.T3.9.11.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.9.11.4.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr id=\"S4.T3.9.12.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.9.12.5.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T3.9.12.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">video-SALMONN 7B (ours)</span></td>\n<td id=\"S4.T3.9.12.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.12.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">8.7%</span></td>\n<td id=\"S4.T3.9.12.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.12.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">36.2%</span></td>\n<td id=\"S4.T3.9.12.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.12.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">41.3%</span></td>\n<td id=\"S4.T3.9.12.5.5\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.T3.9.12.5.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">50.5</span><span id=\"S4.T3.9.12.5.5.2\" class=\"ltx_text\" style=\"font-size:80%;\">%</span>\n</td>\n<td id=\"S4.T3.9.12.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.9.12.5.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">74.3%</span></td>\n</tr>\n<tr id=\"S4.T3.9.13.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.9.13.6.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S4.T3.9.13.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">video-SALMONN 13B (ours)</span></td>\n<td id=\"S4.T3.9.13.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S4.T3.9.13.6.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">7.7</span><span id=\"S4.T3.9.13.6.2.2\" class=\"ltx_text\" style=\"font-size:80%;\">%</span>\n</td>\n<td id=\"S4.T3.9.13.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S4.T3.9.13.6.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">49.8</span><span id=\"S4.T3.9.13.6.3.2\" class=\"ltx_text\" style=\"font-size:80%;\">%</span>\n</td>\n<td id=\"S4.T3.9.13.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S4.T3.9.13.6.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">70.5</span><span id=\"S4.T3.9.13.6.4.2\" class=\"ltx_text\" style=\"font-size:80%;\">%</span>\n</td>\n<td id=\"S4.T3.9.13.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.9.13.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">47.6%</span></td>\n<td id=\"S4.T3.9.13.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S4.T3.9.13.6.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">79.7</span><span id=\"S4.T3.9.13.6.6.2\" class=\"ltx_text\" style=\"font-size:80%;\">%</span>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "The results of video-SALMONN on the SAVE benchmark tasks are summarised in Table 2 and Table 3 for single-modal and audio-visual tasks respectively. While other models can only perform a subset of SAVE tasks, video-SALMONN is the first single model that achieves competitive performance on all tasks with remarkably better performance on audio-visual tasks. In particular, video-SALMONN effectively achieves zero-shot audio-visual co-reasoning as an emergent ability, which is reflected by the performance on the two AVQA datasets, the AVSSD and AVM tasks.",
            "On audio-visual tasks in Table 3, video-SALMONN achieved 7.2% relative WER reduction on the AVSR task compared to Whisper-large-v2. On the AVQA tasks, video-SALMONN achieved over 30% accuracy improvements compared to the Video-LLaMA baseline which does not understand human speech, showcasing its comprehensive understanding ability for speech-audio-visual inputs."
        ]
    },
    "S4.T4": {
        "caption": "Table 4:  Ablation studies on the core components of video-SALMONN based on single modal and audio-visual tasks. Each row represents removing one or more components with other parts remaining the same. Note the last row is equivalent to Video-LLaMA with the same training data, high frame rate video, speech encoder and LoRA, and the comparison to complete video-SALMONN directly reflected the benefit of the proposed structural and training design. AVQA takes the average among the two datasets.",
        "table": "<figure id=\"S4.T4\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Ablation studies on the core components of video-SALMONN based on single modal and audio-visual tasks. Each row represents removing one or more components with other parts remaining the same. Note the last row is equivalent to Video-LLaMA with the same training data, high frame rate video, speech encoder and LoRA, and the comparison to complete video-SALMONN directly reflected the benefit of the proposed structural and training design. AVQA takes the average among the two datasets.</figcaption>\n<table id=\"S4.T4.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.6.6\" class=\"ltx_tr\">\n<th id=\"S4.T4.6.6.7\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.6.6.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Systems</span></th>\n<th id=\"S4.T4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S4.T4.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">ASR</span><span id=\"S4.T4.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T4.1.1.1.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T4.1.1.1.m1.1.1\" xref=\"S4.T4.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.1.1.1.m1.1b\"><ci id=\"S4.T4.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>\n</th>\n<th id=\"S4.T4.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S4.T4.2.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">OCR</span><span id=\"S4.T4.2.2.2.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T4.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T4.2.2.2.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T4.2.2.2.m1.1.1\" xref=\"S4.T4.2.2.2.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.2.2.2.m1.1b\"><ci id=\"S4.T4.2.2.2.m1.1.1.cmml\" xref=\"S4.T4.2.2.2.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.2.2.2.m1.1c\">\\uparrow</annotation></semantics></math>\n</th>\n<th id=\"S4.T4.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S4.T4.3.3.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Video QA</span><span id=\"S4.T4.3.3.3.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T4.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T4.3.3.3.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T4.3.3.3.m1.1.1\" xref=\"S4.T4.3.3.3.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.3.3.3.m1.1b\"><ci id=\"S4.T4.3.3.3.m1.1.1.cmml\" xref=\"S4.T4.3.3.3.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.3.3.3.m1.1c\">\\uparrow</annotation></semantics></math>\n</th>\n<th id=\"S4.T4.4.4.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S4.T4.4.4.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">AVSR</span><span id=\"S4.T4.4.4.4.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T4.4.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T4.4.4.4.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T4.4.4.4.m1.1.1\" xref=\"S4.T4.4.4.4.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.4.4.4.m1.1b\"><ci id=\"S4.T4.4.4.4.m1.1.1.cmml\" xref=\"S4.T4.4.4.4.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.4.4.4.m1.1c\">\\downarrow</annotation></semantics></math>\n</th>\n<th id=\"S4.T4.5.5.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S4.T4.5.5.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">AVQA</span><span id=\"S4.T4.5.5.5.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T4.5.5.5.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T4.5.5.5.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T4.5.5.5.m1.1.1\" xref=\"S4.T4.5.5.5.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.5.5.5.m1.1b\"><ci id=\"S4.T4.5.5.5.m1.1.1.cmml\" xref=\"S4.T4.5.5.5.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.5.5.5.m1.1c\">\\uparrow</annotation></semantics></math>\n</th>\n<th id=\"S4.T4.6.6.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S4.T4.6.6.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">AVM</span><span id=\"S4.T4.6.6.6.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S4.T4.6.6.6.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T4.6.6.6.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S4.T4.6.6.6.m1.1.1\" xref=\"S4.T4.6.6.6.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.6.6.6.m1.1b\"><ci id=\"S4.T4.6.6.6.m1.1.1.cmml\" xref=\"S4.T4.6.6.6.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.6.6.6.m1.1c\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.6.7.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.6.7.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T4.6.7.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">video-SALMONN</span></td>\n<td id=\"S4.T4.6.7.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.6.7.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">2.6%</span></td>\n<td id=\"S4.T4.6.7.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.6.7.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">37.8%</span></td>\n<td id=\"S4.T4.6.7.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.6.7.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">49.6%</span></td>\n<td id=\"S4.T4.6.7.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.6.7.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">7.7%</span></td>\n<td id=\"S4.T4.6.7.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.6.7.1.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">60.2%</span></td>\n<td id=\"S4.T4.6.7.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.6.7.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">79.7%</span></td>\n</tr>\n<tr id=\"S4.T4.6.8.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.6.8.2.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T4.6.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">video-SALMONN without 5s-resolution</span></td>\n<td id=\"S4.T4.6.8.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.8.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">2.5%</span></td>\n<td id=\"S4.T4.6.8.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.8.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">35.4%</span></td>\n<td id=\"S4.T4.6.8.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.8.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">47.2%</span></td>\n<td id=\"S4.T4.6.8.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.8.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">7.7%</span></td>\n<td id=\"S4.T4.6.8.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.8.2.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">57.2%</span></td>\n<td id=\"S4.T4.6.8.2.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.8.2.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">77.5%</span></td>\n</tr>\n<tr id=\"S4.T4.6.9.3\" class=\"ltx_tr\">\n<td id=\"S4.T4.6.9.3.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T4.6.9.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">video-SALMONN without 0.5s-resolution</span></td>\n<td id=\"S4.T4.6.9.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.9.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">2.9%</span></td>\n<td id=\"S4.T4.6.9.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.9.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">37.1%</span></td>\n<td id=\"S4.T4.6.9.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.9.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">49.9%</span></td>\n<td id=\"S4.T4.6.9.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.9.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">8.3%</span></td>\n<td id=\"S4.T4.6.9.3.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.9.3.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">58.9%</span></td>\n<td id=\"S4.T4.6.9.3.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.9.3.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">80.6%</span></td>\n</tr>\n<tr id=\"S4.T4.6.10.4\" class=\"ltx_tr\">\n<td id=\"S4.T4.6.10.4.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T4.6.10.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">video-SALMONN without mixed training scheme</span></td>\n<td id=\"S4.T4.6.10.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.10.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">2.6%</span></td>\n<td id=\"S4.T4.6.10.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.10.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">34.0%</span></td>\n<td id=\"S4.T4.6.10.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.10.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">46.9%</span></td>\n<td id=\"S4.T4.6.10.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.10.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">8.3%</span></td>\n<td id=\"S4.T4.6.10.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.10.4.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">54.0%</span></td>\n<td id=\"S4.T4.6.10.4.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.10.4.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">75.3%</span></td>\n</tr>\n<tr id=\"S4.T4.6.11.5\" class=\"ltx_tr\">\n<td id=\"S4.T4.6.11.5.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T4.6.11.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">video-SALMONN without diversity loss</span></td>\n<td id=\"S4.T4.6.11.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.11.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">2.5%</span></td>\n<td id=\"S4.T4.6.11.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.11.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">36.8%</span></td>\n<td id=\"S4.T4.6.11.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.11.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">49.3%</span></td>\n<td id=\"S4.T4.6.11.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.11.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">7.7%</span></td>\n<td id=\"S4.T4.6.11.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.11.5.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">53.5%</span></td>\n<td id=\"S4.T4.6.11.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.11.5.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">78.6%</span></td>\n</tr>\n<tr id=\"S4.T4.6.12.6\" class=\"ltx_tr\">\n<td id=\"S4.T4.6.12.6.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T4.6.12.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">video-SALMONN without MRC Q-Former</span></td>\n<td id=\"S4.T4.6.12.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.12.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">3.3%</span></td>\n<td id=\"S4.T4.6.12.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.12.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">34.6%</span></td>\n<td id=\"S4.T4.6.12.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.12.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">42.7%</span></td>\n<td id=\"S4.T4.6.12.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.12.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">8.5%</span></td>\n<td id=\"S4.T4.6.12.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.12.6.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">45.3%</span></td>\n<td id=\"S4.T4.6.12.6.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.6.12.6.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">74.5%</span></td>\n</tr>\n<tr id=\"S4.T4.6.13.7\" class=\"ltx_tr\">\n<td id=\"S4.T4.6.13.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S4.T4.6.13.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">video-SALMONN without MRC Q-Former, sync. and div.</span></td>\n<td id=\"S4.T4.6.13.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.6.13.7.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">3.1%</span></td>\n<td id=\"S4.T4.6.13.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.6.13.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">34.7%</span></td>\n<td id=\"S4.T4.6.13.7.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.6.13.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">36.0%</span></td>\n<td id=\"S4.T4.6.13.7.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.6.13.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">8.9%</span></td>\n<td id=\"S4.T4.6.13.7.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.6.13.7.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">44.6%</span></td>\n<td id=\"S4.T4.6.13.7.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.6.13.7.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">72.0%</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [
            "Ablation studies on the core components of video-SALMONN based on single modal and audio-visual tasks. Each row represents removing one or more components with other parts remaining the same. Note the last row is equivalent to Video-LLaMA with the same training data, high frame rate video, speech encoder and LoRA, and the comparison to complete video-SALMONN directly reflected the benefit of the proposed structural and training design. AVQA takes the average among the two datasets."
        ],
        "references": [
            "This section particularly focuses on the key structural novelty, including MRC Q-Former, the fine-grained synchronisation, as well as training techniques in video-SALMONN on selected SAVE benchmark tasks, as summarised in Table 4.",
            "Next, the effect of the mixed training scheme and diversity loss can be seen by comparing row 4 and row 5 to row 1 in Table 4. Both techniques provide improvements, particularly to audio-visual understanding tasks including AVQA and AVM, as the model pays balanced attention to both audio and visual streams as well as to different input frames."
        ]
    },
    "S5.T5": {
        "caption": "Table 5:  Analysis of the effect of each resolution level reflected by ASR, IC and Video-QA tasks, with average cosine similarity between query vectors and word embeddings shown in brackets.",
        "table": "<figure id=\"S5.T5\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Analysis of the effect of each resolution level reflected by ASR, IC and Video-QA tasks, with average cosine similarity between query vectors and word embeddings shown in brackets.</figcaption>\n<table id=\"S5.T5.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T5.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.3.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S5.T5.3.3.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Resolution level</span></th>\n<th id=\"S5.T5.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S5.T5.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">ASR</span><span id=\"S5.T5.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T5.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S5.T5.1.1.1.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S5.T5.1.1.1.m1.1.1\" xref=\"S5.T5.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.1.1.1.m1.1b\"><ci id=\"S5.T5.1.1.1.m1.1.1.cmml\" xref=\"S5.T5.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>\n</th>\n<th id=\"S5.T5.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S5.T5.2.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">IC</span><span id=\"S5.T5.2.2.2.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T5.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S5.T5.2.2.2.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S5.T5.2.2.2.m1.1.1\" xref=\"S5.T5.2.2.2.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.2.2.2.m1.1b\"><ci id=\"S5.T5.2.2.2.m1.1.1.cmml\" xref=\"S5.T5.2.2.2.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.2.2.2.m1.1c\">\\uparrow</annotation></semantics></math>\n</th>\n<th id=\"S5.T5.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S5.T5.3.3.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Video QA</span><span id=\"S5.T5.3.3.3.2\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><math id=\"S5.T5.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S5.T5.3.3.3.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S5.T5.3.3.3.m1.1.1\" xref=\"S5.T5.3.3.3.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.3.3.3.m1.1b\"><ci id=\"S5.T5.3.3.3.m1.1.1.cmml\" xref=\"S5.T5.3.3.3.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.3.3.3.m1.1c\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.3.4.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S5.T5.3.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Both</span></th>\n<td id=\"S5.T5.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.3.4.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">2.6%</span></td>\n<td id=\"S5.T5.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.3.4.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">89.6</span></td>\n<td id=\"S5.T5.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.3.4.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">49.6%</span></td>\n</tr>\n<tr id=\"S5.T5.3.5.2\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.5.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S5.T5.3.5.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.5s</span></th>\n<td id=\"S5.T5.3.5.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.3.5.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">2.6%</span></td>\n<td id=\"S5.T5.3.5.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.3.5.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">35.8</span></td>\n<td id=\"S5.T5.3.5.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.3.5.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">14.4%</span></td>\n</tr>\n<tr id=\"S5.T5.3.6.3\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.6.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span id=\"S5.T5.3.6.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">5.0s</span></th>\n<td id=\"S5.T5.3.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T5.3.6.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">100+%</span></td>\n<td id=\"S5.T5.3.6.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T5.3.6.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">23.0</span></td>\n<td id=\"S5.T5.3.6.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T5.3.6.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">41.9%</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "To illustrate the functionality of each resolution level, we apply zero masks to the output query of one resolution level and observe the performance of another, as shown in Table 5. The system learns to split the functionality into two resolutions: the high resolution takes care of speech content-related information and the low resolution takes care of high-level information such as Video QA. This agrees with our findings from the ablation studies. Moreover, the complementarity of the two resolution levels is further processed by the LLM to achieve the best outcome."
        ]
    },
    "A1.T6": {
        "caption": "Table 6:  Dataset and benchmark details part 1",
        "table": "<figure id=\"A1.T6\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Dataset and benchmark details part 1</figcaption>\n<table id=\"A1.T6.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T6.3.1.1\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"A1.T6.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Data</span></td>\n<td id=\"A1.T6.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T6.3.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">In Train</span></td>\n<td id=\"A1.T6.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A1.T6.3.1.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">In SAVE</span></td>\n<td id=\"A1.T6.3.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span id=\"A1.T6.3.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.1.1.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.1.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Description</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.2.2\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T6.3.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">LibriSpeech</span></td>\n<td id=\"A1.T6.3.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.2.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T6.3.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.2.2.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.2.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">LibriSpeech is an English audiobook data. The train-clean-100 and train-clean-360 splits were used for training, and test-clean was used in SAVE. Prompt example: \u201cTranscribe the speech into text.\u201d</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.3.3\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T6.3.3.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">AudioCaps</span></td>\n<td id=\"A1.T6.3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.3.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.3.3.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T6.3.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.3.3.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.3.3.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">AudioCaps is a widely used audio caption dataset containing 46k 10-second audio samples with manually annotated captions. Example prompt: \u201cPlease describe the audio.\u201d</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.4.4\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T6.3.4.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">LLAVA-150k</span></td>\n<td id=\"A1.T6.3.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.4.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.4.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">No</span></td>\n<td id=\"A1.T6.3.4.4.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T6.3.4.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.4.4.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.4.4.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">LLAVA-150k contain QA pairs generated using ChatGPT. Example prompt: \u201cWhat does the man hold in the image?\u201d</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.5.5\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T6.3.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">OCRVQA</span></td>\n<td id=\"A1.T6.3.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.5.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.5.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">No</span></td>\n<td id=\"A1.T6.3.5.5.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T6.3.5.5.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.5.5.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.5.5.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">OCRVQA is an OCR-based QA dataset containing questions mostly about printed words in an image. Example prompt: \u201cWho wrote this book?\u201d</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.6.6\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.6.6.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T6.3.6.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">TextVQA</span></td>\n<td id=\"A1.T6.3.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.6.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">No</span></td>\n<td id=\"A1.T6.3.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.6.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.6.6.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T6.3.6.6.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.6.6.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.6.6.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">OCR-based QA dataset containing questions about various words in realistic scenes (</span><span id=\"A1.T6.3.6.6.4.1.1.2\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">c.f.</span><span id=\"A1.T6.3.6.6.4.1.1.3\" class=\"ltx_text\" style=\"font-size:80%;\"> printed words). Example prompt: \u201cWhat is the brand of this camera?\u201d</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.7.7\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.7.7.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T6.3.7.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Flickr30k</span></td>\n<td id=\"A1.T6.3.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.7.7.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">No</span></td>\n<td id=\"A1.T6.3.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.7.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.7.7.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T6.3.7.7.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.7.7.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.7.7.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Image caption dataset where each image is annotated with manual single-sentence descriptions. Example prompt: \u201cDescribe this image in one short sentence.\u201d</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.8.8\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.8.8.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T6.3.8.8.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">GQA</span></td>\n<td id=\"A1.T6.3.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.8.8.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">No</span></td>\n<td id=\"A1.T6.3.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.8.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.8.8.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T6.3.8.8.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.8.8.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.8.8.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">GQA consists of questions about various day-to-day real-world images. This involves reasoning skills about the objects in the image. Example prompt: \u201cWhat kind of device is on top of the desk?\u201d</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.9.9\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.9.9.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T6.3.9.9.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">TextCaps</span></td>\n<td id=\"A1.T6.3.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.9.9.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.9.9.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">No</span></td>\n<td id=\"A1.T6.3.9.9.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T6.3.9.9.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.9.9.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.9.9.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Image caption data particularly focusing on capturing text in the image. Only 80k samples were randomly selected for training. Example prompt: \u201cDescribe the image.\u201d</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.10.10\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.10.10.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T6.3.10.10.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">MSVD-QA</span></td>\n<td id=\"A1.T6.3.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.10.10.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.10.10.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">No</span></td>\n<td id=\"A1.T6.3.10.10.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T6.3.10.10.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.10.10.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.10.10.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">MSVD-QA is a dataset with questions about real-world video clips. Example prompt: \u201cIn the video, what is the man with long hair playing?\u201d</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.11.11\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.11.11.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T6.3.11.11.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">NExT-QA</span></td>\n<td id=\"A1.T6.3.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.11.11.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.11.11.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.11.11.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T6.3.11.11.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.11.11.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.11.11.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">NExT-QA is a video QA dataset, particularly focusing on causal and temporal correlations. Example prompt: \u201cWhat does the girl in white do after bending down in the middle? Options/Choose one from: (Add choices here during inference)\u201d.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.12.12\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.12.12.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T6.3.12.12.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">VideoChat</span></td>\n<td id=\"A1.T6.3.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.12.12.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.12.12.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">No</span></td>\n<td id=\"A1.T6.3.12.12.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T6.3.12.12.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.12.12.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.12.12.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A GPT4-generated video QA dataset where the question mainly asks for detailed descriptions of the video. Example prompt: \u201cProvide a detailed description of the given video.\u201d</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.13.13\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.13.13.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T6.3.13.13.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">AVSD</span></td>\n<td id=\"A1.T6.3.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.13.13.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.13.13.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.13.13.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T6.3.13.13.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.13.13.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.13.13.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Audio-visual scene-aware dialogue data where questions are raised in turns about the video and the audio in the video. Example prompt: \u201cAnd then what happened?\u201d and \u201cIs the man saying anything?\u201d</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.14.14\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.14.14.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T6.3.14.14.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Ego4D</span></td>\n<td id=\"A1.T6.3.14.14.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.14.14.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.14.14.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T6.3.14.14.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.14.14.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T6.3.14.14.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.14.14.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.14.14.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">An audio-visual dataset containing egocentric videos. Video descriptions were used as supervision signals which came from single-sentence short clip descriptions that were concatenated and refined using ChatGPT. Example prompt: \u201cDescribe the video in detail.\u201d</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.15.15\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.15.15.1\" class=\"ltx_td\"></td>\n<td id=\"A1.T6.3.15.15.2\" class=\"ltx_td\"></td>\n<td id=\"A1.T6.3.15.15.3\" class=\"ltx_td\"></td>\n<td id=\"A1.T6.3.15.15.4\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T6.3.15.15.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.15.15.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.15.15.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">1000 video clips from the test set were used to make multiple choice questions by prompting ChatGPT with audio-visual caption and ASR transcription.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T6.3.16.16\" class=\"ltx_tr\">\n<td id=\"A1.T6.3.16.16.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"A1.T6.3.16.16.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">How2</span></td>\n<td id=\"A1.T6.3.16.16.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"A1.T6.3.16.16.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.16.16.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"A1.T6.3.16.16.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T6.3.16.16.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"A1.T6.3.16.16.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T6.3.16.16.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T6.3.16.16.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">An audio-visual speech recognition dataset containing videos explaining how to perform various tasks. Example prompt: \u201cTranscribe the speech into text, paying attention to both audio and video.\u201d</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "A range of datasets spanning audio and visual tasks are used in our experiments. Table 6 and 7 summarise these datasets in detail, with individual descriptions and relevant prompt designs."
        ]
    },
    "A1.T7": {
        "caption": "Table 7:  Dataset and benchmark details part 2",
        "table": "<figure id=\"A1.T7\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:80%;\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Dataset and benchmark details part 2</figcaption>\n<table id=\"A1.T7.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T7.3.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T7.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span id=\"A1.T7.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Data</span></th>\n<th id=\"A1.T7.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A1.T7.3.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">In Train</span></th>\n<th id=\"A1.T7.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A1.T7.3.1.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">In SAVE</span></th>\n<th id=\"A1.T7.3.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"A1.T7.3.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T7.3.1.1.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T7.3.1.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Description</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T7.3.2.1\" class=\"ltx_tr\">\n<td id=\"A1.T7.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A1.T7.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">VGGSS</span></td>\n<td id=\"A1.T7.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T7.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">No</span></td>\n<td id=\"A1.T7.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T7.3.2.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T7.3.2.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T7.3.2.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T7.3.2.1.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T7.3.2.1.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Sound source localisation data containing questions about the sound source in a 5-to-10-second video clip. Example prompt: \u201cWhat is the source of the sound?\u201d</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"A1.T7.3.3.2\" class=\"ltx_tr\">\n<td id=\"A1.T7.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"A1.T7.3.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Presentation-QA</span></td>\n<td id=\"A1.T7.3.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"A1.T7.3.3.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">No</span></td>\n<td id=\"A1.T7.3.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"A1.T7.3.3.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Yes</span></td>\n<td id=\"A1.T7.3.3.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"A1.T7.3.3.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T7.3.3.2.4.1.1\" class=\"ltx_p\" style=\"width:274.6pt;\"><span id=\"A1.T7.3.3.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A presentation video dataset labelled with slides text and speech transcriptions. 1000 video clips from the test set were used to make multiple-choice questions by prompting ChatGPT with slide content and ASR transcription.</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": []
    },
    "A4.T8": {
        "caption": "Table 8:  Prompt designs for ChatGPT-based evaluation. Note that  QUESTION  refers to the question,  HYPOTHESIS  is the model-generated answer and  REFERENCE  is the reference answer.",
        "table": "<figure id=\"A4.T8\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Prompt designs for ChatGPT-based evaluation. Note that <span id=\"A4.T8.4.1\" class=\"ltx_text ltx_font_typewriter\">QUESTION</span> refers to the question, <span id=\"A4.T8.5.2\" class=\"ltx_text ltx_font_typewriter\">HYPOTHESIS</span> is the model-generated answer and <span id=\"A4.T8.6.3\" class=\"ltx_text ltx_font_typewriter\">REFERENCE</span> is the reference answer.</figcaption>\n<table id=\"A4.T8.7\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A4.T8.7.1.1\" class=\"ltx_tr\">\n<th id=\"A4.T8.7.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Task</th>\n<td id=\"A4.T8.7.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span id=\"A4.T8.7.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A4.T8.7.1.1.2.1.1\" class=\"ltx_p\" style=\"width:339.7pt;\">Description</span>\n</span>\n</td>\n</tr>\n<tr id=\"A4.T8.7.2.2\" class=\"ltx_tr\">\n<th id=\"A4.T8.7.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">VGGSS</th>\n<td id=\"A4.T8.7.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"A4.T8.7.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A4.T8.7.2.2.2.1.1\" class=\"ltx_p\" style=\"width:339.7pt;\">Is the sound source mentioned in answer \u201c<span id=\"A4.T8.7.2.2.2.1.1.1\" class=\"ltx_text ltx_font_typewriter\">REFERENCE</span>\u201d the same as the sound source mentioned in answer \u201c<span id=\"A4.T8.7.2.2.2.1.1.2\" class=\"ltx_text ltx_font_typewriter\">HYPOTHESIS</span>\u201d? Answer \u201cYes\u201d if they are the same, and \u201dNo\u201d if they are different or one does not mention the sound source.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [
            "Prompt designs for ChatGPT-based evaluation. Note that ",
            " refers to the question, ",
            " is the model-generated answer and ",
            " is the reference answer."
        ],
        "references": [
            "As open-ended questions in VGGSS dataset contain full-sentence answers rather than one or two words, it is difficult to evaluate via string matching. Therefore, ChatGPT (GPT-3.5-turbo) was used to assist with the evaluation. Prompt designs for each task are described in Table 8."
        ]
    },
    "A6.T9": {
        "caption": "Table 9:  %WER on LRS2 lip-reading test set with clean speech, or with speech corrupted by 0dB Gaussian noise.",
        "table": "<figure id=\"A6.T9\" class=\"ltx_table\">\n<table id=\"A6.T9.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A6.T9.1.1.1\" class=\"ltx_tr\">\n<th id=\"A6.T9.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">System</th>\n<th id=\"A6.T9.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">LRS2 %WER</th>\n<th id=\"A6.T9.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">LSR2 + 0dB Gaussian noise %WER</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A6.T9.1.2.1\" class=\"ltx_tr\">\n<th id=\"A6.T9.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Whisper large-v2</th>\n<td id=\"A6.T9.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">5.3</td>\n<td id=\"A6.T9.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">22.4</td>\n</tr>\n<tr id=\"A6.T9.1.3.2\" class=\"ltx_tr\">\n<th id=\"A6.T9.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">video-SALMONN audio alone</th>\n<td id=\"A6.T9.1.3.2.2\" class=\"ltx_td ltx_align_center\">5.1</td>\n<td id=\"A6.T9.1.3.2.3\" class=\"ltx_td ltx_align_center\">22.4</td>\n</tr>\n<tr id=\"A6.T9.1.4.3\" class=\"ltx_tr\">\n<th id=\"A6.T9.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">video-SALMONN audio + video</th>\n<td id=\"A6.T9.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T9.1.4.3.2.1\" class=\"ltx_text ltx_font_bold\">4.9</span></td>\n<td id=\"A6.T9.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T9.1.4.3.3.1\" class=\"ltx_text ltx_font_bold\">21.6</span></td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>%WER on LRS2 lip-reading test set with clean speech, or with speech corrupted by 0dB Gaussian noise.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We further include the performance of video-SALMONN on the Oxford-BBC lip reading sentences 2 (LRS2) dataset. Results are shown in Table 9. video-SALMONN achieved better results than the Whisper baseline by a relative 7.5% WER reduction."
        ]
    },
    "A7.T10": {
        "caption": "Table 10:  %Acc on MUSIC-AVQA using Video-LLaMA, AV-LLM and video-SALMONN.",
        "table": "<figure id=\"A7.T10\" class=\"ltx_table\">\n<table id=\"A7.T10.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A7.T10.1.1.1\" class=\"ltx_tr\">\n<th id=\"A7.T10.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">System</th>\n<td id=\"A7.T10.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">MUSIC-AVQA Acc (%)</td>\n</tr>\n<tr id=\"A7.T10.1.2.2\" class=\"ltx_tr\">\n<th id=\"A7.T10.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Video-LLaMA</th>\n<td id=\"A7.T10.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">36.6%</td>\n</tr>\n<tr id=\"A7.T10.1.3.3\" class=\"ltx_tr\">\n<th id=\"A7.T10.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AV-LLM</th>\n<td id=\"A7.T10.1.3.3.2\" class=\"ltx_td ltx_align_center\">45.2%</td>\n</tr>\n<tr id=\"A7.T10.1.4.4\" class=\"ltx_tr\">\n<th id=\"A7.T10.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">video-SALMONN</th>\n<td id=\"A7.T10.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A7.T10.1.4.4.2.1\" class=\"ltx_text ltx_font_bold\">52.6</span>%</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span>%Acc on MUSIC-AVQA using Video-LLaMA, AV-LLM and video-SALMONN.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": []
    },
    "A8.T11": {
        "caption": "Table 11:  Audio or visual-only tasks in SAVE for comparison between Llama-2 and Vicuna-v1.5 backbone LLM.",
        "table": "<figure id=\"A8.T11\" class=\"ltx_table\">\n<table id=\"A8.T11.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A8.T11.1.1.1\" class=\"ltx_tr\">\n<th id=\"A8.T11.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">System</th>\n<th id=\"A8.T11.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">ASR</th>\n<th id=\"A8.T11.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">AC</th>\n<th id=\"A8.T11.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Video QA</th>\n<th id=\"A8.T11.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">IC</th>\n<th id=\"A8.T11.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">OCR</th>\n<th id=\"A8.T11.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">VQA</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A8.T11.1.2.1\" class=\"ltx_tr\">\n<th id=\"A8.T11.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">video-SALMONN Vicuna-v1.5</th>\n<td id=\"A8.T11.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">2.6%</td>\n<td id=\"A8.T11.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">49.7%</td>\n<td id=\"A8.T11.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">49.6%</td>\n<td id=\"A8.T11.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">89.6%</td>\n<td id=\"A8.T11.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">37.8%</td>\n<td id=\"A8.T11.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">44.8%</td>\n</tr>\n<tr id=\"A8.T11.1.3.2\" class=\"ltx_tr\">\n<th id=\"A8.T11.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">video-SALMONN Llama-2</th>\n<td id=\"A8.T11.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">2.6%</td>\n<td id=\"A8.T11.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">50.6%</td>\n<td id=\"A8.T11.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">36.7%</td>\n<td id=\"A8.T11.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">91.6%</td>\n<td id=\"A8.T11.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">33.8%</td>\n<td id=\"A8.T11.1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">45.4%</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 11: </span>Audio or visual-only tasks in SAVE for comparison between Llama-2 and Vicuna-v1.5 backbone LLM.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We provide the additional results using Llama-2 in contrast to Vicuna-v1.5 on SAVE in Table 11 and 12."
        ]
    },
    "A8.T12": {
        "caption": "Table 12:  Audio-visual tasks in SAVE for comparison between Llama-2 and Vicuna-v1.5 backbone LLM.",
        "table": "<figure id=\"A8.T12\" class=\"ltx_table\">\n<table id=\"A8.T12.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A8.T12.1.1.1\" class=\"ltx_tr\">\n<th id=\"A8.T12.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">System</th>\n<th id=\"A8.T12.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">AVSR</th>\n<th id=\"A8.T12.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">AVQA (E)</th>\n<th id=\"A8.T12.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">AVQA (P)</th>\n<th id=\"A8.T12.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">AVSSD</th>\n<th id=\"A8.T12.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">AVM</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A8.T12.1.2.1\" class=\"ltx_tr\">\n<th id=\"A8.T12.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">video-SALMONN Vicuna-v1.5</th>\n<td id=\"A8.T12.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">7.7%</td>\n<td id=\"A8.T12.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">49.8%</td>\n<td id=\"A8.T12.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">70.5%</td>\n<td id=\"A8.T12.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">47.6%</td>\n<td id=\"A8.T12.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">79.7%</td>\n</tr>\n<tr id=\"A8.T12.1.3.2\" class=\"ltx_tr\">\n<th id=\"A8.T12.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">video-SALMONN Llama-2</th>\n<td id=\"A8.T12.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">7.8%</td>\n<td id=\"A8.T12.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">40.6%</td>\n<td id=\"A8.T12.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">53.5%</td>\n<td id=\"A8.T12.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">48.6%</td>\n<td id=\"A8.T12.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">79.6%</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 12: </span>Audio-visual tasks in SAVE for comparison between Llama-2 and Vicuna-v1.5 backbone LLM.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": []
    },
    "A9.T13": {
        "caption": "Table 13:  The SAVE benchmark single-modal task results using the spotlight of the static image.",
        "table": "<figure id=\"A9.T13\" class=\"ltx_table\">\n<table id=\"A9.T13.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A9.T13.1.1.1\" class=\"ltx_tr\">\n<th id=\"A9.T13.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">System</th>\n<th id=\"A9.T13.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">ASR</th>\n<th id=\"A9.T13.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">AC</th>\n<th id=\"A9.T13.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Video QA</th>\n<th id=\"A9.T13.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">IC</th>\n<th id=\"A9.T13.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">OCR</th>\n<th id=\"A9.T13.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">VQA</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A9.T13.1.2.1\" class=\"ltx_tr\">\n<td id=\"A9.T13.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">InstructBLIP</td>\n<td id=\"A9.T13.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td id=\"A9.T13.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td id=\"A9.T13.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">21.0%</td>\n<td id=\"A9.T13.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">84.5</td>\n<td id=\"A9.T13.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">36.5%</td>\n<td id=\"A9.T13.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">48.9%</td>\n</tr>\n<tr id=\"A9.T13.1.3.2\" class=\"ltx_tr\">\n<td id=\"A9.T13.1.3.2.1\" class=\"ltx_td ltx_align_left\">video-SALMONN</td>\n<td id=\"A9.T13.1.3.2.2\" class=\"ltx_td ltx_align_center\">2.6%</td>\n<td id=\"A9.T13.1.3.2.3\" class=\"ltx_td ltx_align_center\">49.7%</td>\n<td id=\"A9.T13.1.3.2.4\" class=\"ltx_td ltx_align_center\">\n<span id=\"A9.T13.1.3.2.4.1\" class=\"ltx_text ltx_font_bold\">49.6</span>%</td>\n<td id=\"A9.T13.1.3.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"A9.T13.1.3.2.5.1\" class=\"ltx_text ltx_font_bold\">89.6</span></td>\n<td id=\"A9.T13.1.3.2.6\" class=\"ltx_td ltx_align_center\">37.8%</td>\n<td id=\"A9.T13.1.3.2.7\" class=\"ltx_td ltx_align_center\">44.8%</td>\n</tr>\n<tr id=\"A9.T13.1.4.3\" class=\"ltx_tr\">\n<td id=\"A9.T13.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">video-SALMONN + image spotlight</td>\n<td id=\"A9.T13.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A9.T13.1.4.3.2.1\" class=\"ltx_text ltx_font_bold\">2.6</span>%</td>\n<td id=\"A9.T13.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A9.T13.1.4.3.3.1\" class=\"ltx_text ltx_font_bold\">50.6</span>%</td>\n<td id=\"A9.T13.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">49.1%</td>\n<td id=\"A9.T13.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">87.3</td>\n<td id=\"A9.T13.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">56.1%</td>\n<td id=\"A9.T13.1.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">46.2%</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 13: </span>The SAVE benchmark single-modal task results using the spotlight of the static image.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We noticed that the performance of video-SALMONN on image tasks (e.g. VQA and OCR) may be limited by the lack of spatial resolution such that it is insufficient to extract details. To capture the finer details of an image, we make an extension to the MRC Q-Former by applying an image spotlight approach. We split the original image into a sequence of sub-images, and send the encodings of these sub-images to the MRC Q-Former in sequence. This is analogous to a video clip that scans the image patch by patch using a spotlight from the top left to the bottom right. The results of using the spotlight method (applied from the beginning of the instruction tuning) yielded better performance on OCR as shown in Table 13."
        ]
    }
}