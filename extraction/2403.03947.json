{
    "S2.SS2.tab1": {
        "caption": "TABLE I:  Stastistics of the datasets available for estimating piano performance difficulty. Figure 4:  Era distribution of PSyllabus dataset Figure 5:  Composers distribution of PSyllabus dataset Figure 6:  Grades distribution of PSyllabus dataset about women and men composers Figure 7:  Shared difficulty pieces and their ranking correlation between PSyllabus and other rankings. Figure 8:  Distribution of difficulty of the Hidden Voices dataset. Figure 9:  The input representations are piano roll with two channels, frames and onsets, and CQT.  Figure 10:  The classifier architecture for score difficulty classification, utilizing audio features CQT and piano roll. Beginning with a residual convolutional network for feature extraction and followed by a recurrent with attention model. Figure 11:  The primary modification in the multimodal approach involves adding two branches: one for CQT and another for piano roll features, to facilitate the early fusion of both representations. TABLE II:  Results training with monomodal representations, CQT and PR, and multimodal ones. TABLE III:  Multi-tasks experiments training the models on the PSyllabus dataset and auxiliary tasks.  TABLE IV:  Comparative analysis for Basic and Multi-task with Era experiments across musical periods. TABLE V:  Analysis of model performance differentiated by the composer\u2019s gender. TABLE VI:  Zero-shot experiment on Hidden Voices benchmark. The benchmark is a collection of piano pieces by black women composers, out of the distribution from the PSyllabus dataset.",
        "table": "<figure id=\"S2.SS2.tab1\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE I: </span>Stastistics of the datasets available for estimating piano performance difficulty.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S2.SS2.tab1.1\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:433.6pt;height:208.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(29.3pt,-14.0pt) scale(1.15607053051663,1.15607053051663) ;\">\n<table id=\"S2.SS2.tab1.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.SS2.tab1.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S2.SS2.tab1.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S2.SS2.tab1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"S2.SS2.tab1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S2.SS2.tab1.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Pieces</span></th>\n<th id=\"S2.SS2.tab1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S2.SS2.tab1.1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Classes</span></th>\n<th id=\"S2.SS2.tab1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S2.SS2.tab1.1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">AIR</span></th>\n<th id=\"S2.SS2.tab1.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S2.SS2.tab1.1.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Noteheads</span></th>\n<th id=\"S2.SS2.tab1.1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S2.SS2.tab1.1.1.1.1.6.1\" class=\"ltx_text ltx_font_bold\">Composers</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.SS2.tab1.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S2.SS2.tab1.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S2.SS2.tab1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_italic\">Symbolic data</span></td>\n<td id=\"S2.SS2.tab1.1.1.2.1.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.2.1.3\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.2.1.4\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.2.1.5\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.2.1.6\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S2.SS2.tab1.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S2.SS2.tab1.1.1.3.2.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.3.2.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MK\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>\n</td>\n<td id=\"S2.SS2.tab1.1.1.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">147</td>\n<td id=\"S2.SS2.tab1.1.1.3.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">147</td>\n<td id=\"S2.SS2.tab1.1.1.3.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.78</td>\n<td id=\"S2.SS2.tab1.1.1.3.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.2k</td>\n<td id=\"S2.SS2.tab1.1.1.3.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1</td>\n</tr>\n<tr id=\"S2.SS2.tab1.1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S2.SS2.tab1.1.1.4.3.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.4.3.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CIPI\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">14</a>]</cite>\n</td>\n<td id=\"S2.SS2.tab1.1.1.4.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">652</td>\n<td id=\"S2.SS2.tab1.1.1.4.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">9</td>\n<td id=\"S2.SS2.tab1.1.1.4.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.33</td>\n<td id=\"S2.SS2.tab1.1.1.4.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.1M</td>\n<td id=\"S2.SS2.tab1.1.1.4.3.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29</td>\n</tr>\n<tr id=\"S2.SS2.tab1.1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S2.SS2.tab1.1.1.5.4.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S2.SS2.tab1.1.1.5.4.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S2.SS2.tab1.1.1.5.4.1.2\" class=\"ltx_text ltx_font_italic\">Image sheets</span>\n</td>\n<td id=\"S2.SS2.tab1.1.1.5.4.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.5.4.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.5.4.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.5.4.5\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.5.4.6\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S2.SS2.tab1.1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S2.SS2.tab1.1.1.6.5.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.6.5.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PS\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite>\n</td>\n<td id=\"S2.SS2.tab1.1.1.6.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2816</td>\n<td id=\"S2.SS2.tab1.1.1.6.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">9</td>\n<td id=\"S2.SS2.tab1.1.1.6.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.24</td>\n<td id=\"S2.SS2.tab1.1.1.6.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7.2M</td>\n<td id=\"S2.SS2.tab1.1.1.6.5.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">92</td>\n</tr>\n<tr id=\"S2.SS2.tab1.1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S2.SS2.tab1.1.1.7.6.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.7.6.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">FS\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite>\n</td>\n<td id=\"S2.SS2.tab1.1.1.7.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4193</td>\n<td id=\"S2.SS2.tab1.1.1.7.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">5</td>\n<td id=\"S2.SS2.tab1.1.1.7.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.37</td>\n<td id=\"S2.SS2.tab1.1.1.7.6.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">5.8M</td>\n<td id=\"S2.SS2.tab1.1.1.7.6.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">747</td>\n</tr>\n<tr id=\"S2.SS2.tab1.1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S2.SS2.tab1.1.1.8.7.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.8.7.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">HV\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite>\n</td>\n<td id=\"S2.SS2.tab1.1.1.8.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">17</td>\n<td id=\"S2.SS2.tab1.1.1.8.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4</td>\n<td id=\"S2.SS2.tab1.1.1.8.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1</td>\n<td id=\"S2.SS2.tab1.1.1.8.7.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">21.5k</td>\n<td id=\"S2.SS2.tab1.1.1.8.7.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10</td>\n</tr>\n<tr id=\"S2.SS2.tab1.1.1.9.8\" class=\"ltx_tr\">\n<td id=\"S2.SS2.tab1.1.1.9.8.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S2.SS2.tab1.1.1.9.8.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S2.SS2.tab1.1.1.9.8.1.2\" class=\"ltx_text ltx_font_italic\">Audio recordings</span>\n</td>\n<td id=\"S2.SS2.tab1.1.1.9.8.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.9.8.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.9.8.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.9.8.5\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.9.8.6\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S2.SS2.tab1.1.1.10.9\" class=\"ltx_tr\">\n<td id=\"S2.SS2.tab1.1.1.10.9.1\" class=\"ltx_td ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S2.SS2.tab1.1.1.10.9.2\" class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PSyllabus</td>\n<td id=\"S2.SS2.tab1.1.1.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7901</td>\n<td id=\"S2.SS2.tab1.1.1.10.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">11</td>\n<td id=\"S2.SS2.tab1.1.1.10.9.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.02</td>\n<td id=\"S2.SS2.tab1.1.1.10.9.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.1M</td>\n<td id=\"S2.SS2.tab1.1.1.10.9.7\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1233</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S2.SS2.tab1.2\" class=\"ltx_p ltx_figure_panel\">In Figure\u00a0<a href=\"#S2.F4\" title=\"Figure 4 \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> we illustrate the distribution of musical pieces across different historical eras. The Romantic era leads with the highest number of pieces, closely followed by the 20th Century (20th C) era, showing more than 2,000 pieces each. The Modern era exhibits a slightly lower count, yet still maintains a significant presence in the dataset. In contrast, the Baroque and Classical eras have fewer pieces, with the Baroque era having a modest representation and the Classical era having the least, indicating a heavier concentration of pieces in the later musical periods within this particular dataset.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure id=\"S2.F4\" class=\"ltx_figure ltx_figure_panel\"><img src=\"/html/2403.03947/assets/era_distribution_histogram.png\" id=\"S2.F4.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"449\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span>Era distribution of PSyllabus dataset</figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S2.SS2.tab1.3\" class=\"ltx_p ltx_figure_panel\">Figure\u00a0<a href=\"#S2.F5\" title=\"Figure 5 \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents a distribution of composers as part of a dataset containing 1,233 individuals. A dominant portion, 65.9%, are grouped under \u2019Others\u2019, indicating a long tail of composers with smaller representation in the dataset. The remaining portion is divided among several named composers, with the largest represented groups being Scarlatti D., Liszt F., Bach J.S., and Chopin F., each constituting around 3.0% to 3.1% of the dataset. Following them, a gradual decrease in representation is observed with composers like Grieg E., Czerny C., and others, each accounting for percentages ranging from 2.8% down to 1.2%. This suggests a diverse dataset with a few composers being more frequently represented than a vast number of others.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure id=\"S2.F5\" class=\"ltx_figure ltx_figure_panel\"><img src=\"/html/2403.03947/assets/composer_distribution_ordered.png\" id=\"S2.F5.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"479\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span>Composers distribution of PSyllabus dataset</figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure id=\"S2.F6\" class=\"ltx_figure ltx_figure_panel\"><img src=\"/html/2403.03947/assets/composer_grades_histogram_unisex_colors.png\" id=\"S2.F6.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"479\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span>Grades distribution of PSyllabus dataset about women and men composers</figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S2.SS2.tab1.4\" class=\"ltx_p ltx_figure_panel\">Figure\u00a0<a href=\"#S2.F6\" title=\"Figure 6 \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> histograms display the grade levels of musical pieces composed by men and women within the PSyllabus dataset. In the upper chart, works of male composers are spread across the grade levels, with the highest number of pieces at grade level 10 and a notable presence in levels 5 through 8. In the lower chart, the contributions of female composers are shown, with the most substantial number of pieces at grade level 4. Although the level distribution of the women peaks at a lower grade level compared to men, the PSyllabus dataset includes over 14 percent of pieces by female composers, marking it as one of the datasets with a higher representation of women in this field. This is a positive step towards promoting the repertoire of a group that has been historically underrepresented, yet it underscores the ongoing need for efforts to support and enhance the visibility of women composers in the musical canon.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure id=\"S2.F7\" class=\"ltx_figure ltx_figure_panel\"><img src=\"/html/2403.03947/assets/PSyllabus_2.jpg\" id=\"S2.F7.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"598\" height=\"957\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span>Shared difficulty pieces and their ranking correlation between PSyllabus and other rankings.</figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S2.SS2.tab1.5\" class=\"ltx_p ltx_figure_panel\">Finally, in Figure\u00a0<a href=\"#S2.F7\" title=\"Figure 7 \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, we compare the PSylabus partially with other well-known rankings provided by established examination boards: Australian Guild of Music Education (AGME), Trinity College London (Trinity), Australian Music Examinations Board (AMEB), Royal Conservatory of Music (RCM), General Certificate of Secondary Education (GCSE), St. Cecilia School of Music (SCSM), Royal Irish Academy of Music (RIAM), Associated Board of the Royal Schools of Music (ABRSM), New Zealand Music Examinations Board (NZMEB), Australian and New Zealand Cultural Arts (ANZCA), Performance Series (PS), London College of Music (LCM) and the web community Piano Street (Piano St). Consequently, we use tau-c correlation to compare rankings because it evaluates how similar two sets of rankings are, focusing on their order, not the exact values. It\u2019s ideal for ordinal data and handles ties well, offering a robust, non-parametric way to analyze rank correlations. A tau-c number close to 1 means the two rankings are very similar.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S2.SS2.tab1.6\" class=\"ltx_p ltx_figure_panel\">For instance, we observed that the rankings from ABRSM and PSyllabus are very similar, with a tau-c correlation of .97, one of the highest similarities in our dataset. This means they almost always agree on how they rank the pieces. On the other hand, the tau-c correlation between ANZCA and GCSE is .67, which is lower, showing they have different opinions on how to rank the pieces. Finally, the average tau-c correlation across all the rankings is .81, which suggests they mostly agree on the rankings of pieces. However, the variation, shown by a standard deviation of .099, means there are some differences in opinion on the rankings.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S2.SS2.tab1.7\" class=\"ltx_p ltx_figure_panel\">We distribute the PSyllabus dataset for research purposes with the license Creative Commons 4.0, limiting access to the data upon request under the Zenodo platform. In addition, we distribute the links to all the source pieces, composer, and work metadata we have used in collecting the PSyllabus dataset.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S2.SS3\" class=\"ltx_subsection ltx_figure_panel\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S2.SS3.5.1.1\" class=\"ltx_text\">II-C</span> </span><span id=\"S2.SS3.6.2\" class=\"ltx_text ltx_font_italic\">Benchmark datasets</span>\n</h3>\n\n<figure id=\"S2.F8\" class=\"ltx_figure\"><img src=\"/html/2403.03947/assets/hv_difficulty_distribution.png\" id=\"S2.F8.g1\" class=\"ltx_graphics ltx_centering ltx_img_square\" width=\"538\" height=\"453\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 8: </span>Distribution of difficulty of the Hidden Voices dataset.</figcaption>\n</figure>\n<section id=\"S2.SS3.SSS1\" class=\"ltx_subsubsection\">\n<h4 class=\"ltx_title ltx_title_subsubsection\">\n<span class=\"ltx_tag ltx_tag_subsubsection\"><span id=\"S2.SS3.SSS1.5.1.1\" class=\"ltx_text\">II-C</span>1 </span>Hidden Voices dataset</h4>\n\n<div id=\"S2.SS3.SSS1.p1\" class=\"ltx_para\">\n<p id=\"S2.SS3.SSS1.p1.1\" class=\"ltx_p\">Beyond 65 pieces from the Hidden Voices Project, we got 35 videos provided and collected 22 videos from YouTube. As a result, we provide videos of 57 pieces in total. The pieces are divided into 7 difficulty levels - Elementary, Late Elementary, Early Intermediate, Mid Intermediate, Late Intermediate, Early Advanced, and Advanced. We labeled these difficulty levels as integers ranging from 0 to 6. Figure\u00a0<a href=\"#S2.F8\" title=\"Figure 8 \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows the difficulty distribution of the pieces.</p>\n</div>\n<section id=\"S3\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">III </span><span id=\"S3.1.1\" class=\"ltx_text ltx_font_smallcaps\">Methodology</span>\n</h2>\n\n<div id=\"S3.p1\" class=\"ltx_para\">\n<p id=\"S3.p1.1\" class=\"ltx_p\">We introduce input representations extracted from audio: CQT, and piano roll, as detailed in Section\u00a0<a href=\"#S3.SS1\" title=\"III-A Input representations \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span></span></a>. Furthermore, we employ a machine learning classification approach, discussed in Section\u00a0<a href=\"#S3.SS2\" title=\"III-B Classifier Architecture \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-B</span></span></a>, to address automatic score difficulty classification on the PSyllabus dataset. We also explore various methods for combining representations, as described in Sections\u00a0<a href=\"#S3.SS3\" title=\"III-C Multimodal approach \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-C</span></span></a>, and multi-task approaches, in Section\u00a0<a href=\"#S4\" title=\"IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> for leveraging the other dataset labels.</p>\n</div>\n<figure id=\"S3.F9\" class=\"ltx_figure\"><img src=\"/html/2403.03947/assets/random_pianoroll_comparison.png\" id=\"S3.F9.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"538\" height=\"718\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 9: </span>The input representations are piano roll with two channels, frames and onsets, and CQT. </figcaption>\n</figure>\n<section id=\"S3.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S3.SS1.5.1.1\" class=\"ltx_text\">III-A</span> </span><span id=\"S3.SS1.6.2\" class=\"ltx_text ltx_font_italic\">Input representations</span>\n</h3>\n\n<div id=\"S3.SS1.p1\" class=\"ltx_para\">\n<p id=\"S3.SS1.p1.1\" class=\"ltx_p\">We employ two primary forms of input representations, shown in Figure\u00a0<a href=\"#S3.F9\" title=\"Figure 9 \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> to capture the comprehensive characteristics of musical pieces: piano roll and the Constant-Q Transform (CQT). Each representation offers unique insights into the music\u2019s structure and content, serving as a foundational element for our early fusion strategy.</p>\n</div>\n<section id=\"S3.SS1.SSS0.Px1\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">piano roll Features</h5>\n\n<div id=\"S3.SS1.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S3.SS1.SSS0.Px1.p1.1\" class=\"ltx_p\">The piano roll representation splits into two matrices for frames and onsets, enhancing music analysis. The frames matrix maps sustained notes over time, with time on the x-axis and pitches on the y-axis, while the onsets matrix pinpoints the start of notes, clarifying rhythm and articulation. This division allows for detailed insights into melody, rhythm, and harmony, essential for our analysis. We adopt a specific computational method for piano roll transcription <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite>, favored for its broad application and effectiveness across various tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">24</a>, <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">29</a>]</cite>, despite newer models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">20</a>]</cite>.</p>\n</div>\n</section>\n<section id=\"S3.SS1.SSS0.Px2\" class=\"ltx_paragraph\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Constant-Q Transform (CQT) Features</h5>\n\n<div id=\"S3.SS1.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"S3.SS1.SSS0.Px2.p1.1\" class=\"ltx_p\">The CQT feature\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">23</a>]</cite> provides a spectral representation of the audio signal, offering a detailed account of harmonic, timbre, and tonal content. The CQT aligns with piano harmonic constraints, which have a fixed center frequency for each note, and provide equal frequency resolution across all pitch ranges, with center frequencies aligned with piano temperament. This representation is invaluable for analyzing the harmonic structure and texture of the music\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">30</a>, <a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">31</a>]</cite>, complementing the rhythmic and melodic insights provided by the piano roll.</p>\n</div>\n<div id=\"S3.SS1.SSS0.Px2.p2\" class=\"ltx_para\">\n<p id=\"S3.SS1.SSS0.Px2.p2.1\" class=\"ltx_p\">By incorporating both piano roll and CQT features into our model, we aim to leverage the strengths of each representation. The piano roll brings detailed information about note sequences and timing, while the CQT offers a deep understanding of the music\u2019s timbral content. Together, these inputs provide a rich, multimodal feature set that enhances our model\u2019s ability to assess the complexity and difficulty of musical pieces accurately.</p>\n</div>\n</section>\n</section>\n<section id=\"S3.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S3.SS2.5.1.1\" class=\"ltx_text\">III-B</span> </span><span id=\"S3.SS2.6.2\" class=\"ltx_text ltx_font_italic\">Classifier Architecture</span>\n</h3>\n\n<div id=\"S3.SS2.p1\" class=\"ltx_para\">\n<p id=\"S3.SS2.p1.1\" class=\"ltx_p\">We propose a straightforward architecture for assessing the difficulty of musical pieces, employing a residual convolutional network as the first layer for extracting features from the audio-derived input. After this initial feature extraction, we leverage the streamlined architecture outlined in Figure\u00a0<a href=\"#S3.F10\" title=\"Figure 10 \u2023 III-B Classifier Architecture \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, drawing inspiration from models previously utilized for evaluating language understanding <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">32</a>]</cite>.</p>\n</div>\n<figure id=\"S3.F10\" class=\"ltx_figure\"><img src=\"/html/2403.03947/assets/arquitectura_principal.jpg\" id=\"S3.F10.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"180\" height=\"277\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 10: </span>The classifier architecture for score difficulty classification, utilizing audio features CQT and piano roll. Beginning with a residual convolutional network for feature extraction and followed by a recurrent with attention model.</figcaption>\n</figure>\n<div id=\"S3.SS2.p2\" class=\"ltx_para\">\n<p id=\"S3.SS2.p2.1\" class=\"ltx_p\">The convolutional residual network is chosen for its effectiveness in capturing complex audio characteristics\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">33</a>, <a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">34</a>, <a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">35</a>]</cite>. This approach addresses the vanishing gradient problem, allowing for deeper learning models that can provide nuanced feature representations, crucial in music analysis\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">36</a>]</cite>. Following this, a Gated Recurrent Unit (GRU) network models temporal dependencies within the music, capturing the evolution of features over time, enhancing the model\u2019s ability to analyze musical complexity and progress through time axis\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">37</a>, <a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">38</a>]</cite>.</p>\n</div>\n<div id=\"S3.SS2.p3\" class=\"ltx_para\">\n<p id=\"S3.SS2.p3.1\" class=\"ltx_p\">This architecture uses the same attention mechanism as <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">14</a>]</cite>, context attention, initially designed for summarizing texts <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a>]</cite> and later adapted to music score analysis <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">17</a>]</cite>. Hierarchical context attention aggregates note-level hidden states of varying lengths into a singular vector, as demonstrated in Figure\u00a0<a href=\"#S3.F10\" title=\"Figure 10 \u2023 III-B Classifier Architecture \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, enhancing our model\u2019s ability to assess musical piece difficulty comprehensively.</p>\n</div>\n<div id=\"S3.SS2.p4\" class=\"ltx_para\">\n<p id=\"S3.SS2.p4.5\" class=\"ltx_p\">For a sequence of note-level hidden states <math id=\"S3.SS2.p4.1.m1.4\" class=\"ltx_Math\" alttext=\"\\textbf{x}_{T}=[x_{0},x_{1},...,x_{t}]\" display=\"inline\"><semantics id=\"S3.SS2.p4.1.m1.4a\"><mrow id=\"S3.SS2.p4.1.m1.4.4\" xref=\"S3.SS2.p4.1.m1.4.4.cmml\"><msub id=\"S3.SS2.p4.1.m1.4.4.5\" xref=\"S3.SS2.p4.1.m1.4.4.5.cmml\"><mtext class=\"ltx_mathvariant_bold\" id=\"S3.SS2.p4.1.m1.4.4.5.2\" xref=\"S3.SS2.p4.1.m1.4.4.5.2a.cmml\">x</mtext><mi id=\"S3.SS2.p4.1.m1.4.4.5.3\" xref=\"S3.SS2.p4.1.m1.4.4.5.3.cmml\">T</mi></msub><mo id=\"S3.SS2.p4.1.m1.4.4.4\" xref=\"S3.SS2.p4.1.m1.4.4.4.cmml\">=</mo><mrow id=\"S3.SS2.p4.1.m1.4.4.3.3\" xref=\"S3.SS2.p4.1.m1.4.4.3.4.cmml\"><mo stretchy=\"false\" id=\"S3.SS2.p4.1.m1.4.4.3.3.4\" xref=\"S3.SS2.p4.1.m1.4.4.3.4.cmml\">[</mo><msub id=\"S3.SS2.p4.1.m1.2.2.1.1.1\" xref=\"S3.SS2.p4.1.m1.2.2.1.1.1.cmml\"><mi id=\"S3.SS2.p4.1.m1.2.2.1.1.1.2\" xref=\"S3.SS2.p4.1.m1.2.2.1.1.1.2.cmml\">x</mi><mn id=\"S3.SS2.p4.1.m1.2.2.1.1.1.3\" xref=\"S3.SS2.p4.1.m1.2.2.1.1.1.3.cmml\">0</mn></msub><mo id=\"S3.SS2.p4.1.m1.4.4.3.3.5\" xref=\"S3.SS2.p4.1.m1.4.4.3.4.cmml\">,</mo><msub id=\"S3.SS2.p4.1.m1.3.3.2.2.2\" xref=\"S3.SS2.p4.1.m1.3.3.2.2.2.cmml\"><mi id=\"S3.SS2.p4.1.m1.3.3.2.2.2.2\" xref=\"S3.SS2.p4.1.m1.3.3.2.2.2.2.cmml\">x</mi><mn id=\"S3.SS2.p4.1.m1.3.3.2.2.2.3\" xref=\"S3.SS2.p4.1.m1.3.3.2.2.2.3.cmml\">1</mn></msub><mo id=\"S3.SS2.p4.1.m1.4.4.3.3.6\" xref=\"S3.SS2.p4.1.m1.4.4.3.4.cmml\">,</mo><mi mathvariant=\"normal\" id=\"S3.SS2.p4.1.m1.1.1\" xref=\"S3.SS2.p4.1.m1.1.1.cmml\">\u2026</mi><mo id=\"S3.SS2.p4.1.m1.4.4.3.3.7\" xref=\"S3.SS2.p4.1.m1.4.4.3.4.cmml\">,</mo><msub id=\"S3.SS2.p4.1.m1.4.4.3.3.3\" xref=\"S3.SS2.p4.1.m1.4.4.3.3.3.cmml\"><mi id=\"S3.SS2.p4.1.m1.4.4.3.3.3.2\" xref=\"S3.SS2.p4.1.m1.4.4.3.3.3.2.cmml\">x</mi><mi id=\"S3.SS2.p4.1.m1.4.4.3.3.3.3\" xref=\"S3.SS2.p4.1.m1.4.4.3.3.3.3.cmml\">t</mi></msub><mo stretchy=\"false\" id=\"S3.SS2.p4.1.m1.4.4.3.3.8\" xref=\"S3.SS2.p4.1.m1.4.4.3.4.cmml\">]</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.p4.1.m1.4b\"><apply id=\"S3.SS2.p4.1.m1.4.4.cmml\" xref=\"S3.SS2.p4.1.m1.4.4\"><eq id=\"S3.SS2.p4.1.m1.4.4.4.cmml\" xref=\"S3.SS2.p4.1.m1.4.4.4\"></eq><apply id=\"S3.SS2.p4.1.m1.4.4.5.cmml\" xref=\"S3.SS2.p4.1.m1.4.4.5\"><csymbol cd=\"ambiguous\" id=\"S3.SS2.p4.1.m1.4.4.5.1.cmml\" xref=\"S3.SS2.p4.1.m1.4.4.5\">subscript</csymbol><ci id=\"S3.SS2.p4.1.m1.4.4.5.2a.cmml\" xref=\"S3.SS2.p4.1.m1.4.4.5.2\"><mtext class=\"ltx_mathvariant_bold\" id=\"S3.SS2.p4.1.m1.4.4.5.2.cmml\" xref=\"S3.SS2.p4.1.m1.4.4.5.2\">x</mtext></ci><ci id=\"S3.SS2.p4.1.m1.4.4.5.3.cmml\" xref=\"S3.SS2.p4.1.m1.4.4.5.3\">\ud835\udc47</ci></apply><list id=\"S3.SS2.p4.1.m1.4.4.3.4.cmml\" xref=\"S3.SS2.p4.1.m1.4.4.3.3\"><apply id=\"S3.SS2.p4.1.m1.2.2.1.1.1.cmml\" xref=\"S3.SS2.p4.1.m1.2.2.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS2.p4.1.m1.2.2.1.1.1.1.cmml\" xref=\"S3.SS2.p4.1.m1.2.2.1.1.1\">subscript</csymbol><ci id=\"S3.SS2.p4.1.m1.2.2.1.1.1.2.cmml\" xref=\"S3.SS2.p4.1.m1.2.2.1.1.1.2\">\ud835\udc65</ci><cn type=\"integer\" id=\"S3.SS2.p4.1.m1.2.2.1.1.1.3.cmml\" xref=\"S3.SS2.p4.1.m1.2.2.1.1.1.3\">0</cn></apply><apply id=\"S3.SS2.p4.1.m1.3.3.2.2.2.cmml\" xref=\"S3.SS2.p4.1.m1.3.3.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS2.p4.1.m1.3.3.2.2.2.1.cmml\" xref=\"S3.SS2.p4.1.m1.3.3.2.2.2\">subscript</csymbol><ci id=\"S3.SS2.p4.1.m1.3.3.2.2.2.2.cmml\" xref=\"S3.SS2.p4.1.m1.3.3.2.2.2.2\">\ud835\udc65</ci><cn type=\"integer\" id=\"S3.SS2.p4.1.m1.3.3.2.2.2.3.cmml\" xref=\"S3.SS2.p4.1.m1.3.3.2.2.2.3\">1</cn></apply><ci id=\"S3.SS2.p4.1.m1.1.1.cmml\" xref=\"S3.SS2.p4.1.m1.1.1\">\u2026</ci><apply id=\"S3.SS2.p4.1.m1.4.4.3.3.3.cmml\" xref=\"S3.SS2.p4.1.m1.4.4.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS2.p4.1.m1.4.4.3.3.3.1.cmml\" xref=\"S3.SS2.p4.1.m1.4.4.3.3.3\">subscript</csymbol><ci id=\"S3.SS2.p4.1.m1.4.4.3.3.3.2.cmml\" xref=\"S3.SS2.p4.1.m1.4.4.3.3.3.2\">\ud835\udc65</ci><ci id=\"S3.SS2.p4.1.m1.4.4.3.3.3.3.cmml\" xref=\"S3.SS2.p4.1.m1.4.4.3.3.3.3\">\ud835\udc61</ci></apply></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.p4.1.m1.4c\">\\textbf{x}_{T}=[x_{0},x_{1},...,x_{t}]</annotation></semantics></math>, hierarchical context attention summarizes this sequence as <math id=\"S3.SS2.p4.2.m2.1\" class=\"ltx_Math\" alttext=\"y=\\sum_{t}^{T}\\alpha_{t}x_{t}\" display=\"inline\"><semantics id=\"S3.SS2.p4.2.m2.1a\"><mrow id=\"S3.SS2.p4.2.m2.1.1\" xref=\"S3.SS2.p4.2.m2.1.1.cmml\"><mi id=\"S3.SS2.p4.2.m2.1.1.2\" xref=\"S3.SS2.p4.2.m2.1.1.2.cmml\">y</mi><mo rspace=\"0.111em\" id=\"S3.SS2.p4.2.m2.1.1.1\" xref=\"S3.SS2.p4.2.m2.1.1.1.cmml\">=</mo><mrow id=\"S3.SS2.p4.2.m2.1.1.3\" xref=\"S3.SS2.p4.2.m2.1.1.3.cmml\"><msubsup id=\"S3.SS2.p4.2.m2.1.1.3.1\" xref=\"S3.SS2.p4.2.m2.1.1.3.1.cmml\"><mo id=\"S3.SS2.p4.2.m2.1.1.3.1.2.2\" xref=\"S3.SS2.p4.2.m2.1.1.3.1.2.2.cmml\">\u2211</mo><mi id=\"S3.SS2.p4.2.m2.1.1.3.1.2.3\" xref=\"S3.SS2.p4.2.m2.1.1.3.1.2.3.cmml\">t</mi><mi id=\"S3.SS2.p4.2.m2.1.1.3.1.3\" xref=\"S3.SS2.p4.2.m2.1.1.3.1.3.cmml\">T</mi></msubsup><mrow id=\"S3.SS2.p4.2.m2.1.1.3.2\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.cmml\"><msub id=\"S3.SS2.p4.2.m2.1.1.3.2.2\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.2.cmml\"><mi id=\"S3.SS2.p4.2.m2.1.1.3.2.2.2\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.2.2.cmml\">\u03b1</mi><mi id=\"S3.SS2.p4.2.m2.1.1.3.2.2.3\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.2.3.cmml\">t</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS2.p4.2.m2.1.1.3.2.1\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.1.cmml\">\u200b</mo><msub id=\"S3.SS2.p4.2.m2.1.1.3.2.3\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.3.cmml\"><mi id=\"S3.SS2.p4.2.m2.1.1.3.2.3.2\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.3.2.cmml\">x</mi><mi id=\"S3.SS2.p4.2.m2.1.1.3.2.3.3\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.3.3.cmml\">t</mi></msub></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.p4.2.m2.1b\"><apply id=\"S3.SS2.p4.2.m2.1.1.cmml\" xref=\"S3.SS2.p4.2.m2.1.1\"><eq id=\"S3.SS2.p4.2.m2.1.1.1.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.1\"></eq><ci id=\"S3.SS2.p4.2.m2.1.1.2.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.2\">\ud835\udc66</ci><apply id=\"S3.SS2.p4.2.m2.1.1.3.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3\"><apply id=\"S3.SS2.p4.2.m2.1.1.3.1.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS2.p4.2.m2.1.1.3.1.1.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.1\">superscript</csymbol><apply id=\"S3.SS2.p4.2.m2.1.1.3.1.2.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS2.p4.2.m2.1.1.3.1.2.1.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.1\">subscript</csymbol><sum id=\"S3.SS2.p4.2.m2.1.1.3.1.2.2.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.1.2.2\"></sum><ci id=\"S3.SS2.p4.2.m2.1.1.3.1.2.3.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.1.2.3\">\ud835\udc61</ci></apply><ci id=\"S3.SS2.p4.2.m2.1.1.3.1.3.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.1.3\">\ud835\udc47</ci></apply><apply id=\"S3.SS2.p4.2.m2.1.1.3.2.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.2\"><times id=\"S3.SS2.p4.2.m2.1.1.3.2.1.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.1\"></times><apply id=\"S3.SS2.p4.2.m2.1.1.3.2.2.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS2.p4.2.m2.1.1.3.2.2.1.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.2\">subscript</csymbol><ci id=\"S3.SS2.p4.2.m2.1.1.3.2.2.2.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.2.2\">\ud835\udefc</ci><ci id=\"S3.SS2.p4.2.m2.1.1.3.2.2.3.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.2.3\">\ud835\udc61</ci></apply><apply id=\"S3.SS2.p4.2.m2.1.1.3.2.3.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS2.p4.2.m2.1.1.3.2.3.1.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.3\">subscript</csymbol><ci id=\"S3.SS2.p4.2.m2.1.1.3.2.3.2.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.3.2\">\ud835\udc65</ci><ci id=\"S3.SS2.p4.2.m2.1.1.3.2.3.3.cmml\" xref=\"S3.SS2.p4.2.m2.1.1.3.2.3.3\">\ud835\udc61</ci></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.p4.2.m2.1c\">y=\\sum_{t}^{T}\\alpha_{t}x_{t}</annotation></semantics></math>, with <math id=\"S3.SS2.p4.3.m3.1\" class=\"ltx_math_unparsed\" alttext=\"\\alpha_{t}=\\text{Softmax}(\\tanh(\\textbf{W}x_{t}+b)^{\\top}c)\" display=\"inline\"><semantics id=\"S3.SS2.p4.3.m3.1a\"><mrow id=\"S3.SS2.p4.3.m3.1b\"><msub id=\"S3.SS2.p4.3.m3.1.2\"><mi id=\"S3.SS2.p4.3.m3.1.2.2\">\u03b1</mi><mi id=\"S3.SS2.p4.3.m3.1.2.3\">t</mi></msub><mo id=\"S3.SS2.p4.3.m3.1.3\">=</mo><mtext id=\"S3.SS2.p4.3.m3.1.4\">Softmax</mtext><mrow id=\"S3.SS2.p4.3.m3.1.5\"><mo stretchy=\"false\" id=\"S3.SS2.p4.3.m3.1.5.1\">(</mo><mi id=\"S3.SS2.p4.3.m3.1.1\">tanh</mi><msup id=\"S3.SS2.p4.3.m3.1.5.2\"><mrow id=\"S3.SS2.p4.3.m3.1.5.2.2\"><mo stretchy=\"false\" id=\"S3.SS2.p4.3.m3.1.5.2.2.1\">(</mo><mtext class=\"ltx_mathvariant_bold\" id=\"S3.SS2.p4.3.m3.1.5.2.2.2\">W</mtext><msub id=\"S3.SS2.p4.3.m3.1.5.2.2.3\"><mi id=\"S3.SS2.p4.3.m3.1.5.2.2.3.2\">x</mi><mi id=\"S3.SS2.p4.3.m3.1.5.2.2.3.3\">t</mi></msub><mo id=\"S3.SS2.p4.3.m3.1.5.2.2.4\">+</mo><mi id=\"S3.SS2.p4.3.m3.1.5.2.2.5\">b</mi><mo stretchy=\"false\" id=\"S3.SS2.p4.3.m3.1.5.2.2.6\">)</mo></mrow><mo id=\"S3.SS2.p4.3.m3.1.5.2.3\">\u22a4</mo></msup><mi id=\"S3.SS2.p4.3.m3.1.5.3\">c</mi><mo stretchy=\"false\" id=\"S3.SS2.p4.3.m3.1.5.4\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S3.SS2.p4.3.m3.1c\">\\alpha_{t}=\\text{Softmax}(\\tanh(\\textbf{W}x_{t}+b)^{\\top}c)</annotation></semantics></math>, where <math id=\"S3.SS2.p4.4.m4.1\" class=\"ltx_Math\" alttext=\"c\" display=\"inline\"><semantics id=\"S3.SS2.p4.4.m4.1a\"><mi id=\"S3.SS2.p4.4.m4.1.1\" xref=\"S3.SS2.p4.4.m4.1.1.cmml\">c</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.p4.4.m4.1b\"><ci id=\"S3.SS2.p4.4.m4.1.1.cmml\" xref=\"S3.SS2.p4.4.m4.1.1\">\ud835\udc50</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.p4.4.m4.1c\">c</annotation></semantics></math> is a trainable context vector. This mechanism allows for dynamic weighting of each note\u2019s representation, based on its dot product with <math id=\"S3.SS2.p4.5.m5.1\" class=\"ltx_Math\" alttext=\"c\" display=\"inline\"><semantics id=\"S3.SS2.p4.5.m5.1a\"><mi id=\"S3.SS2.p4.5.m5.1.1\" xref=\"S3.SS2.p4.5.m5.1.1.cmml\">c</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.p4.5.m5.1b\"><ci id=\"S3.SS2.p4.5.m5.1.1.cmml\" xref=\"S3.SS2.p4.5.m5.1.1\">\ud835\udc50</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.p4.5.m5.1c\">c</annotation></semantics></math>, offering a nuanced understanding of its importance in difficulty prediction. This approach diverges from the DeepGRU model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite>, which uses the last hidden state for attention weighting, in favor of a method that does not singularly focus on the beginning or end of the piece, aligning with our premise that evaluating musical difficulty requires a holistic view. The process culminates in a linear (FC) layer that leads to the difficult prediction output. In the experiments with auxiliary tasks, we add a linear layer for each classification.</p>\n</div>\n</section>\n<section id=\"S3.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S3.SS3.5.1.1\" class=\"ltx_text\">III-C</span> </span><span id=\"S3.SS3.6.2\" class=\"ltx_text ltx_font_italic\">Multimodal approach</span>\n</h3>\n\n<div id=\"S3.SS3.p1\" class=\"ltx_para\">\n<p id=\"S3.SS3.p1.1\" class=\"ltx_p\">In our approach, the Constant-Q Transform (CQT) and transcribed piano roll data are integrated using an early fusion strategy. This process involves the extraction of features from both data types via dedicated residual convolutional networks (FeatureExtraction), followed by their combination, as shown in Figure\u00a0<a href=\"#S3.F11\" title=\"Figure 11 \u2023 III-C Multimodal approach \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n</div>\n<div id=\"S3.SS3.p2\" class=\"ltx_para\">\n<p id=\"S3.SS3.p2.3\" class=\"ltx_p\">For feature extraction, we apply two specialized residual convolutional networks:</p>\n<table id=\"S3.E1\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S3.E1.m1.2\" class=\"ltx_Math\" alttext=\"F_{CQT}=FeatureExtraction_{CQT}(CQT(x)),\" display=\"block\"><semantics id=\"S3.E1.m1.2a\"><mrow id=\"S3.E1.m1.2.2.1\" xref=\"S3.E1.m1.2.2.1.1.cmml\"><mrow id=\"S3.E1.m1.2.2.1.1\" xref=\"S3.E1.m1.2.2.1.1.cmml\"><msub id=\"S3.E1.m1.2.2.1.1.3\" xref=\"S3.E1.m1.2.2.1.1.3.cmml\"><mi id=\"S3.E1.m1.2.2.1.1.3.2\" xref=\"S3.E1.m1.2.2.1.1.3.2.cmml\">F</mi><mrow id=\"S3.E1.m1.2.2.1.1.3.3\" xref=\"S3.E1.m1.2.2.1.1.3.3.cmml\"><mi id=\"S3.E1.m1.2.2.1.1.3.3.2\" xref=\"S3.E1.m1.2.2.1.1.3.3.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.3.3.1\" xref=\"S3.E1.m1.2.2.1.1.3.3.1.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.3.3.3\" xref=\"S3.E1.m1.2.2.1.1.3.3.3.cmml\">Q</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.3.3.1a\" xref=\"S3.E1.m1.2.2.1.1.3.3.1.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.3.3.4\" xref=\"S3.E1.m1.2.2.1.1.3.3.4.cmml\">T</mi></mrow></msub><mo id=\"S3.E1.m1.2.2.1.1.2\" xref=\"S3.E1.m1.2.2.1.1.2.cmml\">=</mo><mrow id=\"S3.E1.m1.2.2.1.1.1\" xref=\"S3.E1.m1.2.2.1.1.1.cmml\"><mi id=\"S3.E1.m1.2.2.1.1.1.3\" xref=\"S3.E1.m1.2.2.1.1.1.3.cmml\">F</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.4\" xref=\"S3.E1.m1.2.2.1.1.1.4.cmml\">e</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2a\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.5\" xref=\"S3.E1.m1.2.2.1.1.1.5.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2b\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.6\" xref=\"S3.E1.m1.2.2.1.1.1.6.cmml\">t</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2c\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.7\" xref=\"S3.E1.m1.2.2.1.1.1.7.cmml\">u</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2d\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.8\" xref=\"S3.E1.m1.2.2.1.1.1.8.cmml\">r</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2e\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.9\" xref=\"S3.E1.m1.2.2.1.1.1.9.cmml\">e</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2f\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.10\" xref=\"S3.E1.m1.2.2.1.1.1.10.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2g\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.11\" xref=\"S3.E1.m1.2.2.1.1.1.11.cmml\">x</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2h\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.12\" xref=\"S3.E1.m1.2.2.1.1.1.12.cmml\">t</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2i\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.13\" xref=\"S3.E1.m1.2.2.1.1.1.13.cmml\">r</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2j\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.14\" xref=\"S3.E1.m1.2.2.1.1.1.14.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2k\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.15\" xref=\"S3.E1.m1.2.2.1.1.1.15.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2l\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.16\" xref=\"S3.E1.m1.2.2.1.1.1.16.cmml\">t</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2m\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.17\" xref=\"S3.E1.m1.2.2.1.1.1.17.cmml\">i</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2n\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.18\" xref=\"S3.E1.m1.2.2.1.1.1.18.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2o\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><msub id=\"S3.E1.m1.2.2.1.1.1.19\" xref=\"S3.E1.m1.2.2.1.1.1.19.cmml\"><mi id=\"S3.E1.m1.2.2.1.1.1.19.2\" xref=\"S3.E1.m1.2.2.1.1.1.19.2.cmml\">n</mi><mrow id=\"S3.E1.m1.2.2.1.1.1.19.3\" xref=\"S3.E1.m1.2.2.1.1.1.19.3.cmml\"><mi id=\"S3.E1.m1.2.2.1.1.1.19.3.2\" xref=\"S3.E1.m1.2.2.1.1.1.19.3.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.19.3.1\" xref=\"S3.E1.m1.2.2.1.1.1.19.3.1.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.19.3.3\" xref=\"S3.E1.m1.2.2.1.1.1.19.3.3.cmml\">Q</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.19.3.1a\" xref=\"S3.E1.m1.2.2.1.1.1.19.3.1.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.19.3.4\" xref=\"S3.E1.m1.2.2.1.1.1.19.3.4.cmml\">T</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.2p\" xref=\"S3.E1.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mrow id=\"S3.E1.m1.2.2.1.1.1.1.1\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S3.E1.m1.2.2.1.1.1.1.1.2\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S3.E1.m1.2.2.1.1.1.1.1.1\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.cmml\"><mi id=\"S3.E1.m1.2.2.1.1.1.1.1.1.2\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.1.1.1.1\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.1.1.1.3\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml\">Q</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.1.1.1.1a\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml\">\u200b</mo><mi id=\"S3.E1.m1.2.2.1.1.1.1.1.1.4\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.4.cmml\">T</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E1.m1.2.2.1.1.1.1.1.1.1b\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml\">\u200b</mo><mrow id=\"S3.E1.m1.2.2.1.1.1.1.1.1.5.2\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S3.E1.m1.2.2.1.1.1.1.1.1.5.2.1\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.cmml\">(</mo><mi id=\"S3.E1.m1.1.1\" xref=\"S3.E1.m1.1.1.cmml\">x</mi><mo stretchy=\"false\" id=\"S3.E1.m1.2.2.1.1.1.1.1.1.5.2.2\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo stretchy=\"false\" id=\"S3.E1.m1.2.2.1.1.1.1.1.3\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S3.E1.m1.2.2.1.2\" xref=\"S3.E1.m1.2.2.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.E1.m1.2b\"><apply id=\"S3.E1.m1.2.2.1.1.cmml\" xref=\"S3.E1.m1.2.2.1\"><eq id=\"S3.E1.m1.2.2.1.1.2.cmml\" xref=\"S3.E1.m1.2.2.1.1.2\"></eq><apply id=\"S3.E1.m1.2.2.1.1.3.cmml\" xref=\"S3.E1.m1.2.2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S3.E1.m1.2.2.1.1.3.1.cmml\" xref=\"S3.E1.m1.2.2.1.1.3\">subscript</csymbol><ci id=\"S3.E1.m1.2.2.1.1.3.2.cmml\" xref=\"S3.E1.m1.2.2.1.1.3.2\">\ud835\udc39</ci><apply id=\"S3.E1.m1.2.2.1.1.3.3.cmml\" xref=\"S3.E1.m1.2.2.1.1.3.3\"><times id=\"S3.E1.m1.2.2.1.1.3.3.1.cmml\" xref=\"S3.E1.m1.2.2.1.1.3.3.1\"></times><ci id=\"S3.E1.m1.2.2.1.1.3.3.2.cmml\" xref=\"S3.E1.m1.2.2.1.1.3.3.2\">\ud835\udc36</ci><ci id=\"S3.E1.m1.2.2.1.1.3.3.3.cmml\" xref=\"S3.E1.m1.2.2.1.1.3.3.3\">\ud835\udc44</ci><ci id=\"S3.E1.m1.2.2.1.1.3.3.4.cmml\" xref=\"S3.E1.m1.2.2.1.1.3.3.4\">\ud835\udc47</ci></apply></apply><apply id=\"S3.E1.m1.2.2.1.1.1.cmml\" xref=\"S3.E1.m1.2.2.1.1.1\"><times id=\"S3.E1.m1.2.2.1.1.1.2.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.2\"></times><ci id=\"S3.E1.m1.2.2.1.1.1.3.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.3\">\ud835\udc39</ci><ci id=\"S3.E1.m1.2.2.1.1.1.4.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.4\">\ud835\udc52</ci><ci id=\"S3.E1.m1.2.2.1.1.1.5.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.5\">\ud835\udc4e</ci><ci id=\"S3.E1.m1.2.2.1.1.1.6.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.6\">\ud835\udc61</ci><ci id=\"S3.E1.m1.2.2.1.1.1.7.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.7\">\ud835\udc62</ci><ci id=\"S3.E1.m1.2.2.1.1.1.8.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.8\">\ud835\udc5f</ci><ci id=\"S3.E1.m1.2.2.1.1.1.9.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.9\">\ud835\udc52</ci><ci id=\"S3.E1.m1.2.2.1.1.1.10.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.10\">\ud835\udc38</ci><ci id=\"S3.E1.m1.2.2.1.1.1.11.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.11\">\ud835\udc65</ci><ci id=\"S3.E1.m1.2.2.1.1.1.12.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.12\">\ud835\udc61</ci><ci id=\"S3.E1.m1.2.2.1.1.1.13.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.13\">\ud835\udc5f</ci><ci id=\"S3.E1.m1.2.2.1.1.1.14.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.14\">\ud835\udc4e</ci><ci id=\"S3.E1.m1.2.2.1.1.1.15.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.15\">\ud835\udc50</ci><ci id=\"S3.E1.m1.2.2.1.1.1.16.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.16\">\ud835\udc61</ci><ci id=\"S3.E1.m1.2.2.1.1.1.17.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.17\">\ud835\udc56</ci><ci id=\"S3.E1.m1.2.2.1.1.1.18.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.18\">\ud835\udc5c</ci><apply id=\"S3.E1.m1.2.2.1.1.1.19.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.19\"><csymbol cd=\"ambiguous\" id=\"S3.E1.m1.2.2.1.1.1.19.1.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.19\">subscript</csymbol><ci id=\"S3.E1.m1.2.2.1.1.1.19.2.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.19.2\">\ud835\udc5b</ci><apply id=\"S3.E1.m1.2.2.1.1.1.19.3.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.19.3\"><times id=\"S3.E1.m1.2.2.1.1.1.19.3.1.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.19.3.1\"></times><ci id=\"S3.E1.m1.2.2.1.1.1.19.3.2.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.19.3.2\">\ud835\udc36</ci><ci id=\"S3.E1.m1.2.2.1.1.1.19.3.3.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.19.3.3\">\ud835\udc44</ci><ci id=\"S3.E1.m1.2.2.1.1.1.19.3.4.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.19.3.4\">\ud835\udc47</ci></apply></apply><apply id=\"S3.E1.m1.2.2.1.1.1.1.1.1.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.1.1\"><times id=\"S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.1\"></times><ci id=\"S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.2\">\ud835\udc36</ci><ci id=\"S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.3\">\ud835\udc44</ci><ci id=\"S3.E1.m1.2.2.1.1.1.1.1.1.4.cmml\" xref=\"S3.E1.m1.2.2.1.1.1.1.1.1.4\">\ud835\udc47</ci><ci id=\"S3.E1.m1.1.1.cmml\" xref=\"S3.E1.m1.1.1\">\ud835\udc65</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.E1.m1.2c\">F_{CQT}=FeatureExtraction_{CQT}(CQT(x)),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n<table id=\"S3.E2\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S3.E2.m1.2\" class=\"ltx_Math\" alttext=\"F_{pianoroll}=FeatureExtraction_{pianoroll}(pianoroll(y)),\" display=\"block\"><semantics id=\"S3.E2.m1.2a\"><mrow id=\"S3.E2.m1.2.2.1\" xref=\"S3.E2.m1.2.2.1.1.cmml\"><mrow id=\"S3.E2.m1.2.2.1.1\" xref=\"S3.E2.m1.2.2.1.1.cmml\"><msub id=\"S3.E2.m1.2.2.1.1.3\" xref=\"S3.E2.m1.2.2.1.1.3.cmml\"><mi id=\"S3.E2.m1.2.2.1.1.3.2\" xref=\"S3.E2.m1.2.2.1.1.3.2.cmml\">F</mi><mrow id=\"S3.E2.m1.2.2.1.1.3.3\" xref=\"S3.E2.m1.2.2.1.1.3.3.cmml\"><mi id=\"S3.E2.m1.2.2.1.1.3.3.2\" xref=\"S3.E2.m1.2.2.1.1.3.3.2.cmml\">p</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.3.3.1\" xref=\"S3.E2.m1.2.2.1.1.3.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.3.3.3\" xref=\"S3.E2.m1.2.2.1.1.3.3.3.cmml\">i</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.3.3.1a\" xref=\"S3.E2.m1.2.2.1.1.3.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.3.3.4\" xref=\"S3.E2.m1.2.2.1.1.3.3.4.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.3.3.1b\" xref=\"S3.E2.m1.2.2.1.1.3.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.3.3.5\" xref=\"S3.E2.m1.2.2.1.1.3.3.5.cmml\">n</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.3.3.1c\" xref=\"S3.E2.m1.2.2.1.1.3.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.3.3.6\" xref=\"S3.E2.m1.2.2.1.1.3.3.6.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.3.3.1d\" xref=\"S3.E2.m1.2.2.1.1.3.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.3.3.7\" xref=\"S3.E2.m1.2.2.1.1.3.3.7.cmml\">r</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.3.3.1e\" xref=\"S3.E2.m1.2.2.1.1.3.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.3.3.8\" xref=\"S3.E2.m1.2.2.1.1.3.3.8.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.3.3.1f\" xref=\"S3.E2.m1.2.2.1.1.3.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.3.3.9\" xref=\"S3.E2.m1.2.2.1.1.3.3.9.cmml\">l</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.3.3.1g\" xref=\"S3.E2.m1.2.2.1.1.3.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.3.3.10\" xref=\"S3.E2.m1.2.2.1.1.3.3.10.cmml\">l</mi></mrow></msub><mo id=\"S3.E2.m1.2.2.1.1.2\" xref=\"S3.E2.m1.2.2.1.1.2.cmml\">=</mo><mrow id=\"S3.E2.m1.2.2.1.1.1\" xref=\"S3.E2.m1.2.2.1.1.1.cmml\"><mi id=\"S3.E2.m1.2.2.1.1.1.3\" xref=\"S3.E2.m1.2.2.1.1.1.3.cmml\">F</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.4\" xref=\"S3.E2.m1.2.2.1.1.1.4.cmml\">e</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2a\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.5\" xref=\"S3.E2.m1.2.2.1.1.1.5.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2b\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.6\" xref=\"S3.E2.m1.2.2.1.1.1.6.cmml\">t</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2c\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.7\" xref=\"S3.E2.m1.2.2.1.1.1.7.cmml\">u</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2d\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.8\" xref=\"S3.E2.m1.2.2.1.1.1.8.cmml\">r</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2e\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.9\" xref=\"S3.E2.m1.2.2.1.1.1.9.cmml\">e</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2f\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.10\" xref=\"S3.E2.m1.2.2.1.1.1.10.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2g\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.11\" xref=\"S3.E2.m1.2.2.1.1.1.11.cmml\">x</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2h\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.12\" xref=\"S3.E2.m1.2.2.1.1.1.12.cmml\">t</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2i\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.13\" xref=\"S3.E2.m1.2.2.1.1.1.13.cmml\">r</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2j\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.14\" xref=\"S3.E2.m1.2.2.1.1.1.14.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2k\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.15\" xref=\"S3.E2.m1.2.2.1.1.1.15.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2l\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.16\" xref=\"S3.E2.m1.2.2.1.1.1.16.cmml\">t</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2m\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.17\" xref=\"S3.E2.m1.2.2.1.1.1.17.cmml\">i</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2n\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.18\" xref=\"S3.E2.m1.2.2.1.1.1.18.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2o\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><msub id=\"S3.E2.m1.2.2.1.1.1.19\" xref=\"S3.E2.m1.2.2.1.1.1.19.cmml\"><mi id=\"S3.E2.m1.2.2.1.1.1.19.2\" xref=\"S3.E2.m1.2.2.1.1.1.19.2.cmml\">n</mi><mrow id=\"S3.E2.m1.2.2.1.1.1.19.3\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.cmml\"><mi id=\"S3.E2.m1.2.2.1.1.1.19.3.2\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.2.cmml\">p</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.19.3.1\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.19.3.3\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.3.cmml\">i</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.19.3.1a\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.19.3.4\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.4.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.19.3.1b\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.19.3.5\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.5.cmml\">n</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.19.3.1c\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.19.3.6\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.6.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.19.3.1d\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.19.3.7\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.7.cmml\">r</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.19.3.1e\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.19.3.8\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.8.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.19.3.1f\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.19.3.9\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.9.cmml\">l</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.19.3.1g\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.19.3.10\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.10.cmml\">l</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.2p\" xref=\"S3.E2.m1.2.2.1.1.1.2.cmml\">\u200b</mo><mrow id=\"S3.E2.m1.2.2.1.1.1.1.1\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S3.E2.m1.2.2.1.1.1.1.1.2\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S3.E2.m1.2.2.1.1.1.1.1.1\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.cmml\"><mi id=\"S3.E2.m1.2.2.1.1.1.1.1.1.2\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml\">p</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.1.1.1.1\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.1.1.1.3\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml\">i</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.1.1.1.1a\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.1.1.1.4\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.4.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.1.1.1.1b\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.1.1.1.5\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.5.cmml\">n</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.1.1.1.1c\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.1.1.1.6\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.6.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.1.1.1.1d\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.1.1.1.7\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.7.cmml\">r</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.1.1.1.1e\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.1.1.1.8\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.8.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.1.1.1.1f\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.1.1.1.9\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.9.cmml\">l</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.1.1.1.1g\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml\">\u200b</mo><mi id=\"S3.E2.m1.2.2.1.1.1.1.1.1.10\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.10.cmml\">l</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E2.m1.2.2.1.1.1.1.1.1.1h\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml\">\u200b</mo><mrow id=\"S3.E2.m1.2.2.1.1.1.1.1.1.11.2\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S3.E2.m1.2.2.1.1.1.1.1.1.11.2.1\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.cmml\">(</mo><mi id=\"S3.E2.m1.1.1\" xref=\"S3.E2.m1.1.1.cmml\">y</mi><mo stretchy=\"false\" id=\"S3.E2.m1.2.2.1.1.1.1.1.1.11.2.2\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow><mo stretchy=\"false\" id=\"S3.E2.m1.2.2.1.1.1.1.1.3\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S3.E2.m1.2.2.1.2\" xref=\"S3.E2.m1.2.2.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.E2.m1.2b\"><apply id=\"S3.E2.m1.2.2.1.1.cmml\" xref=\"S3.E2.m1.2.2.1\"><eq id=\"S3.E2.m1.2.2.1.1.2.cmml\" xref=\"S3.E2.m1.2.2.1.1.2\"></eq><apply id=\"S3.E2.m1.2.2.1.1.3.cmml\" xref=\"S3.E2.m1.2.2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S3.E2.m1.2.2.1.1.3.1.cmml\" xref=\"S3.E2.m1.2.2.1.1.3\">subscript</csymbol><ci id=\"S3.E2.m1.2.2.1.1.3.2.cmml\" xref=\"S3.E2.m1.2.2.1.1.3.2\">\ud835\udc39</ci><apply id=\"S3.E2.m1.2.2.1.1.3.3.cmml\" xref=\"S3.E2.m1.2.2.1.1.3.3\"><times id=\"S3.E2.m1.2.2.1.1.3.3.1.cmml\" xref=\"S3.E2.m1.2.2.1.1.3.3.1\"></times><ci id=\"S3.E2.m1.2.2.1.1.3.3.2.cmml\" xref=\"S3.E2.m1.2.2.1.1.3.3.2\">\ud835\udc5d</ci><ci id=\"S3.E2.m1.2.2.1.1.3.3.3.cmml\" xref=\"S3.E2.m1.2.2.1.1.3.3.3\">\ud835\udc56</ci><ci id=\"S3.E2.m1.2.2.1.1.3.3.4.cmml\" xref=\"S3.E2.m1.2.2.1.1.3.3.4\">\ud835\udc4e</ci><ci id=\"S3.E2.m1.2.2.1.1.3.3.5.cmml\" xref=\"S3.E2.m1.2.2.1.1.3.3.5\">\ud835\udc5b</ci><ci id=\"S3.E2.m1.2.2.1.1.3.3.6.cmml\" xref=\"S3.E2.m1.2.2.1.1.3.3.6\">\ud835\udc5c</ci><ci id=\"S3.E2.m1.2.2.1.1.3.3.7.cmml\" xref=\"S3.E2.m1.2.2.1.1.3.3.7\">\ud835\udc5f</ci><ci id=\"S3.E2.m1.2.2.1.1.3.3.8.cmml\" xref=\"S3.E2.m1.2.2.1.1.3.3.8\">\ud835\udc5c</ci><ci id=\"S3.E2.m1.2.2.1.1.3.3.9.cmml\" xref=\"S3.E2.m1.2.2.1.1.3.3.9\">\ud835\udc59</ci><ci id=\"S3.E2.m1.2.2.1.1.3.3.10.cmml\" xref=\"S3.E2.m1.2.2.1.1.3.3.10\">\ud835\udc59</ci></apply></apply><apply id=\"S3.E2.m1.2.2.1.1.1.cmml\" xref=\"S3.E2.m1.2.2.1.1.1\"><times id=\"S3.E2.m1.2.2.1.1.1.2.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.2\"></times><ci id=\"S3.E2.m1.2.2.1.1.1.3.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.3\">\ud835\udc39</ci><ci id=\"S3.E2.m1.2.2.1.1.1.4.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.4\">\ud835\udc52</ci><ci id=\"S3.E2.m1.2.2.1.1.1.5.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.5\">\ud835\udc4e</ci><ci id=\"S3.E2.m1.2.2.1.1.1.6.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.6\">\ud835\udc61</ci><ci id=\"S3.E2.m1.2.2.1.1.1.7.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.7\">\ud835\udc62</ci><ci id=\"S3.E2.m1.2.2.1.1.1.8.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.8\">\ud835\udc5f</ci><ci id=\"S3.E2.m1.2.2.1.1.1.9.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.9\">\ud835\udc52</ci><ci id=\"S3.E2.m1.2.2.1.1.1.10.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.10\">\ud835\udc38</ci><ci id=\"S3.E2.m1.2.2.1.1.1.11.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.11\">\ud835\udc65</ci><ci id=\"S3.E2.m1.2.2.1.1.1.12.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.12\">\ud835\udc61</ci><ci id=\"S3.E2.m1.2.2.1.1.1.13.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.13\">\ud835\udc5f</ci><ci id=\"S3.E2.m1.2.2.1.1.1.14.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.14\">\ud835\udc4e</ci><ci id=\"S3.E2.m1.2.2.1.1.1.15.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.15\">\ud835\udc50</ci><ci id=\"S3.E2.m1.2.2.1.1.1.16.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.16\">\ud835\udc61</ci><ci id=\"S3.E2.m1.2.2.1.1.1.17.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.17\">\ud835\udc56</ci><ci id=\"S3.E2.m1.2.2.1.1.1.18.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.18\">\ud835\udc5c</ci><apply id=\"S3.E2.m1.2.2.1.1.1.19.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19\"><csymbol cd=\"ambiguous\" id=\"S3.E2.m1.2.2.1.1.1.19.1.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19\">subscript</csymbol><ci id=\"S3.E2.m1.2.2.1.1.1.19.2.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19.2\">\ud835\udc5b</ci><apply id=\"S3.E2.m1.2.2.1.1.1.19.3.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19.3\"><times id=\"S3.E2.m1.2.2.1.1.1.19.3.1.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.1\"></times><ci id=\"S3.E2.m1.2.2.1.1.1.19.3.2.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.2\">\ud835\udc5d</ci><ci id=\"S3.E2.m1.2.2.1.1.1.19.3.3.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.3\">\ud835\udc56</ci><ci id=\"S3.E2.m1.2.2.1.1.1.19.3.4.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.4\">\ud835\udc4e</ci><ci id=\"S3.E2.m1.2.2.1.1.1.19.3.5.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.5\">\ud835\udc5b</ci><ci id=\"S3.E2.m1.2.2.1.1.1.19.3.6.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.6\">\ud835\udc5c</ci><ci id=\"S3.E2.m1.2.2.1.1.1.19.3.7.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.7\">\ud835\udc5f</ci><ci id=\"S3.E2.m1.2.2.1.1.1.19.3.8.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.8\">\ud835\udc5c</ci><ci id=\"S3.E2.m1.2.2.1.1.1.19.3.9.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.9\">\ud835\udc59</ci><ci id=\"S3.E2.m1.2.2.1.1.1.19.3.10.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.19.3.10\">\ud835\udc59</ci></apply></apply><apply id=\"S3.E2.m1.2.2.1.1.1.1.1.1.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.1.1\"><times id=\"S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.1\"></times><ci id=\"S3.E2.m1.2.2.1.1.1.1.1.1.2.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.2\">\ud835\udc5d</ci><ci id=\"S3.E2.m1.2.2.1.1.1.1.1.1.3.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.3\">\ud835\udc56</ci><ci id=\"S3.E2.m1.2.2.1.1.1.1.1.1.4.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.4\">\ud835\udc4e</ci><ci id=\"S3.E2.m1.2.2.1.1.1.1.1.1.5.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.5\">\ud835\udc5b</ci><ci id=\"S3.E2.m1.2.2.1.1.1.1.1.1.6.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.6\">\ud835\udc5c</ci><ci id=\"S3.E2.m1.2.2.1.1.1.1.1.1.7.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.7\">\ud835\udc5f</ci><ci id=\"S3.E2.m1.2.2.1.1.1.1.1.1.8.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.8\">\ud835\udc5c</ci><ci id=\"S3.E2.m1.2.2.1.1.1.1.1.1.9.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.9\">\ud835\udc59</ci><ci id=\"S3.E2.m1.2.2.1.1.1.1.1.1.10.cmml\" xref=\"S3.E2.m1.2.2.1.1.1.1.1.1.10\">\ud835\udc59</ci><ci id=\"S3.E2.m1.1.1.cmml\" xref=\"S3.E2.m1.1.1\">\ud835\udc66</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.E2.m1.2c\">F_{pianoroll}=FeatureExtraction_{pianoroll}(pianoroll(y)),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S3.SS3.p2.2\" class=\"ltx_p\">where <math id=\"S3.SS3.p2.1.m1.1\" class=\"ltx_Math\" alttext=\"F_{CQT}\" display=\"inline\"><semantics id=\"S3.SS3.p2.1.m1.1a\"><msub id=\"S3.SS3.p2.1.m1.1.1\" xref=\"S3.SS3.p2.1.m1.1.1.cmml\"><mi id=\"S3.SS3.p2.1.m1.1.1.2\" xref=\"S3.SS3.p2.1.m1.1.1.2.cmml\">F</mi><mrow id=\"S3.SS3.p2.1.m1.1.1.3\" xref=\"S3.SS3.p2.1.m1.1.1.3.cmml\"><mi id=\"S3.SS3.p2.1.m1.1.1.3.2\" xref=\"S3.SS3.p2.1.m1.1.1.3.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p2.1.m1.1.1.3.1\" xref=\"S3.SS3.p2.1.m1.1.1.3.1.cmml\">\u200b</mo><mi id=\"S3.SS3.p2.1.m1.1.1.3.3\" xref=\"S3.SS3.p2.1.m1.1.1.3.3.cmml\">Q</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p2.1.m1.1.1.3.1a\" xref=\"S3.SS3.p2.1.m1.1.1.3.1.cmml\">\u200b</mo><mi id=\"S3.SS3.p2.1.m1.1.1.3.4\" xref=\"S3.SS3.p2.1.m1.1.1.3.4.cmml\">T</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p2.1.m1.1b\"><apply id=\"S3.SS3.p2.1.m1.1.1.cmml\" xref=\"S3.SS3.p2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p2.1.m1.1.1.1.cmml\" xref=\"S3.SS3.p2.1.m1.1.1\">subscript</csymbol><ci id=\"S3.SS3.p2.1.m1.1.1.2.cmml\" xref=\"S3.SS3.p2.1.m1.1.1.2\">\ud835\udc39</ci><apply id=\"S3.SS3.p2.1.m1.1.1.3.cmml\" xref=\"S3.SS3.p2.1.m1.1.1.3\"><times id=\"S3.SS3.p2.1.m1.1.1.3.1.cmml\" xref=\"S3.SS3.p2.1.m1.1.1.3.1\"></times><ci id=\"S3.SS3.p2.1.m1.1.1.3.2.cmml\" xref=\"S3.SS3.p2.1.m1.1.1.3.2\">\ud835\udc36</ci><ci id=\"S3.SS3.p2.1.m1.1.1.3.3.cmml\" xref=\"S3.SS3.p2.1.m1.1.1.3.3\">\ud835\udc44</ci><ci id=\"S3.SS3.p2.1.m1.1.1.3.4.cmml\" xref=\"S3.SS3.p2.1.m1.1.1.3.4\">\ud835\udc47</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p2.1.m1.1c\">F_{CQT}</annotation></semantics></math> and <math id=\"S3.SS3.p2.2.m2.1\" class=\"ltx_Math\" alttext=\"F_{pianoroll}\" display=\"inline\"><semantics id=\"S3.SS3.p2.2.m2.1a\"><msub id=\"S3.SS3.p2.2.m2.1.1\" xref=\"S3.SS3.p2.2.m2.1.1.cmml\"><mi id=\"S3.SS3.p2.2.m2.1.1.2\" xref=\"S3.SS3.p2.2.m2.1.1.2.cmml\">F</mi><mrow id=\"S3.SS3.p2.2.m2.1.1.3\" xref=\"S3.SS3.p2.2.m2.1.1.3.cmml\"><mi id=\"S3.SS3.p2.2.m2.1.1.3.2\" xref=\"S3.SS3.p2.2.m2.1.1.3.2.cmml\">p</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p2.2.m2.1.1.3.1\" xref=\"S3.SS3.p2.2.m2.1.1.3.1.cmml\">\u200b</mo><mi id=\"S3.SS3.p2.2.m2.1.1.3.3\" xref=\"S3.SS3.p2.2.m2.1.1.3.3.cmml\">i</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p2.2.m2.1.1.3.1a\" xref=\"S3.SS3.p2.2.m2.1.1.3.1.cmml\">\u200b</mo><mi id=\"S3.SS3.p2.2.m2.1.1.3.4\" xref=\"S3.SS3.p2.2.m2.1.1.3.4.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p2.2.m2.1.1.3.1b\" xref=\"S3.SS3.p2.2.m2.1.1.3.1.cmml\">\u200b</mo><mi id=\"S3.SS3.p2.2.m2.1.1.3.5\" xref=\"S3.SS3.p2.2.m2.1.1.3.5.cmml\">n</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p2.2.m2.1.1.3.1c\" xref=\"S3.SS3.p2.2.m2.1.1.3.1.cmml\">\u200b</mo><mi id=\"S3.SS3.p2.2.m2.1.1.3.6\" xref=\"S3.SS3.p2.2.m2.1.1.3.6.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p2.2.m2.1.1.3.1d\" xref=\"S3.SS3.p2.2.m2.1.1.3.1.cmml\">\u200b</mo><mi id=\"S3.SS3.p2.2.m2.1.1.3.7\" xref=\"S3.SS3.p2.2.m2.1.1.3.7.cmml\">r</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p2.2.m2.1.1.3.1e\" xref=\"S3.SS3.p2.2.m2.1.1.3.1.cmml\">\u200b</mo><mi id=\"S3.SS3.p2.2.m2.1.1.3.8\" xref=\"S3.SS3.p2.2.m2.1.1.3.8.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p2.2.m2.1.1.3.1f\" xref=\"S3.SS3.p2.2.m2.1.1.3.1.cmml\">\u200b</mo><mi id=\"S3.SS3.p2.2.m2.1.1.3.9\" xref=\"S3.SS3.p2.2.m2.1.1.3.9.cmml\">l</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p2.2.m2.1.1.3.1g\" xref=\"S3.SS3.p2.2.m2.1.1.3.1.cmml\">\u200b</mo><mi id=\"S3.SS3.p2.2.m2.1.1.3.10\" xref=\"S3.SS3.p2.2.m2.1.1.3.10.cmml\">l</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p2.2.m2.1b\"><apply id=\"S3.SS3.p2.2.m2.1.1.cmml\" xref=\"S3.SS3.p2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p2.2.m2.1.1.1.cmml\" xref=\"S3.SS3.p2.2.m2.1.1\">subscript</csymbol><ci id=\"S3.SS3.p2.2.m2.1.1.2.cmml\" xref=\"S3.SS3.p2.2.m2.1.1.2\">\ud835\udc39</ci><apply id=\"S3.SS3.p2.2.m2.1.1.3.cmml\" xref=\"S3.SS3.p2.2.m2.1.1.3\"><times id=\"S3.SS3.p2.2.m2.1.1.3.1.cmml\" xref=\"S3.SS3.p2.2.m2.1.1.3.1\"></times><ci id=\"S3.SS3.p2.2.m2.1.1.3.2.cmml\" xref=\"S3.SS3.p2.2.m2.1.1.3.2\">\ud835\udc5d</ci><ci id=\"S3.SS3.p2.2.m2.1.1.3.3.cmml\" xref=\"S3.SS3.p2.2.m2.1.1.3.3\">\ud835\udc56</ci><ci id=\"S3.SS3.p2.2.m2.1.1.3.4.cmml\" xref=\"S3.SS3.p2.2.m2.1.1.3.4\">\ud835\udc4e</ci><ci id=\"S3.SS3.p2.2.m2.1.1.3.5.cmml\" xref=\"S3.SS3.p2.2.m2.1.1.3.5\">\ud835\udc5b</ci><ci id=\"S3.SS3.p2.2.m2.1.1.3.6.cmml\" xref=\"S3.SS3.p2.2.m2.1.1.3.6\">\ud835\udc5c</ci><ci id=\"S3.SS3.p2.2.m2.1.1.3.7.cmml\" xref=\"S3.SS3.p2.2.m2.1.1.3.7\">\ud835\udc5f</ci><ci id=\"S3.SS3.p2.2.m2.1.1.3.8.cmml\" xref=\"S3.SS3.p2.2.m2.1.1.3.8\">\ud835\udc5c</ci><ci id=\"S3.SS3.p2.2.m2.1.1.3.9.cmml\" xref=\"S3.SS3.p2.2.m2.1.1.3.9\">\ud835\udc59</ci><ci id=\"S3.SS3.p2.2.m2.1.1.3.10.cmml\" xref=\"S3.SS3.p2.2.m2.1.1.3.10\">\ud835\udc59</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p2.2.m2.1c\">F_{pianoroll}</annotation></semantics></math> represent the features extracted from the CQT and piano roll inputs, respectively. These features are then horizontally concatenated to produce a unified feature set:</p>\n<table id=\"S3.E3\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S3.E3.m1.1\" class=\"ltx_Math\" alttext=\"F_{unified}=Concat(F_{CQT},F_{pianoroll}),\" display=\"block\"><semantics id=\"S3.E3.m1.1a\"><mrow id=\"S3.E3.m1.1.1.1\" xref=\"S3.E3.m1.1.1.1.1.cmml\"><mrow id=\"S3.E3.m1.1.1.1.1\" xref=\"S3.E3.m1.1.1.1.1.cmml\"><msub id=\"S3.E3.m1.1.1.1.1.4\" xref=\"S3.E3.m1.1.1.1.1.4.cmml\"><mi id=\"S3.E3.m1.1.1.1.1.4.2\" xref=\"S3.E3.m1.1.1.1.1.4.2.cmml\">F</mi><mrow id=\"S3.E3.m1.1.1.1.1.4.3\" xref=\"S3.E3.m1.1.1.1.1.4.3.cmml\"><mi id=\"S3.E3.m1.1.1.1.1.4.3.2\" xref=\"S3.E3.m1.1.1.1.1.4.3.2.cmml\">u</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.4.3.1\" xref=\"S3.E3.m1.1.1.1.1.4.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.4.3.3\" xref=\"S3.E3.m1.1.1.1.1.4.3.3.cmml\">n</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.4.3.1a\" xref=\"S3.E3.m1.1.1.1.1.4.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.4.3.4\" xref=\"S3.E3.m1.1.1.1.1.4.3.4.cmml\">i</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.4.3.1b\" xref=\"S3.E3.m1.1.1.1.1.4.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.4.3.5\" xref=\"S3.E3.m1.1.1.1.1.4.3.5.cmml\">f</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.4.3.1c\" xref=\"S3.E3.m1.1.1.1.1.4.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.4.3.6\" xref=\"S3.E3.m1.1.1.1.1.4.3.6.cmml\">i</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.4.3.1d\" xref=\"S3.E3.m1.1.1.1.1.4.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.4.3.7\" xref=\"S3.E3.m1.1.1.1.1.4.3.7.cmml\">e</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.4.3.1e\" xref=\"S3.E3.m1.1.1.1.1.4.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.4.3.8\" xref=\"S3.E3.m1.1.1.1.1.4.3.8.cmml\">d</mi></mrow></msub><mo id=\"S3.E3.m1.1.1.1.1.3\" xref=\"S3.E3.m1.1.1.1.1.3.cmml\">=</mo><mrow id=\"S3.E3.m1.1.1.1.1.2\" xref=\"S3.E3.m1.1.1.1.1.2.cmml\"><mi id=\"S3.E3.m1.1.1.1.1.2.4\" xref=\"S3.E3.m1.1.1.1.1.2.4.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.3\" xref=\"S3.E3.m1.1.1.1.1.2.3.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.2.5\" xref=\"S3.E3.m1.1.1.1.1.2.5.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.3a\" xref=\"S3.E3.m1.1.1.1.1.2.3.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.2.6\" xref=\"S3.E3.m1.1.1.1.1.2.6.cmml\">n</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.3b\" xref=\"S3.E3.m1.1.1.1.1.2.3.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.2.7\" xref=\"S3.E3.m1.1.1.1.1.2.7.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.3c\" xref=\"S3.E3.m1.1.1.1.1.2.3.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.2.8\" xref=\"S3.E3.m1.1.1.1.1.2.8.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.3d\" xref=\"S3.E3.m1.1.1.1.1.2.3.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.2.9\" xref=\"S3.E3.m1.1.1.1.1.2.9.cmml\">t</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.3e\" xref=\"S3.E3.m1.1.1.1.1.2.3.cmml\">\u200b</mo><mrow id=\"S3.E3.m1.1.1.1.1.2.2.2\" xref=\"S3.E3.m1.1.1.1.1.2.2.3.cmml\"><mo stretchy=\"false\" id=\"S3.E3.m1.1.1.1.1.2.2.2.3\" xref=\"S3.E3.m1.1.1.1.1.2.2.3.cmml\">(</mo><msub id=\"S3.E3.m1.1.1.1.1.1.1.1.1\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S3.E3.m1.1.1.1.1.1.1.1.1.2\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml\">F</mi><mrow id=\"S3.E3.m1.1.1.1.1.1.1.1.1.3\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml\"><mi id=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.2\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml\">C</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.1\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.3\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml\">Q</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.1a\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.4\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.4.cmml\">T</mi></mrow></msub><mo id=\"S3.E3.m1.1.1.1.1.2.2.2.4\" xref=\"S3.E3.m1.1.1.1.1.2.2.3.cmml\">,</mo><msub id=\"S3.E3.m1.1.1.1.1.2.2.2.2\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.cmml\"><mi id=\"S3.E3.m1.1.1.1.1.2.2.2.2.2\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.2.cmml\">F</mi><mrow id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.cmml\"><mi id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.2\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.2.cmml\">p</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.3\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.3.cmml\">i</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1a\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.4\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.4.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1b\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.5\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.5.cmml\">n</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1c\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.6\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.6.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1d\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.7\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.7.cmml\">r</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1e\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.8\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.8.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1f\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.9\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.9.cmml\">l</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1g\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1.cmml\">\u200b</mo><mi id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.10\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.10.cmml\">l</mi></mrow></msub><mo stretchy=\"false\" id=\"S3.E3.m1.1.1.1.1.2.2.2.5\" xref=\"S3.E3.m1.1.1.1.1.2.2.3.cmml\">)</mo></mrow></mrow></mrow><mo id=\"S3.E3.m1.1.1.1.2\" xref=\"S3.E3.m1.1.1.1.1.cmml\">,</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.E3.m1.1b\"><apply id=\"S3.E3.m1.1.1.1.1.cmml\" xref=\"S3.E3.m1.1.1.1\"><eq id=\"S3.E3.m1.1.1.1.1.3.cmml\" xref=\"S3.E3.m1.1.1.1.1.3\"></eq><apply id=\"S3.E3.m1.1.1.1.1.4.cmml\" xref=\"S3.E3.m1.1.1.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S3.E3.m1.1.1.1.1.4.1.cmml\" xref=\"S3.E3.m1.1.1.1.1.4\">subscript</csymbol><ci id=\"S3.E3.m1.1.1.1.1.4.2.cmml\" xref=\"S3.E3.m1.1.1.1.1.4.2\">\ud835\udc39</ci><apply id=\"S3.E3.m1.1.1.1.1.4.3.cmml\" xref=\"S3.E3.m1.1.1.1.1.4.3\"><times id=\"S3.E3.m1.1.1.1.1.4.3.1.cmml\" xref=\"S3.E3.m1.1.1.1.1.4.3.1\"></times><ci id=\"S3.E3.m1.1.1.1.1.4.3.2.cmml\" xref=\"S3.E3.m1.1.1.1.1.4.3.2\">\ud835\udc62</ci><ci id=\"S3.E3.m1.1.1.1.1.4.3.3.cmml\" xref=\"S3.E3.m1.1.1.1.1.4.3.3\">\ud835\udc5b</ci><ci id=\"S3.E3.m1.1.1.1.1.4.3.4.cmml\" xref=\"S3.E3.m1.1.1.1.1.4.3.4\">\ud835\udc56</ci><ci id=\"S3.E3.m1.1.1.1.1.4.3.5.cmml\" xref=\"S3.E3.m1.1.1.1.1.4.3.5\">\ud835\udc53</ci><ci id=\"S3.E3.m1.1.1.1.1.4.3.6.cmml\" xref=\"S3.E3.m1.1.1.1.1.4.3.6\">\ud835\udc56</ci><ci id=\"S3.E3.m1.1.1.1.1.4.3.7.cmml\" xref=\"S3.E3.m1.1.1.1.1.4.3.7\">\ud835\udc52</ci><ci id=\"S3.E3.m1.1.1.1.1.4.3.8.cmml\" xref=\"S3.E3.m1.1.1.1.1.4.3.8\">\ud835\udc51</ci></apply></apply><apply id=\"S3.E3.m1.1.1.1.1.2.cmml\" xref=\"S3.E3.m1.1.1.1.1.2\"><times id=\"S3.E3.m1.1.1.1.1.2.3.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.3\"></times><ci id=\"S3.E3.m1.1.1.1.1.2.4.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.4\">\ud835\udc36</ci><ci id=\"S3.E3.m1.1.1.1.1.2.5.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.5\">\ud835\udc5c</ci><ci id=\"S3.E3.m1.1.1.1.1.2.6.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.6\">\ud835\udc5b</ci><ci id=\"S3.E3.m1.1.1.1.1.2.7.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.7\">\ud835\udc50</ci><ci id=\"S3.E3.m1.1.1.1.1.2.8.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.8\">\ud835\udc4e</ci><ci id=\"S3.E3.m1.1.1.1.1.2.9.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.9\">\ud835\udc61</ci><interval closure=\"open\" id=\"S3.E3.m1.1.1.1.1.2.2.3.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2\"><apply id=\"S3.E3.m1.1.1.1.1.1.1.1.1.cmml\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.2\">\ud835\udc39</ci><apply id=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.3\"><times id=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.1\"></times><ci id=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.2\">\ud835\udc36</ci><ci id=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.3\">\ud835\udc44</ci><ci id=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.4.cmml\" xref=\"S3.E3.m1.1.1.1.1.1.1.1.1.3.4\">\ud835\udc47</ci></apply></apply><apply id=\"S3.E3.m1.1.1.1.1.2.2.2.2.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.E3.m1.1.1.1.1.2.2.2.2.1.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2\">subscript</csymbol><ci id=\"S3.E3.m1.1.1.1.1.2.2.2.2.2.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.2\">\ud835\udc39</ci><apply id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3\"><times id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.1\"></times><ci id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.2.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.2\">\ud835\udc5d</ci><ci id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.3.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.3\">\ud835\udc56</ci><ci id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.4.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.4\">\ud835\udc4e</ci><ci id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.5.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.5\">\ud835\udc5b</ci><ci id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.6.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.6\">\ud835\udc5c</ci><ci id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.7.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.7\">\ud835\udc5f</ci><ci id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.8.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.8\">\ud835\udc5c</ci><ci id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.9.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.9\">\ud835\udc59</ci><ci id=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.10.cmml\" xref=\"S3.E3.m1.1.1.1.1.2.2.2.2.3.10\">\ud835\udc59</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.E3.m1.1c\">F_{unified}=Concat(F_{CQT},F_{pianoroll}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S3.SS3.p2.4\" class=\"ltx_p\">ensuring that the temporal alignment of features from both data sources is maintained.</p>\n</div>\n<div id=\"S3.SS3.p3\" class=\"ltx_para\">\n<p id=\"S3.SS3.p3.1\" class=\"ltx_p\">The early fusion of CQT and piano roll features offers several advantages\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib41\" title=\"\" class=\"ltx_ref\">41</a>]</cite>. Firstly, it allows the model to leverage both the timbral content provided by the CQT and the detailed rhythmic and melodic information encoded in the piano roll data from the initial stages of the analysis. This comprehensive feature set may facilitate a better understanding of performance, akin to the multi-dimensional analysis conducted by musicians when assessing the difficulty of pieces.</p>\n</div>\n<figure id=\"S3.F11\" class=\"ltx_figure\"><img src=\"/html/2403.03947/assets/x1.png\" id=\"S3.F11.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"230\" height=\"132\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 11: </span>The primary modification in the multimodal approach involves adding two branches: one for CQT and another for piano roll features, to facilitate the early fusion of both representations.</figcaption>\n</figure>\n<div id=\"S3.SS3.p4\" class=\"ltx_para\">\n<p id=\"S3.SS3.p4.1\" class=\"ltx_p\">Furthermore, by integrating these features early in the process, the model can more effectively learn the interdependencies between the harmonic and temporal characteristics of music, which are crucial for accurately predicting musical piece difficulty. This approach enhances the model\u2019s predictive power and ensures that its assessments are robust and closely aligned with human perception.</p>\n</div>\n<div id=\"S3.SS3.p5\" class=\"ltx_para\">\n<p id=\"S3.SS3.p5.1\" class=\"ltx_p\">In summary, the early fusion of CQT and piano roll data through dedicated FeatureExtraction branches not only enriches the feature set available for difficulty classification but also mirrors the holistic way in which musicians evaluate music, thereby promising improvements in both the accuracy and relevance of the predictions.</p>\n</div>\n</section>\n<section id=\"S3.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S3.SS4.5.1.1\" class=\"ltx_text\">III-D</span> </span><span id=\"S3.SS4.6.2\" class=\"ltx_text ltx_font_italic\">Difficulty Loss</span>\n</h3>\n\n<div id=\"S3.SS4.p1\" class=\"ltx_para\">\n<p id=\"S3.SS4.p1.1\" class=\"ltx_p\">The ordinal loss, as proposed by\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">18</a>]</cite>, acknowledges that the predicted labels maintain an ordinal relationship among them. It has been widely used for difficulty performance analysis\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">14</a>]</cite>. This method is founded on ordinal encoding, in contrast to the one-hot encoding utilized in prior approaches. With ordinal encoding, the model is compelled to learn an ordered structure in which the prediction of one class implicitly suggests that all preceding classes, as per the defined order, are predicted concurrently. Thus, if the model predicts a class with a higher encoded integer value, such as difficulty level 3, it inherently implies that classes of difficulty levels 1 and 2 are also predicted. This approach ensures a hierarchical prediction mechanism.</p>\n</div>\n<div id=\"S3.SS4.p2\" class=\"ltx_para\">\n<p id=\"S3.SS4.p2.1\" class=\"ltx_p\">To enforce the ordinal structure of the predictions, we employ the mean squared error (MSE) as follows:</p>\n</div>\n<div id=\"S3.SS4.p3\" class=\"ltx_para\">\n<table id=\"S3.E4\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S3.E4.m1.3\" class=\"ltx_Math\" alttext=\"\\text{OrdinalLoss}(x,y)=\\frac{1}{N}\\sum_{i=1}^{N}(y_{i}-x_{i})^{2}\" display=\"block\"><semantics id=\"S3.E4.m1.3a\"><mrow id=\"S3.E4.m1.3.3\" xref=\"S3.E4.m1.3.3.cmml\"><mrow id=\"S3.E4.m1.3.3.3\" xref=\"S3.E4.m1.3.3.3.cmml\"><mtext id=\"S3.E4.m1.3.3.3.2\" xref=\"S3.E4.m1.3.3.3.2a.cmml\">OrdinalLoss</mtext><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E4.m1.3.3.3.1\" xref=\"S3.E4.m1.3.3.3.1.cmml\">\u200b</mo><mrow id=\"S3.E4.m1.3.3.3.3.2\" xref=\"S3.E4.m1.3.3.3.3.1.cmml\"><mo stretchy=\"false\" id=\"S3.E4.m1.3.3.3.3.2.1\" xref=\"S3.E4.m1.3.3.3.3.1.cmml\">(</mo><mi id=\"S3.E4.m1.1.1\" xref=\"S3.E4.m1.1.1.cmml\">x</mi><mo id=\"S3.E4.m1.3.3.3.3.2.2\" xref=\"S3.E4.m1.3.3.3.3.1.cmml\">,</mo><mi id=\"S3.E4.m1.2.2\" xref=\"S3.E4.m1.2.2.cmml\">y</mi><mo stretchy=\"false\" id=\"S3.E4.m1.3.3.3.3.2.3\" xref=\"S3.E4.m1.3.3.3.3.1.cmml\">)</mo></mrow></mrow><mo id=\"S3.E4.m1.3.3.2\" xref=\"S3.E4.m1.3.3.2.cmml\">=</mo><mrow id=\"S3.E4.m1.3.3.1\" xref=\"S3.E4.m1.3.3.1.cmml\"><mfrac id=\"S3.E4.m1.3.3.1.3\" xref=\"S3.E4.m1.3.3.1.3.cmml\"><mn id=\"S3.E4.m1.3.3.1.3.2\" xref=\"S3.E4.m1.3.3.1.3.2.cmml\">1</mn><mi id=\"S3.E4.m1.3.3.1.3.3\" xref=\"S3.E4.m1.3.3.1.3.3.cmml\">N</mi></mfrac><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.E4.m1.3.3.1.2\" xref=\"S3.E4.m1.3.3.1.2.cmml\">\u200b</mo><mrow id=\"S3.E4.m1.3.3.1.1\" xref=\"S3.E4.m1.3.3.1.1.cmml\"><munderover id=\"S3.E4.m1.3.3.1.1.2\" xref=\"S3.E4.m1.3.3.1.1.2.cmml\"><mo movablelimits=\"false\" rspace=\"0em\" id=\"S3.E4.m1.3.3.1.1.2.2.2\" xref=\"S3.E4.m1.3.3.1.1.2.2.2.cmml\">\u2211</mo><mrow id=\"S3.E4.m1.3.3.1.1.2.2.3\" xref=\"S3.E4.m1.3.3.1.1.2.2.3.cmml\"><mi id=\"S3.E4.m1.3.3.1.1.2.2.3.2\" xref=\"S3.E4.m1.3.3.1.1.2.2.3.2.cmml\">i</mi><mo id=\"S3.E4.m1.3.3.1.1.2.2.3.1\" xref=\"S3.E4.m1.3.3.1.1.2.2.3.1.cmml\">=</mo><mn id=\"S3.E4.m1.3.3.1.1.2.2.3.3\" xref=\"S3.E4.m1.3.3.1.1.2.2.3.3.cmml\">1</mn></mrow><mi id=\"S3.E4.m1.3.3.1.1.2.3\" xref=\"S3.E4.m1.3.3.1.1.2.3.cmml\">N</mi></munderover><msup id=\"S3.E4.m1.3.3.1.1.1\" xref=\"S3.E4.m1.3.3.1.1.1.cmml\"><mrow id=\"S3.E4.m1.3.3.1.1.1.1.1\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S3.E4.m1.3.3.1.1.1.1.1.2\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.cmml\">(</mo><mrow id=\"S3.E4.m1.3.3.1.1.1.1.1.1\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.cmml\"><msub id=\"S3.E4.m1.3.3.1.1.1.1.1.1.2\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.2.cmml\"><mi id=\"S3.E4.m1.3.3.1.1.1.1.1.1.2.2\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.2.2.cmml\">y</mi><mi id=\"S3.E4.m1.3.3.1.1.1.1.1.1.2.3\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.2.3.cmml\">i</mi></msub><mo id=\"S3.E4.m1.3.3.1.1.1.1.1.1.1\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml\">\u2212</mo><msub id=\"S3.E4.m1.3.3.1.1.1.1.1.1.3\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.3.cmml\"><mi id=\"S3.E4.m1.3.3.1.1.1.1.1.1.3.2\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.3.2.cmml\">x</mi><mi id=\"S3.E4.m1.3.3.1.1.1.1.1.1.3.3\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.3.3.cmml\">i</mi></msub></mrow><mo stretchy=\"false\" id=\"S3.E4.m1.3.3.1.1.1.1.1.3\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.cmml\">)</mo></mrow><mn id=\"S3.E4.m1.3.3.1.1.1.3\" xref=\"S3.E4.m1.3.3.1.1.1.3.cmml\">2</mn></msup></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.E4.m1.3b\"><apply id=\"S3.E4.m1.3.3.cmml\" xref=\"S3.E4.m1.3.3\"><eq id=\"S3.E4.m1.3.3.2.cmml\" xref=\"S3.E4.m1.3.3.2\"></eq><apply id=\"S3.E4.m1.3.3.3.cmml\" xref=\"S3.E4.m1.3.3.3\"><times id=\"S3.E4.m1.3.3.3.1.cmml\" xref=\"S3.E4.m1.3.3.3.1\"></times><ci id=\"S3.E4.m1.3.3.3.2a.cmml\" xref=\"S3.E4.m1.3.3.3.2\"><mtext id=\"S3.E4.m1.3.3.3.2.cmml\" xref=\"S3.E4.m1.3.3.3.2\">OrdinalLoss</mtext></ci><interval closure=\"open\" id=\"S3.E4.m1.3.3.3.3.1.cmml\" xref=\"S3.E4.m1.3.3.3.3.2\"><ci id=\"S3.E4.m1.1.1.cmml\" xref=\"S3.E4.m1.1.1\">\ud835\udc65</ci><ci id=\"S3.E4.m1.2.2.cmml\" xref=\"S3.E4.m1.2.2\">\ud835\udc66</ci></interval></apply><apply id=\"S3.E4.m1.3.3.1.cmml\" xref=\"S3.E4.m1.3.3.1\"><times id=\"S3.E4.m1.3.3.1.2.cmml\" xref=\"S3.E4.m1.3.3.1.2\"></times><apply id=\"S3.E4.m1.3.3.1.3.cmml\" xref=\"S3.E4.m1.3.3.1.3\"><divide id=\"S3.E4.m1.3.3.1.3.1.cmml\" xref=\"S3.E4.m1.3.3.1.3\"></divide><cn type=\"integer\" id=\"S3.E4.m1.3.3.1.3.2.cmml\" xref=\"S3.E4.m1.3.3.1.3.2\">1</cn><ci id=\"S3.E4.m1.3.3.1.3.3.cmml\" xref=\"S3.E4.m1.3.3.1.3.3\">\ud835\udc41</ci></apply><apply id=\"S3.E4.m1.3.3.1.1.cmml\" xref=\"S3.E4.m1.3.3.1.1\"><apply id=\"S3.E4.m1.3.3.1.1.2.cmml\" xref=\"S3.E4.m1.3.3.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.E4.m1.3.3.1.1.2.1.cmml\" xref=\"S3.E4.m1.3.3.1.1.2\">superscript</csymbol><apply id=\"S3.E4.m1.3.3.1.1.2.2.cmml\" xref=\"S3.E4.m1.3.3.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.E4.m1.3.3.1.1.2.2.1.cmml\" xref=\"S3.E4.m1.3.3.1.1.2\">subscript</csymbol><sum id=\"S3.E4.m1.3.3.1.1.2.2.2.cmml\" xref=\"S3.E4.m1.3.3.1.1.2.2.2\"></sum><apply id=\"S3.E4.m1.3.3.1.1.2.2.3.cmml\" xref=\"S3.E4.m1.3.3.1.1.2.2.3\"><eq id=\"S3.E4.m1.3.3.1.1.2.2.3.1.cmml\" xref=\"S3.E4.m1.3.3.1.1.2.2.3.1\"></eq><ci id=\"S3.E4.m1.3.3.1.1.2.2.3.2.cmml\" xref=\"S3.E4.m1.3.3.1.1.2.2.3.2\">\ud835\udc56</ci><cn type=\"integer\" id=\"S3.E4.m1.3.3.1.1.2.2.3.3.cmml\" xref=\"S3.E4.m1.3.3.1.1.2.2.3.3\">1</cn></apply></apply><ci id=\"S3.E4.m1.3.3.1.1.2.3.cmml\" xref=\"S3.E4.m1.3.3.1.1.2.3\">\ud835\udc41</ci></apply><apply id=\"S3.E4.m1.3.3.1.1.1.cmml\" xref=\"S3.E4.m1.3.3.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.E4.m1.3.3.1.1.1.2.cmml\" xref=\"S3.E4.m1.3.3.1.1.1\">superscript</csymbol><apply id=\"S3.E4.m1.3.3.1.1.1.1.1.1.cmml\" xref=\"S3.E4.m1.3.3.1.1.1.1.1\"><minus id=\"S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.1\"></minus><apply id=\"S3.E4.m1.3.3.1.1.1.1.1.1.2.cmml\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S3.E4.m1.3.3.1.1.1.1.1.1.2.1.cmml\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.2\">subscript</csymbol><ci id=\"S3.E4.m1.3.3.1.1.1.1.1.1.2.2.cmml\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.2.2\">\ud835\udc66</ci><ci id=\"S3.E4.m1.3.3.1.1.1.1.1.1.2.3.cmml\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.2.3\">\ud835\udc56</ci></apply><apply id=\"S3.E4.m1.3.3.1.1.1.1.1.1.3.cmml\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S3.E4.m1.3.3.1.1.1.1.1.1.3.1.cmml\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.3\">subscript</csymbol><ci id=\"S3.E4.m1.3.3.1.1.1.1.1.1.3.2.cmml\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.3.2\">\ud835\udc65</ci><ci id=\"S3.E4.m1.3.3.1.1.1.1.1.1.3.3.cmml\" xref=\"S3.E4.m1.3.3.1.1.1.1.1.1.3.3\">\ud835\udc56</ci></apply></apply><cn type=\"integer\" id=\"S3.E4.m1.3.3.1.1.1.3.cmml\" xref=\"S3.E4.m1.3.3.1.1.1.3\">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.E4.m1.3c\">\\text{OrdinalLoss}(x,y)=\\frac{1}{N}\\sum_{i=1}^{N}(y_{i}-x_{i})^{2}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(4)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"S3.SS4.p4\" class=\"ltx_para\">\n<p id=\"S3.SS4.p4.6\" class=\"ltx_p\">Here, <math id=\"S3.SS4.p4.1.m1.1\" class=\"ltx_Math\" alttext=\"N\" display=\"inline\"><semantics id=\"S3.SS4.p4.1.m1.1a\"><mi id=\"S3.SS4.p4.1.m1.1.1\" xref=\"S3.SS4.p4.1.m1.1.1.cmml\">N</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p4.1.m1.1b\"><ci id=\"S3.SS4.p4.1.m1.1.1.cmml\" xref=\"S3.SS4.p4.1.m1.1.1\">\ud835\udc41</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p4.1.m1.1c\">N</annotation></semantics></math> represents the number of samples in the dataset, <math id=\"S3.SS4.p4.2.m2.1\" class=\"ltx_Math\" alttext=\"y_{i}\" display=\"inline\"><semantics id=\"S3.SS4.p4.2.m2.1a\"><msub id=\"S3.SS4.p4.2.m2.1.1\" xref=\"S3.SS4.p4.2.m2.1.1.cmml\"><mi id=\"S3.SS4.p4.2.m2.1.1.2\" xref=\"S3.SS4.p4.2.m2.1.1.2.cmml\">y</mi><mi id=\"S3.SS4.p4.2.m2.1.1.3\" xref=\"S3.SS4.p4.2.m2.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p4.2.m2.1b\"><apply id=\"S3.SS4.p4.2.m2.1.1.cmml\" xref=\"S3.SS4.p4.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p4.2.m2.1.1.1.cmml\" xref=\"S3.SS4.p4.2.m2.1.1\">subscript</csymbol><ci id=\"S3.SS4.p4.2.m2.1.1.2.cmml\" xref=\"S3.SS4.p4.2.m2.1.1.2\">\ud835\udc66</ci><ci id=\"S3.SS4.p4.2.m2.1.1.3.cmml\" xref=\"S3.SS4.p4.2.m2.1.1.3\">\ud835\udc56</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p4.2.m2.1c\">y_{i}</annotation></semantics></math> is the ground-truth value of the sample (ordinal encoded), and <math id=\"S3.SS4.p4.3.m3.1\" class=\"ltx_Math\" alttext=\"x_{i}\" display=\"inline\"><semantics id=\"S3.SS4.p4.3.m3.1a\"><msub id=\"S3.SS4.p4.3.m3.1.1\" xref=\"S3.SS4.p4.3.m3.1.1.cmml\"><mi id=\"S3.SS4.p4.3.m3.1.1.2\" xref=\"S3.SS4.p4.3.m3.1.1.2.cmml\">x</mi><mi id=\"S3.SS4.p4.3.m3.1.1.3\" xref=\"S3.SS4.p4.3.m3.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p4.3.m3.1b\"><apply id=\"S3.SS4.p4.3.m3.1.1.cmml\" xref=\"S3.SS4.p4.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p4.3.m3.1.1.1.cmml\" xref=\"S3.SS4.p4.3.m3.1.1\">subscript</csymbol><ci id=\"S3.SS4.p4.3.m3.1.1.2.cmml\" xref=\"S3.SS4.p4.3.m3.1.1.2\">\ud835\udc65</ci><ci id=\"S3.SS4.p4.3.m3.1.1.3.cmml\" xref=\"S3.SS4.p4.3.m3.1.1.3\">\ud835\udc56</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p4.3.m3.1c\">x_{i}</annotation></semantics></math> is the predicted value of the sample. Both <math id=\"S3.SS4.p4.4.m4.2\" class=\"ltx_Math\" alttext=\"y_{i}\\in\\{0,1\\}^{C}\" display=\"inline\"><semantics id=\"S3.SS4.p4.4.m4.2a\"><mrow id=\"S3.SS4.p4.4.m4.2.3\" xref=\"S3.SS4.p4.4.m4.2.3.cmml\"><msub id=\"S3.SS4.p4.4.m4.2.3.2\" xref=\"S3.SS4.p4.4.m4.2.3.2.cmml\"><mi id=\"S3.SS4.p4.4.m4.2.3.2.2\" xref=\"S3.SS4.p4.4.m4.2.3.2.2.cmml\">y</mi><mi id=\"S3.SS4.p4.4.m4.2.3.2.3\" xref=\"S3.SS4.p4.4.m4.2.3.2.3.cmml\">i</mi></msub><mo id=\"S3.SS4.p4.4.m4.2.3.1\" xref=\"S3.SS4.p4.4.m4.2.3.1.cmml\">\u2208</mo><msup id=\"S3.SS4.p4.4.m4.2.3.3\" xref=\"S3.SS4.p4.4.m4.2.3.3.cmml\"><mrow id=\"S3.SS4.p4.4.m4.2.3.3.2.2\" xref=\"S3.SS4.p4.4.m4.2.3.3.2.1.cmml\"><mo stretchy=\"false\" id=\"S3.SS4.p4.4.m4.2.3.3.2.2.1\" xref=\"S3.SS4.p4.4.m4.2.3.3.2.1.cmml\">{</mo><mn id=\"S3.SS4.p4.4.m4.1.1\" xref=\"S3.SS4.p4.4.m4.1.1.cmml\">0</mn><mo id=\"S3.SS4.p4.4.m4.2.3.3.2.2.2\" xref=\"S3.SS4.p4.4.m4.2.3.3.2.1.cmml\">,</mo><mn id=\"S3.SS4.p4.4.m4.2.2\" xref=\"S3.SS4.p4.4.m4.2.2.cmml\">1</mn><mo stretchy=\"false\" id=\"S3.SS4.p4.4.m4.2.3.3.2.2.3\" xref=\"S3.SS4.p4.4.m4.2.3.3.2.1.cmml\">}</mo></mrow><mi id=\"S3.SS4.p4.4.m4.2.3.3.3\" xref=\"S3.SS4.p4.4.m4.2.3.3.3.cmml\">C</mi></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p4.4.m4.2b\"><apply id=\"S3.SS4.p4.4.m4.2.3.cmml\" xref=\"S3.SS4.p4.4.m4.2.3\"><in id=\"S3.SS4.p4.4.m4.2.3.1.cmml\" xref=\"S3.SS4.p4.4.m4.2.3.1\"></in><apply id=\"S3.SS4.p4.4.m4.2.3.2.cmml\" xref=\"S3.SS4.p4.4.m4.2.3.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p4.4.m4.2.3.2.1.cmml\" xref=\"S3.SS4.p4.4.m4.2.3.2\">subscript</csymbol><ci id=\"S3.SS4.p4.4.m4.2.3.2.2.cmml\" xref=\"S3.SS4.p4.4.m4.2.3.2.2\">\ud835\udc66</ci><ci id=\"S3.SS4.p4.4.m4.2.3.2.3.cmml\" xref=\"S3.SS4.p4.4.m4.2.3.2.3\">\ud835\udc56</ci></apply><apply id=\"S3.SS4.p4.4.m4.2.3.3.cmml\" xref=\"S3.SS4.p4.4.m4.2.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p4.4.m4.2.3.3.1.cmml\" xref=\"S3.SS4.p4.4.m4.2.3.3\">superscript</csymbol><set id=\"S3.SS4.p4.4.m4.2.3.3.2.1.cmml\" xref=\"S3.SS4.p4.4.m4.2.3.3.2.2\"><cn type=\"integer\" id=\"S3.SS4.p4.4.m4.1.1.cmml\" xref=\"S3.SS4.p4.4.m4.1.1\">0</cn><cn type=\"integer\" id=\"S3.SS4.p4.4.m4.2.2.cmml\" xref=\"S3.SS4.p4.4.m4.2.2\">1</cn></set><ci id=\"S3.SS4.p4.4.m4.2.3.3.3.cmml\" xref=\"S3.SS4.p4.4.m4.2.3.3.3\">\ud835\udc36</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p4.4.m4.2c\">y_{i}\\in\\{0,1\\}^{C}</annotation></semantics></math> and <math id=\"S3.SS4.p4.5.m5.2\" class=\"ltx_Math\" alttext=\"x_{i}\\in\\{0,1\\}^{C}\" display=\"inline\"><semantics id=\"S3.SS4.p4.5.m5.2a\"><mrow id=\"S3.SS4.p4.5.m5.2.3\" xref=\"S3.SS4.p4.5.m5.2.3.cmml\"><msub id=\"S3.SS4.p4.5.m5.2.3.2\" xref=\"S3.SS4.p4.5.m5.2.3.2.cmml\"><mi id=\"S3.SS4.p4.5.m5.2.3.2.2\" xref=\"S3.SS4.p4.5.m5.2.3.2.2.cmml\">x</mi><mi id=\"S3.SS4.p4.5.m5.2.3.2.3\" xref=\"S3.SS4.p4.5.m5.2.3.2.3.cmml\">i</mi></msub><mo id=\"S3.SS4.p4.5.m5.2.3.1\" xref=\"S3.SS4.p4.5.m5.2.3.1.cmml\">\u2208</mo><msup id=\"S3.SS4.p4.5.m5.2.3.3\" xref=\"S3.SS4.p4.5.m5.2.3.3.cmml\"><mrow id=\"S3.SS4.p4.5.m5.2.3.3.2.2\" xref=\"S3.SS4.p4.5.m5.2.3.3.2.1.cmml\"><mo stretchy=\"false\" id=\"S3.SS4.p4.5.m5.2.3.3.2.2.1\" xref=\"S3.SS4.p4.5.m5.2.3.3.2.1.cmml\">{</mo><mn id=\"S3.SS4.p4.5.m5.1.1\" xref=\"S3.SS4.p4.5.m5.1.1.cmml\">0</mn><mo id=\"S3.SS4.p4.5.m5.2.3.3.2.2.2\" xref=\"S3.SS4.p4.5.m5.2.3.3.2.1.cmml\">,</mo><mn id=\"S3.SS4.p4.5.m5.2.2\" xref=\"S3.SS4.p4.5.m5.2.2.cmml\">1</mn><mo stretchy=\"false\" id=\"S3.SS4.p4.5.m5.2.3.3.2.2.3\" xref=\"S3.SS4.p4.5.m5.2.3.3.2.1.cmml\">}</mo></mrow><mi id=\"S3.SS4.p4.5.m5.2.3.3.3\" xref=\"S3.SS4.p4.5.m5.2.3.3.3.cmml\">C</mi></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p4.5.m5.2b\"><apply id=\"S3.SS4.p4.5.m5.2.3.cmml\" xref=\"S3.SS4.p4.5.m5.2.3\"><in id=\"S3.SS4.p4.5.m5.2.3.1.cmml\" xref=\"S3.SS4.p4.5.m5.2.3.1\"></in><apply id=\"S3.SS4.p4.5.m5.2.3.2.cmml\" xref=\"S3.SS4.p4.5.m5.2.3.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p4.5.m5.2.3.2.1.cmml\" xref=\"S3.SS4.p4.5.m5.2.3.2\">subscript</csymbol><ci id=\"S3.SS4.p4.5.m5.2.3.2.2.cmml\" xref=\"S3.SS4.p4.5.m5.2.3.2.2\">\ud835\udc65</ci><ci id=\"S3.SS4.p4.5.m5.2.3.2.3.cmml\" xref=\"S3.SS4.p4.5.m5.2.3.2.3\">\ud835\udc56</ci></apply><apply id=\"S3.SS4.p4.5.m5.2.3.3.cmml\" xref=\"S3.SS4.p4.5.m5.2.3.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS4.p4.5.m5.2.3.3.1.cmml\" xref=\"S3.SS4.p4.5.m5.2.3.3\">superscript</csymbol><set id=\"S3.SS4.p4.5.m5.2.3.3.2.1.cmml\" xref=\"S3.SS4.p4.5.m5.2.3.3.2.2\"><cn type=\"integer\" id=\"S3.SS4.p4.5.m5.1.1.cmml\" xref=\"S3.SS4.p4.5.m5.1.1\">0</cn><cn type=\"integer\" id=\"S3.SS4.p4.5.m5.2.2.cmml\" xref=\"S3.SS4.p4.5.m5.2.2\">1</cn></set><ci id=\"S3.SS4.p4.5.m5.2.3.3.3.cmml\" xref=\"S3.SS4.p4.5.m5.2.3.3.3\">\ud835\udc36</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p4.5.m5.2c\">x_{i}\\in\\{0,1\\}^{C}</annotation></semantics></math> are ordinally encoded multi hot vectors, where <math id=\"S3.SS4.p4.6.m6.1\" class=\"ltx_Math\" alttext=\"C\" display=\"inline\"><semantics id=\"S3.SS4.p4.6.m6.1a\"><mi id=\"S3.SS4.p4.6.m6.1.1\" xref=\"S3.SS4.p4.6.m6.1.1.cmml\">C</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p4.6.m6.1b\"><ci id=\"S3.SS4.p4.6.m6.1.1.cmml\" xref=\"S3.SS4.p4.6.m6.1.1\">\ud835\udc36</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p4.6.m6.1c\">C</annotation></semantics></math> denotes the number of classes.</p>\n</div>\n<div id=\"S3.SS4.p5\" class=\"ltx_para\">\n<p id=\"S3.SS4.p5.1\" class=\"ltx_p\">Finally, it is important to note that during inference, a class is identified if it reaches a specific threshold, in this case, <math id=\"S3.SS4.p5.1.m1.1\" class=\"ltx_Math\" alttext=\"0.5\" display=\"inline\"><semantics id=\"S3.SS4.p5.1.m1.1a\"><mn id=\"S3.SS4.p5.1.m1.1.1\" xref=\"S3.SS4.p5.1.m1.1.1.cmml\">0.5</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p5.1.m1.1b\"><cn type=\"float\" id=\"S3.SS4.p5.1.m1.1.1.cmml\" xref=\"S3.SS4.p5.1.m1.1.1\">0.5</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p5.1.m1.1c\">0.5</annotation></semantics></math>.</p>\n</div>\n<section id=\"S4\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">IV </span><span id=\"S4.1.1\" class=\"ltx_text ltx_font_smallcaps\">Experimental setup</span>\n</h2>\n\n<section id=\"S4.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S4.SS1.5.1.1\" class=\"ltx_text\">IV-A</span> </span><span id=\"S4.SS1.6.2\" class=\"ltx_text ltx_font_italic\">Data collections and assessment metrics</span>\n</h3>\n\n<div id=\"S4.SS1.p1\" class=\"ltx_para\">\n<p id=\"S4.SS1.p1.1\" class=\"ltx_p\">To evaluate the effectiveness of our proposed method, we utilized the PSyllabus dataset outlined in Section\u00a0<a href=\"#S2\" title=\"II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>. We used mainly the PSyllabus ranking labels but also era, composer, and other ranking labels. We also collected a benchmark dataset: the audio version of HV.</p>\n</div>\n<div id=\"S4.SS1.p2\" class=\"ltx_para\">\n<p id=\"S4.SS1.p2.1\" class=\"ltx_p\">For validation, we implemented a 5-fold cross-validation approach. Specifically, we allocated 60% of each dataset for training. The remaining data was equally divided into two parts for validation and testing purposes. It is important to note that the benchmark datasets were solely used for testing and, hence were not divided further.</p>\n</div>\n<div id=\"S4.SS1.p3\" class=\"ltx_para\">\n<p id=\"S4.SS1.p3.7\" class=\"ltx_p\">For evaluating the model\u2019s performance, we utilized two commonly employed metrics in ordinal classification\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">42</a>]</cite>: <span id=\"S4.SS1.p3.1.1\" class=\"ltx_text ltx_font_italic\">accuracy within <math id=\"S4.SS1.p3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"n\" display=\"inline\"><semantics id=\"S4.SS1.p3.1.1.m1.1a\"><mi id=\"S4.SS1.p3.1.1.m1.1.1\" xref=\"S4.SS1.p3.1.1.m1.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p3.1.1.m1.1b\"><ci id=\"S4.SS1.p3.1.1.m1.1.1.cmml\" xref=\"S4.SS1.p3.1.1.m1.1.1\">\ud835\udc5b</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p3.1.1.m1.1c\">n</annotation></semantics></math></span> (<math id=\"S4.SS1.p3.2.m1.1\" class=\"ltx_Math\" alttext=\"\\mbox{Acc}_{n}\" display=\"inline\"><semantics id=\"S4.SS1.p3.2.m1.1a\"><msub id=\"S4.SS1.p3.2.m1.1.1\" xref=\"S4.SS1.p3.2.m1.1.1.cmml\"><mtext id=\"S4.SS1.p3.2.m1.1.1.2\" xref=\"S4.SS1.p3.2.m1.1.1.2a.cmml\">Acc</mtext><mi id=\"S4.SS1.p3.2.m1.1.1.3\" xref=\"S4.SS1.p3.2.m1.1.1.3.cmml\">n</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p3.2.m1.1b\"><apply id=\"S4.SS1.p3.2.m1.1.1.cmml\" xref=\"S4.SS1.p3.2.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p3.2.m1.1.1.1.cmml\" xref=\"S4.SS1.p3.2.m1.1.1\">subscript</csymbol><ci id=\"S4.SS1.p3.2.m1.1.1.2a.cmml\" xref=\"S4.SS1.p3.2.m1.1.1.2\"><mtext id=\"S4.SS1.p3.2.m1.1.1.2.cmml\" xref=\"S4.SS1.p3.2.m1.1.1.2\">Acc</mtext></ci><ci id=\"S4.SS1.p3.2.m1.1.1.3.cmml\" xref=\"S4.SS1.p3.2.m1.1.1.3\">\ud835\udc5b</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p3.2.m1.1c\">\\mbox{Acc}_{n}</annotation></semantics></math>) and <span id=\"S4.SS1.p3.7.2\" class=\"ltx_text ltx_font_italic\">mean squared error</span> (MSE). Let\u2019s define <math id=\"S4.SS1.p3.3.m2.1\" class=\"ltx_Math\" alttext=\"\\mathcal{S}\\subset\\mathcal{X}\\times\\mathcal{C}\" display=\"inline\"><semantics id=\"S4.SS1.p3.3.m2.1a\"><mrow id=\"S4.SS1.p3.3.m2.1.1\" xref=\"S4.SS1.p3.3.m2.1.1.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.SS1.p3.3.m2.1.1.2\" xref=\"S4.SS1.p3.3.m2.1.1.2.cmml\">\ud835\udcae</mi><mo id=\"S4.SS1.p3.3.m2.1.1.1\" xref=\"S4.SS1.p3.3.m2.1.1.1.cmml\">\u2282</mo><mrow id=\"S4.SS1.p3.3.m2.1.1.3\" xref=\"S4.SS1.p3.3.m2.1.1.3.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.SS1.p3.3.m2.1.1.3.2\" xref=\"S4.SS1.p3.3.m2.1.1.3.2.cmml\">\ud835\udcb3</mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S4.SS1.p3.3.m2.1.1.3.1\" xref=\"S4.SS1.p3.3.m2.1.1.3.1.cmml\">\u00d7</mo><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.SS1.p3.3.m2.1.1.3.3\" xref=\"S4.SS1.p3.3.m2.1.1.3.3.cmml\">\ud835\udc9e</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p3.3.m2.1b\"><apply id=\"S4.SS1.p3.3.m2.1.1.cmml\" xref=\"S4.SS1.p3.3.m2.1.1\"><subset id=\"S4.SS1.p3.3.m2.1.1.1.cmml\" xref=\"S4.SS1.p3.3.m2.1.1.1\"></subset><ci id=\"S4.SS1.p3.3.m2.1.1.2.cmml\" xref=\"S4.SS1.p3.3.m2.1.1.2\">\ud835\udcae</ci><apply id=\"S4.SS1.p3.3.m2.1.1.3.cmml\" xref=\"S4.SS1.p3.3.m2.1.1.3\"><times id=\"S4.SS1.p3.3.m2.1.1.3.1.cmml\" xref=\"S4.SS1.p3.3.m2.1.1.3.1\"></times><ci id=\"S4.SS1.p3.3.m2.1.1.3.2.cmml\" xref=\"S4.SS1.p3.3.m2.1.1.3.2\">\ud835\udcb3</ci><ci id=\"S4.SS1.p3.3.m2.1.1.3.3.cmml\" xref=\"S4.SS1.p3.3.m2.1.1.3.3\">\ud835\udc9e</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p3.3.m2.1c\">\\mathcal{S}\\subset\\mathcal{X}\\times\\mathcal{C}</annotation></semantics></math> as a set of test data, where <math id=\"S4.SS1.p3.4.m3.2\" class=\"ltx_Math\" alttext=\"\\mathcal{S}_{c}=\\{(x_{i},y_{i})\\in\\mathcal{S}:y_{i}=c\\}\" display=\"inline\"><semantics id=\"S4.SS1.p3.4.m3.2a\"><mrow id=\"S4.SS1.p3.4.m3.2.2\" xref=\"S4.SS1.p3.4.m3.2.2.cmml\"><msub id=\"S4.SS1.p3.4.m3.2.2.4\" xref=\"S4.SS1.p3.4.m3.2.2.4.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.SS1.p3.4.m3.2.2.4.2\" xref=\"S4.SS1.p3.4.m3.2.2.4.2.cmml\">\ud835\udcae</mi><mi id=\"S4.SS1.p3.4.m3.2.2.4.3\" xref=\"S4.SS1.p3.4.m3.2.2.4.3.cmml\">c</mi></msub><mo id=\"S4.SS1.p3.4.m3.2.2.3\" xref=\"S4.SS1.p3.4.m3.2.2.3.cmml\">=</mo><mrow id=\"S4.SS1.p3.4.m3.2.2.2.2\" xref=\"S4.SS1.p3.4.m3.2.2.2.3.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.p3.4.m3.2.2.2.2.3\" xref=\"S4.SS1.p3.4.m3.2.2.2.3.1.cmml\">{</mo><mrow id=\"S4.SS1.p3.4.m3.1.1.1.1.1\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.cmml\"><mrow id=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.3.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.3\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.3.cmml\">(</mo><msub id=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1.cmml\"><mi id=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1.2\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1.2.cmml\">x</mi><mi id=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1.3\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1.3.cmml\">i</mi></msub><mo id=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.4\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.3.cmml\">,</mo><msub id=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2.cmml\"><mi id=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2.2\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2.2.cmml\">y</mi><mi id=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2.3\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2.3.cmml\">i</mi></msub><mo stretchy=\"false\" id=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.5\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.3.cmml\">)</mo></mrow><mo id=\"S4.SS1.p3.4.m3.1.1.1.1.1.3\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.3.cmml\">\u2208</mo><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.SS1.p3.4.m3.1.1.1.1.1.4\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.4.cmml\">\ud835\udcae</mi></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\" id=\"S4.SS1.p3.4.m3.2.2.2.2.4\" xref=\"S4.SS1.p3.4.m3.2.2.2.3.1.cmml\">:</mo><mrow id=\"S4.SS1.p3.4.m3.2.2.2.2.2\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.2.cmml\"><msub id=\"S4.SS1.p3.4.m3.2.2.2.2.2.2\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.2.2.cmml\"><mi id=\"S4.SS1.p3.4.m3.2.2.2.2.2.2.2\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.2.2.2.cmml\">y</mi><mi id=\"S4.SS1.p3.4.m3.2.2.2.2.2.2.3\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.2.2.3.cmml\">i</mi></msub><mo id=\"S4.SS1.p3.4.m3.2.2.2.2.2.1\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.2.1.cmml\">=</mo><mi id=\"S4.SS1.p3.4.m3.2.2.2.2.2.3\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.2.3.cmml\">c</mi></mrow><mo stretchy=\"false\" id=\"S4.SS1.p3.4.m3.2.2.2.2.5\" xref=\"S4.SS1.p3.4.m3.2.2.2.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p3.4.m3.2b\"><apply id=\"S4.SS1.p3.4.m3.2.2.cmml\" xref=\"S4.SS1.p3.4.m3.2.2\"><eq id=\"S4.SS1.p3.4.m3.2.2.3.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.3\"></eq><apply id=\"S4.SS1.p3.4.m3.2.2.4.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.4\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p3.4.m3.2.2.4.1.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.4\">subscript</csymbol><ci id=\"S4.SS1.p3.4.m3.2.2.4.2.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.4.2\">\ud835\udcae</ci><ci id=\"S4.SS1.p3.4.m3.2.2.4.3.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.4.3\">\ud835\udc50</ci></apply><apply id=\"S4.SS1.p3.4.m3.2.2.2.3.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.2.2\"><csymbol cd=\"latexml\" id=\"S4.SS1.p3.4.m3.2.2.2.3.1.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.3\">conditional-set</csymbol><apply id=\"S4.SS1.p3.4.m3.1.1.1.1.1.cmml\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1\"><in id=\"S4.SS1.p3.4.m3.1.1.1.1.1.3.cmml\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.3\"></in><interval closure=\"open\" id=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.3.cmml\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2\"><apply id=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1.1.cmml\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1.2.cmml\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1.2\">\ud835\udc65</ci><ci id=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1.3.cmml\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.1.1.1.3\">\ud835\udc56</ci></apply><apply id=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2.cmml\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2.1.cmml\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2\">subscript</csymbol><ci id=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2.2.cmml\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2.2\">\ud835\udc66</ci><ci id=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2.3.cmml\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.2.2.2.3\">\ud835\udc56</ci></apply></interval><ci id=\"S4.SS1.p3.4.m3.1.1.1.1.1.4.cmml\" xref=\"S4.SS1.p3.4.m3.1.1.1.1.1.4\">\ud835\udcae</ci></apply><apply id=\"S4.SS1.p3.4.m3.2.2.2.2.2.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.2\"><eq id=\"S4.SS1.p3.4.m3.2.2.2.2.2.1.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.2.1\"></eq><apply id=\"S4.SS1.p3.4.m3.2.2.2.2.2.2.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p3.4.m3.2.2.2.2.2.2.1.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.2.2\">subscript</csymbol><ci id=\"S4.SS1.p3.4.m3.2.2.2.2.2.2.2.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.2.2.2\">\ud835\udc66</ci><ci id=\"S4.SS1.p3.4.m3.2.2.2.2.2.2.3.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.2.2.3\">\ud835\udc56</ci></apply><ci id=\"S4.SS1.p3.4.m3.2.2.2.2.2.3.cmml\" xref=\"S4.SS1.p3.4.m3.2.2.2.2.2.3\">\ud835\udc50</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p3.4.m3.2c\">\\mathcal{S}_{c}=\\{(x_{i},y_{i})\\in\\mathcal{S}:y_{i}=c\\}</annotation></semantics></math> for <math id=\"S4.SS1.p3.5.m4.1\" class=\"ltx_Math\" alttext=\"1\\leq i\\leq|\\mathcal{S}|\" display=\"inline\"><semantics id=\"S4.SS1.p3.5.m4.1a\"><mrow id=\"S4.SS1.p3.5.m4.1.2\" xref=\"S4.SS1.p3.5.m4.1.2.cmml\"><mn id=\"S4.SS1.p3.5.m4.1.2.2\" xref=\"S4.SS1.p3.5.m4.1.2.2.cmml\">1</mn><mo id=\"S4.SS1.p3.5.m4.1.2.3\" xref=\"S4.SS1.p3.5.m4.1.2.3.cmml\">\u2264</mo><mi id=\"S4.SS1.p3.5.m4.1.2.4\" xref=\"S4.SS1.p3.5.m4.1.2.4.cmml\">i</mi><mo id=\"S4.SS1.p3.5.m4.1.2.5\" xref=\"S4.SS1.p3.5.m4.1.2.5.cmml\">\u2264</mo><mrow id=\"S4.SS1.p3.5.m4.1.2.6.2\" xref=\"S4.SS1.p3.5.m4.1.2.6.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.p3.5.m4.1.2.6.2.1\" xref=\"S4.SS1.p3.5.m4.1.2.6.1.1.cmml\">|</mo><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.SS1.p3.5.m4.1.1\" xref=\"S4.SS1.p3.5.m4.1.1.cmml\">\ud835\udcae</mi><mo stretchy=\"false\" id=\"S4.SS1.p3.5.m4.1.2.6.2.2\" xref=\"S4.SS1.p3.5.m4.1.2.6.1.1.cmml\">|</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p3.5.m4.1b\"><apply id=\"S4.SS1.p3.5.m4.1.2.cmml\" xref=\"S4.SS1.p3.5.m4.1.2\"><and id=\"S4.SS1.p3.5.m4.1.2a.cmml\" xref=\"S4.SS1.p3.5.m4.1.2\"></and><apply id=\"S4.SS1.p3.5.m4.1.2b.cmml\" xref=\"S4.SS1.p3.5.m4.1.2\"><leq id=\"S4.SS1.p3.5.m4.1.2.3.cmml\" xref=\"S4.SS1.p3.5.m4.1.2.3\"></leq><cn type=\"integer\" id=\"S4.SS1.p3.5.m4.1.2.2.cmml\" xref=\"S4.SS1.p3.5.m4.1.2.2\">1</cn><ci id=\"S4.SS1.p3.5.m4.1.2.4.cmml\" xref=\"S4.SS1.p3.5.m4.1.2.4\">\ud835\udc56</ci></apply><apply id=\"S4.SS1.p3.5.m4.1.2c.cmml\" xref=\"S4.SS1.p3.5.m4.1.2\"><leq id=\"S4.SS1.p3.5.m4.1.2.5.cmml\" xref=\"S4.SS1.p3.5.m4.1.2.5\"></leq><share href=\"#S4.SS1.p3.5.m4.1.2.4.cmml\" id=\"S4.SS1.p3.5.m4.1.2d.cmml\" xref=\"S4.SS1.p3.5.m4.1.2\"></share><apply id=\"S4.SS1.p3.5.m4.1.2.6.1.cmml\" xref=\"S4.SS1.p3.5.m4.1.2.6.2\"><abs id=\"S4.SS1.p3.5.m4.1.2.6.1.1.cmml\" xref=\"S4.SS1.p3.5.m4.1.2.6.2.1\"></abs><ci id=\"S4.SS1.p3.5.m4.1.1.cmml\" xref=\"S4.SS1.p3.5.m4.1.1\">\ud835\udcae</ci></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p3.5.m4.1c\">1\\leq i\\leq|\\mathcal{S}|</annotation></semantics></math> represents the subset of elements in <math id=\"S4.SS1.p3.6.m5.1\" class=\"ltx_Math\" alttext=\"\\mathcal{S}\" display=\"inline\"><semantics id=\"S4.SS1.p3.6.m5.1a\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.SS1.p3.6.m5.1.1\" xref=\"S4.SS1.p3.6.m5.1.1.cmml\">\ud835\udcae</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p3.6.m5.1b\"><ci id=\"S4.SS1.p3.6.m5.1.1.cmml\" xref=\"S4.SS1.p3.6.m5.1.1\">\ud835\udcae</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p3.6.m5.1c\">\\mathcal{S}</annotation></semantics></math> belonging to class <math id=\"S4.SS1.p3.7.m6.1\" class=\"ltx_Math\" alttext=\"c\" display=\"inline\"><semantics id=\"S4.SS1.p3.7.m6.1a\"><mi id=\"S4.SS1.p3.7.m6.1.1\" xref=\"S4.SS1.p3.7.m6.1.1.cmml\">c</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p3.7.m6.1b\"><ci id=\"S4.SS1.p3.7.m6.1.1.cmml\" xref=\"S4.SS1.p3.7.m6.1.1\">\ud835\udc50</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p3.7.m6.1c\">c</annotation></semantics></math>.</p>\n</div>\n<div id=\"S4.SS1.p4\" class=\"ltx_para\">\n<p id=\"S4.SS1.p4.1\" class=\"ltx_p\">The metric <math id=\"S4.SS1.p4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mbox{Acc}_{n}\" display=\"inline\"><semantics id=\"S4.SS1.p4.1.m1.1a\"><msub id=\"S4.SS1.p4.1.m1.1.1\" xref=\"S4.SS1.p4.1.m1.1.1.cmml\"><mtext id=\"S4.SS1.p4.1.m1.1.1.2\" xref=\"S4.SS1.p4.1.m1.1.1.2a.cmml\">Acc</mtext><mi id=\"S4.SS1.p4.1.m1.1.1.3\" xref=\"S4.SS1.p4.1.m1.1.1.3.cmml\">n</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p4.1.m1.1b\"><apply id=\"S4.SS1.p4.1.m1.1.1.cmml\" xref=\"S4.SS1.p4.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p4.1.m1.1.1.1.cmml\" xref=\"S4.SS1.p4.1.m1.1.1\">subscript</csymbol><ci id=\"S4.SS1.p4.1.m1.1.1.2a.cmml\" xref=\"S4.SS1.p4.1.m1.1.1.2\"><mtext id=\"S4.SS1.p4.1.m1.1.1.2.cmml\" xref=\"S4.SS1.p4.1.m1.1.1.2\">Acc</mtext></ci><ci id=\"S4.SS1.p4.1.m1.1.1.3.cmml\" xref=\"S4.SS1.p4.1.m1.1.1.3\">\ud835\udc5b</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p4.1.m1.1c\">\\mbox{Acc}_{n}</annotation></semantics></math> is defined by the equation:</p>\n<table id=\"S4.E5\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E5.m1.4\" class=\"ltx_Math\" alttext=\"\\mbox{Acc}_{n}=\\frac{1}{|\\mathcal{C}|}\\sum_{c\\in\\mathcal{C}}\\frac{|\\{y\\in\\mathcal{S}_{c}:|\\hat{f}(x)-c|\\leq n\\}|}{|\\mathcal{S}_{c}|}\" display=\"block\"><semantics id=\"S4.E5.m1.4a\"><mrow id=\"S4.E5.m1.4.5\" xref=\"S4.E5.m1.4.5.cmml\"><msub id=\"S4.E5.m1.4.5.2\" xref=\"S4.E5.m1.4.5.2.cmml\"><mtext id=\"S4.E5.m1.4.5.2.2\" xref=\"S4.E5.m1.4.5.2.2a.cmml\">Acc</mtext><mi id=\"S4.E5.m1.4.5.2.3\" xref=\"S4.E5.m1.4.5.2.3.cmml\">n</mi></msub><mo id=\"S4.E5.m1.4.5.1\" xref=\"S4.E5.m1.4.5.1.cmml\">=</mo><mrow id=\"S4.E5.m1.4.5.3\" xref=\"S4.E5.m1.4.5.3.cmml\"><mfrac id=\"S4.E5.m1.1.1\" xref=\"S4.E5.m1.1.1.cmml\"><mn id=\"S4.E5.m1.1.1.3\" xref=\"S4.E5.m1.1.1.3.cmml\">1</mn><mrow id=\"S4.E5.m1.1.1.1.3\" xref=\"S4.E5.m1.1.1.1.2.cmml\"><mo stretchy=\"false\" id=\"S4.E5.m1.1.1.1.3.1\" xref=\"S4.E5.m1.1.1.1.2.1.cmml\">|</mo><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.E5.m1.1.1.1.1\" xref=\"S4.E5.m1.1.1.1.1.cmml\">\ud835\udc9e</mi><mo stretchy=\"false\" id=\"S4.E5.m1.1.1.1.3.2\" xref=\"S4.E5.m1.1.1.1.2.1.cmml\">|</mo></mrow></mfrac><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E5.m1.4.5.3.1\" xref=\"S4.E5.m1.4.5.3.1.cmml\">\u200b</mo><mrow id=\"S4.E5.m1.4.5.3.2\" xref=\"S4.E5.m1.4.5.3.2.cmml\"><munder id=\"S4.E5.m1.4.5.3.2.1\" xref=\"S4.E5.m1.4.5.3.2.1.cmml\"><mo movablelimits=\"false\" id=\"S4.E5.m1.4.5.3.2.1.2\" xref=\"S4.E5.m1.4.5.3.2.1.2.cmml\">\u2211</mo><mrow id=\"S4.E5.m1.4.5.3.2.1.3\" xref=\"S4.E5.m1.4.5.3.2.1.3.cmml\"><mi id=\"S4.E5.m1.4.5.3.2.1.3.2\" xref=\"S4.E5.m1.4.5.3.2.1.3.2.cmml\">c</mi><mo id=\"S4.E5.m1.4.5.3.2.1.3.1\" xref=\"S4.E5.m1.4.5.3.2.1.3.1.cmml\">\u2208</mo><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.E5.m1.4.5.3.2.1.3.3\" xref=\"S4.E5.m1.4.5.3.2.1.3.3.cmml\">\ud835\udc9e</mi></mrow></munder><mfrac id=\"S4.E5.m1.4.4\" xref=\"S4.E5.m1.4.4.cmml\"><mrow id=\"S4.E5.m1.3.3.2.2\" xref=\"S4.E5.m1.3.3.2.3.cmml\"><mo stretchy=\"false\" id=\"S4.E5.m1.3.3.2.2.2\" xref=\"S4.E5.m1.3.3.2.3.1.cmml\">|</mo><mrow id=\"S4.E5.m1.3.3.2.2.1.2\" xref=\"S4.E5.m1.3.3.2.2.1.3.cmml\"><mo stretchy=\"false\" id=\"S4.E5.m1.3.3.2.2.1.2.3\" xref=\"S4.E5.m1.3.3.2.2.1.3.1.cmml\">{</mo><mrow id=\"S4.E5.m1.3.3.2.2.1.1.1\" xref=\"S4.E5.m1.3.3.2.2.1.1.1.cmml\"><mi id=\"S4.E5.m1.3.3.2.2.1.1.1.2\" xref=\"S4.E5.m1.3.3.2.2.1.1.1.2.cmml\">y</mi><mo id=\"S4.E5.m1.3.3.2.2.1.1.1.1\" xref=\"S4.E5.m1.3.3.2.2.1.1.1.1.cmml\">\u2208</mo><msub id=\"S4.E5.m1.3.3.2.2.1.1.1.3\" xref=\"S4.E5.m1.3.3.2.2.1.1.1.3.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.E5.m1.3.3.2.2.1.1.1.3.2\" xref=\"S4.E5.m1.3.3.2.2.1.1.1.3.2.cmml\">\ud835\udcae</mi><mi id=\"S4.E5.m1.3.3.2.2.1.1.1.3.3\" xref=\"S4.E5.m1.3.3.2.2.1.1.1.3.3.cmml\">c</mi></msub></mrow><mo lspace=\"0.278em\" rspace=\"0.278em\" id=\"S4.E5.m1.3.3.2.2.1.2.4\" xref=\"S4.E5.m1.3.3.2.2.1.3.1.cmml\">:</mo><mrow id=\"S4.E5.m1.3.3.2.2.1.2.2\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.cmml\"><mrow id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.2.cmml\"><mo stretchy=\"false\" id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.2\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.2.1.cmml\">|</mo><mrow id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.cmml\"><mrow id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.cmml\"><mover accent=\"true\" id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.2\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.2.cmml\"><mi id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.2.2\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.2.2.cmml\">f</mi><mo id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.2.1\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.2.1.cmml\">^</mo></mover><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.1\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.1.cmml\">\u200b</mo><mrow id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.3.2\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.cmml\"><mo stretchy=\"false\" id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.3.2.1\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.cmml\">(</mo><mi id=\"S4.E5.m1.2.2.1.1\" xref=\"S4.E5.m1.2.2.1.1.cmml\">x</mi><mo stretchy=\"false\" id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.3.2.2\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.1\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.1.cmml\">\u2212</mo><mi id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.3\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.3.cmml\">c</mi></mrow><mo stretchy=\"false\" id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.3\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.2.1.cmml\">|</mo></mrow><mo id=\"S4.E5.m1.3.3.2.2.1.2.2.2\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.2.cmml\">\u2264</mo><mi id=\"S4.E5.m1.3.3.2.2.1.2.2.3\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.3.cmml\">n</mi></mrow><mo stretchy=\"false\" id=\"S4.E5.m1.3.3.2.2.1.2.5\" xref=\"S4.E5.m1.3.3.2.2.1.3.1.cmml\">}</mo></mrow><mo stretchy=\"false\" id=\"S4.E5.m1.3.3.2.2.3\" xref=\"S4.E5.m1.3.3.2.3.1.cmml\">|</mo></mrow><mrow id=\"S4.E5.m1.4.4.3.1\" xref=\"S4.E5.m1.4.4.3.2.cmml\"><mo stretchy=\"false\" id=\"S4.E5.m1.4.4.3.1.2\" xref=\"S4.E5.m1.4.4.3.2.1.cmml\">|</mo><msub id=\"S4.E5.m1.4.4.3.1.1\" xref=\"S4.E5.m1.4.4.3.1.1.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.E5.m1.4.4.3.1.1.2\" xref=\"S4.E5.m1.4.4.3.1.1.2.cmml\">\ud835\udcae</mi><mi id=\"S4.E5.m1.4.4.3.1.1.3\" xref=\"S4.E5.m1.4.4.3.1.1.3.cmml\">c</mi></msub><mo stretchy=\"false\" id=\"S4.E5.m1.4.4.3.1.3\" xref=\"S4.E5.m1.4.4.3.2.1.cmml\">|</mo></mrow></mfrac></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.E5.m1.4b\"><apply id=\"S4.E5.m1.4.5.cmml\" xref=\"S4.E5.m1.4.5\"><eq id=\"S4.E5.m1.4.5.1.cmml\" xref=\"S4.E5.m1.4.5.1\"></eq><apply id=\"S4.E5.m1.4.5.2.cmml\" xref=\"S4.E5.m1.4.5.2\"><csymbol cd=\"ambiguous\" id=\"S4.E5.m1.4.5.2.1.cmml\" xref=\"S4.E5.m1.4.5.2\">subscript</csymbol><ci id=\"S4.E5.m1.4.5.2.2a.cmml\" xref=\"S4.E5.m1.4.5.2.2\"><mtext id=\"S4.E5.m1.4.5.2.2.cmml\" xref=\"S4.E5.m1.4.5.2.2\">Acc</mtext></ci><ci id=\"S4.E5.m1.4.5.2.3.cmml\" xref=\"S4.E5.m1.4.5.2.3\">\ud835\udc5b</ci></apply><apply id=\"S4.E5.m1.4.5.3.cmml\" xref=\"S4.E5.m1.4.5.3\"><times id=\"S4.E5.m1.4.5.3.1.cmml\" xref=\"S4.E5.m1.4.5.3.1\"></times><apply id=\"S4.E5.m1.1.1.cmml\" xref=\"S4.E5.m1.1.1\"><divide id=\"S4.E5.m1.1.1.2.cmml\" xref=\"S4.E5.m1.1.1\"></divide><cn type=\"integer\" id=\"S4.E5.m1.1.1.3.cmml\" xref=\"S4.E5.m1.1.1.3\">1</cn><apply id=\"S4.E5.m1.1.1.1.2.cmml\" xref=\"S4.E5.m1.1.1.1.3\"><abs id=\"S4.E5.m1.1.1.1.2.1.cmml\" xref=\"S4.E5.m1.1.1.1.3.1\"></abs><ci id=\"S4.E5.m1.1.1.1.1.cmml\" xref=\"S4.E5.m1.1.1.1.1\">\ud835\udc9e</ci></apply></apply><apply id=\"S4.E5.m1.4.5.3.2.cmml\" xref=\"S4.E5.m1.4.5.3.2\"><apply id=\"S4.E5.m1.4.5.3.2.1.cmml\" xref=\"S4.E5.m1.4.5.3.2.1\"><csymbol cd=\"ambiguous\" id=\"S4.E5.m1.4.5.3.2.1.1.cmml\" xref=\"S4.E5.m1.4.5.3.2.1\">subscript</csymbol><sum id=\"S4.E5.m1.4.5.3.2.1.2.cmml\" xref=\"S4.E5.m1.4.5.3.2.1.2\"></sum><apply id=\"S4.E5.m1.4.5.3.2.1.3.cmml\" xref=\"S4.E5.m1.4.5.3.2.1.3\"><in id=\"S4.E5.m1.4.5.3.2.1.3.1.cmml\" xref=\"S4.E5.m1.4.5.3.2.1.3.1\"></in><ci id=\"S4.E5.m1.4.5.3.2.1.3.2.cmml\" xref=\"S4.E5.m1.4.5.3.2.1.3.2\">\ud835\udc50</ci><ci id=\"S4.E5.m1.4.5.3.2.1.3.3.cmml\" xref=\"S4.E5.m1.4.5.3.2.1.3.3\">\ud835\udc9e</ci></apply></apply><apply id=\"S4.E5.m1.4.4.cmml\" xref=\"S4.E5.m1.4.4\"><divide id=\"S4.E5.m1.4.4.4.cmml\" xref=\"S4.E5.m1.4.4\"></divide><apply id=\"S4.E5.m1.3.3.2.3.cmml\" xref=\"S4.E5.m1.3.3.2.2\"><abs id=\"S4.E5.m1.3.3.2.3.1.cmml\" xref=\"S4.E5.m1.3.3.2.2.2\"></abs><apply id=\"S4.E5.m1.3.3.2.2.1.3.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2\"><csymbol cd=\"latexml\" id=\"S4.E5.m1.3.3.2.2.1.3.1.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.3\">conditional-set</csymbol><apply id=\"S4.E5.m1.3.3.2.2.1.1.1.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.1.1\"><in id=\"S4.E5.m1.3.3.2.2.1.1.1.1.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.1.1.1\"></in><ci id=\"S4.E5.m1.3.3.2.2.1.1.1.2.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.1.1.2\">\ud835\udc66</ci><apply id=\"S4.E5.m1.3.3.2.2.1.1.1.3.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.E5.m1.3.3.2.2.1.1.1.3.1.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.1.1.3\">subscript</csymbol><ci id=\"S4.E5.m1.3.3.2.2.1.1.1.3.2.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.1.1.3.2\">\ud835\udcae</ci><ci id=\"S4.E5.m1.3.3.2.2.1.1.1.3.3.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.1.1.3.3\">\ud835\udc50</ci></apply></apply><apply id=\"S4.E5.m1.3.3.2.2.1.2.2.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.2\"><leq id=\"S4.E5.m1.3.3.2.2.1.2.2.2.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.2\"></leq><apply id=\"S4.E5.m1.3.3.2.2.1.2.2.1.2.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1\"><abs id=\"S4.E5.m1.3.3.2.2.1.2.2.1.2.1.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.2\"></abs><apply id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1\"><minus id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.1.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.1\"></minus><apply id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2\"><times id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.1.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.1\"></times><apply id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.2.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.2\"><ci id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.2.1.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.2.1\">^</ci><ci id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.2.2.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.2.2.2\">\ud835\udc53</ci></apply><ci id=\"S4.E5.m1.2.2.1.1.cmml\" xref=\"S4.E5.m1.2.2.1.1\">\ud835\udc65</ci></apply><ci id=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.3.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.1.1.1.3\">\ud835\udc50</ci></apply></apply><ci id=\"S4.E5.m1.3.3.2.2.1.2.2.3.cmml\" xref=\"S4.E5.m1.3.3.2.2.1.2.2.3\">\ud835\udc5b</ci></apply></apply></apply><apply id=\"S4.E5.m1.4.4.3.2.cmml\" xref=\"S4.E5.m1.4.4.3.1\"><abs id=\"S4.E5.m1.4.4.3.2.1.cmml\" xref=\"S4.E5.m1.4.4.3.1.2\"></abs><apply id=\"S4.E5.m1.4.4.3.1.1.cmml\" xref=\"S4.E5.m1.4.4.3.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.E5.m1.4.4.3.1.1.1.cmml\" xref=\"S4.E5.m1.4.4.3.1.1\">subscript</csymbol><ci id=\"S4.E5.m1.4.4.3.1.1.2.cmml\" xref=\"S4.E5.m1.4.4.3.1.1.2\">\ud835\udcae</ci><ci id=\"S4.E5.m1.4.4.3.1.1.3.cmml\" xref=\"S4.E5.m1.4.4.3.1.1.3\">\ud835\udc50</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.E5.m1.4c\">\\mbox{Acc}_{n}=\\frac{1}{|\\mathcal{C}|}\\sum_{c\\in\\mathcal{C}}\\frac{|\\{y\\in\\mathcal{S}_{c}:|\\hat{f}(x)-c|\\leq n\\}|}{|\\mathcal{S}_{c}|}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(5)</span></td>\n</tr></tbody>\n</table>\n<p id=\"S4.SS1.p4.7\" class=\"ltx_p\">where <math id=\"S4.SS1.p4.2.m1.1\" class=\"ltx_Math\" alttext=\"\\hat{f}(\\cdot)\" display=\"inline\"><semantics id=\"S4.SS1.p4.2.m1.1a\"><mrow id=\"S4.SS1.p4.2.m1.1.2\" xref=\"S4.SS1.p4.2.m1.1.2.cmml\"><mover accent=\"true\" id=\"S4.SS1.p4.2.m1.1.2.2\" xref=\"S4.SS1.p4.2.m1.1.2.2.cmml\"><mi id=\"S4.SS1.p4.2.m1.1.2.2.2\" xref=\"S4.SS1.p4.2.m1.1.2.2.2.cmml\">f</mi><mo id=\"S4.SS1.p4.2.m1.1.2.2.1\" xref=\"S4.SS1.p4.2.m1.1.2.2.1.cmml\">^</mo></mover><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.p4.2.m1.1.2.1\" xref=\"S4.SS1.p4.2.m1.1.2.1.cmml\">\u200b</mo><mrow id=\"S4.SS1.p4.2.m1.1.2.3.2\" xref=\"S4.SS1.p4.2.m1.1.2.cmml\"><mo stretchy=\"false\" id=\"S4.SS1.p4.2.m1.1.2.3.2.1\" xref=\"S4.SS1.p4.2.m1.1.2.cmml\">(</mo><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.p4.2.m1.1.1\" xref=\"S4.SS1.p4.2.m1.1.1.cmml\">\u22c5</mo><mo stretchy=\"false\" id=\"S4.SS1.p4.2.m1.1.2.3.2.2\" xref=\"S4.SS1.p4.2.m1.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p4.2.m1.1b\"><apply id=\"S4.SS1.p4.2.m1.1.2.cmml\" xref=\"S4.SS1.p4.2.m1.1.2\"><times id=\"S4.SS1.p4.2.m1.1.2.1.cmml\" xref=\"S4.SS1.p4.2.m1.1.2.1\"></times><apply id=\"S4.SS1.p4.2.m1.1.2.2.cmml\" xref=\"S4.SS1.p4.2.m1.1.2.2\"><ci id=\"S4.SS1.p4.2.m1.1.2.2.1.cmml\" xref=\"S4.SS1.p4.2.m1.1.2.2.1\">^</ci><ci id=\"S4.SS1.p4.2.m1.1.2.2.2.cmml\" xref=\"S4.SS1.p4.2.m1.1.2.2.2\">\ud835\udc53</ci></apply><ci id=\"S4.SS1.p4.2.m1.1.1.cmml\" xref=\"S4.SS1.p4.2.m1.1.1\">\u22c5</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p4.2.m1.1c\">\\hat{f}(\\cdot)</annotation></semantics></math> is the prediction of the trained model and <math id=\"S4.SS1.p4.3.m2.1\" class=\"ltx_Math\" alttext=\"n\\in\\mathbb{N}_{0}\" display=\"inline\"><semantics id=\"S4.SS1.p4.3.m2.1a\"><mrow id=\"S4.SS1.p4.3.m2.1.1\" xref=\"S4.SS1.p4.3.m2.1.1.cmml\"><mi id=\"S4.SS1.p4.3.m2.1.1.2\" xref=\"S4.SS1.p4.3.m2.1.1.2.cmml\">n</mi><mo id=\"S4.SS1.p4.3.m2.1.1.1\" xref=\"S4.SS1.p4.3.m2.1.1.1.cmml\">\u2208</mo><msub id=\"S4.SS1.p4.3.m2.1.1.3\" xref=\"S4.SS1.p4.3.m2.1.1.3.cmml\"><mi id=\"S4.SS1.p4.3.m2.1.1.3.2\" xref=\"S4.SS1.p4.3.m2.1.1.3.2.cmml\">\u2115</mi><mn id=\"S4.SS1.p4.3.m2.1.1.3.3\" xref=\"S4.SS1.p4.3.m2.1.1.3.3.cmml\">0</mn></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p4.3.m2.1b\"><apply id=\"S4.SS1.p4.3.m2.1.1.cmml\" xref=\"S4.SS1.p4.3.m2.1.1\"><in id=\"S4.SS1.p4.3.m2.1.1.1.cmml\" xref=\"S4.SS1.p4.3.m2.1.1.1\"></in><ci id=\"S4.SS1.p4.3.m2.1.1.2.cmml\" xref=\"S4.SS1.p4.3.m2.1.1.2\">\ud835\udc5b</ci><apply id=\"S4.SS1.p4.3.m2.1.1.3.cmml\" xref=\"S4.SS1.p4.3.m2.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p4.3.m2.1.1.3.1.cmml\" xref=\"S4.SS1.p4.3.m2.1.1.3\">subscript</csymbol><ci id=\"S4.SS1.p4.3.m2.1.1.3.2.cmml\" xref=\"S4.SS1.p4.3.m2.1.1.3.2\">\u2115</ci><cn type=\"integer\" id=\"S4.SS1.p4.3.m2.1.1.3.3.cmml\" xref=\"S4.SS1.p4.3.m2.1.1.3.3\">0</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p4.3.m2.1c\">n\\in\\mathbb{N}_{0}</annotation></semantics></math> represents the tolerance level or allowed deviation from the true class labels. Specifically, we analyze <math id=\"S4.SS1.p4.4.m3.1\" class=\"ltx_Math\" alttext=\"n=0\" display=\"inline\"><semantics id=\"S4.SS1.p4.4.m3.1a\"><mrow id=\"S4.SS1.p4.4.m3.1.1\" xref=\"S4.SS1.p4.4.m3.1.1.cmml\"><mi id=\"S4.SS1.p4.4.m3.1.1.2\" xref=\"S4.SS1.p4.4.m3.1.1.2.cmml\">n</mi><mo id=\"S4.SS1.p4.4.m3.1.1.1\" xref=\"S4.SS1.p4.4.m3.1.1.1.cmml\">=</mo><mn id=\"S4.SS1.p4.4.m3.1.1.3\" xref=\"S4.SS1.p4.4.m3.1.1.3.cmml\">0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p4.4.m3.1b\"><apply id=\"S4.SS1.p4.4.m3.1.1.cmml\" xref=\"S4.SS1.p4.4.m3.1.1\"><eq id=\"S4.SS1.p4.4.m3.1.1.1.cmml\" xref=\"S4.SS1.p4.4.m3.1.1.1\"></eq><ci id=\"S4.SS1.p4.4.m3.1.1.2.cmml\" xref=\"S4.SS1.p4.4.m3.1.1.2\">\ud835\udc5b</ci><cn type=\"integer\" id=\"S4.SS1.p4.4.m3.1.1.3.cmml\" xref=\"S4.SS1.p4.4.m3.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p4.4.m3.1c\">n=0</annotation></semantics></math> (exact match) and <math id=\"S4.SS1.p4.5.m4.1\" class=\"ltx_Math\" alttext=\"n=1\" display=\"inline\"><semantics id=\"S4.SS1.p4.5.m4.1a\"><mrow id=\"S4.SS1.p4.5.m4.1.1\" xref=\"S4.SS1.p4.5.m4.1.1.cmml\"><mi id=\"S4.SS1.p4.5.m4.1.1.2\" xref=\"S4.SS1.p4.5.m4.1.1.2.cmml\">n</mi><mo id=\"S4.SS1.p4.5.m4.1.1.1\" xref=\"S4.SS1.p4.5.m4.1.1.1.cmml\">=</mo><mn id=\"S4.SS1.p4.5.m4.1.1.3\" xref=\"S4.SS1.p4.5.m4.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p4.5.m4.1b\"><apply id=\"S4.SS1.p4.5.m4.1.1.cmml\" xref=\"S4.SS1.p4.5.m4.1.1\"><eq id=\"S4.SS1.p4.5.m4.1.1.1.cmml\" xref=\"S4.SS1.p4.5.m4.1.1.1\"></eq><ci id=\"S4.SS1.p4.5.m4.1.1.2.cmml\" xref=\"S4.SS1.p4.5.m4.1.1.2\">\ud835\udc5b</ci><cn type=\"integer\" id=\"S4.SS1.p4.5.m4.1.1.3.cmml\" xref=\"S4.SS1.p4.5.m4.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p4.5.m4.1c\">n=1</annotation></semantics></math> (allowing for one-class deviation), referred to as <math id=\"S4.SS1.p4.6.m5.1\" class=\"ltx_Math\" alttext=\"\\mbox{Acc}_{0}\" display=\"inline\"><semantics id=\"S4.SS1.p4.6.m5.1a\"><msub id=\"S4.SS1.p4.6.m5.1.1\" xref=\"S4.SS1.p4.6.m5.1.1.cmml\"><mtext id=\"S4.SS1.p4.6.m5.1.1.2\" xref=\"S4.SS1.p4.6.m5.1.1.2a.cmml\">Acc</mtext><mn id=\"S4.SS1.p4.6.m5.1.1.3\" xref=\"S4.SS1.p4.6.m5.1.1.3.cmml\">0</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p4.6.m5.1b\"><apply id=\"S4.SS1.p4.6.m5.1.1.cmml\" xref=\"S4.SS1.p4.6.m5.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p4.6.m5.1.1.1.cmml\" xref=\"S4.SS1.p4.6.m5.1.1\">subscript</csymbol><ci id=\"S4.SS1.p4.6.m5.1.1.2a.cmml\" xref=\"S4.SS1.p4.6.m5.1.1.2\"><mtext id=\"S4.SS1.p4.6.m5.1.1.2.cmml\" xref=\"S4.SS1.p4.6.m5.1.1.2\">Acc</mtext></ci><cn type=\"integer\" id=\"S4.SS1.p4.6.m5.1.1.3.cmml\" xref=\"S4.SS1.p4.6.m5.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p4.6.m5.1c\">\\mbox{Acc}_{0}</annotation></semantics></math> and <math id=\"S4.SS1.p4.7.m6.1\" class=\"ltx_Math\" alttext=\"\\mbox{Acc}_{1}\" display=\"inline\"><semantics id=\"S4.SS1.p4.7.m6.1a\"><msub id=\"S4.SS1.p4.7.m6.1.1\" xref=\"S4.SS1.p4.7.m6.1.1.cmml\"><mtext id=\"S4.SS1.p4.7.m6.1.1.2\" xref=\"S4.SS1.p4.7.m6.1.1.2a.cmml\">Acc</mtext><mn id=\"S4.SS1.p4.7.m6.1.1.3\" xref=\"S4.SS1.p4.7.m6.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p4.7.m6.1b\"><apply id=\"S4.SS1.p4.7.m6.1.1.cmml\" xref=\"S4.SS1.p4.7.m6.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS1.p4.7.m6.1.1.1.cmml\" xref=\"S4.SS1.p4.7.m6.1.1\">subscript</csymbol><ci id=\"S4.SS1.p4.7.m6.1.1.2a.cmml\" xref=\"S4.SS1.p4.7.m6.1.1.2\"><mtext id=\"S4.SS1.p4.7.m6.1.1.2.cmml\" xref=\"S4.SS1.p4.7.m6.1.1.2\">Acc</mtext></ci><cn type=\"integer\" id=\"S4.SS1.p4.7.m6.1.1.3.cmml\" xref=\"S4.SS1.p4.7.m6.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p4.7.m6.1c\">\\mbox{Acc}_{1}</annotation></semantics></math>, respectively, throughout our analysis.</p>\n</div>\n<div id=\"S4.SS1.p5\" class=\"ltx_para\">\n<p id=\"S4.SS1.p5.1\" class=\"ltx_p\">For MSE, it is defined as:</p>\n<table id=\"S4.E6\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.E6.m1.4\" class=\"ltx_Math\" alttext=\"\\mbox{MSE}=\\frac{1}{|\\mathcal{C}|}\\sum_{c\\in\\mathcal{C}}\\frac{\\sum_{x\\in\\mathcal{S}_{c}}(\\hat{f}(x)-c)^{2}}{|\\mathcal{S}_{c}|}\" display=\"block\"><semantics id=\"S4.E6.m1.4a\"><mrow id=\"S4.E6.m1.4.5\" xref=\"S4.E6.m1.4.5.cmml\"><mtext id=\"S4.E6.m1.4.5.2\" xref=\"S4.E6.m1.4.5.2a.cmml\">MSE</mtext><mo id=\"S4.E6.m1.4.5.1\" xref=\"S4.E6.m1.4.5.1.cmml\">=</mo><mrow id=\"S4.E6.m1.4.5.3\" xref=\"S4.E6.m1.4.5.3.cmml\"><mfrac id=\"S4.E6.m1.1.1\" xref=\"S4.E6.m1.1.1.cmml\"><mn id=\"S4.E6.m1.1.1.3\" xref=\"S4.E6.m1.1.1.3.cmml\">1</mn><mrow id=\"S4.E6.m1.1.1.1.3\" xref=\"S4.E6.m1.1.1.1.2.cmml\"><mo stretchy=\"false\" id=\"S4.E6.m1.1.1.1.3.1\" xref=\"S4.E6.m1.1.1.1.2.1.cmml\">|</mo><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.E6.m1.1.1.1.1\" xref=\"S4.E6.m1.1.1.1.1.cmml\">\ud835\udc9e</mi><mo stretchy=\"false\" id=\"S4.E6.m1.1.1.1.3.2\" xref=\"S4.E6.m1.1.1.1.2.1.cmml\">|</mo></mrow></mfrac><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E6.m1.4.5.3.1\" xref=\"S4.E6.m1.4.5.3.1.cmml\">\u200b</mo><mrow id=\"S4.E6.m1.4.5.3.2\" xref=\"S4.E6.m1.4.5.3.2.cmml\"><munder id=\"S4.E6.m1.4.5.3.2.1\" xref=\"S4.E6.m1.4.5.3.2.1.cmml\"><mo movablelimits=\"false\" id=\"S4.E6.m1.4.5.3.2.1.2\" xref=\"S4.E6.m1.4.5.3.2.1.2.cmml\">\u2211</mo><mrow id=\"S4.E6.m1.4.5.3.2.1.3\" xref=\"S4.E6.m1.4.5.3.2.1.3.cmml\"><mi id=\"S4.E6.m1.4.5.3.2.1.3.2\" xref=\"S4.E6.m1.4.5.3.2.1.3.2.cmml\">c</mi><mo id=\"S4.E6.m1.4.5.3.2.1.3.1\" xref=\"S4.E6.m1.4.5.3.2.1.3.1.cmml\">\u2208</mo><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.E6.m1.4.5.3.2.1.3.3\" xref=\"S4.E6.m1.4.5.3.2.1.3.3.cmml\">\ud835\udc9e</mi></mrow></munder><mfrac id=\"S4.E6.m1.4.4\" xref=\"S4.E6.m1.4.4.cmml\"><mrow id=\"S4.E6.m1.3.3.2\" xref=\"S4.E6.m1.3.3.2.cmml\"><msub id=\"S4.E6.m1.3.3.2.3\" xref=\"S4.E6.m1.3.3.2.3.cmml\"><mo id=\"S4.E6.m1.3.3.2.3.2\" xref=\"S4.E6.m1.3.3.2.3.2.cmml\">\u2211</mo><mrow id=\"S4.E6.m1.3.3.2.3.3\" xref=\"S4.E6.m1.3.3.2.3.3.cmml\"><mi id=\"S4.E6.m1.3.3.2.3.3.2\" xref=\"S4.E6.m1.3.3.2.3.3.2.cmml\">x</mi><mo id=\"S4.E6.m1.3.3.2.3.3.1\" xref=\"S4.E6.m1.3.3.2.3.3.1.cmml\">\u2208</mo><msub id=\"S4.E6.m1.3.3.2.3.3.3\" xref=\"S4.E6.m1.3.3.2.3.3.3.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.E6.m1.3.3.2.3.3.3.2\" xref=\"S4.E6.m1.3.3.2.3.3.3.2.cmml\">\ud835\udcae</mi><mi id=\"S4.E6.m1.3.3.2.3.3.3.3\" xref=\"S4.E6.m1.3.3.2.3.3.3.3.cmml\">c</mi></msub></mrow></msub><msup id=\"S4.E6.m1.3.3.2.2\" xref=\"S4.E6.m1.3.3.2.2.cmml\"><mrow id=\"S4.E6.m1.3.3.2.2.1.1\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.cmml\"><mo lspace=\"0em\" stretchy=\"false\" id=\"S4.E6.m1.3.3.2.2.1.1.2\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.cmml\">(</mo><mrow id=\"S4.E6.m1.3.3.2.2.1.1.1\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.cmml\"><mrow id=\"S4.E6.m1.3.3.2.2.1.1.1.2\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.2.cmml\"><mover accent=\"true\" id=\"S4.E6.m1.3.3.2.2.1.1.1.2.2\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.2.2.cmml\"><mi id=\"S4.E6.m1.3.3.2.2.1.1.1.2.2.2\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.2.2.2.cmml\">f</mi><mo id=\"S4.E6.m1.3.3.2.2.1.1.1.2.2.1\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.2.2.1.cmml\">^</mo></mover><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.E6.m1.3.3.2.2.1.1.1.2.1\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.2.1.cmml\">\u200b</mo><mrow id=\"S4.E6.m1.3.3.2.2.1.1.1.2.3.2\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.2.cmml\"><mo stretchy=\"false\" id=\"S4.E6.m1.3.3.2.2.1.1.1.2.3.2.1\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.2.cmml\">(</mo><mi id=\"S4.E6.m1.2.2.1.1\" xref=\"S4.E6.m1.2.2.1.1.cmml\">x</mi><mo stretchy=\"false\" id=\"S4.E6.m1.3.3.2.2.1.1.1.2.3.2.2\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.2.cmml\">)</mo></mrow></mrow><mo id=\"S4.E6.m1.3.3.2.2.1.1.1.1\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.1.cmml\">\u2212</mo><mi id=\"S4.E6.m1.3.3.2.2.1.1.1.3\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.3.cmml\">c</mi></mrow><mo stretchy=\"false\" id=\"S4.E6.m1.3.3.2.2.1.1.3\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.cmml\">)</mo></mrow><mn id=\"S4.E6.m1.3.3.2.2.3\" xref=\"S4.E6.m1.3.3.2.2.3.cmml\">2</mn></msup></mrow><mrow id=\"S4.E6.m1.4.4.3.1\" xref=\"S4.E6.m1.4.4.3.2.cmml\"><mo stretchy=\"false\" id=\"S4.E6.m1.4.4.3.1.2\" xref=\"S4.E6.m1.4.4.3.2.1.cmml\">|</mo><msub id=\"S4.E6.m1.4.4.3.1.1\" xref=\"S4.E6.m1.4.4.3.1.1.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.E6.m1.4.4.3.1.1.2\" xref=\"S4.E6.m1.4.4.3.1.1.2.cmml\">\ud835\udcae</mi><mi id=\"S4.E6.m1.4.4.3.1.1.3\" xref=\"S4.E6.m1.4.4.3.1.1.3.cmml\">c</mi></msub><mo stretchy=\"false\" id=\"S4.E6.m1.4.4.3.1.3\" xref=\"S4.E6.m1.4.4.3.2.1.cmml\">|</mo></mrow></mfrac></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.E6.m1.4b\"><apply id=\"S4.E6.m1.4.5.cmml\" xref=\"S4.E6.m1.4.5\"><eq id=\"S4.E6.m1.4.5.1.cmml\" xref=\"S4.E6.m1.4.5.1\"></eq><ci id=\"S4.E6.m1.4.5.2a.cmml\" xref=\"S4.E6.m1.4.5.2\"><mtext id=\"S4.E6.m1.4.5.2.cmml\" xref=\"S4.E6.m1.4.5.2\">MSE</mtext></ci><apply id=\"S4.E6.m1.4.5.3.cmml\" xref=\"S4.E6.m1.4.5.3\"><times id=\"S4.E6.m1.4.5.3.1.cmml\" xref=\"S4.E6.m1.4.5.3.1\"></times><apply id=\"S4.E6.m1.1.1.cmml\" xref=\"S4.E6.m1.1.1\"><divide id=\"S4.E6.m1.1.1.2.cmml\" xref=\"S4.E6.m1.1.1\"></divide><cn type=\"integer\" id=\"S4.E6.m1.1.1.3.cmml\" xref=\"S4.E6.m1.1.1.3\">1</cn><apply id=\"S4.E6.m1.1.1.1.2.cmml\" xref=\"S4.E6.m1.1.1.1.3\"><abs id=\"S4.E6.m1.1.1.1.2.1.cmml\" xref=\"S4.E6.m1.1.1.1.3.1\"></abs><ci id=\"S4.E6.m1.1.1.1.1.cmml\" xref=\"S4.E6.m1.1.1.1.1\">\ud835\udc9e</ci></apply></apply><apply id=\"S4.E6.m1.4.5.3.2.cmml\" xref=\"S4.E6.m1.4.5.3.2\"><apply id=\"S4.E6.m1.4.5.3.2.1.cmml\" xref=\"S4.E6.m1.4.5.3.2.1\"><csymbol cd=\"ambiguous\" id=\"S4.E6.m1.4.5.3.2.1.1.cmml\" xref=\"S4.E6.m1.4.5.3.2.1\">subscript</csymbol><sum id=\"S4.E6.m1.4.5.3.2.1.2.cmml\" xref=\"S4.E6.m1.4.5.3.2.1.2\"></sum><apply id=\"S4.E6.m1.4.5.3.2.1.3.cmml\" xref=\"S4.E6.m1.4.5.3.2.1.3\"><in id=\"S4.E6.m1.4.5.3.2.1.3.1.cmml\" xref=\"S4.E6.m1.4.5.3.2.1.3.1\"></in><ci id=\"S4.E6.m1.4.5.3.2.1.3.2.cmml\" xref=\"S4.E6.m1.4.5.3.2.1.3.2\">\ud835\udc50</ci><ci id=\"S4.E6.m1.4.5.3.2.1.3.3.cmml\" xref=\"S4.E6.m1.4.5.3.2.1.3.3\">\ud835\udc9e</ci></apply></apply><apply id=\"S4.E6.m1.4.4.cmml\" xref=\"S4.E6.m1.4.4\"><divide id=\"S4.E6.m1.4.4.4.cmml\" xref=\"S4.E6.m1.4.4\"></divide><apply id=\"S4.E6.m1.3.3.2.cmml\" xref=\"S4.E6.m1.3.3.2\"><apply id=\"S4.E6.m1.3.3.2.3.cmml\" xref=\"S4.E6.m1.3.3.2.3\"><csymbol cd=\"ambiguous\" id=\"S4.E6.m1.3.3.2.3.1.cmml\" xref=\"S4.E6.m1.3.3.2.3\">subscript</csymbol><sum id=\"S4.E6.m1.3.3.2.3.2.cmml\" xref=\"S4.E6.m1.3.3.2.3.2\"></sum><apply id=\"S4.E6.m1.3.3.2.3.3.cmml\" xref=\"S4.E6.m1.3.3.2.3.3\"><in id=\"S4.E6.m1.3.3.2.3.3.1.cmml\" xref=\"S4.E6.m1.3.3.2.3.3.1\"></in><ci id=\"S4.E6.m1.3.3.2.3.3.2.cmml\" xref=\"S4.E6.m1.3.3.2.3.3.2\">\ud835\udc65</ci><apply id=\"S4.E6.m1.3.3.2.3.3.3.cmml\" xref=\"S4.E6.m1.3.3.2.3.3.3\"><csymbol cd=\"ambiguous\" id=\"S4.E6.m1.3.3.2.3.3.3.1.cmml\" xref=\"S4.E6.m1.3.3.2.3.3.3\">subscript</csymbol><ci id=\"S4.E6.m1.3.3.2.3.3.3.2.cmml\" xref=\"S4.E6.m1.3.3.2.3.3.3.2\">\ud835\udcae</ci><ci id=\"S4.E6.m1.3.3.2.3.3.3.3.cmml\" xref=\"S4.E6.m1.3.3.2.3.3.3.3\">\ud835\udc50</ci></apply></apply></apply><apply id=\"S4.E6.m1.3.3.2.2.cmml\" xref=\"S4.E6.m1.3.3.2.2\"><csymbol cd=\"ambiguous\" id=\"S4.E6.m1.3.3.2.2.2.cmml\" xref=\"S4.E6.m1.3.3.2.2\">superscript</csymbol><apply id=\"S4.E6.m1.3.3.2.2.1.1.1.cmml\" xref=\"S4.E6.m1.3.3.2.2.1.1\"><minus id=\"S4.E6.m1.3.3.2.2.1.1.1.1.cmml\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.1\"></minus><apply id=\"S4.E6.m1.3.3.2.2.1.1.1.2.cmml\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.2\"><times id=\"S4.E6.m1.3.3.2.2.1.1.1.2.1.cmml\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.2.1\"></times><apply id=\"S4.E6.m1.3.3.2.2.1.1.1.2.2.cmml\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.2.2\"><ci id=\"S4.E6.m1.3.3.2.2.1.1.1.2.2.1.cmml\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.2.2.1\">^</ci><ci id=\"S4.E6.m1.3.3.2.2.1.1.1.2.2.2.cmml\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.2.2.2\">\ud835\udc53</ci></apply><ci id=\"S4.E6.m1.2.2.1.1.cmml\" xref=\"S4.E6.m1.2.2.1.1\">\ud835\udc65</ci></apply><ci id=\"S4.E6.m1.3.3.2.2.1.1.1.3.cmml\" xref=\"S4.E6.m1.3.3.2.2.1.1.1.3\">\ud835\udc50</ci></apply><cn type=\"integer\" id=\"S4.E6.m1.3.3.2.2.3.cmml\" xref=\"S4.E6.m1.3.3.2.2.3\">2</cn></apply></apply><apply id=\"S4.E6.m1.4.4.3.2.cmml\" xref=\"S4.E6.m1.4.4.3.1\"><abs id=\"S4.E6.m1.4.4.3.2.1.cmml\" xref=\"S4.E6.m1.4.4.3.1.2\"></abs><apply id=\"S4.E6.m1.4.4.3.1.1.cmml\" xref=\"S4.E6.m1.4.4.3.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.E6.m1.4.4.3.1.1.1.cmml\" xref=\"S4.E6.m1.4.4.3.1.1\">subscript</csymbol><ci id=\"S4.E6.m1.4.4.3.1.1.2.cmml\" xref=\"S4.E6.m1.4.4.3.1.1.2\">\ud835\udcae</ci><ci id=\"S4.E6.m1.4.4.3.1.1.3.cmml\" xref=\"S4.E6.m1.4.4.3.1.1.3\">\ud835\udc50</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.E6.m1.4c\">\\mbox{MSE}=\\frac{1}{|\\mathcal{C}|}\\sum_{c\\in\\mathcal{C}}\\frac{\\sum_{x\\in\\mathcal{S}_{c}}(\\hat{f}(x)-c)^{2}}{|\\mathcal{S}_{c}|}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(6)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div id=\"S4.SS1.p6\" class=\"ltx_para\">\n<p id=\"S4.SS1.p6.1\" class=\"ltx_p\">It is important to note that we macro-average all metrics to mitigate the impact of data imbalance across the datasets used in our study.</p>\n</div>\n<div id=\"S4.SS1.p7\" class=\"ltx_para\">\n<p id=\"S4.SS1.p7.2\" class=\"ltx_p\">Finally, we employ the tau-c correlation coefficient as a measure to evaluate the correctness of the estimated ranking in comparison to the ground truth rankings. This statistical method is particularly relevant to our study, as outlined in Section\u00a0<a href=\"#S2\" title=\"II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> because we proposed <math id=\"S4.SS1.p7.1.m1.1\" class=\"ltx_Math\" alttext=\"tau\\_c\" display=\"inline\"><semantics id=\"S4.SS1.p7.1.m1.1a\"><mrow id=\"S4.SS1.p7.1.m1.1.1\" xref=\"S4.SS1.p7.1.m1.1.1.cmml\"><mi id=\"S4.SS1.p7.1.m1.1.1.2\" xref=\"S4.SS1.p7.1.m1.1.1.2.cmml\">t</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.p7.1.m1.1.1.1\" xref=\"S4.SS1.p7.1.m1.1.1.1.cmml\">\u200b</mo><mi id=\"S4.SS1.p7.1.m1.1.1.3\" xref=\"S4.SS1.p7.1.m1.1.1.3.cmml\">a</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.p7.1.m1.1.1.1a\" xref=\"S4.SS1.p7.1.m1.1.1.1.cmml\">\u200b</mo><mi id=\"S4.SS1.p7.1.m1.1.1.4\" xref=\"S4.SS1.p7.1.m1.1.1.4.cmml\">u</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.p7.1.m1.1.1.1b\" xref=\"S4.SS1.p7.1.m1.1.1.1.cmml\">\u200b</mo><mi mathvariant=\"normal\" id=\"S4.SS1.p7.1.m1.1.1.5\" xref=\"S4.SS1.p7.1.m1.1.1.5.cmml\">_</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.SS1.p7.1.m1.1.1.1c\" xref=\"S4.SS1.p7.1.m1.1.1.1.cmml\">\u200b</mo><mi id=\"S4.SS1.p7.1.m1.1.1.6\" xref=\"S4.SS1.p7.1.m1.1.1.6.cmml\">c</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p7.1.m1.1b\"><apply id=\"S4.SS1.p7.1.m1.1.1.cmml\" xref=\"S4.SS1.p7.1.m1.1.1\"><times id=\"S4.SS1.p7.1.m1.1.1.1.cmml\" xref=\"S4.SS1.p7.1.m1.1.1.1\"></times><ci id=\"S4.SS1.p7.1.m1.1.1.2.cmml\" xref=\"S4.SS1.p7.1.m1.1.1.2\">\ud835\udc61</ci><ci id=\"S4.SS1.p7.1.m1.1.1.3.cmml\" xref=\"S4.SS1.p7.1.m1.1.1.3\">\ud835\udc4e</ci><ci id=\"S4.SS1.p7.1.m1.1.1.4.cmml\" xref=\"S4.SS1.p7.1.m1.1.1.4\">\ud835\udc62</ci><ci id=\"S4.SS1.p7.1.m1.1.1.5.cmml\" xref=\"S4.SS1.p7.1.m1.1.1.5\">_</ci><ci id=\"S4.SS1.p7.1.m1.1.1.6.cmml\" xref=\"S4.SS1.p7.1.m1.1.1.6\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p7.1.m1.1c\">tau\\_c</annotation></semantics></math> equal to <math id=\"S4.SS1.p7.2.m2.1\" class=\"ltx_Math\" alttext=\"0.80\" display=\"inline\"><semantics id=\"S4.SS1.p7.2.m2.1a\"><mn id=\"S4.SS1.p7.2.m2.1.1\" xref=\"S4.SS1.p7.2.m2.1.1.cmml\">0.80</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p7.2.m2.1b\"><cn type=\"float\" id=\"S4.SS1.p7.2.m2.1.1.cmml\" xref=\"S4.SS1.p7.2.m2.1.1\">0.80</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p7.2.m2.1c\">0.80</annotation></semantics></math> for the agreement among human evaluators on the task. The tau-c correlation offers a robust framework for assessing the ordinal association between the ranked lists, enabling us to quantify the extent to which our model\u2019s predictions align with the consensus rankings derived from human judgments. This approach validates the predictive accurateness in terms of ranking and underscores the importance of considering human evaluative consistency in our analysis.</p>\n</div>\n</section>\n<section id=\"S4.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S4.SS2.5.1.1\" class=\"ltx_text\">IV-B</span> </span><span id=\"S4.SS2.6.2\" class=\"ltx_text ltx_font_italic\">Training procedure</span>\n</h3>\n\n<div id=\"S4.SS2.p1\" class=\"ltx_para\">\n<p id=\"S4.SS2.p1.3\" class=\"ltx_p\">In the training procedure for our models, aimed at estimating musical difficulty, we utilize the Adam optimizer with a learning rate of <math id=\"S4.SS2.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"10^{-3}\" display=\"inline\"><semantics id=\"S4.SS2.p1.1.m1.1a\"><msup id=\"S4.SS2.p1.1.m1.1.1\" xref=\"S4.SS2.p1.1.m1.1.1.cmml\"><mn id=\"S4.SS2.p1.1.m1.1.1.2\" xref=\"S4.SS2.p1.1.m1.1.1.2.cmml\">10</mn><mrow id=\"S4.SS2.p1.1.m1.1.1.3\" xref=\"S4.SS2.p1.1.m1.1.1.3.cmml\"><mo id=\"S4.SS2.p1.1.m1.1.1.3a\" xref=\"S4.SS2.p1.1.m1.1.1.3.cmml\">\u2212</mo><mn id=\"S4.SS2.p1.1.m1.1.1.3.2\" xref=\"S4.SS2.p1.1.m1.1.1.3.2.cmml\">3</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p1.1.m1.1b\"><apply id=\"S4.SS2.p1.1.m1.1.1.cmml\" xref=\"S4.SS2.p1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS2.p1.1.m1.1.1.1.cmml\" xref=\"S4.SS2.p1.1.m1.1.1\">superscript</csymbol><cn type=\"integer\" id=\"S4.SS2.p1.1.m1.1.1.2.cmml\" xref=\"S4.SS2.p1.1.m1.1.1.2\">10</cn><apply id=\"S4.SS2.p1.1.m1.1.1.3.cmml\" xref=\"S4.SS2.p1.1.m1.1.1.3\"><minus id=\"S4.SS2.p1.1.m1.1.1.3.1.cmml\" xref=\"S4.SS2.p1.1.m1.1.1.3\"></minus><cn type=\"integer\" id=\"S4.SS2.p1.1.m1.1.1.3.2.cmml\" xref=\"S4.SS2.p1.1.m1.1.1.3.2\">3</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p1.1.m1.1c\">10^{-3}</annotation></semantics></math>. The training process incorporates early stopping, based on the Acc<sub id=\"S4.SS2.p1.3.1\" class=\"ltx_sub\">0</sub> and MSE metrics from the validation set, to prevent overfitting. We address data imbalance by using a balanced sampler and applying Ordinal Loss to frame difficulty prediction as an ordinal classification task. This approach remains consistent across multi-task settings, where the task involving composer recognition uses cross-entropy for the composer aspect and Ordinal Loss for difficulty estimation and era classification. Regularization strategies, including gradient clipping at <math id=\"S4.SS2.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"10^{-4}\" display=\"inline\"><semantics id=\"S4.SS2.p1.3.m3.1a\"><msup id=\"S4.SS2.p1.3.m3.1.1\" xref=\"S4.SS2.p1.3.m3.1.1.cmml\"><mn id=\"S4.SS2.p1.3.m3.1.1.2\" xref=\"S4.SS2.p1.3.m3.1.1.2.cmml\">10</mn><mrow id=\"S4.SS2.p1.3.m3.1.1.3\" xref=\"S4.SS2.p1.3.m3.1.1.3.cmml\"><mo id=\"S4.SS2.p1.3.m3.1.1.3a\" xref=\"S4.SS2.p1.3.m3.1.1.3.cmml\">\u2212</mo><mn id=\"S4.SS2.p1.3.m3.1.1.3.2\" xref=\"S4.SS2.p1.3.m3.1.1.3.2.cmml\">4</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p1.3.m3.1b\"><apply id=\"S4.SS2.p1.3.m3.1.1.cmml\" xref=\"S4.SS2.p1.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS2.p1.3.m3.1.1.1.cmml\" xref=\"S4.SS2.p1.3.m3.1.1\">superscript</csymbol><cn type=\"integer\" id=\"S4.SS2.p1.3.m3.1.1.2.cmml\" xref=\"S4.SS2.p1.3.m3.1.1.2\">10</cn><apply id=\"S4.SS2.p1.3.m3.1.1.3.cmml\" xref=\"S4.SS2.p1.3.m3.1.1.3\"><minus id=\"S4.SS2.p1.3.m3.1.1.3.1.cmml\" xref=\"S4.SS2.p1.3.m3.1.1.3\"></minus><cn type=\"integer\" id=\"S4.SS2.p1.3.m3.1.1.3.2.cmml\" xref=\"S4.SS2.p1.3.m3.1.1.3.2\">4</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p1.3.m3.1c\">10^{-4}</annotation></semantics></math> and L2 regularization, along with a batch size of 16, are employed to ensure stable training conditions.</p>\n</div>\n<div id=\"S4.SS2.p2\" class=\"ltx_para\">\n<p id=\"S4.SS2.p2.1\" class=\"ltx_p\">Optimizing these training parameters and strategies further could potentially enhance model performance, but such investigations are reserved for future research.</p>\n</div>\n<div id=\"S4.SS2.p3\" class=\"ltx_para\">\n<p id=\"S4.SS2.p3.1\" class=\"ltx_p\">For analyzing piano roll and CQT representations, we select a frame rate of 5 frames per second. This decision balances the need for operational efficiency with the requirement to process full audio sequences, given the task annotations focus on entire musical pieces. Initial experiments indicate that this frame rate allows for accurate difficulty prediction. However, we acknowledge exploring higher frame rates could improve model performance by providing more detailed temporal data.</p>\n</div>\n<div id=\"S4.SS2.p4\" class=\"ltx_para\">\n<p id=\"S4.SS2.p4.1\" class=\"ltx_p\">In our CQT analysis, we set the hop length to 160 to achieve detailed temporal resolution. The CQT extraction involves specific parameters, such as bins equal to 88 and bins per octave equal to 12, reflecting the piano\u2019s frequency range. After extraction, we apply a logarithmic transformation to the amplitude of the CQT to obtain the log CQT, which is then transposed for model input. This process ensures that our models receive a comprehensive and nuanced representation of the audio content for effective analysis.</p>\n</div>\n<section id=\"S5\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">V </span><span id=\"S5.4.1\" class=\"ltx_text ltx_font_smallcaps\">Experiments</span>\n</h2>\n\n<div id=\"S5.p1\" class=\"ltx_para\">\n<p id=\"S5.p1.1\" class=\"ltx_p\">In our comparative analysis of input strategies, as detailed in Table\u00a0<a href=\"#S6.SS1\" title=\"VI-A Model performance on works by female composers \u2023 VI A case study on woman composers \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">VI-A</span></span></a>, we explore both monomodal and multimodal methods. Initially, we assess monomodal strategies by comparing the performance of the Constant-Q Transform (CQT) and piano roll (PR) representations. Our findings show that PR surpasses CQT, with a lower Mean Squared Error (MSE) of 1.85 compared to CQT\u2019s 2.29. This indicates PR\u2019s greater effectiveness in identifying patterns crucial for the task. Furthermore, PR demonstrates superior performance in other key metrics, achieving a higher zero-error accuracy (Acc<sub id=\"S5.p1.1.1\" class=\"ltx_sub\">0</sub>) of 36.7% versus CQT\u2019s 32.9%, and a better Tau-c score of .771 compared to .741 for CQT. These results highlight PR\u2019s enhanced capability in accurately understanding and processing task-specific patterns, making it a more effective choice in monomodal learning contexts.</p>\n</div>\n<div id=\"S5.p2\" class=\"ltx_para\">\n<p id=\"S5.p2.2\" class=\"ltx_p\">Transitioning to multimodal strategies, the MM experiment is described in Section\u00a0<a href=\"#S3.SS3\" title=\"III-C Multimodal approach \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-C</span></span></a> and ENSEMBLE average the predictions of CQT AND PR. In table\u00a0<a href=\"#S6.SS1\" title=\"VI-A Model performance on works by female composers \u2023 VI A case study on woman composers \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">VI-A</span></span></a>, the MM experiment outperformed both monomodal methods, recording the lowest MSE at 1.81 and the highest Acc<sub id=\"S5.p2.2.1\" class=\"ltx_sub\">0</sub> of 37.3%. This underscores the benefits of leveraging combined data representations for enhanced performance. Despite achieving a slightly higher MSE of 1.82 and a marginally lower Acc<sub id=\"S5.p2.2.2\" class=\"ltx_sub\">0</sub> of 36.2% compared to MM, the ENSEMBLE method still exhibited competitive performance, especially notable in its Tau-c score of .777. These numerical comparisons highlight the superiority of multimodal approaches in enhancing model accuracy and robustness. Future research will focus on further optimizing these multimodal configurations for improved performance across all metrics.</p>\n</div>\n<figure id=\"S5.3\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE II: </span>Results training with monomodal representations, CQT and PR, and multimodal ones.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S5.3.3\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:390.3pt;height:219.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(83.0pt,-46.6pt) scale(1.73983164653568,1.73983164653568) ;\">\n<table id=\"S5.3.3.3\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</th>\n<td id=\"S5.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MSE</td>\n<td id=\"S5.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Acc<sub id=\"S5.1.1.1.1.1.1\" class=\"ltx_sub\">0</sub>\n</td>\n<td id=\"S5.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S5.2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S5.2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S5.2.2.2.2.1.1\" class=\"ltx_text ltx_markedasmath\">Monomodal</span></th>\n<td id=\"S5.2.2.2.2.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.2.2.2.2.3\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.2.2.2.2.4\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.3.3.3.4.1\" class=\"ltx_tr\">\n<th id=\"S5.3.3.3.4.1.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.3.3.3.4.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.3.3.3.4.1.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.29(.18)</td>\n<td id=\"S5.3.3.3.4.1.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(1.61)</td>\n<td id=\"S5.3.3.3.4.1.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.741(.012)</td>\n</tr>\n<tr id=\"S5.3.3.3.5.2\" class=\"ltx_tr\">\n<th id=\"S5.3.3.3.5.2.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.3.3.3.5.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.3.3.3.5.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.85(.07)</td>\n<td id=\"S5.3.3.3.5.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.7(.84)</td>\n<td id=\"S5.3.3.3.5.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.771(.005)</td>\n</tr>\n<tr id=\"S5.3.3.3.3\" class=\"ltx_tr\">\n<th id=\"S5.3.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S5.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S5.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Multimodal</span>\n</th>\n<td id=\"S5.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.3.3.3.3.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.3.3.3.3.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.3.3.3.6.3\" class=\"ltx_tr\">\n<th id=\"S5.3.3.3.6.3.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.3.3.3.6.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S5.3.3.3.6.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.3.3.3.6.3.3.1\" class=\"ltx_text ltx_font_bold\">1.81(.11)</span></td>\n<td id=\"S5.3.3.3.6.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.3.3.3.6.3.4.1\" class=\"ltx_text ltx_font_bold\">37.3(1.97)</span></td>\n<td id=\"S5.3.3.3.6.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.3.3.3.6.3.5.1\" class=\"ltx_text ltx_font_bold\">.778(.004)</span></td>\n</tr>\n<tr id=\"S5.3.3.3.7.4\" class=\"ltx_tr\">\n<th id=\"S5.3.3.3.7.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.3.3.3.7.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">ENSEMBLE</th>\n<td id=\"S5.3.3.3.7.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.82(.09)</td>\n<td id=\"S5.3.3.3.7.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.2(.58)</td>\n<td id=\"S5.3.3.3.7.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.777(.006)</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S5.SS1\" class=\"ltx_subsection ltx_figure_panel\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S5.SS1.25.1.1\" class=\"ltx_text\">V-A</span> </span><span id=\"S5.SS1.26.2\" class=\"ltx_text ltx_font_italic\">Auxiliary tasks for pretraining</span>\n</h3>\n\n<figure id=\"S5.SS1.20\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE III: </span>Multi-tasks experiments training the models on the PSyllabus dataset and auxiliary tasks. </figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S5.SS1.11.11\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:390.3pt;height:416.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(85.5pt,-91.3pt) scale(1.7799857981267,1.7799857981267) ;\">\n<table id=\"S5.SS1.11.11.11\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.SS1.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.SS1.1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</th>\n<td id=\"S5.SS1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MSE</td>\n<td id=\"S5.SS1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Acc<sub id=\"S5.SS1.1.1.1.1.1.1\" class=\"ltx_sub\">0</sub>\n</td>\n<td id=\"S5.SS1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S5.SS1.2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S5.SS1.2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S5.SS1.2.2.2.2.1.1\" class=\"ltx_text ltx_markedasmath\">Single-task</span></th>\n<td id=\"S5.SS1.2.2.2.2.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.2.2.2.2.3\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.2.2.2.2.4\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.SS1.11.11.11.12.1\" class=\"ltx_tr\">\n<th id=\"S5.SS1.11.11.11.12.1.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.11.11.11.12.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.SS1.11.11.11.12.1.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.29(.18)</td>\n<td id=\"S5.SS1.11.11.11.12.1.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(1.61)</td>\n<td id=\"S5.SS1.11.11.11.12.1.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.741(.012)</td>\n</tr>\n<tr id=\"S5.SS1.11.11.11.13.2\" class=\"ltx_tr\">\n<th id=\"S5.SS1.11.11.11.13.2.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.11.11.11.13.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.SS1.11.11.11.13.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.85(.07)</td>\n<td id=\"S5.SS1.11.11.11.13.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.7(.84)</td>\n<td id=\"S5.SS1.11.11.11.13.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.771(.005)</td>\n</tr>\n<tr id=\"S5.SS1.3.3.3.3\" class=\"ltx_tr\">\n<th id=\"S5.SS1.3.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S5.SS1.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S5.SS1.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with era</span>\n</th>\n<td id=\"S5.SS1.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.3.3.3.3.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.3.3.3.3.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.SS1.4.4.4.4\" class=\"ltx_tr\">\n<th id=\"S5.SS1.4.4.4.4.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.4.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT<sub id=\"S5.SS1.4.4.4.4.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.4.4.4.4.1.1.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub>\n</th>\n<td id=\"S5.SS1.4.4.4.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.99(.15)</td>\n<td id=\"S5.SS1.4.4.4.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.6(.97)</td>\n<td id=\"S5.SS1.4.4.4.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.766(.008)</td>\n</tr>\n<tr id=\"S5.SS1.5.5.5.5\" class=\"ltx_tr\">\n<th id=\"S5.SS1.5.5.5.5.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.5.5.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR<sub id=\"S5.SS1.5.5.5.5.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.5.5.5.5.1.1.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub>\n</th>\n<td id=\"S5.SS1.5.5.5.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.SS1.5.5.5.5.3.1\" class=\"ltx_text ltx_font_bold\">1.83(.10)</span></td>\n<td id=\"S5.SS1.5.5.5.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.SS1.5.5.5.5.4.1\" class=\"ltx_text ltx_font_bold\">37.7(.91)</span></td>\n<td id=\"S5.SS1.5.5.5.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.SS1.5.5.5.5.5.1\" class=\"ltx_text ltx_font_bold\">.775(.008)</span></td>\n</tr>\n<tr id=\"S5.SS1.6.6.6.6\" class=\"ltx_tr\">\n<th id=\"S5.SS1.6.6.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S5.SS1.6.6.6.6.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S5.SS1.6.6.6.6.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with composer</span>\n</th>\n<td id=\"S5.SS1.6.6.6.6.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.6.6.6.6.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.6.6.6.6.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.SS1.7.7.7.7\" class=\"ltx_tr\">\n<th id=\"S5.SS1.7.7.7.7.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.7.7.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT<sub id=\"S5.SS1.7.7.7.7.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.7.7.7.7.1.1.1\" class=\"ltx_text ltx_font_italic\">Composer</span></sub>\n</th>\n<td id=\"S5.SS1.7.7.7.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.74(.16)</td>\n<td id=\"S5.SS1.7.7.7.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">27.0(1.09)</td>\n<td id=\"S5.SS1.7.7.7.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.701(.008)</td>\n</tr>\n<tr id=\"S5.SS1.8.8.8.8\" class=\"ltx_tr\">\n<th id=\"S5.SS1.8.8.8.8.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.8.8.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR<sub id=\"S5.SS1.8.8.8.8.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.8.8.8.8.1.1.1\" class=\"ltx_text ltx_font_italic\">Composer</span></sub>\n</th>\n<td id=\"S5.SS1.8.8.8.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.57(.18)</td>\n<td id=\"S5.SS1.8.8.8.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.0(1.09)</td>\n<td id=\"S5.SS1.8.8.8.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.708(.007)</td>\n</tr>\n<tr id=\"S5.SS1.9.9.9.9\" class=\"ltx_tr\">\n<th id=\"S5.SS1.9.9.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S5.SS1.9.9.9.9.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S5.SS1.9.9.9.9.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with multiple ranks</span>\n</th>\n<td id=\"S5.SS1.9.9.9.9.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.9.9.9.9.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.9.9.9.9.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.SS1.10.10.10.10\" class=\"ltx_tr\">\n<th id=\"S5.SS1.10.10.10.10.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.10.10.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT<sub id=\"S5.SS1.10.10.10.10.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.10.10.10.10.1.1.1\" class=\"ltx_text ltx_font_italic\">MultiRank</span></sub>\n</th>\n<td id=\"S5.SS1.10.10.10.10.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.16(.16)</td>\n<td id=\"S5.SS1.10.10.10.10.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.4(1.35)</td>\n<td id=\"S5.SS1.10.10.10.10.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.754(.007)</td>\n</tr>\n<tr id=\"S5.SS1.11.11.11.11\" class=\"ltx_tr\">\n<th id=\"S5.SS1.11.11.11.11.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.11.11.11.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR<sub id=\"S5.SS1.11.11.11.11.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.11.11.11.11.1.1.1\" class=\"ltx_text ltx_font_italic\">MultiRank</span></sub>\n</th>\n<td id=\"S5.SS1.11.11.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.88(.06)</td>\n<td id=\"S5.SS1.11.11.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.4(.61)</td>\n<td id=\"S5.SS1.11.11.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.768(.003)</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.20.21\" class=\"ltx_p ltx_figure_panel\">In our exploration of multi-task learning\u2019s influence on model efficacy, we conducted a series of experiments shown in Table\u00a0<a href=\"#S5\" title=\"V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>. These experiments aim to measure the impact of incorporating auxiliary tasks\u2014namely, musical era, composer identification, and multiple rankings\u2014on models trained using the PSyllabus difficulty labels.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.20.22\" class=\"ltx_p ltx_figure_panel\">Initial single-task experiments, described in Section\u00a0<a href=\"#S5\" title=\"V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> employing CQT and PR, established our baseline. Among these, PR emerged as the more potent baseline.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.15.15\" class=\"ltx_p ltx_figure_panel\">The integration of multi-task learning, particularly with the musical era (represented as CQT<sub id=\"S5.SS1.15.15.1\" class=\"ltx_sub\"><span id=\"S5.SS1.15.15.1.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub> and PR<sub id=\"S5.SS1.15.15.2\" class=\"ltx_sub\"><span id=\"S5.SS1.15.15.2.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub>), resulted in discernible performance enhancements. PR<sub id=\"S5.SS1.15.15.3\" class=\"ltx_sub\"><span id=\"S5.SS1.15.15.3.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub>, for instance, manifested a reduction in MSE to 1.83 and an increase in <math id=\"S5.SS1.15.15.m4.1\" class=\"ltx_Math\" alttext=\"Acc_{0}\" display=\"inline\"><semantics id=\"S5.SS1.15.15.m4.1a\"><mrow id=\"S5.SS1.15.15.m4.1.1\" xref=\"S5.SS1.15.15.m4.1.1.cmml\"><mi id=\"S5.SS1.15.15.m4.1.1.2\" xref=\"S5.SS1.15.15.m4.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.SS1.15.15.m4.1.1.1\" xref=\"S5.SS1.15.15.m4.1.1.1.cmml\">\u200b</mo><mi id=\"S5.SS1.15.15.m4.1.1.3\" xref=\"S5.SS1.15.15.m4.1.1.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.SS1.15.15.m4.1.1.1a\" xref=\"S5.SS1.15.15.m4.1.1.1.cmml\">\u200b</mo><msub id=\"S5.SS1.15.15.m4.1.1.4\" xref=\"S5.SS1.15.15.m4.1.1.4.cmml\"><mi id=\"S5.SS1.15.15.m4.1.1.4.2\" xref=\"S5.SS1.15.15.m4.1.1.4.2.cmml\">c</mi><mn id=\"S5.SS1.15.15.m4.1.1.4.3\" xref=\"S5.SS1.15.15.m4.1.1.4.3.cmml\">0</mn></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.15.15.m4.1b\"><apply id=\"S5.SS1.15.15.m4.1.1.cmml\" xref=\"S5.SS1.15.15.m4.1.1\"><times id=\"S5.SS1.15.15.m4.1.1.1.cmml\" xref=\"S5.SS1.15.15.m4.1.1.1\"></times><ci id=\"S5.SS1.15.15.m4.1.1.2.cmml\" xref=\"S5.SS1.15.15.m4.1.1.2\">\ud835\udc34</ci><ci id=\"S5.SS1.15.15.m4.1.1.3.cmml\" xref=\"S5.SS1.15.15.m4.1.1.3\">\ud835\udc50</ci><apply id=\"S5.SS1.15.15.m4.1.1.4.cmml\" xref=\"S5.SS1.15.15.m4.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.15.15.m4.1.1.4.1.cmml\" xref=\"S5.SS1.15.15.m4.1.1.4\">subscript</csymbol><ci id=\"S5.SS1.15.15.m4.1.1.4.2.cmml\" xref=\"S5.SS1.15.15.m4.1.1.4.2\">\ud835\udc50</ci><cn type=\"integer\" id=\"S5.SS1.15.15.m4.1.1.4.3.cmml\" xref=\"S5.SS1.15.15.m4.1.1.4.3\">0</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.15.15.m4.1c\">Acc_{0}</annotation></semantics></math> to 37.7%, illustrating the advantages of embedding contextual musical insights into the learning process.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.17.17\" class=\"ltx_p ltx_figure_panel\">Nevertheless, when composer identification was introduced as an auxiliary task (via CQT<sub id=\"S5.SS1.17.17.1\" class=\"ltx_sub\"><span id=\"S5.SS1.17.17.1.1\" class=\"ltx_text ltx_font_italic\">Composer</span></sub> and PR<sub id=\"S5.SS1.17.17.2\" class=\"ltx_sub\"><span id=\"S5.SS1.17.17.2.1\" class=\"ltx_text ltx_font_italic\">Composer</span></sub>), we observed a decrease in performance relative to the baseline. This outcome suggests that the added complexity from composer identification might not synergize well with the primary task, possibly because the diverse stylistic idiosyncrasies of composers could impede the model\u2019s generalization capabilities.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.20.20\" class=\"ltx_p ltx_figure_panel\">Experiments involving the multiple ranking task (CQT<sub id=\"S5.SS1.20.20.1\" class=\"ltx_sub\"><span id=\"S5.SS1.20.20.1.1\" class=\"ltx_text ltx_font_italic\">MultiRank</span></sub> and PR<sub id=\"S5.SS1.20.20.2\" class=\"ltx_sub\"><span id=\"S5.SS1.20.20.2.1\" class=\"ltx_text ltx_font_italic\">MultiRank</span></sub>) yielded mixed outcomes. Although there was a slight improvement in MSE and <math id=\"S5.SS1.20.20.m3.1\" class=\"ltx_Math\" alttext=\"Acc_{0}\" display=\"inline\"><semantics id=\"S5.SS1.20.20.m3.1a\"><mrow id=\"S5.SS1.20.20.m3.1.1\" xref=\"S5.SS1.20.20.m3.1.1.cmml\"><mi id=\"S5.SS1.20.20.m3.1.1.2\" xref=\"S5.SS1.20.20.m3.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.SS1.20.20.m3.1.1.1\" xref=\"S5.SS1.20.20.m3.1.1.1.cmml\">\u200b</mo><mi id=\"S5.SS1.20.20.m3.1.1.3\" xref=\"S5.SS1.20.20.m3.1.1.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.SS1.20.20.m3.1.1.1a\" xref=\"S5.SS1.20.20.m3.1.1.1.cmml\">\u200b</mo><msub id=\"S5.SS1.20.20.m3.1.1.4\" xref=\"S5.SS1.20.20.m3.1.1.4.cmml\"><mi id=\"S5.SS1.20.20.m3.1.1.4.2\" xref=\"S5.SS1.20.20.m3.1.1.4.2.cmml\">c</mi><mn id=\"S5.SS1.20.20.m3.1.1.4.3\" xref=\"S5.SS1.20.20.m3.1.1.4.3.cmml\">0</mn></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.20.20.m3.1b\"><apply id=\"S5.SS1.20.20.m3.1.1.cmml\" xref=\"S5.SS1.20.20.m3.1.1\"><times id=\"S5.SS1.20.20.m3.1.1.1.cmml\" xref=\"S5.SS1.20.20.m3.1.1.1\"></times><ci id=\"S5.SS1.20.20.m3.1.1.2.cmml\" xref=\"S5.SS1.20.20.m3.1.1.2\">\ud835\udc34</ci><ci id=\"S5.SS1.20.20.m3.1.1.3.cmml\" xref=\"S5.SS1.20.20.m3.1.1.3\">\ud835\udc50</ci><apply id=\"S5.SS1.20.20.m3.1.1.4.cmml\" xref=\"S5.SS1.20.20.m3.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.20.20.m3.1.1.4.1.cmml\" xref=\"S5.SS1.20.20.m3.1.1.4\">subscript</csymbol><ci id=\"S5.SS1.20.20.m3.1.1.4.2.cmml\" xref=\"S5.SS1.20.20.m3.1.1.4.2\">\ud835\udc50</ci><cn type=\"integer\" id=\"S5.SS1.20.20.m3.1.1.4.3.cmml\" xref=\"S5.SS1.20.20.m3.1.1.4.3\">0</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.20.20.m3.1c\">Acc_{0}</annotation></semantics></math> compared to the single-task baseline, these enhancements were not as pronounced as those observed with the musical era task, indicating a moderate benefit from incorporating ranking information. The outcomes for multiple ranking tasks stand in contrast to those involving music sheet images in previous research\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite>, where the multi-rank approach outperformed other experiments. However, a direct comparison between these results is not straightforward due to differences in the datasets used, and further research is needed to compare both approaches.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S5.SS1.SSS0.Px1\" class=\"ltx_paragraph ltx_figure_panel\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Impact of Musical Era on Difficulty Estimation.</h5>\n\n<div id=\"S5.SS1.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S5.SS1.SSS0.Px1.p1.1\" class=\"ltx_p\">Our analysis, as shown in Table\u00a0<a href=\"#S5.T4\" title=\"TABLE IV \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, investigates the auxiliary task of era identification on difficulty prediction across musical periods. The inclusion of era significantly enhances model performance, as evidenced by improvements in Mean Squared Error (MSE), Accuracy at zero (Acc<sub id=\"S5.SS1.SSS0.Px1.p1.1.1\" class=\"ltx_sub\">0</sub>), and Tau-c metrics.</p>\n</div>\n<div id=\"S5.SS1.SSS0.Px1.p2\" class=\"ltx_para\">\n<p id=\"S5.SS1.SSS0.Px1.p2.2\" class=\"ltx_p\">For the Baroque period, we observed MSE improvements from 2.73 to 2.49 (CQT) and 2.25 to 2.03 (PR), with corresponding increases in Acc<sub id=\"S5.SS1.SSS0.Px1.p2.2.1\" class=\"ltx_sub\">0</sub>. The Classical period saw similar enhancements, with MSE decreasing to 1.96 (CQT) and Acc<sub id=\"S5.SS1.SSS0.Px1.p2.2.2\" class=\"ltx_sub\">0</sub> improving across methods. The Romantic and Modern periods further confirmed these trends, demonstrating the general efficacy of integrating era information into difficulty estimation models.</p>\n</div>\n<div id=\"S5.SS1.SSS0.Px1.p3\" class=\"ltx_para\">\n<p id=\"S5.SS1.SSS0.Px1.p3.1\" class=\"ltx_p\">These findings highlight the significance of era-specific characteristics in music difficulty prediction, suggesting a nuanced relationship between historical context and model accuracy. The varied improvements across periods underscore the potential of tailored, era-specific modeling approaches to enhance prediction accuracy.</p>\n</div>\n<figure id=\"S5.T4\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE IV: </span>Comparative analysis for Basic and Multi-task with Era experiments across musical periods.</figcaption>\n<div id=\"S5.T4.1\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:346.9pt;height:182.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-31.4pt,16.6pt) scale(0.846604287885487,0.846604287885487) ;\">\n<table id=\"S5.T4.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Period</th>\n<th id=\"S5.T4.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Method</th>\n<th id=\"S5.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">MSE</th>\n<th id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Acc<sub id=\"S5.T4.1.1.1.1.1\" class=\"ltx_sub\">0</sub>\n</th>\n<th id=\"S5.T4.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Tau-c</th>\n</tr>\n<tr id=\"S5.T4.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.2.1.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.T4.1.1.2.1.2\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.T4.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Single</th>\n<th id=\"S5.T4.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">with Era</th>\n<th id=\"S5.T4.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Single</th>\n<th id=\"S5.T4.1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">with Era</th>\n<th id=\"S5.T4.1.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Single</th>\n<th id=\"S5.T4.1.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">with Era</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.3.1.1.1\" class=\"ltx_text\">Baroque</span></th>\n<th id=\"S5.T4.1.1.3.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.73(.49)</td>\n<td id=\"S5.T4.1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.49(.42)</td>\n<td id=\"S5.T4.1.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.1(4.82)</td>\n<td id=\"S5.T4.1.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.3(5.51)</td>\n<td id=\"S5.T4.1.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.621(.041)</td>\n<td id=\"S5.T4.1.1.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.631(.036)</td>\n</tr>\n<tr id=\"S5.T4.1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.4.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.25(.50)</td>\n<td id=\"S5.T4.1.1.4.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.4.2.3.1\" class=\"ltx_text ltx_font_bold\">2.03(.62)</span></td>\n<td id=\"S5.T4.1.1.4.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(4.69)</td>\n<td id=\"S5.T4.1.1.4.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.4.2.5.1\" class=\"ltx_text ltx_font_bold\">34.2(6.46)</span></td>\n<td id=\"S5.T4.1.1.4.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.702(.066)</td>\n<td id=\"S5.T4.1.1.4.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.4.2.7.1\" class=\"ltx_text ltx_font_bold\">.707(.088)</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.5.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.5.3.1.1\" class=\"ltx_text\">Classical</span></th>\n<th id=\"S5.T4.1.1.5.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.25(.66)</td>\n<td id=\"S5.T4.1.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.5.3.4.1\" class=\"ltx_text ltx_font_bold\">1.96(.51)</span></td>\n<td id=\"S5.T4.1.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.2(6.06)</td>\n<td id=\"S5.T4.1.1.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.1(5.55)</td>\n<td id=\"S5.T4.1.1.5.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.698(.088)</td>\n<td id=\"S5.T4.1.1.5.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.719(.073)</td>\n</tr>\n<tr id=\"S5.T4.1.1.6.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.6.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.97(.79)</td>\n<td id=\"S5.T4.1.1.6.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.98(.45)</td>\n<td id=\"S5.T4.1.1.6.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.2(8.23)</td>\n<td id=\"S5.T4.1.1.6.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.6.4.5.1\" class=\"ltx_text ltx_font_bold\">34.3(4.90)</span></td>\n<td id=\"S5.T4.1.1.6.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.726(.093)</td>\n<td id=\"S5.T4.1.1.6.4.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.6.4.7.1\" class=\"ltx_text ltx_font_bold\">.732(.067)</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.7.5\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.7.5.1.1\" class=\"ltx_text\">Romantic</span></th>\n<th id=\"S5.T4.1.1.7.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.16(.56)</td>\n<td id=\"S5.T4.1.1.7.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.05(.44)</td>\n<td id=\"S5.T4.1.1.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.6(5.41)</td>\n<td id=\"S5.T4.1.1.7.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.1(4.50)</td>\n<td id=\"S5.T4.1.1.7.5.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.704(.071)</td>\n<td id=\"S5.T4.1.1.7.5.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.724(.062)</td>\n</tr>\n<tr id=\"S5.T4.1.1.8.6\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.8.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.8.6.2.1\" class=\"ltx_text ltx_font_bold\">1.84(.66)</span></td>\n<td id=\"S5.T4.1.1.8.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.87(.63)</td>\n<td id=\"S5.T4.1.1.8.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.2(6.92)</td>\n<td id=\"S5.T4.1.1.8.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.8.6.5.1\" class=\"ltx_text ltx_font_bold\">35.4(7.59)</span></td>\n<td id=\"S5.T4.1.1.8.6.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.8.6.6.1\" class=\"ltx_text ltx_font_bold\">.739(.077)</span></td>\n<td id=\"S5.T4.1.1.8.6.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.730(.085)</td>\n</tr>\n<tr id=\"S5.T4.1.1.9.7\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.9.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.9.7.1.1\" class=\"ltx_text\">20th Century</span></th>\n<th id=\"S5.T4.1.1.9.7.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.9.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.16(.49)</td>\n<td id=\"S5.T4.1.1.9.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.33(.42)</td>\n<td id=\"S5.T4.1.1.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">33.3(4.94)</td>\n<td id=\"S5.T4.1.1.9.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">30.3(6.58)</td>\n<td id=\"S5.T4.1.1.9.7.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.715(.065)</td>\n<td id=\"S5.T4.1.1.9.7.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.658(.043)</td>\n</tr>\n<tr id=\"S5.T4.1.1.10.8\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.10.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.10.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.86(.57)</td>\n<td id=\"S5.T4.1.1.10.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.10.8.3.1\" class=\"ltx_text ltx_font_bold\">1.78(.52)</span></td>\n<td id=\"S5.T4.1.1.10.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.6(6.07)</td>\n<td id=\"S5.T4.1.1.10.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.10.8.5.1\" class=\"ltx_text ltx_font_bold\">37.3(6.95)</span></td>\n<td id=\"S5.T4.1.1.10.8.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.10.8.6.1\" class=\"ltx_text ltx_font_bold\">.745(.068)</span></td>\n<td id=\"S5.T4.1.1.10.8.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.742(.070)</td>\n</tr>\n<tr id=\"S5.T4.1.1.11.9\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.11.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.11.9.1.1\" class=\"ltx_text\">Modern</span></th>\n<th id=\"S5.T4.1.1.11.9.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.11.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.25(.50)</td>\n<td id=\"S5.T4.1.1.11.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.11.9.4.1\" class=\"ltx_text ltx_font_bold\">1.84(.47)</span></td>\n<td id=\"S5.T4.1.1.11.9.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(4.69)</td>\n<td id=\"S5.T4.1.1.11.9.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.11.9.6.1\" class=\"ltx_text ltx_font_bold\">37.0(6.10)</span></td>\n<td id=\"S5.T4.1.1.11.9.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.702(.066)</td>\n<td id=\"S5.T4.1.1.11.9.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.11.9.8.1\" class=\"ltx_text ltx_font_bold\">.747(.061)</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.12.10\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.12.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.12.10.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.94(.57)</td>\n<td id=\"S5.T4.1.1.12.10.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.92(.50)</td>\n<td id=\"S5.T4.1.1.12.10.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.8(5.54)</td>\n<td id=\"S5.T4.1.1.12.10.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.3(5.63)</td>\n<td id=\"S5.T4.1.1.12.10.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.736(.063)</td>\n<td id=\"S5.T4.1.1.12.10.7\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.739(.057)</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n<section id=\"S6\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">VI </span><span id=\"S6.1.1\" class=\"ltx_text ltx_font_smallcaps\">A case study on woman composers</span>\n</h2>\n\n<div id=\"S6.p1\" class=\"ltx_para\">\n<p id=\"S6.p1.1\" class=\"ltx_p\">In this section, we explore the capabilities of the models proposed in this paper, specifically focusing on the classification of an unrepresented group: women composers. In Subsection\u00a0<a href=\"#S6.SS1\" title=\"VI-A Model performance on works by female composers \u2023 VI A case study on woman composers \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">VI-A</span></span></a>, we examine the models\u2019 performance across different genders. Subsequently, in Subsection\u00a0<a href=\"#S6.SS2\" title=\"VI-B Zero-shot experiment on HV Benchmark \u2023 VI-A Model performance on works by female composers \u2023 VI A case study on woman composers \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">VI-B</span></span></a>, we assess the generalization capabilities of the proposed models using a benchmark dataset of compositions by black women composers.</p>\n</div>\n<section id=\"S6.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S6.SS1.9.1.1\" class=\"ltx_text\">VI-A</span> </span><span id=\"S6.SS1.10.2\" class=\"ltx_text ltx_font_italic\">Model performance on works by female composers</span>\n</h3>\n\n<div id=\"S6.SS1.p1\" class=\"ltx_para\">\n<p id=\"S6.SS1.p1.1\" class=\"ltx_p\">In this section, we present the results of experiments designed to analyze gender bias in the models proposed in this paper, which were trained on the PSyllabus dataset. We focus on the classification capabilities of these models in distinguishing between compositions by male and female composers.</p>\n</div>\n<div id=\"S6.SS1.p2\" class=\"ltx_para\">\n<p id=\"S6.SS1.p2.1\" class=\"ltx_p\">The analysis reveals that, despite uniform training across all models, performance disparities emerge when testing on gender-specific data. The MM model, when tested on mixed-gender data, exhibited the best performance with an accuracy of 37.3% and a Tau-c of .778. This indicates a well-rounded capability to interpret compositions by both genders. However, a significant performance decline was observed in tests on data exclusively from women composers, with CQT\u2019s Tau-c dropping to .666 and accuracy to 31.6%. This underscores the importance of diverse and balanced evaluation sets to uncover model biases and ensure fair performance, as seen in the contrasted outcomes against gender-specific test data. Further research is needed to understand the performance decrease between male and female compositions, and whether it comes from musical characteristics or labeling biases.</p>\n</div>\n<figure id=\"S6.SS1.4\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE V: </span>Analysis of model performance differentiated by the composer\u2019s gender.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S6.SS1.4.4\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:390.3pt;height:479.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(99.8pt,-122.6pt) scale(2.0479794841506,2.0479794841506) ;\">\n<table id=\"S6.SS1.4.4.4\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.SS1.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.SS1.1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</th>\n<td id=\"S6.SS1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MSE</td>\n<td id=\"S6.SS1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Acc<sub id=\"S6.SS1.1.1.1.1.1.1\" class=\"ltx_sub\">0</sub>\n</td>\n<td id=\"S6.SS1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S6.SS1.2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S6.SS1.2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S6.SS1.2.2.2.2.1.1\" class=\"ltx_text ltx_markedasmath\">Both genres</span></th>\n<td id=\"S6.SS1.2.2.2.2.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.2.2.2.2.3\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.2.2.2.2.4\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.5.1\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.5.1.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.5.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S6.SS1.4.4.4.5.1.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.29(.18)</td>\n<td id=\"S6.SS1.4.4.4.5.1.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(1.61)</td>\n<td id=\"S6.SS1.4.4.4.5.1.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.741(.012)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.6.2\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.6.2.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.6.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S6.SS1.4.4.4.6.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.85(.07)</td>\n<td id=\"S6.SS1.4.4.4.6.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.7(.84)</td>\n<td id=\"S6.SS1.4.4.4.6.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.771(.005)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.7.3\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.7.3.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.7.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S6.SS1.4.4.4.7.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.7.3.3.1\" class=\"ltx_text ltx_font_bold\">1.81(.11)</span></td>\n<td id=\"S6.SS1.4.4.4.7.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.7.3.4.1\" class=\"ltx_text ltx_font_bold\">37.3(1.97)</span></td>\n<td id=\"S6.SS1.4.4.4.7.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.7.3.5.1\" class=\"ltx_text ltx_font_bold\">.778(.004)</span></td>\n</tr>\n<tr id=\"S6.SS1.3.3.3.3\" class=\"ltx_tr\">\n<th id=\"S6.SS1.3.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS1.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS1.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Only men</span>\n</th>\n<td id=\"S6.SS1.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.3.3.3.3.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.3.3.3.3.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.8.4\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.8.4.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.8.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S6.SS1.4.4.4.8.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.24(.19)</td>\n<td id=\"S6.SS1.4.4.4.8.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">33.2(1.13)</td>\n<td id=\"S6.SS1.4.4.4.8.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.734(.012)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.9.5\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.9.5.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.9.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S6.SS1.4.4.4.9.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.83(.05)</td>\n<td id=\"S6.SS1.4.4.4.9.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">37.3(.75)</td>\n<td id=\"S6.SS1.4.4.4.9.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.764(.004)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.10.6\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.10.6.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.10.6.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S6.SS1.4.4.4.10.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.10.6.3.1\" class=\"ltx_text ltx_font_bold\">1.81(.09)</span></td>\n<td id=\"S6.SS1.4.4.4.10.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.10.6.4.1\" class=\"ltx_text ltx_font_bold\">37.7(2.46)</span></td>\n<td id=\"S6.SS1.4.4.4.10.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.10.6.5.1\" class=\"ltx_text ltx_font_bold\">.769(.002)</span></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.4\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS1.4.4.4.4.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS1.4.4.4.4.1.2\" class=\"ltx_text ltx_markedasmath\">Only women</span>\n</th>\n<td id=\"S6.SS1.4.4.4.4.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.4.4.4.4.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.4.4.4.4.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.11.7\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.11.7.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.11.7.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S6.SS1.4.4.4.11.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.47(.59)</td>\n<td id=\"S6.SS1.4.4.4.11.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">31.6(4.74)</td>\n<td id=\"S6.SS1.4.4.4.11.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.666(.032)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.12.8\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.12.8.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.12.8.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S6.SS1.4.4.4.12.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.91(.60)</td>\n<td id=\"S6.SS1.4.4.4.12.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.6(4.24)</td>\n<td id=\"S6.SS1.4.4.4.12.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.722(.021)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.13.9\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.13.9.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.13.9.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S6.SS1.4.4.4.13.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.13.9.3.1\" class=\"ltx_text ltx_font_bold\">1.72(.49)</span></td>\n<td id=\"S6.SS1.4.4.4.13.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.13.9.4.1\" class=\"ltx_text ltx_font_bold\">35.7(5.74)</span></td>\n<td id=\"S6.SS1.4.4.4.13.9.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.13.9.5.1\" class=\"ltx_text ltx_font_bold\">.733(.025)</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S6.SS2\" class=\"ltx_subsection ltx_figure_panel\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S6.SS2.11.1.1\" class=\"ltx_text\">VI-B</span> </span><span id=\"S6.SS2.12.2\" class=\"ltx_text ltx_font_italic\">Zero-shot experiment on HV Benchmark</span>\n</h3>\n\n<div id=\"S6.SS2.p1\" class=\"ltx_para\">\n<p id=\"S6.SS2.p1.1\" class=\"ltx_p\">Historically underrepresented groups of composers, such as black women, have not been significantly explored in MIR datasets. We believe it is important to evaluate the capabilities of models in these out-of-distribution scenarios. In this experiment, we assess the ranking capabilities of the proposed approach in a zero-shot setting over the HV dataset by utilizing the model\u2019s logits. These logits follow a monotonic order, as indicated in Section\u00a0<a href=\"#S3.SS4\" title=\"III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-D</span></span></a>. Therefore, we can use them to order the HV collection and compare it with the original ranking.</p>\n</div>\n<div id=\"S6.SS2.p2\" class=\"ltx_para\">\n<p id=\"S6.SS2.p2.1\" class=\"ltx_p\">Table\u00a0<a href=\"#S6.SS2\" title=\"VI-B Zero-shot experiment on HV Benchmark \u2023 VI-A Model performance on works by female composers \u2023 VI A case study on woman composers \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">VI-B</span></span></a> shows the results obtained resorting to the Kendall rank correlation coefficient, <math id=\"S6.SS2.p2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.p2.1.m1.1a\"><msub id=\"S6.SS2.p2.1.m1.1.1\" xref=\"S6.SS2.p2.1.m1.1.1.cmml\"><mi id=\"S6.SS2.p2.1.m1.1.1.2\" xref=\"S6.SS2.p2.1.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.p2.1.m1.1.1.3\" xref=\"S6.SS2.p2.1.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.p2.1.m1.1b\"><apply id=\"S6.SS2.p2.1.m1.1.1.cmml\" xref=\"S6.SS2.p2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.p2.1.m1.1.1.1.cmml\" xref=\"S6.SS2.p2.1.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.p2.1.m1.1.1.2.cmml\" xref=\"S6.SS2.p2.1.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.p2.1.m1.1.1.3.cmml\" xref=\"S6.SS2.p2.1.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.p2.1.m1.1c\">\\tau_{c}</annotation></semantics></math>, for all data collections discussed in the experiment, considering both the single-task and multi-task models posed. Note that HV is only used for benchmarking purposes.</p>\n</div>\n<figure id=\"S6.SS2.6\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE VI: </span>Zero-shot experiment on Hidden Voices benchmark. The benchmark is a collection of piano pieces by black women composers, out of the distribution from the PSyllabus dataset.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S6.SS2.3.3\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:303.5pt;height:647pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(105.3pt,-224.5pt) scale(3.26768397143762,3.26768397143762) ;\">\n<table id=\"S6.SS2.3.3.3\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.SS2.3.3.3.4.1\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</td>\n<td id=\"S6.SS2.3.3.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S6.SS2.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.SS2.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S6.SS2.1.1.1.1.1.1\" class=\"ltx_text ltx_markedasmath\">Single-task</span></td>\n<td id=\"S6.SS2.1.1.1.1.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.5.2\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.5.2.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.5.2.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.5.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.592(.020)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.6.3\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.6.3.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.6.3.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.6.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.6.3.3.1\" class=\"ltx_text ltx_font_bold\">.661(.018)</span></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.7.4\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.7.4.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.7.4.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</td>\n<td id=\"S6.SS2.3.3.3.7.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.660(.029)</td>\n</tr>\n<tr id=\"S6.SS2.2.2.2.2\" class=\"ltx_tr\">\n<td id=\"S6.SS2.2.2.2.2.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS2.2.2.2.2.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS2.2.2.2.2.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with Era</span>\n</td>\n<td id=\"S6.SS2.2.2.2.2.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.8.5\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.8.5.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.8.5.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.8.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.634(.039)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.9.6\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.9.6.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.9.6.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.9.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.9.6.3.1\" class=\"ltx_text ltx_font_bold\">.668(.047)</span></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.3\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.3.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS2.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS2.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with Multiranking</span>\n</td>\n<td id=\"S6.SS2.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.10.7\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.10.7.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.10.7.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.10.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.632(.025)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.11.8\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.11.8.1\" class=\"ltx_td ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.11.8.2\" class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.11.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.11.8.3.1\" class=\"ltx_text ltx_font_bold\">.672(.031)</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S6.SS2.5.5\" class=\"ltx_p ltx_figure_panel\">In the zero-shot experiment conducted on the Hidden Voices benchmark, the findings underscore the advantages of multi-task models based on PR, particularly those that incorporate era information and multi-ranking strategies. The experiment reveals that the multi-task model utilizing era data with the PR approach achieved a <math id=\"S6.SS2.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.4.4.m1.1a\"><msub id=\"S6.SS2.4.4.m1.1.1\" xref=\"S6.SS2.4.4.m1.1.1.cmml\"><mi id=\"S6.SS2.4.4.m1.1.1.2\" xref=\"S6.SS2.4.4.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.4.4.m1.1.1.3\" xref=\"S6.SS2.4.4.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.4.4.m1.1b\"><apply id=\"S6.SS2.4.4.m1.1.1.cmml\" xref=\"S6.SS2.4.4.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.4.4.m1.1.1.1.cmml\" xref=\"S6.SS2.4.4.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.4.4.m1.1.1.2.cmml\" xref=\"S6.SS2.4.4.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.4.4.m1.1.1.3.cmml\" xref=\"S6.SS2.4.4.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.4.4.m1.1c\">\\tau_{c}</annotation></semantics></math> value of 0.668, emphasizing the importance of historical context in improving the performance of models on out-of-distribution data. Moreover, the PR model in the multi-task with Multiranking setup demonstrated the highest adaptability and performance, achieving a <math id=\"S6.SS2.5.5.m2.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.5.5.m2.1a\"><msub id=\"S6.SS2.5.5.m2.1.1\" xref=\"S6.SS2.5.5.m2.1.1.cmml\"><mi id=\"S6.SS2.5.5.m2.1.1.2\" xref=\"S6.SS2.5.5.m2.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.5.5.m2.1.1.3\" xref=\"S6.SS2.5.5.m2.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.5.5.m2.1b\"><apply id=\"S6.SS2.5.5.m2.1.1.cmml\" xref=\"S6.SS2.5.5.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.5.5.m2.1.1.1.cmml\" xref=\"S6.SS2.5.5.m2.1.1\">subscript</csymbol><ci id=\"S6.SS2.5.5.m2.1.1.2.cmml\" xref=\"S6.SS2.5.5.m2.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.5.5.m2.1.1.3.cmml\" xref=\"S6.SS2.5.5.m2.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.5.5.m2.1c\">\\tau_{c}</annotation></semantics></math> value of 0.672. This insight is vital for the development of inclusive machine-learning tools in the field of music research, highlighting the potential of combining multiple rankings to improve model performance on out-of-distribution datasets.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S6.SS2.6.6\" class=\"ltx_p ltx_figure_panel\">The results are not fully comparable with those of the HV with sheet music image presented in\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite> due to the dataset containing only 17 pieces. However, in the previous experiment, the best <math id=\"S6.SS2.6.6.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.6.6.m1.1a\"><msub id=\"S6.SS2.6.6.m1.1.1\" xref=\"S6.SS2.6.6.m1.1.1.cmml\"><mi id=\"S6.SS2.6.6.m1.1.1.2\" xref=\"S6.SS2.6.6.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.6.6.m1.1.1.3\" xref=\"S6.SS2.6.6.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.6.6.m1.1b\"><apply id=\"S6.SS2.6.6.m1.1.1.cmml\" xref=\"S6.SS2.6.6.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.6.6.m1.1.1.1.cmml\" xref=\"S6.SS2.6.6.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.6.6.m1.1.1.2.cmml\" xref=\"S6.SS2.6.6.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.6.6.m1.1.1.3.cmml\" xref=\"S6.SS2.6.6.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.6.6.m1.1c\">\\tau_{c}</annotation></semantics></math> result was 0.56, indicating that the audio-based version is more robust. Further research is needed to compare sheet music image classification audio to determine whether the modality or the quality of the datasets influences generalization.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S7\" class=\"ltx_section ltx_figure_panel\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">VII </span><span id=\"S7.1.1\" class=\"ltx_text ltx_font_smallcaps\">Conclusion</span>\n</h2>\n\n<div id=\"S7.p1\" class=\"ltx_para\">\n<p id=\"S7.p1.1\" class=\"ltx_p\">In conclusion, our study bridges a significant gap in music education and Music Information Retrieval by harnessing the widespread availability of audio via streaming platforms to estimate the difficulty of musical scores. This approach enhances students\u2019 ability to explore an extensive repertoire and collaborate with educators, significantly improving their learning experience and motivation. Our research introduces several key advancements.</p>\n</div>\n<div id=\"S7.p2\" class=\"ltx_para\">\n<p id=\"S7.p2.1\" class=\"ltx_p\">We have developed a model using a CNN+RNN+Attention network as a baseline for capturing performance difficulty from diverse audio representations. This model is assessed using a novel and expansive audio collection, setting a new benchmark for the scale of datasets in difficulty classification. Furthermore, we propose a multi-performance benchmark to investigate the impact of various performances on difficulty prediction, expanding the understanding of performance variability.</p>\n</div>\n<div id=\"S7.p3\" class=\"ltx_para\">\n<p id=\"S7.p3.1\" class=\"ltx_p\">Our comprehensive experimental framework includes testing generalization through a zero-shot scenario in out-of-domain distributions, employing multi-task learning with tasks related to music performance, and training across multiple difficulty rankings. These experiments underscore the robustness and versatility of our methodologies.</p>\n</div>\n<div id=\"S7.p4\" class=\"ltx_para\">\n<p id=\"S7.p4.1\" class=\"ltx_p\">To catalyze further research and collaboration within the music education community, we have publicly made our code, models, and the <em id=\"S7.p4.1.1\" class=\"ltx_emph ltx_font_italic\">Piano Syllabus (PSyllabus)</em> dataset available, including transcribed midis and CQT features. This initiative aims to create a shared platform for advancing automated performance difficulty understanding and enhancing music learning.</p>\n</div>\n<div id=\"S7.p5\" class=\"ltx_para\">\n<p id=\"S7.p5.1\" class=\"ltx_p\">Our contributions lay a solid foundation for the future of audio-based difficulty classification in music, demonstrating the potential for meaningful advancements in the automated analysis of musical scores. By providing comprehensive resources for the community, we invite educators, students, and researchers to engage with our work, fostering a collaborative ecosystem that supports the evolution of music education.</p>\n</div>\n<section id=\"Sx1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_font_smallcaps ltx_title_section\">Acknowledgment</h2>\n\n<div id=\"Sx1.p1\" class=\"ltx_para\">\n<p id=\"Sx1.p1.1\" class=\"ltx_p\">The authors thank Ian Wheaton, the creator of the Piano Syllabus web community, for his helpful guidance and for answering our questions. We are also grateful to him and the entire web community for their hard work in putting together the corpus to create a single source of information on piano difficulty. This effort is a strong base for our research, as we aim to use this resource to learn more and do deeper analysis. We hope our work benefits the Piano Syllabus and the music education community by aiding in the labeling of pieces and enabling the exploration of the forgotten cultural heritage.</p>\n</div>\n<div id=\"Sx1.p2\" class=\"ltx_para\">\n<p id=\"Sx1.p2.1\" class=\"ltx_p\">Pedro would also like to thank Nazif C. Tamer for his insistence on shifting focus to audio, and Pablo Alonso and Oguz Araz for their insightful discussions about audio features.</p>\n</div>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[1]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Mellizo, \u201cMusic education, curriculum design, and assessment: Imagining a more equitable approach,\u201d <em id=\"bib.bib1.1.1\" class=\"ltx_emph ltx_font_italic\">Music Educators Journal</em>, vol. 106, no.\u00a04, pp. 57\u201365, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[2]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0R. Jorgensen, \u201cThe curriculum design process in music,\u201d <em id=\"bib.bib2.1.1\" class=\"ltx_emph ltx_font_italic\">College Music Symposium</em>, vol.\u00a028, pp. 94\u2013105, 1988. [Online]. Available: <a target=\"_blank\" href=\"http://www.jstor.org/stable/40374590\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">http://www.jstor.org/stable/40374590</a>\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[3]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0S. Deconto, E.\u00a0L.\u00a0F. Valenga, and C.\u00a0N. Silla, \u201cAutomatic music score difficulty classification,\u201d in <em id=\"bib.bib3.1.1\" class=\"ltx_emph ltx_font_italic\">2023 30th International Conference on Systems, Signals and Image Processing (IWSSIP)</em>.\u00a0\u00a0\u00a0IEEE, 2023, pp. 1\u20135.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[4]</span>\n<span class=\"ltx_bibblock\">\nV.\u00a0S\u00e9bastien, H.\u00a0Ralambondrainy, O.\u00a0S\u00e9bastien, and N.\u00a0Conruyt, \u201cScore analyzer: Automatically determining scores difficulty level for instrumental e-learning,\u201d in <em id=\"bib.bib4.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of 13th International Society for Music Information Retrieval Conference, ISMIR</em>, Porto, Portugal, 2012.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[5]</span>\n<span class=\"ltx_bibblock\">\nS.-C. Chiu and M.-S. Chen, \u201cA study on difficulty level recognition of piano sheet music,\u201d in <em id=\"bib.bib5.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Symposium on Multimedia</em>.\u00a0\u00a0\u00a0IEEE, 2012, pp. 17\u201323.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[6]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura and K.\u00a0Yoshii, \u201cStatistical piano reduction controlling performance difficulty,\u201d <em id=\"bib.bib6.1.1\" class=\"ltx_emph ltx_font_italic\">APSIPA Transactions on Signal and Information Processing</em>, vol.\u00a07, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[7]</span>\n<span class=\"ltx_bibblock\">\n\u201cMusescore have automatic difficulty categories from year 2022,\u201d <a target=\"_blank\" href=\"https://musescore.com/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://musescore.com/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[8]</span>\n<span class=\"ltx_bibblock\">\n\u201cUltimate guitar have automatic difficulty categories from year 2022,\u201d <a target=\"_blank\" href=\"https://www.ultimate-guitar.com/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://www.ultimate-guitar.com/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[9]</span>\n<span class=\"ltx_bibblock\">\n\u201cSystem for estimating user\u2019s skill in playing a music instrument and determining virtual exercises thereof,\u201d Patent US9\u2009767\u2009705B1, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[10]</span>\n<span class=\"ltx_bibblock\">\nQ.\u00a0Kong, B.\u00a0Li, X.\u00a0Song, Y.\u00a0Wan, and Y.\u00a0Wang, \u201cHigh-resolution piano transcription with pedals by regressing onset and offset times,\u201d vol.\u00a029, p. 3707\u20133717, oct 2021.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[11]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura, N.\u00a0Ono, and S.\u00a0Sagayama, \u201cMerged-output hmm for piano fingering of both hands.\u201d in <em id=\"bib.bib11.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR</em>, Taipei, Taiwan, 2014, pp. 531\u2013536.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[12]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura and S.\u00a0Sagayama, \u201cAutomatic piano reduction from ensemble scores based on merged-output hidden markov model,\u201d in <em id=\"bib.bib12.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 41st International Computer Music Conference, ICMC</em>, Denton, USA, 2015.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[13]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, N.\u00a0C. Tamer, V.\u00a0Eremenko, M.\u00a0Miron, and X.\u00a0Serra, \u201cScore difficulty analysis for piano performance education,\u201d in <em id=\"bib.bib13.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP</em>, Singapore, Singapore, 2022, pp. 201\u2013205.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[14]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, D.\u00a0Jeong, V.\u00a0Eremenko, N.\u00a0C. Tamer, M.\u00a0Miron, and X.\u00a0Serra, \u201cCombining piano performance dimensions for score difficulty classification,\u201d <em id=\"bib.bib14.1.1\" class=\"ltx_emph ltx_font_italic\">Expert Systems with Applications</em>, vol. 238, p. 121776, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[15]</span>\n<span class=\"ltx_bibblock\">\nH.\u00a0Zhang, E.\u00a0Karystinaios, S.\u00a0Dixon, G.\u00a0Widmer, and C.\u00a0E. Cancino-Chac\u00f3n, \u201cSymbolic music representations for classification tasks: A systematic evaluation,\u201d <em id=\"bib.bib15.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2309.02567</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[16]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, D.\u00a0Jeong, E.\u00a0Nakamura, X.\u00a0Serra, and M.\u00a0Miron, \u201cAutomatic piano fingering from partially annotated scores using autoregressive neural networks,\u201d in <em id=\"bib.bib16.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 30th ACM International Conference on Multimedia</em>, 2022, pp. 6502\u20136510.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[17]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0Jeong, T.\u00a0Kwon, Y.\u00a0Kim, K.\u00a0Lee, and J.\u00a0Nam, \u201cVirtuosoNet: A hierarchical RNN-based system for modeling expressive piano performance,\u201d in <em id=\"bib.bib17.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR</em>, 2019, pp. 908\u2013915.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[18]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Cheng, Z.\u00a0Wang, and G.\u00a0Pollastri, \u201cA neural network approach to ordinal regression,\u201d in <em id=\"bib.bib18.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Joint Conference on Neural Networks, IJCNN</em>.\u00a0\u00a0\u00a0Hong Kong, China: IEEE, 2008, pp. 1279\u20131284.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[19]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0A.\u00a0V. V\u00e1squez, M.\u00a0Baelemans, J.\u00a0Driedger, W.\u00a0Zuidema, and J.\u00a0A. Burgoyne, \u201cQuantifying the ease of playing song chords on the guitar,\u201d in <em id=\"bib.bib19.1.1\" class=\"ltx_emph ltx_font_italic\">Ismir 2023 Hybrid Conference</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[20]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Toyama, T.\u00a0Akama, Y.\u00a0Ikemiya, Y.\u00a0Takida, W.-H. Liao, and Y.\u00a0Mitsufuji, \u201cAutomatic piano transcription with hierarchical frequency-time transformer,\u201d in <em id=\"bib.bib20.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[21]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Rege and R.\u00a0Sindal, \u201cReview of f0 estimation in the context of indian classical music expression detection,\u201d in <em id=\"bib.bib21.1.1\" class=\"ltx_emph ltx_font_italic\">Social Networking and Computational Intelligence: Proceedings of SCI-2018</em>.\u00a0\u00a0\u00a0Springer, 2020, pp. 257\u2013268.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[22]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Alonso-Jim\u00e9nez, X.\u00a0Serra, and D.\u00a0Bogdanov, \u201cMusic representation learning based on editorial metadata from discogs,\u201d in <em id=\"bib.bib22.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[23]</span>\n<span class=\"ltx_bibblock\">\nC.\u00a0Sch\u00f6rkhuber and A.\u00a0Klapuri, \u201cConstant-q transform toolbox for music processing,\u201d in <em id=\"bib.bib23.1.1\" class=\"ltx_emph ltx_font_italic\">7th sound and music computing conference, Barcelona, Spain</em>, 2010, pp. 3\u201364.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[24]</span>\n<span class=\"ltx_bibblock\">\nQ.\u00a0Kong, B.\u00a0Li, J.\u00a0Chen, and Y.\u00a0Wang, \u201cGiantmidi-piano: A large-scale MIDI dataset for classical piano music,\u201d <em id=\"bib.bib24.1.1\" class=\"ltx_emph ltx_font_italic\">Transactions of the International Society for Music Information Retrieval, 5(1), pp.87\u201398</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[25]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, J.\u00a0J. Valero-Mas, D.\u00a0Jeong, and X.\u00a0Serra, \u201cPredicting performance difficulty from piano sheet music images,\u201d in <em id=\"bib.bib25.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. of the 24th Int. Society for Music Information Retrieval Conf.</em>, Milan, Italy, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[26]</span>\n<span class=\"ltx_bibblock\">\nUniversity of Colorado, \u201cHidden voices project,\u201d <a target=\"_blank\" href=\"https://www.colorado.edu/project/hidden-voices/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://www.colorado.edu/project/hidden-voices/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[27]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0F. Gemmeke, D.\u00a0P. Ellis, D.\u00a0Freedman, A.\u00a0Jansen, W.\u00a0Lawrence, R.\u00a0C. Moore, M.\u00a0Plakal, and M.\u00a0Ritter, \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in <em id=\"bib.bib27.1.1\" class=\"ltx_emph ltx_font_italic\">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.\u00a0\u00a0\u00a0IEEE, 2017, pp. 776\u2013780.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[28]</span>\n<span class=\"ltx_bibblock\">\nV.\u00a0Lakic, L.\u00a0Rossetto, and A.\u00a0Bernstein, \u201cLink-rot in web-sourced multimedia datasets,\u201d in <em id=\"bib.bib28.1.1\" class=\"ltx_emph ltx_font_italic\">MultiMedia Modeling</em>.\u00a0\u00a0\u00a0Cham: Springer International Publishing, 2023, pp. 476\u2013488.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[29]</span>\n<span class=\"ltx_bibblock\">\nN.\u00a0Kato, E.\u00a0Nakamura, K.\u00a0Mine, O.\u00a0Doeda, and M.\u00a0Yamada, \u201cComputational analysis of audio recordings of piano performance for automatic evaluation,\u201d in <em id=\"bib.bib29.1.1\" class=\"ltx_emph ltx_font_italic\">Responsive and Sustainable Educational Futures</em>.\u00a0\u00a0\u00a0Cham: Springer Nature Switzerland, 2023, pp. 586\u2013592.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[30]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0O\u2019Hanlon and M.\u00a0B. Sandler, \u201cComparing cqt and reassignment based chroma features for template-based automatic chord recognition,\u201d in <em id=\"bib.bib30.1.1\" class=\"ltx_emph ltx_font_italic\">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP</em>.\u00a0\u00a0\u00a0IEEE, 2019, pp. 860\u2013864.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[31]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda and G.\u00a0Bernardes, \u201cRevisiting harmonic change detection,\u201d in <em id=\"bib.bib31.1.1\" class=\"ltx_emph ltx_font_italic\">Audio Engineering Society Convention 149</em>, Oct 2020.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[32]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Wang, A.\u00a0Singh, J.\u00a0Michael, F.\u00a0Hill, O.\u00a0Levy, and S.\u00a0R. Bowman, \u201cGlue: A multi-task benchmark and analysis platform for natural language understanding,\u201d in <em id=\"bib.bib32.1.1\" class=\"ltx_emph ltx_font_italic\">7th International Conference on Learning Representations, ICLR</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[33]</span>\n<span class=\"ltx_bibblock\">\nI.\u00a0Laina, C.\u00a0Rupprecht, V.\u00a0Belagiannis, F.\u00a0Tombari, and N.\u00a0Navab, \u201cDeeper depth prediction with fully convolutional residual networks,\u201d in <em id=\"bib.bib33.1.1\" class=\"ltx_emph ltx_font_italic\">2016 Fourth international conference on 3D vision (3DV)</em>.\u00a0\u00a0\u00a0IEEE, 2016, pp. 239\u2013248.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[34]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Wu, C.\u00a0Shen, and A.\u00a0Van Den\u00a0Hengel, \u201cWider or deeper: Revisiting the resnet model for visual recognition,\u201d <em id=\"bib.bib34.1.1\" class=\"ltx_emph ltx_font_italic\">Pattern Recognition</em>, vol.\u00a090, pp. 119\u2013133, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[35]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Won, K.\u00a0Choi, and X.\u00a0Serra, \u201cSemi-supervised music tagging transformer,\u201d in <em id=\"bib.bib35.1.1\" class=\"ltx_emph ltx_font_italic\">International Society for Music Information Retrieval Conference</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[36]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Allen-Zhu and Y.\u00a0Li, \u201cWhat can resnet learn efficiently, going beyond kernels?\u201d <em id=\"bib.bib36.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, vol.\u00a032, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[37]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Cho, B.\u00a0van Merrienboer, \u00c7.\u00a0G\u00fcl\u00e7ehre, D.\u00a0Bahdanau, F.\u00a0Bougares, H.\u00a0Schwenk, and Y.\u00a0Bengio, \u201cLearning phrase representations using RNN encoder-decoder for statistical machine translation,\u201d in <em id=\"bib.bib37.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL</em>, A.\u00a0Moschitti, B.\u00a0Pang, and W.\u00a0Daelemans, Eds.\u00a0\u00a0\u00a0ACL, 2014, pp. 1724\u20131734. [Online]. Available: <a target=\"_blank\" href=\"https://doi.org/10.3115/v1/d14-1179\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://doi.org/10.3115/v1/d14-1179</a>\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[38]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Chung, \u00c7.\u00a0G\u00fcl\u00e7ehre, K.\u00a0Cho, and Y.\u00a0Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d <em id=\"bib.bib38.1.1\" class=\"ltx_emph ltx_font_italic\">CoRR</em>, vol. abs/1412.3555, 2014. [Online]. Available: <a target=\"_blank\" href=\"http://arxiv.org/abs/1412.3555\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">http://arxiv.org/abs/1412.3555</a>\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[39]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Yang, D.\u00a0Yang, C.\u00a0Dyer, X.\u00a0He, A.\u00a0Smola, and E.\u00a0Hovy, \u201cHierarchical attention networks for document classification,\u201d in <em id=\"bib.bib39.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</em>, 2016, pp. 1480\u20131489.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[40]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Maghoumi and J.\u00a0J. LaViola, \u201cDeepgru: Deep gesture recognition utility,\u201d in <em id=\"bib.bib40.1.1\" class=\"ltx_emph ltx_font_italic\">International Symposium on Visual Computing</em>.\u00a0\u00a0\u00a0Springer, 2019, pp. 16\u201331.\n\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[41]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Alfaro-Contreras, J.\u00a0J. Valero-Mas, J.\u00a0M. I\u00f1esta, and J.\u00a0Calvo-Zaragoza, \u201cLate multimodal fusion for image and audio music transcription,\u201d <em id=\"bib.bib41.1.1\" class=\"ltx_emph ltx_font_italic\">Expert Systems with Applications</em>, vol. 216, p. 119491, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[42]</span>\n<span class=\"ltx_bibblock\">\nL.\u00a0Gaudette and N.\u00a0Japkowicz, \u201cEvaluation methods for ordinal classification,\u201d in <em id=\"bib.bib42.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 22nd Canadian Conference on Advances in Artificial Intelligence</em>, Kelowna, Canada, 2009, pp. 207\u2013210.\n\n</span>\n</li>\n</ul>\n</section>\n<figure id=\"Sx1.1\" class=\"ltx_float biography\">\n<table id=\"Sx1.1.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.1.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.1.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/pedro.png\" id=\"Sx1.1.1.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"100\" height=\"110\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.1.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.1.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.1.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Pedro Ramoneda</span> \nPedro Ramoneda holds a BSc in Computer Science from the University of Zaragoza, a Professional Degree in Piano Performance from the Conservatory of Music in Zaragoza, and an MSc in Sound and Music Computing from the Universitat Pompeu Fabra. He is currently a third-year PhD student in the Music Technology Group of the Universitat Pompeu Fabra under the supervision of Prof. Xavier Serra, focusing on the use of technologies from the Music Information Retrieval and Signal Processing field for supporting music education.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.2\" class=\"ltx_float biography\">\n<table id=\"Sx1.2.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.2.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.2.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/Minhee_Lee.jpeg\" id=\"Sx1.2.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"94\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.2.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.2.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.2.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Minhee Lee</span> \nMinhee Lee is an undergraduate student pursuing a B.S. degree in the Department of Computer Science and Engineering at Sogang University in South Korea. She is an undergraduate intern in the MALer Lab under the supervision of Prof. Dasaem Jeong since 2023. Before joining the research group, she did internships as a software engineer at Google in 2021 and 2022, and at FuriosaAI in 2022. Her research interest is about various music information retrieval tasks of understanding music with deep learning technologies.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.3\" class=\"ltx_float biography\">\n<table id=\"Sx1.3.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.3.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.3.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/Dasaem_Jeong.jpg\" id=\"Sx1.3.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"100\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.3.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.3.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.3.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.3.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Dasaem Jeong</span>  is currently working as an Assistant Professor in the Department of Art &amp; Technology at Sogang University in South Korea since 2021. Before joining Sogang University, he worked as a research scientist in T-Brain X, SK Telecom from 2020 to 2021. He obtained his Ph.D. and M.S. degrees in culture technology, and B.S. in mechanical engineering from Korea Advanced Institute of Science and Technology (KAIST). His research primarily focuses on a diverse range of music information retrieval tasks, including music generation and computational musicology.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.4\" class=\"ltx_float biography\">\n<table id=\"Sx1.4.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.4.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.4.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/JJVM.png\" id=\"Sx1.4.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"85\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.4.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.4.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.4.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.4.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Jose J. Valero-Mas</span>  received the M.Sc. degree in Telecommunications Engineering from the University Miguel Hern\u00e1ndez of Elche in 2012, the M.Sc. degree in Sound and Music Computing from the Universitat Pompeu Fabra in 2013, and the Ph.D. degree in Computer Science from the University of Alicante in 2017. After a three-year period in industry in which he developed as a data scientist, he acted as a Postdoctoral Researcher from 2020 to 2023 in the University of Alicante, the Universitat Pompeu Fabra, and the Queen Mary University of London. He is currently an Assistant Professor in the Department of Software and Computing Systems of the University of Alicante. His research interests include Pattern Recognition, Machine Learning, Music Information Retrieval, and Signal Processing for which he has co-authored more than 40 works within international journals, conference communications, and book chapters.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.5\" class=\"ltx_float biography\">\n<table id=\"Sx1.5.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.5.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.5.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/serra.png\" id=\"Sx1.5.1.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"100\" height=\"110\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.5.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.5.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.5.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.5.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Xavier Serra</span> \nHe has had a progressive academic career in the field of music technology since earning his PhD from Stanford University in 1989. His interest lies in the analysis, synthesis, and description of sound and music signals. He always seeks a balance between basic and applied research while integrating both scientific/technological and humanistic/artistic disciplines. Currently, he supervises research at the Music Technology Group (MTG), focusing on understanding sound and music signals through signal processing, machine learning, and semantic technologies. His work emphasizes data-driven and knowledge-driven methodologies, involving the development of large data collections and the application of domain-specific knowledge. He leads projects funded both publicly and privately, tackling practical issues such as music exploration, sound classification, and music performance analysis for educational purposes. He champions open science by promoting open data, software, and access, and is keen on using open innovation strategies to enhance the social and economic impact of his research.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n</section>\n</section>\n</div>\n</div>\n</figure>\n</section>\n</div>\n</div>\n</figure>\n</section>\n</section>\n</section>\n</div>\n</div>\n</figure>\n</section>\n</div>\n</div>\n</figure>\n</section>\n</section>\n</section>\n</section>\n</section>\n</section>\n</section>\n</div>\n</div>\n</figure>\n",
        "footnotes": [],
        "references": []
    },
    "S5.3": {
        "caption": "TABLE II:  Results training with monomodal representations, CQT and PR, and multimodal ones. TABLE III:  Multi-tasks experiments training the models on the PSyllabus dataset and auxiliary tasks.  TABLE IV:  Comparative analysis for Basic and Multi-task with Era experiments across musical periods. TABLE V:  Analysis of model performance differentiated by the composer\u2019s gender. TABLE VI:  Zero-shot experiment on Hidden Voices benchmark. The benchmark is a collection of piano pieces by black women composers, out of the distribution from the PSyllabus dataset.",
        "table": "<figure id=\"S5.3\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE II: </span>Results training with monomodal representations, CQT and PR, and multimodal ones.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S5.3.3\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:390.3pt;height:219.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(83.0pt,-46.6pt) scale(1.73983164653568,1.73983164653568) ;\">\n<table id=\"S5.3.3.3\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</th>\n<td id=\"S5.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MSE</td>\n<td id=\"S5.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Acc<sub id=\"S5.1.1.1.1.1.1\" class=\"ltx_sub\">0</sub>\n</td>\n<td id=\"S5.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S5.2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S5.2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S5.2.2.2.2.1.1\" class=\"ltx_text ltx_markedasmath\">Monomodal</span></th>\n<td id=\"S5.2.2.2.2.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.2.2.2.2.3\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.2.2.2.2.4\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.3.3.3.4.1\" class=\"ltx_tr\">\n<th id=\"S5.3.3.3.4.1.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.3.3.3.4.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.3.3.3.4.1.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.29(.18)</td>\n<td id=\"S5.3.3.3.4.1.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(1.61)</td>\n<td id=\"S5.3.3.3.4.1.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.741(.012)</td>\n</tr>\n<tr id=\"S5.3.3.3.5.2\" class=\"ltx_tr\">\n<th id=\"S5.3.3.3.5.2.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.3.3.3.5.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.3.3.3.5.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.85(.07)</td>\n<td id=\"S5.3.3.3.5.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.7(.84)</td>\n<td id=\"S5.3.3.3.5.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.771(.005)</td>\n</tr>\n<tr id=\"S5.3.3.3.3\" class=\"ltx_tr\">\n<th id=\"S5.3.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S5.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S5.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Multimodal</span>\n</th>\n<td id=\"S5.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.3.3.3.3.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.3.3.3.3.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.3.3.3.6.3\" class=\"ltx_tr\">\n<th id=\"S5.3.3.3.6.3.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.3.3.3.6.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S5.3.3.3.6.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.3.3.3.6.3.3.1\" class=\"ltx_text ltx_font_bold\">1.81(.11)</span></td>\n<td id=\"S5.3.3.3.6.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.3.3.3.6.3.4.1\" class=\"ltx_text ltx_font_bold\">37.3(1.97)</span></td>\n<td id=\"S5.3.3.3.6.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.3.3.3.6.3.5.1\" class=\"ltx_text ltx_font_bold\">.778(.004)</span></td>\n</tr>\n<tr id=\"S5.3.3.3.7.4\" class=\"ltx_tr\">\n<th id=\"S5.3.3.3.7.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.3.3.3.7.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">ENSEMBLE</th>\n<td id=\"S5.3.3.3.7.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.82(.09)</td>\n<td id=\"S5.3.3.3.7.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.2(.58)</td>\n<td id=\"S5.3.3.3.7.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.777(.006)</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S5.SS1\" class=\"ltx_subsection ltx_figure_panel\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S5.SS1.25.1.1\" class=\"ltx_text\">V-A</span> </span><span id=\"S5.SS1.26.2\" class=\"ltx_text ltx_font_italic\">Auxiliary tasks for pretraining</span>\n</h3>\n\n<figure id=\"S5.SS1.20\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE III: </span>Multi-tasks experiments training the models on the PSyllabus dataset and auxiliary tasks. </figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S5.SS1.11.11\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:390.3pt;height:416.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(85.5pt,-91.3pt) scale(1.7799857981267,1.7799857981267) ;\">\n<table id=\"S5.SS1.11.11.11\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.SS1.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.SS1.1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</th>\n<td id=\"S5.SS1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MSE</td>\n<td id=\"S5.SS1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Acc<sub id=\"S5.SS1.1.1.1.1.1.1\" class=\"ltx_sub\">0</sub>\n</td>\n<td id=\"S5.SS1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S5.SS1.2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S5.SS1.2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S5.SS1.2.2.2.2.1.1\" class=\"ltx_text ltx_markedasmath\">Single-task</span></th>\n<td id=\"S5.SS1.2.2.2.2.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.2.2.2.2.3\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.2.2.2.2.4\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.SS1.11.11.11.12.1\" class=\"ltx_tr\">\n<th id=\"S5.SS1.11.11.11.12.1.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.11.11.11.12.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.SS1.11.11.11.12.1.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.29(.18)</td>\n<td id=\"S5.SS1.11.11.11.12.1.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(1.61)</td>\n<td id=\"S5.SS1.11.11.11.12.1.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.741(.012)</td>\n</tr>\n<tr id=\"S5.SS1.11.11.11.13.2\" class=\"ltx_tr\">\n<th id=\"S5.SS1.11.11.11.13.2.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.11.11.11.13.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.SS1.11.11.11.13.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.85(.07)</td>\n<td id=\"S5.SS1.11.11.11.13.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.7(.84)</td>\n<td id=\"S5.SS1.11.11.11.13.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.771(.005)</td>\n</tr>\n<tr id=\"S5.SS1.3.3.3.3\" class=\"ltx_tr\">\n<th id=\"S5.SS1.3.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S5.SS1.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S5.SS1.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with era</span>\n</th>\n<td id=\"S5.SS1.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.3.3.3.3.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.3.3.3.3.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.SS1.4.4.4.4\" class=\"ltx_tr\">\n<th id=\"S5.SS1.4.4.4.4.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.4.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT<sub id=\"S5.SS1.4.4.4.4.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.4.4.4.4.1.1.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub>\n</th>\n<td id=\"S5.SS1.4.4.4.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.99(.15)</td>\n<td id=\"S5.SS1.4.4.4.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.6(.97)</td>\n<td id=\"S5.SS1.4.4.4.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.766(.008)</td>\n</tr>\n<tr id=\"S5.SS1.5.5.5.5\" class=\"ltx_tr\">\n<th id=\"S5.SS1.5.5.5.5.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.5.5.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR<sub id=\"S5.SS1.5.5.5.5.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.5.5.5.5.1.1.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub>\n</th>\n<td id=\"S5.SS1.5.5.5.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.SS1.5.5.5.5.3.1\" class=\"ltx_text ltx_font_bold\">1.83(.10)</span></td>\n<td id=\"S5.SS1.5.5.5.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.SS1.5.5.5.5.4.1\" class=\"ltx_text ltx_font_bold\">37.7(.91)</span></td>\n<td id=\"S5.SS1.5.5.5.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.SS1.5.5.5.5.5.1\" class=\"ltx_text ltx_font_bold\">.775(.008)</span></td>\n</tr>\n<tr id=\"S5.SS1.6.6.6.6\" class=\"ltx_tr\">\n<th id=\"S5.SS1.6.6.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S5.SS1.6.6.6.6.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S5.SS1.6.6.6.6.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with composer</span>\n</th>\n<td id=\"S5.SS1.6.6.6.6.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.6.6.6.6.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.6.6.6.6.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.SS1.7.7.7.7\" class=\"ltx_tr\">\n<th id=\"S5.SS1.7.7.7.7.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.7.7.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT<sub id=\"S5.SS1.7.7.7.7.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.7.7.7.7.1.1.1\" class=\"ltx_text ltx_font_italic\">Composer</span></sub>\n</th>\n<td id=\"S5.SS1.7.7.7.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.74(.16)</td>\n<td id=\"S5.SS1.7.7.7.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">27.0(1.09)</td>\n<td id=\"S5.SS1.7.7.7.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.701(.008)</td>\n</tr>\n<tr id=\"S5.SS1.8.8.8.8\" class=\"ltx_tr\">\n<th id=\"S5.SS1.8.8.8.8.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.8.8.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR<sub id=\"S5.SS1.8.8.8.8.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.8.8.8.8.1.1.1\" class=\"ltx_text ltx_font_italic\">Composer</span></sub>\n</th>\n<td id=\"S5.SS1.8.8.8.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.57(.18)</td>\n<td id=\"S5.SS1.8.8.8.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.0(1.09)</td>\n<td id=\"S5.SS1.8.8.8.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.708(.007)</td>\n</tr>\n<tr id=\"S5.SS1.9.9.9.9\" class=\"ltx_tr\">\n<th id=\"S5.SS1.9.9.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S5.SS1.9.9.9.9.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S5.SS1.9.9.9.9.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with multiple ranks</span>\n</th>\n<td id=\"S5.SS1.9.9.9.9.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.9.9.9.9.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.9.9.9.9.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.SS1.10.10.10.10\" class=\"ltx_tr\">\n<th id=\"S5.SS1.10.10.10.10.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.10.10.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT<sub id=\"S5.SS1.10.10.10.10.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.10.10.10.10.1.1.1\" class=\"ltx_text ltx_font_italic\">MultiRank</span></sub>\n</th>\n<td id=\"S5.SS1.10.10.10.10.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.16(.16)</td>\n<td id=\"S5.SS1.10.10.10.10.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.4(1.35)</td>\n<td id=\"S5.SS1.10.10.10.10.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.754(.007)</td>\n</tr>\n<tr id=\"S5.SS1.11.11.11.11\" class=\"ltx_tr\">\n<th id=\"S5.SS1.11.11.11.11.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.11.11.11.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR<sub id=\"S5.SS1.11.11.11.11.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.11.11.11.11.1.1.1\" class=\"ltx_text ltx_font_italic\">MultiRank</span></sub>\n</th>\n<td id=\"S5.SS1.11.11.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.88(.06)</td>\n<td id=\"S5.SS1.11.11.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.4(.61)</td>\n<td id=\"S5.SS1.11.11.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.768(.003)</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.20.21\" class=\"ltx_p ltx_figure_panel\">In our exploration of multi-task learning\u2019s influence on model efficacy, we conducted a series of experiments shown in Table\u00a0<a href=\"#S5\" title=\"V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>. These experiments aim to measure the impact of incorporating auxiliary tasks\u2014namely, musical era, composer identification, and multiple rankings\u2014on models trained using the PSyllabus difficulty labels.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.20.22\" class=\"ltx_p ltx_figure_panel\">Initial single-task experiments, described in Section\u00a0<a href=\"#S5\" title=\"V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> employing CQT and PR, established our baseline. Among these, PR emerged as the more potent baseline.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.15.15\" class=\"ltx_p ltx_figure_panel\">The integration of multi-task learning, particularly with the musical era (represented as CQT<sub id=\"S5.SS1.15.15.1\" class=\"ltx_sub\"><span id=\"S5.SS1.15.15.1.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub> and PR<sub id=\"S5.SS1.15.15.2\" class=\"ltx_sub\"><span id=\"S5.SS1.15.15.2.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub>), resulted in discernible performance enhancements. PR<sub id=\"S5.SS1.15.15.3\" class=\"ltx_sub\"><span id=\"S5.SS1.15.15.3.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub>, for instance, manifested a reduction in MSE to 1.83 and an increase in <math id=\"S5.SS1.15.15.m4.1\" class=\"ltx_Math\" alttext=\"Acc_{0}\" display=\"inline\"><semantics id=\"S5.SS1.15.15.m4.1a\"><mrow id=\"S5.SS1.15.15.m4.1.1\" xref=\"S5.SS1.15.15.m4.1.1.cmml\"><mi id=\"S5.SS1.15.15.m4.1.1.2\" xref=\"S5.SS1.15.15.m4.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.SS1.15.15.m4.1.1.1\" xref=\"S5.SS1.15.15.m4.1.1.1.cmml\">\u200b</mo><mi id=\"S5.SS1.15.15.m4.1.1.3\" xref=\"S5.SS1.15.15.m4.1.1.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.SS1.15.15.m4.1.1.1a\" xref=\"S5.SS1.15.15.m4.1.1.1.cmml\">\u200b</mo><msub id=\"S5.SS1.15.15.m4.1.1.4\" xref=\"S5.SS1.15.15.m4.1.1.4.cmml\"><mi id=\"S5.SS1.15.15.m4.1.1.4.2\" xref=\"S5.SS1.15.15.m4.1.1.4.2.cmml\">c</mi><mn id=\"S5.SS1.15.15.m4.1.1.4.3\" xref=\"S5.SS1.15.15.m4.1.1.4.3.cmml\">0</mn></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.15.15.m4.1b\"><apply id=\"S5.SS1.15.15.m4.1.1.cmml\" xref=\"S5.SS1.15.15.m4.1.1\"><times id=\"S5.SS1.15.15.m4.1.1.1.cmml\" xref=\"S5.SS1.15.15.m4.1.1.1\"></times><ci id=\"S5.SS1.15.15.m4.1.1.2.cmml\" xref=\"S5.SS1.15.15.m4.1.1.2\">\ud835\udc34</ci><ci id=\"S5.SS1.15.15.m4.1.1.3.cmml\" xref=\"S5.SS1.15.15.m4.1.1.3\">\ud835\udc50</ci><apply id=\"S5.SS1.15.15.m4.1.1.4.cmml\" xref=\"S5.SS1.15.15.m4.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.15.15.m4.1.1.4.1.cmml\" xref=\"S5.SS1.15.15.m4.1.1.4\">subscript</csymbol><ci id=\"S5.SS1.15.15.m4.1.1.4.2.cmml\" xref=\"S5.SS1.15.15.m4.1.1.4.2\">\ud835\udc50</ci><cn type=\"integer\" id=\"S5.SS1.15.15.m4.1.1.4.3.cmml\" xref=\"S5.SS1.15.15.m4.1.1.4.3\">0</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.15.15.m4.1c\">Acc_{0}</annotation></semantics></math> to 37.7%, illustrating the advantages of embedding contextual musical insights into the learning process.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.17.17\" class=\"ltx_p ltx_figure_panel\">Nevertheless, when composer identification was introduced as an auxiliary task (via CQT<sub id=\"S5.SS1.17.17.1\" class=\"ltx_sub\"><span id=\"S5.SS1.17.17.1.1\" class=\"ltx_text ltx_font_italic\">Composer</span></sub> and PR<sub id=\"S5.SS1.17.17.2\" class=\"ltx_sub\"><span id=\"S5.SS1.17.17.2.1\" class=\"ltx_text ltx_font_italic\">Composer</span></sub>), we observed a decrease in performance relative to the baseline. This outcome suggests that the added complexity from composer identification might not synergize well with the primary task, possibly because the diverse stylistic idiosyncrasies of composers could impede the model\u2019s generalization capabilities.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.20.20\" class=\"ltx_p ltx_figure_panel\">Experiments involving the multiple ranking task (CQT<sub id=\"S5.SS1.20.20.1\" class=\"ltx_sub\"><span id=\"S5.SS1.20.20.1.1\" class=\"ltx_text ltx_font_italic\">MultiRank</span></sub> and PR<sub id=\"S5.SS1.20.20.2\" class=\"ltx_sub\"><span id=\"S5.SS1.20.20.2.1\" class=\"ltx_text ltx_font_italic\">MultiRank</span></sub>) yielded mixed outcomes. Although there was a slight improvement in MSE and <math id=\"S5.SS1.20.20.m3.1\" class=\"ltx_Math\" alttext=\"Acc_{0}\" display=\"inline\"><semantics id=\"S5.SS1.20.20.m3.1a\"><mrow id=\"S5.SS1.20.20.m3.1.1\" xref=\"S5.SS1.20.20.m3.1.1.cmml\"><mi id=\"S5.SS1.20.20.m3.1.1.2\" xref=\"S5.SS1.20.20.m3.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.SS1.20.20.m3.1.1.1\" xref=\"S5.SS1.20.20.m3.1.1.1.cmml\">\u200b</mo><mi id=\"S5.SS1.20.20.m3.1.1.3\" xref=\"S5.SS1.20.20.m3.1.1.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.SS1.20.20.m3.1.1.1a\" xref=\"S5.SS1.20.20.m3.1.1.1.cmml\">\u200b</mo><msub id=\"S5.SS1.20.20.m3.1.1.4\" xref=\"S5.SS1.20.20.m3.1.1.4.cmml\"><mi id=\"S5.SS1.20.20.m3.1.1.4.2\" xref=\"S5.SS1.20.20.m3.1.1.4.2.cmml\">c</mi><mn id=\"S5.SS1.20.20.m3.1.1.4.3\" xref=\"S5.SS1.20.20.m3.1.1.4.3.cmml\">0</mn></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.20.20.m3.1b\"><apply id=\"S5.SS1.20.20.m3.1.1.cmml\" xref=\"S5.SS1.20.20.m3.1.1\"><times id=\"S5.SS1.20.20.m3.1.1.1.cmml\" xref=\"S5.SS1.20.20.m3.1.1.1\"></times><ci id=\"S5.SS1.20.20.m3.1.1.2.cmml\" xref=\"S5.SS1.20.20.m3.1.1.2\">\ud835\udc34</ci><ci id=\"S5.SS1.20.20.m3.1.1.3.cmml\" xref=\"S5.SS1.20.20.m3.1.1.3\">\ud835\udc50</ci><apply id=\"S5.SS1.20.20.m3.1.1.4.cmml\" xref=\"S5.SS1.20.20.m3.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.20.20.m3.1.1.4.1.cmml\" xref=\"S5.SS1.20.20.m3.1.1.4\">subscript</csymbol><ci id=\"S5.SS1.20.20.m3.1.1.4.2.cmml\" xref=\"S5.SS1.20.20.m3.1.1.4.2\">\ud835\udc50</ci><cn type=\"integer\" id=\"S5.SS1.20.20.m3.1.1.4.3.cmml\" xref=\"S5.SS1.20.20.m3.1.1.4.3\">0</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.20.20.m3.1c\">Acc_{0}</annotation></semantics></math> compared to the single-task baseline, these enhancements were not as pronounced as those observed with the musical era task, indicating a moderate benefit from incorporating ranking information. The outcomes for multiple ranking tasks stand in contrast to those involving music sheet images in previous research\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite>, where the multi-rank approach outperformed other experiments. However, a direct comparison between these results is not straightforward due to differences in the datasets used, and further research is needed to compare both approaches.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S5.SS1.SSS0.Px1\" class=\"ltx_paragraph ltx_figure_panel\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Impact of Musical Era on Difficulty Estimation.</h5>\n\n<div id=\"S5.SS1.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S5.SS1.SSS0.Px1.p1.1\" class=\"ltx_p\">Our analysis, as shown in Table\u00a0<a href=\"#S5.T4\" title=\"TABLE IV \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, investigates the auxiliary task of era identification on difficulty prediction across musical periods. The inclusion of era significantly enhances model performance, as evidenced by improvements in Mean Squared Error (MSE), Accuracy at zero (Acc<sub id=\"S5.SS1.SSS0.Px1.p1.1.1\" class=\"ltx_sub\">0</sub>), and Tau-c metrics.</p>\n</div>\n<div id=\"S5.SS1.SSS0.Px1.p2\" class=\"ltx_para\">\n<p id=\"S5.SS1.SSS0.Px1.p2.2\" class=\"ltx_p\">For the Baroque period, we observed MSE improvements from 2.73 to 2.49 (CQT) and 2.25 to 2.03 (PR), with corresponding increases in Acc<sub id=\"S5.SS1.SSS0.Px1.p2.2.1\" class=\"ltx_sub\">0</sub>. The Classical period saw similar enhancements, with MSE decreasing to 1.96 (CQT) and Acc<sub id=\"S5.SS1.SSS0.Px1.p2.2.2\" class=\"ltx_sub\">0</sub> improving across methods. The Romantic and Modern periods further confirmed these trends, demonstrating the general efficacy of integrating era information into difficulty estimation models.</p>\n</div>\n<div id=\"S5.SS1.SSS0.Px1.p3\" class=\"ltx_para\">\n<p id=\"S5.SS1.SSS0.Px1.p3.1\" class=\"ltx_p\">These findings highlight the significance of era-specific characteristics in music difficulty prediction, suggesting a nuanced relationship between historical context and model accuracy. The varied improvements across periods underscore the potential of tailored, era-specific modeling approaches to enhance prediction accuracy.</p>\n</div>\n<figure id=\"S5.T4\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE IV: </span>Comparative analysis for Basic and Multi-task with Era experiments across musical periods.</figcaption>\n<div id=\"S5.T4.1\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:346.9pt;height:182.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-31.4pt,16.6pt) scale(0.846604287885487,0.846604287885487) ;\">\n<table id=\"S5.T4.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Period</th>\n<th id=\"S5.T4.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Method</th>\n<th id=\"S5.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">MSE</th>\n<th id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Acc<sub id=\"S5.T4.1.1.1.1.1\" class=\"ltx_sub\">0</sub>\n</th>\n<th id=\"S5.T4.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Tau-c</th>\n</tr>\n<tr id=\"S5.T4.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.2.1.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.T4.1.1.2.1.2\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.T4.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Single</th>\n<th id=\"S5.T4.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">with Era</th>\n<th id=\"S5.T4.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Single</th>\n<th id=\"S5.T4.1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">with Era</th>\n<th id=\"S5.T4.1.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Single</th>\n<th id=\"S5.T4.1.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">with Era</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.3.1.1.1\" class=\"ltx_text\">Baroque</span></th>\n<th id=\"S5.T4.1.1.3.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.73(.49)</td>\n<td id=\"S5.T4.1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.49(.42)</td>\n<td id=\"S5.T4.1.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.1(4.82)</td>\n<td id=\"S5.T4.1.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.3(5.51)</td>\n<td id=\"S5.T4.1.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.621(.041)</td>\n<td id=\"S5.T4.1.1.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.631(.036)</td>\n</tr>\n<tr id=\"S5.T4.1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.4.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.25(.50)</td>\n<td id=\"S5.T4.1.1.4.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.4.2.3.1\" class=\"ltx_text ltx_font_bold\">2.03(.62)</span></td>\n<td id=\"S5.T4.1.1.4.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(4.69)</td>\n<td id=\"S5.T4.1.1.4.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.4.2.5.1\" class=\"ltx_text ltx_font_bold\">34.2(6.46)</span></td>\n<td id=\"S5.T4.1.1.4.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.702(.066)</td>\n<td id=\"S5.T4.1.1.4.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.4.2.7.1\" class=\"ltx_text ltx_font_bold\">.707(.088)</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.5.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.5.3.1.1\" class=\"ltx_text\">Classical</span></th>\n<th id=\"S5.T4.1.1.5.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.25(.66)</td>\n<td id=\"S5.T4.1.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.5.3.4.1\" class=\"ltx_text ltx_font_bold\">1.96(.51)</span></td>\n<td id=\"S5.T4.1.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.2(6.06)</td>\n<td id=\"S5.T4.1.1.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.1(5.55)</td>\n<td id=\"S5.T4.1.1.5.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.698(.088)</td>\n<td id=\"S5.T4.1.1.5.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.719(.073)</td>\n</tr>\n<tr id=\"S5.T4.1.1.6.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.6.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.97(.79)</td>\n<td id=\"S5.T4.1.1.6.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.98(.45)</td>\n<td id=\"S5.T4.1.1.6.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.2(8.23)</td>\n<td id=\"S5.T4.1.1.6.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.6.4.5.1\" class=\"ltx_text ltx_font_bold\">34.3(4.90)</span></td>\n<td id=\"S5.T4.1.1.6.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.726(.093)</td>\n<td id=\"S5.T4.1.1.6.4.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.6.4.7.1\" class=\"ltx_text ltx_font_bold\">.732(.067)</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.7.5\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.7.5.1.1\" class=\"ltx_text\">Romantic</span></th>\n<th id=\"S5.T4.1.1.7.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.16(.56)</td>\n<td id=\"S5.T4.1.1.7.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.05(.44)</td>\n<td id=\"S5.T4.1.1.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.6(5.41)</td>\n<td id=\"S5.T4.1.1.7.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.1(4.50)</td>\n<td id=\"S5.T4.1.1.7.5.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.704(.071)</td>\n<td id=\"S5.T4.1.1.7.5.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.724(.062)</td>\n</tr>\n<tr id=\"S5.T4.1.1.8.6\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.8.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.8.6.2.1\" class=\"ltx_text ltx_font_bold\">1.84(.66)</span></td>\n<td id=\"S5.T4.1.1.8.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.87(.63)</td>\n<td id=\"S5.T4.1.1.8.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.2(6.92)</td>\n<td id=\"S5.T4.1.1.8.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.8.6.5.1\" class=\"ltx_text ltx_font_bold\">35.4(7.59)</span></td>\n<td id=\"S5.T4.1.1.8.6.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.8.6.6.1\" class=\"ltx_text ltx_font_bold\">.739(.077)</span></td>\n<td id=\"S5.T4.1.1.8.6.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.730(.085)</td>\n</tr>\n<tr id=\"S5.T4.1.1.9.7\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.9.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.9.7.1.1\" class=\"ltx_text\">20th Century</span></th>\n<th id=\"S5.T4.1.1.9.7.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.9.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.16(.49)</td>\n<td id=\"S5.T4.1.1.9.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.33(.42)</td>\n<td id=\"S5.T4.1.1.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">33.3(4.94)</td>\n<td id=\"S5.T4.1.1.9.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">30.3(6.58)</td>\n<td id=\"S5.T4.1.1.9.7.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.715(.065)</td>\n<td id=\"S5.T4.1.1.9.7.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.658(.043)</td>\n</tr>\n<tr id=\"S5.T4.1.1.10.8\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.10.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.10.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.86(.57)</td>\n<td id=\"S5.T4.1.1.10.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.10.8.3.1\" class=\"ltx_text ltx_font_bold\">1.78(.52)</span></td>\n<td id=\"S5.T4.1.1.10.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.6(6.07)</td>\n<td id=\"S5.T4.1.1.10.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.10.8.5.1\" class=\"ltx_text ltx_font_bold\">37.3(6.95)</span></td>\n<td id=\"S5.T4.1.1.10.8.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.10.8.6.1\" class=\"ltx_text ltx_font_bold\">.745(.068)</span></td>\n<td id=\"S5.T4.1.1.10.8.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.742(.070)</td>\n</tr>\n<tr id=\"S5.T4.1.1.11.9\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.11.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.11.9.1.1\" class=\"ltx_text\">Modern</span></th>\n<th id=\"S5.T4.1.1.11.9.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.11.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.25(.50)</td>\n<td id=\"S5.T4.1.1.11.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.11.9.4.1\" class=\"ltx_text ltx_font_bold\">1.84(.47)</span></td>\n<td id=\"S5.T4.1.1.11.9.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(4.69)</td>\n<td id=\"S5.T4.1.1.11.9.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.11.9.6.1\" class=\"ltx_text ltx_font_bold\">37.0(6.10)</span></td>\n<td id=\"S5.T4.1.1.11.9.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.702(.066)</td>\n<td id=\"S5.T4.1.1.11.9.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.11.9.8.1\" class=\"ltx_text ltx_font_bold\">.747(.061)</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.12.10\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.12.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.12.10.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.94(.57)</td>\n<td id=\"S5.T4.1.1.12.10.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.92(.50)</td>\n<td id=\"S5.T4.1.1.12.10.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.8(5.54)</td>\n<td id=\"S5.T4.1.1.12.10.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.3(5.63)</td>\n<td id=\"S5.T4.1.1.12.10.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.736(.063)</td>\n<td id=\"S5.T4.1.1.12.10.7\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.739(.057)</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n<section id=\"S6\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">VI </span><span id=\"S6.1.1\" class=\"ltx_text ltx_font_smallcaps\">A case study on woman composers</span>\n</h2>\n\n<div id=\"S6.p1\" class=\"ltx_para\">\n<p id=\"S6.p1.1\" class=\"ltx_p\">In this section, we explore the capabilities of the models proposed in this paper, specifically focusing on the classification of an unrepresented group: women composers. In Subsection\u00a0<a href=\"#S6.SS1\" title=\"VI-A Model performance on works by female composers \u2023 VI A case study on woman composers \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">VI-A</span></span></a>, we examine the models\u2019 performance across different genders. Subsequently, in Subsection\u00a0<a href=\"#S6.SS2\" title=\"VI-B Zero-shot experiment on HV Benchmark \u2023 VI-A Model performance on works by female composers \u2023 VI A case study on woman composers \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">VI-B</span></span></a>, we assess the generalization capabilities of the proposed models using a benchmark dataset of compositions by black women composers.</p>\n</div>\n<section id=\"S6.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S6.SS1.9.1.1\" class=\"ltx_text\">VI-A</span> </span><span id=\"S6.SS1.10.2\" class=\"ltx_text ltx_font_italic\">Model performance on works by female composers</span>\n</h3>\n\n<div id=\"S6.SS1.p1\" class=\"ltx_para\">\n<p id=\"S6.SS1.p1.1\" class=\"ltx_p\">In this section, we present the results of experiments designed to analyze gender bias in the models proposed in this paper, which were trained on the PSyllabus dataset. We focus on the classification capabilities of these models in distinguishing between compositions by male and female composers.</p>\n</div>\n<div id=\"S6.SS1.p2\" class=\"ltx_para\">\n<p id=\"S6.SS1.p2.1\" class=\"ltx_p\">The analysis reveals that, despite uniform training across all models, performance disparities emerge when testing on gender-specific data. The MM model, when tested on mixed-gender data, exhibited the best performance with an accuracy of 37.3% and a Tau-c of .778. This indicates a well-rounded capability to interpret compositions by both genders. However, a significant performance decline was observed in tests on data exclusively from women composers, with CQT\u2019s Tau-c dropping to .666 and accuracy to 31.6%. This underscores the importance of diverse and balanced evaluation sets to uncover model biases and ensure fair performance, as seen in the contrasted outcomes against gender-specific test data. Further research is needed to understand the performance decrease between male and female compositions, and whether it comes from musical characteristics or labeling biases.</p>\n</div>\n<figure id=\"S6.SS1.4\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE V: </span>Analysis of model performance differentiated by the composer\u2019s gender.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S6.SS1.4.4\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:390.3pt;height:479.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(99.8pt,-122.6pt) scale(2.0479794841506,2.0479794841506) ;\">\n<table id=\"S6.SS1.4.4.4\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.SS1.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.SS1.1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</th>\n<td id=\"S6.SS1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MSE</td>\n<td id=\"S6.SS1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Acc<sub id=\"S6.SS1.1.1.1.1.1.1\" class=\"ltx_sub\">0</sub>\n</td>\n<td id=\"S6.SS1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S6.SS1.2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S6.SS1.2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S6.SS1.2.2.2.2.1.1\" class=\"ltx_text ltx_markedasmath\">Both genres</span></th>\n<td id=\"S6.SS1.2.2.2.2.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.2.2.2.2.3\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.2.2.2.2.4\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.5.1\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.5.1.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.5.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S6.SS1.4.4.4.5.1.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.29(.18)</td>\n<td id=\"S6.SS1.4.4.4.5.1.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(1.61)</td>\n<td id=\"S6.SS1.4.4.4.5.1.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.741(.012)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.6.2\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.6.2.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.6.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S6.SS1.4.4.4.6.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.85(.07)</td>\n<td id=\"S6.SS1.4.4.4.6.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.7(.84)</td>\n<td id=\"S6.SS1.4.4.4.6.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.771(.005)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.7.3\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.7.3.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.7.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S6.SS1.4.4.4.7.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.7.3.3.1\" class=\"ltx_text ltx_font_bold\">1.81(.11)</span></td>\n<td id=\"S6.SS1.4.4.4.7.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.7.3.4.1\" class=\"ltx_text ltx_font_bold\">37.3(1.97)</span></td>\n<td id=\"S6.SS1.4.4.4.7.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.7.3.5.1\" class=\"ltx_text ltx_font_bold\">.778(.004)</span></td>\n</tr>\n<tr id=\"S6.SS1.3.3.3.3\" class=\"ltx_tr\">\n<th id=\"S6.SS1.3.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS1.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS1.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Only men</span>\n</th>\n<td id=\"S6.SS1.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.3.3.3.3.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.3.3.3.3.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.8.4\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.8.4.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.8.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S6.SS1.4.4.4.8.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.24(.19)</td>\n<td id=\"S6.SS1.4.4.4.8.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">33.2(1.13)</td>\n<td id=\"S6.SS1.4.4.4.8.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.734(.012)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.9.5\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.9.5.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.9.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S6.SS1.4.4.4.9.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.83(.05)</td>\n<td id=\"S6.SS1.4.4.4.9.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">37.3(.75)</td>\n<td id=\"S6.SS1.4.4.4.9.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.764(.004)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.10.6\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.10.6.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.10.6.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S6.SS1.4.4.4.10.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.10.6.3.1\" class=\"ltx_text ltx_font_bold\">1.81(.09)</span></td>\n<td id=\"S6.SS1.4.4.4.10.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.10.6.4.1\" class=\"ltx_text ltx_font_bold\">37.7(2.46)</span></td>\n<td id=\"S6.SS1.4.4.4.10.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.10.6.5.1\" class=\"ltx_text ltx_font_bold\">.769(.002)</span></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.4\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS1.4.4.4.4.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS1.4.4.4.4.1.2\" class=\"ltx_text ltx_markedasmath\">Only women</span>\n</th>\n<td id=\"S6.SS1.4.4.4.4.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.4.4.4.4.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.4.4.4.4.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.11.7\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.11.7.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.11.7.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S6.SS1.4.4.4.11.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.47(.59)</td>\n<td id=\"S6.SS1.4.4.4.11.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">31.6(4.74)</td>\n<td id=\"S6.SS1.4.4.4.11.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.666(.032)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.12.8\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.12.8.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.12.8.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S6.SS1.4.4.4.12.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.91(.60)</td>\n<td id=\"S6.SS1.4.4.4.12.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.6(4.24)</td>\n<td id=\"S6.SS1.4.4.4.12.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.722(.021)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.13.9\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.13.9.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.13.9.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S6.SS1.4.4.4.13.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.13.9.3.1\" class=\"ltx_text ltx_font_bold\">1.72(.49)</span></td>\n<td id=\"S6.SS1.4.4.4.13.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.13.9.4.1\" class=\"ltx_text ltx_font_bold\">35.7(5.74)</span></td>\n<td id=\"S6.SS1.4.4.4.13.9.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.13.9.5.1\" class=\"ltx_text ltx_font_bold\">.733(.025)</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S6.SS2\" class=\"ltx_subsection ltx_figure_panel\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S6.SS2.11.1.1\" class=\"ltx_text\">VI-B</span> </span><span id=\"S6.SS2.12.2\" class=\"ltx_text ltx_font_italic\">Zero-shot experiment on HV Benchmark</span>\n</h3>\n\n<div id=\"S6.SS2.p1\" class=\"ltx_para\">\n<p id=\"S6.SS2.p1.1\" class=\"ltx_p\">Historically underrepresented groups of composers, such as black women, have not been significantly explored in MIR datasets. We believe it is important to evaluate the capabilities of models in these out-of-distribution scenarios. In this experiment, we assess the ranking capabilities of the proposed approach in a zero-shot setting over the HV dataset by utilizing the model\u2019s logits. These logits follow a monotonic order, as indicated in Section\u00a0<a href=\"#S3.SS4\" title=\"III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-D</span></span></a>. Therefore, we can use them to order the HV collection and compare it with the original ranking.</p>\n</div>\n<div id=\"S6.SS2.p2\" class=\"ltx_para\">\n<p id=\"S6.SS2.p2.1\" class=\"ltx_p\">Table\u00a0<a href=\"#S6.SS2\" title=\"VI-B Zero-shot experiment on HV Benchmark \u2023 VI-A Model performance on works by female composers \u2023 VI A case study on woman composers \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">VI-B</span></span></a> shows the results obtained resorting to the Kendall rank correlation coefficient, <math id=\"S6.SS2.p2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.p2.1.m1.1a\"><msub id=\"S6.SS2.p2.1.m1.1.1\" xref=\"S6.SS2.p2.1.m1.1.1.cmml\"><mi id=\"S6.SS2.p2.1.m1.1.1.2\" xref=\"S6.SS2.p2.1.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.p2.1.m1.1.1.3\" xref=\"S6.SS2.p2.1.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.p2.1.m1.1b\"><apply id=\"S6.SS2.p2.1.m1.1.1.cmml\" xref=\"S6.SS2.p2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.p2.1.m1.1.1.1.cmml\" xref=\"S6.SS2.p2.1.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.p2.1.m1.1.1.2.cmml\" xref=\"S6.SS2.p2.1.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.p2.1.m1.1.1.3.cmml\" xref=\"S6.SS2.p2.1.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.p2.1.m1.1c\">\\tau_{c}</annotation></semantics></math>, for all data collections discussed in the experiment, considering both the single-task and multi-task models posed. Note that HV is only used for benchmarking purposes.</p>\n</div>\n<figure id=\"S6.SS2.6\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE VI: </span>Zero-shot experiment on Hidden Voices benchmark. The benchmark is a collection of piano pieces by black women composers, out of the distribution from the PSyllabus dataset.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S6.SS2.3.3\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:303.5pt;height:647pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(105.3pt,-224.5pt) scale(3.26768397143762,3.26768397143762) ;\">\n<table id=\"S6.SS2.3.3.3\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.SS2.3.3.3.4.1\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</td>\n<td id=\"S6.SS2.3.3.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S6.SS2.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.SS2.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S6.SS2.1.1.1.1.1.1\" class=\"ltx_text ltx_markedasmath\">Single-task</span></td>\n<td id=\"S6.SS2.1.1.1.1.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.5.2\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.5.2.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.5.2.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.5.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.592(.020)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.6.3\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.6.3.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.6.3.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.6.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.6.3.3.1\" class=\"ltx_text ltx_font_bold\">.661(.018)</span></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.7.4\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.7.4.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.7.4.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</td>\n<td id=\"S6.SS2.3.3.3.7.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.660(.029)</td>\n</tr>\n<tr id=\"S6.SS2.2.2.2.2\" class=\"ltx_tr\">\n<td id=\"S6.SS2.2.2.2.2.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS2.2.2.2.2.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS2.2.2.2.2.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with Era</span>\n</td>\n<td id=\"S6.SS2.2.2.2.2.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.8.5\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.8.5.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.8.5.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.8.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.634(.039)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.9.6\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.9.6.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.9.6.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.9.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.9.6.3.1\" class=\"ltx_text ltx_font_bold\">.668(.047)</span></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.3\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.3.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS2.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS2.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with Multiranking</span>\n</td>\n<td id=\"S6.SS2.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.10.7\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.10.7.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.10.7.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.10.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.632(.025)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.11.8\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.11.8.1\" class=\"ltx_td ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.11.8.2\" class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.11.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.11.8.3.1\" class=\"ltx_text ltx_font_bold\">.672(.031)</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S6.SS2.5.5\" class=\"ltx_p ltx_figure_panel\">In the zero-shot experiment conducted on the Hidden Voices benchmark, the findings underscore the advantages of multi-task models based on PR, particularly those that incorporate era information and multi-ranking strategies. The experiment reveals that the multi-task model utilizing era data with the PR approach achieved a <math id=\"S6.SS2.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.4.4.m1.1a\"><msub id=\"S6.SS2.4.4.m1.1.1\" xref=\"S6.SS2.4.4.m1.1.1.cmml\"><mi id=\"S6.SS2.4.4.m1.1.1.2\" xref=\"S6.SS2.4.4.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.4.4.m1.1.1.3\" xref=\"S6.SS2.4.4.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.4.4.m1.1b\"><apply id=\"S6.SS2.4.4.m1.1.1.cmml\" xref=\"S6.SS2.4.4.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.4.4.m1.1.1.1.cmml\" xref=\"S6.SS2.4.4.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.4.4.m1.1.1.2.cmml\" xref=\"S6.SS2.4.4.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.4.4.m1.1.1.3.cmml\" xref=\"S6.SS2.4.4.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.4.4.m1.1c\">\\tau_{c}</annotation></semantics></math> value of 0.668, emphasizing the importance of historical context in improving the performance of models on out-of-distribution data. Moreover, the PR model in the multi-task with Multiranking setup demonstrated the highest adaptability and performance, achieving a <math id=\"S6.SS2.5.5.m2.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.5.5.m2.1a\"><msub id=\"S6.SS2.5.5.m2.1.1\" xref=\"S6.SS2.5.5.m2.1.1.cmml\"><mi id=\"S6.SS2.5.5.m2.1.1.2\" xref=\"S6.SS2.5.5.m2.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.5.5.m2.1.1.3\" xref=\"S6.SS2.5.5.m2.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.5.5.m2.1b\"><apply id=\"S6.SS2.5.5.m2.1.1.cmml\" xref=\"S6.SS2.5.5.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.5.5.m2.1.1.1.cmml\" xref=\"S6.SS2.5.5.m2.1.1\">subscript</csymbol><ci id=\"S6.SS2.5.5.m2.1.1.2.cmml\" xref=\"S6.SS2.5.5.m2.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.5.5.m2.1.1.3.cmml\" xref=\"S6.SS2.5.5.m2.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.5.5.m2.1c\">\\tau_{c}</annotation></semantics></math> value of 0.672. This insight is vital for the development of inclusive machine-learning tools in the field of music research, highlighting the potential of combining multiple rankings to improve model performance on out-of-distribution datasets.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S6.SS2.6.6\" class=\"ltx_p ltx_figure_panel\">The results are not fully comparable with those of the HV with sheet music image presented in\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite> due to the dataset containing only 17 pieces. However, in the previous experiment, the best <math id=\"S6.SS2.6.6.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.6.6.m1.1a\"><msub id=\"S6.SS2.6.6.m1.1.1\" xref=\"S6.SS2.6.6.m1.1.1.cmml\"><mi id=\"S6.SS2.6.6.m1.1.1.2\" xref=\"S6.SS2.6.6.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.6.6.m1.1.1.3\" xref=\"S6.SS2.6.6.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.6.6.m1.1b\"><apply id=\"S6.SS2.6.6.m1.1.1.cmml\" xref=\"S6.SS2.6.6.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.6.6.m1.1.1.1.cmml\" xref=\"S6.SS2.6.6.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.6.6.m1.1.1.2.cmml\" xref=\"S6.SS2.6.6.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.6.6.m1.1.1.3.cmml\" xref=\"S6.SS2.6.6.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.6.6.m1.1c\">\\tau_{c}</annotation></semantics></math> result was 0.56, indicating that the audio-based version is more robust. Further research is needed to compare sheet music image classification audio to determine whether the modality or the quality of the datasets influences generalization.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S7\" class=\"ltx_section ltx_figure_panel\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">VII </span><span id=\"S7.1.1\" class=\"ltx_text ltx_font_smallcaps\">Conclusion</span>\n</h2>\n\n<div id=\"S7.p1\" class=\"ltx_para\">\n<p id=\"S7.p1.1\" class=\"ltx_p\">In conclusion, our study bridges a significant gap in music education and Music Information Retrieval by harnessing the widespread availability of audio via streaming platforms to estimate the difficulty of musical scores. This approach enhances students\u2019 ability to explore an extensive repertoire and collaborate with educators, significantly improving their learning experience and motivation. Our research introduces several key advancements.</p>\n</div>\n<div id=\"S7.p2\" class=\"ltx_para\">\n<p id=\"S7.p2.1\" class=\"ltx_p\">We have developed a model using a CNN+RNN+Attention network as a baseline for capturing performance difficulty from diverse audio representations. This model is assessed using a novel and expansive audio collection, setting a new benchmark for the scale of datasets in difficulty classification. Furthermore, we propose a multi-performance benchmark to investigate the impact of various performances on difficulty prediction, expanding the understanding of performance variability.</p>\n</div>\n<div id=\"S7.p3\" class=\"ltx_para\">\n<p id=\"S7.p3.1\" class=\"ltx_p\">Our comprehensive experimental framework includes testing generalization through a zero-shot scenario in out-of-domain distributions, employing multi-task learning with tasks related to music performance, and training across multiple difficulty rankings. These experiments underscore the robustness and versatility of our methodologies.</p>\n</div>\n<div id=\"S7.p4\" class=\"ltx_para\">\n<p id=\"S7.p4.1\" class=\"ltx_p\">To catalyze further research and collaboration within the music education community, we have publicly made our code, models, and the <em id=\"S7.p4.1.1\" class=\"ltx_emph ltx_font_italic\">Piano Syllabus (PSyllabus)</em> dataset available, including transcribed midis and CQT features. This initiative aims to create a shared platform for advancing automated performance difficulty understanding and enhancing music learning.</p>\n</div>\n<div id=\"S7.p5\" class=\"ltx_para\">\n<p id=\"S7.p5.1\" class=\"ltx_p\">Our contributions lay a solid foundation for the future of audio-based difficulty classification in music, demonstrating the potential for meaningful advancements in the automated analysis of musical scores. By providing comprehensive resources for the community, we invite educators, students, and researchers to engage with our work, fostering a collaborative ecosystem that supports the evolution of music education.</p>\n</div>\n<section id=\"Sx1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_font_smallcaps ltx_title_section\">Acknowledgment</h2>\n\n<div id=\"Sx1.p1\" class=\"ltx_para\">\n<p id=\"Sx1.p1.1\" class=\"ltx_p\">The authors thank Ian Wheaton, the creator of the Piano Syllabus web community, for his helpful guidance and for answering our questions. We are also grateful to him and the entire web community for their hard work in putting together the corpus to create a single source of information on piano difficulty. This effort is a strong base for our research, as we aim to use this resource to learn more and do deeper analysis. We hope our work benefits the Piano Syllabus and the music education community by aiding in the labeling of pieces and enabling the exploration of the forgotten cultural heritage.</p>\n</div>\n<div id=\"Sx1.p2\" class=\"ltx_para\">\n<p id=\"Sx1.p2.1\" class=\"ltx_p\">Pedro would also like to thank Nazif C. Tamer for his insistence on shifting focus to audio, and Pablo Alonso and Oguz Araz for their insightful discussions about audio features.</p>\n</div>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[1]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Mellizo, \u201cMusic education, curriculum design, and assessment: Imagining a more equitable approach,\u201d <em id=\"bib.bib1.1.1\" class=\"ltx_emph ltx_font_italic\">Music Educators Journal</em>, vol. 106, no.\u00a04, pp. 57\u201365, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[2]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0R. Jorgensen, \u201cThe curriculum design process in music,\u201d <em id=\"bib.bib2.1.1\" class=\"ltx_emph ltx_font_italic\">College Music Symposium</em>, vol.\u00a028, pp. 94\u2013105, 1988. [Online]. Available: <a target=\"_blank\" href=\"http://www.jstor.org/stable/40374590\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">http://www.jstor.org/stable/40374590</a>\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[3]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0S. Deconto, E.\u00a0L.\u00a0F. Valenga, and C.\u00a0N. Silla, \u201cAutomatic music score difficulty classification,\u201d in <em id=\"bib.bib3.1.1\" class=\"ltx_emph ltx_font_italic\">2023 30th International Conference on Systems, Signals and Image Processing (IWSSIP)</em>.\u00a0\u00a0\u00a0IEEE, 2023, pp. 1\u20135.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[4]</span>\n<span class=\"ltx_bibblock\">\nV.\u00a0S\u00e9bastien, H.\u00a0Ralambondrainy, O.\u00a0S\u00e9bastien, and N.\u00a0Conruyt, \u201cScore analyzer: Automatically determining scores difficulty level for instrumental e-learning,\u201d in <em id=\"bib.bib4.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of 13th International Society for Music Information Retrieval Conference, ISMIR</em>, Porto, Portugal, 2012.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[5]</span>\n<span class=\"ltx_bibblock\">\nS.-C. Chiu and M.-S. Chen, \u201cA study on difficulty level recognition of piano sheet music,\u201d in <em id=\"bib.bib5.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Symposium on Multimedia</em>.\u00a0\u00a0\u00a0IEEE, 2012, pp. 17\u201323.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[6]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura and K.\u00a0Yoshii, \u201cStatistical piano reduction controlling performance difficulty,\u201d <em id=\"bib.bib6.1.1\" class=\"ltx_emph ltx_font_italic\">APSIPA Transactions on Signal and Information Processing</em>, vol.\u00a07, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[7]</span>\n<span class=\"ltx_bibblock\">\n\u201cMusescore have automatic difficulty categories from year 2022,\u201d <a target=\"_blank\" href=\"https://musescore.com/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://musescore.com/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[8]</span>\n<span class=\"ltx_bibblock\">\n\u201cUltimate guitar have automatic difficulty categories from year 2022,\u201d <a target=\"_blank\" href=\"https://www.ultimate-guitar.com/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://www.ultimate-guitar.com/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[9]</span>\n<span class=\"ltx_bibblock\">\n\u201cSystem for estimating user\u2019s skill in playing a music instrument and determining virtual exercises thereof,\u201d Patent US9\u2009767\u2009705B1, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[10]</span>\n<span class=\"ltx_bibblock\">\nQ.\u00a0Kong, B.\u00a0Li, X.\u00a0Song, Y.\u00a0Wan, and Y.\u00a0Wang, \u201cHigh-resolution piano transcription with pedals by regressing onset and offset times,\u201d vol.\u00a029, p. 3707\u20133717, oct 2021.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[11]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura, N.\u00a0Ono, and S.\u00a0Sagayama, \u201cMerged-output hmm for piano fingering of both hands.\u201d in <em id=\"bib.bib11.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR</em>, Taipei, Taiwan, 2014, pp. 531\u2013536.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[12]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura and S.\u00a0Sagayama, \u201cAutomatic piano reduction from ensemble scores based on merged-output hidden markov model,\u201d in <em id=\"bib.bib12.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 41st International Computer Music Conference, ICMC</em>, Denton, USA, 2015.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[13]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, N.\u00a0C. Tamer, V.\u00a0Eremenko, M.\u00a0Miron, and X.\u00a0Serra, \u201cScore difficulty analysis for piano performance education,\u201d in <em id=\"bib.bib13.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP</em>, Singapore, Singapore, 2022, pp. 201\u2013205.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[14]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, D.\u00a0Jeong, V.\u00a0Eremenko, N.\u00a0C. Tamer, M.\u00a0Miron, and X.\u00a0Serra, \u201cCombining piano performance dimensions for score difficulty classification,\u201d <em id=\"bib.bib14.1.1\" class=\"ltx_emph ltx_font_italic\">Expert Systems with Applications</em>, vol. 238, p. 121776, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[15]</span>\n<span class=\"ltx_bibblock\">\nH.\u00a0Zhang, E.\u00a0Karystinaios, S.\u00a0Dixon, G.\u00a0Widmer, and C.\u00a0E. Cancino-Chac\u00f3n, \u201cSymbolic music representations for classification tasks: A systematic evaluation,\u201d <em id=\"bib.bib15.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2309.02567</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[16]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, D.\u00a0Jeong, E.\u00a0Nakamura, X.\u00a0Serra, and M.\u00a0Miron, \u201cAutomatic piano fingering from partially annotated scores using autoregressive neural networks,\u201d in <em id=\"bib.bib16.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 30th ACM International Conference on Multimedia</em>, 2022, pp. 6502\u20136510.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[17]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0Jeong, T.\u00a0Kwon, Y.\u00a0Kim, K.\u00a0Lee, and J.\u00a0Nam, \u201cVirtuosoNet: A hierarchical RNN-based system for modeling expressive piano performance,\u201d in <em id=\"bib.bib17.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR</em>, 2019, pp. 908\u2013915.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[18]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Cheng, Z.\u00a0Wang, and G.\u00a0Pollastri, \u201cA neural network approach to ordinal regression,\u201d in <em id=\"bib.bib18.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Joint Conference on Neural Networks, IJCNN</em>.\u00a0\u00a0\u00a0Hong Kong, China: IEEE, 2008, pp. 1279\u20131284.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[19]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0A.\u00a0V. V\u00e1squez, M.\u00a0Baelemans, J.\u00a0Driedger, W.\u00a0Zuidema, and J.\u00a0A. Burgoyne, \u201cQuantifying the ease of playing song chords on the guitar,\u201d in <em id=\"bib.bib19.1.1\" class=\"ltx_emph ltx_font_italic\">Ismir 2023 Hybrid Conference</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[20]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Toyama, T.\u00a0Akama, Y.\u00a0Ikemiya, Y.\u00a0Takida, W.-H. Liao, and Y.\u00a0Mitsufuji, \u201cAutomatic piano transcription with hierarchical frequency-time transformer,\u201d in <em id=\"bib.bib20.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[21]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Rege and R.\u00a0Sindal, \u201cReview of f0 estimation in the context of indian classical music expression detection,\u201d in <em id=\"bib.bib21.1.1\" class=\"ltx_emph ltx_font_italic\">Social Networking and Computational Intelligence: Proceedings of SCI-2018</em>.\u00a0\u00a0\u00a0Springer, 2020, pp. 257\u2013268.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[22]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Alonso-Jim\u00e9nez, X.\u00a0Serra, and D.\u00a0Bogdanov, \u201cMusic representation learning based on editorial metadata from discogs,\u201d in <em id=\"bib.bib22.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[23]</span>\n<span class=\"ltx_bibblock\">\nC.\u00a0Sch\u00f6rkhuber and A.\u00a0Klapuri, \u201cConstant-q transform toolbox for music processing,\u201d in <em id=\"bib.bib23.1.1\" class=\"ltx_emph ltx_font_italic\">7th sound and music computing conference, Barcelona, Spain</em>, 2010, pp. 3\u201364.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[24]</span>\n<span class=\"ltx_bibblock\">\nQ.\u00a0Kong, B.\u00a0Li, J.\u00a0Chen, and Y.\u00a0Wang, \u201cGiantmidi-piano: A large-scale MIDI dataset for classical piano music,\u201d <em id=\"bib.bib24.1.1\" class=\"ltx_emph ltx_font_italic\">Transactions of the International Society for Music Information Retrieval, 5(1), pp.87\u201398</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[25]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, J.\u00a0J. Valero-Mas, D.\u00a0Jeong, and X.\u00a0Serra, \u201cPredicting performance difficulty from piano sheet music images,\u201d in <em id=\"bib.bib25.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. of the 24th Int. Society for Music Information Retrieval Conf.</em>, Milan, Italy, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[26]</span>\n<span class=\"ltx_bibblock\">\nUniversity of Colorado, \u201cHidden voices project,\u201d <a target=\"_blank\" href=\"https://www.colorado.edu/project/hidden-voices/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://www.colorado.edu/project/hidden-voices/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[27]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0F. Gemmeke, D.\u00a0P. Ellis, D.\u00a0Freedman, A.\u00a0Jansen, W.\u00a0Lawrence, R.\u00a0C. Moore, M.\u00a0Plakal, and M.\u00a0Ritter, \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in <em id=\"bib.bib27.1.1\" class=\"ltx_emph ltx_font_italic\">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.\u00a0\u00a0\u00a0IEEE, 2017, pp. 776\u2013780.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[28]</span>\n<span class=\"ltx_bibblock\">\nV.\u00a0Lakic, L.\u00a0Rossetto, and A.\u00a0Bernstein, \u201cLink-rot in web-sourced multimedia datasets,\u201d in <em id=\"bib.bib28.1.1\" class=\"ltx_emph ltx_font_italic\">MultiMedia Modeling</em>.\u00a0\u00a0\u00a0Cham: Springer International Publishing, 2023, pp. 476\u2013488.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[29]</span>\n<span class=\"ltx_bibblock\">\nN.\u00a0Kato, E.\u00a0Nakamura, K.\u00a0Mine, O.\u00a0Doeda, and M.\u00a0Yamada, \u201cComputational analysis of audio recordings of piano performance for automatic evaluation,\u201d in <em id=\"bib.bib29.1.1\" class=\"ltx_emph ltx_font_italic\">Responsive and Sustainable Educational Futures</em>.\u00a0\u00a0\u00a0Cham: Springer Nature Switzerland, 2023, pp. 586\u2013592.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[30]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0O\u2019Hanlon and M.\u00a0B. Sandler, \u201cComparing cqt and reassignment based chroma features for template-based automatic chord recognition,\u201d in <em id=\"bib.bib30.1.1\" class=\"ltx_emph ltx_font_italic\">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP</em>.\u00a0\u00a0\u00a0IEEE, 2019, pp. 860\u2013864.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[31]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda and G.\u00a0Bernardes, \u201cRevisiting harmonic change detection,\u201d in <em id=\"bib.bib31.1.1\" class=\"ltx_emph ltx_font_italic\">Audio Engineering Society Convention 149</em>, Oct 2020.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[32]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Wang, A.\u00a0Singh, J.\u00a0Michael, F.\u00a0Hill, O.\u00a0Levy, and S.\u00a0R. Bowman, \u201cGlue: A multi-task benchmark and analysis platform for natural language understanding,\u201d in <em id=\"bib.bib32.1.1\" class=\"ltx_emph ltx_font_italic\">7th International Conference on Learning Representations, ICLR</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[33]</span>\n<span class=\"ltx_bibblock\">\nI.\u00a0Laina, C.\u00a0Rupprecht, V.\u00a0Belagiannis, F.\u00a0Tombari, and N.\u00a0Navab, \u201cDeeper depth prediction with fully convolutional residual networks,\u201d in <em id=\"bib.bib33.1.1\" class=\"ltx_emph ltx_font_italic\">2016 Fourth international conference on 3D vision (3DV)</em>.\u00a0\u00a0\u00a0IEEE, 2016, pp. 239\u2013248.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[34]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Wu, C.\u00a0Shen, and A.\u00a0Van Den\u00a0Hengel, \u201cWider or deeper: Revisiting the resnet model for visual recognition,\u201d <em id=\"bib.bib34.1.1\" class=\"ltx_emph ltx_font_italic\">Pattern Recognition</em>, vol.\u00a090, pp. 119\u2013133, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[35]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Won, K.\u00a0Choi, and X.\u00a0Serra, \u201cSemi-supervised music tagging transformer,\u201d in <em id=\"bib.bib35.1.1\" class=\"ltx_emph ltx_font_italic\">International Society for Music Information Retrieval Conference</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[36]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Allen-Zhu and Y.\u00a0Li, \u201cWhat can resnet learn efficiently, going beyond kernels?\u201d <em id=\"bib.bib36.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, vol.\u00a032, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[37]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Cho, B.\u00a0van Merrienboer, \u00c7.\u00a0G\u00fcl\u00e7ehre, D.\u00a0Bahdanau, F.\u00a0Bougares, H.\u00a0Schwenk, and Y.\u00a0Bengio, \u201cLearning phrase representations using RNN encoder-decoder for statistical machine translation,\u201d in <em id=\"bib.bib37.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL</em>, A.\u00a0Moschitti, B.\u00a0Pang, and W.\u00a0Daelemans, Eds.\u00a0\u00a0\u00a0ACL, 2014, pp. 1724\u20131734. [Online]. Available: <a target=\"_blank\" href=\"https://doi.org/10.3115/v1/d14-1179\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://doi.org/10.3115/v1/d14-1179</a>\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[38]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Chung, \u00c7.\u00a0G\u00fcl\u00e7ehre, K.\u00a0Cho, and Y.\u00a0Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d <em id=\"bib.bib38.1.1\" class=\"ltx_emph ltx_font_italic\">CoRR</em>, vol. abs/1412.3555, 2014. [Online]. Available: <a target=\"_blank\" href=\"http://arxiv.org/abs/1412.3555\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">http://arxiv.org/abs/1412.3555</a>\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[39]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Yang, D.\u00a0Yang, C.\u00a0Dyer, X.\u00a0He, A.\u00a0Smola, and E.\u00a0Hovy, \u201cHierarchical attention networks for document classification,\u201d in <em id=\"bib.bib39.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</em>, 2016, pp. 1480\u20131489.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[40]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Maghoumi and J.\u00a0J. LaViola, \u201cDeepgru: Deep gesture recognition utility,\u201d in <em id=\"bib.bib40.1.1\" class=\"ltx_emph ltx_font_italic\">International Symposium on Visual Computing</em>.\u00a0\u00a0\u00a0Springer, 2019, pp. 16\u201331.\n\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[41]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Alfaro-Contreras, J.\u00a0J. Valero-Mas, J.\u00a0M. I\u00f1esta, and J.\u00a0Calvo-Zaragoza, \u201cLate multimodal fusion for image and audio music transcription,\u201d <em id=\"bib.bib41.1.1\" class=\"ltx_emph ltx_font_italic\">Expert Systems with Applications</em>, vol. 216, p. 119491, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[42]</span>\n<span class=\"ltx_bibblock\">\nL.\u00a0Gaudette and N.\u00a0Japkowicz, \u201cEvaluation methods for ordinal classification,\u201d in <em id=\"bib.bib42.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 22nd Canadian Conference on Advances in Artificial Intelligence</em>, Kelowna, Canada, 2009, pp. 207\u2013210.\n\n</span>\n</li>\n</ul>\n</section>\n<figure id=\"Sx1.1\" class=\"ltx_float biography\">\n<table id=\"Sx1.1.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.1.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.1.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/pedro.png\" id=\"Sx1.1.1.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"100\" height=\"110\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.1.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.1.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.1.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Pedro Ramoneda</span> \nPedro Ramoneda holds a BSc in Computer Science from the University of Zaragoza, a Professional Degree in Piano Performance from the Conservatory of Music in Zaragoza, and an MSc in Sound and Music Computing from the Universitat Pompeu Fabra. He is currently a third-year PhD student in the Music Technology Group of the Universitat Pompeu Fabra under the supervision of Prof. Xavier Serra, focusing on the use of technologies from the Music Information Retrieval and Signal Processing field for supporting music education.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.2\" class=\"ltx_float biography\">\n<table id=\"Sx1.2.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.2.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.2.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/Minhee_Lee.jpeg\" id=\"Sx1.2.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"94\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.2.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.2.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.2.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Minhee Lee</span> \nMinhee Lee is an undergraduate student pursuing a B.S. degree in the Department of Computer Science and Engineering at Sogang University in South Korea. She is an undergraduate intern in the MALer Lab under the supervision of Prof. Dasaem Jeong since 2023. Before joining the research group, she did internships as a software engineer at Google in 2021 and 2022, and at FuriosaAI in 2022. Her research interest is about various music information retrieval tasks of understanding music with deep learning technologies.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.3\" class=\"ltx_float biography\">\n<table id=\"Sx1.3.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.3.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.3.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/Dasaem_Jeong.jpg\" id=\"Sx1.3.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"100\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.3.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.3.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.3.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.3.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Dasaem Jeong</span>  is currently working as an Assistant Professor in the Department of Art &amp; Technology at Sogang University in South Korea since 2021. Before joining Sogang University, he worked as a research scientist in T-Brain X, SK Telecom from 2020 to 2021. He obtained his Ph.D. and M.S. degrees in culture technology, and B.S. in mechanical engineering from Korea Advanced Institute of Science and Technology (KAIST). His research primarily focuses on a diverse range of music information retrieval tasks, including music generation and computational musicology.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.4\" class=\"ltx_float biography\">\n<table id=\"Sx1.4.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.4.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.4.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/JJVM.png\" id=\"Sx1.4.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"85\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.4.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.4.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.4.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.4.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Jose J. Valero-Mas</span>  received the M.Sc. degree in Telecommunications Engineering from the University Miguel Hern\u00e1ndez of Elche in 2012, the M.Sc. degree in Sound and Music Computing from the Universitat Pompeu Fabra in 2013, and the Ph.D. degree in Computer Science from the University of Alicante in 2017. After a three-year period in industry in which he developed as a data scientist, he acted as a Postdoctoral Researcher from 2020 to 2023 in the University of Alicante, the Universitat Pompeu Fabra, and the Queen Mary University of London. He is currently an Assistant Professor in the Department of Software and Computing Systems of the University of Alicante. His research interests include Pattern Recognition, Machine Learning, Music Information Retrieval, and Signal Processing for which he has co-authored more than 40 works within international journals, conference communications, and book chapters.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.5\" class=\"ltx_float biography\">\n<table id=\"Sx1.5.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.5.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.5.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/serra.png\" id=\"Sx1.5.1.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"100\" height=\"110\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.5.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.5.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.5.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.5.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Xavier Serra</span> \nHe has had a progressive academic career in the field of music technology since earning his PhD from Stanford University in 1989. His interest lies in the analysis, synthesis, and description of sound and music signals. He always seeks a balance between basic and applied research while integrating both scientific/technological and humanistic/artistic disciplines. Currently, he supervises research at the Music Technology Group (MTG), focusing on understanding sound and music signals through signal processing, machine learning, and semantic technologies. His work emphasizes data-driven and knowledge-driven methodologies, involving the development of large data collections and the application of domain-specific knowledge. He leads projects funded both publicly and privately, tackling practical issues such as music exploration, sound classification, and music performance analysis for educational purposes. He champions open science by promoting open data, software, and access, and is keen on using open innovation strategies to enhance the social and economic impact of his research.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n</section>\n</section>\n</div>\n</div>\n</figure>\n</section>\n</div>\n</div>\n</figure>\n</section>\n</section>\n</section>\n</div>\n</div>\n</figure>\n</section>\n</div>\n</div>\n</figure>\n",
        "footnotes": [],
        "references": []
    },
    "S5.SS1.20": {
        "caption": "TABLE III:  Multi-tasks experiments training the models on the PSyllabus dataset and auxiliary tasks.  TABLE IV:  Comparative analysis for Basic and Multi-task with Era experiments across musical periods. TABLE V:  Analysis of model performance differentiated by the composer\u2019s gender. TABLE VI:  Zero-shot experiment on Hidden Voices benchmark. The benchmark is a collection of piano pieces by black women composers, out of the distribution from the PSyllabus dataset.",
        "table": "<figure id=\"S5.SS1.20\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE III: </span>Multi-tasks experiments training the models on the PSyllabus dataset and auxiliary tasks. </figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S5.SS1.11.11\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:390.3pt;height:416.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(85.5pt,-91.3pt) scale(1.7799857981267,1.7799857981267) ;\">\n<table id=\"S5.SS1.11.11.11\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.SS1.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.SS1.1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</th>\n<td id=\"S5.SS1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MSE</td>\n<td id=\"S5.SS1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Acc<sub id=\"S5.SS1.1.1.1.1.1.1\" class=\"ltx_sub\">0</sub>\n</td>\n<td id=\"S5.SS1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S5.SS1.2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S5.SS1.2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S5.SS1.2.2.2.2.1.1\" class=\"ltx_text ltx_markedasmath\">Single-task</span></th>\n<td id=\"S5.SS1.2.2.2.2.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.2.2.2.2.3\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.2.2.2.2.4\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.SS1.11.11.11.12.1\" class=\"ltx_tr\">\n<th id=\"S5.SS1.11.11.11.12.1.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.11.11.11.12.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.SS1.11.11.11.12.1.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.29(.18)</td>\n<td id=\"S5.SS1.11.11.11.12.1.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(1.61)</td>\n<td id=\"S5.SS1.11.11.11.12.1.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.741(.012)</td>\n</tr>\n<tr id=\"S5.SS1.11.11.11.13.2\" class=\"ltx_tr\">\n<th id=\"S5.SS1.11.11.11.13.2.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.11.11.11.13.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.SS1.11.11.11.13.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.85(.07)</td>\n<td id=\"S5.SS1.11.11.11.13.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.7(.84)</td>\n<td id=\"S5.SS1.11.11.11.13.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.771(.005)</td>\n</tr>\n<tr id=\"S5.SS1.3.3.3.3\" class=\"ltx_tr\">\n<th id=\"S5.SS1.3.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S5.SS1.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S5.SS1.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with era</span>\n</th>\n<td id=\"S5.SS1.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.3.3.3.3.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.3.3.3.3.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.SS1.4.4.4.4\" class=\"ltx_tr\">\n<th id=\"S5.SS1.4.4.4.4.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.4.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT<sub id=\"S5.SS1.4.4.4.4.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.4.4.4.4.1.1.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub>\n</th>\n<td id=\"S5.SS1.4.4.4.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.99(.15)</td>\n<td id=\"S5.SS1.4.4.4.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.6(.97)</td>\n<td id=\"S5.SS1.4.4.4.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.766(.008)</td>\n</tr>\n<tr id=\"S5.SS1.5.5.5.5\" class=\"ltx_tr\">\n<th id=\"S5.SS1.5.5.5.5.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.5.5.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR<sub id=\"S5.SS1.5.5.5.5.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.5.5.5.5.1.1.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub>\n</th>\n<td id=\"S5.SS1.5.5.5.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.SS1.5.5.5.5.3.1\" class=\"ltx_text ltx_font_bold\">1.83(.10)</span></td>\n<td id=\"S5.SS1.5.5.5.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.SS1.5.5.5.5.4.1\" class=\"ltx_text ltx_font_bold\">37.7(.91)</span></td>\n<td id=\"S5.SS1.5.5.5.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.SS1.5.5.5.5.5.1\" class=\"ltx_text ltx_font_bold\">.775(.008)</span></td>\n</tr>\n<tr id=\"S5.SS1.6.6.6.6\" class=\"ltx_tr\">\n<th id=\"S5.SS1.6.6.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S5.SS1.6.6.6.6.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S5.SS1.6.6.6.6.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with composer</span>\n</th>\n<td id=\"S5.SS1.6.6.6.6.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.6.6.6.6.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.6.6.6.6.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.SS1.7.7.7.7\" class=\"ltx_tr\">\n<th id=\"S5.SS1.7.7.7.7.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.7.7.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT<sub id=\"S5.SS1.7.7.7.7.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.7.7.7.7.1.1.1\" class=\"ltx_text ltx_font_italic\">Composer</span></sub>\n</th>\n<td id=\"S5.SS1.7.7.7.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.74(.16)</td>\n<td id=\"S5.SS1.7.7.7.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">27.0(1.09)</td>\n<td id=\"S5.SS1.7.7.7.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.701(.008)</td>\n</tr>\n<tr id=\"S5.SS1.8.8.8.8\" class=\"ltx_tr\">\n<th id=\"S5.SS1.8.8.8.8.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.8.8.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR<sub id=\"S5.SS1.8.8.8.8.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.8.8.8.8.1.1.1\" class=\"ltx_text ltx_font_italic\">Composer</span></sub>\n</th>\n<td id=\"S5.SS1.8.8.8.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.57(.18)</td>\n<td id=\"S5.SS1.8.8.8.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.0(1.09)</td>\n<td id=\"S5.SS1.8.8.8.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.708(.007)</td>\n</tr>\n<tr id=\"S5.SS1.9.9.9.9\" class=\"ltx_tr\">\n<th id=\"S5.SS1.9.9.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S5.SS1.9.9.9.9.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S5.SS1.9.9.9.9.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with multiple ranks</span>\n</th>\n<td id=\"S5.SS1.9.9.9.9.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.9.9.9.9.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S5.SS1.9.9.9.9.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S5.SS1.10.10.10.10\" class=\"ltx_tr\">\n<th id=\"S5.SS1.10.10.10.10.2\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.10.10.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT<sub id=\"S5.SS1.10.10.10.10.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.10.10.10.10.1.1.1\" class=\"ltx_text ltx_font_italic\">MultiRank</span></sub>\n</th>\n<td id=\"S5.SS1.10.10.10.10.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.16(.16)</td>\n<td id=\"S5.SS1.10.10.10.10.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.4(1.35)</td>\n<td id=\"S5.SS1.10.10.10.10.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.754(.007)</td>\n</tr>\n<tr id=\"S5.SS1.11.11.11.11\" class=\"ltx_tr\">\n<th id=\"S5.SS1.11.11.11.11.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.SS1.11.11.11.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR<sub id=\"S5.SS1.11.11.11.11.1.1\" class=\"ltx_sub\"><span id=\"S5.SS1.11.11.11.11.1.1.1\" class=\"ltx_text ltx_font_italic\">MultiRank</span></sub>\n</th>\n<td id=\"S5.SS1.11.11.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.88(.06)</td>\n<td id=\"S5.SS1.11.11.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.4(.61)</td>\n<td id=\"S5.SS1.11.11.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.768(.003)</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.20.21\" class=\"ltx_p ltx_figure_panel\">In our exploration of multi-task learning\u2019s influence on model efficacy, we conducted a series of experiments shown in Table\u00a0<a href=\"#S5\" title=\"V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">V</span></a>. These experiments aim to measure the impact of incorporating auxiliary tasks\u2014namely, musical era, composer identification, and multiple rankings\u2014on models trained using the PSyllabus difficulty labels.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.20.22\" class=\"ltx_p ltx_figure_panel\">Initial single-task experiments, described in Section\u00a0<a href=\"#S5\" title=\"V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> employing CQT and PR, established our baseline. Among these, PR emerged as the more potent baseline.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.15.15\" class=\"ltx_p ltx_figure_panel\">The integration of multi-task learning, particularly with the musical era (represented as CQT<sub id=\"S5.SS1.15.15.1\" class=\"ltx_sub\"><span id=\"S5.SS1.15.15.1.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub> and PR<sub id=\"S5.SS1.15.15.2\" class=\"ltx_sub\"><span id=\"S5.SS1.15.15.2.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub>), resulted in discernible performance enhancements. PR<sub id=\"S5.SS1.15.15.3\" class=\"ltx_sub\"><span id=\"S5.SS1.15.15.3.1\" class=\"ltx_text ltx_font_italic\">Era</span></sub>, for instance, manifested a reduction in MSE to 1.83 and an increase in <math id=\"S5.SS1.15.15.m4.1\" class=\"ltx_Math\" alttext=\"Acc_{0}\" display=\"inline\"><semantics id=\"S5.SS1.15.15.m4.1a\"><mrow id=\"S5.SS1.15.15.m4.1.1\" xref=\"S5.SS1.15.15.m4.1.1.cmml\"><mi id=\"S5.SS1.15.15.m4.1.1.2\" xref=\"S5.SS1.15.15.m4.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.SS1.15.15.m4.1.1.1\" xref=\"S5.SS1.15.15.m4.1.1.1.cmml\">\u200b</mo><mi id=\"S5.SS1.15.15.m4.1.1.3\" xref=\"S5.SS1.15.15.m4.1.1.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.SS1.15.15.m4.1.1.1a\" xref=\"S5.SS1.15.15.m4.1.1.1.cmml\">\u200b</mo><msub id=\"S5.SS1.15.15.m4.1.1.4\" xref=\"S5.SS1.15.15.m4.1.1.4.cmml\"><mi id=\"S5.SS1.15.15.m4.1.1.4.2\" xref=\"S5.SS1.15.15.m4.1.1.4.2.cmml\">c</mi><mn id=\"S5.SS1.15.15.m4.1.1.4.3\" xref=\"S5.SS1.15.15.m4.1.1.4.3.cmml\">0</mn></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.15.15.m4.1b\"><apply id=\"S5.SS1.15.15.m4.1.1.cmml\" xref=\"S5.SS1.15.15.m4.1.1\"><times id=\"S5.SS1.15.15.m4.1.1.1.cmml\" xref=\"S5.SS1.15.15.m4.1.1.1\"></times><ci id=\"S5.SS1.15.15.m4.1.1.2.cmml\" xref=\"S5.SS1.15.15.m4.1.1.2\">\ud835\udc34</ci><ci id=\"S5.SS1.15.15.m4.1.1.3.cmml\" xref=\"S5.SS1.15.15.m4.1.1.3\">\ud835\udc50</ci><apply id=\"S5.SS1.15.15.m4.1.1.4.cmml\" xref=\"S5.SS1.15.15.m4.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.15.15.m4.1.1.4.1.cmml\" xref=\"S5.SS1.15.15.m4.1.1.4\">subscript</csymbol><ci id=\"S5.SS1.15.15.m4.1.1.4.2.cmml\" xref=\"S5.SS1.15.15.m4.1.1.4.2\">\ud835\udc50</ci><cn type=\"integer\" id=\"S5.SS1.15.15.m4.1.1.4.3.cmml\" xref=\"S5.SS1.15.15.m4.1.1.4.3\">0</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.15.15.m4.1c\">Acc_{0}</annotation></semantics></math> to 37.7%, illustrating the advantages of embedding contextual musical insights into the learning process.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.17.17\" class=\"ltx_p ltx_figure_panel\">Nevertheless, when composer identification was introduced as an auxiliary task (via CQT<sub id=\"S5.SS1.17.17.1\" class=\"ltx_sub\"><span id=\"S5.SS1.17.17.1.1\" class=\"ltx_text ltx_font_italic\">Composer</span></sub> and PR<sub id=\"S5.SS1.17.17.2\" class=\"ltx_sub\"><span id=\"S5.SS1.17.17.2.1\" class=\"ltx_text ltx_font_italic\">Composer</span></sub>), we observed a decrease in performance relative to the baseline. This outcome suggests that the added complexity from composer identification might not synergize well with the primary task, possibly because the diverse stylistic idiosyncrasies of composers could impede the model\u2019s generalization capabilities.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S5.SS1.20.20\" class=\"ltx_p ltx_figure_panel\">Experiments involving the multiple ranking task (CQT<sub id=\"S5.SS1.20.20.1\" class=\"ltx_sub\"><span id=\"S5.SS1.20.20.1.1\" class=\"ltx_text ltx_font_italic\">MultiRank</span></sub> and PR<sub id=\"S5.SS1.20.20.2\" class=\"ltx_sub\"><span id=\"S5.SS1.20.20.2.1\" class=\"ltx_text ltx_font_italic\">MultiRank</span></sub>) yielded mixed outcomes. Although there was a slight improvement in MSE and <math id=\"S5.SS1.20.20.m3.1\" class=\"ltx_Math\" alttext=\"Acc_{0}\" display=\"inline\"><semantics id=\"S5.SS1.20.20.m3.1a\"><mrow id=\"S5.SS1.20.20.m3.1.1\" xref=\"S5.SS1.20.20.m3.1.1.cmml\"><mi id=\"S5.SS1.20.20.m3.1.1.2\" xref=\"S5.SS1.20.20.m3.1.1.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.SS1.20.20.m3.1.1.1\" xref=\"S5.SS1.20.20.m3.1.1.1.cmml\">\u200b</mo><mi id=\"S5.SS1.20.20.m3.1.1.3\" xref=\"S5.SS1.20.20.m3.1.1.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.SS1.20.20.m3.1.1.1a\" xref=\"S5.SS1.20.20.m3.1.1.1.cmml\">\u200b</mo><msub id=\"S5.SS1.20.20.m3.1.1.4\" xref=\"S5.SS1.20.20.m3.1.1.4.cmml\"><mi id=\"S5.SS1.20.20.m3.1.1.4.2\" xref=\"S5.SS1.20.20.m3.1.1.4.2.cmml\">c</mi><mn id=\"S5.SS1.20.20.m3.1.1.4.3\" xref=\"S5.SS1.20.20.m3.1.1.4.3.cmml\">0</mn></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.20.20.m3.1b\"><apply id=\"S5.SS1.20.20.m3.1.1.cmml\" xref=\"S5.SS1.20.20.m3.1.1\"><times id=\"S5.SS1.20.20.m3.1.1.1.cmml\" xref=\"S5.SS1.20.20.m3.1.1.1\"></times><ci id=\"S5.SS1.20.20.m3.1.1.2.cmml\" xref=\"S5.SS1.20.20.m3.1.1.2\">\ud835\udc34</ci><ci id=\"S5.SS1.20.20.m3.1.1.3.cmml\" xref=\"S5.SS1.20.20.m3.1.1.3\">\ud835\udc50</ci><apply id=\"S5.SS1.20.20.m3.1.1.4.cmml\" xref=\"S5.SS1.20.20.m3.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S5.SS1.20.20.m3.1.1.4.1.cmml\" xref=\"S5.SS1.20.20.m3.1.1.4\">subscript</csymbol><ci id=\"S5.SS1.20.20.m3.1.1.4.2.cmml\" xref=\"S5.SS1.20.20.m3.1.1.4.2\">\ud835\udc50</ci><cn type=\"integer\" id=\"S5.SS1.20.20.m3.1.1.4.3.cmml\" xref=\"S5.SS1.20.20.m3.1.1.4.3\">0</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.SS1.20.20.m3.1c\">Acc_{0}</annotation></semantics></math> compared to the single-task baseline, these enhancements were not as pronounced as those observed with the musical era task, indicating a moderate benefit from incorporating ranking information. The outcomes for multiple ranking tasks stand in contrast to those involving music sheet images in previous research\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite>, where the multi-rank approach outperformed other experiments. However, a direct comparison between these results is not straightforward due to differences in the datasets used, and further research is needed to compare both approaches.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S5.SS1.SSS0.Px1\" class=\"ltx_paragraph ltx_figure_panel\">\n<h5 class=\"ltx_title ltx_title_paragraph\">Impact of Musical Era on Difficulty Estimation.</h5>\n\n<div id=\"S5.SS1.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"S5.SS1.SSS0.Px1.p1.1\" class=\"ltx_p\">Our analysis, as shown in Table\u00a0<a href=\"#S5.T4\" title=\"TABLE IV \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a>, investigates the auxiliary task of era identification on difficulty prediction across musical periods. The inclusion of era significantly enhances model performance, as evidenced by improvements in Mean Squared Error (MSE), Accuracy at zero (Acc<sub id=\"S5.SS1.SSS0.Px1.p1.1.1\" class=\"ltx_sub\">0</sub>), and Tau-c metrics.</p>\n</div>\n<div id=\"S5.SS1.SSS0.Px1.p2\" class=\"ltx_para\">\n<p id=\"S5.SS1.SSS0.Px1.p2.2\" class=\"ltx_p\">For the Baroque period, we observed MSE improvements from 2.73 to 2.49 (CQT) and 2.25 to 2.03 (PR), with corresponding increases in Acc<sub id=\"S5.SS1.SSS0.Px1.p2.2.1\" class=\"ltx_sub\">0</sub>. The Classical period saw similar enhancements, with MSE decreasing to 1.96 (CQT) and Acc<sub id=\"S5.SS1.SSS0.Px1.p2.2.2\" class=\"ltx_sub\">0</sub> improving across methods. The Romantic and Modern periods further confirmed these trends, demonstrating the general efficacy of integrating era information into difficulty estimation models.</p>\n</div>\n<div id=\"S5.SS1.SSS0.Px1.p3\" class=\"ltx_para\">\n<p id=\"S5.SS1.SSS0.Px1.p3.1\" class=\"ltx_p\">These findings highlight the significance of era-specific characteristics in music difficulty prediction, suggesting a nuanced relationship between historical context and model accuracy. The varied improvements across periods underscore the potential of tailored, era-specific modeling approaches to enhance prediction accuracy.</p>\n</div>\n<figure id=\"S5.T4\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE IV: </span>Comparative analysis for Basic and Multi-task with Era experiments across musical periods.</figcaption>\n<div id=\"S5.T4.1\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:346.9pt;height:182.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-31.4pt,16.6pt) scale(0.846604287885487,0.846604287885487) ;\">\n<table id=\"S5.T4.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Period</th>\n<th id=\"S5.T4.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Method</th>\n<th id=\"S5.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">MSE</th>\n<th id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Acc<sub id=\"S5.T4.1.1.1.1.1\" class=\"ltx_sub\">0</sub>\n</th>\n<th id=\"S5.T4.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Tau-c</th>\n</tr>\n<tr id=\"S5.T4.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.2.1.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.T4.1.1.2.1.2\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.T4.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Single</th>\n<th id=\"S5.T4.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">with Era</th>\n<th id=\"S5.T4.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Single</th>\n<th id=\"S5.T4.1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">with Era</th>\n<th id=\"S5.T4.1.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Single</th>\n<th id=\"S5.T4.1.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">with Era</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.3.1.1.1\" class=\"ltx_text\">Baroque</span></th>\n<th id=\"S5.T4.1.1.3.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.73(.49)</td>\n<td id=\"S5.T4.1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.49(.42)</td>\n<td id=\"S5.T4.1.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.1(4.82)</td>\n<td id=\"S5.T4.1.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.3(5.51)</td>\n<td id=\"S5.T4.1.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.621(.041)</td>\n<td id=\"S5.T4.1.1.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.631(.036)</td>\n</tr>\n<tr id=\"S5.T4.1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.4.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.25(.50)</td>\n<td id=\"S5.T4.1.1.4.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.4.2.3.1\" class=\"ltx_text ltx_font_bold\">2.03(.62)</span></td>\n<td id=\"S5.T4.1.1.4.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(4.69)</td>\n<td id=\"S5.T4.1.1.4.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.4.2.5.1\" class=\"ltx_text ltx_font_bold\">34.2(6.46)</span></td>\n<td id=\"S5.T4.1.1.4.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.702(.066)</td>\n<td id=\"S5.T4.1.1.4.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.4.2.7.1\" class=\"ltx_text ltx_font_bold\">.707(.088)</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.5.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.5.3.1.1\" class=\"ltx_text\">Classical</span></th>\n<th id=\"S5.T4.1.1.5.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.25(.66)</td>\n<td id=\"S5.T4.1.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.5.3.4.1\" class=\"ltx_text ltx_font_bold\">1.96(.51)</span></td>\n<td id=\"S5.T4.1.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.2(6.06)</td>\n<td id=\"S5.T4.1.1.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.1(5.55)</td>\n<td id=\"S5.T4.1.1.5.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.698(.088)</td>\n<td id=\"S5.T4.1.1.5.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.719(.073)</td>\n</tr>\n<tr id=\"S5.T4.1.1.6.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.6.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.97(.79)</td>\n<td id=\"S5.T4.1.1.6.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.98(.45)</td>\n<td id=\"S5.T4.1.1.6.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.2(8.23)</td>\n<td id=\"S5.T4.1.1.6.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.6.4.5.1\" class=\"ltx_text ltx_font_bold\">34.3(4.90)</span></td>\n<td id=\"S5.T4.1.1.6.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.726(.093)</td>\n<td id=\"S5.T4.1.1.6.4.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.6.4.7.1\" class=\"ltx_text ltx_font_bold\">.732(.067)</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.7.5\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.7.5.1.1\" class=\"ltx_text\">Romantic</span></th>\n<th id=\"S5.T4.1.1.7.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.16(.56)</td>\n<td id=\"S5.T4.1.1.7.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.05(.44)</td>\n<td id=\"S5.T4.1.1.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.6(5.41)</td>\n<td id=\"S5.T4.1.1.7.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.1(4.50)</td>\n<td id=\"S5.T4.1.1.7.5.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.704(.071)</td>\n<td id=\"S5.T4.1.1.7.5.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.724(.062)</td>\n</tr>\n<tr id=\"S5.T4.1.1.8.6\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.8.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.8.6.2.1\" class=\"ltx_text ltx_font_bold\">1.84(.66)</span></td>\n<td id=\"S5.T4.1.1.8.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.87(.63)</td>\n<td id=\"S5.T4.1.1.8.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.2(6.92)</td>\n<td id=\"S5.T4.1.1.8.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.8.6.5.1\" class=\"ltx_text ltx_font_bold\">35.4(7.59)</span></td>\n<td id=\"S5.T4.1.1.8.6.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.8.6.6.1\" class=\"ltx_text ltx_font_bold\">.739(.077)</span></td>\n<td id=\"S5.T4.1.1.8.6.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.730(.085)</td>\n</tr>\n<tr id=\"S5.T4.1.1.9.7\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.9.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.9.7.1.1\" class=\"ltx_text\">20th Century</span></th>\n<th id=\"S5.T4.1.1.9.7.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.9.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.16(.49)</td>\n<td id=\"S5.T4.1.1.9.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.33(.42)</td>\n<td id=\"S5.T4.1.1.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">33.3(4.94)</td>\n<td id=\"S5.T4.1.1.9.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">30.3(6.58)</td>\n<td id=\"S5.T4.1.1.9.7.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.715(.065)</td>\n<td id=\"S5.T4.1.1.9.7.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.658(.043)</td>\n</tr>\n<tr id=\"S5.T4.1.1.10.8\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.10.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.10.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.86(.57)</td>\n<td id=\"S5.T4.1.1.10.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.10.8.3.1\" class=\"ltx_text ltx_font_bold\">1.78(.52)</span></td>\n<td id=\"S5.T4.1.1.10.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.6(6.07)</td>\n<td id=\"S5.T4.1.1.10.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.10.8.5.1\" class=\"ltx_text ltx_font_bold\">37.3(6.95)</span></td>\n<td id=\"S5.T4.1.1.10.8.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.10.8.6.1\" class=\"ltx_text ltx_font_bold\">.745(.068)</span></td>\n<td id=\"S5.T4.1.1.10.8.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.742(.070)</td>\n</tr>\n<tr id=\"S5.T4.1.1.11.9\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.11.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.11.9.1.1\" class=\"ltx_text\">Modern</span></th>\n<th id=\"S5.T4.1.1.11.9.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.11.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.25(.50)</td>\n<td id=\"S5.T4.1.1.11.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.11.9.4.1\" class=\"ltx_text ltx_font_bold\">1.84(.47)</span></td>\n<td id=\"S5.T4.1.1.11.9.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(4.69)</td>\n<td id=\"S5.T4.1.1.11.9.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.11.9.6.1\" class=\"ltx_text ltx_font_bold\">37.0(6.10)</span></td>\n<td id=\"S5.T4.1.1.11.9.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.702(.066)</td>\n<td id=\"S5.T4.1.1.11.9.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.11.9.8.1\" class=\"ltx_text ltx_font_bold\">.747(.061)</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.12.10\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.12.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.12.10.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.94(.57)</td>\n<td id=\"S5.T4.1.1.12.10.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.92(.50)</td>\n<td id=\"S5.T4.1.1.12.10.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.8(5.54)</td>\n<td id=\"S5.T4.1.1.12.10.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.3(5.63)</td>\n<td id=\"S5.T4.1.1.12.10.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.736(.063)</td>\n<td id=\"S5.T4.1.1.12.10.7\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.739(.057)</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n<section id=\"S6\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">VI </span><span id=\"S6.1.1\" class=\"ltx_text ltx_font_smallcaps\">A case study on woman composers</span>\n</h2>\n\n<div id=\"S6.p1\" class=\"ltx_para\">\n<p id=\"S6.p1.1\" class=\"ltx_p\">In this section, we explore the capabilities of the models proposed in this paper, specifically focusing on the classification of an unrepresented group: women composers. In Subsection\u00a0<a href=\"#S6.SS1\" title=\"VI-A Model performance on works by female composers \u2023 VI A case study on woman composers \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">VI-A</span></span></a>, we examine the models\u2019 performance across different genders. Subsequently, in Subsection\u00a0<a href=\"#S6.SS2\" title=\"VI-B Zero-shot experiment on HV Benchmark \u2023 VI-A Model performance on works by female composers \u2023 VI A case study on woman composers \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">VI-B</span></span></a>, we assess the generalization capabilities of the proposed models using a benchmark dataset of compositions by black women composers.</p>\n</div>\n<section id=\"S6.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S6.SS1.9.1.1\" class=\"ltx_text\">VI-A</span> </span><span id=\"S6.SS1.10.2\" class=\"ltx_text ltx_font_italic\">Model performance on works by female composers</span>\n</h3>\n\n<div id=\"S6.SS1.p1\" class=\"ltx_para\">\n<p id=\"S6.SS1.p1.1\" class=\"ltx_p\">In this section, we present the results of experiments designed to analyze gender bias in the models proposed in this paper, which were trained on the PSyllabus dataset. We focus on the classification capabilities of these models in distinguishing between compositions by male and female composers.</p>\n</div>\n<div id=\"S6.SS1.p2\" class=\"ltx_para\">\n<p id=\"S6.SS1.p2.1\" class=\"ltx_p\">The analysis reveals that, despite uniform training across all models, performance disparities emerge when testing on gender-specific data. The MM model, when tested on mixed-gender data, exhibited the best performance with an accuracy of 37.3% and a Tau-c of .778. This indicates a well-rounded capability to interpret compositions by both genders. However, a significant performance decline was observed in tests on data exclusively from women composers, with CQT\u2019s Tau-c dropping to .666 and accuracy to 31.6%. This underscores the importance of diverse and balanced evaluation sets to uncover model biases and ensure fair performance, as seen in the contrasted outcomes against gender-specific test data. Further research is needed to understand the performance decrease between male and female compositions, and whether it comes from musical characteristics or labeling biases.</p>\n</div>\n<figure id=\"S6.SS1.4\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE V: </span>Analysis of model performance differentiated by the composer\u2019s gender.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S6.SS1.4.4\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:390.3pt;height:479.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(99.8pt,-122.6pt) scale(2.0479794841506,2.0479794841506) ;\">\n<table id=\"S6.SS1.4.4.4\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.SS1.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.SS1.1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</th>\n<td id=\"S6.SS1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MSE</td>\n<td id=\"S6.SS1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Acc<sub id=\"S6.SS1.1.1.1.1.1.1\" class=\"ltx_sub\">0</sub>\n</td>\n<td id=\"S6.SS1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S6.SS1.2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S6.SS1.2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S6.SS1.2.2.2.2.1.1\" class=\"ltx_text ltx_markedasmath\">Both genres</span></th>\n<td id=\"S6.SS1.2.2.2.2.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.2.2.2.2.3\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.2.2.2.2.4\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.5.1\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.5.1.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.5.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S6.SS1.4.4.4.5.1.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.29(.18)</td>\n<td id=\"S6.SS1.4.4.4.5.1.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(1.61)</td>\n<td id=\"S6.SS1.4.4.4.5.1.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.741(.012)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.6.2\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.6.2.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.6.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S6.SS1.4.4.4.6.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.85(.07)</td>\n<td id=\"S6.SS1.4.4.4.6.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.7(.84)</td>\n<td id=\"S6.SS1.4.4.4.6.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.771(.005)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.7.3\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.7.3.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.7.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S6.SS1.4.4.4.7.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.7.3.3.1\" class=\"ltx_text ltx_font_bold\">1.81(.11)</span></td>\n<td id=\"S6.SS1.4.4.4.7.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.7.3.4.1\" class=\"ltx_text ltx_font_bold\">37.3(1.97)</span></td>\n<td id=\"S6.SS1.4.4.4.7.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.7.3.5.1\" class=\"ltx_text ltx_font_bold\">.778(.004)</span></td>\n</tr>\n<tr id=\"S6.SS1.3.3.3.3\" class=\"ltx_tr\">\n<th id=\"S6.SS1.3.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS1.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS1.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Only men</span>\n</th>\n<td id=\"S6.SS1.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.3.3.3.3.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.3.3.3.3.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.8.4\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.8.4.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.8.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S6.SS1.4.4.4.8.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.24(.19)</td>\n<td id=\"S6.SS1.4.4.4.8.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">33.2(1.13)</td>\n<td id=\"S6.SS1.4.4.4.8.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.734(.012)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.9.5\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.9.5.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.9.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S6.SS1.4.4.4.9.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.83(.05)</td>\n<td id=\"S6.SS1.4.4.4.9.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">37.3(.75)</td>\n<td id=\"S6.SS1.4.4.4.9.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.764(.004)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.10.6\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.10.6.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.10.6.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S6.SS1.4.4.4.10.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.10.6.3.1\" class=\"ltx_text ltx_font_bold\">1.81(.09)</span></td>\n<td id=\"S6.SS1.4.4.4.10.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.10.6.4.1\" class=\"ltx_text ltx_font_bold\">37.7(2.46)</span></td>\n<td id=\"S6.SS1.4.4.4.10.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.10.6.5.1\" class=\"ltx_text ltx_font_bold\">.769(.002)</span></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.4\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS1.4.4.4.4.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS1.4.4.4.4.1.2\" class=\"ltx_text ltx_markedasmath\">Only women</span>\n</th>\n<td id=\"S6.SS1.4.4.4.4.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.4.4.4.4.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.4.4.4.4.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.11.7\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.11.7.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.11.7.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S6.SS1.4.4.4.11.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.47(.59)</td>\n<td id=\"S6.SS1.4.4.4.11.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">31.6(4.74)</td>\n<td id=\"S6.SS1.4.4.4.11.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.666(.032)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.12.8\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.12.8.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.12.8.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S6.SS1.4.4.4.12.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.91(.60)</td>\n<td id=\"S6.SS1.4.4.4.12.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.6(4.24)</td>\n<td id=\"S6.SS1.4.4.4.12.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.722(.021)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.13.9\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.13.9.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.13.9.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S6.SS1.4.4.4.13.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.13.9.3.1\" class=\"ltx_text ltx_font_bold\">1.72(.49)</span></td>\n<td id=\"S6.SS1.4.4.4.13.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.13.9.4.1\" class=\"ltx_text ltx_font_bold\">35.7(5.74)</span></td>\n<td id=\"S6.SS1.4.4.4.13.9.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.13.9.5.1\" class=\"ltx_text ltx_font_bold\">.733(.025)</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S6.SS2\" class=\"ltx_subsection ltx_figure_panel\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S6.SS2.11.1.1\" class=\"ltx_text\">VI-B</span> </span><span id=\"S6.SS2.12.2\" class=\"ltx_text ltx_font_italic\">Zero-shot experiment on HV Benchmark</span>\n</h3>\n\n<div id=\"S6.SS2.p1\" class=\"ltx_para\">\n<p id=\"S6.SS2.p1.1\" class=\"ltx_p\">Historically underrepresented groups of composers, such as black women, have not been significantly explored in MIR datasets. We believe it is important to evaluate the capabilities of models in these out-of-distribution scenarios. In this experiment, we assess the ranking capabilities of the proposed approach in a zero-shot setting over the HV dataset by utilizing the model\u2019s logits. These logits follow a monotonic order, as indicated in Section\u00a0<a href=\"#S3.SS4\" title=\"III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-D</span></span></a>. Therefore, we can use them to order the HV collection and compare it with the original ranking.</p>\n</div>\n<div id=\"S6.SS2.p2\" class=\"ltx_para\">\n<p id=\"S6.SS2.p2.1\" class=\"ltx_p\">Table\u00a0<a href=\"#S6.SS2\" title=\"VI-B Zero-shot experiment on HV Benchmark \u2023 VI-A Model performance on works by female composers \u2023 VI A case study on woman composers \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">VI-B</span></span></a> shows the results obtained resorting to the Kendall rank correlation coefficient, <math id=\"S6.SS2.p2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.p2.1.m1.1a\"><msub id=\"S6.SS2.p2.1.m1.1.1\" xref=\"S6.SS2.p2.1.m1.1.1.cmml\"><mi id=\"S6.SS2.p2.1.m1.1.1.2\" xref=\"S6.SS2.p2.1.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.p2.1.m1.1.1.3\" xref=\"S6.SS2.p2.1.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.p2.1.m1.1b\"><apply id=\"S6.SS2.p2.1.m1.1.1.cmml\" xref=\"S6.SS2.p2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.p2.1.m1.1.1.1.cmml\" xref=\"S6.SS2.p2.1.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.p2.1.m1.1.1.2.cmml\" xref=\"S6.SS2.p2.1.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.p2.1.m1.1.1.3.cmml\" xref=\"S6.SS2.p2.1.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.p2.1.m1.1c\">\\tau_{c}</annotation></semantics></math>, for all data collections discussed in the experiment, considering both the single-task and multi-task models posed. Note that HV is only used for benchmarking purposes.</p>\n</div>\n<figure id=\"S6.SS2.6\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE VI: </span>Zero-shot experiment on Hidden Voices benchmark. The benchmark is a collection of piano pieces by black women composers, out of the distribution from the PSyllabus dataset.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S6.SS2.3.3\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:303.5pt;height:647pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(105.3pt,-224.5pt) scale(3.26768397143762,3.26768397143762) ;\">\n<table id=\"S6.SS2.3.3.3\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.SS2.3.3.3.4.1\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</td>\n<td id=\"S6.SS2.3.3.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S6.SS2.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.SS2.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S6.SS2.1.1.1.1.1.1\" class=\"ltx_text ltx_markedasmath\">Single-task</span></td>\n<td id=\"S6.SS2.1.1.1.1.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.5.2\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.5.2.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.5.2.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.5.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.592(.020)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.6.3\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.6.3.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.6.3.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.6.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.6.3.3.1\" class=\"ltx_text ltx_font_bold\">.661(.018)</span></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.7.4\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.7.4.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.7.4.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</td>\n<td id=\"S6.SS2.3.3.3.7.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.660(.029)</td>\n</tr>\n<tr id=\"S6.SS2.2.2.2.2\" class=\"ltx_tr\">\n<td id=\"S6.SS2.2.2.2.2.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS2.2.2.2.2.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS2.2.2.2.2.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with Era</span>\n</td>\n<td id=\"S6.SS2.2.2.2.2.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.8.5\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.8.5.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.8.5.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.8.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.634(.039)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.9.6\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.9.6.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.9.6.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.9.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.9.6.3.1\" class=\"ltx_text ltx_font_bold\">.668(.047)</span></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.3\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.3.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS2.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS2.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with Multiranking</span>\n</td>\n<td id=\"S6.SS2.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.10.7\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.10.7.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.10.7.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.10.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.632(.025)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.11.8\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.11.8.1\" class=\"ltx_td ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.11.8.2\" class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.11.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.11.8.3.1\" class=\"ltx_text ltx_font_bold\">.672(.031)</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S6.SS2.5.5\" class=\"ltx_p ltx_figure_panel\">In the zero-shot experiment conducted on the Hidden Voices benchmark, the findings underscore the advantages of multi-task models based on PR, particularly those that incorporate era information and multi-ranking strategies. The experiment reveals that the multi-task model utilizing era data with the PR approach achieved a <math id=\"S6.SS2.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.4.4.m1.1a\"><msub id=\"S6.SS2.4.4.m1.1.1\" xref=\"S6.SS2.4.4.m1.1.1.cmml\"><mi id=\"S6.SS2.4.4.m1.1.1.2\" xref=\"S6.SS2.4.4.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.4.4.m1.1.1.3\" xref=\"S6.SS2.4.4.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.4.4.m1.1b\"><apply id=\"S6.SS2.4.4.m1.1.1.cmml\" xref=\"S6.SS2.4.4.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.4.4.m1.1.1.1.cmml\" xref=\"S6.SS2.4.4.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.4.4.m1.1.1.2.cmml\" xref=\"S6.SS2.4.4.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.4.4.m1.1.1.3.cmml\" xref=\"S6.SS2.4.4.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.4.4.m1.1c\">\\tau_{c}</annotation></semantics></math> value of 0.668, emphasizing the importance of historical context in improving the performance of models on out-of-distribution data. Moreover, the PR model in the multi-task with Multiranking setup demonstrated the highest adaptability and performance, achieving a <math id=\"S6.SS2.5.5.m2.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.5.5.m2.1a\"><msub id=\"S6.SS2.5.5.m2.1.1\" xref=\"S6.SS2.5.5.m2.1.1.cmml\"><mi id=\"S6.SS2.5.5.m2.1.1.2\" xref=\"S6.SS2.5.5.m2.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.5.5.m2.1.1.3\" xref=\"S6.SS2.5.5.m2.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.5.5.m2.1b\"><apply id=\"S6.SS2.5.5.m2.1.1.cmml\" xref=\"S6.SS2.5.5.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.5.5.m2.1.1.1.cmml\" xref=\"S6.SS2.5.5.m2.1.1\">subscript</csymbol><ci id=\"S6.SS2.5.5.m2.1.1.2.cmml\" xref=\"S6.SS2.5.5.m2.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.5.5.m2.1.1.3.cmml\" xref=\"S6.SS2.5.5.m2.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.5.5.m2.1c\">\\tau_{c}</annotation></semantics></math> value of 0.672. This insight is vital for the development of inclusive machine-learning tools in the field of music research, highlighting the potential of combining multiple rankings to improve model performance on out-of-distribution datasets.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S6.SS2.6.6\" class=\"ltx_p ltx_figure_panel\">The results are not fully comparable with those of the HV with sheet music image presented in\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite> due to the dataset containing only 17 pieces. However, in the previous experiment, the best <math id=\"S6.SS2.6.6.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.6.6.m1.1a\"><msub id=\"S6.SS2.6.6.m1.1.1\" xref=\"S6.SS2.6.6.m1.1.1.cmml\"><mi id=\"S6.SS2.6.6.m1.1.1.2\" xref=\"S6.SS2.6.6.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.6.6.m1.1.1.3\" xref=\"S6.SS2.6.6.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.6.6.m1.1b\"><apply id=\"S6.SS2.6.6.m1.1.1.cmml\" xref=\"S6.SS2.6.6.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.6.6.m1.1.1.1.cmml\" xref=\"S6.SS2.6.6.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.6.6.m1.1.1.2.cmml\" xref=\"S6.SS2.6.6.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.6.6.m1.1.1.3.cmml\" xref=\"S6.SS2.6.6.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.6.6.m1.1c\">\\tau_{c}</annotation></semantics></math> result was 0.56, indicating that the audio-based version is more robust. Further research is needed to compare sheet music image classification audio to determine whether the modality or the quality of the datasets influences generalization.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S7\" class=\"ltx_section ltx_figure_panel\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">VII </span><span id=\"S7.1.1\" class=\"ltx_text ltx_font_smallcaps\">Conclusion</span>\n</h2>\n\n<div id=\"S7.p1\" class=\"ltx_para\">\n<p id=\"S7.p1.1\" class=\"ltx_p\">In conclusion, our study bridges a significant gap in music education and Music Information Retrieval by harnessing the widespread availability of audio via streaming platforms to estimate the difficulty of musical scores. This approach enhances students\u2019 ability to explore an extensive repertoire and collaborate with educators, significantly improving their learning experience and motivation. Our research introduces several key advancements.</p>\n</div>\n<div id=\"S7.p2\" class=\"ltx_para\">\n<p id=\"S7.p2.1\" class=\"ltx_p\">We have developed a model using a CNN+RNN+Attention network as a baseline for capturing performance difficulty from diverse audio representations. This model is assessed using a novel and expansive audio collection, setting a new benchmark for the scale of datasets in difficulty classification. Furthermore, we propose a multi-performance benchmark to investigate the impact of various performances on difficulty prediction, expanding the understanding of performance variability.</p>\n</div>\n<div id=\"S7.p3\" class=\"ltx_para\">\n<p id=\"S7.p3.1\" class=\"ltx_p\">Our comprehensive experimental framework includes testing generalization through a zero-shot scenario in out-of-domain distributions, employing multi-task learning with tasks related to music performance, and training across multiple difficulty rankings. These experiments underscore the robustness and versatility of our methodologies.</p>\n</div>\n<div id=\"S7.p4\" class=\"ltx_para\">\n<p id=\"S7.p4.1\" class=\"ltx_p\">To catalyze further research and collaboration within the music education community, we have publicly made our code, models, and the <em id=\"S7.p4.1.1\" class=\"ltx_emph ltx_font_italic\">Piano Syllabus (PSyllabus)</em> dataset available, including transcribed midis and CQT features. This initiative aims to create a shared platform for advancing automated performance difficulty understanding and enhancing music learning.</p>\n</div>\n<div id=\"S7.p5\" class=\"ltx_para\">\n<p id=\"S7.p5.1\" class=\"ltx_p\">Our contributions lay a solid foundation for the future of audio-based difficulty classification in music, demonstrating the potential for meaningful advancements in the automated analysis of musical scores. By providing comprehensive resources for the community, we invite educators, students, and researchers to engage with our work, fostering a collaborative ecosystem that supports the evolution of music education.</p>\n</div>\n<section id=\"Sx1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_font_smallcaps ltx_title_section\">Acknowledgment</h2>\n\n<div id=\"Sx1.p1\" class=\"ltx_para\">\n<p id=\"Sx1.p1.1\" class=\"ltx_p\">The authors thank Ian Wheaton, the creator of the Piano Syllabus web community, for his helpful guidance and for answering our questions. We are also grateful to him and the entire web community for their hard work in putting together the corpus to create a single source of information on piano difficulty. This effort is a strong base for our research, as we aim to use this resource to learn more and do deeper analysis. We hope our work benefits the Piano Syllabus and the music education community by aiding in the labeling of pieces and enabling the exploration of the forgotten cultural heritage.</p>\n</div>\n<div id=\"Sx1.p2\" class=\"ltx_para\">\n<p id=\"Sx1.p2.1\" class=\"ltx_p\">Pedro would also like to thank Nazif C. Tamer for his insistence on shifting focus to audio, and Pablo Alonso and Oguz Araz for their insightful discussions about audio features.</p>\n</div>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[1]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Mellizo, \u201cMusic education, curriculum design, and assessment: Imagining a more equitable approach,\u201d <em id=\"bib.bib1.1.1\" class=\"ltx_emph ltx_font_italic\">Music Educators Journal</em>, vol. 106, no.\u00a04, pp. 57\u201365, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[2]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0R. Jorgensen, \u201cThe curriculum design process in music,\u201d <em id=\"bib.bib2.1.1\" class=\"ltx_emph ltx_font_italic\">College Music Symposium</em>, vol.\u00a028, pp. 94\u2013105, 1988. [Online]. Available: <a target=\"_blank\" href=\"http://www.jstor.org/stable/40374590\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">http://www.jstor.org/stable/40374590</a>\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[3]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0S. Deconto, E.\u00a0L.\u00a0F. Valenga, and C.\u00a0N. Silla, \u201cAutomatic music score difficulty classification,\u201d in <em id=\"bib.bib3.1.1\" class=\"ltx_emph ltx_font_italic\">2023 30th International Conference on Systems, Signals and Image Processing (IWSSIP)</em>.\u00a0\u00a0\u00a0IEEE, 2023, pp. 1\u20135.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[4]</span>\n<span class=\"ltx_bibblock\">\nV.\u00a0S\u00e9bastien, H.\u00a0Ralambondrainy, O.\u00a0S\u00e9bastien, and N.\u00a0Conruyt, \u201cScore analyzer: Automatically determining scores difficulty level for instrumental e-learning,\u201d in <em id=\"bib.bib4.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of 13th International Society for Music Information Retrieval Conference, ISMIR</em>, Porto, Portugal, 2012.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[5]</span>\n<span class=\"ltx_bibblock\">\nS.-C. Chiu and M.-S. Chen, \u201cA study on difficulty level recognition of piano sheet music,\u201d in <em id=\"bib.bib5.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Symposium on Multimedia</em>.\u00a0\u00a0\u00a0IEEE, 2012, pp. 17\u201323.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[6]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura and K.\u00a0Yoshii, \u201cStatistical piano reduction controlling performance difficulty,\u201d <em id=\"bib.bib6.1.1\" class=\"ltx_emph ltx_font_italic\">APSIPA Transactions on Signal and Information Processing</em>, vol.\u00a07, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[7]</span>\n<span class=\"ltx_bibblock\">\n\u201cMusescore have automatic difficulty categories from year 2022,\u201d <a target=\"_blank\" href=\"https://musescore.com/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://musescore.com/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[8]</span>\n<span class=\"ltx_bibblock\">\n\u201cUltimate guitar have automatic difficulty categories from year 2022,\u201d <a target=\"_blank\" href=\"https://www.ultimate-guitar.com/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://www.ultimate-guitar.com/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[9]</span>\n<span class=\"ltx_bibblock\">\n\u201cSystem for estimating user\u2019s skill in playing a music instrument and determining virtual exercises thereof,\u201d Patent US9\u2009767\u2009705B1, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[10]</span>\n<span class=\"ltx_bibblock\">\nQ.\u00a0Kong, B.\u00a0Li, X.\u00a0Song, Y.\u00a0Wan, and Y.\u00a0Wang, \u201cHigh-resolution piano transcription with pedals by regressing onset and offset times,\u201d vol.\u00a029, p. 3707\u20133717, oct 2021.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[11]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura, N.\u00a0Ono, and S.\u00a0Sagayama, \u201cMerged-output hmm for piano fingering of both hands.\u201d in <em id=\"bib.bib11.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR</em>, Taipei, Taiwan, 2014, pp. 531\u2013536.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[12]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura and S.\u00a0Sagayama, \u201cAutomatic piano reduction from ensemble scores based on merged-output hidden markov model,\u201d in <em id=\"bib.bib12.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 41st International Computer Music Conference, ICMC</em>, Denton, USA, 2015.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[13]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, N.\u00a0C. Tamer, V.\u00a0Eremenko, M.\u00a0Miron, and X.\u00a0Serra, \u201cScore difficulty analysis for piano performance education,\u201d in <em id=\"bib.bib13.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP</em>, Singapore, Singapore, 2022, pp. 201\u2013205.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[14]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, D.\u00a0Jeong, V.\u00a0Eremenko, N.\u00a0C. Tamer, M.\u00a0Miron, and X.\u00a0Serra, \u201cCombining piano performance dimensions for score difficulty classification,\u201d <em id=\"bib.bib14.1.1\" class=\"ltx_emph ltx_font_italic\">Expert Systems with Applications</em>, vol. 238, p. 121776, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[15]</span>\n<span class=\"ltx_bibblock\">\nH.\u00a0Zhang, E.\u00a0Karystinaios, S.\u00a0Dixon, G.\u00a0Widmer, and C.\u00a0E. Cancino-Chac\u00f3n, \u201cSymbolic music representations for classification tasks: A systematic evaluation,\u201d <em id=\"bib.bib15.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2309.02567</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[16]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, D.\u00a0Jeong, E.\u00a0Nakamura, X.\u00a0Serra, and M.\u00a0Miron, \u201cAutomatic piano fingering from partially annotated scores using autoregressive neural networks,\u201d in <em id=\"bib.bib16.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 30th ACM International Conference on Multimedia</em>, 2022, pp. 6502\u20136510.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[17]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0Jeong, T.\u00a0Kwon, Y.\u00a0Kim, K.\u00a0Lee, and J.\u00a0Nam, \u201cVirtuosoNet: A hierarchical RNN-based system for modeling expressive piano performance,\u201d in <em id=\"bib.bib17.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR</em>, 2019, pp. 908\u2013915.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[18]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Cheng, Z.\u00a0Wang, and G.\u00a0Pollastri, \u201cA neural network approach to ordinal regression,\u201d in <em id=\"bib.bib18.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Joint Conference on Neural Networks, IJCNN</em>.\u00a0\u00a0\u00a0Hong Kong, China: IEEE, 2008, pp. 1279\u20131284.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[19]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0A.\u00a0V. V\u00e1squez, M.\u00a0Baelemans, J.\u00a0Driedger, W.\u00a0Zuidema, and J.\u00a0A. Burgoyne, \u201cQuantifying the ease of playing song chords on the guitar,\u201d in <em id=\"bib.bib19.1.1\" class=\"ltx_emph ltx_font_italic\">Ismir 2023 Hybrid Conference</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[20]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Toyama, T.\u00a0Akama, Y.\u00a0Ikemiya, Y.\u00a0Takida, W.-H. Liao, and Y.\u00a0Mitsufuji, \u201cAutomatic piano transcription with hierarchical frequency-time transformer,\u201d in <em id=\"bib.bib20.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[21]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Rege and R.\u00a0Sindal, \u201cReview of f0 estimation in the context of indian classical music expression detection,\u201d in <em id=\"bib.bib21.1.1\" class=\"ltx_emph ltx_font_italic\">Social Networking and Computational Intelligence: Proceedings of SCI-2018</em>.\u00a0\u00a0\u00a0Springer, 2020, pp. 257\u2013268.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[22]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Alonso-Jim\u00e9nez, X.\u00a0Serra, and D.\u00a0Bogdanov, \u201cMusic representation learning based on editorial metadata from discogs,\u201d in <em id=\"bib.bib22.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[23]</span>\n<span class=\"ltx_bibblock\">\nC.\u00a0Sch\u00f6rkhuber and A.\u00a0Klapuri, \u201cConstant-q transform toolbox for music processing,\u201d in <em id=\"bib.bib23.1.1\" class=\"ltx_emph ltx_font_italic\">7th sound and music computing conference, Barcelona, Spain</em>, 2010, pp. 3\u201364.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[24]</span>\n<span class=\"ltx_bibblock\">\nQ.\u00a0Kong, B.\u00a0Li, J.\u00a0Chen, and Y.\u00a0Wang, \u201cGiantmidi-piano: A large-scale MIDI dataset for classical piano music,\u201d <em id=\"bib.bib24.1.1\" class=\"ltx_emph ltx_font_italic\">Transactions of the International Society for Music Information Retrieval, 5(1), pp.87\u201398</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[25]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, J.\u00a0J. Valero-Mas, D.\u00a0Jeong, and X.\u00a0Serra, \u201cPredicting performance difficulty from piano sheet music images,\u201d in <em id=\"bib.bib25.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. of the 24th Int. Society for Music Information Retrieval Conf.</em>, Milan, Italy, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[26]</span>\n<span class=\"ltx_bibblock\">\nUniversity of Colorado, \u201cHidden voices project,\u201d <a target=\"_blank\" href=\"https://www.colorado.edu/project/hidden-voices/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://www.colorado.edu/project/hidden-voices/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[27]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0F. Gemmeke, D.\u00a0P. Ellis, D.\u00a0Freedman, A.\u00a0Jansen, W.\u00a0Lawrence, R.\u00a0C. Moore, M.\u00a0Plakal, and M.\u00a0Ritter, \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in <em id=\"bib.bib27.1.1\" class=\"ltx_emph ltx_font_italic\">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.\u00a0\u00a0\u00a0IEEE, 2017, pp. 776\u2013780.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[28]</span>\n<span class=\"ltx_bibblock\">\nV.\u00a0Lakic, L.\u00a0Rossetto, and A.\u00a0Bernstein, \u201cLink-rot in web-sourced multimedia datasets,\u201d in <em id=\"bib.bib28.1.1\" class=\"ltx_emph ltx_font_italic\">MultiMedia Modeling</em>.\u00a0\u00a0\u00a0Cham: Springer International Publishing, 2023, pp. 476\u2013488.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[29]</span>\n<span class=\"ltx_bibblock\">\nN.\u00a0Kato, E.\u00a0Nakamura, K.\u00a0Mine, O.\u00a0Doeda, and M.\u00a0Yamada, \u201cComputational analysis of audio recordings of piano performance for automatic evaluation,\u201d in <em id=\"bib.bib29.1.1\" class=\"ltx_emph ltx_font_italic\">Responsive and Sustainable Educational Futures</em>.\u00a0\u00a0\u00a0Cham: Springer Nature Switzerland, 2023, pp. 586\u2013592.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[30]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0O\u2019Hanlon and M.\u00a0B. Sandler, \u201cComparing cqt and reassignment based chroma features for template-based automatic chord recognition,\u201d in <em id=\"bib.bib30.1.1\" class=\"ltx_emph ltx_font_italic\">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP</em>.\u00a0\u00a0\u00a0IEEE, 2019, pp. 860\u2013864.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[31]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda and G.\u00a0Bernardes, \u201cRevisiting harmonic change detection,\u201d in <em id=\"bib.bib31.1.1\" class=\"ltx_emph ltx_font_italic\">Audio Engineering Society Convention 149</em>, Oct 2020.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[32]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Wang, A.\u00a0Singh, J.\u00a0Michael, F.\u00a0Hill, O.\u00a0Levy, and S.\u00a0R. Bowman, \u201cGlue: A multi-task benchmark and analysis platform for natural language understanding,\u201d in <em id=\"bib.bib32.1.1\" class=\"ltx_emph ltx_font_italic\">7th International Conference on Learning Representations, ICLR</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[33]</span>\n<span class=\"ltx_bibblock\">\nI.\u00a0Laina, C.\u00a0Rupprecht, V.\u00a0Belagiannis, F.\u00a0Tombari, and N.\u00a0Navab, \u201cDeeper depth prediction with fully convolutional residual networks,\u201d in <em id=\"bib.bib33.1.1\" class=\"ltx_emph ltx_font_italic\">2016 Fourth international conference on 3D vision (3DV)</em>.\u00a0\u00a0\u00a0IEEE, 2016, pp. 239\u2013248.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[34]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Wu, C.\u00a0Shen, and A.\u00a0Van Den\u00a0Hengel, \u201cWider or deeper: Revisiting the resnet model for visual recognition,\u201d <em id=\"bib.bib34.1.1\" class=\"ltx_emph ltx_font_italic\">Pattern Recognition</em>, vol.\u00a090, pp. 119\u2013133, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[35]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Won, K.\u00a0Choi, and X.\u00a0Serra, \u201cSemi-supervised music tagging transformer,\u201d in <em id=\"bib.bib35.1.1\" class=\"ltx_emph ltx_font_italic\">International Society for Music Information Retrieval Conference</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[36]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Allen-Zhu and Y.\u00a0Li, \u201cWhat can resnet learn efficiently, going beyond kernels?\u201d <em id=\"bib.bib36.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, vol.\u00a032, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[37]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Cho, B.\u00a0van Merrienboer, \u00c7.\u00a0G\u00fcl\u00e7ehre, D.\u00a0Bahdanau, F.\u00a0Bougares, H.\u00a0Schwenk, and Y.\u00a0Bengio, \u201cLearning phrase representations using RNN encoder-decoder for statistical machine translation,\u201d in <em id=\"bib.bib37.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL</em>, A.\u00a0Moschitti, B.\u00a0Pang, and W.\u00a0Daelemans, Eds.\u00a0\u00a0\u00a0ACL, 2014, pp. 1724\u20131734. [Online]. Available: <a target=\"_blank\" href=\"https://doi.org/10.3115/v1/d14-1179\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://doi.org/10.3115/v1/d14-1179</a>\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[38]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Chung, \u00c7.\u00a0G\u00fcl\u00e7ehre, K.\u00a0Cho, and Y.\u00a0Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d <em id=\"bib.bib38.1.1\" class=\"ltx_emph ltx_font_italic\">CoRR</em>, vol. abs/1412.3555, 2014. [Online]. Available: <a target=\"_blank\" href=\"http://arxiv.org/abs/1412.3555\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">http://arxiv.org/abs/1412.3555</a>\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[39]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Yang, D.\u00a0Yang, C.\u00a0Dyer, X.\u00a0He, A.\u00a0Smola, and E.\u00a0Hovy, \u201cHierarchical attention networks for document classification,\u201d in <em id=\"bib.bib39.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</em>, 2016, pp. 1480\u20131489.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[40]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Maghoumi and J.\u00a0J. LaViola, \u201cDeepgru: Deep gesture recognition utility,\u201d in <em id=\"bib.bib40.1.1\" class=\"ltx_emph ltx_font_italic\">International Symposium on Visual Computing</em>.\u00a0\u00a0\u00a0Springer, 2019, pp. 16\u201331.\n\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[41]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Alfaro-Contreras, J.\u00a0J. Valero-Mas, J.\u00a0M. I\u00f1esta, and J.\u00a0Calvo-Zaragoza, \u201cLate multimodal fusion for image and audio music transcription,\u201d <em id=\"bib.bib41.1.1\" class=\"ltx_emph ltx_font_italic\">Expert Systems with Applications</em>, vol. 216, p. 119491, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[42]</span>\n<span class=\"ltx_bibblock\">\nL.\u00a0Gaudette and N.\u00a0Japkowicz, \u201cEvaluation methods for ordinal classification,\u201d in <em id=\"bib.bib42.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 22nd Canadian Conference on Advances in Artificial Intelligence</em>, Kelowna, Canada, 2009, pp. 207\u2013210.\n\n</span>\n</li>\n</ul>\n</section>\n<figure id=\"Sx1.1\" class=\"ltx_float biography\">\n<table id=\"Sx1.1.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.1.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.1.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/pedro.png\" id=\"Sx1.1.1.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"100\" height=\"110\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.1.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.1.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.1.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Pedro Ramoneda</span> \nPedro Ramoneda holds a BSc in Computer Science from the University of Zaragoza, a Professional Degree in Piano Performance from the Conservatory of Music in Zaragoza, and an MSc in Sound and Music Computing from the Universitat Pompeu Fabra. He is currently a third-year PhD student in the Music Technology Group of the Universitat Pompeu Fabra under the supervision of Prof. Xavier Serra, focusing on the use of technologies from the Music Information Retrieval and Signal Processing field for supporting music education.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.2\" class=\"ltx_float biography\">\n<table id=\"Sx1.2.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.2.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.2.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/Minhee_Lee.jpeg\" id=\"Sx1.2.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"94\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.2.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.2.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.2.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Minhee Lee</span> \nMinhee Lee is an undergraduate student pursuing a B.S. degree in the Department of Computer Science and Engineering at Sogang University in South Korea. She is an undergraduate intern in the MALer Lab under the supervision of Prof. Dasaem Jeong since 2023. Before joining the research group, she did internships as a software engineer at Google in 2021 and 2022, and at FuriosaAI in 2022. Her research interest is about various music information retrieval tasks of understanding music with deep learning technologies.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.3\" class=\"ltx_float biography\">\n<table id=\"Sx1.3.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.3.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.3.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/Dasaem_Jeong.jpg\" id=\"Sx1.3.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"100\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.3.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.3.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.3.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.3.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Dasaem Jeong</span>  is currently working as an Assistant Professor in the Department of Art &amp; Technology at Sogang University in South Korea since 2021. Before joining Sogang University, he worked as a research scientist in T-Brain X, SK Telecom from 2020 to 2021. He obtained his Ph.D. and M.S. degrees in culture technology, and B.S. in mechanical engineering from Korea Advanced Institute of Science and Technology (KAIST). His research primarily focuses on a diverse range of music information retrieval tasks, including music generation and computational musicology.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.4\" class=\"ltx_float biography\">\n<table id=\"Sx1.4.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.4.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.4.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/JJVM.png\" id=\"Sx1.4.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"85\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.4.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.4.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.4.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.4.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Jose J. Valero-Mas</span>  received the M.Sc. degree in Telecommunications Engineering from the University Miguel Hern\u00e1ndez of Elche in 2012, the M.Sc. degree in Sound and Music Computing from the Universitat Pompeu Fabra in 2013, and the Ph.D. degree in Computer Science from the University of Alicante in 2017. After a three-year period in industry in which he developed as a data scientist, he acted as a Postdoctoral Researcher from 2020 to 2023 in the University of Alicante, the Universitat Pompeu Fabra, and the Queen Mary University of London. He is currently an Assistant Professor in the Department of Software and Computing Systems of the University of Alicante. His research interests include Pattern Recognition, Machine Learning, Music Information Retrieval, and Signal Processing for which he has co-authored more than 40 works within international journals, conference communications, and book chapters.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.5\" class=\"ltx_float biography\">\n<table id=\"Sx1.5.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.5.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.5.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/serra.png\" id=\"Sx1.5.1.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"100\" height=\"110\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.5.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.5.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.5.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.5.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Xavier Serra</span> \nHe has had a progressive academic career in the field of music technology since earning his PhD from Stanford University in 1989. His interest lies in the analysis, synthesis, and description of sound and music signals. He always seeks a balance between basic and applied research while integrating both scientific/technological and humanistic/artistic disciplines. Currently, he supervises research at the Music Technology Group (MTG), focusing on understanding sound and music signals through signal processing, machine learning, and semantic technologies. His work emphasizes data-driven and knowledge-driven methodologies, involving the development of large data collections and the application of domain-specific knowledge. He leads projects funded both publicly and privately, tackling practical issues such as music exploration, sound classification, and music performance analysis for educational purposes. He champions open science by promoting open data, software, and access, and is keen on using open innovation strategies to enhance the social and economic impact of his research.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n</section>\n</section>\n</div>\n</div>\n</figure>\n</section>\n</div>\n</div>\n</figure>\n</section>\n</section>\n</section>\n</div>\n</div>\n</figure>\n",
        "footnotes": [],
        "references": []
    },
    "S5.T4": {
        "caption": "TABLE IV:  Comparative analysis for Basic and Multi-task with Era experiments across musical periods.",
        "table": "<figure id=\"S5.T4\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE IV: </span>Comparative analysis for Basic and Multi-task with Era experiments across musical periods.</figcaption>\n<div id=\"S5.T4.1\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:346.9pt;height:182.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-31.4pt,16.6pt) scale(0.846604287885487,0.846604287885487) ;\">\n<table id=\"S5.T4.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Period</th>\n<th id=\"S5.T4.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Method</th>\n<th id=\"S5.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">MSE</th>\n<th id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Acc<sub id=\"S5.T4.1.1.1.1.1\" class=\"ltx_sub\">0</sub>\n</th>\n<th id=\"S5.T4.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Tau-c</th>\n</tr>\n<tr id=\"S5.T4.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.2.1.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.T4.1.1.2.1.2\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S5.T4.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Single</th>\n<th id=\"S5.T4.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">with Era</th>\n<th id=\"S5.T4.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Single</th>\n<th id=\"S5.T4.1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">with Era</th>\n<th id=\"S5.T4.1.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Single</th>\n<th id=\"S5.T4.1.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">with Era</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.3.1.1.1\" class=\"ltx_text\">Baroque</span></th>\n<th id=\"S5.T4.1.1.3.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.73(.49)</td>\n<td id=\"S5.T4.1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.49(.42)</td>\n<td id=\"S5.T4.1.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.1(4.82)</td>\n<td id=\"S5.T4.1.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.3(5.51)</td>\n<td id=\"S5.T4.1.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.621(.041)</td>\n<td id=\"S5.T4.1.1.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.631(.036)</td>\n</tr>\n<tr id=\"S5.T4.1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.4.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.25(.50)</td>\n<td id=\"S5.T4.1.1.4.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.4.2.3.1\" class=\"ltx_text ltx_font_bold\">2.03(.62)</span></td>\n<td id=\"S5.T4.1.1.4.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(4.69)</td>\n<td id=\"S5.T4.1.1.4.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.4.2.5.1\" class=\"ltx_text ltx_font_bold\">34.2(6.46)</span></td>\n<td id=\"S5.T4.1.1.4.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.702(.066)</td>\n<td id=\"S5.T4.1.1.4.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.4.2.7.1\" class=\"ltx_text ltx_font_bold\">.707(.088)</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.5.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.5.3.1.1\" class=\"ltx_text\">Classical</span></th>\n<th id=\"S5.T4.1.1.5.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.25(.66)</td>\n<td id=\"S5.T4.1.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.5.3.4.1\" class=\"ltx_text ltx_font_bold\">1.96(.51)</span></td>\n<td id=\"S5.T4.1.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.2(6.06)</td>\n<td id=\"S5.T4.1.1.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.1(5.55)</td>\n<td id=\"S5.T4.1.1.5.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.698(.088)</td>\n<td id=\"S5.T4.1.1.5.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.719(.073)</td>\n</tr>\n<tr id=\"S5.T4.1.1.6.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.6.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.97(.79)</td>\n<td id=\"S5.T4.1.1.6.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.98(.45)</td>\n<td id=\"S5.T4.1.1.6.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.2(8.23)</td>\n<td id=\"S5.T4.1.1.6.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.6.4.5.1\" class=\"ltx_text ltx_font_bold\">34.3(4.90)</span></td>\n<td id=\"S5.T4.1.1.6.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.726(.093)</td>\n<td id=\"S5.T4.1.1.6.4.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.6.4.7.1\" class=\"ltx_text ltx_font_bold\">.732(.067)</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.7.5\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.7.5.1.1\" class=\"ltx_text\">Romantic</span></th>\n<th id=\"S5.T4.1.1.7.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.16(.56)</td>\n<td id=\"S5.T4.1.1.7.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.05(.44)</td>\n<td id=\"S5.T4.1.1.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.6(5.41)</td>\n<td id=\"S5.T4.1.1.7.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.1(4.50)</td>\n<td id=\"S5.T4.1.1.7.5.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.704(.071)</td>\n<td id=\"S5.T4.1.1.7.5.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.724(.062)</td>\n</tr>\n<tr id=\"S5.T4.1.1.8.6\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.8.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.8.6.2.1\" class=\"ltx_text ltx_font_bold\">1.84(.66)</span></td>\n<td id=\"S5.T4.1.1.8.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.87(.63)</td>\n<td id=\"S5.T4.1.1.8.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.2(6.92)</td>\n<td id=\"S5.T4.1.1.8.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.8.6.5.1\" class=\"ltx_text ltx_font_bold\">35.4(7.59)</span></td>\n<td id=\"S5.T4.1.1.8.6.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.8.6.6.1\" class=\"ltx_text ltx_font_bold\">.739(.077)</span></td>\n<td id=\"S5.T4.1.1.8.6.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.730(.085)</td>\n</tr>\n<tr id=\"S5.T4.1.1.9.7\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.9.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.9.7.1.1\" class=\"ltx_text\">20th Century</span></th>\n<th id=\"S5.T4.1.1.9.7.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.9.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.16(.49)</td>\n<td id=\"S5.T4.1.1.9.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.33(.42)</td>\n<td id=\"S5.T4.1.1.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">33.3(4.94)</td>\n<td id=\"S5.T4.1.1.9.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">30.3(6.58)</td>\n<td id=\"S5.T4.1.1.9.7.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.715(.065)</td>\n<td id=\"S5.T4.1.1.9.7.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.658(.043)</td>\n</tr>\n<tr id=\"S5.T4.1.1.10.8\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.10.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.10.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.86(.57)</td>\n<td id=\"S5.T4.1.1.10.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.10.8.3.1\" class=\"ltx_text ltx_font_bold\">1.78(.52)</span></td>\n<td id=\"S5.T4.1.1.10.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.6(6.07)</td>\n<td id=\"S5.T4.1.1.10.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.10.8.5.1\" class=\"ltx_text ltx_font_bold\">37.3(6.95)</span></td>\n<td id=\"S5.T4.1.1.10.8.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.10.8.6.1\" class=\"ltx_text ltx_font_bold\">.745(.068)</span></td>\n<td id=\"S5.T4.1.1.10.8.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.742(.070)</td>\n</tr>\n<tr id=\"S5.T4.1.1.11.9\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.11.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T4.1.1.11.9.1.1\" class=\"ltx_text\">Modern</span></th>\n<th id=\"S5.T4.1.1.11.9.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S5.T4.1.1.11.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.25(.50)</td>\n<td id=\"S5.T4.1.1.11.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.11.9.4.1\" class=\"ltx_text ltx_font_bold\">1.84(.47)</span></td>\n<td id=\"S5.T4.1.1.11.9.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(4.69)</td>\n<td id=\"S5.T4.1.1.11.9.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.11.9.6.1\" class=\"ltx_text ltx_font_bold\">37.0(6.10)</span></td>\n<td id=\"S5.T4.1.1.11.9.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.702(.066)</td>\n<td id=\"S5.T4.1.1.11.9.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T4.1.1.11.9.8.1\" class=\"ltx_text ltx_font_bold\">.747(.061)</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.12.10\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.12.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S5.T4.1.1.12.10.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.94(.57)</td>\n<td id=\"S5.T4.1.1.12.10.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.92(.50)</td>\n<td id=\"S5.T4.1.1.12.10.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">35.8(5.54)</td>\n<td id=\"S5.T4.1.1.12.10.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.3(5.63)</td>\n<td id=\"S5.T4.1.1.12.10.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.736(.063)</td>\n<td id=\"S5.T4.1.1.12.10.7\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.739(.057)</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Our analysis, as shown in Table IV, investigates the auxiliary task of era identification on difficulty prediction across musical periods. The inclusion of era significantly enhances model performance, as evidenced by improvements in Mean Squared Error (MSE), Accuracy at zero (Acc0), and Tau-c metrics."
        ]
    },
    "S6.SS1.4": {
        "caption": "TABLE V:  Analysis of model performance differentiated by the composer\u2019s gender. TABLE VI:  Zero-shot experiment on Hidden Voices benchmark. The benchmark is a collection of piano pieces by black women composers, out of the distribution from the PSyllabus dataset.",
        "table": "<figure id=\"S6.SS1.4\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE V: </span>Analysis of model performance differentiated by the composer\u2019s gender.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S6.SS1.4.4\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:390.3pt;height:479.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(99.8pt,-122.6pt) scale(2.0479794841506,2.0479794841506) ;\">\n<table id=\"S6.SS1.4.4.4\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.SS1.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.SS1.1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</th>\n<td id=\"S6.SS1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MSE</td>\n<td id=\"S6.SS1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Acc<sub id=\"S6.SS1.1.1.1.1.1.1\" class=\"ltx_sub\">0</sub>\n</td>\n<td id=\"S6.SS1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S6.SS1.2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S6.SS1.2.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S6.SS1.2.2.2.2.1.1\" class=\"ltx_text ltx_markedasmath\">Both genres</span></th>\n<td id=\"S6.SS1.2.2.2.2.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.2.2.2.2.3\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.2.2.2.2.4\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.5.1\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.5.1.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.5.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S6.SS1.4.4.4.5.1.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.29(.18)</td>\n<td id=\"S6.SS1.4.4.4.5.1.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">32.9(1.61)</td>\n<td id=\"S6.SS1.4.4.4.5.1.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.741(.012)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.6.2\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.6.2.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.6.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S6.SS1.4.4.4.6.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.85(.07)</td>\n<td id=\"S6.SS1.4.4.4.6.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.7(.84)</td>\n<td id=\"S6.SS1.4.4.4.6.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.771(.005)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.7.3\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.7.3.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.7.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S6.SS1.4.4.4.7.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.7.3.3.1\" class=\"ltx_text ltx_font_bold\">1.81(.11)</span></td>\n<td id=\"S6.SS1.4.4.4.7.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.7.3.4.1\" class=\"ltx_text ltx_font_bold\">37.3(1.97)</span></td>\n<td id=\"S6.SS1.4.4.4.7.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.7.3.5.1\" class=\"ltx_text ltx_font_bold\">.778(.004)</span></td>\n</tr>\n<tr id=\"S6.SS1.3.3.3.3\" class=\"ltx_tr\">\n<th id=\"S6.SS1.3.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS1.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS1.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Only men</span>\n</th>\n<td id=\"S6.SS1.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.3.3.3.3.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.3.3.3.3.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.8.4\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.8.4.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.8.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S6.SS1.4.4.4.8.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.24(.19)</td>\n<td id=\"S6.SS1.4.4.4.8.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">33.2(1.13)</td>\n<td id=\"S6.SS1.4.4.4.8.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.734(.012)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.9.5\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.9.5.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.9.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S6.SS1.4.4.4.9.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.83(.05)</td>\n<td id=\"S6.SS1.4.4.4.9.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">37.3(.75)</td>\n<td id=\"S6.SS1.4.4.4.9.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.764(.004)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.10.6\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.10.6.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.10.6.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S6.SS1.4.4.4.10.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.10.6.3.1\" class=\"ltx_text ltx_font_bold\">1.81(.09)</span></td>\n<td id=\"S6.SS1.4.4.4.10.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.10.6.4.1\" class=\"ltx_text ltx_font_bold\">37.7(2.46)</span></td>\n<td id=\"S6.SS1.4.4.4.10.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.10.6.5.1\" class=\"ltx_text ltx_font_bold\">.769(.002)</span></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.4\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS1.4.4.4.4.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS1.4.4.4.4.1.2\" class=\"ltx_text ltx_markedasmath\">Only women</span>\n</th>\n<td id=\"S6.SS1.4.4.4.4.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.4.4.4.4.3\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS1.4.4.4.4.4\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.11.7\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.11.7.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.11.7.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</th>\n<td id=\"S6.SS1.4.4.4.11.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.47(.59)</td>\n<td id=\"S6.SS1.4.4.4.11.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">31.6(4.74)</td>\n<td id=\"S6.SS1.4.4.4.11.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.666(.032)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.12.8\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.12.8.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.12.8.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</th>\n<td id=\"S6.SS1.4.4.4.12.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.91(.60)</td>\n<td id=\"S6.SS1.4.4.4.12.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34.6(4.24)</td>\n<td id=\"S6.SS1.4.4.4.12.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.722(.021)</td>\n</tr>\n<tr id=\"S6.SS1.4.4.4.13.9\" class=\"ltx_tr\">\n<th id=\"S6.SS1.4.4.4.13.9.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<th id=\"S6.SS1.4.4.4.13.9.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</th>\n<td id=\"S6.SS1.4.4.4.13.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.13.9.3.1\" class=\"ltx_text ltx_font_bold\">1.72(.49)</span></td>\n<td id=\"S6.SS1.4.4.4.13.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.13.9.4.1\" class=\"ltx_text ltx_font_bold\">35.7(5.74)</span></td>\n<td id=\"S6.SS1.4.4.4.13.9.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS1.4.4.4.13.9.5.1\" class=\"ltx_text ltx_font_bold\">.733(.025)</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S6.SS2\" class=\"ltx_subsection ltx_figure_panel\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\"><span id=\"S6.SS2.11.1.1\" class=\"ltx_text\">VI-B</span> </span><span id=\"S6.SS2.12.2\" class=\"ltx_text ltx_font_italic\">Zero-shot experiment on HV Benchmark</span>\n</h3>\n\n<div id=\"S6.SS2.p1\" class=\"ltx_para\">\n<p id=\"S6.SS2.p1.1\" class=\"ltx_p\">Historically underrepresented groups of composers, such as black women, have not been significantly explored in MIR datasets. We believe it is important to evaluate the capabilities of models in these out-of-distribution scenarios. In this experiment, we assess the ranking capabilities of the proposed approach in a zero-shot setting over the HV dataset by utilizing the model\u2019s logits. These logits follow a monotonic order, as indicated in Section\u00a0<a href=\"#S3.SS4\" title=\"III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-D</span></span></a>. Therefore, we can use them to order the HV collection and compare it with the original ranking.</p>\n</div>\n<div id=\"S6.SS2.p2\" class=\"ltx_para\">\n<p id=\"S6.SS2.p2.1\" class=\"ltx_p\">Table\u00a0<a href=\"#S6.SS2\" title=\"VI-B Zero-shot experiment on HV Benchmark \u2023 VI-A Model performance on works by female composers \u2023 VI A case study on woman composers \u2023 Impact of Musical Era on Difficulty Estimation. \u2023 V-A Auxiliary tasks for pretraining \u2023 V Experiments \u2023 IV-B Training procedure \u2023 IV Experimental setup \u2023 III-D Difficulty Loss \u2023 III Methodology \u2023 II-C1 Hidden Voices dataset \u2023 II-C Benchmark datasets \u2023 II-B Dataset analysis \u2023 II The Piano Syllabus Dataset \u2023 Can Audio Reveal Music Performance Difficulty? Insights from the Piano Syllabus Dataset\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">VI-B</span></span></a> shows the results obtained resorting to the Kendall rank correlation coefficient, <math id=\"S6.SS2.p2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.p2.1.m1.1a\"><msub id=\"S6.SS2.p2.1.m1.1.1\" xref=\"S6.SS2.p2.1.m1.1.1.cmml\"><mi id=\"S6.SS2.p2.1.m1.1.1.2\" xref=\"S6.SS2.p2.1.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.p2.1.m1.1.1.3\" xref=\"S6.SS2.p2.1.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.p2.1.m1.1b\"><apply id=\"S6.SS2.p2.1.m1.1.1.cmml\" xref=\"S6.SS2.p2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.p2.1.m1.1.1.1.cmml\" xref=\"S6.SS2.p2.1.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.p2.1.m1.1.1.2.cmml\" xref=\"S6.SS2.p2.1.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.p2.1.m1.1.1.3.cmml\" xref=\"S6.SS2.p2.1.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.p2.1.m1.1c\">\\tau_{c}</annotation></semantics></math>, for all data collections discussed in the experiment, considering both the single-task and multi-task models posed. Note that HV is only used for benchmarking purposes.</p>\n</div>\n<figure id=\"S6.SS2.6\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE VI: </span>Zero-shot experiment on Hidden Voices benchmark. The benchmark is a collection of piano pieces by black women composers, out of the distribution from the PSyllabus dataset.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S6.SS2.3.3\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:303.5pt;height:647pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(105.3pt,-224.5pt) scale(3.26768397143762,3.26768397143762) ;\">\n<table id=\"S6.SS2.3.3.3\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.SS2.3.3.3.4.1\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</td>\n<td id=\"S6.SS2.3.3.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S6.SS2.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.SS2.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S6.SS2.1.1.1.1.1.1\" class=\"ltx_text ltx_markedasmath\">Single-task</span></td>\n<td id=\"S6.SS2.1.1.1.1.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.5.2\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.5.2.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.5.2.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.5.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.592(.020)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.6.3\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.6.3.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.6.3.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.6.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.6.3.3.1\" class=\"ltx_text ltx_font_bold\">.661(.018)</span></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.7.4\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.7.4.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.7.4.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</td>\n<td id=\"S6.SS2.3.3.3.7.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.660(.029)</td>\n</tr>\n<tr id=\"S6.SS2.2.2.2.2\" class=\"ltx_tr\">\n<td id=\"S6.SS2.2.2.2.2.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS2.2.2.2.2.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS2.2.2.2.2.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with Era</span>\n</td>\n<td id=\"S6.SS2.2.2.2.2.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.8.5\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.8.5.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.8.5.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.8.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.634(.039)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.9.6\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.9.6.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.9.6.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.9.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.9.6.3.1\" class=\"ltx_text ltx_font_bold\">.668(.047)</span></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.3\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.3.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS2.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS2.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with Multiranking</span>\n</td>\n<td id=\"S6.SS2.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.10.7\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.10.7.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.10.7.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.10.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.632(.025)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.11.8\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.11.8.1\" class=\"ltx_td ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.11.8.2\" class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.11.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.11.8.3.1\" class=\"ltx_text ltx_font_bold\">.672(.031)</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S6.SS2.5.5\" class=\"ltx_p ltx_figure_panel\">In the zero-shot experiment conducted on the Hidden Voices benchmark, the findings underscore the advantages of multi-task models based on PR, particularly those that incorporate era information and multi-ranking strategies. The experiment reveals that the multi-task model utilizing era data with the PR approach achieved a <math id=\"S6.SS2.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.4.4.m1.1a\"><msub id=\"S6.SS2.4.4.m1.1.1\" xref=\"S6.SS2.4.4.m1.1.1.cmml\"><mi id=\"S6.SS2.4.4.m1.1.1.2\" xref=\"S6.SS2.4.4.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.4.4.m1.1.1.3\" xref=\"S6.SS2.4.4.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.4.4.m1.1b\"><apply id=\"S6.SS2.4.4.m1.1.1.cmml\" xref=\"S6.SS2.4.4.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.4.4.m1.1.1.1.cmml\" xref=\"S6.SS2.4.4.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.4.4.m1.1.1.2.cmml\" xref=\"S6.SS2.4.4.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.4.4.m1.1.1.3.cmml\" xref=\"S6.SS2.4.4.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.4.4.m1.1c\">\\tau_{c}</annotation></semantics></math> value of 0.668, emphasizing the importance of historical context in improving the performance of models on out-of-distribution data. Moreover, the PR model in the multi-task with Multiranking setup demonstrated the highest adaptability and performance, achieving a <math id=\"S6.SS2.5.5.m2.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.5.5.m2.1a\"><msub id=\"S6.SS2.5.5.m2.1.1\" xref=\"S6.SS2.5.5.m2.1.1.cmml\"><mi id=\"S6.SS2.5.5.m2.1.1.2\" xref=\"S6.SS2.5.5.m2.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.5.5.m2.1.1.3\" xref=\"S6.SS2.5.5.m2.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.5.5.m2.1b\"><apply id=\"S6.SS2.5.5.m2.1.1.cmml\" xref=\"S6.SS2.5.5.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.5.5.m2.1.1.1.cmml\" xref=\"S6.SS2.5.5.m2.1.1\">subscript</csymbol><ci id=\"S6.SS2.5.5.m2.1.1.2.cmml\" xref=\"S6.SS2.5.5.m2.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.5.5.m2.1.1.3.cmml\" xref=\"S6.SS2.5.5.m2.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.5.5.m2.1c\">\\tau_{c}</annotation></semantics></math> value of 0.672. This insight is vital for the development of inclusive machine-learning tools in the field of music research, highlighting the potential of combining multiple rankings to improve model performance on out-of-distribution datasets.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S6.SS2.6.6\" class=\"ltx_p ltx_figure_panel\">The results are not fully comparable with those of the HV with sheet music image presented in\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite> due to the dataset containing only 17 pieces. However, in the previous experiment, the best <math id=\"S6.SS2.6.6.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.6.6.m1.1a\"><msub id=\"S6.SS2.6.6.m1.1.1\" xref=\"S6.SS2.6.6.m1.1.1.cmml\"><mi id=\"S6.SS2.6.6.m1.1.1.2\" xref=\"S6.SS2.6.6.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.6.6.m1.1.1.3\" xref=\"S6.SS2.6.6.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.6.6.m1.1b\"><apply id=\"S6.SS2.6.6.m1.1.1.cmml\" xref=\"S6.SS2.6.6.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.6.6.m1.1.1.1.cmml\" xref=\"S6.SS2.6.6.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.6.6.m1.1.1.2.cmml\" xref=\"S6.SS2.6.6.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.6.6.m1.1.1.3.cmml\" xref=\"S6.SS2.6.6.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.6.6.m1.1c\">\\tau_{c}</annotation></semantics></math> result was 0.56, indicating that the audio-based version is more robust. Further research is needed to compare sheet music image classification audio to determine whether the modality or the quality of the datasets influences generalization.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S7\" class=\"ltx_section ltx_figure_panel\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">VII </span><span id=\"S7.1.1\" class=\"ltx_text ltx_font_smallcaps\">Conclusion</span>\n</h2>\n\n<div id=\"S7.p1\" class=\"ltx_para\">\n<p id=\"S7.p1.1\" class=\"ltx_p\">In conclusion, our study bridges a significant gap in music education and Music Information Retrieval by harnessing the widespread availability of audio via streaming platforms to estimate the difficulty of musical scores. This approach enhances students\u2019 ability to explore an extensive repertoire and collaborate with educators, significantly improving their learning experience and motivation. Our research introduces several key advancements.</p>\n</div>\n<div id=\"S7.p2\" class=\"ltx_para\">\n<p id=\"S7.p2.1\" class=\"ltx_p\">We have developed a model using a CNN+RNN+Attention network as a baseline for capturing performance difficulty from diverse audio representations. This model is assessed using a novel and expansive audio collection, setting a new benchmark for the scale of datasets in difficulty classification. Furthermore, we propose a multi-performance benchmark to investigate the impact of various performances on difficulty prediction, expanding the understanding of performance variability.</p>\n</div>\n<div id=\"S7.p3\" class=\"ltx_para\">\n<p id=\"S7.p3.1\" class=\"ltx_p\">Our comprehensive experimental framework includes testing generalization through a zero-shot scenario in out-of-domain distributions, employing multi-task learning with tasks related to music performance, and training across multiple difficulty rankings. These experiments underscore the robustness and versatility of our methodologies.</p>\n</div>\n<div id=\"S7.p4\" class=\"ltx_para\">\n<p id=\"S7.p4.1\" class=\"ltx_p\">To catalyze further research and collaboration within the music education community, we have publicly made our code, models, and the <em id=\"S7.p4.1.1\" class=\"ltx_emph ltx_font_italic\">Piano Syllabus (PSyllabus)</em> dataset available, including transcribed midis and CQT features. This initiative aims to create a shared platform for advancing automated performance difficulty understanding and enhancing music learning.</p>\n</div>\n<div id=\"S7.p5\" class=\"ltx_para\">\n<p id=\"S7.p5.1\" class=\"ltx_p\">Our contributions lay a solid foundation for the future of audio-based difficulty classification in music, demonstrating the potential for meaningful advancements in the automated analysis of musical scores. By providing comprehensive resources for the community, we invite educators, students, and researchers to engage with our work, fostering a collaborative ecosystem that supports the evolution of music education.</p>\n</div>\n<section id=\"Sx1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_font_smallcaps ltx_title_section\">Acknowledgment</h2>\n\n<div id=\"Sx1.p1\" class=\"ltx_para\">\n<p id=\"Sx1.p1.1\" class=\"ltx_p\">The authors thank Ian Wheaton, the creator of the Piano Syllabus web community, for his helpful guidance and for answering our questions. We are also grateful to him and the entire web community for their hard work in putting together the corpus to create a single source of information on piano difficulty. This effort is a strong base for our research, as we aim to use this resource to learn more and do deeper analysis. We hope our work benefits the Piano Syllabus and the music education community by aiding in the labeling of pieces and enabling the exploration of the forgotten cultural heritage.</p>\n</div>\n<div id=\"Sx1.p2\" class=\"ltx_para\">\n<p id=\"Sx1.p2.1\" class=\"ltx_p\">Pedro would also like to thank Nazif C. Tamer for his insistence on shifting focus to audio, and Pablo Alonso and Oguz Araz for their insightful discussions about audio features.</p>\n</div>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[1]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Mellizo, \u201cMusic education, curriculum design, and assessment: Imagining a more equitable approach,\u201d <em id=\"bib.bib1.1.1\" class=\"ltx_emph ltx_font_italic\">Music Educators Journal</em>, vol. 106, no.\u00a04, pp. 57\u201365, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[2]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0R. Jorgensen, \u201cThe curriculum design process in music,\u201d <em id=\"bib.bib2.1.1\" class=\"ltx_emph ltx_font_italic\">College Music Symposium</em>, vol.\u00a028, pp. 94\u2013105, 1988. [Online]. Available: <a target=\"_blank\" href=\"http://www.jstor.org/stable/40374590\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">http://www.jstor.org/stable/40374590</a>\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[3]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0S. Deconto, E.\u00a0L.\u00a0F. Valenga, and C.\u00a0N. Silla, \u201cAutomatic music score difficulty classification,\u201d in <em id=\"bib.bib3.1.1\" class=\"ltx_emph ltx_font_italic\">2023 30th International Conference on Systems, Signals and Image Processing (IWSSIP)</em>.\u00a0\u00a0\u00a0IEEE, 2023, pp. 1\u20135.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[4]</span>\n<span class=\"ltx_bibblock\">\nV.\u00a0S\u00e9bastien, H.\u00a0Ralambondrainy, O.\u00a0S\u00e9bastien, and N.\u00a0Conruyt, \u201cScore analyzer: Automatically determining scores difficulty level for instrumental e-learning,\u201d in <em id=\"bib.bib4.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of 13th International Society for Music Information Retrieval Conference, ISMIR</em>, Porto, Portugal, 2012.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[5]</span>\n<span class=\"ltx_bibblock\">\nS.-C. Chiu and M.-S. Chen, \u201cA study on difficulty level recognition of piano sheet music,\u201d in <em id=\"bib.bib5.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Symposium on Multimedia</em>.\u00a0\u00a0\u00a0IEEE, 2012, pp. 17\u201323.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[6]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura and K.\u00a0Yoshii, \u201cStatistical piano reduction controlling performance difficulty,\u201d <em id=\"bib.bib6.1.1\" class=\"ltx_emph ltx_font_italic\">APSIPA Transactions on Signal and Information Processing</em>, vol.\u00a07, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[7]</span>\n<span class=\"ltx_bibblock\">\n\u201cMusescore have automatic difficulty categories from year 2022,\u201d <a target=\"_blank\" href=\"https://musescore.com/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://musescore.com/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[8]</span>\n<span class=\"ltx_bibblock\">\n\u201cUltimate guitar have automatic difficulty categories from year 2022,\u201d <a target=\"_blank\" href=\"https://www.ultimate-guitar.com/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://www.ultimate-guitar.com/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[9]</span>\n<span class=\"ltx_bibblock\">\n\u201cSystem for estimating user\u2019s skill in playing a music instrument and determining virtual exercises thereof,\u201d Patent US9\u2009767\u2009705B1, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[10]</span>\n<span class=\"ltx_bibblock\">\nQ.\u00a0Kong, B.\u00a0Li, X.\u00a0Song, Y.\u00a0Wan, and Y.\u00a0Wang, \u201cHigh-resolution piano transcription with pedals by regressing onset and offset times,\u201d vol.\u00a029, p. 3707\u20133717, oct 2021.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[11]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura, N.\u00a0Ono, and S.\u00a0Sagayama, \u201cMerged-output hmm for piano fingering of both hands.\u201d in <em id=\"bib.bib11.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR</em>, Taipei, Taiwan, 2014, pp. 531\u2013536.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[12]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura and S.\u00a0Sagayama, \u201cAutomatic piano reduction from ensemble scores based on merged-output hidden markov model,\u201d in <em id=\"bib.bib12.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 41st International Computer Music Conference, ICMC</em>, Denton, USA, 2015.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[13]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, N.\u00a0C. Tamer, V.\u00a0Eremenko, M.\u00a0Miron, and X.\u00a0Serra, \u201cScore difficulty analysis for piano performance education,\u201d in <em id=\"bib.bib13.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP</em>, Singapore, Singapore, 2022, pp. 201\u2013205.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[14]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, D.\u00a0Jeong, V.\u00a0Eremenko, N.\u00a0C. Tamer, M.\u00a0Miron, and X.\u00a0Serra, \u201cCombining piano performance dimensions for score difficulty classification,\u201d <em id=\"bib.bib14.1.1\" class=\"ltx_emph ltx_font_italic\">Expert Systems with Applications</em>, vol. 238, p. 121776, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[15]</span>\n<span class=\"ltx_bibblock\">\nH.\u00a0Zhang, E.\u00a0Karystinaios, S.\u00a0Dixon, G.\u00a0Widmer, and C.\u00a0E. Cancino-Chac\u00f3n, \u201cSymbolic music representations for classification tasks: A systematic evaluation,\u201d <em id=\"bib.bib15.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2309.02567</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[16]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, D.\u00a0Jeong, E.\u00a0Nakamura, X.\u00a0Serra, and M.\u00a0Miron, \u201cAutomatic piano fingering from partially annotated scores using autoregressive neural networks,\u201d in <em id=\"bib.bib16.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 30th ACM International Conference on Multimedia</em>, 2022, pp. 6502\u20136510.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[17]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0Jeong, T.\u00a0Kwon, Y.\u00a0Kim, K.\u00a0Lee, and J.\u00a0Nam, \u201cVirtuosoNet: A hierarchical RNN-based system for modeling expressive piano performance,\u201d in <em id=\"bib.bib17.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR</em>, 2019, pp. 908\u2013915.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[18]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Cheng, Z.\u00a0Wang, and G.\u00a0Pollastri, \u201cA neural network approach to ordinal regression,\u201d in <em id=\"bib.bib18.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Joint Conference on Neural Networks, IJCNN</em>.\u00a0\u00a0\u00a0Hong Kong, China: IEEE, 2008, pp. 1279\u20131284.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[19]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0A.\u00a0V. V\u00e1squez, M.\u00a0Baelemans, J.\u00a0Driedger, W.\u00a0Zuidema, and J.\u00a0A. Burgoyne, \u201cQuantifying the ease of playing song chords on the guitar,\u201d in <em id=\"bib.bib19.1.1\" class=\"ltx_emph ltx_font_italic\">Ismir 2023 Hybrid Conference</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[20]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Toyama, T.\u00a0Akama, Y.\u00a0Ikemiya, Y.\u00a0Takida, W.-H. Liao, and Y.\u00a0Mitsufuji, \u201cAutomatic piano transcription with hierarchical frequency-time transformer,\u201d in <em id=\"bib.bib20.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[21]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Rege and R.\u00a0Sindal, \u201cReview of f0 estimation in the context of indian classical music expression detection,\u201d in <em id=\"bib.bib21.1.1\" class=\"ltx_emph ltx_font_italic\">Social Networking and Computational Intelligence: Proceedings of SCI-2018</em>.\u00a0\u00a0\u00a0Springer, 2020, pp. 257\u2013268.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[22]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Alonso-Jim\u00e9nez, X.\u00a0Serra, and D.\u00a0Bogdanov, \u201cMusic representation learning based on editorial metadata from discogs,\u201d in <em id=\"bib.bib22.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[23]</span>\n<span class=\"ltx_bibblock\">\nC.\u00a0Sch\u00f6rkhuber and A.\u00a0Klapuri, \u201cConstant-q transform toolbox for music processing,\u201d in <em id=\"bib.bib23.1.1\" class=\"ltx_emph ltx_font_italic\">7th sound and music computing conference, Barcelona, Spain</em>, 2010, pp. 3\u201364.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[24]</span>\n<span class=\"ltx_bibblock\">\nQ.\u00a0Kong, B.\u00a0Li, J.\u00a0Chen, and Y.\u00a0Wang, \u201cGiantmidi-piano: A large-scale MIDI dataset for classical piano music,\u201d <em id=\"bib.bib24.1.1\" class=\"ltx_emph ltx_font_italic\">Transactions of the International Society for Music Information Retrieval, 5(1), pp.87\u201398</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[25]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, J.\u00a0J. Valero-Mas, D.\u00a0Jeong, and X.\u00a0Serra, \u201cPredicting performance difficulty from piano sheet music images,\u201d in <em id=\"bib.bib25.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. of the 24th Int. Society for Music Information Retrieval Conf.</em>, Milan, Italy, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[26]</span>\n<span class=\"ltx_bibblock\">\nUniversity of Colorado, \u201cHidden voices project,\u201d <a target=\"_blank\" href=\"https://www.colorado.edu/project/hidden-voices/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://www.colorado.edu/project/hidden-voices/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[27]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0F. Gemmeke, D.\u00a0P. Ellis, D.\u00a0Freedman, A.\u00a0Jansen, W.\u00a0Lawrence, R.\u00a0C. Moore, M.\u00a0Plakal, and M.\u00a0Ritter, \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in <em id=\"bib.bib27.1.1\" class=\"ltx_emph ltx_font_italic\">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.\u00a0\u00a0\u00a0IEEE, 2017, pp. 776\u2013780.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[28]</span>\n<span class=\"ltx_bibblock\">\nV.\u00a0Lakic, L.\u00a0Rossetto, and A.\u00a0Bernstein, \u201cLink-rot in web-sourced multimedia datasets,\u201d in <em id=\"bib.bib28.1.1\" class=\"ltx_emph ltx_font_italic\">MultiMedia Modeling</em>.\u00a0\u00a0\u00a0Cham: Springer International Publishing, 2023, pp. 476\u2013488.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[29]</span>\n<span class=\"ltx_bibblock\">\nN.\u00a0Kato, E.\u00a0Nakamura, K.\u00a0Mine, O.\u00a0Doeda, and M.\u00a0Yamada, \u201cComputational analysis of audio recordings of piano performance for automatic evaluation,\u201d in <em id=\"bib.bib29.1.1\" class=\"ltx_emph ltx_font_italic\">Responsive and Sustainable Educational Futures</em>.\u00a0\u00a0\u00a0Cham: Springer Nature Switzerland, 2023, pp. 586\u2013592.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[30]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0O\u2019Hanlon and M.\u00a0B. Sandler, \u201cComparing cqt and reassignment based chroma features for template-based automatic chord recognition,\u201d in <em id=\"bib.bib30.1.1\" class=\"ltx_emph ltx_font_italic\">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP</em>.\u00a0\u00a0\u00a0IEEE, 2019, pp. 860\u2013864.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[31]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda and G.\u00a0Bernardes, \u201cRevisiting harmonic change detection,\u201d in <em id=\"bib.bib31.1.1\" class=\"ltx_emph ltx_font_italic\">Audio Engineering Society Convention 149</em>, Oct 2020.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[32]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Wang, A.\u00a0Singh, J.\u00a0Michael, F.\u00a0Hill, O.\u00a0Levy, and S.\u00a0R. Bowman, \u201cGlue: A multi-task benchmark and analysis platform for natural language understanding,\u201d in <em id=\"bib.bib32.1.1\" class=\"ltx_emph ltx_font_italic\">7th International Conference on Learning Representations, ICLR</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[33]</span>\n<span class=\"ltx_bibblock\">\nI.\u00a0Laina, C.\u00a0Rupprecht, V.\u00a0Belagiannis, F.\u00a0Tombari, and N.\u00a0Navab, \u201cDeeper depth prediction with fully convolutional residual networks,\u201d in <em id=\"bib.bib33.1.1\" class=\"ltx_emph ltx_font_italic\">2016 Fourth international conference on 3D vision (3DV)</em>.\u00a0\u00a0\u00a0IEEE, 2016, pp. 239\u2013248.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[34]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Wu, C.\u00a0Shen, and A.\u00a0Van Den\u00a0Hengel, \u201cWider or deeper: Revisiting the resnet model for visual recognition,\u201d <em id=\"bib.bib34.1.1\" class=\"ltx_emph ltx_font_italic\">Pattern Recognition</em>, vol.\u00a090, pp. 119\u2013133, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[35]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Won, K.\u00a0Choi, and X.\u00a0Serra, \u201cSemi-supervised music tagging transformer,\u201d in <em id=\"bib.bib35.1.1\" class=\"ltx_emph ltx_font_italic\">International Society for Music Information Retrieval Conference</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[36]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Allen-Zhu and Y.\u00a0Li, \u201cWhat can resnet learn efficiently, going beyond kernels?\u201d <em id=\"bib.bib36.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, vol.\u00a032, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[37]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Cho, B.\u00a0van Merrienboer, \u00c7.\u00a0G\u00fcl\u00e7ehre, D.\u00a0Bahdanau, F.\u00a0Bougares, H.\u00a0Schwenk, and Y.\u00a0Bengio, \u201cLearning phrase representations using RNN encoder-decoder for statistical machine translation,\u201d in <em id=\"bib.bib37.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL</em>, A.\u00a0Moschitti, B.\u00a0Pang, and W.\u00a0Daelemans, Eds.\u00a0\u00a0\u00a0ACL, 2014, pp. 1724\u20131734. [Online]. Available: <a target=\"_blank\" href=\"https://doi.org/10.3115/v1/d14-1179\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://doi.org/10.3115/v1/d14-1179</a>\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[38]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Chung, \u00c7.\u00a0G\u00fcl\u00e7ehre, K.\u00a0Cho, and Y.\u00a0Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d <em id=\"bib.bib38.1.1\" class=\"ltx_emph ltx_font_italic\">CoRR</em>, vol. abs/1412.3555, 2014. [Online]. Available: <a target=\"_blank\" href=\"http://arxiv.org/abs/1412.3555\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">http://arxiv.org/abs/1412.3555</a>\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[39]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Yang, D.\u00a0Yang, C.\u00a0Dyer, X.\u00a0He, A.\u00a0Smola, and E.\u00a0Hovy, \u201cHierarchical attention networks for document classification,\u201d in <em id=\"bib.bib39.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</em>, 2016, pp. 1480\u20131489.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[40]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Maghoumi and J.\u00a0J. LaViola, \u201cDeepgru: Deep gesture recognition utility,\u201d in <em id=\"bib.bib40.1.1\" class=\"ltx_emph ltx_font_italic\">International Symposium on Visual Computing</em>.\u00a0\u00a0\u00a0Springer, 2019, pp. 16\u201331.\n\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[41]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Alfaro-Contreras, J.\u00a0J. Valero-Mas, J.\u00a0M. I\u00f1esta, and J.\u00a0Calvo-Zaragoza, \u201cLate multimodal fusion for image and audio music transcription,\u201d <em id=\"bib.bib41.1.1\" class=\"ltx_emph ltx_font_italic\">Expert Systems with Applications</em>, vol. 216, p. 119491, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[42]</span>\n<span class=\"ltx_bibblock\">\nL.\u00a0Gaudette and N.\u00a0Japkowicz, \u201cEvaluation methods for ordinal classification,\u201d in <em id=\"bib.bib42.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 22nd Canadian Conference on Advances in Artificial Intelligence</em>, Kelowna, Canada, 2009, pp. 207\u2013210.\n\n</span>\n</li>\n</ul>\n</section>\n<figure id=\"Sx1.1\" class=\"ltx_float biography\">\n<table id=\"Sx1.1.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.1.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.1.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/pedro.png\" id=\"Sx1.1.1.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"100\" height=\"110\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.1.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.1.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.1.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Pedro Ramoneda</span> \nPedro Ramoneda holds a BSc in Computer Science from the University of Zaragoza, a Professional Degree in Piano Performance from the Conservatory of Music in Zaragoza, and an MSc in Sound and Music Computing from the Universitat Pompeu Fabra. He is currently a third-year PhD student in the Music Technology Group of the Universitat Pompeu Fabra under the supervision of Prof. Xavier Serra, focusing on the use of technologies from the Music Information Retrieval and Signal Processing field for supporting music education.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.2\" class=\"ltx_float biography\">\n<table id=\"Sx1.2.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.2.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.2.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/Minhee_Lee.jpeg\" id=\"Sx1.2.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"94\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.2.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.2.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.2.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Minhee Lee</span> \nMinhee Lee is an undergraduate student pursuing a B.S. degree in the Department of Computer Science and Engineering at Sogang University in South Korea. She is an undergraduate intern in the MALer Lab under the supervision of Prof. Dasaem Jeong since 2023. Before joining the research group, she did internships as a software engineer at Google in 2021 and 2022, and at FuriosaAI in 2022. Her research interest is about various music information retrieval tasks of understanding music with deep learning technologies.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.3\" class=\"ltx_float biography\">\n<table id=\"Sx1.3.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.3.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.3.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/Dasaem_Jeong.jpg\" id=\"Sx1.3.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"100\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.3.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.3.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.3.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.3.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Dasaem Jeong</span>  is currently working as an Assistant Professor in the Department of Art &amp; Technology at Sogang University in South Korea since 2021. Before joining Sogang University, he worked as a research scientist in T-Brain X, SK Telecom from 2020 to 2021. He obtained his Ph.D. and M.S. degrees in culture technology, and B.S. in mechanical engineering from Korea Advanced Institute of Science and Technology (KAIST). His research primarily focuses on a diverse range of music information retrieval tasks, including music generation and computational musicology.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.4\" class=\"ltx_float biography\">\n<table id=\"Sx1.4.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.4.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.4.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/JJVM.png\" id=\"Sx1.4.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"85\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.4.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.4.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.4.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.4.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Jose J. Valero-Mas</span>  received the M.Sc. degree in Telecommunications Engineering from the University Miguel Hern\u00e1ndez of Elche in 2012, the M.Sc. degree in Sound and Music Computing from the Universitat Pompeu Fabra in 2013, and the Ph.D. degree in Computer Science from the University of Alicante in 2017. After a three-year period in industry in which he developed as a data scientist, he acted as a Postdoctoral Researcher from 2020 to 2023 in the University of Alicante, the Universitat Pompeu Fabra, and the Queen Mary University of London. He is currently an Assistant Professor in the Department of Software and Computing Systems of the University of Alicante. His research interests include Pattern Recognition, Machine Learning, Music Information Retrieval, and Signal Processing for which he has co-authored more than 40 works within international journals, conference communications, and book chapters.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.5\" class=\"ltx_float biography\">\n<table id=\"Sx1.5.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.5.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.5.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/serra.png\" id=\"Sx1.5.1.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"100\" height=\"110\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.5.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.5.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.5.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.5.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Xavier Serra</span> \nHe has had a progressive academic career in the field of music technology since earning his PhD from Stanford University in 1989. His interest lies in the analysis, synthesis, and description of sound and music signals. He always seeks a balance between basic and applied research while integrating both scientific/technological and humanistic/artistic disciplines. Currently, he supervises research at the Music Technology Group (MTG), focusing on understanding sound and music signals through signal processing, machine learning, and semantic technologies. His work emphasizes data-driven and knowledge-driven methodologies, involving the development of large data collections and the application of domain-specific knowledge. He leads projects funded both publicly and privately, tackling practical issues such as music exploration, sound classification, and music performance analysis for educational purposes. He champions open science by promoting open data, software, and access, and is keen on using open innovation strategies to enhance the social and economic impact of his research.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n</section>\n</section>\n</div>\n</div>\n</figure>\n</section>\n</div>\n</div>\n</figure>\n",
        "footnotes": [],
        "references": []
    },
    "S6.SS2.6": {
        "caption": "TABLE VI:  Zero-shot experiment on Hidden Voices benchmark. The benchmark is a collection of piano pieces by black women composers, out of the distribution from the PSyllabus dataset.",
        "table": "<figure id=\"S6.SS2.6\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE VI: </span>Zero-shot experiment on Hidden Voices benchmark. The benchmark is a collection of piano pieces by black women composers, out of the distribution from the PSyllabus dataset.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S6.SS2.3.3\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:303.5pt;height:647pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(105.3pt,-224.5pt) scale(3.26768397143762,3.26768397143762) ;\">\n<table id=\"S6.SS2.3.3.3\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.SS2.3.3.3.4.1\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">Experiment</td>\n<td id=\"S6.SS2.3.3.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tau-c</td>\n</tr>\n<tr id=\"S6.SS2.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.SS2.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\"><span id=\"S6.SS2.1.1.1.1.1.1\" class=\"ltx_text ltx_markedasmath\">Single-task</span></td>\n<td id=\"S6.SS2.1.1.1.1.2\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.5.2\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.5.2.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.5.2.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.5.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.592(.020)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.6.3\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.6.3.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.6.3.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.6.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.6.3.3.1\" class=\"ltx_text ltx_font_bold\">.661(.018)</span></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.7.4\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.7.4.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.7.4.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MM</td>\n<td id=\"S6.SS2.3.3.3.7.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.660(.029)</td>\n</tr>\n<tr id=\"S6.SS2.2.2.2.2\" class=\"ltx_tr\">\n<td id=\"S6.SS2.2.2.2.2.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS2.2.2.2.2.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS2.2.2.2.2.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with Era</span>\n</td>\n<td id=\"S6.SS2.2.2.2.2.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.8.5\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.8.5.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.8.5.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.8.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.634(.039)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.9.6\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.9.6.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.9.6.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.9.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.9.6.3.1\" class=\"ltx_text ltx_font_bold\">.668(.047)</span></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.3\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.3.1\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"2\">\n<span id=\"S6.SS2.3.3.3.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S6.SS2.3.3.3.3.1.2\" class=\"ltx_text ltx_markedasmath\">Multi-task with Multiranking</span>\n</td>\n<td id=\"S6.SS2.3.3.3.3.2\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.10.7\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.10.7.1\" class=\"ltx_td\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.10.7.2\" class=\"ltx_td ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CQT</td>\n<td id=\"S6.SS2.3.3.3.10.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">.632(.025)</td>\n</tr>\n<tr id=\"S6.SS2.3.3.3.11.8\" class=\"ltx_tr\">\n<td id=\"S6.SS2.3.3.3.11.8.1\" class=\"ltx_td ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></td>\n<td id=\"S6.SS2.3.3.3.11.8.2\" class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PR</td>\n<td id=\"S6.SS2.3.3.3.11.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S6.SS2.3.3.3.11.8.3.1\" class=\"ltx_text ltx_font_bold\">.672(.031)</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S6.SS2.5.5\" class=\"ltx_p ltx_figure_panel\">In the zero-shot experiment conducted on the Hidden Voices benchmark, the findings underscore the advantages of multi-task models based on PR, particularly those that incorporate era information and multi-ranking strategies. The experiment reveals that the multi-task model utilizing era data with the PR approach achieved a <math id=\"S6.SS2.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.4.4.m1.1a\"><msub id=\"S6.SS2.4.4.m1.1.1\" xref=\"S6.SS2.4.4.m1.1.1.cmml\"><mi id=\"S6.SS2.4.4.m1.1.1.2\" xref=\"S6.SS2.4.4.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.4.4.m1.1.1.3\" xref=\"S6.SS2.4.4.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.4.4.m1.1b\"><apply id=\"S6.SS2.4.4.m1.1.1.cmml\" xref=\"S6.SS2.4.4.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.4.4.m1.1.1.1.cmml\" xref=\"S6.SS2.4.4.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.4.4.m1.1.1.2.cmml\" xref=\"S6.SS2.4.4.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.4.4.m1.1.1.3.cmml\" xref=\"S6.SS2.4.4.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.4.4.m1.1c\">\\tau_{c}</annotation></semantics></math> value of 0.668, emphasizing the importance of historical context in improving the performance of models on out-of-distribution data. Moreover, the PR model in the multi-task with Multiranking setup demonstrated the highest adaptability and performance, achieving a <math id=\"S6.SS2.5.5.m2.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.5.5.m2.1a\"><msub id=\"S6.SS2.5.5.m2.1.1\" xref=\"S6.SS2.5.5.m2.1.1.cmml\"><mi id=\"S6.SS2.5.5.m2.1.1.2\" xref=\"S6.SS2.5.5.m2.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.5.5.m2.1.1.3\" xref=\"S6.SS2.5.5.m2.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.5.5.m2.1b\"><apply id=\"S6.SS2.5.5.m2.1.1.cmml\" xref=\"S6.SS2.5.5.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.5.5.m2.1.1.1.cmml\" xref=\"S6.SS2.5.5.m2.1.1\">subscript</csymbol><ci id=\"S6.SS2.5.5.m2.1.1.2.cmml\" xref=\"S6.SS2.5.5.m2.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.5.5.m2.1.1.3.cmml\" xref=\"S6.SS2.5.5.m2.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.5.5.m2.1c\">\\tau_{c}</annotation></semantics></math> value of 0.672. This insight is vital for the development of inclusive machine-learning tools in the field of music research, highlighting the potential of combining multiple rankings to improve model performance on out-of-distribution datasets.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S6.SS2.6.6\" class=\"ltx_p ltx_figure_panel\">The results are not fully comparable with those of the HV with sheet music image presented in\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite> due to the dataset containing only 17 pieces. However, in the previous experiment, the best <math id=\"S6.SS2.6.6.m1.1\" class=\"ltx_Math\" alttext=\"\\tau_{c}\" display=\"inline\"><semantics id=\"S6.SS2.6.6.m1.1a\"><msub id=\"S6.SS2.6.6.m1.1.1\" xref=\"S6.SS2.6.6.m1.1.1.cmml\"><mi id=\"S6.SS2.6.6.m1.1.1.2\" xref=\"S6.SS2.6.6.m1.1.1.2.cmml\">\u03c4</mi><mi id=\"S6.SS2.6.6.m1.1.1.3\" xref=\"S6.SS2.6.6.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.SS2.6.6.m1.1b\"><apply id=\"S6.SS2.6.6.m1.1.1.cmml\" xref=\"S6.SS2.6.6.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.SS2.6.6.m1.1.1.1.cmml\" xref=\"S6.SS2.6.6.m1.1.1\">subscript</csymbol><ci id=\"S6.SS2.6.6.m1.1.1.2.cmml\" xref=\"S6.SS2.6.6.m1.1.1.2\">\ud835\udf0f</ci><ci id=\"S6.SS2.6.6.m1.1.1.3.cmml\" xref=\"S6.SS2.6.6.m1.1.1.3\">\ud835\udc50</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.SS2.6.6.m1.1c\">\\tau_{c}</annotation></semantics></math> result was 0.56, indicating that the audio-based version is more robust. Further research is needed to compare sheet music image classification audio to determine whether the modality or the quality of the datasets influences generalization.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S7\" class=\"ltx_section ltx_figure_panel\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">VII </span><span id=\"S7.1.1\" class=\"ltx_text ltx_font_smallcaps\">Conclusion</span>\n</h2>\n\n<div id=\"S7.p1\" class=\"ltx_para\">\n<p id=\"S7.p1.1\" class=\"ltx_p\">In conclusion, our study bridges a significant gap in music education and Music Information Retrieval by harnessing the widespread availability of audio via streaming platforms to estimate the difficulty of musical scores. This approach enhances students\u2019 ability to explore an extensive repertoire and collaborate with educators, significantly improving their learning experience and motivation. Our research introduces several key advancements.</p>\n</div>\n<div id=\"S7.p2\" class=\"ltx_para\">\n<p id=\"S7.p2.1\" class=\"ltx_p\">We have developed a model using a CNN+RNN+Attention network as a baseline for capturing performance difficulty from diverse audio representations. This model is assessed using a novel and expansive audio collection, setting a new benchmark for the scale of datasets in difficulty classification. Furthermore, we propose a multi-performance benchmark to investigate the impact of various performances on difficulty prediction, expanding the understanding of performance variability.</p>\n</div>\n<div id=\"S7.p3\" class=\"ltx_para\">\n<p id=\"S7.p3.1\" class=\"ltx_p\">Our comprehensive experimental framework includes testing generalization through a zero-shot scenario in out-of-domain distributions, employing multi-task learning with tasks related to music performance, and training across multiple difficulty rankings. These experiments underscore the robustness and versatility of our methodologies.</p>\n</div>\n<div id=\"S7.p4\" class=\"ltx_para\">\n<p id=\"S7.p4.1\" class=\"ltx_p\">To catalyze further research and collaboration within the music education community, we have publicly made our code, models, and the <em id=\"S7.p4.1.1\" class=\"ltx_emph ltx_font_italic\">Piano Syllabus (PSyllabus)</em> dataset available, including transcribed midis and CQT features. This initiative aims to create a shared platform for advancing automated performance difficulty understanding and enhancing music learning.</p>\n</div>\n<div id=\"S7.p5\" class=\"ltx_para\">\n<p id=\"S7.p5.1\" class=\"ltx_p\">Our contributions lay a solid foundation for the future of audio-based difficulty classification in music, demonstrating the potential for meaningful advancements in the automated analysis of musical scores. By providing comprehensive resources for the community, we invite educators, students, and researchers to engage with our work, fostering a collaborative ecosystem that supports the evolution of music education.</p>\n</div>\n<section id=\"Sx1\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_font_smallcaps ltx_title_section\">Acknowledgment</h2>\n\n<div id=\"Sx1.p1\" class=\"ltx_para\">\n<p id=\"Sx1.p1.1\" class=\"ltx_p\">The authors thank Ian Wheaton, the creator of the Piano Syllabus web community, for his helpful guidance and for answering our questions. We are also grateful to him and the entire web community for their hard work in putting together the corpus to create a single source of information on piano difficulty. This effort is a strong base for our research, as we aim to use this resource to learn more and do deeper analysis. We hope our work benefits the Piano Syllabus and the music education community by aiding in the labeling of pieces and enabling the exploration of the forgotten cultural heritage.</p>\n</div>\n<div id=\"Sx1.p2\" class=\"ltx_para\">\n<p id=\"Sx1.p2.1\" class=\"ltx_p\">Pedro would also like to thank Nazif C. Tamer for his insistence on shifting focus to audio, and Pablo Alonso and Oguz Araz for their insightful discussions about audio features.</p>\n</div>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[1]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Mellizo, \u201cMusic education, curriculum design, and assessment: Imagining a more equitable approach,\u201d <em id=\"bib.bib1.1.1\" class=\"ltx_emph ltx_font_italic\">Music Educators Journal</em>, vol. 106, no.\u00a04, pp. 57\u201365, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[2]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0R. Jorgensen, \u201cThe curriculum design process in music,\u201d <em id=\"bib.bib2.1.1\" class=\"ltx_emph ltx_font_italic\">College Music Symposium</em>, vol.\u00a028, pp. 94\u2013105, 1988. [Online]. Available: <a target=\"_blank\" href=\"http://www.jstor.org/stable/40374590\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">http://www.jstor.org/stable/40374590</a>\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[3]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0S. Deconto, E.\u00a0L.\u00a0F. Valenga, and C.\u00a0N. Silla, \u201cAutomatic music score difficulty classification,\u201d in <em id=\"bib.bib3.1.1\" class=\"ltx_emph ltx_font_italic\">2023 30th International Conference on Systems, Signals and Image Processing (IWSSIP)</em>.\u00a0\u00a0\u00a0IEEE, 2023, pp. 1\u20135.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[4]</span>\n<span class=\"ltx_bibblock\">\nV.\u00a0S\u00e9bastien, H.\u00a0Ralambondrainy, O.\u00a0S\u00e9bastien, and N.\u00a0Conruyt, \u201cScore analyzer: Automatically determining scores difficulty level for instrumental e-learning,\u201d in <em id=\"bib.bib4.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of 13th International Society for Music Information Retrieval Conference, ISMIR</em>, Porto, Portugal, 2012.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[5]</span>\n<span class=\"ltx_bibblock\">\nS.-C. Chiu and M.-S. Chen, \u201cA study on difficulty level recognition of piano sheet music,\u201d in <em id=\"bib.bib5.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Symposium on Multimedia</em>.\u00a0\u00a0\u00a0IEEE, 2012, pp. 17\u201323.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[6]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura and K.\u00a0Yoshii, \u201cStatistical piano reduction controlling performance difficulty,\u201d <em id=\"bib.bib6.1.1\" class=\"ltx_emph ltx_font_italic\">APSIPA Transactions on Signal and Information Processing</em>, vol.\u00a07, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[7]</span>\n<span class=\"ltx_bibblock\">\n\u201cMusescore have automatic difficulty categories from year 2022,\u201d <a target=\"_blank\" href=\"https://musescore.com/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://musescore.com/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[8]</span>\n<span class=\"ltx_bibblock\">\n\u201cUltimate guitar have automatic difficulty categories from year 2022,\u201d <a target=\"_blank\" href=\"https://www.ultimate-guitar.com/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://www.ultimate-guitar.com/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[9]</span>\n<span class=\"ltx_bibblock\">\n\u201cSystem for estimating user\u2019s skill in playing a music instrument and determining virtual exercises thereof,\u201d Patent US9\u2009767\u2009705B1, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[10]</span>\n<span class=\"ltx_bibblock\">\nQ.\u00a0Kong, B.\u00a0Li, X.\u00a0Song, Y.\u00a0Wan, and Y.\u00a0Wang, \u201cHigh-resolution piano transcription with pedals by regressing onset and offset times,\u201d vol.\u00a029, p. 3707\u20133717, oct 2021.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[11]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura, N.\u00a0Ono, and S.\u00a0Sagayama, \u201cMerged-output hmm for piano fingering of both hands.\u201d in <em id=\"bib.bib11.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR</em>, Taipei, Taiwan, 2014, pp. 531\u2013536.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[12]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Nakamura and S.\u00a0Sagayama, \u201cAutomatic piano reduction from ensemble scores based on merged-output hidden markov model,\u201d in <em id=\"bib.bib12.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 41st International Computer Music Conference, ICMC</em>, Denton, USA, 2015.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[13]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, N.\u00a0C. Tamer, V.\u00a0Eremenko, M.\u00a0Miron, and X.\u00a0Serra, \u201cScore difficulty analysis for piano performance education,\u201d in <em id=\"bib.bib13.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP</em>, Singapore, Singapore, 2022, pp. 201\u2013205.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[14]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, D.\u00a0Jeong, V.\u00a0Eremenko, N.\u00a0C. Tamer, M.\u00a0Miron, and X.\u00a0Serra, \u201cCombining piano performance dimensions for score difficulty classification,\u201d <em id=\"bib.bib14.1.1\" class=\"ltx_emph ltx_font_italic\">Expert Systems with Applications</em>, vol. 238, p. 121776, 2024.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[15]</span>\n<span class=\"ltx_bibblock\">\nH.\u00a0Zhang, E.\u00a0Karystinaios, S.\u00a0Dixon, G.\u00a0Widmer, and C.\u00a0E. Cancino-Chac\u00f3n, \u201cSymbolic music representations for classification tasks: A systematic evaluation,\u201d <em id=\"bib.bib15.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2309.02567</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[16]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, D.\u00a0Jeong, E.\u00a0Nakamura, X.\u00a0Serra, and M.\u00a0Miron, \u201cAutomatic piano fingering from partially annotated scores using autoregressive neural networks,\u201d in <em id=\"bib.bib16.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 30th ACM International Conference on Multimedia</em>, 2022, pp. 6502\u20136510.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[17]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0Jeong, T.\u00a0Kwon, Y.\u00a0Kim, K.\u00a0Lee, and J.\u00a0Nam, \u201cVirtuosoNet: A hierarchical RNN-based system for modeling expressive piano performance,\u201d in <em id=\"bib.bib17.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR</em>, 2019, pp. 908\u2013915.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[18]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Cheng, Z.\u00a0Wang, and G.\u00a0Pollastri, \u201cA neural network approach to ordinal regression,\u201d in <em id=\"bib.bib18.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE International Joint Conference on Neural Networks, IJCNN</em>.\u00a0\u00a0\u00a0Hong Kong, China: IEEE, 2008, pp. 1279\u20131284.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[19]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0A.\u00a0V. V\u00e1squez, M.\u00a0Baelemans, J.\u00a0Driedger, W.\u00a0Zuidema, and J.\u00a0A. Burgoyne, \u201cQuantifying the ease of playing song chords on the guitar,\u201d in <em id=\"bib.bib19.1.1\" class=\"ltx_emph ltx_font_italic\">Ismir 2023 Hybrid Conference</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[20]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Toyama, T.\u00a0Akama, Y.\u00a0Ikemiya, Y.\u00a0Takida, W.-H. Liao, and Y.\u00a0Mitsufuji, \u201cAutomatic piano transcription with hierarchical frequency-time transformer,\u201d in <em id=\"bib.bib20.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR</em>, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[21]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Rege and R.\u00a0Sindal, \u201cReview of f0 estimation in the context of indian classical music expression detection,\u201d in <em id=\"bib.bib21.1.1\" class=\"ltx_emph ltx_font_italic\">Social Networking and Computational Intelligence: Proceedings of SCI-2018</em>.\u00a0\u00a0\u00a0Springer, 2020, pp. 257\u2013268.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[22]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Alonso-Jim\u00e9nez, X.\u00a0Serra, and D.\u00a0Bogdanov, \u201cMusic representation learning based on editorial metadata from discogs,\u201d in <em id=\"bib.bib22.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[23]</span>\n<span class=\"ltx_bibblock\">\nC.\u00a0Sch\u00f6rkhuber and A.\u00a0Klapuri, \u201cConstant-q transform toolbox for music processing,\u201d in <em id=\"bib.bib23.1.1\" class=\"ltx_emph ltx_font_italic\">7th sound and music computing conference, Barcelona, Spain</em>, 2010, pp. 3\u201364.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[24]</span>\n<span class=\"ltx_bibblock\">\nQ.\u00a0Kong, B.\u00a0Li, J.\u00a0Chen, and Y.\u00a0Wang, \u201cGiantmidi-piano: A large-scale MIDI dataset for classical piano music,\u201d <em id=\"bib.bib24.1.1\" class=\"ltx_emph ltx_font_italic\">Transactions of the International Society for Music Information Retrieval, 5(1), pp.87\u201398</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[25]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda, J.\u00a0J. Valero-Mas, D.\u00a0Jeong, and X.\u00a0Serra, \u201cPredicting performance difficulty from piano sheet music images,\u201d in <em id=\"bib.bib25.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. of the 24th Int. Society for Music Information Retrieval Conf.</em>, Milan, Italy, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[26]</span>\n<span class=\"ltx_bibblock\">\nUniversity of Colorado, \u201cHidden voices project,\u201d <a target=\"_blank\" href=\"https://www.colorado.edu/project/hidden-voices/\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://www.colorado.edu/project/hidden-voices/</a>, accessed on April 11, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[27]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0F. Gemmeke, D.\u00a0P. Ellis, D.\u00a0Freedman, A.\u00a0Jansen, W.\u00a0Lawrence, R.\u00a0C. Moore, M.\u00a0Plakal, and M.\u00a0Ritter, \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in <em id=\"bib.bib27.1.1\" class=\"ltx_emph ltx_font_italic\">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.\u00a0\u00a0\u00a0IEEE, 2017, pp. 776\u2013780.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[28]</span>\n<span class=\"ltx_bibblock\">\nV.\u00a0Lakic, L.\u00a0Rossetto, and A.\u00a0Bernstein, \u201cLink-rot in web-sourced multimedia datasets,\u201d in <em id=\"bib.bib28.1.1\" class=\"ltx_emph ltx_font_italic\">MultiMedia Modeling</em>.\u00a0\u00a0\u00a0Cham: Springer International Publishing, 2023, pp. 476\u2013488.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[29]</span>\n<span class=\"ltx_bibblock\">\nN.\u00a0Kato, E.\u00a0Nakamura, K.\u00a0Mine, O.\u00a0Doeda, and M.\u00a0Yamada, \u201cComputational analysis of audio recordings of piano performance for automatic evaluation,\u201d in <em id=\"bib.bib29.1.1\" class=\"ltx_emph ltx_font_italic\">Responsive and Sustainable Educational Futures</em>.\u00a0\u00a0\u00a0Cham: Springer Nature Switzerland, 2023, pp. 586\u2013592.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[30]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0O\u2019Hanlon and M.\u00a0B. Sandler, \u201cComparing cqt and reassignment based chroma features for template-based automatic chord recognition,\u201d in <em id=\"bib.bib30.1.1\" class=\"ltx_emph ltx_font_italic\">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP</em>.\u00a0\u00a0\u00a0IEEE, 2019, pp. 860\u2013864.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[31]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Ramoneda and G.\u00a0Bernardes, \u201cRevisiting harmonic change detection,\u201d in <em id=\"bib.bib31.1.1\" class=\"ltx_emph ltx_font_italic\">Audio Engineering Society Convention 149</em>, Oct 2020.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[32]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Wang, A.\u00a0Singh, J.\u00a0Michael, F.\u00a0Hill, O.\u00a0Levy, and S.\u00a0R. Bowman, \u201cGlue: A multi-task benchmark and analysis platform for natural language understanding,\u201d in <em id=\"bib.bib32.1.1\" class=\"ltx_emph ltx_font_italic\">7th International Conference on Learning Representations, ICLR</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[33]</span>\n<span class=\"ltx_bibblock\">\nI.\u00a0Laina, C.\u00a0Rupprecht, V.\u00a0Belagiannis, F.\u00a0Tombari, and N.\u00a0Navab, \u201cDeeper depth prediction with fully convolutional residual networks,\u201d in <em id=\"bib.bib33.1.1\" class=\"ltx_emph ltx_font_italic\">2016 Fourth international conference on 3D vision (3DV)</em>.\u00a0\u00a0\u00a0IEEE, 2016, pp. 239\u2013248.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[34]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Wu, C.\u00a0Shen, and A.\u00a0Van Den\u00a0Hengel, \u201cWider or deeper: Revisiting the resnet model for visual recognition,\u201d <em id=\"bib.bib34.1.1\" class=\"ltx_emph ltx_font_italic\">Pattern Recognition</em>, vol.\u00a090, pp. 119\u2013133, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[35]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Won, K.\u00a0Choi, and X.\u00a0Serra, \u201cSemi-supervised music tagging transformer,\u201d in <em id=\"bib.bib35.1.1\" class=\"ltx_emph ltx_font_italic\">International Society for Music Information Retrieval Conference</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[36]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Allen-Zhu and Y.\u00a0Li, \u201cWhat can resnet learn efficiently, going beyond kernels?\u201d <em id=\"bib.bib36.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, vol.\u00a032, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[37]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Cho, B.\u00a0van Merrienboer, \u00c7.\u00a0G\u00fcl\u00e7ehre, D.\u00a0Bahdanau, F.\u00a0Bougares, H.\u00a0Schwenk, and Y.\u00a0Bengio, \u201cLearning phrase representations using RNN encoder-decoder for statistical machine translation,\u201d in <em id=\"bib.bib37.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL</em>, A.\u00a0Moschitti, B.\u00a0Pang, and W.\u00a0Daelemans, Eds.\u00a0\u00a0\u00a0ACL, 2014, pp. 1724\u20131734. [Online]. Available: <a target=\"_blank\" href=\"https://doi.org/10.3115/v1/d14-1179\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://doi.org/10.3115/v1/d14-1179</a>\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[38]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Chung, \u00c7.\u00a0G\u00fcl\u00e7ehre, K.\u00a0Cho, and Y.\u00a0Bengio, \u201cEmpirical evaluation of gated recurrent neural networks on sequence modeling,\u201d <em id=\"bib.bib38.1.1\" class=\"ltx_emph ltx_font_italic\">CoRR</em>, vol. abs/1412.3555, 2014. [Online]. Available: <a target=\"_blank\" href=\"http://arxiv.org/abs/1412.3555\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">http://arxiv.org/abs/1412.3555</a>\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[39]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Yang, D.\u00a0Yang, C.\u00a0Dyer, X.\u00a0He, A.\u00a0Smola, and E.\u00a0Hovy, \u201cHierarchical attention networks for document classification,\u201d in <em id=\"bib.bib39.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies</em>, 2016, pp. 1480\u20131489.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[40]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Maghoumi and J.\u00a0J. LaViola, \u201cDeepgru: Deep gesture recognition utility,\u201d in <em id=\"bib.bib40.1.1\" class=\"ltx_emph ltx_font_italic\">International Symposium on Visual Computing</em>.\u00a0\u00a0\u00a0Springer, 2019, pp. 16\u201331.\n\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[41]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0Alfaro-Contreras, J.\u00a0J. Valero-Mas, J.\u00a0M. I\u00f1esta, and J.\u00a0Calvo-Zaragoza, \u201cLate multimodal fusion for image and audio music transcription,\u201d <em id=\"bib.bib41.1.1\" class=\"ltx_emph ltx_font_italic\">Expert Systems with Applications</em>, vol. 216, p. 119491, 2023.\n\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[42]</span>\n<span class=\"ltx_bibblock\">\nL.\u00a0Gaudette and N.\u00a0Japkowicz, \u201cEvaluation methods for ordinal classification,\u201d in <em id=\"bib.bib42.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 22nd Canadian Conference on Advances in Artificial Intelligence</em>, Kelowna, Canada, 2009, pp. 207\u2013210.\n\n</span>\n</li>\n</ul>\n</section>\n<figure id=\"Sx1.1\" class=\"ltx_float biography\">\n<table id=\"Sx1.1.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.1.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.1.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/pedro.png\" id=\"Sx1.1.1.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"100\" height=\"110\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.1.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.1.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.1.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Pedro Ramoneda</span> \nPedro Ramoneda holds a BSc in Computer Science from the University of Zaragoza, a Professional Degree in Piano Performance from the Conservatory of Music in Zaragoza, and an MSc in Sound and Music Computing from the Universitat Pompeu Fabra. He is currently a third-year PhD student in the Music Technology Group of the Universitat Pompeu Fabra under the supervision of Prof. Xavier Serra, focusing on the use of technologies from the Music Information Retrieval and Signal Processing field for supporting music education.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.2\" class=\"ltx_float biography\">\n<table id=\"Sx1.2.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.2.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.2.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/Minhee_Lee.jpeg\" id=\"Sx1.2.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"94\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.2.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.2.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.2.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Minhee Lee</span> \nMinhee Lee is an undergraduate student pursuing a B.S. degree in the Department of Computer Science and Engineering at Sogang University in South Korea. She is an undergraduate intern in the MALer Lab under the supervision of Prof. Dasaem Jeong since 2023. Before joining the research group, she did internships as a software engineer at Google in 2021 and 2022, and at FuriosaAI in 2022. Her research interest is about various music information retrieval tasks of understanding music with deep learning technologies.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.3\" class=\"ltx_float biography\">\n<table id=\"Sx1.3.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.3.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.3.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/Dasaem_Jeong.jpg\" id=\"Sx1.3.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"100\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.3.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.3.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.3.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.3.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Dasaem Jeong</span>  is currently working as an Assistant Professor in the Department of Art &amp; Technology at Sogang University in South Korea since 2021. Before joining Sogang University, he worked as a research scientist in T-Brain X, SK Telecom from 2020 to 2021. He obtained his Ph.D. and M.S. degrees in culture technology, and B.S. in mechanical engineering from Korea Advanced Institute of Science and Technology (KAIST). His research primarily focuses on a diverse range of music information retrieval tasks, including music generation and computational musicology.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.4\" class=\"ltx_float biography\">\n<table id=\"Sx1.4.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.4.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.4.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/JJVM.png\" id=\"Sx1.4.1.1.1.g1\" class=\"ltx_graphics ltx_img_portrait\" width=\"85\" height=\"125\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.4.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.4.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.4.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.4.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Jose J. Valero-Mas</span>  received the M.Sc. degree in Telecommunications Engineering from the University Miguel Hern\u00e1ndez of Elche in 2012, the M.Sc. degree in Sound and Music Computing from the Universitat Pompeu Fabra in 2013, and the Ph.D. degree in Computer Science from the University of Alicante in 2017. After a three-year period in industry in which he developed as a data scientist, he acted as a Postdoctoral Researcher from 2020 to 2023 in the University of Alicante, the Universitat Pompeu Fabra, and the Queen Mary University of London. He is currently an Assistant Professor in the Department of Software and Computing Systems of the University of Alicante. His research interests include Pattern Recognition, Machine Learning, Music Information Retrieval, and Signal Processing for which he has co-authored more than 40 works within international journals, conference communications, and book chapters.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n<figure id=\"Sx1.5\" class=\"ltx_float biography\">\n<table id=\"Sx1.5.1\" class=\"ltx_tabular\">\n<tr id=\"Sx1.5.1.1\" class=\"ltx_tr\">\n<td id=\"Sx1.5.1.1.1\" class=\"ltx_td\"><img src=\"/html/2403.03947/assets/authors/serra.png\" id=\"Sx1.5.1.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"100\" height=\"110\" alt=\"[Uncaptioned image]\"></td>\n<td id=\"Sx1.5.1.1.2\" class=\"ltx_td\">\n<span id=\"Sx1.5.1.1.2.1\" class=\"ltx_inline-block\">\n<span id=\"Sx1.5.1.1.2.1.1\" class=\"ltx_p\"><span id=\"Sx1.5.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Xavier Serra</span> \nHe has had a progressive academic career in the field of music technology since earning his PhD from Stanford University in 1989. His interest lies in the analysis, synthesis, and description of sound and music signals. He always seeks a balance between basic and applied research while integrating both scientific/technological and humanistic/artistic disciplines. Currently, he supervises research at the Music Technology Group (MTG), focusing on understanding sound and music signals through signal processing, machine learning, and semantic technologies. His work emphasizes data-driven and knowledge-driven methodologies, involving the development of large data collections and the application of domain-specific knowledge. He leads projects funded both publicly and privately, tackling practical issues such as music exploration, sound classification, and music performance analysis for educational purposes. He champions open science by promoting open data, software, and access, and is keen on using open innovation strategies to enhance the social and economic impact of his research.</span>\n</span>\n</td>\n</tr>\n</table>\n</figure>\n</section>\n</section>\n</div>\n</div>\n</figure>\n",
        "footnotes": [],
        "references": []
    }
}