{
    "S5.T1": {
        "caption": "Table 1:  LibriSpeech validation and test sets word error rate (WER, %) for: i) Transformer ASR model (256M,  baseline ) with greedy decoding trained on LibriSpeech (960h), denoted as \u201cbaseline ASR\u201d, or on LibriSpeech (960h) and extra TTS data generated from LibriSpeech LM corpus by RichTTS, denoted as \u201cbaseline ASR (LS+TTS)\u201d (mixing proportion is 1:1); ii) DLM (484M) with DSR-greedy/DSR-decoding and a neural LM (neLM, 500M) rescoring applied on top of ASR models.\nDLM is trained on data generated by the  baseline ASR  applied to LibriSpeech (960h) and YourTTS and RichTTS synthetic data (LibriSpeech LM corpus), used in proportion real:synthetic of 1:9 in the minibatch, with frequency masking and 10% random substitutions of characters.",
        "table": "<figure id=\"S5.T1\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>LibriSpeech validation and test sets word error rate (WER, %) for: i) Transformer ASR model (256M, <span id=\"S5.T1.3.1\" class=\"ltx_text ltx_font_italic\">baseline</span>) with greedy decoding trained on LibriSpeech (960h), denoted as \u201cbaseline ASR\u201d, or on LibriSpeech (960h) and extra TTS data generated from LibriSpeech LM corpus by RichTTS, denoted as \u201cbaseline ASR (LS+TTS)\u201d (mixing proportion is 1:1); ii) DLM (484M) with DSR-greedy/DSR-decoding and a neural LM (neLM, 500M) rescoring applied on top of ASR models.\nDLM is trained on data generated by the <span id=\"S5.T1.4.2\" class=\"ltx_text ltx_font_italic\">baseline ASR</span> applied to LibriSpeech (960h) and YourTTS and RichTTS synthetic data (LibriSpeech LM corpus), used in proportion real:synthetic of 1:9 in the minibatch, with frequency masking and 10% random substitutions of characters.</figcaption>\n<div id=\"S5.T1.5\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:346.9pt;height:156.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-6.0pt,2.7pt) scale(0.966323869484448,0.966323869484448) ;\">\n<table id=\"S5.T1.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T1.5.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Model</td>\n<td id=\"S5.T1.5.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">dev-clean</td>\n<td id=\"S5.T1.5.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">dev-other</td>\n<td id=\"S5.T1.5.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">test-clean</td>\n<td id=\"S5.T1.5.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">test-other</td>\n</tr>\n<tr id=\"S5.T1.5.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T1.5.1.2.1.1\" class=\"ltx_text ltx_font_italic\">baseline ASR</span></td>\n<td id=\"S5.T1.5.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">2.1</td>\n<td id=\"S5.T1.5.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">5.5</td>\n<td id=\"S5.T1.5.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">2.2</td>\n<td id=\"S5.T1.5.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">5.3</td>\n</tr>\n<tr id=\"S5.T1.5.1.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.1.3.1\" class=\"ltx_td ltx_align_left\">+ neLM (LM-rescoring)</td>\n<td id=\"S5.T1.5.1.3.2\" class=\"ltx_td ltx_align_center\">1.5</td>\n<td id=\"S5.T1.5.1.3.3\" class=\"ltx_td ltx_align_center\">4.0</td>\n<td id=\"S5.T1.5.1.3.4\" class=\"ltx_td ltx_align_center\">2.0</td>\n<td id=\"S5.T1.5.1.3.5\" class=\"ltx_td ltx_align_center\">4.1</td>\n</tr>\n<tr id=\"S5.T1.5.1.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.1.4.1\" class=\"ltx_td ltx_align_left\">+ DLM (Greedy-decoding)</td>\n<td id=\"S5.T1.5.1.4.2\" class=\"ltx_td ltx_align_center\">1.9</td>\n<td id=\"S5.T1.5.1.4.3\" class=\"ltx_td ltx_align_center\">3.9</td>\n<td id=\"S5.T1.5.1.4.4\" class=\"ltx_td ltx_align_center\">2.0</td>\n<td id=\"S5.T1.5.1.4.5\" class=\"ltx_td ltx_align_center\">4.1</td>\n</tr>\n<tr id=\"S5.T1.5.1.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.1.5.1\" class=\"ltx_td ltx_align_left\">+ DLM (DSR-decoding)</td>\n<td id=\"S5.T1.5.1.5.2\" class=\"ltx_td ltx_align_center\">1.5</td>\n<td id=\"S5.T1.5.1.5.3\" class=\"ltx_td ltx_align_center\">3.4</td>\n<td id=\"S5.T1.5.1.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.5.1.5.4.1\" class=\"ltx_text ltx_font_bold\">1.6</span></td>\n<td id=\"S5.T1.5.1.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.5.1.5.5.1\" class=\"ltx_text ltx_font_bold\">3.6</span></td>\n</tr>\n<tr id=\"S5.T1.5.1.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.1.6.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T1.5.1.6.1.1\" class=\"ltx_text ltx_font_italic\">baseline ASR (LS+TTS)</span></td>\n<td id=\"S5.T1.5.1.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\">1.9</td>\n<td id=\"S5.T1.5.1.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\">4.7</td>\n<td id=\"S5.T1.5.1.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\">2.0</td>\n<td id=\"S5.T1.5.1.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\">4.7</td>\n</tr>\n<tr id=\"S5.T1.5.1.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.1.7.1\" class=\"ltx_td ltx_align_left\">+ neLM (LM-rescoring)</td>\n<td id=\"S5.T1.5.1.7.2\" class=\"ltx_td ltx_align_center\">1.3</td>\n<td id=\"S5.T1.5.1.7.3\" class=\"ltx_td ltx_align_center\">3.4</td>\n<td id=\"S5.T1.5.1.7.4\" class=\"ltx_td ltx_align_center\">1.8</td>\n<td id=\"S5.T1.5.1.7.5\" class=\"ltx_td ltx_align_center\">3.7</td>\n</tr>\n<tr id=\"S5.T1.5.1.8\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.1.8.1\" class=\"ltx_td ltx_align_left\">+ DLM (Greedy-decoding)</td>\n<td id=\"S5.T1.5.1.8.2\" class=\"ltx_td ltx_align_center\">1.8</td>\n<td id=\"S5.T1.5.1.8.3\" class=\"ltx_td ltx_align_center\">3.6</td>\n<td id=\"S5.T1.5.1.8.4\" class=\"ltx_td ltx_align_center\">1.9</td>\n<td id=\"S5.T1.5.1.8.5\" class=\"ltx_td ltx_align_center\">3.9</td>\n</tr>\n<tr id=\"S5.T1.5.1.9\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.1.9.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">+ DLM (DSR-decoding)</td>\n<td id=\"S5.T1.5.1.9.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">1.4</td>\n<td id=\"S5.T1.5.1.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">3.1</td>\n<td id=\"S5.T1.5.1.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.5.1.9.4.1\" class=\"ltx_text ltx_font_bold\">1.5</span></td>\n<td id=\"S5.T1.5.1.9.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.5.1.9.5.1\" class=\"ltx_text ltx_font_bold\">3.3</span></td>\n</tr>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Our main results are in Table 1.\nWe consider two ASR systems: one is the baseline ASR (Transformer CTC-based model with 255M parameters) trained on LibriSpeech; the other is the baseline ASR (LS+TTS) which is the same as baseline ASR but trained on LibriSpeech together with TTS audio data generated by RichTTS from the LibriSpeech LM corpus. The mixing ratio of LibriSpeech audio and TTS audio in a minibatch is 1:1, which leads to improved performance compared to baseline ASR. We show the other mixtures of LibriSpeech audio and TTS audio in a minibatch for baseline ASR (LS+TTS) in Appendix D.2 Table 8.",
            "In Table 1, DLMs are trained on the mix of LibriSpeech audio and audio generated by YourTTS and RichTTS (total corpus is 2x) from LibriSpeech LM corpus. In both cases, DLM with greedy-decoding is comparable to neural LM with more expensive beam search and rescoring.\nDLM with DSR-decoding significantly outperforms a similar size neural LM with rescoring. Note that DSR-decoding does not necessarily increase the computational cost comparing with standard rescoring.\nIn Table 2, we show error analyses of hypotheses obtained from ASR, neural LM and DLM. In these cases, neural LM fails to choose the correct answer from the beam of ASR but DLM is able to correctly identify the error and generate the correct transcription."
        ]
    },
    "S5.T2": {
        "caption": "Table 2:  Examples of ASR hypotheses, neural LM hypotheses and DLM corrections.",
        "table": "<figure id=\"S5.T2\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Examples of ASR hypotheses, neural LM hypotheses and DLM corrections.</figcaption>\n<div id=\"S5.T2.1\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:56pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-131.7pt,17.0pt) scale(0.6220727726804,0.6220727726804) ;\">\n<table id=\"S5.T2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T2.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">ASR</td>\n<td id=\"S5.T2.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_tt\">Neural LM</td>\n<td id=\"S5.T2.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_border_tt\">DLM</td>\n<td id=\"S5.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">Error Type</td>\n</tr>\n<tr id=\"S5.T2.1.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T2.1.1.2.1.1\" class=\"ltx_text ltx_font_sansserif\">but he\u2019d <span id=\"S5.T2.1.1.2.1.1.1\" class=\"ltx_text\" style=\"color:#FF0000;\">bi\u2019n</span> too sick and too long <span id=\"S5.T2.1.1.2.1.1.2\" class=\"ltx_text\" style=\"color:#FF0000;\">a bed</span></span></td>\n<td id=\"S5.T2.1.1.2.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T2.1.1.2.2.1\" class=\"ltx_text ltx_font_sansserif\">but he\u2019d <span id=\"S5.T2.1.1.2.2.1.1\" class=\"ltx_text\" style=\"color:#0000FF;\"> been</span> too sick and too long <span id=\"S5.T2.1.1.2.2.1.2\" class=\"ltx_text\" style=\"color:#FF0000;\">a bed</span></span></td>\n<td id=\"S5.T2.1.1.2.3\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T2.1.1.2.3.1\" class=\"ltx_text ltx_font_sansserif\">but he\u2019d <span id=\"S5.T2.1.1.2.3.1.1\" class=\"ltx_text\" style=\"color:#0000FF;\"> been</span> too sick and too long <span id=\"S5.T2.1.1.2.3.1.2\" class=\"ltx_text\" style=\"color:#0000FF;\"> abed</span></span></td>\n<td id=\"S5.T2.1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">word boundary error</td>\n</tr>\n<tr id=\"S5.T2.1.1.3\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.3.1\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.1.1.3.1.1\" class=\"ltx_text ltx_font_sansserif\">humanity was <span id=\"S5.T2.1.1.3.1.1.1\" class=\"ltx_text\" style=\"color:#FF0000;\">torred</span> in her country</span></td>\n<td id=\"S5.T2.1.1.3.2\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.1.1.3.2.1\" class=\"ltx_text ltx_font_sansserif\">humanity was <span id=\"S5.T2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"color:#FF0000;\">stored</span> in our country</span></td>\n<td id=\"S5.T2.1.1.3.3\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.1.1.3.3.1\" class=\"ltx_text ltx_font_sansserif\">humanity was <span id=\"S5.T2.1.1.3.3.1.1\" class=\"ltx_text\" style=\"color:#0000FF;\">stirred</span> in our country</span></td>\n<td id=\"S5.T2.1.1.3.4\" class=\"ltx_td ltx_align_center\">spelling mistake</td>\n</tr>\n<tr id=\"S5.T2.1.1.4\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.4.1\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.1.1.4.1.1\" class=\"ltx_text ltx_font_sansserif\" style=\"color:#FF0000;\">media<span id=\"S5.T2.1.1.4.1.1.1\" class=\"ltx_text\" style=\"color:#000000;\"> too</span></span></td>\n<td id=\"S5.T2.1.1.4.2\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.1.1.4.2.1\" class=\"ltx_text ltx_font_sansserif\" style=\"color:#FF0000;\">media<span id=\"S5.T2.1.1.4.2.1.1\" class=\"ltx_text\" style=\"color:#000000;\"> too</span></span></td>\n<td id=\"S5.T2.1.1.4.3\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.1.1.4.3.1\" class=\"ltx_text ltx_font_sansserif\" style=\"color:#0000FF;\">medea<span id=\"S5.T2.1.1.4.3.1.1\" class=\"ltx_text\" style=\"color:#000000;\"> too</span></span></td>\n<td id=\"S5.T2.1.1.4.4\" class=\"ltx_td ltx_align_center\">wrong word</td>\n</tr>\n<tr id=\"S5.T2.1.1.5\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.5.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S5.T2.1.1.5.1.1\" class=\"ltx_text ltx_font_sansserif\">the fact is the castle <span id=\"S5.T2.1.1.5.1.1.1\" class=\"ltx_text\" style=\"color:#FF0000;\"> was</span> much later than</span></td>\n<td id=\"S5.T2.1.1.5.2\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S5.T2.1.1.5.2.1\" class=\"ltx_text ltx_font_sansserif\">the fact is the castle <span id=\"S5.T2.1.1.5.2.1.1\" class=\"ltx_text\" style=\"color:#FF0000;\"> was</span> much later than</span></td>\n<td id=\"S5.T2.1.1.5.3\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S5.T2.1.1.5.3.1\" class=\"ltx_text ltx_font_sansserif\">the fact is the castle <span id=\"S5.T2.1.1.5.3.1.1\" class=\"ltx_text\" style=\"color:#0000FF;\"> is</span> much later than</span></td>\n<td id=\"S5.T2.1.1.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">grammatical error</td>\n</tr>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "In Table 1, DLMs are trained on the mix of LibriSpeech audio and audio generated by YourTTS and RichTTS (total corpus is 2x) from LibriSpeech LM corpus. In both cases, DLM with greedy-decoding is comparable to neural LM with more expensive beam search and rescoring.\nDLM with DSR-decoding significantly outperforms a similar size neural LM with rescoring. Note that DSR-decoding does not necessarily increase the computational cost comparing with standard rescoring.\nIn Table 2, we show error analyses of hypotheses obtained from ASR, neural LM and DLM. In these cases, neural LM fails to choose the correct answer from the beam of ASR but DLM is able to correctly identify the error and generate the correct transcription."
        ]
    },
    "S5.T3": {
        "caption": "Table 3:  LibriSpeech test sets WER (%) for DLM and prior works that either do not use external audio data, or use self-supervised pretraining on Libri-Light\u00a0 [ 19 ]  audio (LL-60k).",
        "table": "<figure id=\"S5.T3\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>LibriSpeech test sets WER (%) for DLM and prior works that either do not use external audio data, or use self-supervised pretraining on Libri-Light\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">19</a>]</cite> audio (LL-60k).</figcaption>\n<div id=\"S5.T3.1\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:390.3pt;height:259.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-17.6pt,11.7pt) scale(0.917347106000043,0.917347106000043) ;\">\n<table id=\"S5.T3.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T3.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Prior work</td>\n<td id=\"S5.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S5.T3.1.1.1.2.1\" class=\"ltx_text\"></span> <span id=\"S5.T3.1.1.1.2.2\" class=\"ltx_text\">\n<span id=\"S5.T3.1.1.1.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.1.1.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.1.1.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">External Data</span></span>\n</span></span><span id=\"S5.T3.1.1.1.2.3\" class=\"ltx_text\"></span></td>\n<td id=\"S5.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">test-clean</td>\n<td id=\"S5.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">test-other</td>\n</tr>\n<tr id=\"S5.T3.1.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S5.T3.1.1.2.1.1\" class=\"ltx_text\"></span><span id=\"S5.T3.1.1.2.1.2\" class=\"ltx_text\">\n<span id=\"S5.T3.1.1.2.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.1.2.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.1.2.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Transformer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">42</a>]</cite></span></span>\n</span></span><span id=\"S5.T3.1.1.2.1.3\" class=\"ltx_text\"></span></td>\n<td id=\"S5.T3.1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td id=\"S5.T3.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">2.3</td>\n<td id=\"S5.T3.1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">5.2</td>\n</tr>\n<tr id=\"S5.T3.1.1.3\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.3.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S5.T3.1.1.3.1.1\" class=\"ltx_text\"></span><span id=\"S5.T3.1.1.3.1.2\" class=\"ltx_text\">\n<span id=\"S5.T3.1.1.3.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.1.3.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.1.3.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Context-Net(L) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite></span></span>\n</span></span><span id=\"S5.T3.1.1.3.1.3\" class=\"ltx_text\"></span></td>\n<td id=\"S5.T3.1.1.3.2\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S5.T3.1.1.3.3\" class=\"ltx_td ltx_align_center\">1.9</td>\n<td id=\"S5.T3.1.1.3.4\" class=\"ltx_td ltx_align_center\">4.1</td>\n</tr>\n<tr id=\"S5.T3.1.1.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.4.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S5.T3.1.1.4.1.1\" class=\"ltx_text\"></span><span id=\"S5.T3.1.1.4.1.2\" class=\"ltx_text\">\n<span id=\"S5.T3.1.1.4.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.1.4.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.1.4.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Conformer (Transducer) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">9</a>]</cite></span></span>\n</span></span><span id=\"S5.T3.1.1.4.1.3\" class=\"ltx_text\"></span></td>\n<td id=\"S5.T3.1.1.4.2\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S5.T3.1.1.4.3\" class=\"ltx_td ltx_align_center\">1.9</td>\n<td id=\"S5.T3.1.1.4.4\" class=\"ltx_td ltx_align_center\">3.9</td>\n</tr>\n<tr id=\"S5.T3.1.1.5\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.5.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S5.T3.1.1.5.1.1\" class=\"ltx_text\"></span><span id=\"S5.T3.1.1.5.1.2\" class=\"ltx_text\">\n<span id=\"S5.T3.1.1.5.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.1.5.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.1.5.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">ASAPP-ASR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">33</a>]</cite></span></span>\n</span></span><span id=\"S5.T3.1.1.5.1.3\" class=\"ltx_text\"></span></td>\n<td id=\"S5.T3.1.1.5.2\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S5.T3.1.1.5.3\" class=\"ltx_td ltx_align_center\">1.8</td>\n<td id=\"S5.T3.1.1.5.4\" class=\"ltx_td ltx_align_center\">4.5</td>\n</tr>\n<tr id=\"S5.T3.1.1.6\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.6.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S5.T3.1.1.6.1.1\" class=\"ltx_text\"></span><span id=\"S5.T3.1.1.6.1.2\" class=\"ltx_text\">\n<span id=\"S5.T3.1.1.6.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.1.6.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.1.6.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">E-branchformer + ILME <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a>]</cite></span></span>\n</span></span><span id=\"S5.T3.1.1.6.1.3\" class=\"ltx_text\"></span></td>\n<td id=\"S5.T3.1.1.6.2\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S5.T3.1.1.6.3\" class=\"ltx_td ltx_align_center\">1.8</td>\n<td id=\"S5.T3.1.1.6.4\" class=\"ltx_td ltx_align_center\">3.7</td>\n</tr>\n<tr id=\"S5.T3.1.1.7\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.7.1\" class=\"ltx_td ltx_align_left\">SYNT++\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">17</a>]</cite>\n</td>\n<td id=\"S5.T3.1.1.7.2\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S5.T3.1.1.7.3\" class=\"ltx_td ltx_align_center\">2.4</td>\n<td id=\"S5.T3.1.1.7.4\" class=\"ltx_td ltx_align_center\">6.3</td>\n</tr>\n<tr id=\"S5.T3.1.1.8\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.8.1\" class=\"ltx_td ltx_align_left\">LAS + SC + LM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</td>\n<td id=\"S5.T3.1.1.8.2\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S5.T3.1.1.8.3\" class=\"ltx_td ltx_align_center\">4.3</td>\n<td id=\"S5.T3.1.1.8.4\" class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr id=\"S5.T3.1.1.9\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.9.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S5.T3.1.1.9.1.1\" class=\"ltx_text\"></span><span id=\"S5.T3.1.1.9.1.2\" class=\"ltx_text\">\n<span id=\"S5.T3.1.1.9.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.1.9.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.1.9.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">wav2vec 2.0-Large <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">3</a>]</cite></span></span>\n</span></span><span id=\"S5.T3.1.1.9.1.3\" class=\"ltx_text\"></span></td>\n<td id=\"S5.T3.1.1.9.2\" class=\"ltx_td ltx_align_center ltx_border_t\">LL-60k</td>\n<td id=\"S5.T3.1.1.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1.8</td>\n<td id=\"S5.T3.1.1.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\">3.3</td>\n</tr>\n<tr id=\"S5.T3.1.1.10\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.10.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S5.T3.1.1.10.1.1\" class=\"ltx_text\"></span><span id=\"S5.T3.1.1.10.1.2\" class=\"ltx_text\">\n<span id=\"S5.T3.1.1.10.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.1.10.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.1.10.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">HUBERT-Large <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">16</a>]</cite></span></span>\n</span></span><span id=\"S5.T3.1.1.10.1.3\" class=\"ltx_text\"></span></td>\n<td id=\"S5.T3.1.1.10.2\" class=\"ltx_td ltx_align_center\">LL-60k</td>\n<td id=\"S5.T3.1.1.10.3\" class=\"ltx_td ltx_align_center\">1.9</td>\n<td id=\"S5.T3.1.1.10.4\" class=\"ltx_td ltx_align_center\">3.3</td>\n</tr>\n<tr id=\"S5.T3.1.1.11\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.11.1\" class=\"ltx_td ltx_align_left\">HUBERT-XL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">16</a>]</cite>\n</td>\n<td id=\"S5.T3.1.1.11.2\" class=\"ltx_td ltx_align_center\">LL-60k</td>\n<td id=\"S5.T3.1.1.11.3\" class=\"ltx_td ltx_align_center\">1.8</td>\n<td id=\"S5.T3.1.1.11.4\" class=\"ltx_td ltx_align_center\">2.9</td>\n</tr>\n<tr id=\"S5.T3.1.1.12\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.12.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S5.T3.1.1.12.1.1\" class=\"ltx_text\"></span><span id=\"S5.T3.1.1.12.1.2\" class=\"ltx_text\">\n<span id=\"S5.T3.1.1.12.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.1.1.12.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.1.1.12.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Conformer XXL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">51</a>]</cite></span></span>\n</span></span><span id=\"S5.T3.1.1.12.1.3\" class=\"ltx_text\"></span></td>\n<td id=\"S5.T3.1.1.12.2\" class=\"ltx_td ltx_align_center\">LL-60k</td>\n<td id=\"S5.T3.1.1.12.3\" class=\"ltx_td ltx_align_center\">1.5</td>\n<td id=\"S5.T3.1.1.12.4\" class=\"ltx_td ltx_align_center\">3.1</td>\n</tr>\n<tr id=\"S5.T3.1.1.13\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.13.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">\n<span id=\"S5.T3.1.1.13.1.1\" class=\"ltx_text ltx_font_italic\">baseline ASR (LS+TTS)</span> + DSR-decoding (Ours)</td>\n<td id=\"S5.T3.1.1.13.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">-</td>\n<td id=\"S5.T3.1.1.13.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T3.1.1.13.3.1\" class=\"ltx_text ltx_font_bold\">1.5</span></td>\n<td id=\"S5.T3.1.1.13.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T3.1.1.13.4.1\" class=\"ltx_text ltx_font_bold\">3.3</span></td>\n</tr>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Our best combination comes from applying DLM to baseline ASR (LS+TTS), which results in new SOTA WERs on LibriSpeech: 1.5% on test-clean and 3.3% on test-other.\nAs shown in Table 3, our vanilla Transformer CTC-based ASR with DLM outperforms prior works with more sophisticated models and training criteria that do not use external audio data, and it even matches self-supervised models of the same size trained with additional 60k hours of unlabeled audio data."
        ]
    },
    "S5.T4": {
        "caption": "Table 4:  LibriSpeech dev and test sets WER (%) of different ASR models trained on LibriSpeech (960h) with greedy decoding, with neural LM (neLM, 500M) rescoring or with DLM (484M) DSR-greedy/DSR-decoding. Both neural LM and DLM are the same as reported in Table\u00a0 1 .",
        "table": "<figure id=\"S5.T4\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>LibriSpeech dev and test sets WER (%) of different ASR models trained on LibriSpeech (960h) with greedy decoding, with neural LM (neLM, 500M) rescoring or with DLM (484M) DSR-greedy/DSR-decoding. Both neural LM and DLM are the same as reported in Table\u00a0<a href=\"#S5.T1\" title=\"Table 1 \u2023 5.1 Pushing the limits: DLM achieves new SOTA WER on LibriSpeech \u2023 5 Results \u2023 Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</figcaption>\n<div id=\"S5.T4.1\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:260.2pt;height:215.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-22.2pt,18.4pt) scale(0.853963679358939,0.853963679358939) ;\">\n<table id=\"S5.T4.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T4.1.1.1.1.1\" class=\"ltx_text\">Model</span></td>\n<td id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T4.1.1.1.2.1\" class=\"ltx_text\">Size</span></td>\n<td id=\"S5.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">dev</td>\n<td id=\"S5.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">test</td>\n</tr>\n<tr id=\"S5.T4.1.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.2.1\" class=\"ltx_td ltx_align_right\">clean</td>\n<td id=\"S5.T4.1.1.2.2\" class=\"ltx_td ltx_align_right\">other</td>\n<td id=\"S5.T4.1.1.2.3\" class=\"ltx_td ltx_align_right\">clean</td>\n<td id=\"S5.T4.1.1.2.4\" class=\"ltx_td ltx_align_right\">other</td>\n</tr>\n<tr id=\"S5.T4.1.1.3\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">QuartzNet-CTC (Ours)</td>\n<td id=\"S5.T4.1.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_t\" rowspan=\"4\"><span id=\"S5.T4.1.1.3.2.1\" class=\"ltx_text\">7M</span></td>\n<td id=\"S5.T4.1.1.3.3\" class=\"ltx_td ltx_align_right ltx_border_t\">6.4</td>\n<td id=\"S5.T4.1.1.3.4\" class=\"ltx_td ltx_align_right ltx_border_t\">17.0</td>\n<td id=\"S5.T4.1.1.3.5\" class=\"ltx_td ltx_align_right ltx_border_t\">6.5</td>\n<td id=\"S5.T4.1.1.3.6\" class=\"ltx_td ltx_align_right ltx_border_t\">16.9</td>\n</tr>\n<tr id=\"S5.T4.1.1.4\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.4.1\" class=\"ltx_td ltx_align_left\">+ neLM (LM-rescoring)</td>\n<td id=\"S5.T4.1.1.4.2\" class=\"ltx_td ltx_align_right\">2.3</td>\n<td id=\"S5.T4.1.1.4.3\" class=\"ltx_td ltx_align_right\">8.1</td>\n<td id=\"S5.T4.1.1.4.4\" class=\"ltx_td ltx_align_right\">2.9</td>\n<td id=\"S5.T4.1.1.4.5\" class=\"ltx_td ltx_align_right\">8.6</td>\n</tr>\n<tr id=\"S5.T4.1.1.5\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.5.1\" class=\"ltx_td ltx_align_left\">+ DLM (DSR-greedy)</td>\n<td id=\"S5.T4.1.1.5.2\" class=\"ltx_td ltx_align_right\">2.7</td>\n<td id=\"S5.T4.1.1.5.3\" class=\"ltx_td ltx_align_right\">8.1</td>\n<td id=\"S5.T4.1.1.5.4\" class=\"ltx_td ltx_align_right\">2.8</td>\n<td id=\"S5.T4.1.1.5.5\" class=\"ltx_td ltx_align_right\">8.3</td>\n</tr>\n<tr id=\"S5.T4.1.1.6\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.6.1\" class=\"ltx_td ltx_align_left\">+ DLM (DSR-decoding)</td>\n<td id=\"S5.T4.1.1.6.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.1.6.2.1\" class=\"ltx_text ltx_font_bold\">2.1</span></td>\n<td id=\"S5.T4.1.1.6.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.1.6.3.1\" class=\"ltx_text ltx_font_bold\">6.9</span></td>\n<td id=\"S5.T4.1.1.6.4\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.1.6.4.1\" class=\"ltx_text ltx_font_bold\">2.3</span></td>\n<td id=\"S5.T4.1.1.6.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.1.6.5.1\" class=\"ltx_text ltx_font_bold\">7.1</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.7\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.7.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Conformer-CTC (Ours)</td>\n<td id=\"S5.T4.1.1.7.2\" class=\"ltx_td ltx_align_right ltx_border_t\" rowspan=\"4\"><span id=\"S5.T4.1.1.7.2.1\" class=\"ltx_text\">102M</span></td>\n<td id=\"S5.T4.1.1.7.3\" class=\"ltx_td ltx_align_right ltx_border_t\">2.4</td>\n<td id=\"S5.T4.1.1.7.4\" class=\"ltx_td ltx_align_right ltx_border_t\">5.7</td>\n<td id=\"S5.T4.1.1.7.5\" class=\"ltx_td ltx_align_right ltx_border_t\">2.6</td>\n<td id=\"S5.T4.1.1.7.6\" class=\"ltx_td ltx_align_right ltx_border_t\">5.6</td>\n</tr>\n<tr id=\"S5.T4.1.1.8\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.8.1\" class=\"ltx_td ltx_align_left\">+ neLM (LM-rescoring)</td>\n<td id=\"S5.T4.1.1.8.2\" class=\"ltx_td ltx_align_right\">1.6</td>\n<td id=\"S5.T4.1.1.8.3\" class=\"ltx_td ltx_align_right\">4.0</td>\n<td id=\"S5.T4.1.1.8.4\" class=\"ltx_td ltx_align_right\">2.2</td>\n<td id=\"S5.T4.1.1.8.5\" class=\"ltx_td ltx_align_right\">4.2</td>\n</tr>\n<tr id=\"S5.T4.1.1.9\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.9.1\" class=\"ltx_td ltx_align_left\">+ DLM (DSR-greedy)</td>\n<td id=\"S5.T4.1.1.9.2\" class=\"ltx_td ltx_align_right\">1.9</td>\n<td id=\"S5.T4.1.1.9.3\" class=\"ltx_td ltx_align_right\">4.0</td>\n<td id=\"S5.T4.1.1.9.4\" class=\"ltx_td ltx_align_right\">2.2</td>\n<td id=\"S5.T4.1.1.9.5\" class=\"ltx_td ltx_align_right\">4.1</td>\n</tr>\n<tr id=\"S5.T4.1.1.10\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.10.1\" class=\"ltx_td ltx_align_left\">+ DLM (DSR-decoding)</td>\n<td id=\"S5.T4.1.1.10.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.1.10.2.1\" class=\"ltx_text ltx_font_bold\">1.5</span></td>\n<td id=\"S5.T4.1.1.10.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.1.10.3.1\" class=\"ltx_text ltx_font_bold\">3.4</span></td>\n<td id=\"S5.T4.1.1.10.4\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.1.10.4.1\" class=\"ltx_text ltx_font_bold\">1.7</span></td>\n<td id=\"S5.T4.1.1.10.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.1.10.5.1\" class=\"ltx_text ltx_font_bold\">3.6</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.11\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.11.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Whisper-base</td>\n<td id=\"S5.T4.1.1.11.2\" class=\"ltx_td ltx_align_right ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T4.1.1.11.2.1\" class=\"ltx_text\">74M</span></td>\n<td id=\"S5.T4.1.1.11.3\" class=\"ltx_td ltx_align_right ltx_border_tt\">4.5</td>\n<td id=\"S5.T4.1.1.11.4\" class=\"ltx_td ltx_align_right ltx_border_tt\">10.3</td>\n<td id=\"S5.T4.1.1.11.5\" class=\"ltx_td ltx_align_right ltx_border_tt\">4.5</td>\n<td id=\"S5.T4.1.1.11.6\" class=\"ltx_td ltx_align_right ltx_border_tt\">10.9</td>\n</tr>\n<tr id=\"S5.T4.1.1.12\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.12.1\" class=\"ltx_td ltx_align_left\">+ DLM (DSR-greedy)</td>\n<td id=\"S5.T4.1.1.12.2\" class=\"ltx_td ltx_align_right\">2.9</td>\n<td id=\"S5.T4.1.1.12.3\" class=\"ltx_td ltx_align_right\">7.1</td>\n<td id=\"S5.T4.1.1.12.4\" class=\"ltx_td ltx_align_right\">3.1</td>\n<td id=\"S5.T4.1.1.12.5\" class=\"ltx_td ltx_align_right\">7.9</td>\n</tr>\n<tr id=\"S5.T4.1.1.13\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.13.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Whisper-small</td>\n<td id=\"S5.T4.1.1.13.2\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" rowspan=\"2\"><span id=\"S5.T4.1.1.13.2.1\" class=\"ltx_text\">244M</span></td>\n<td id=\"S5.T4.1.1.13.3\" class=\"ltx_td ltx_align_right ltx_border_t\">3.6</td>\n<td id=\"S5.T4.1.1.13.4\" class=\"ltx_td ltx_align_right ltx_border_t\">7.1</td>\n<td id=\"S5.T4.1.1.13.5\" class=\"ltx_td ltx_align_right ltx_border_t\">3.3</td>\n<td id=\"S5.T4.1.1.13.6\" class=\"ltx_td ltx_align_right ltx_border_t\">7.6</td>\n</tr>\n<tr id=\"S5.T4.1.1.14\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.14.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">+ DLM (DSR-greedy)</td>\n<td id=\"S5.T4.1.1.14.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">2.8</td>\n<td id=\"S5.T4.1.1.14.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">5.4</td>\n<td id=\"S5.T4.1.1.14.4\" class=\"ltx_td ltx_align_right ltx_border_bb\">2.7</td>\n<td id=\"S5.T4.1.1.14.5\" class=\"ltx_td ltx_align_right ltx_border_bb\">6.1</td>\n</tr>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "A single DLM has capability of correcting errors for different ASRs and different datasets while outperforming the neural LMs.\nIn Table 4, we show results of DLM trained to correct hypotheses from the baseline ASR on out-of-domain ASRs of different sizes and architectures: DLM not only improves over the ASR models but also consistently outperforms the same size neural LM.\nAlso, the same DLM can be applied to Whisper models to further correct hypothesis generated by their default beam-search decoding.\nNote that Whisper models were trained on an entirely different and much larger proprietary data with word-piece tokens instead of characters; further Whisper is an encoder-decoder Transformer model with an architecture that is quite different from our CTC based baseline ASR."
        ]
    },
    "S5.T5": {
        "caption": "Table 5:  LibriSpeech WER (%) for  baseline ASR  with (left) DLMs and neural LMs of different size with 800M words of text; (right) DLMs trained on different data size with 484M params. DSR-greedy (DSR-decoding) is reported for DLMs and rescoring for neural LMs. (left) YourTTS synthetic data, 1:9 real:synthetic data proportion, frequency masking and 10% random substitutions of characters are used; (right) RichTTS with 10% random substitutions of characters are used.",
        "table": "<figure id=\"S5.T5\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>LibriSpeech WER (%) for <span id=\"S5.T5.2.1\" class=\"ltx_text ltx_font_italic\">baseline ASR</span> with (left) DLMs and neural LMs of different size with 800M words of text; (right) DLMs trained on different data size with 484M params. DSR-greedy (DSR-decoding) is reported for DLMs and rescoring for neural LMs. (left) YourTTS synthetic data, 1:9 real:synthetic data proportion, frequency masking and 10% random substitutions of characters are used; (right) RichTTS with 10% random substitutions of characters are used.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S5.T5.3\" class=\"ltx_inline-block ltx_figure_panel ltx_minipage ltx_align_middle ltx_transformed_outer\" style=\"width:216.8pt;height:174.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(65.9pt,-33.1pt) scale(1.61347321905915,1.61347321905915) ;\">\n<table id=\"S5.T5.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T5.3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.1.1\" class=\"ltx_td ltx_align_right ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T5.3.1.1.1.1\" class=\"ltx_text\">Size</span></td>\n<td id=\"S5.T5.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">dev-clean</td>\n<td id=\"S5.T5.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">dev-other</td>\n</tr>\n<tr id=\"S5.T5.3.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.2.1\" class=\"ltx_td ltx_align_right\">DLM</td>\n<td id=\"S5.T5.3.1.2.2\" class=\"ltx_td ltx_align_right\">neLM</td>\n<td id=\"S5.T5.3.1.2.3\" class=\"ltx_td ltx_align_right\">DLM</td>\n<td id=\"S5.T5.3.1.2.4\" class=\"ltx_td ltx_align_right\">neLM</td>\n</tr>\n<tr id=\"S5.T5.3.1.3\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.3.1\" class=\"ltx_td ltx_align_right ltx_border_t\">69M</td>\n<td id=\"S5.T5.3.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_t\">2.1 (1.7)</td>\n<td id=\"S5.T5.3.1.3.3\" class=\"ltx_td ltx_align_right ltx_border_t\">1.8</td>\n<td id=\"S5.T5.3.1.3.4\" class=\"ltx_td ltx_align_right ltx_border_t\">4.8 (4.2)</td>\n<td id=\"S5.T5.3.1.3.5\" class=\"ltx_td ltx_align_right ltx_border_t\">4.6</td>\n</tr>\n<tr id=\"S5.T5.3.1.4\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.4.1\" class=\"ltx_td ltx_align_right\">155M</td>\n<td id=\"S5.T5.3.1.4.2\" class=\"ltx_td ltx_align_right\">2.1 (1.6)</td>\n<td id=\"S5.T5.3.1.4.3\" class=\"ltx_td ltx_align_right\">1.6</td>\n<td id=\"S5.T5.3.1.4.4\" class=\"ltx_td ltx_align_right\">4.5 (4.0)</td>\n<td id=\"S5.T5.3.1.4.5\" class=\"ltx_td ltx_align_right\">4.1</td>\n</tr>\n<tr id=\"S5.T5.3.1.5\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.5.1\" class=\"ltx_td ltx_align_right\">484M</td>\n<td id=\"S5.T5.3.1.5.2\" class=\"ltx_td ltx_align_right\">2.0 (1.5)</td>\n<td id=\"S5.T5.3.1.5.3\" class=\"ltx_td ltx_align_right\">1.6</td>\n<td id=\"S5.T5.3.1.5.4\" class=\"ltx_td ltx_align_right\">4.3 (3.8)</td>\n<td id=\"S5.T5.3.1.5.5\" class=\"ltx_td ltx_align_right\">4.0</td>\n</tr>\n<tr id=\"S5.T5.3.1.6\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.1.6.1\" class=\"ltx_td ltx_align_right ltx_border_bb\">1B</td>\n<td id=\"S5.T5.3.1.6.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">2.0 (1.5)</td>\n<td id=\"S5.T5.3.1.6.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">-</td>\n<td id=\"S5.T5.3.1.6.4\" class=\"ltx_td ltx_align_right ltx_border_bb\">4.3 (3.7)</td>\n<td id=\"S5.T5.3.1.6.5\" class=\"ltx_td ltx_align_right ltx_border_bb\">-</td>\n</tr>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S5.T5.4\" class=\"ltx_inline-block ltx_figure_panel ltx_minipage ltx_align_middle ltx_transformed_outer\" style=\"width:195.1pt;height:124.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(64.3pt,-26.5pt) scale(1.73529580075925,1.73529580075925) ;\">\n<table id=\"S5.T5.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T5.4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T5.4.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\"># of words</td>\n<td id=\"S5.T5.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">dev-clean</td>\n<td id=\"S5.T5.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">dev-other</td>\n</tr>\n<tr id=\"S5.T5.4.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T5.4.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">0.5x (400M)</td>\n<td id=\"S5.T5.4.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">2.6 (1.8)</td>\n<td id=\"S5.T5.4.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">4.7 (3.8)</td>\n</tr>\n<tr id=\"S5.T5.4.1.3\" class=\"ltx_tr\">\n<td id=\"S5.T5.4.1.3.1\" class=\"ltx_td ltx_align_left\">1x (800M)</td>\n<td id=\"S5.T5.4.1.3.2\" class=\"ltx_td ltx_align_center\">2.2 (1.6)</td>\n<td id=\"S5.T5.4.1.3.3\" class=\"ltx_td ltx_align_center\">4.3 (3.6)</td>\n</tr>\n<tr id=\"S5.T5.4.1.4\" class=\"ltx_tr\">\n<td id=\"S5.T5.4.1.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">2x (1.6B)</td>\n<td id=\"S5.T5.4.1.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">2.2 (1.6)</td>\n<td id=\"S5.T5.4.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">4.2 (3.5)</td>\n</tr>\n</table>\n</span></div>\n</div>\n</div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Table 5 (left) compares DLMs and neural LMs of different size: i) while model size increases the WER decreases; ii) DLMs outperforms neural LMs of the same size.",
            "To vary the text corpus used to generate the synthetic audio for DLM training, i) we randomly selected 20M sentences from the LibriSpeech LM corpus and formed 0.5x corpus (400M words); ii) we added in 40M sentences randomly selected from the Gutenberg dataset [49] and formed the 2x corpus (1.6B words).\nTable 5 (right) shows that DLMs benefits from more trainig data.\nWe expect to see further gains from scaling to larger corpora in future work."
        ]
    },
    "S5.T6": {
        "caption": "Table 6:  LibriSpeech DSR-greedy (DSR-decoding) WER (%) for DLMs (155M) trained on RichTTS synthetic data (400M words) with 10% random substitutions and different # of speakers.",
        "table": "<figure id=\"S5.T6\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>LibriSpeech DSR-greedy (DSR-decoding) WER (%) for DLMs (155M) trained on RichTTS synthetic data (400M words) with 10% random substitutions and different # of speakers.</figcaption>\n<div id=\"S5.T6.1\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:390.3pt;height:45.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-36.9pt,4.3pt) scale(0.841066023115248,0.841066023115248) ;\">\n<table id=\"S5.T6.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T6.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"># of speakers</td>\n<td id=\"S5.T6.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">1</td>\n<td id=\"S5.T6.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">10</td>\n<td id=\"S5.T6.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">100</td>\n<td id=\"S5.T6.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">1000</td>\n<td id=\"S5.T6.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">2000</td>\n<td id=\"S5.T6.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_tt\">1 (w/ 100 speaker embeddings)</td>\n</tr>\n<tr id=\"S5.T6.1.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">dev-clean</td>\n<td id=\"S5.T6.1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">3.2 (2.0)</td>\n<td id=\"S5.T6.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">2.6 (1.8)</td>\n<td id=\"S5.T6.1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">2.6 (1.8)</td>\n<td id=\"S5.T6.1.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">2.5 (1.8)</td>\n<td id=\"S5.T6.1.1.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.6 (1.8)</td>\n<td id=\"S5.T6.1.1.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">2.9 (1.9)</td>\n</tr>\n<tr id=\"S5.T6.1.1.3\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">dev-other</td>\n<td id=\"S5.T6.1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">5.3 (4.1)</td>\n<td id=\"S5.T6.1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">4.9 (4.0)</td>\n<td id=\"S5.T6.1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">4.7 (4.1)</td>\n<td id=\"S5.T6.1.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">4.7 (4.1)</td>\n<td id=\"S5.T6.1.1.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">4.7 (4.0)</td>\n<td id=\"S5.T6.1.1.3.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">5.1 (4.1)</td>\n</tr>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "To see the impact of the number of speakers used in audio generation, we obtained 20M unique sentences from 1, 10, 100, 1000, and 2000 speakers to form five training sets.\nWe train five different DLMs (155M) on these data: using more speakers leads to lower WER, probably due to larger variability in types of errors generated, see Table 6.\n100 speakers seems to be enough for Librispeech dev sets, but we keep using all speakers from LibriSpeech for the other experiments.\nEven if we use only one speaker, we can improve results by using a random example of their utterances for style-conditioning (see the last column in Table 6). This indicates that the variance from the same speaker\u2019s different embeddings can also provide a certain amount of noise, which is beneficial for the DLM to learn and correct.",
            "To see the impact of the number of speakers used in audio generation, we obtained 20M unique sentences from 1, 10, 100, 1000, and 2000 speakers to form five training sets.\nWe train five different DLMs (155M) on these data: using more speakers leads to lower WER, probably due to larger variability in types of errors generated, see Table 6.\n100 speakers seems to be enough for Librispeech dev sets, but we keep using all speakers from LibriSpeech for the other experiments.\nEven if we use only one speaker, we can improve results by using a random example of their utterances for style-conditioning (see the last column in Table 6). This indicates that the variance from the same speaker\u2019s different embeddings can also provide a certain amount of noise, which is beneficial for the DLM to learn and correct."
        ]
    },
    "S5.T7": {
        "caption": "Table 7:  LibriSpeech WER (%) for DLMs (484M) trained on different data reported with DSR-greedy (DSR-decoding):  s \ud835\udc60 s  is a random character substitution rate; \u201cFM\u201d is a frequency masking; \u201creal\u201d refers to hypotheses from a real audio.",
        "table": "<figure id=\"S5.T7\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>LibriSpeech WER (%) for DLMs (484M) trained on different data reported with DSR-greedy (DSR-decoding): <math id=\"S5.T7.2.m1.1\" class=\"ltx_Math\" alttext=\"s\" display=\"inline\"><semantics id=\"S5.T7.2.m1.1b\"><mi id=\"S5.T7.2.m1.1.1\" xref=\"S5.T7.2.m1.1.1.cmml\">s</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.2.m1.1c\"><ci id=\"S5.T7.2.m1.1.1.cmml\" xref=\"S5.T7.2.m1.1.1\">\ud835\udc60</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.2.m1.1d\">s</annotation></semantics></math> is a random character substitution rate; \u201cFM\u201d is a frequency masking; \u201creal\u201d refers to hypotheses from a real audio.</figcaption>\n<div id=\"S5.T7.5\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:216.8pt;height:144.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-27.0pt,18.0pt) scale(0.8003028440235,0.8003028440235) ;\">\n<table id=\"S5.T7.5.3\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T7.5.3.4\" class=\"ltx_tr\">\n<td id=\"S5.T7.5.3.4.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Model</td>\n<td id=\"S5.T7.5.3.4.2\" class=\"ltx_td ltx_align_left ltx_border_tt\">dev-clean</td>\n<td id=\"S5.T7.5.3.4.3\" class=\"ltx_td ltx_align_left ltx_border_tt\">dev-other</td>\n</tr>\n<tr id=\"S5.T7.5.3.5\" class=\"ltx_tr\">\n<td id=\"S5.T7.5.3.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T7.5.3.5.1.1\" class=\"ltx_text ltx_font_italic\">baseline ASR</span></td>\n<td id=\"S5.T7.5.3.5.2\" class=\"ltx_td ltx_align_left ltx_border_t\">2.1</td>\n<td id=\"S5.T7.5.3.5.3\" class=\"ltx_td ltx_align_left ltx_border_t\">5.5</td>\n</tr>\n<tr id=\"S5.T7.3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T7.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">+ DLM, Tacotron + <math id=\"S5.T7.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"s=10\\%\" display=\"inline\"><semantics id=\"S5.T7.3.1.1.1.m1.1a\"><mrow id=\"S5.T7.3.1.1.1.m1.1.1\" xref=\"S5.T7.3.1.1.1.m1.1.1.cmml\"><mi id=\"S5.T7.3.1.1.1.m1.1.1.2\" xref=\"S5.T7.3.1.1.1.m1.1.1.2.cmml\">s</mi><mo id=\"S5.T7.3.1.1.1.m1.1.1.1\" xref=\"S5.T7.3.1.1.1.m1.1.1.1.cmml\">=</mo><mrow id=\"S5.T7.3.1.1.1.m1.1.1.3\" xref=\"S5.T7.3.1.1.1.m1.1.1.3.cmml\"><mn id=\"S5.T7.3.1.1.1.m1.1.1.3.2\" xref=\"S5.T7.3.1.1.1.m1.1.1.3.2.cmml\">10</mn><mo id=\"S5.T7.3.1.1.1.m1.1.1.3.1\" xref=\"S5.T7.3.1.1.1.m1.1.1.3.1.cmml\">%</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.3.1.1.1.m1.1b\"><apply id=\"S5.T7.3.1.1.1.m1.1.1.cmml\" xref=\"S5.T7.3.1.1.1.m1.1.1\"><eq id=\"S5.T7.3.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T7.3.1.1.1.m1.1.1.1\"></eq><ci id=\"S5.T7.3.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T7.3.1.1.1.m1.1.1.2\">\ud835\udc60</ci><apply id=\"S5.T7.3.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T7.3.1.1.1.m1.1.1.3\"><csymbol cd=\"latexml\" id=\"S5.T7.3.1.1.1.m1.1.1.3.1.cmml\" xref=\"S5.T7.3.1.1.1.m1.1.1.3.1\">percent</csymbol><cn type=\"integer\" id=\"S5.T7.3.1.1.1.m1.1.1.3.2.cmml\" xref=\"S5.T7.3.1.1.1.m1.1.1.3.2\">10</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.3.1.1.1.m1.1c\">s=10\\%</annotation></semantics></math>\n</td>\n<td id=\"S5.T7.3.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">4.1 (2.0)</td>\n<td id=\"S5.T7.3.1.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">6.6 (4.8)</td>\n</tr>\n<tr id=\"S5.T7.5.3.6\" class=\"ltx_tr\">\n<td id=\"S5.T7.5.3.6.1\" class=\"ltx_td ltx_align_left ltx_border_t\">+ DLM (YourTTS)</td>\n<td id=\"S5.T7.5.3.6.2\" class=\"ltx_td ltx_align_left ltx_border_t\">2.3 (1.6)</td>\n<td id=\"S5.T7.5.3.6.3\" class=\"ltx_td ltx_align_left ltx_border_t\">4.9 (4.0)</td>\n</tr>\n<tr id=\"S5.T7.4.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T7.4.2.2.1\" class=\"ltx_td ltx_align_left\">\u00a0\u00a0\u00a0\u00a0\u00a0 + <math id=\"S5.T7.4.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"s=10\\%\" display=\"inline\"><semantics id=\"S5.T7.4.2.2.1.m1.1a\"><mrow id=\"S5.T7.4.2.2.1.m1.1.1\" xref=\"S5.T7.4.2.2.1.m1.1.1.cmml\"><mi id=\"S5.T7.4.2.2.1.m1.1.1.2\" xref=\"S5.T7.4.2.2.1.m1.1.1.2.cmml\">s</mi><mo id=\"S5.T7.4.2.2.1.m1.1.1.1\" xref=\"S5.T7.4.2.2.1.m1.1.1.1.cmml\">=</mo><mrow id=\"S5.T7.4.2.2.1.m1.1.1.3\" xref=\"S5.T7.4.2.2.1.m1.1.1.3.cmml\"><mn id=\"S5.T7.4.2.2.1.m1.1.1.3.2\" xref=\"S5.T7.4.2.2.1.m1.1.1.3.2.cmml\">10</mn><mo id=\"S5.T7.4.2.2.1.m1.1.1.3.1\" xref=\"S5.T7.4.2.2.1.m1.1.1.3.1.cmml\">%</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.4.2.2.1.m1.1b\"><apply id=\"S5.T7.4.2.2.1.m1.1.1.cmml\" xref=\"S5.T7.4.2.2.1.m1.1.1\"><eq id=\"S5.T7.4.2.2.1.m1.1.1.1.cmml\" xref=\"S5.T7.4.2.2.1.m1.1.1.1\"></eq><ci id=\"S5.T7.4.2.2.1.m1.1.1.2.cmml\" xref=\"S5.T7.4.2.2.1.m1.1.1.2\">\ud835\udc60</ci><apply id=\"S5.T7.4.2.2.1.m1.1.1.3.cmml\" xref=\"S5.T7.4.2.2.1.m1.1.1.3\"><csymbol cd=\"latexml\" id=\"S5.T7.4.2.2.1.m1.1.1.3.1.cmml\" xref=\"S5.T7.4.2.2.1.m1.1.1.3.1\">percent</csymbol><cn type=\"integer\" id=\"S5.T7.4.2.2.1.m1.1.1.3.2.cmml\" xref=\"S5.T7.4.2.2.1.m1.1.1.3.2\">10</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.4.2.2.1.m1.1c\">s=10\\%</annotation></semantics></math>\n</td>\n<td id=\"S5.T7.4.2.2.2\" class=\"ltx_td ltx_align_left\">2.2 (1.5)</td>\n<td id=\"S5.T7.4.2.2.3\" class=\"ltx_td ltx_align_left\">4.6 (3.8)</td>\n</tr>\n<tr id=\"S5.T7.5.3.7\" class=\"ltx_tr\">\n<td id=\"S5.T7.5.3.7.1\" class=\"ltx_td ltx_align_left\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 + FM</td>\n<td id=\"S5.T7.5.3.7.2\" class=\"ltx_td ltx_align_left\">2.3 (1.6)</td>\n<td id=\"S5.T7.5.3.7.3\" class=\"ltx_td ltx_align_left\">4.6 (3.7)</td>\n</tr>\n<tr id=\"S5.T7.5.3.8\" class=\"ltx_tr\">\n<td id=\"S5.T7.5.3.8.1\" class=\"ltx_td ltx_align_left\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 + RichTTS</td>\n<td id=\"S5.T7.5.3.8.2\" class=\"ltx_td ltx_align_left\">2.1 (1.5)</td>\n<td id=\"S5.T7.5.3.8.3\" class=\"ltx_td ltx_align_left\">4.2 (3.7)</td>\n</tr>\n<tr id=\"S5.T7.5.3.9\" class=\"ltx_tr\">\n<td id=\"S5.T7.5.3.9.1\" class=\"ltx_td ltx_align_left\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 + FM + real</td>\n<td id=\"S5.T7.5.3.9.2\" class=\"ltx_td ltx_align_left\">2.0 (1.5)</td>\n<td id=\"S5.T7.5.3.9.3\" class=\"ltx_td ltx_align_left\">4.3 (3.8)</td>\n</tr>\n<tr id=\"S5.T7.5.3.10\" class=\"ltx_tr\">\n<td id=\"S5.T7.5.3.10.1\" class=\"ltx_td ltx_align_left\">\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 + RichTTS + FM + real</td>\n<td id=\"S5.T7.5.3.10.2\" class=\"ltx_td ltx_align_left\">1.9 (1.5)</td>\n<td id=\"S5.T7.5.3.10.3\" class=\"ltx_td ltx_align_left\">3.9 (3.4)</td>\n</tr>\n<tr id=\"S5.T7.5.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T7.5.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">+ DLM, RichTTS + <math id=\"S5.T7.5.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"s=10\\%\" display=\"inline\"><semantics id=\"S5.T7.5.3.3.1.m1.1a\"><mrow id=\"S5.T7.5.3.3.1.m1.1.1\" xref=\"S5.T7.5.3.3.1.m1.1.1.cmml\"><mi id=\"S5.T7.5.3.3.1.m1.1.1.2\" xref=\"S5.T7.5.3.3.1.m1.1.1.2.cmml\">s</mi><mo id=\"S5.T7.5.3.3.1.m1.1.1.1\" xref=\"S5.T7.5.3.3.1.m1.1.1.1.cmml\">=</mo><mrow id=\"S5.T7.5.3.3.1.m1.1.1.3\" xref=\"S5.T7.5.3.3.1.m1.1.1.3.cmml\"><mn id=\"S5.T7.5.3.3.1.m1.1.1.3.2\" xref=\"S5.T7.5.3.3.1.m1.1.1.3.2.cmml\">10</mn><mo id=\"S5.T7.5.3.3.1.m1.1.1.3.1\" xref=\"S5.T7.5.3.3.1.m1.1.1.3.1.cmml\">%</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.5.3.3.1.m1.1b\"><apply id=\"S5.T7.5.3.3.1.m1.1.1.cmml\" xref=\"S5.T7.5.3.3.1.m1.1.1\"><eq id=\"S5.T7.5.3.3.1.m1.1.1.1.cmml\" xref=\"S5.T7.5.3.3.1.m1.1.1.1\"></eq><ci id=\"S5.T7.5.3.3.1.m1.1.1.2.cmml\" xref=\"S5.T7.5.3.3.1.m1.1.1.2\">\ud835\udc60</ci><apply id=\"S5.T7.5.3.3.1.m1.1.1.3.cmml\" xref=\"S5.T7.5.3.3.1.m1.1.1.3\"><csymbol cd=\"latexml\" id=\"S5.T7.5.3.3.1.m1.1.1.3.1.cmml\" xref=\"S5.T7.5.3.3.1.m1.1.1.3.1\">percent</csymbol><cn type=\"integer\" id=\"S5.T7.5.3.3.1.m1.1.1.3.2.cmml\" xref=\"S5.T7.5.3.3.1.m1.1.1.3.2\">10</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.5.3.3.1.m1.1c\">s=10\\%</annotation></semantics></math>\n</td>\n<td id=\"S5.T7.5.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">2.2 (1.6)</td>\n<td id=\"S5.T7.5.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">4.3 (3.6)</td>\n</tr>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "In this section, we show the data construction path to our final results highlighting the importance to generate a variety of relevant error types (see Table 7, left).\nWe first generate a dataset of 40M {(y^,y)}^\ud835\udc66\ud835\udc66\\{(\\hat{y},y)\\} pairs using YourTTS.\nTraining on these data improves WER of ASR on dev-other, with slight degradation on dev-clean (row 2). We hypothesise that training data are still relatively clean and there are not many corrections for the model to learn. We experiment with adding different types of noise carefully such that the noise distribution does not shift the training distribution away from the test time distribution too significantly.\nFirst, we found that randomly substituting some percentage of the characters in the input transcript is beneficial (row 4) as long as it is not too high (s=10%\ud835\udc60percent10s=10\\% works quite well).\nSecond, generating hypotheses from noisy audio, where the noise comes from masking frequency bands of the spectrogram input to the ASR model (2 masks with max width 30), in a manner similar to SpecAugment [35], also helps (row 5).\nTo ground the data distribution of the DLM more closely to real data, we added a small portion (10%) of noisy hypotheses by running the ASR system on the LibriSpeech 960h data set.\nThis gave us further gains on both dev-clean and dev-other sets (row 7).",
            "We assess the impact of using different TTS systems for generating DLMs training data.\nWe compare Tacotron, YourTTS and RichTTS by evaluating ASR systems on the audio generated by these TTS models: synthetic audio from Tacotron (6%) has significantly lower WER than that from YourTTS (10%) and RichTTS (12%) when evaluated with Whisper-small. See Table 11 in Appendix E for more details.\nWe then trained DLMs using data generated by the three TTS systems separately, and applied them to the baseline ASR model, see Table 7.\nThough Tacotron data has higher quality than YourTTS and RichTTS data, it performs poorly in terms of DLM error correction.\nWe posit that there might not be enough meaningful noise in the Tacotron data for the error correction model to learn.\nIn this sense, improving ASR systems in the proposed error correction paradigm is easy \u2013 it does not require high-quality TTS."
        ]
    },
    "A4.T8": {
        "caption": "Table 8:  LibriSpeech dev sets WER (%) for Transformer  baseline ASR  models (255M) trained with a mixture of real (LibriSpeech train data) and synthetic (generated from LibriSpeech LM text corpus by RichTTS) audio data. \u201creal:synthetic\u201d denotes the proportion (%) of corresponding data in a minibatch.",
        "table": "<figure id=\"A4.T8\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>LibriSpeech dev sets WER (%) for Transformer<span id=\"A4.T8.2.1\" class=\"ltx_text ltx_font_italic\"> baseline ASR</span> models (255M) trained with a mixture of real (LibriSpeech train data) and synthetic (generated from LibriSpeech LM text corpus by RichTTS) audio data. \u201creal:synthetic\u201d denotes the proportion (%) of corresponding data in a minibatch.</figcaption>\n<div id=\"A4.T8.3\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:173.4pt;height:102.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-4.9pt,2.9pt) scale(0.9460641950237,0.9460641950237) ;\">\n<table id=\"A4.T8.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A4.T8.3.1.1\" class=\"ltx_tr\">\n<td id=\"A4.T8.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">real:synthetic</td>\n<td id=\"A4.T8.3.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\">dev-clean</td>\n<td id=\"A4.T8.3.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\">dev-other</td>\n</tr>\n<tr id=\"A4.T8.3.1.2\" class=\"ltx_tr\">\n<td id=\"A4.T8.3.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">0:100</td>\n<td id=\"A4.T8.3.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\">12.7</td>\n<td id=\"A4.T8.3.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\">33.4</td>\n</tr>\n<tr id=\"A4.T8.3.1.3\" class=\"ltx_tr\">\n<td id=\"A4.T8.3.1.3.1\" class=\"ltx_td ltx_align_center\">30:70</td>\n<td id=\"A4.T8.3.1.3.2\" class=\"ltx_td ltx_align_right\">2.1</td>\n<td id=\"A4.T8.3.1.3.3\" class=\"ltx_td ltx_align_right\">5.3</td>\n</tr>\n<tr id=\"A4.T8.3.1.4\" class=\"ltx_tr\">\n<td id=\"A4.T8.3.1.4.1\" class=\"ltx_td ltx_align_center\">50:50</td>\n<td id=\"A4.T8.3.1.4.2\" class=\"ltx_td ltx_align_right\">1.9</td>\n<td id=\"A4.T8.3.1.4.3\" class=\"ltx_td ltx_align_right\">4.7</td>\n</tr>\n<tr id=\"A4.T8.3.1.5\" class=\"ltx_tr\">\n<td id=\"A4.T8.3.1.5.1\" class=\"ltx_td ltx_align_center\">70:30</td>\n<td id=\"A4.T8.3.1.5.2\" class=\"ltx_td ltx_align_right\">2.0</td>\n<td id=\"A4.T8.3.1.5.3\" class=\"ltx_td ltx_align_right\">4.9</td>\n</tr>\n<tr id=\"A4.T8.3.1.6\" class=\"ltx_tr\">\n<td id=\"A4.T8.3.1.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">100:0</td>\n<td id=\"A4.T8.3.1.6.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">2.1</td>\n<td id=\"A4.T8.3.1.6.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">5.5</td>\n</tr>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Our main results are in Table 1.\nWe consider two ASR systems: one is the baseline ASR (Transformer CTC-based model with 255M parameters) trained on LibriSpeech; the other is the baseline ASR (LS+TTS) which is the same as baseline ASR but trained on LibriSpeech together with TTS audio data generated by RichTTS from the LibriSpeech LM corpus. The mixing ratio of LibriSpeech audio and TTS audio in a minibatch is 1:1, which leads to improved performance compared to baseline ASR. We show the other mixtures of LibriSpeech audio and TTS audio in a minibatch for baseline ASR (LS+TTS) in Appendix D.2 Table 8.",
            "In addition, we train baseline ASR with the mixture of LibriSpeech 960h train data (real) and synthetic audio data generated by RichTTS model from the LibriSpeech LM text corpus. The training setup is the same as for the baseline ASR trained on real data only, but we vary the mixture proportion of real and synthetic audio in the minibatch. Results are shown in Table 8."
        ]
    },
    "A5.T9": {
        "caption": "Table 9:  LibriSpeech dev and test sets WER (%) of  baseline ASR  model trained on LibriSpeech (960h) with greedy decoding, with neural LM (neLM, 500M) rescoring or with DLM (484M) DSR-greedy/DSR-decoding. Both neural LM and DLM are the same as reported in Table\u00a0 1 . We also report WER (%) on dev and test sets of out-of-domain TED-LIUM.",
        "table": "<figure id=\"A5.T9\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>LibriSpeech dev and test sets WER (%) of <span id=\"A5.T9.2.1\" class=\"ltx_text ltx_font_italic\">baseline ASR</span> model trained on LibriSpeech (960h) with greedy decoding, with neural LM (neLM, 500M) rescoring or with DLM (484M) DSR-greedy/DSR-decoding. Both neural LM and DLM are the same as reported in Table\u00a0<a href=\"#S5.T1\" title=\"Table 1 \u2023 5.1 Pushing the limits: DLM achieves new SOTA WER on LibriSpeech \u2023 5 Results \u2023 Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. We also report WER (%) on dev and test sets of out-of-domain TED-LIUM.</figcaption>\n<table id=\"A5.T9.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A5.T9.3.1\" class=\"ltx_tr\">\n<td id=\"A5.T9.3.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span id=\"A5.T9.3.1.1.1\" class=\"ltx_text\">Model</span></td>\n<td id=\"A5.T9.3.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\" rowspan=\"2\"><span id=\"A5.T9.3.1.2.1\" class=\"ltx_text\">Size</span></td>\n<td id=\"A5.T9.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">dev</td>\n<td id=\"A5.T9.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">test</td>\n<td id=\"A5.T9.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">TED-LIUM</td>\n</tr>\n<tr id=\"A5.T9.3.2\" class=\"ltx_tr\">\n<td id=\"A5.T9.3.2.1\" class=\"ltx_td ltx_align_right ltx_border_t\">clean</td>\n<td id=\"A5.T9.3.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\">other</td>\n<td id=\"A5.T9.3.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\">clean</td>\n<td id=\"A5.T9.3.2.4\" class=\"ltx_td ltx_align_right ltx_border_t\">other</td>\n<td id=\"A5.T9.3.2.5\" class=\"ltx_td ltx_align_right ltx_border_t\">dev</td>\n<td id=\"A5.T9.3.2.6\" class=\"ltx_td ltx_align_right ltx_border_t\">test</td>\n</tr>\n<tr id=\"A5.T9.3.3\" class=\"ltx_tr\">\n<td id=\"A5.T9.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Transformer ASR (LS)</td>\n<td id=\"A5.T9.3.3.2\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" rowspan=\"4\"><span id=\"A5.T9.3.3.2.1\" class=\"ltx_text\">256M</span></td>\n<td id=\"A5.T9.3.3.3\" class=\"ltx_td ltx_align_right ltx_border_t\">2.1</td>\n<td id=\"A5.T9.3.3.4\" class=\"ltx_td ltx_align_right ltx_border_t\">5.5</td>\n<td id=\"A5.T9.3.3.5\" class=\"ltx_td ltx_align_right ltx_border_t\">2.2</td>\n<td id=\"A5.T9.3.3.6\" class=\"ltx_td ltx_align_right ltx_border_t\">5.3</td>\n<td id=\"A5.T9.3.3.7\" class=\"ltx_td ltx_align_right ltx_border_t\">11.6</td>\n<td id=\"A5.T9.3.3.8\" class=\"ltx_td ltx_align_right ltx_border_t\">10.8</td>\n</tr>\n<tr id=\"A5.T9.3.4\" class=\"ltx_tr\">\n<td id=\"A5.T9.3.4.1\" class=\"ltx_td ltx_align_left\">+ neLM (LM-rescoring)</td>\n<td id=\"A5.T9.3.4.2\" class=\"ltx_td ltx_align_right\">1.5</td>\n<td id=\"A5.T9.3.4.3\" class=\"ltx_td ltx_align_right\">4.0</td>\n<td id=\"A5.T9.3.4.4\" class=\"ltx_td ltx_align_right\">2.0</td>\n<td id=\"A5.T9.3.4.5\" class=\"ltx_td ltx_align_right\">4.1</td>\n<td id=\"A5.T9.3.4.6\" class=\"ltx_td ltx_align_right\">10.6</td>\n<td id=\"A5.T9.3.4.7\" class=\"ltx_td ltx_align_right\">9.7</td>\n</tr>\n<tr id=\"A5.T9.3.5\" class=\"ltx_tr\">\n<td id=\"A5.T9.3.5.1\" class=\"ltx_td ltx_align_left\">+ DLM (DSR-greedy)</td>\n<td id=\"A5.T9.3.5.2\" class=\"ltx_td ltx_align_right\">1.8</td>\n<td id=\"A5.T9.3.5.3\" class=\"ltx_td ltx_align_right\">3.9</td>\n<td id=\"A5.T9.3.5.4\" class=\"ltx_td ltx_align_right\">2.0</td>\n<td id=\"A5.T9.3.5.5\" class=\"ltx_td ltx_align_right\">4.1</td>\n<td id=\"A5.T9.3.5.6\" class=\"ltx_td ltx_align_right\">11.0</td>\n<td id=\"A5.T9.3.5.7\" class=\"ltx_td ltx_align_right\">10.8</td>\n</tr>\n<tr id=\"A5.T9.3.6\" class=\"ltx_tr\">\n<td id=\"A5.T9.3.6.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">+ DLM (DSR-decoding)</td>\n<td id=\"A5.T9.3.6.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"A5.T9.3.6.2.1\" class=\"ltx_text ltx_font_bold\">1.5</span></td>\n<td id=\"A5.T9.3.6.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"A5.T9.3.6.3.1\" class=\"ltx_text ltx_font_bold\">3.4</span></td>\n<td id=\"A5.T9.3.6.4\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"A5.T9.3.6.4.1\" class=\"ltx_text ltx_font_bold\">1.6</span></td>\n<td id=\"A5.T9.3.6.5\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"A5.T9.3.6.5.1\" class=\"ltx_text ltx_font_bold\">3.6</span></td>\n<td id=\"A5.T9.3.6.6\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"A5.T9.3.6.6.1\" class=\"ltx_text ltx_font_bold\">9.7</span></td>\n<td id=\"A5.T9.3.6.7\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"A5.T9.3.6.7.1\" class=\"ltx_text ltx_font_bold\">9.2</span></td>\n</tr>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Furthermore, we show that the DLM trained on the LibriSpeech LM corpus also improves results of the baseline ASR system when evaluating on different data. We use the TED-LIUM dataset [13], which is based on TeD talks, conversational in nature, and covers more contemporary topics. We observe around 16% WER improvement on dev and test set of TED-LIUM after apply DLM with DSR-decoding, which still outperforms neural LM (see Table 9 in Appendix E for details). Note that TED-LIUM is different from the read speech of the LibriSpeech, which corresponds to Gutenberg books, and hence the gains here are expected to be small.",
            "In Table 9 we show results for baseline ASR when neural LM and DLM are applied on top for both LibriSpeech and TED-LIUM data.\nNote, that for TED-LIUM evaluation both ASR model and neural LM / DLM are out-of-domain as trained on LibriSpeech audio data and LibriSpeech LM text corpus, emulating both domain shifts: in acoustics and linguistics."
        ]
    },
    "A5.T10": {
        "caption": "Table 10:  LibriSpeech WER (%) for DLMs (484M) reported with DSR-greedy (DSR-decoding): DLM-synthetic is using audio generated by RichTTS from Libriheavy transcriptions, DLM-real is using real audio of Libriheavy for  y ^ ^ \ud835\udc66 \\hat{y}  generation by  baseline ASR . Both DLMs are trained with 10% random characters substitutions.",
        "table": "<figure id=\"A5.T10\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span>LibriSpeech WER (%) for DLMs (484M) reported with DSR-greedy (DSR-decoding): DLM-synthetic is using audio generated by RichTTS from Libriheavy transcriptions, DLM-real is using real audio of Libriheavy for <math id=\"A5.T10.2.m1.1\" class=\"ltx_Math\" alttext=\"\\hat{y}\" display=\"inline\"><semantics id=\"A5.T10.2.m1.1b\"><mover accent=\"true\" id=\"A5.T10.2.m1.1.1\" xref=\"A5.T10.2.m1.1.1.cmml\"><mi id=\"A5.T10.2.m1.1.1.2\" xref=\"A5.T10.2.m1.1.1.2.cmml\">y</mi><mo id=\"A5.T10.2.m1.1.1.1\" xref=\"A5.T10.2.m1.1.1.1.cmml\">^</mo></mover><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.2.m1.1c\"><apply id=\"A5.T10.2.m1.1.1.cmml\" xref=\"A5.T10.2.m1.1.1\"><ci id=\"A5.T10.2.m1.1.1.1.cmml\" xref=\"A5.T10.2.m1.1.1.1\">^</ci><ci id=\"A5.T10.2.m1.1.1.2.cmml\" xref=\"A5.T10.2.m1.1.1.2\">\ud835\udc66</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.2.m1.1d\">\\hat{y}</annotation></semantics></math> generation by <span id=\"A5.T10.4.1\" class=\"ltx_text ltx_font_italic\">baseline ASR</span>. Both DLMs are trained with 10% random characters substitutions.</figcaption>\n<table id=\"A5.T10.5\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A5.T10.5.1\" class=\"ltx_tr\">\n<td id=\"A5.T10.5.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Model</td>\n<td id=\"A5.T10.5.1.2\" class=\"ltx_td ltx_align_left ltx_border_tt\">dev-clean</td>\n<td id=\"A5.T10.5.1.3\" class=\"ltx_td ltx_align_left ltx_border_tt\">dev-other</td>\n</tr>\n<tr id=\"A5.T10.5.2\" class=\"ltx_tr\">\n<td id=\"A5.T10.5.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"A5.T10.5.2.1.1\" class=\"ltx_text ltx_font_italic\">baseline ASR</span></td>\n<td id=\"A5.T10.5.2.2\" class=\"ltx_td ltx_align_left ltx_border_t\">2.1</td>\n<td id=\"A5.T10.5.2.3\" class=\"ltx_td ltx_align_left ltx_border_t\">5.5</td>\n</tr>\n<tr id=\"A5.T10.5.3\" class=\"ltx_tr\">\n<td id=\"A5.T10.5.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">+ DLM-synthetic</td>\n<td id=\"A5.T10.5.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\">2.4 (1.7)</td>\n<td id=\"A5.T10.5.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\">4.5 (3.7)</td>\n</tr>\n<tr id=\"A5.T10.5.4\" class=\"ltx_tr\">\n<td id=\"A5.T10.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">+ DLM-real</td>\n<td id=\"A5.T10.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">2.0 (1.5)</td>\n<td id=\"A5.T10.5.4.3\" class=\"ltx_td ltx_align_left ltx_border_bb\">4.6 (4.1)</td>\n</tr>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We explore whether there is a gap between the performance of DLMs trained on y^^\ud835\udc66\\hat{y} obtained from synthetic audio vs from real audio.\nWe use \u223csimilar-to\\sim60k hours of real labeled data from Libriheavy [20] which gives 400M transcribed words, and generate y^^\ud835\udc66\\hat{y} from it using the baseline ASR.\nFor a fair comparison, we also generate TTS data by RichTTS for the same sentences. For both datasets of (y^,y)^\ud835\udc66\ud835\udc66(\\hat{y},y) we train a DLM-real and DLM-synthetic.\nOn the dev-clean set, the DLM-synthetic (WER 1.7%) is worse than the DLM-real (WER 1.5%).\nSurprisingly, on the dev-other set, DLM-synthetic (WER 3.7%) outperforms the DLM-real (WER 4.1%).\nOverall, the gap between DLM-synthetic and DLM-real is small and thus effect of TTS generated data is small for DLMs. See Table 10 in Appendix E for more details.",
            "Comparison between usage real audio and synthetic audio for DLM data generation is given in Table 10 and between TTS systems is given in Table 11.\nFor the later, we first generate audio data by TTS models from the text data of LibriSpeech dev-clean and dev-other and then evaluate quality of ASR models in WER on these audio data."
        ]
    },
    "A5.T11": {
        "caption": "Table 11:  Whisper and  baseline ASR  WER (%) on audio generated from LibriSpeech dev sets by different TTS models.",
        "table": "<figure id=\"A5.T11\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 11: </span>Whisper and <span id=\"A5.T11.2.1\" class=\"ltx_text ltx_font_italic\">baseline ASR</span> WER (%) on audio generated from LibriSpeech dev sets by different TTS models.</figcaption>\n<table id=\"A5.T11.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A5.T11.3.1\" class=\"ltx_tr\">\n<td id=\"A5.T11.3.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span id=\"A5.T11.3.1.1.1\" class=\"ltx_text\">TTS System</span></td>\n<td id=\"A5.T11.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Whisper-small</td>\n<td id=\"A5.T11.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Baseline ASR</td>\n</tr>\n<tr id=\"A5.T11.3.2\" class=\"ltx_tr\">\n<td id=\"A5.T11.3.2.1\" class=\"ltx_td ltx_align_right\">dev-clean</td>\n<td id=\"A5.T11.3.2.2\" class=\"ltx_td ltx_align_right\">dev-other</td>\n<td id=\"A5.T11.3.2.3\" class=\"ltx_td ltx_align_right\">dev-clean</td>\n<td id=\"A5.T11.3.2.4\" class=\"ltx_td ltx_align_right\">dev-other</td>\n</tr>\n<tr id=\"A5.T11.3.3\" class=\"ltx_tr\">\n<td id=\"A5.T11.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Tacotron</td>\n<td id=\"A5.T11.3.3.2\" class=\"ltx_td ltx_align_right ltx_border_t\">5.9</td>\n<td id=\"A5.T11.3.3.3\" class=\"ltx_td ltx_align_right ltx_border_t\">5.2</td>\n<td id=\"A5.T11.3.3.4\" class=\"ltx_td ltx_align_right ltx_border_t\">6.8</td>\n<td id=\"A5.T11.3.3.5\" class=\"ltx_td ltx_align_right ltx_border_t\">6.2</td>\n</tr>\n<tr id=\"A5.T11.3.4\" class=\"ltx_tr\">\n<td id=\"A5.T11.3.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">YourTTS</td>\n<td id=\"A5.T11.3.4.2\" class=\"ltx_td ltx_align_right ltx_border_t\">8.2</td>\n<td id=\"A5.T11.3.4.3\" class=\"ltx_td ltx_align_right ltx_border_t\">9.8</td>\n<td id=\"A5.T11.3.4.4\" class=\"ltx_td ltx_align_right ltx_border_t\">8.6</td>\n<td id=\"A5.T11.3.4.5\" class=\"ltx_td ltx_align_right ltx_border_t\">10.5</td>\n</tr>\n<tr id=\"A5.T11.3.5\" class=\"ltx_tr\">\n<td id=\"A5.T11.3.5.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">RichTTS</td>\n<td id=\"A5.T11.3.5.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">7.7</td>\n<td id=\"A5.T11.3.5.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">11.9</td>\n<td id=\"A5.T11.3.5.4\" class=\"ltx_td ltx_align_right ltx_border_bb\">9.5</td>\n<td id=\"A5.T11.3.5.5\" class=\"ltx_td ltx_align_right ltx_border_bb\">16.7</td>\n</tr>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We assess the impact of using different TTS systems for generating DLMs training data.\nWe compare Tacotron, YourTTS and RichTTS by evaluating ASR systems on the audio generated by these TTS models: synthetic audio from Tacotron (6%) has significantly lower WER than that from YourTTS (10%) and RichTTS (12%) when evaluated with Whisper-small. See Table 11 in Appendix E for more details.\nWe then trained DLMs using data generated by the three TTS systems separately, and applied them to the baseline ASR model, see Table 7.\nThough Tacotron data has higher quality than YourTTS and RichTTS data, it performs poorly in terms of DLM error correction.\nWe posit that there might not be enough meaningful noise in the Tacotron data for the error correction model to learn.\nIn this sense, improving ASR systems in the proposed error correction paradigm is easy \u2013 it does not require high-quality TTS.",
            "Comparison between usage real audio and synthetic audio for DLM data generation is given in Table 10 and between TTS systems is given in Table 11.\nFor the later, we first generate audio data by TTS models from the text data of LibriSpeech dev-clean and dev-other and then evaluate quality of ASR models in WER on these audio data."
        ]
    },
    "A5.T12": {
        "caption": "Table 12:  Key differences between DLMs and spelling correction models from  [ 11 ] .",
        "table": "<figure id=\"A5.T12\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 12: </span>Key differences between DLMs and spelling correction models from <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>.</figcaption>\n<table id=\"A5.T12.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A5.T12.1.1\" class=\"ltx_tr\">\n<td id=\"A5.T12.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"A5.T12.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">DLM</td>\n<td id=\"A5.T12.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">SC</td>\n<td id=\"A5.T12.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">Reference</td>\n</tr>\n<tr id=\"A5.T12.1.2\" class=\"ltx_tr\">\n<td id=\"A5.T12.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">TTS</td>\n<td id=\"A5.T12.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">multi-speaker</td>\n<td id=\"A5.T12.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">single-speaker</td>\n<td id=\"A5.T12.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">Table <a href=\"#S5.T7\" title=\"Table 7 \u2023 5.4 Ablation: design proper error correction data distribution \u2023 5 Results \u2023 Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>\n</td>\n</tr>\n<tr id=\"A5.T12.1.3\" class=\"ltx_tr\">\n<td id=\"A5.T12.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Augmentation</td>\n<td id=\"A5.T12.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A5.T12.1.3.2.1\" class=\"ltx_text\"></span> <span id=\"A5.T12.1.3.2.2\" class=\"ltx_text\">\n<span id=\"A5.T12.1.3.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"A5.T12.1.3.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"A5.T12.1.3.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">text substitution</span></span>\n<span id=\"A5.T12.1.3.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"A5.T12.1.3.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">frequency masking</span></span>\n<span id=\"A5.T12.1.3.2.2.1.3\" class=\"ltx_tr\">\n<span id=\"A5.T12.1.3.2.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">multiple TTS</span></span>\n<span id=\"A5.T12.1.3.2.2.1.4\" class=\"ltx_tr\">\n<span id=\"A5.T12.1.3.2.2.1.4.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">real data</span></span>\n</span></span><span id=\"A5.T12.1.3.2.3\" class=\"ltx_text\"></span></td>\n<td id=\"A5.T12.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A5.T12.1.3.3.1\" class=\"ltx_text\"></span> <span id=\"A5.T12.1.3.3.2\" class=\"ltx_text\">\n<span id=\"A5.T12.1.3.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"A5.T12.1.3.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"A5.T12.1.3.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">reverberation</span></span>\n<span id=\"A5.T12.1.3.3.2.1.2\" class=\"ltx_tr\">\n<span id=\"A5.T12.1.3.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">background noise</span></span>\n<span id=\"A5.T12.1.3.3.2.1.3\" class=\"ltx_tr\">\n<span id=\"A5.T12.1.3.3.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">environmental noise</span></span>\n</span></span><span id=\"A5.T12.1.3.3.3\" class=\"ltx_text\"></span></td>\n<td id=\"A5.T12.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">Table <a href=\"#S5.T7\" title=\"Table 7 \u2023 5.4 Ablation: design proper error correction data distribution \u2023 5 Results \u2023 Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>\n</td>\n</tr>\n<tr id=\"A5.T12.1.4\" class=\"ltx_tr\">\n<td id=\"A5.T12.1.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">Decoding</td>\n<td id=\"A5.T12.1.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">DSR-decoding</td>\n<td id=\"A5.T12.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">LM-rescoring</td>\n<td id=\"A5.T12.1.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">Table <a href=\"#S5.T1\" title=\"Table 1 \u2023 5.1 Pushing the limits: DLM achieves new SOTA WER on LibriSpeech \u2023 5 Results \u2023 Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>\n</td>\n</tr>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Using TTS data for ASR error correction is not new, as prior work [11] has made such attempts to Listen, Attend and Spell (LAS) models.\nHowever, it is worth noting that the improvement from their spelling correction (SC) model is marginal, even when applying to a relatively weak ASR system.\nBesides some fundamental differences like the ASR type and tokenization, we outline several key ingredients in Table 12 that distinguish us from this prior work and are crucial to push the limits of DLMs to outperform neural LMs."
        ]
    },
    "A5.T13": {
        "caption": "Table 13:  Oracle WER (%) in the beam for DLM (484M) and neLM (500M) reported in Table  1",
        "table": "<figure id=\"A5.T13\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 13: </span>Oracle WER (%) in the beam for DLM (484M) and neLM (500M) reported in Table <a href=\"#S5.T1\" title=\"Table 1 \u2023 5.1 Pushing the limits: DLM achieves new SOTA WER on LibriSpeech \u2023 5 Results \u2023 Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a></figcaption>\n<table id=\"A5.T13.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A5.T13.1.1\" class=\"ltx_tr\">\n<td id=\"A5.T13.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">Model</td>\n<td id=\"A5.T13.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">dev-clean</td>\n<td id=\"A5.T13.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">dev-other</td>\n<td id=\"A5.T13.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">test-clean</td>\n<td id=\"A5.T13.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">test-other</td>\n</tr>\n<tr id=\"A5.T13.1.2\" class=\"ltx_tr\">\n<td id=\"A5.T13.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">DLM</td>\n<td id=\"A5.T13.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.36</td>\n<td id=\"A5.T13.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1.29</td>\n<td id=\"A5.T13.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.46</td>\n<td id=\"A5.T13.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">1.33</td>\n</tr>\n<tr id=\"A5.T13.1.3\" class=\"ltx_tr\">\n<td id=\"A5.T13.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">neLM</td>\n<td id=\"A5.T13.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.24</td>\n<td id=\"A5.T13.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">1.52</td>\n<td id=\"A5.T13.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.69</td>\n<td id=\"A5.T13.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">1.60</td>\n</tr>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Oracle WER in the beam for our best DLM and neLM is reported in Table 13."
        ]
    }
}