{
    "S4.T1": {
        "caption": "Table 1:  Evaluation results on several methods. For convenience, we use video prompt and random prompt to test our T3M. - means the results are not available. We focus on the comparison with TalkSHOW.",
        "table": "<figure id=\"S4.T1\" class=\"ltx_table\">\n<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span id=\"S4.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">RS</span></th>\n<th id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">BCS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Habbie et al.</th>\n<td id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.146</td>\n<td id=\"S4.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr id=\"S4.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Audio Encoder-Decoder</th>\n<td id=\"S4.T1.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.214</td>\n<td id=\"S4.T1.1.3.2.3\" class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr id=\"S4.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Audio VAE</th>\n<td id=\"S4.T1.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.182</td>\n<td id=\"S4.T1.1.4.3.3\" class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr id=\"S4.T1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Audio+Motion VAE</th>\n<td id=\"S4.T1.1.5.4.2\" class=\"ltx_td ltx_align_center\">0.240</td>\n<td id=\"S4.T1.1.5.4.3\" class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr id=\"S4.T1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">TalkSHOW</th>\n<td id=\"S4.T1.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.414</td>\n<td id=\"S4.T1.1.6.5.3\" class=\"ltx_td ltx_align_center\">0.8130</td>\n</tr>\n<tr id=\"S4.T1.1.7.6\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">T3M(video prompt)</th>\n<td id=\"S4.T1.1.7.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.7.6.2.1\" class=\"ltx_text ltx_font_bold\">0.483</span></td>\n<td id=\"S4.T1.1.7.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.7.6.3.1\" class=\"ltx_text ltx_font_bold\">0.8586</span></td>\n</tr>\n<tr id=\"S4.T1.1.8.7\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">T3M(random prompt)</th>\n<td id=\"S4.T1.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0.364</td>\n<td id=\"S4.T1.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.8398</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Evaluation results on several methods. For convenience, we use video prompt and random prompt to test our T3M. - means the results are not available. We focus on the comparison with TalkSHOW.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Our experiment results are presented in Table 1. When using T3M, we utilize two distinct prompt types for generating the context features. The initial type replicates the training stage, utilizing a video prompt. In contrast, the second type entails the generation of a random vector with a mean of \u22120.040.04-0.04 and a variance of 0.120.120.12, which is utilized as the context features. In this setup, T3M generates synthetic motions solely based on the speech input.",
            "Based on the data presented in Table 1, it is evident that our T3M, when using a video prompt, demonstrates superior performance in terms of both RS and BCS indicators. Furthermore, we note that employing a random prompt yields a slightly lower RS score compared to TalkSHOW; however, it outperforms TalkSHOW in terms of BCS, which demonstrate that the generated motions by our T3M are more consistent with the audio,"
        ]
    },
    "S4.T2": {
        "caption": "Table 2:  User study results (higher scores indicating better quality). We use the video prompt and the random prompt to evaluate the quality of our generated motions.",
        "table": "<figure id=\"S4.T2\" class=\"ltx_table\">\n<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span id=\"S4.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">hands and body</span></th>\n<th id=\"S4.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">holistic</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">TalkSHOW</th>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">3.43</td>\n<td id=\"S4.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3.36</td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">T3M (random)</th>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center\">3.25</td>\n<td id=\"S4.T2.1.3.2.3\" class=\"ltx_td ltx_align_center\">3.08</td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">T3M (video)</th>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b\">3.86</td>\n<td id=\"S4.T2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">3.95</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>User study results (higher scores indicating better quality). We use the video prompt and the random prompt to evaluate the quality of our generated motions.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "To offer a more comprehensive evaluation of T3M, we have devised a thorough user study questionnaire. Following the methodology employed in TalkSHOW, we randomly selected 40 videos from four different speakers in the SHOW dataset, with each video having a duration of 10 seconds. We have invited 12 participants to participate in the evaluation process.\nEach participant will give a score ranging from 1 to 5 to rate the video in terms of the generated motions. We use random prompt and video prompt for our T3M model. Subsequently, we compute the average scores and document the results in Table 2.",
            "Table 2 reveals that our T3M, when using the video prompt, attains the highest scores. Furthermore, it is evident from the table that utilizing a random prompt yields only slightly lower scores compared to the TalkSHOW method."
        ]
    },
    "S4.T3": {
        "caption": "Table 3:  Ablation results between Mel Frequency Cepstral Coefficients (MFCC) and EnCodec. For the BCS indicator, using EnCodec achieves  + 0.0536 0.0536 +0.0536  test scores. For the user study score of the motion score indicator, we randomly selected a total number of 20 generated videos and invited 5 people to rate them.",
        "table": "<figure id=\"S4.T3\" class=\"ltx_table\">\n<table id=\"S4.T3.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.3.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span id=\"S4.T3.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th id=\"S4.T3.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">BCS</span></th>\n<th id=\"S4.T3.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.3.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Motion Score</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.3.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">T3M (MFCC)</th>\n<td id=\"S4.T3.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8050</td>\n<td id=\"S4.T3.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3.54</td>\n</tr>\n<tr id=\"S4.T3.3.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">T3M (EnCodec)</th>\n<td id=\"S4.T3.3.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0.8586</td>\n<td id=\"S4.T3.3.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b\">3.82</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Ablation results between Mel Frequency Cepstral Coefficients (MFCC) and EnCodec. For the BCS indicator, using EnCodec achieves <math id=\"S4.T3.2.2.m1.1\" class=\"ltx_Math\" alttext=\"+0.0536\" display=\"inline\"><semantics id=\"S4.T3.2.2.m1.1b\"><mrow id=\"S4.T3.2.2.m1.1.1\" xref=\"S4.T3.2.2.m1.1.1.cmml\"><mo id=\"S4.T3.2.2.m1.1.1b\" xref=\"S4.T3.2.2.m1.1.1.cmml\">+</mo><mn id=\"S4.T3.2.2.m1.1.1.2\" xref=\"S4.T3.2.2.m1.1.1.2.cmml\">0.0536</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.2.2.m1.1c\"><apply id=\"S4.T3.2.2.m1.1.1.cmml\" xref=\"S4.T3.2.2.m1.1.1\"><plus id=\"S4.T3.2.2.m1.1.1.1.cmml\" xref=\"S4.T3.2.2.m1.1.1\"></plus><cn type=\"float\" id=\"S4.T3.2.2.m1.1.1.2.cmml\" xref=\"S4.T3.2.2.m1.1.1.2\">0.0536</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.2.2.m1.1d\">+0.0536</annotation></semantics></math> test scores. For the user study score of the motion score indicator, we randomly selected a total number of 20 generated videos and invited 5 people to rate them.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We replace the EnCodec with Mel Frequency Cepstral Coefficients (MFCC) Zheng et al. (2001) and use the BCS and user study score (USS) to measure the effectiveness of generated motions. We report the results in Table 3. Comparing MFCC with EnCodec, we observe a noticeable performance improvement when utilizing EnCodec. Specifically, an increase of 0.0536 in BCS is observed with the usage of EnCodec. A user study was conducted to evaluate the motion score. A total of 20 samples are randomly selected for evaluation. Five individuals were invited to rate the generated videos, and a higher score indicates better performance. From Table 3, we also observe T3M with EnCodec achieves a better performance over MFCC.",
            "We replace the EnCodec with Mel Frequency Cepstral Coefficients (MFCC) Zheng et al. (2001) and use the BCS and user study score (USS) to measure the effectiveness of generated motions. We report the results in Table 3. Comparing MFCC with EnCodec, we observe a noticeable performance improvement when utilizing EnCodec. Specifically, an increase of 0.0536 in BCS is observed with the usage of EnCodec. A user study was conducted to evaluate the motion score. A total of 20 samples are randomly selected for evaluation. Five individuals were invited to rate the generated videos, and a higher score indicates better performance. From Table 3, we also observe T3M with EnCodec achieves a better performance over MFCC."
        ]
    },
    "S4.T4": {
        "caption": "Table 4:  Ablation results to evaluate different context embeddings. \"Zero\" means using an all-0 vector as the context. We use the user study results to evaluate the generated motions.",
        "table": "<figure id=\"S4.T4\" class=\"ltx_table\">\n<table id=\"S4.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span id=\"S4.T4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th id=\"S4.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T4.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Motion Score</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">T3M (random)</th>\n<td id=\"S4.T4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">3.12</td>\n</tr>\n<tr id=\"S4.T4.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">T3M (zero)</th>\n<td id=\"S4.T4.1.3.2.2\" class=\"ltx_td ltx_align_center\">2.52</td>\n</tr>\n<tr id=\"S4.T4.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">T3M (text)</th>\n<td id=\"S4.T4.1.4.3.2\" class=\"ltx_td ltx_align_center\">3.69</td>\n</tr>\n<tr id=\"S4.T4.1.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">T3M (video)</th>\n<td id=\"S4.T4.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b\">3.85</td>\n</tr>\n</tbody>\n</table>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Ablation results to evaluate different context embeddings. \"Zero\" means using an all-0 vector as the context. We use the user study results to evaluate the generated motions.</figcaption>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We aim to investigate the impact of context feature over the synthesis effect. Particularly, we use four different types of embeddings to encode context: random prompt, text prompt, video prompt, zero prompt. For the text prompt, we use \"I am giving a speech, I feel really excite\". In contrast, for the zero prompt, we employ a context feature vector consisting entirely of zeros. We invite five individuals to rate ten videos which are generated from ten randomly selected speech samples. We present the USS in Table 4. We observe that text prompt and the video prompt both achieve better performance over random prompt and zero prompt."
        ]
    }
}