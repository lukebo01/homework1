{
    "S4.T1": {
        "caption": "Table 1:  Comparison of the root mean squared error of the U-Net models trained using L2 loss, KL-divergence loss, and KL-divergence loss with articulatory weighting. More details are available in Section\u00a0 4.1 .",
        "table": "<figure id=\"S4.T1\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Comparison of the root mean squared error of the U-Net models trained using L2 loss, KL-divergence loss, and KL-divergence loss with articulatory weighting. More details are available in Section\u00a0<a href=\"#S4.SS1\" title=\"4.1 Vision-only U-Net \u2023 4 Results \u2023 Multimodal Segmentation for Vocal Tract Modeling\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>.</figcaption>\n<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Loss</span></th>\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">RMSE</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">MSE (L2)</td>\n<td id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">7.33</td>\n</tr>\n<tr id=\"S4.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.3.2.1\" class=\"ltx_td ltx_align_center\">KL-div</td>\n<td id=\"S4.T1.1.3.2.2\" class=\"ltx_td ltx_align_center\">3.74</td>\n</tr>\n<tr id=\"S4.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">KL-div + Weighting</td>\n<td id=\"S4.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">3.92</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "The first experiment compared L2 (mean squared error) loss against our new pixel-wise KL-divergence loss with and without articulatory weighting for the U-Net model. This was evaluated using the root mean squared error (RMSE) of the predicted x-y points for the 95 articulator points on an unseen speaker. The results in Table 1 demonstrate that the KL-divergence loss is better suited for low-resolution point recognition for air-tissue boundary segmentation. As RMSE and MSE have the same convergence point, articulatory weighting predictably appears worse using this metric. However, manual inspection reveals that most of this error can be attributed to shifts in less phonologically important articulators such as the hard palate, with significant improvement on the more important articulators."
        ]
    },
    "S4.T2": {
        "caption": "Table 2:  Speech synthesis ASR WER finetuning on segmentations from a seen speaker during segmentation model training, but unseen utterances.\n(S) denotes synthesis model pretrained using single\nMRI speaker.\nAll other models are pretrained with 75-speaker MRI.",
        "table": "<figure id=\"S4.T2\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Speech synthesis ASR WER finetuning on segmentations from a seen speaker during segmentation model training, but unseen utterances.\n(S) denotes synthesis model pretrained using single\nMRI speaker.\nAll other models are pretrained with 75-speaker MRI.</figcaption>\n<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">WER [%]</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">U-Net + WavLM</td>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T2.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">31.3% (16.4%-49.3%)</span></td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_left\">U-Net</td>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_left\">36.4% (20.9%-55.1%)</td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_left\">Ground Truth</td>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_left\">34.7% (18.6%-53.2%)</td>\n</tr>\n<tr id=\"S4.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">U-Net + WavLM (S)</td>\n<td id=\"S4.T2.1.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">34.9% (20.3%-52.8%)</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": []
    },
    "S4.T3": {
        "caption": "Table 3:  Speech synthesis ASR WER finetuning on segmentations from an unseen speaker during segmentation model training.\n(S) denotes synthesis model pretrained using single\nMRI speaker.\nAll other models are pretrained with 75-speaker MRI.",
        "table": "<figure id=\"S4.T3\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Speech synthesis ASR WER finetuning on segmentations from an unseen speaker during segmentation model training.\n(S) denotes synthesis model pretrained using single\nMRI speaker.\nAll other models are pretrained with 75-speaker MRI.</figcaption>\n<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">WER [%]</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">U-Net + WavLM</td>\n<td id=\"S4.T3.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T3.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">33.3% (20.2%-49.8%)</span></td>\n</tr>\n<tr id=\"S4.T3.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.3.2.1\" class=\"ltx_td ltx_align_left\">U-Net</td>\n<td id=\"S4.T3.1.3.2.2\" class=\"ltx_td ltx_align_left\">35.2% (17.2%-56.8%)</td>\n</tr>\n<tr id=\"S4.T3.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.4.3.1\" class=\"ltx_td ltx_align_left\">Ground Truth</td>\n<td id=\"S4.T3.1.4.3.2\" class=\"ltx_td ltx_align_left\">49.7% (34.8%-66.6%)</td>\n</tr>\n<tr id=\"S4.T3.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">U-Net + WavLM (S)</td>\n<td id=\"S4.T3.1.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">50.1% (28.0%-72.8%)</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Similarly, we evaluate our segmentation methods on downstream speech tasks using speech synthesis\nwithin seen and unseen speaker contexts. Using the state-of-the-art MRI synthesis model [29] pretrained on\nthe newly-labeled 75-speaker dataset, we finetune on the projected MRI trajectories of a USC-TIMIT speaker provided by the different feature extraction models (i.e. baseline, U-Net, and multimodal). To evaluate the intelligibility of synthesized speech, we compute the word error\nrate (WER) on test unseen examples using Whisper [30], a\nstate-of-the-art automatic speech recognition (ASR) model. For seen speakers, speech synthesized using the multimodal U-Net + WavLM based segmentations is more intelligible than speech synthesized from either\nthe ground truth baseline or the U-Net outputs, suggesting that the addition\nof the speech modality helps preserves more speech-related information within the\npredicted MRI point trajectories compared to a purely image-based approach. Table 2\nsummarizes these results. The results in Table 3 highlight that\nthe U-Net + WavLM based model has the lowest WER when testing on an\nunseen USC-TIMIT speaker, documenting that the segmentations from the multimodal model on unseen speakers still capture representative articulatory kinematics for naturalistic speech. Pretraining the synthesis model on the 75 speakers also results in much better unseen speaker\ngeneralization, demonstrating that the new labels for the Speech MRI Open Dataset are beneficial for\nfuture work in articulatory speech."
        ]
    }
}