{
    "S4.T1": {
        "caption": "Table 1:  Transducer models used for evaluation",
        "table": "<figure id=\"S4.T1\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Transducer models used for evaluation</figcaption>\n<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">model</th>\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">model type</th>\n<th id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"># params (millions)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">RNNT-L</td>\n<td id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Transducer</td>\n<td id=\"S4.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">114</td>\n</tr>\n<tr id=\"S4.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.3.2.1\" class=\"ltx_td ltx_align_center\">RNNT-XXL</td>\n<td id=\"S4.T1.1.3.2.2\" class=\"ltx_td ltx_align_center\">Transducer</td>\n<td id=\"S4.T1.1.3.2.3\" class=\"ltx_td ltx_align_center\">1100</td>\n</tr>\n<tr id=\"S4.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.4.3.1\" class=\"ltx_td ltx_align_center\">TDT-L</td>\n<td id=\"S4.T1.1.4.3.2\" class=\"ltx_td ltx_align_center\">TDT</td>\n<td id=\"S4.T1.1.4.3.3\" class=\"ltx_td ltx_align_center\">114</td>\n</tr>\n<tr id=\"S4.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">TDT-XXL</td>\n<td id=\"S4.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">TDT</td>\n<td id=\"S4.T1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">1100</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We compared our label-looping algorithm against the baseline algorithm shown in Algorithm 2.\nAll experiments are conducted with the NeMo [1] toolkit.\nTable 1 shows the information of models that we use for evaluation. All these models are publicly available.\nThey use 8X subsampling in its Fast-Conformer [18] encoder's output, and 1024 BPE [19] vocabulary size at the output side. 111The model names are stt_en_fastconformer_transducer_large, parakeet-rnnt-1.1b, stt_en_fastconformer_tdt_large, parakeet-tdt-1.1b respectively. Modes can be accessed by appending the model name after https://huggingface.co/nvidia/.\nFor each model, we report its inversed real-time factor 222defined as the ratio of audio length to its decoding time. E.g. if it takes 1 second to decode a 2-second audio, then RTFx = 2. (RTFx) on the LibriSpeech [20] test-other dataset. To provide a better picture, we also include the RTFx for time spent in decoding only, excluding encoder computations, since the latter can more directly reflect the speed-up brought by our algorithms. To get a more accurate measurement, we first run decoding twice to ``warm up'' the cache required for those algorithms to work more efficiently, and report the average time from the third to fifth measurement. All experiments are done on NVIDIA RTX A6000 GPU."
        ]
    },
    "S4.T2": {
        "caption": "Table 2:  Transducer (RNN-T) decoding speed of different algorithms and batch-sizes on LibriSpeech test-other. We report ``total computation RTFx / non-encoder RTFx'', where RTF is the inversed real-time factor. Bf16 precision is used for all models. WER is 3.9% for the L model, 2.7% for the XXL model.",
        "table": "<figure id=\"S4.T2\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Transducer (RNN-T) decoding speed of different algorithms and batch-sizes on LibriSpeech test-other. We report ``total computation RTFx / non-encoder RTFx'', where RTF is the inversed real-time factor. Bf16 precision is used for all models. WER is 3.9% for the L model, 2.7% for the XXL model.</figcaption>\n<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">model-batch</th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">baseline</th>\n<th id=\"S4.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">ours</th>\n<th id=\"S4.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">rel. speedup</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">L-1</td>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">70.1 / 105.6</td>\n<td id=\"S4.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">95.5 / 179.0</td>\n<td id=\"S4.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1.4X / 1.7X</td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_center\">L-4</td>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center\">158.4 / 200.1</td>\n<td id=\"S4.T2.1.3.2.3\" class=\"ltx_td ltx_align_center\">257.1 / 385.4</td>\n<td id=\"S4.T2.1.3.2.4\" class=\"ltx_td ltx_align_center\">1.6X / 1.9X</td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_center\">L-16</td>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_center\">313.7 / 390.1</td>\n<td id=\"S4.T2.1.4.3.3\" class=\"ltx_td ltx_align_center\">584.5 / 894.4</td>\n<td id=\"S4.T2.1.4.3.4\" class=\"ltx_td ltx_align_center\">1.9X / 2.3X</td>\n</tr>\n<tr id=\"S4.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.5.4.1\" class=\"ltx_td ltx_align_center\">L-32</td>\n<td id=\"S4.T2.1.5.4.2\" class=\"ltx_td ltx_align_center\">383.8 / 512.8</td>\n<td id=\"S4.T2.1.5.4.3\" class=\"ltx_td ltx_align_center\">751.2 / 1403.6</td>\n<td id=\"S4.T2.1.5.4.4\" class=\"ltx_td ltx_align_center\">2.0X / 2.7X</td>\n</tr>\n<tr id=\"S4.T2.1.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\">XXL-1</td>\n<td id=\"S4.T2.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">47.2 / 98.8</td>\n<td id=\"S4.T2.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">59.3 / 174.3</td>\n<td id=\"S4.T2.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1.3X / 1.8X</td>\n</tr>\n<tr id=\"S4.T2.1.7.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.7.6.1\" class=\"ltx_td ltx_align_center\">XXL-4</td>\n<td id=\"S4.T2.1.7.6.2\" class=\"ltx_td ltx_align_center\">123.9 / 191.3</td>\n<td id=\"S4.T2.1.7.6.3\" class=\"ltx_td ltx_align_center\">183.1 / 380.0</td>\n<td id=\"S4.T2.1.7.6.4\" class=\"ltx_td ltx_align_center\">1.5X / 2.0X</td>\n</tr>\n<tr id=\"S4.T2.1.8.7\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.8.7.1\" class=\"ltx_td ltx_align_center\">XXL-16</td>\n<td id=\"S4.T2.1.8.7.2\" class=\"ltx_td ltx_align_center\">218.0 / 388.5</td>\n<td id=\"S4.T2.1.8.7.3\" class=\"ltx_td ltx_align_center\">321.6 / 886.2</td>\n<td id=\"S4.T2.1.8.7.4\" class=\"ltx_td ltx_align_center\">1.5X / 2.3X</td>\n</tr>\n<tr id=\"S4.T2.1.9.8\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">XXL-32</td>\n<td id=\"S4.T2.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">248.4 / 532.7</td>\n<td id=\"S4.T2.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">352.2 / 1393.4</td>\n<td id=\"S4.T2.1.9.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">1.4X / 2.6X</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Table 2 presents our results with conventional Transducers.\nWe see the label-looping algorithm consistently speeds up decoding regardless of batch size. Also, a larger relative speedup can be seen for larger batch sizes. This is because, as the batch size grows, the baseline algorithm introduces more overhead, since the more utterances, the more likely one utterance would need to wait for other utterances to advance t\ud835\udc61t in decoding. We also observe greater speed-up for non-encoder computations. Notably, for both models, our algorithm brings around 2.6X speed-up for batch-size=32 on non-encoder computations."
        ]
    },
    "S4.T3": {
        "caption": "Table 3:  TDT decoding speed of different algorithms with different batchsizes on LibriSpeech test-other. Total RTFx / non-encoder RTFx. WER is 3.7% for the L model, 2.8% for XXL for our label-looping algorithm; for baseline, the WER fluctuates.",
        "table": "<figure id=\"S4.T3\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>TDT decoding speed of different algorithms with different batchsizes on LibriSpeech test-other. Total RTFx / non-encoder RTFx. WER is 3.7% for the L model, 2.8% for XXL for our label-looping algorithm; for baseline, the WER fluctuates.</figcaption>\n<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">model name</th>\n<th id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">baseline</th>\n<th id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">ours</th>\n<th id=\"S4.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">rel. speedup</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">L-1</td>\n<td id=\"S4.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">116.0 / 264.5</td>\n<td id=\"S4.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">116.5 / 269.3</td>\n<td id=\"S4.T3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1.0X / 1.0X</td>\n</tr>\n<tr id=\"S4.T3.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.3.2.1\" class=\"ltx_td ltx_align_center\">L-4</td>\n<td id=\"S4.T3.1.3.2.2\" class=\"ltx_td ltx_align_center\">243.4 / 358.1</td>\n<td id=\"S4.T3.1.3.2.3\" class=\"ltx_td ltx_align_center\">347.7 / 632.6</td>\n<td id=\"S4.T3.1.3.2.4\" class=\"ltx_td ltx_align_center\">1.4X / 1.8X</td>\n</tr>\n<tr id=\"S4.T3.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.4.3.1\" class=\"ltx_td ltx_align_center\">L-16</td>\n<td id=\"S4.T3.1.4.3.2\" class=\"ltx_td ltx_align_center\">416.2 / 563.9</td>\n<td id=\"S4.T3.1.4.3.3\" class=\"ltx_td ltx_align_center\">825.3 / 1615.9</td>\n<td id=\"S4.T3.1.4.3.4\" class=\"ltx_td ltx_align_center\">2.0X / 2.9X</td>\n</tr>\n<tr id=\"S4.T3.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.5.4.1\" class=\"ltx_td ltx_align_center\">L-32</td>\n<td id=\"S4.T3.1.5.4.2\" class=\"ltx_td ltx_align_center\">477.2 / 696.7</td>\n<td id=\"S4.T3.1.5.4.3\" class=\"ltx_td ltx_align_center\">1006.8 / 2634.2</td>\n<td id=\"S4.T3.1.5.4.4\" class=\"ltx_td ltx_align_center\">2.1X / 3.8X</td>\n</tr>\n<tr id=\"S4.T3.1.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\">XXL-1</td>\n<td id=\"S4.T3.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">66.1 / 241.3</td>\n<td id=\"S4.T3.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">66.1 / 251.7</td>\n<td id=\"S4.T3.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1.0X / 1.0X</td>\n</tr>\n<tr id=\"S4.T3.1.7.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.7.6.1\" class=\"ltx_td ltx_align_center\">XXL-4</td>\n<td id=\"S4.T3.1.7.6.2\" class=\"ltx_td ltx_align_center\">147.9 / 258.8</td>\n<td id=\"S4.T3.1.7.6.3\" class=\"ltx_td ltx_align_center\">219.5 / 584.5</td>\n<td id=\"S4.T3.1.7.6.4\" class=\"ltx_td ltx_align_center\">1.5X / 2.3X</td>\n</tr>\n<tr id=\"S4.T3.1.8.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.8.7.1\" class=\"ltx_td ltx_align_center\">XXL-16</td>\n<td id=\"S4.T3.1.8.7.2\" class=\"ltx_td ltx_align_center\">243.7 / 478.3</td>\n<td id=\"S4.T3.1.8.7.3\" class=\"ltx_td ltx_align_center\">374.8 / 1467.9</td>\n<td id=\"S4.T3.1.8.7.4\" class=\"ltx_td ltx_align_center\">1.5X / 3.1X</td>\n</tr>\n<tr id=\"S4.T3.1.9.8\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">XXL-32</td>\n<td id=\"S4.T3.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">265.2 / 616.3</td>\n<td id=\"S4.T3.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">394.0 / 2434.1</td>\n<td id=\"S4.T3.1.9.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">1.5X / 3.9X</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Our TDT model experiments are shown in Table 3. Note, [11] states that TDT's batched decoding is tricky, and proposes an approximate algorithm, which is not deterministic and requires special modifications during model training. At the time of writing, there is no open-sourced implementation of exact and general-purpose TDT batched decoding, and ours is the first such implementation available to the public.\nFor the baseline, we adopt the approximate method from [11], which takes the minimum of predicted durations in the batch for advancement. Note: this method has non-deterministic outputs and is only included here for time comparison purposes. We can see from the Table that greater speed-ups can be seen with TDT models than conventional Transducers, and we see over 3.8X speed-up for non-encoder computation for both L and XXL models."
        ]
    },
    "S5.T4": {
        "caption": "Table 4:  Decoding RTFx (excluding encoder) between precomputation of projections and on-the-fly projections. Decoded on LibriSpeech test-other with RNNT-Large, bf16 precision.",
        "table": "<figure id=\"S5.T4\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Decoding RTFx (excluding encoder) between precomputation of projections and on-the-fly projections. Decoded on LibriSpeech test-other with RNNT-Large, bf16 precision.</figcaption>\n<table id=\"S5.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\">batch-size</th>\n<th id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">w/o</th>\n<th id=\"S5.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">w/precomputation</th>\n<th id=\"S5.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">rel. speed-up</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">1</th>\n<td id=\"S5.T4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">147.7</td>\n<td id=\"S5.T4.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">179.0</td>\n<td id=\"S5.T4.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1.21X</td>\n</tr>\n<tr id=\"S5.T4.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">4</th>\n<td id=\"S5.T4.1.3.2.2\" class=\"ltx_td ltx_align_center\">303.8</td>\n<td id=\"S5.T4.1.3.2.3\" class=\"ltx_td ltx_align_center\">385.4</td>\n<td id=\"S5.T4.1.3.2.4\" class=\"ltx_td ltx_align_center\">1.27X</td>\n</tr>\n<tr id=\"S5.T4.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">32</th>\n<td id=\"S5.T4.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">1080.3</td>\n<td id=\"S5.T4.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">1403.6</td>\n<td id=\"S5.T4.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">1.30X</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "In this section, we study the effects of precomputing the encoder and decoder projections, where we compare the decoding RTFx with or without the precomputation of projections. The results are in Table 4. We see that overall, precomputing the encoder and decoder outputs brings over 20% speedup for the decoding process excluding encoder computations."
        ]
    },
    "S5.T5": {
        "caption": "Table 5:  Performance on conformer-large models with 4x subsampling, bf16 precision. We report the total / non-encoder RTFx for decoding Librispeech-testother.",
        "table": "<figure id=\"S5.T5\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Performance on conformer-large models with 4x subsampling, bf16 precision. We report the total / non-encoder RTFx for decoding Librispeech-testother.</figcaption>\n<table id=\"S5.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\">batch-size</th>\n<th id=\"S5.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">baseline</th>\n<th id=\"S5.T5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">ours</th>\n<th id=\"S5.T5.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">rel. speed-up</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">1</th>\n<td id=\"S5.T5.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">48.2 / 63.2</td>\n<td id=\"S5.T5.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">79.5 / 130.8</td>\n<td id=\"S5.T5.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1.6X / 2.1X</td>\n</tr>\n<tr id=\"S5.T5.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">4</th>\n<td id=\"S5.T5.1.3.2.2\" class=\"ltx_td ltx_align_center\">104.7 / 123.1</td>\n<td id=\"S5.T5.1.3.2.3\" class=\"ltx_td ltx_align_center\">192.9 / 262.3</td>\n<td id=\"S5.T5.1.3.2.4\" class=\"ltx_td ltx_align_center\">1.8X / 2.1X</td>\n</tr>\n<tr id=\"S5.T5.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">16</th>\n<td id=\"S5.T5.1.4.3.2\" class=\"ltx_td ltx_align_center\">187.2 / 243.1</td>\n<td id=\"S5.T5.1.4.3.3\" class=\"ltx_td ltx_align_center\">341.6 / 577.5</td>\n<td id=\"S5.T5.1.4.3.4\" class=\"ltx_td ltx_align_center\">1.8X / 2.4X</td>\n</tr>\n<tr id=\"S5.T5.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">32</th>\n<td id=\"S5.T5.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">226.0 / 328.7</td>\n<td id=\"S5.T5.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">399.0 / 866.2</td>\n<td id=\"S5.T5.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">1.8X / 2.6X</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "In all experiments we report above, the encoder uses 8X subsampling. Another commonly used subsampling rate is 4X and we test our model's speed on a publicly available model 333https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_en_conformer_transducer_large and show the results in Table 5.\nWe see that with 4X models, our algorithm also significantly improves inference speed, up to 1.8X for total runtime and 2.6X for non-encoder computation."
        ]
    },
    "S5.T6": {
        "caption": "Table 6:  Combining Label-looping with TorchScript or CUDA graphs. Batchsize = 32. Full-time RTFx / Non-encoder RTFx.",
        "table": "<figure id=\"S5.T6\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Combining Label-looping with TorchScript or CUDA graphs. Batchsize = 32. Full-time RTFx / Non-encoder RTFx.</figcaption>\n<table id=\"S5.T6.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T6.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">model</th>\n<th id=\"S5.T6.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">RNNT</th>\n<th id=\"S5.T6.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">TDT</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T6.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">label-looping</td>\n<td id=\"S5.T6.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">751.2 / 1403.6</td>\n<td id=\"S5.T6.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1006.8 / 2634.2</td>\n</tr>\n<tr id=\"S5.T6.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.3.2.1\" class=\"ltx_td ltx_align_center\">+ TorchScript</td>\n<td id=\"S5.T6.1.3.2.2\" class=\"ltx_td ltx_align_center\">882.1 / 1942.4</td>\n<td id=\"S5.T6.1.3.2.3\" class=\"ltx_td ltx_align_center\">1118.0 / 3628.2</td>\n</tr>\n<tr id=\"S5.T6.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">+ cudagraphs</td>\n<td id=\"S5.T6.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">1232.7 / 5197.2</td>\n<td id=\"S5.T6.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">1393.4 / 9614.8</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": []
    }
}