{
    "S4.T1": {
        "caption": "Table 1:  Summary of Word Error Rate (WER) of the two models with/without GPT in Fleurs and KSC test sets.",
        "table": "<figure id=\"S4.T1\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Summary of Word Error Rate (WER) of the two models with/without GPT in Fleurs and KSC test sets.</figcaption>\n<div id=\"S4.T1.1\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:261.3pt;height:85.5pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-6.9pt,2.3pt) scale(0.95,0.95) ;\">\n<table id=\"S4.T1.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S4.T1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th id=\"S4.T1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T1.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Fleurs WER(%)</span></th>\n<th id=\"S4.T1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T1.1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">KSC WER(%)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Whisper-base-KF</th>\n<td id=\"S4.T1.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">37.31</td>\n<td id=\"S4.T1.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">61.51</td>\n</tr>\n<tr id=\"S4.T1.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\u00a0\u00a0\u2003+GPT for decoding</th>\n<td id=\"S4.T1.1.1.3.2.2\" class=\"ltx_td ltx_align_center\">28.60</td>\n<td id=\"S4.T1.1.1.3.2.3\" class=\"ltx_td ltx_align_center\">50.53</td>\n</tr>\n<tr id=\"S4.T1.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Whisper-large</th>\n<td id=\"S4.T1.1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">43.58</td>\n<td id=\"S4.T1.1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">56.18</td>\n</tr>\n<tr id=\"S4.T1.1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">\u00a0\u00a0\u2003+GPT for decoding</th>\n<td id=\"S4.T1.1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">36.64</td>\n<td id=\"S4.T1.1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">49.24</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "After training the GPT model using text data, we conducted tests to measure its perplexity (ppl) on the Fleurs and KSC test sets. The perplexity results for the Fleurs and KSC test sets were 2.61 and 6.20, respectively. Furthermore, the trained GPT model was integrated into Whisper, and the decoding results of the Whisper-base-KF and whisper-large models on the Fleurs and KSC test sets are shown in Table 1."
        ]
    },
    "S4.T2": {
        "caption": "Table 2:  Results of models decoding with GPT, EOT Judgment Modification (EOT-JM), and Hallucination Penalty (HP) on Fleurs-test.",
        "table": "<figure id=\"S4.T2\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Results of models decoding with GPT, EOT Judgment Modification (EOT-JM), and Hallucination Penalty (HP) on Fleurs-test.</figcaption>\n<div id=\"S4.T2.1\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:246.5pt;height:153.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-6.5pt,4.1pt) scale(0.95,0.95) ;\">\n<table id=\"S4.T2.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S4.T2.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th id=\"S4.T2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">GPT</span></th>\n<th id=\"S4.T2.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">EOT-JM</span></th>\n<th id=\"S4.T2.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">HP</span></th>\n<th id=\"S4.T2.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.1.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">WER(%)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"4\"><span id=\"S4.T2.1.1.2.1.1.1\" class=\"ltx_text\">Whisper-base-KF</span></th>\n<td id=\"S4.T2.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\u2717</td>\n<td id=\"S4.T2.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">\u2717</td>\n<td id=\"S4.T2.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">\u2717</td>\n<td id=\"S4.T2.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">37.31</td>\n</tr>\n<tr id=\"S4.T2.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.3.2.1\" class=\"ltx_td ltx_align_center\">\u2713</td>\n<td id=\"S4.T2.1.1.3.2.2\" class=\"ltx_td ltx_align_center\">\u2717</td>\n<td id=\"S4.T2.1.1.3.2.3\" class=\"ltx_td ltx_align_center\">\u2717</td>\n<td id=\"S4.T2.1.1.3.2.4\" class=\"ltx_td ltx_align_center\">34.49</td>\n</tr>\n<tr id=\"S4.T2.1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.4.3.1\" class=\"ltx_td ltx_align_center\">\u2713</td>\n<td id=\"S4.T2.1.1.4.3.2\" class=\"ltx_td ltx_align_center\">\u2713</td>\n<td id=\"S4.T2.1.1.4.3.3\" class=\"ltx_td ltx_align_center\">\u2717</td>\n<td id=\"S4.T2.1.1.4.3.4\" class=\"ltx_td ltx_align_center\">28.78</td>\n</tr>\n<tr id=\"S4.T2.1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.5.4.1\" class=\"ltx_td ltx_align_center\">\u2713</td>\n<td id=\"S4.T2.1.1.5.4.2\" class=\"ltx_td ltx_align_center\">\u2713</td>\n<td id=\"S4.T2.1.1.5.4.3\" class=\"ltx_td ltx_align_center\">\u2713</td>\n<td id=\"S4.T2.1.1.5.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.1.1.5.4.4.1\" class=\"ltx_text ltx_font_bold\">28.60</span></td>\n</tr>\n<tr id=\"S4.T2.1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"4\"><span id=\"S4.T2.1.1.6.5.1.1\" class=\"ltx_text\">Whisper-large</span></th>\n<td id=\"S4.T2.1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\u2717</td>\n<td id=\"S4.T2.1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">\u2717</td>\n<td id=\"S4.T2.1.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">\u2717</td>\n<td id=\"S4.T2.1.1.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">43.58</td>\n</tr>\n<tr id=\"S4.T2.1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.7.6.1\" class=\"ltx_td ltx_align_center\">\u2713</td>\n<td id=\"S4.T2.1.1.7.6.2\" class=\"ltx_td ltx_align_center\">\u2717</td>\n<td id=\"S4.T2.1.1.7.6.3\" class=\"ltx_td ltx_align_center\">\u2717</td>\n<td id=\"S4.T2.1.1.7.6.4\" class=\"ltx_td ltx_align_center\">36.75</td>\n</tr>\n<tr id=\"S4.T2.1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.8.7.1\" class=\"ltx_td ltx_align_center\">\u2713</td>\n<td id=\"S4.T2.1.1.8.7.2\" class=\"ltx_td ltx_align_center\">\u2713</td>\n<td id=\"S4.T2.1.1.8.7.3\" class=\"ltx_td ltx_align_center\">\u2717</td>\n<td id=\"S4.T2.1.1.8.7.4\" class=\"ltx_td ltx_align_center\">36.68</td>\n</tr>\n<tr id=\"S4.T2.1.1.9.8\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">\u2713</td>\n<td id=\"S4.T2.1.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">\u2713</td>\n<td id=\"S4.T2.1.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">\u2713</td>\n<td id=\"S4.T2.1.1.9.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.1.9.8.4.1\" class=\"ltx_text ltx_font_bold\">36.64</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Table 2 presents the Word Error Rate (WER) of two models with each improvement step on Fleurs-test. For the EOT Judgment Modification (EOT-JM) and Hallucination Penalty (HP), the smaller-scale Whisper-base-KF model with a higher language model weight exhibits a greater decrease in WER, particularly for the former improvement. This suggests that smaller-scale models rely more on larger language models during decoding, even for judging endings. EOT-JM ensures that the ending of transcription relies on audio information, reducing the generation of nonexistent information in the audio by GPT, resulting in a significant reduction in WER. The overall effect of HP on WER is not significant because HP affects only the candidate options and average token log probability (ALP) in the final beam search after decoding all tokens. However, for high-priority samples with higher ALP, HP can have a substantial impact."
        ]
    },
    "S4.T3": {
        "caption": "Table 3:  Summary of WER for the Fleurs-test subset with a high average token log probability (ALP). The values highlighted in red represent the difference compared to the case without GPT.",
        "table": "<figure id=\"S4.T3\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Summary of WER for the Fleurs-test subset with a high average token log probability (ALP). The values highlighted in red represent the difference compared to the case without GPT.</figcaption>\n<div id=\"S4.T3.1\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:253.5pt;height:205.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-17.3pt,14.0pt) scale(0.88,0.88) ;\">\n<table id=\"S4.T3.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Samples WER(%)</span></td>\n<td id=\"S4.T3.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">All</span></td>\n<td id=\"S4.T3.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Top 20% ALP</span></td>\n<td id=\"S4.T3.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Top 50% ALP</span></td>\n</tr>\n<tr id=\"S4.T3.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"4\"><span id=\"S4.T3.1.1.2.2.1.1\" class=\"ltx_text ltx_font_bold\">Whisper-base-KF:</span></td>\n</tr>\n<tr id=\"S4.T3.1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">w/o GPT</td>\n<td id=\"S4.T3.1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">37.31</td>\n<td id=\"S4.T3.1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">22.17</td>\n<td id=\"S4.T3.1.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">29.32</td>\n</tr>\n<tr id=\"S4.T3.1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.4.4.1\" class=\"ltx_td ltx_align_center\" rowspan=\"2\"><span id=\"S4.T3.1.1.4.4.1.1\" class=\"ltx_text\">w/o HP</span></td>\n<td id=\"S4.T3.1.1.4.4.2\" class=\"ltx_td ltx_align_center\">28.78</td>\n<td id=\"S4.T3.1.1.4.4.3\" class=\"ltx_td ltx_align_center\">20.34</td>\n<td id=\"S4.T3.1.1.4.4.4\" class=\"ltx_td ltx_align_center\">21.18</td>\n</tr>\n<tr id=\"S4.T3.1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.5.5.1\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.1.1.5.5.1.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-8.53)</span></td>\n<td id=\"S4.T3.1.1.5.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.1.1.5.5.2.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-1.73)</span></td>\n<td id=\"S4.T3.1.1.5.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.1.1.5.5.3.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-8.14)</span></td>\n</tr>\n<tr id=\"S4.T3.1.1.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.6.6.1\" class=\"ltx_td ltx_align_center\" rowspan=\"2\"><span id=\"S4.T3.1.1.6.6.1.1\" class=\"ltx_text\">with HP</span></td>\n<td id=\"S4.T3.1.1.6.6.2\" class=\"ltx_td ltx_align_center\">28.60</td>\n<td id=\"S4.T3.1.1.6.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.1.1.6.6.3.1\" class=\"ltx_text ltx_font_bold\">12.42</span></td>\n<td id=\"S4.T3.1.1.6.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.1.1.6.6.4.1\" class=\"ltx_text ltx_font_bold\">18.07</span></td>\n</tr>\n<tr id=\"S4.T3.1.1.7.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.7.7.1\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.1.1.7.7.1.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-8.71)</span></td>\n<td id=\"S4.T3.1.1.7.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.1.1.7.7.2.1\" class=\"ltx_text ltx_font_bold\" style=\"color:#FF0000;\">(-9.75)</span></td>\n<td id=\"S4.T3.1.1.7.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.1.1.7.7.3.1\" class=\"ltx_text ltx_font_bold\" style=\"color:#FF0000;\">(-11.25)</span></td>\n</tr>\n<tr id=\"S4.T3.1.1.8.8\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"4\"><span id=\"S4.T3.1.1.8.8.1.1\" class=\"ltx_text ltx_font_bold\">Whisper-large:</span></td>\n</tr>\n<tr id=\"S4.T3.1.1.9.9\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_t\">w/o GPT</td>\n<td id=\"S4.T3.1.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_t\">43.58</td>\n<td id=\"S4.T3.1.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\">34.55</td>\n<td id=\"S4.T3.1.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\">38.14</td>\n</tr>\n<tr id=\"S4.T3.1.1.10.10\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.10.10.1\" class=\"ltx_td ltx_align_center\" rowspan=\"2\"><span id=\"S4.T3.1.1.10.10.1.1\" class=\"ltx_text\">w/o HP</span></td>\n<td id=\"S4.T3.1.1.10.10.2\" class=\"ltx_td ltx_align_center\">36.68</td>\n<td id=\"S4.T3.1.1.10.10.3\" class=\"ltx_td ltx_align_center\">28.96</td>\n<td id=\"S4.T3.1.1.10.10.4\" class=\"ltx_td ltx_align_center\">30.94</td>\n</tr>\n<tr id=\"S4.T3.1.1.11.11\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.11.11.1\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.1.1.11.11.1.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-6.90)</span></td>\n<td id=\"S4.T3.1.1.11.11.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.1.1.11.11.2.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-5.59)</span></td>\n<td id=\"S4.T3.1.1.11.11.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.1.1.11.11.3.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-7.20)</span></td>\n</tr>\n<tr id=\"S4.T3.1.1.12.12\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.12.12.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" rowspan=\"2\"><span id=\"S4.T3.1.1.12.12.1.1\" class=\"ltx_text\">with HP</span></td>\n<td id=\"S4.T3.1.1.12.12.2\" class=\"ltx_td ltx_align_center\">36.64</td>\n<td id=\"S4.T3.1.1.12.12.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.1.1.12.12.3.1\" class=\"ltx_text ltx_font_bold\">26.83</span></td>\n<td id=\"S4.T3.1.1.12.12.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.1.1.12.12.4.1\" class=\"ltx_text ltx_font_bold\">30.39</span></td>\n</tr>\n<tr id=\"S4.T3.1.1.13.13\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.13.13.1\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.1.1.13.13.1.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-6.94)</span></td>\n<td id=\"S4.T3.1.1.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.1.1.13.13.2.1\" class=\"ltx_text ltx_font_bold\" style=\"color:#FF0000;\">(-7.72)</span></td>\n<td id=\"S4.T3.1.1.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.1.1.13.13.3.1\" class=\"ltx_text ltx_font_bold\" style=\"color:#FF0000;\">(-7.75)</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "The system is able to calculate the average token log probability (ALP) for each sample during decoding, and ALP values are usually statistically correlated with the WER of the sample, as shown in Figure 2. However, when combined with GPT during decoding, there are some ``outliers\" in the left half, which corresponds to higher ALP values, indicating that these samples have a significantly higher WER. Upon examination, we found that these samples were trapped in hallucination, where a portion of the content was repeated incorrectly multiple times. However, after applying Hallucination Penalty (HP), the phenomenon of ``outliers\" is significantly mitigated. Table 3 provides a summary of the WER on the high ALP test subset, where high ALP samples are given higher priority in speech pseudo-label training. It can be seen that compared to not using GPT, selecting the high ALP subset results in a significant additional decrease in WER compared to selecting all samples, and HP plays a significant role in this improvement."
        ]
    },
    "S4.T4": {
        "caption": "Table 4:  Summary of the overall WER for systems leveraging unpaired speech and text data.",
        "table": "<figure id=\"S4.T4\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Summary of the overall WER for systems leveraging unpaired speech and text data.</figcaption>\n<div id=\"S4.T4.1\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:261.3pt;height:118.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-28.7pt,13.0pt) scale(0.82,0.82) ;\">\n<table id=\"S4.T4.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T4.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Systems</span></th>\n<td id=\"S4.T4.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T4.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Whisper-large</span></td>\n<td id=\"S4.T4.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T4.1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Whisper-base-KF</span></td>\n</tr>\n<tr id=\"S4.T4.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.1.2.2.1\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.1.1.2.2.1.1\" class=\"ltx_text ltx_font_bold\">Fleurs-test WER</span></td>\n<td id=\"S4.T4.1.1.2.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.1.1.2.2.2.1\" class=\"ltx_text ltx_font_bold\">KSC-test WER</span></td>\n</tr>\n<tr id=\"S4.T4.1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">1.Origin (baseline)</th>\n<td id=\"S4.T4.1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">43.58%</td>\n<td id=\"S4.T4.1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">61.51%</td>\n</tr>\n<tr id=\"S4.T4.1.1.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">2.(1)+GPT for decoding</th>\n<td id=\"S4.T4.1.1.4.4.2\" class=\"ltx_td ltx_align_center\">36.64%</td>\n<td id=\"S4.T4.1.1.4.4.3\" class=\"ltx_td ltx_align_center\">50.53%</td>\n</tr>\n<tr id=\"S4.T4.1.1.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">3.Pseudo-label fine-tuning</th>\n<td id=\"S4.T4.1.1.5.5.2\" class=\"ltx_td ltx_align_center\">35.79%</td>\n<td id=\"S4.T4.1.1.5.5.3\" class=\"ltx_td ltx_align_center\">48.66%</td>\n</tr>\n<tr id=\"S4.T4.1.1.6.6\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">4.(3)+GPT for decoding</th>\n<td id=\"S4.T4.1.1.6.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.1.1.6.6.2.1\" class=\"ltx_text ltx_font_bold\">32.36%</span></td>\n<td id=\"S4.T4.1.1.6.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.1.1.6.6.3.1\" class=\"ltx_text ltx_font_bold\">48.23%</span></td>\n</tr>\n<tr id=\"S4.T4.1.1.7.7\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">5.Reference label fine-tuning</th>\n<td id=\"S4.T4.1.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\">23.24%</td>\n<td id=\"S4.T4.1.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\">25.26%</td>\n</tr>\n<tr id=\"S4.T4.1.1.8.8\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Gap-filling Ratio</th>\n<td id=\"S4.T4.1.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.552</td>\n<td id=\"S4.T4.1.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.366</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Table 4 summarizes the WER of systems leveraging unpaired speech and text data. The selection of system (3) corresponds to the best result among all data selection proportions. Reference label fine-tuning is performed using the same amount of data as in system (3). The Gap-filling Ratio is the ratio of the reduction in difference between system (4) and system (5) compared to system (1), which is calculated as (WER.(1)-WER.(4))/(WER.(1)-WER.(5)). In Whisper-large, by leveraging text data and unlabeled Fleurs-train speech data, we achieved an absolute WER reduction of 11.24% on the in-domain test set. This method can achieve more than half the efficacy of reference labels, without incurring the associated human labor costs. Similarly, in Whisper-base-KF, by utilizing text data and unlabeled KSC-train speech data, we observed an absolute WER reduction of 13.28% on the in-domain test set. Even at higher scales of data, more than one-third of the performance of the reference label can be achieved using this pipeline, resulting in a significant reduction in WER for Whisper on Kazakh."
        ]
    }
}