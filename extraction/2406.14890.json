{
    "S4.T1": {
        "caption": "Table 1:  Summary of keyword set sizes and average number of characters for OOV and Non-OOV keywords in each testset.",
        "table": "<figure id=\"S4.T1\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Summary of keyword set sizes and average number of characters for OOV and Non-OOV keywords in each testset.</figcaption>\n<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.2\" class=\"ltx_td ltx_align_middle ltx_th ltx_th_row ltx_border_tt\" style=\"width:28.5pt;\"></th>\n<th id=\"S4.T1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T1.1.1.3.1\" class=\"ltx_text\">Dataset</span></th>\n<th id=\"S4.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"S4.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"|\\mathcal{K}|\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.m1.1a\"><mrow id=\"S4.T1.1.1.1.m1.1.2.2\" xref=\"S4.T1.1.1.1.m1.1.2.1.cmml\"><mo stretchy=\"false\" id=\"S4.T1.1.1.1.m1.1.2.2.1\" xref=\"S4.T1.1.1.1.m1.1.2.1.1.cmml\">|</mo><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.T1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.m1.1.1.cmml\">\ud835\udca6</mi><mo stretchy=\"false\" id=\"S4.T1.1.1.1.m1.1.2.2.2\" xref=\"S4.T1.1.1.1.m1.1.2.1.1.cmml\">|</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.m1.1b\"><apply id=\"S4.T1.1.1.1.m1.1.2.1.cmml\" xref=\"S4.T1.1.1.1.m1.1.2.2\"><abs id=\"S4.T1.1.1.1.m1.1.2.1.1.cmml\" xref=\"S4.T1.1.1.1.m1.1.2.2.1\"></abs><ci id=\"S4.T1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.m1.1.1\">\ud835\udca6</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.m1.1c\">|\\mathcal{K}|</annotation></semantics></math></th>\n<th id=\"S4.T1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Avg. char length</th>\n</tr>\n<tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_middle ltx_th ltx_th_row\" style=\"width:28.5pt;\"></th>\n<th id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">OOV / Non-OOV</th>\n<th id=\"S4.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">OOV / Non-OOV</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.3.1.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t\" style=\"width:28.5pt;\" rowspan=\"3\">\n<span id=\"S4.T1.1.3.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T1.1.3.1.1.1.1\" class=\"ltx_p\">In-domain</span>\n</span>\n</th>\n<td id=\"S4.T1.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">CSJ 1</td>\n<td id=\"S4.T1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">2 / 8</td>\n<td id=\"S4.T1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">5.0 / 3.1</td>\n</tr>\n<tr id=\"S4.T1.1.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.4.2.1\" class=\"ltx_td ltx_align_center\">CSJ 2</td>\n<td id=\"S4.T1.1.4.2.2\" class=\"ltx_td ltx_align_center\">4 / 3</td>\n<td id=\"S4.T1.1.4.2.3\" class=\"ltx_td ltx_align_center\">4.0 / 5.7</td>\n</tr>\n<tr id=\"S4.T1.1.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.5.3.1\" class=\"ltx_td ltx_align_center\">CSJ 3</td>\n<td id=\"S4.T1.1.5.3.2\" class=\"ltx_td ltx_align_center\">23 / 8</td>\n<td id=\"S4.T1.1.5.3.3\" class=\"ltx_td ltx_align_center\">4.3 / 3.1</td>\n</tr>\n<tr id=\"S4.T1.1.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.6.4.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_bb\" style=\"width:28.5pt;\" rowspan=\"3\">\n<span id=\"S4.T1.1.6.4.1.1\" class=\"ltx_inline-block ltx_align_top\"><span id=\"S4.T1.1.6.4.1.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span>\n<span id=\"S4.T1.1.6.4.1.1.2\" class=\"ltx_p\">Out-of-domain</span>\n</span>\n</th>\n<td id=\"S4.T1.1.6.4.2\" class=\"ltx_td ltx_align_center\">CV</td>\n<td id=\"S4.T1.1.6.4.3\" class=\"ltx_td ltx_align_center\">59 / 54</td>\n<td id=\"S4.T1.1.6.4.4\" class=\"ltx_td ltx_align_center\">5.6 / 4.3</td>\n</tr>\n<tr id=\"S4.T1.1.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.7.5.1\" class=\"ltx_td ltx_align_center\">JSUT</td>\n<td id=\"S4.T1.1.7.5.2\" class=\"ltx_td ltx_align_center\">212 / 103</td>\n<td id=\"S4.T1.1.7.5.3\" class=\"ltx_td ltx_align_center\">3.7 / 2.8</td>\n</tr>\n<tr id=\"S4.T1.1.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">TED</td>\n<td id=\"S4.T1.1.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">99 / 78</td>\n<td id=\"S4.T1.1.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">4.0 / 3.6</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Here, we describe the process of generating bias keywords for our evaluation experiments.\nInitially, we performed speech recognition on each evaluation set using a model trained on the CSJ dataset.\nBy comparing the labels and recognition hypotheses, we identified words that were incorrectly recognized.\nIn this process, word segmentation was conducted using morphological analysis with MeCab [29].\nFrom the extracted misrecognized words, we only retained proper nouns and personal names with two characters or more using morphological labels.\nIn the final stage, we manually removed clear morphological analysis errors from the extracted set of bias keywords.\nWe classified all bias keywords into out-of-vocabulary (OOV) keywords and Non-OOV keywords based on whether they belong to the vocabulary in the CSJ training data.\nThe number of OOV and Non-OOV keywords in each evaluation set is shown in Table 1.",
            "Table 4 summarizes the experimental results.\nFirst, in greedy decoding, it can be seen that the TextSub significantly degraded the F1 score of Non-OOV words in TEDxJP-10k.\nThe TextSub frequently overcorrected recognition results and was less likely to hit the trigger words since they were not always present in the final outputs.\nIn contrast, the InterBiasing had more chances to hit trigger words because text substitution was applied for intermediate predictions on multiple layers.\nCompared to SelfCond and the TextSub, the experimental results show a large improvement of F1 scores of the OOV words, and less degradation of CERs with the proposed InterBiasing.\nIt should be noted that the large fluctuations in the results for CSJ eval1 and eval2 scores have little significance due to the low number of keywords being evaluated, as shown in Table 1."
        ]
    },
    "S4.tab1": {
        "caption": "Table 2:  CERs and F1 scores of Out-of-Vocabulary (OOV) and Non-OOV words in CSJ eval1, eval2, eval3, Common Voice, JSUT basic 5000 and TEDxJP-10K. Reported metrics are in the following format: CER / F1 of OOV words / F1 of Non-OOV words. Figure 2:  F1 scores of Out-of-Vocabulary (OOV) and Non-OOV words in JSUT basic 5000 with different beam sizes. LM shallow fusion and Keyword-boosted Beam Search (KBBS) were utilized. Beam size was set to 2, 3, 5, 10, 20, respectively.",
        "table": "<figure id=\"S4.tab1\" class=\"ltx_table\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>CERs and F1 scores of Out-of-Vocabulary (OOV) and Non-OOV words in CSJ eval1, eval2, eval3, Common Voice, JSUT basic 5000 and TEDxJP-10K. Reported metrics are in the following format: CER / F1 of OOV words / F1 of Non-OOV words.</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<table id=\"S4.tab1.1\" class=\"ltx_tabular ltx_figure_panel ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.tab1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.tab1.1.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S4.tab1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\">CER (%) / OOV F1 (%) / Non-OOV F1 (%)</td>\n<td id=\"S4.tab1.1.1.1.3\" class=\"ltx_td ltx_border_tt\"></td>\n</tr>\n<tr id=\"S4.tab1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.tab1.1.2.2.1\" class=\"ltx_td ltx_align_left\">Methods</td>\n<td id=\"S4.tab1.1.2.2.2\" class=\"ltx_td ltx_align_center\">CSJ eval1</td>\n<td id=\"S4.tab1.1.2.2.3\" class=\"ltx_td ltx_align_center\">CSJ eval2</td>\n<td id=\"S4.tab1.1.2.2.4\" class=\"ltx_td ltx_align_center\">CSJ eval3</td>\n<td id=\"S4.tab1.1.2.2.5\" class=\"ltx_td ltx_align_center\">Common Voice</td>\n<td id=\"S4.tab1.1.2.2.6\" class=\"ltx_td ltx_align_center\">JSUT basic 5000</td>\n<td id=\"S4.tab1.1.2.2.7\" class=\"ltx_td ltx_align_center\">TEDxJP-10K</td>\n</tr>\n<tr id=\"S4.tab1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S4.tab1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"4\"><span id=\"S4.tab1.1.3.3.1.1\" class=\"ltx_text ltx_font_bold\">Greedy decoding</span></td>\n<td id=\"S4.tab1.1.3.3.2\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.tab1.1.3.3.3\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.tab1.1.3.3.4\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S4.tab1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S4.tab1.1.4.4.1\" class=\"ltx_td ltx_align_left\">SelfCond</td>\n<td id=\"S4.tab1.1.4.4.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.4.4.2.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">4.6</span> / <span id=\"S4.tab1.1.4.4.2.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">75.0</span> / 88.9</td>\n<td id=\"S4.tab1.1.4.4.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.4.4.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">3.7</span> / 0.0 / <span id=\"S4.tab1.1.4.4.3.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span>\n</td>\n<td id=\"S4.tab1.1.4.4.4\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.4.4.4.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">3.4</span> / 6.2 / 69.8</td>\n<td id=\"S4.tab1.1.4.4.5\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.4.4.5.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">19.0</span> / 18.6 / 81.4</td>\n<td id=\"S4.tab1.1.4.4.6\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.4.4.6.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">11.7</span> / 9.2 / 54.6</td>\n<td id=\"S4.tab1.1.4.4.7\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.4.4.7.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">16.1</span> / 12.0 / 85.3</td>\n</tr>\n<tr id=\"S4.tab1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S4.tab1.1.5.5.1\" class=\"ltx_td ltx_align_left\">TextSub</td>\n<td id=\"S4.tab1.1.5.5.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.5.5.2.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">4.6</span> / <span id=\"S4.tab1.1.5.5.2.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">75.0</span> / 90.1</td>\n<td id=\"S4.tab1.1.5.5.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.5.5.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">3.7</span> / 0.0 / <span id=\"S4.tab1.1.5.5.3.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span>\n</td>\n<td id=\"S4.tab1.1.5.5.4\" class=\"ltx_td ltx_align_center\">3.6 / 9.1 / 72.7</td>\n<td id=\"S4.tab1.1.5.5.5\" class=\"ltx_td ltx_align_center\">19.9 / 18.6 / 82.3</td>\n<td id=\"S4.tab1.1.5.5.6\" class=\"ltx_td ltx_align_center\">12.0 / 10.0 / 56.3</td>\n<td id=\"S4.tab1.1.5.5.7\" class=\"ltx_td ltx_align_center\">16.2 / <span id=\"S4.tab1.1.5.5.7.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">13.1</span> / 19.4</td>\n</tr>\n<tr id=\"S4.tab1.1.6.6\" class=\"ltx_tr\">\n<td id=\"S4.tab1.1.6.6.1\" class=\"ltx_td ltx_align_left\">InterBiasing</td>\n<td id=\"S4.tab1.1.6.6.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.6.6.2.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">4.6</span> / <span id=\"S4.tab1.1.6.6.2.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">75.0</span> / 92.3</td>\n<td id=\"S4.tab1.1.6.6.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.6.6.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">3.7</span> / <span id=\"S4.tab1.1.6.6.3.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">40.0</span> / <span id=\"S4.tab1.1.6.6.3.3\" class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span>\n</td>\n<td id=\"S4.tab1.1.6.6.4\" class=\"ltx_td ltx_align_center\">3.5 / <span id=\"S4.tab1.1.6.6.4.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">27.4</span> / <span id=\"S4.tab1.1.6.6.4.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">75.6</span>\n</td>\n<td id=\"S4.tab1.1.6.6.5\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.6.6.5.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">19.0</span> / <span id=\"S4.tab1.1.6.6.5.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">22.7</span> / <span id=\"S4.tab1.1.6.6.5.3\" class=\"ltx_text ltx_framed ltx_framed_underline\">82.9</span>\n</td>\n<td id=\"S4.tab1.1.6.6.6\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.6.6.6.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">11.7</span> / <span id=\"S4.tab1.1.6.6.6.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">13.5</span> / <span id=\"S4.tab1.1.6.6.6.3\" class=\"ltx_text ltx_framed ltx_framed_underline\">59.5</span>\n</td>\n<td id=\"S4.tab1.1.6.6.7\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.6.6.7.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">16.1</span> / <span id=\"S4.tab1.1.6.6.7.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">13.1</span> / <span id=\"S4.tab1.1.6.6.7.3\" class=\"ltx_text ltx_framed ltx_framed_underline\">85.9</span>\n</td>\n</tr>\n<tr id=\"S4.tab1.1.7.7\" class=\"ltx_tr\">\n<td id=\"S4.tab1.1.7.7.1\" class=\"ltx_td ltx_align_left\" colspan=\"4\">\n<span id=\"S4.tab1.1.7.7.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S4.tab1.1.7.7.1.2\" class=\"ltx_text ltx_font_bold\">LM shallow fusion + Beam Search decoding</span>\n</td>\n<td id=\"S4.tab1.1.7.7.2\" class=\"ltx_td\"></td>\n<td id=\"S4.tab1.1.7.7.3\" class=\"ltx_td\"></td>\n<td id=\"S4.tab1.1.7.7.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S4.tab1.1.8.8\" class=\"ltx_tr\">\n<td id=\"S4.tab1.1.8.8.1\" class=\"ltx_td ltx_align_left\">SelfCond</td>\n<td id=\"S4.tab1.1.8.8.2\" class=\"ltx_td ltx_align_center\">4.5 / <span id=\"S4.tab1.1.8.8.2.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">82.4</span> / 90.1</td>\n<td id=\"S4.tab1.1.8.8.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.8.8.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">3.7</span> / <span id=\"S4.tab1.1.8.8.3.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span> / <span id=\"S4.tab1.1.8.8.3.3\" class=\"ltx_text ltx_framed ltx_framed_underline\">33.3</span>\n</td>\n<td id=\"S4.tab1.1.8.8.4\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.8.8.4.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">3.4</span> / 9.2 / 72.7</td>\n<td id=\"S4.tab1.1.8.8.5\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.8.8.5.1\" class=\"ltx_text ltx_font_bold\">17.3</span> / <span id=\"S4.tab1.1.8.8.5.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">18.6</span> / 82.8</td>\n<td id=\"S4.tab1.1.8.8.6\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.8.8.6.1\" class=\"ltx_text ltx_font_bold\">11.5</span> / 9.2 / 54.9</td>\n<td id=\"S4.tab1.1.8.8.7\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.8.8.7.1\" class=\"ltx_text ltx_font_bold\">15.8</span> / 12.0 / 85.6</td>\n</tr>\n<tr id=\"S4.tab1.1.9.9\" class=\"ltx_tr\">\n<td id=\"S4.tab1.1.9.9.1\" class=\"ltx_td ltx_align_left\">TextSub</td>\n<td id=\"S4.tab1.1.9.9.2\" class=\"ltx_td ltx_align_center\">4.5 / 77.8 / 15.8</td>\n<td id=\"S4.tab1.1.9.9.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.9.9.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">3.7</span> / <span id=\"S4.tab1.1.9.9.3.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span> / <span id=\"S4.tab1.1.9.9.3.3\" class=\"ltx_text ltx_framed ltx_framed_underline\">33.3</span>\n</td>\n<td id=\"S4.tab1.1.9.9.4\" class=\"ltx_td ltx_align_center\">3.5 / 12.1 / 73.7</td>\n<td id=\"S4.tab1.1.9.9.5\" class=\"ltx_td ltx_align_center\">18.1 / 10.7 / 79.2</td>\n<td id=\"S4.tab1.1.9.9.6\" class=\"ltx_td ltx_align_center\">13.0 / 8.8 / 20.4</td>\n<td id=\"S4.tab1.1.9.9.7\" class=\"ltx_td ltx_align_center\">16.7 / 8.3 / 8.3</td>\n</tr>\n<tr id=\"S4.tab1.1.10.10\" class=\"ltx_tr\">\n<td id=\"S4.tab1.1.10.10.1\" class=\"ltx_td ltx_align_left\">InterBiasing</td>\n<td id=\"S4.tab1.1.10.10.2\" class=\"ltx_td ltx_align_center\">4.5 / <span id=\"S4.tab1.1.10.10.2.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">82.4</span> / <span id=\"S4.tab1.1.10.10.2.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">93.5</span>\n</td>\n<td id=\"S4.tab1.1.10.10.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.10.10.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">3.7</span> / <span id=\"S4.tab1.1.10.10.3.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span> / <span id=\"S4.tab1.1.10.10.3.3\" class=\"ltx_text ltx_framed ltx_framed_underline\">33.3</span>\n</td>\n<td id=\"S4.tab1.1.10.10.4\" class=\"ltx_td ltx_align_center\">3.5 / <span id=\"S4.tab1.1.10.10.4.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">36.8</span> / <span id=\"S4.tab1.1.10.10.4.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">80.9</span>\n</td>\n<td id=\"S4.tab1.1.10.10.5\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.10.10.5.1\" class=\"ltx_text ltx_font_bold\">17.3</span> / <span id=\"S4.tab1.1.10.10.5.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">18.6</span> / <span id=\"S4.tab1.1.10.10.5.3\" class=\"ltx_text ltx_framed ltx_framed_underline\">84.0</span>\n</td>\n<td id=\"S4.tab1.1.10.10.6\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.10.10.6.1\" class=\"ltx_text ltx_font_bold\">11.5</span> / <span id=\"S4.tab1.1.10.10.6.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">12.1</span> / <span id=\"S4.tab1.1.10.10.6.3\" class=\"ltx_text ltx_framed ltx_framed_underline\">59.8</span>\n</td>\n<td id=\"S4.tab1.1.10.10.7\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.10.10.7.1\" class=\"ltx_text ltx_font_bold\">15.8</span> / <span id=\"S4.tab1.1.10.10.7.2\" class=\"ltx_text ltx_framed ltx_framed_underline\">14.2</span> / <span id=\"S4.tab1.1.10.10.7.3\" class=\"ltx_text ltx_framed ltx_framed_underline\">86.3</span>\n</td>\n</tr>\n<tr id=\"S4.tab1.1.11.11\" class=\"ltx_tr\">\n<td id=\"S4.tab1.1.11.11.1\" class=\"ltx_td ltx_align_left\" colspan=\"4\">\n<span id=\"S4.tab1.1.11.11.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span> \u00a0\u00a0\u00a0\u00a0<span id=\"S4.tab1.1.11.11.1.2\" class=\"ltx_text ltx_font_bold\">LM shallow fusion + Keyword-boosted Beam Search decoding</span>\n</td>\n<td id=\"S4.tab1.1.11.11.2\" class=\"ltx_td\"></td>\n<td id=\"S4.tab1.1.11.11.3\" class=\"ltx_td\"></td>\n<td id=\"S4.tab1.1.11.11.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S4.tab1.1.12.12\" class=\"ltx_tr\">\n<td id=\"S4.tab1.1.12.12.1\" class=\"ltx_td ltx_align_left\">SelfCond</td>\n<td id=\"S4.tab1.1.12.12.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.12.12.2.1\" class=\"ltx_text ltx_font_bold\">3.9</span> / <span id=\"S4.tab1.1.12.12.2.2\" class=\"ltx_text ltx_font_bold\">88.9</span> / 90.3</td>\n<td id=\"S4.tab1.1.12.12.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.12.12.3.1\" class=\"ltx_text ltx_font_bold\">2.8</span> / 0.0 / <span id=\"S4.tab1.1.12.12.3.2\" class=\"ltx_text ltx_font_bold\">75.0</span>\n</td>\n<td id=\"S4.tab1.1.12.12.4\" class=\"ltx_td ltx_align_center\">3.4 / 29.3 / 75.6</td>\n<td id=\"S4.tab1.1.12.12.5\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.12.12.5.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">17.7</span> / 50.9 / 85.1</td>\n<td id=\"S4.tab1.1.12.12.6\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.tab1.1.12.12.6.1\" class=\"ltx_text ltx_font_bold\">11.5</span> / 33.9 / 67.5</td>\n<td id=\"S4.tab1.1.12.12.7\" class=\"ltx_td ltx_align_center\">15.9 / 27.7 / 86.8</td>\n</tr>\n<tr id=\"S4.tab1.1.13.13\" class=\"ltx_tr\">\n<td id=\"S4.tab1.1.13.13.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">InterBiasing</td>\n<td id=\"S4.tab1.1.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S4.tab1.1.13.13.2.1\" class=\"ltx_text ltx_font_bold\">3.9</span> / <span id=\"S4.tab1.1.13.13.2.2\" class=\"ltx_text ltx_font_bold\">88.9</span> / <span id=\"S4.tab1.1.13.13.2.3\" class=\"ltx_text ltx_font_bold\">94.7</span>\n</td>\n<td id=\"S4.tab1.1.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S4.tab1.1.13.13.3.1\" class=\"ltx_text ltx_font_bold\">2.8</span> / <span id=\"S4.tab1.1.13.13.3.2\" class=\"ltx_text ltx_font_bold\">40.0</span> / <span id=\"S4.tab1.1.13.13.3.3\" class=\"ltx_text ltx_font_bold\">75.0</span>\n</td>\n<td id=\"S4.tab1.1.13.13.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S4.tab1.1.13.13.4.1\" class=\"ltx_text ltx_font_bold\">3.3</span> / <span id=\"S4.tab1.1.13.13.4.2\" class=\"ltx_text ltx_font_bold\">62.4</span> / <span id=\"S4.tab1.1.13.13.4.3\" class=\"ltx_text ltx_font_bold\">83.3</span>\n</td>\n<td id=\"S4.tab1.1.13.13.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S4.tab1.1.13.13.5.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">17.7</span> / <span id=\"S4.tab1.1.13.13.5.2\" class=\"ltx_text ltx_font_bold\">52.3</span> / <span id=\"S4.tab1.1.13.13.5.3\" class=\"ltx_text ltx_font_bold\">85.8</span>\n</td>\n<td id=\"S4.tab1.1.13.13.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S4.tab1.1.13.13.6.1\" class=\"ltx_text ltx_font_bold\">11.5</span> / <span id=\"S4.tab1.1.13.13.6.2\" class=\"ltx_text ltx_font_bold\">41.5</span> / <span id=\"S4.tab1.1.13.13.6.3\" class=\"ltx_text ltx_font_bold\">70.2</span>\n</td>\n<td id=\"S4.tab1.1.13.13.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S4.tab1.1.13.13.7.1\" class=\"ltx_text ltx_font_bold\">15.8</span> / <span id=\"S4.tab1.1.13.13.7.2\" class=\"ltx_text ltx_font_bold\">33.0</span> / <span id=\"S4.tab1.1.13.13.7.3\" class=\"ltx_text ltx_font_bold\">87.4</span>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S4.tab1.2\" class=\"ltx_p ltx_figure_panel\">To evaluate the effectiveness of the proposed InterBiasing, we conducted speech recognition experiments using the NeMo toolkit<span id=\"footnote1\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/NVIDIA/NeMo</span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a>]</cite>.\nThe performance of the models was evaluated based on character error rates (CERs) and F1 score.\nFollowing the conventional study\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>, we adopted the F1 score as our primary evaluation metric.</p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S4.SS1\" class=\"ltx_subsection ltx_figure_panel\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Data</h3>\n\n<div id=\"S4.SS1.p1\" class=\"ltx_para\">\n<p id=\"S4.SS1.p1.1\" class=\"ltx_p\">We utilized a model that was trained on the CSJ corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a>]</cite>, which contains Japanese public speeches on academic topics.\nThe vocabulary <math id=\"S4.SS1.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathcal{V}\" display=\"inline\"><semantics id=\"S4.SS1.p1.1.m1.1a\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.SS1.p1.1.m1.1.1\" xref=\"S4.SS1.p1.1.m1.1.1.cmml\">\ud835\udcb1</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS1.p1.1.m1.1b\"><ci id=\"S4.SS1.p1.1.m1.1.1.cmml\" xref=\"S4.SS1.p1.1.m1.1.1\">\ud835\udcb1</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS1.p1.1.m1.1c\">\\mathcal{V}</annotation></semantics></math> was a set of 3,260 character units.\nWe used 80-dimensional Mel-spectrogram features as input features.\nSpecAugment <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">23</a>]</cite> and Speed perturbation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">24</a>]</cite> were also applied with the ESPNet recipe\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite>.</p>\n</div>\n<div id=\"S4.SS1.p2\" class=\"ltx_para\">\n<p id=\"S4.SS1.p2.1\" class=\"ltx_p\">We conducted the evaluation on three in-domain (CSJ eval1, eval2, eval3\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a>]</cite>) and three out-of-domain (JSUT-basic 5000\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite>, Common Voice v8.0\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">27</a>]</cite>, TEDxJP-10K\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a>]</cite>) test sets.\nThe out-of-domain test sets are representative of the common scenario in applications where the unseen user data does not match the training data in acoustic or lexical conditions.</p>\n</div>\n<div id=\"S4.SS1.p3\" class=\"ltx_para\">\n<p id=\"S4.SS1.p3.1\" class=\"ltx_p\">Here, we describe the process of generating bias keywords for our evaluation experiments.\nInitially, we performed speech recognition on each evaluation set using a model trained on the CSJ dataset.\nBy comparing the labels and recognition hypotheses, we identified words that were incorrectly recognized.\nIn this process, word segmentation was conducted using morphological analysis with MeCab <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">29</a>]</cite>.\nFrom the extracted misrecognized words, we only retained proper nouns and personal names with two characters or more using morphological labels.\nIn the final stage, we manually removed clear morphological analysis errors from the extracted set of bias keywords.\nWe classified all bias keywords into out-of-vocabulary (OOV) keywords and Non-OOV keywords based on whether they belong to the vocabulary in the CSJ training data.\nThe number of OOV and Non-OOV keywords in each evaluation set is shown in Table <a href=\"#S4.T1\" title=\"Table 1 \u2023 4 Experiments \u2023 InterBiasing: Boost Unseen Word Recognition through Biasing Intermediate Predictions\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n</div>\n<section id=\"S4.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>Model Configurations</h3>\n\n<div id=\"S4.SS2.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"S4.SS2.p1.5\" class=\"ltx_p\"><span id=\"S4.SS2.p1.5.1\" class=\"ltx_text ltx_font_bold\">SelfCond:</span>\nWe used the Self-conditioned CTC model as described in Section\u00a0<a href=\"#S2.SS3\" title=\"2.3 Self-conditioned CTC \u2023 2 Background \u2023 InterBiasing: Boost Unseen Word Recognition through Biasing Intermediate Predictions\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2.3</span></a>.\nThe number of layers <math id=\"S4.SS2.p1.1.m1.1\" class=\"ltx_Math\" alttext=\"N\" display=\"inline\"><semantics id=\"S4.SS2.p1.1.m1.1a\"><mi id=\"S4.SS2.p1.1.m1.1.1\" xref=\"S4.SS2.p1.1.m1.1.1.cmml\">N</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p1.1.m1.1b\"><ci id=\"S4.SS2.p1.1.m1.1.1.cmml\" xref=\"S4.SS2.p1.1.m1.1.1\">\ud835\udc41</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p1.1.m1.1c\">N</annotation></semantics></math> was 18, and the encoder dimension <math id=\"S4.SS2.p1.2.m2.1\" class=\"ltx_Math\" alttext=\"D\" display=\"inline\"><semantics id=\"S4.SS2.p1.2.m2.1a\"><mi id=\"S4.SS2.p1.2.m2.1.1\" xref=\"S4.SS2.p1.2.m2.1.1.cmml\">D</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p1.2.m2.1b\"><ci id=\"S4.SS2.p1.2.m2.1.1.cmml\" xref=\"S4.SS2.p1.2.m2.1.1\">\ud835\udc37</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p1.2.m2.1c\">D</annotation></semantics></math> was 512.\nThe convolution kernel size and the number of attention heads were 31 and 8, respectively.\nThe model was trained for 50 epochs, and the final model was obtained by averaging model parameters over 10 best checkpoints in terms of validation cer values.\nThe effective batch-size was set to 120.\nThe Adam optimizer\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">30</a>]</cite> with <math id=\"S4.SS2.p1.3.m3.1\" class=\"ltx_Math\" alttext=\"\\beta_{1}=0.9\" display=\"inline\"><semantics id=\"S4.SS2.p1.3.m3.1a\"><mrow id=\"S4.SS2.p1.3.m3.1.1\" xref=\"S4.SS2.p1.3.m3.1.1.cmml\"><msub id=\"S4.SS2.p1.3.m3.1.1.2\" xref=\"S4.SS2.p1.3.m3.1.1.2.cmml\"><mi id=\"S4.SS2.p1.3.m3.1.1.2.2\" xref=\"S4.SS2.p1.3.m3.1.1.2.2.cmml\">\u03b2</mi><mn id=\"S4.SS2.p1.3.m3.1.1.2.3\" xref=\"S4.SS2.p1.3.m3.1.1.2.3.cmml\">1</mn></msub><mo id=\"S4.SS2.p1.3.m3.1.1.1\" xref=\"S4.SS2.p1.3.m3.1.1.1.cmml\">=</mo><mn id=\"S4.SS2.p1.3.m3.1.1.3\" xref=\"S4.SS2.p1.3.m3.1.1.3.cmml\">0.9</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p1.3.m3.1b\"><apply id=\"S4.SS2.p1.3.m3.1.1.cmml\" xref=\"S4.SS2.p1.3.m3.1.1\"><eq id=\"S4.SS2.p1.3.m3.1.1.1.cmml\" xref=\"S4.SS2.p1.3.m3.1.1.1\"></eq><apply id=\"S4.SS2.p1.3.m3.1.1.2.cmml\" xref=\"S4.SS2.p1.3.m3.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS2.p1.3.m3.1.1.2.1.cmml\" xref=\"S4.SS2.p1.3.m3.1.1.2\">subscript</csymbol><ci id=\"S4.SS2.p1.3.m3.1.1.2.2.cmml\" xref=\"S4.SS2.p1.3.m3.1.1.2.2\">\ud835\udefd</ci><cn type=\"integer\" id=\"S4.SS2.p1.3.m3.1.1.2.3.cmml\" xref=\"S4.SS2.p1.3.m3.1.1.2.3\">1</cn></apply><cn type=\"float\" id=\"S4.SS2.p1.3.m3.1.1.3.cmml\" xref=\"S4.SS2.p1.3.m3.1.1.3\">0.9</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p1.3.m3.1c\">\\beta_{1}=0.9</annotation></semantics></math>, <math id=\"S4.SS2.p1.4.m4.1\" class=\"ltx_Math\" alttext=\"\\beta_{2}=0.98\" display=\"inline\"><semantics id=\"S4.SS2.p1.4.m4.1a\"><mrow id=\"S4.SS2.p1.4.m4.1.1\" xref=\"S4.SS2.p1.4.m4.1.1.cmml\"><msub id=\"S4.SS2.p1.4.m4.1.1.2\" xref=\"S4.SS2.p1.4.m4.1.1.2.cmml\"><mi id=\"S4.SS2.p1.4.m4.1.1.2.2\" xref=\"S4.SS2.p1.4.m4.1.1.2.2.cmml\">\u03b2</mi><mn id=\"S4.SS2.p1.4.m4.1.1.2.3\" xref=\"S4.SS2.p1.4.m4.1.1.2.3.cmml\">2</mn></msub><mo id=\"S4.SS2.p1.4.m4.1.1.1\" xref=\"S4.SS2.p1.4.m4.1.1.1.cmml\">=</mo><mn id=\"S4.SS2.p1.4.m4.1.1.3\" xref=\"S4.SS2.p1.4.m4.1.1.3.cmml\">0.98</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p1.4.m4.1b\"><apply id=\"S4.SS2.p1.4.m4.1.1.cmml\" xref=\"S4.SS2.p1.4.m4.1.1\"><eq id=\"S4.SS2.p1.4.m4.1.1.1.cmml\" xref=\"S4.SS2.p1.4.m4.1.1.1\"></eq><apply id=\"S4.SS2.p1.4.m4.1.1.2.cmml\" xref=\"S4.SS2.p1.4.m4.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.SS2.p1.4.m4.1.1.2.1.cmml\" xref=\"S4.SS2.p1.4.m4.1.1.2\">subscript</csymbol><ci id=\"S4.SS2.p1.4.m4.1.1.2.2.cmml\" xref=\"S4.SS2.p1.4.m4.1.1.2.2\">\ud835\udefd</ci><cn type=\"integer\" id=\"S4.SS2.p1.4.m4.1.1.2.3.cmml\" xref=\"S4.SS2.p1.4.m4.1.1.2.3\">2</cn></apply><cn type=\"float\" id=\"S4.SS2.p1.4.m4.1.1.3.cmml\" xref=\"S4.SS2.p1.4.m4.1.1.3\">0.98</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p1.4.m4.1c\">\\beta_{2}=0.98</annotation></semantics></math>,\nthe Noam Annealing learning rate scheduling\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">31</a>]</cite> with 1k warmup steps were used for training.\nSelf-conditioning are applied at every layer (<math id=\"S4.SS2.p1.5.m5.4\" class=\"ltx_Math\" alttext=\"\\mathcal{N}=\\{1,2,...,17\\}\" display=\"inline\"><semantics id=\"S4.SS2.p1.5.m5.4a\"><mrow id=\"S4.SS2.p1.5.m5.4.5\" xref=\"S4.SS2.p1.5.m5.4.5.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.SS2.p1.5.m5.4.5.2\" xref=\"S4.SS2.p1.5.m5.4.5.2.cmml\">\ud835\udca9</mi><mo id=\"S4.SS2.p1.5.m5.4.5.1\" xref=\"S4.SS2.p1.5.m5.4.5.1.cmml\">=</mo><mrow id=\"S4.SS2.p1.5.m5.4.5.3.2\" xref=\"S4.SS2.p1.5.m5.4.5.3.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS2.p1.5.m5.4.5.3.2.1\" xref=\"S4.SS2.p1.5.m5.4.5.3.1.cmml\">{</mo><mn id=\"S4.SS2.p1.5.m5.1.1\" xref=\"S4.SS2.p1.5.m5.1.1.cmml\">1</mn><mo id=\"S4.SS2.p1.5.m5.4.5.3.2.2\" xref=\"S4.SS2.p1.5.m5.4.5.3.1.cmml\">,</mo><mn id=\"S4.SS2.p1.5.m5.2.2\" xref=\"S4.SS2.p1.5.m5.2.2.cmml\">2</mn><mo id=\"S4.SS2.p1.5.m5.4.5.3.2.3\" xref=\"S4.SS2.p1.5.m5.4.5.3.1.cmml\">,</mo><mi mathvariant=\"normal\" id=\"S4.SS2.p1.5.m5.3.3\" xref=\"S4.SS2.p1.5.m5.3.3.cmml\">\u2026</mi><mo id=\"S4.SS2.p1.5.m5.4.5.3.2.4\" xref=\"S4.SS2.p1.5.m5.4.5.3.1.cmml\">,</mo><mn id=\"S4.SS2.p1.5.m5.4.4\" xref=\"S4.SS2.p1.5.m5.4.4.cmml\">17</mn><mo stretchy=\"false\" id=\"S4.SS2.p1.5.m5.4.5.3.2.5\" xref=\"S4.SS2.p1.5.m5.4.5.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p1.5.m5.4b\"><apply id=\"S4.SS2.p1.5.m5.4.5.cmml\" xref=\"S4.SS2.p1.5.m5.4.5\"><eq id=\"S4.SS2.p1.5.m5.4.5.1.cmml\" xref=\"S4.SS2.p1.5.m5.4.5.1\"></eq><ci id=\"S4.SS2.p1.5.m5.4.5.2.cmml\" xref=\"S4.SS2.p1.5.m5.4.5.2\">\ud835\udca9</ci><set id=\"S4.SS2.p1.5.m5.4.5.3.1.cmml\" xref=\"S4.SS2.p1.5.m5.4.5.3.2\"><cn type=\"integer\" id=\"S4.SS2.p1.5.m5.1.1.cmml\" xref=\"S4.SS2.p1.5.m5.1.1\">1</cn><cn type=\"integer\" id=\"S4.SS2.p1.5.m5.2.2.cmml\" xref=\"S4.SS2.p1.5.m5.2.2\">2</cn><ci id=\"S4.SS2.p1.5.m5.3.3.cmml\" xref=\"S4.SS2.p1.5.m5.3.3\">\u2026</ci><cn type=\"integer\" id=\"S4.SS2.p1.5.m5.4.4.cmml\" xref=\"S4.SS2.p1.5.m5.4.4\">17</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p1.5.m5.4c\">\\mathcal{N}=\\{1,2,...,17\\}</annotation></semantics></math>).</p>\n</div>\n<div id=\"S4.SS2.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"S4.SS2.p2.1\" class=\"ltx_p\"><span id=\"S4.SS2.p2.1.1\" class=\"ltx_text ltx_font_bold\">TextSub (Text Substitution)</span>:\nA simple text substitution process was applied to the final recognition hypotheses using the pairs of keywords and trigger lists.</p>\n</div>\n<div id=\"S4.SS2.p3\" class=\"ltx_para ltx_noindent\">\n<p id=\"S4.SS2.p3.4\" class=\"ltx_p\"><span id=\"S4.SS2.p3.4.1\" class=\"ltx_text ltx_font_bold\">InterBias:</span>\nTo generate trigger words, we utilized synthetic speech via the in-house TTS system.\nWe then processed this synthesized speech through the above mentioned SelfCond model, using the results from greedy decoding in the intermediate layers (<math id=\"S4.SS2.p3.1.m1.1\" class=\"ltx_Math\" alttext=\"{M_{\\mathsf{bias}}}\" display=\"inline\"><semantics id=\"S4.SS2.p3.1.m1.1a\"><msub id=\"S4.SS2.p3.1.m1.1.1\" xref=\"S4.SS2.p3.1.m1.1.1.cmml\"><mi id=\"S4.SS2.p3.1.m1.1.1.2\" xref=\"S4.SS2.p3.1.m1.1.1.2.cmml\">M</mi><mi id=\"S4.SS2.p3.1.m1.1.1.3\" xref=\"S4.SS2.p3.1.m1.1.1.3.cmml\">\ud835\uddbb\ud835\uddc2\ud835\uddba\ud835\uddcc</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p3.1.m1.1b\"><apply id=\"S4.SS2.p3.1.m1.1.1.cmml\" xref=\"S4.SS2.p3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS2.p3.1.m1.1.1.1.cmml\" xref=\"S4.SS2.p3.1.m1.1.1\">subscript</csymbol><ci id=\"S4.SS2.p3.1.m1.1.1.2.cmml\" xref=\"S4.SS2.p3.1.m1.1.1.2\">\ud835\udc40</ci><ci id=\"S4.SS2.p3.1.m1.1.1.3.cmml\" xref=\"S4.SS2.p3.1.m1.1.1.3\">\ud835\uddbb\ud835\uddc2\ud835\uddba\ud835\uddcc</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p3.1.m1.1c\">{M_{\\mathsf{bias}}}</annotation></semantics></math> = <math id=\"S4.SS2.p3.2.m2.1\" class=\"ltx_Math\" alttext=\"3\" display=\"inline\"><semantics id=\"S4.SS2.p3.2.m2.1a\"><mn id=\"S4.SS2.p3.2.m2.1.1\" xref=\"S4.SS2.p3.2.m2.1.1.cmml\">3</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p3.2.m2.1b\"><cn type=\"integer\" id=\"S4.SS2.p3.2.m2.1.1.cmml\" xref=\"S4.SS2.p3.2.m2.1.1\">3</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p3.2.m2.1c\">3</annotation></semantics></math>).\nThe bias weight <math id=\"S4.SS2.p3.3.m3.1\" class=\"ltx_Math\" alttext=\"w_{\\mathsf{bias}}\" display=\"inline\"><semantics id=\"S4.SS2.p3.3.m3.1a\"><msub id=\"S4.SS2.p3.3.m3.1.1\" xref=\"S4.SS2.p3.3.m3.1.1.cmml\"><mi id=\"S4.SS2.p3.3.m3.1.1.2\" xref=\"S4.SS2.p3.3.m3.1.1.2.cmml\">w</mi><mi id=\"S4.SS2.p3.3.m3.1.1.3\" xref=\"S4.SS2.p3.3.m3.1.1.3.cmml\">\ud835\uddbb\ud835\uddc2\ud835\uddba\ud835\uddcc</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p3.3.m3.1b\"><apply id=\"S4.SS2.p3.3.m3.1.1.cmml\" xref=\"S4.SS2.p3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.SS2.p3.3.m3.1.1.1.cmml\" xref=\"S4.SS2.p3.3.m3.1.1\">subscript</csymbol><ci id=\"S4.SS2.p3.3.m3.1.1.2.cmml\" xref=\"S4.SS2.p3.3.m3.1.1.2\">\ud835\udc64</ci><ci id=\"S4.SS2.p3.3.m3.1.1.3.cmml\" xref=\"S4.SS2.p3.3.m3.1.1.3\">\ud835\uddbb\ud835\uddc2\ud835\uddba\ud835\uddcc</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p3.3.m3.1c\">w_{\\mathsf{bias}}</annotation></semantics></math> was set to 0.9.\nInterBiasing is applied at layer indices (<math id=\"S4.SS2.p3.4.m4.4\" class=\"ltx_Math\" alttext=\"\\mathcal{N}=\\{1,2,...,17\\}\" display=\"inline\"><semantics id=\"S4.SS2.p3.4.m4.4a\"><mrow id=\"S4.SS2.p3.4.m4.4.5\" xref=\"S4.SS2.p3.4.m4.4.5.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.SS2.p3.4.m4.4.5.2\" xref=\"S4.SS2.p3.4.m4.4.5.2.cmml\">\ud835\udca9</mi><mo id=\"S4.SS2.p3.4.m4.4.5.1\" xref=\"S4.SS2.p3.4.m4.4.5.1.cmml\">=</mo><mrow id=\"S4.SS2.p3.4.m4.4.5.3.2\" xref=\"S4.SS2.p3.4.m4.4.5.3.1.cmml\"><mo stretchy=\"false\" id=\"S4.SS2.p3.4.m4.4.5.3.2.1\" xref=\"S4.SS2.p3.4.m4.4.5.3.1.cmml\">{</mo><mn id=\"S4.SS2.p3.4.m4.1.1\" xref=\"S4.SS2.p3.4.m4.1.1.cmml\">1</mn><mo id=\"S4.SS2.p3.4.m4.4.5.3.2.2\" xref=\"S4.SS2.p3.4.m4.4.5.3.1.cmml\">,</mo><mn id=\"S4.SS2.p3.4.m4.2.2\" xref=\"S4.SS2.p3.4.m4.2.2.cmml\">2</mn><mo id=\"S4.SS2.p3.4.m4.4.5.3.2.3\" xref=\"S4.SS2.p3.4.m4.4.5.3.1.cmml\">,</mo><mi mathvariant=\"normal\" id=\"S4.SS2.p3.4.m4.3.3\" xref=\"S4.SS2.p3.4.m4.3.3.cmml\">\u2026</mi><mo id=\"S4.SS2.p3.4.m4.4.5.3.2.4\" xref=\"S4.SS2.p3.4.m4.4.5.3.1.cmml\">,</mo><mn id=\"S4.SS2.p3.4.m4.4.4\" xref=\"S4.SS2.p3.4.m4.4.4.cmml\">17</mn><mo stretchy=\"false\" id=\"S4.SS2.p3.4.m4.4.5.3.2.5\" xref=\"S4.SS2.p3.4.m4.4.5.3.1.cmml\">}</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p3.4.m4.4b\"><apply id=\"S4.SS2.p3.4.m4.4.5.cmml\" xref=\"S4.SS2.p3.4.m4.4.5\"><eq id=\"S4.SS2.p3.4.m4.4.5.1.cmml\" xref=\"S4.SS2.p3.4.m4.4.5.1\"></eq><ci id=\"S4.SS2.p3.4.m4.4.5.2.cmml\" xref=\"S4.SS2.p3.4.m4.4.5.2\">\ud835\udca9</ci><set id=\"S4.SS2.p3.4.m4.4.5.3.1.cmml\" xref=\"S4.SS2.p3.4.m4.4.5.3.2\"><cn type=\"integer\" id=\"S4.SS2.p3.4.m4.1.1.cmml\" xref=\"S4.SS2.p3.4.m4.1.1\">1</cn><cn type=\"integer\" id=\"S4.SS2.p3.4.m4.2.2.cmml\" xref=\"S4.SS2.p3.4.m4.2.2\">2</cn><ci id=\"S4.SS2.p3.4.m4.3.3.cmml\" xref=\"S4.SS2.p3.4.m4.3.3\">\u2026</ci><cn type=\"integer\" id=\"S4.SS2.p3.4.m4.4.4.cmml\" xref=\"S4.SS2.p3.4.m4.4.4\">17</cn></set></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p3.4.m4.4c\">\\mathcal{N}=\\{1,2,...,17\\}</annotation></semantics></math>).</p>\n</div>\n<div id=\"S4.SS2.p4\" class=\"ltx_para ltx_noindent\">\n<p id=\"S4.SS2.p4.1\" class=\"ltx_p\"><span id=\"S4.SS2.p4.1.1\" class=\"ltx_text ltx_font_bold\">Beam Search decoding:</span>\nIn the LM shallow fusion, a 10-gram Ngram was trained using the text corpus from the speech training data with KenLM\u00a0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">32</a>]</cite>.\nBeam size, LM weight, and length penalty were set to 10, 0.5, and 0.2, respectively, optimized using the CSJ dev set.\nThe weight of KBBS was set to <math id=\"S4.SS2.p4.1.m1.1\" class=\"ltx_Math\" alttext=\"3.0\" display=\"inline\"><semantics id=\"S4.SS2.p4.1.m1.1a\"><mn id=\"S4.SS2.p4.1.m1.1.1\" xref=\"S4.SS2.p4.1.m1.1.1.cmml\">3.0</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.SS2.p4.1.m1.1b\"><cn type=\"float\" id=\"S4.SS2.p4.1.m1.1.1.cmml\" xref=\"S4.SS2.p4.1.m1.1.1\">3.0</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.SS2.p4.1.m1.1c\">3.0</annotation></semantics></math>.</p>\n</div>\n<section id=\"S4.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Results</h3>\n\n<div id=\"S4.SS3.p1\" class=\"ltx_para\">\n<p id=\"S4.SS3.p1.1\" class=\"ltx_p\">Table\u00a0<a href=\"#S4\" title=\"4 Experiments \u2023 InterBiasing: Boost Unseen Word Recognition through Biasing Intermediate Predictions\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> summarizes the experimental results.\nFirst, in greedy decoding, it can be seen that the TextSub significantly degraded the F1 score of Non-OOV words in TEDxJP-10k.\nThe TextSub frequently overcorrected recognition results and was less likely to hit the trigger words since they were not always present in the final outputs.\nIn contrast, the InterBiasing had more chances to hit trigger words because text substitution was applied for intermediate predictions on multiple layers.\nCompared to SelfCond and the TextSub, the experimental results show a large improvement of F1 scores of the OOV words, and less degradation of CERs with the proposed InterBiasing.\nIt should be noted that the large fluctuations in the results for CSJ eval1 and eval2 scores have little significance due to the low number of keywords being evaluated, as shown in Table\u00a0<a href=\"#S4.T1\" title=\"Table 1 \u2023 4 Experiments \u2023 InterBiasing: Boost Unseen Word Recognition through Biasing Intermediate Predictions\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n</div>\n<div id=\"S4.SS3.p2\" class=\"ltx_para\">\n<p id=\"S4.SS3.p2.1\" class=\"ltx_p\">Next, in beam search decoding with LM shallow fusion, we also observed that the proposed InterBiasing improved on the F1 scores of SelfCond on many evaluation sets.\nAs in the greedy decoding, especially for OOV keywords in CSJ eval3, InterBiasing significantly improved the OOV and Non-OOV recognition performance with beam search compared with SelfCond.\nThe performance of TextSub tended to degrade similarly to that observed with greedy decoding.</p>\n</div>\n<div id=\"S4.SS3.p3\" class=\"ltx_para\">\n<p id=\"S4.SS3.p3.1\" class=\"ltx_p\">Finally, in LM shallow fusion and KBBS decoding, KBBS remarkably boosted the OOV and Non-OOV recognition performance of SelfCond.\nHowever, OOV F1 scores of SelfCond remained low.\nIt was confirmed that the combination of the proposed InterBiasing and KBBS further boosted the keyword recognition of the SelfCond and achieved the best performance on all evaluation sets.\nIn particular, the recognition performance of OOV words was improved.\nThis appears to stem from InterBiasing increasing the acoustic score of the keywords, causing the keywords to appear in the hypothesis of KBBS.</p>\n</div>\n<section id=\"S4.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.4 </span>Analysis of the Relationship Between Beam Size and Keyword Recognition Rate</h3>\n\n<div id=\"S4.SS4.p1\" class=\"ltx_para\">\n<p id=\"S4.SS4.p1.1\" class=\"ltx_p\">In this section, we report an analysis of how the beam size impacts the keyword recognition performance.\nFigure <a href=\"#S4.F2\" title=\"Figure 2 \u2023 4.4 Analysis of the Relationship Between Beam Size and Keyword Recognition Rate \u2023 4.3 Results \u2023 4.2 Model Configurations \u2023 4.1 Data \u2023 4 Experiments \u2023 InterBiasing: Boost Unseen Word Recognition through Biasing Intermediate Predictions\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the F1 scores of SelfCond and InterBiasing for various beam sizes of KBBS on the JSUT basic 5000.\nFor the OOV keywords, SelfCond improved performance with increasing beam size from 2 to 10, but did not improve F1 scores when the beam size was further increased.\nThis indicates that the acoustic scores of many OOV words output from SelfCond were too low to appear in the beam search hypothesis even when the beam size was increased.\nUsing InterBiasing with a beam size of only 2, the keyword recognition rate was already superior to SelfCond with a beam size of 10.\nFurthermore, when the beam size was increased to 10, the performance of InterBiasing was further improved.\nTherefore, it can be seen that InterBiasing enhanced the acoustic score of OOV words, making these words appear more frequently within the beam search hypothesis.\nAs a result, the effect of KBBS is likely enhanced.\nSimilarly, for Non-OOV words, we observed that high keyword recognition rates were achieved even when using smaller beam sizes.</p>\n</div>\n<figure id=\"S4.F2\" class=\"ltx_figure\"><img src=\"/html/2406.14890/assets/figure/beam_size5.png\" id=\"S4.F2.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"293\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>F1 scores of Out-of-Vocabulary (OOV) and Non-OOV words in JSUT basic 5000 with different beam sizes. LM shallow fusion and Keyword-boosted Beam Search (KBBS) were utilized. Beam size was set to 2, 3, 5, 10, 20, respectively.</figcaption>\n</figure>\n<section id=\"S5\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Conclusions</h2>\n\n<div id=\"S5.p1\" class=\"ltx_para\">\n<p id=\"S5.p1.1\" class=\"ltx_p\">In this paper, we propose a method to improve the speech recognition performance of unknown words without additional training by effectively augmenting the intermediate predictions of the acoustic encoder with a keyword list and integrating it into the subsequent network layers. This approach allows for acoustic analysis of the keywords in the acoustic encoder, thereby improving the recognition performance of unknown words while minimizing side effects such as over-boosting. Experimental results in Japanese confirmed that the proposed method enhances the recognition performance of unknown words.</p>\n</div>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[1]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Graves, S.\u00a0Fern\u00e1ndez, F.\u00a0Gomez, and J.\u00a0Schmidhuber, ``Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,'' in <em id=\"bib.bib1.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. ICML</em>, 2006, p. 369\u2013376.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[2]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Graves, ``Sequence transduction with recurrent neural networks,'' in <em id=\"bib.bib2.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning: Representation Learning Workshop</em>, 2012.\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[3]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0K. Chorowski, D.\u00a0Bahdanau, D.\u00a0Serdyuk, K.\u00a0Cho, and Y.\u00a0Bengio, ``Attention-based models for speech recognition,'' in <em id=\"bib.bib3.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. NeurIPS</em>, 2015, pp. 577\u2013585.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[4]</span>\n<span class=\"ltx_bibblock\">\nW.\u00a0Chan, N.\u00a0Jaitly, Q.\u00a0Le, and O.\u00a0Vinyals, ``Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,'' in <em id=\"bib.bib4.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. ICASSP</em>, 2016, pp. 4960\u20134964.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[5]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0Zhao, T.\u00a0N. Sainath, D.\u00a0Rybach, P.\u00a0Rondon, D.\u00a0Bhatia, B.\u00a0Li, and R.\u00a0Pang, ``Shallow-Fusion End-to-End Contextual Biasing,'' in <em id=\"bib.bib5.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. Interspeech</em>, 2019, pp. 1418\u20131422.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[6]</span>\n<span class=\"ltx_bibblock\">\nR.\u00a0Huang, O.\u00a0Abdel-Hamid, X.\u00a0Li, and G.\u00a0Evermann, ``Class lm and word mapping for contextual biasing in end-to-end asr,'' in <em id=\"bib.bib6.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. Interspeech</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[7]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0Le, M.\u00a0Jain, G.\u00a0Keren, S.\u00a0Kim, Y.\u00a0Shi, J.\u00a0Mahadeokar, J.\u00a0Chan, Y.\u00a0Shangguan, C.\u00a0Fuegen, O.\u00a0Kalinli, Y.\u00a0Saraf, and M.\u00a0Seltzer, ``Contextualized streaming end-to-end speech recognition with trie-based deep biasing and shallow fusion,'' in <em id=\"bib.bib7.1.1\" class=\"ltx_emph ltx_font_italic\">Interspeech</em>, 2021, pp. 1772\u20131776.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[8]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Huang, A.\u00a0Zhang, Z.\u00a0Yang, P.\u00a0Guo, B.\u00a0Mu, T.\u00a0Xu, and L.\u00a0Xie, ``Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network,'' in <em id=\"bib.bib8.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. Interspeech</em>, 2023, pp. 4933\u20134937.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[9]</span>\n<span class=\"ltx_bibblock\">\nY.\u00a0Sudo, M.\u00a0Shakeel, Y.\u00a0Fukumoto, Y.\u00a0Peng, and S.\u00a0Watanabe, ``Contextualized automatic speech recognition with attention-based bias phrase boosted beam search,'' in <em id=\"bib.bib9.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. ICASSP</em>, 2024, pp. 10\u2009896\u201310\u2009900.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[10]</span>\n<span class=\"ltx_bibblock\">\nX.\u00a0Wang, Y.\u00a0Liu, J.\u00a0Li, V.\u00a0Miljanic, S.\u00a0Zhao, and H.\u00a0Khalil, ``Towards contextual spelling correction for customization of end-to-end speech recognition systems,'' <em id=\"bib.bib10.1.1\" class=\"ltx_emph ltx_font_italic\">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.\u00a030, pp. 3089\u20133097, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[11]</span>\n<span class=\"ltx_bibblock\">\nN.\u00a0Jung, G.\u00a0Kim, and J.\u00a0S. Chung, ``Spell my name: Keyword boosted speech recognition,'' in <em id=\"bib.bib11.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. ICASSP</em>, 2022, pp. 6642\u20136646.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[12]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Zeyer, R.\u00a0Schluter, and H.\u00a0Ney, ``Why does ctc result in peaky behavior?'' <em id=\"bib.bib12.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2105.14849</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[13]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Nozaki and T.\u00a0Komatsu, ``Relaxing the Conditional Independence Assumption of CTC-Based ASR by Conditioning on Intermediate Predictions,'' in <em id=\"bib.bib13.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. Interspeech</em>, 2021, pp. 3735\u20133739.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[14]</span>\n<span class=\"ltx_bibblock\">\nY.\u00a0Higuchi, N.\u00a0Chen, Y.\u00a0Fujita, H.\u00a0Inaguma, T.\u00a0Komatsu, J.\u00a0Lee, J.\u00a0Nozaki, T.\u00a0Wang, and S.\u00a0Watanabe, ``A comparative study on non-autoregressive modelings for speech-to-text generation,'' in <em id=\"bib.bib14.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. ASRU</em>, 2021, pp. 47\u201354.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[15]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0A. Chi, J.\u00a0Salazar, and K.\u00a0Kirchhoff, ``Align-Refine: Non-autoregressive speech recognition via iterative realignment,'' in <em id=\"bib.bib15.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. NAACL-HLT</em>, 2021, pp. 1920\u20131927.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[16]</span>\n<span class=\"ltx_bibblock\">\nY.\u00a0Higuchi, S.\u00a0Watanabe, N.\u00a0Chen, T.\u00a0Ogawa, and T.\u00a0Kobayashi, ``Mask CTC: Non-autoregressive end-to-end ASR with CTC and mask predict,'' in <em id=\"bib.bib16.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. Interspeech</em>, 2020, pp. 3655\u20133659.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[17]</span>\n<span class=\"ltx_bibblock\">\nY.\u00a0Nakagome, T.\u00a0Komatsu, Y.\u00a0Fujita, S.\u00a0Ichimura, and Y.\u00a0Kida, ``InterAug: Augmenting Noisy Intermediate Predictions for CTC-based ASR,'' in <em id=\"bib.bib17.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. Interspeech</em>, 2022, pp. 5140\u20135144.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[18]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Gulati, J.\u00a0Qin, C.-C. Chiu, N.\u00a0Parmar, Y.\u00a0Zhang, J.\u00a0Yu, W.\u00a0Han, S.\u00a0Wang, Z.\u00a0Zhang, Y.\u00a0Wu, and R.\u00a0Pang, ``Conformer: Convolution-augmented Transformer for Speech Recognition,'' in <em id=\"bib.bib18.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. Interspeech</em>, 2020, pp. 5036\u20135040.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[19]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Lee and S.\u00a0Watanabe, ``Intermediate loss regularization for ctc-based speech recognition,'' in <em id=\"bib.bib19.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. ICASSP</em>, 2021, pp. 6224\u20136228.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[20]</span>\n<span class=\"ltx_bibblock\">\nT.\u00a0Komatsu, Y.\u00a0Fujita, J.\u00a0Lee, L.\u00a0Lee, S.\u00a0Watanabe, and Y.\u00a0Kida, ``Better Intermediates Improve CTC Inference,'' in <em id=\"bib.bib20.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. Interspeechs</em>, 2022, pp. 4965\u20134969.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[21]</span>\n<span class=\"ltx_bibblock\">\nO.\u00a0Kuchaiev, J.\u00a0Li, H.\u00a0Nguyen, O.\u00a0Hrinchuk, R.\u00a0Leary, B.\u00a0Ginsburg, S.\u00a0Kriman, S.\u00a0Beliaev, V.\u00a0Lavrukhin, J.\u00a0Cook <em id=\"bib.bib21.1.1\" class=\"ltx_emph ltx_font_italic\">et\u00a0al.</em>, ``Nemo: a toolkit for building ai applications using neural modules,'' <em id=\"bib.bib21.2.2\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1909.09577</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[22]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Maekawa, ``Corpus of spontaneous japanese: Its design and evaluation,'' in <em id=\"bib.bib22.1.1\" class=\"ltx_emph ltx_font_italic\">ISCA &amp; IEEE Workshop on Spontaneous Speech Processing and Recognition</em>, 2003.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[23]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0S. Park, W.\u00a0Chan, Y.\u00a0Zhang, C.-C. Chiu, B.\u00a0Zoph, E.\u00a0D. Cubuk, and Q.\u00a0V. Le, ``SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,'' in <em id=\"bib.bib23.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. Interspeech</em>, 2019, pp. 2613\u20132617.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[24]</span>\n<span class=\"ltx_bibblock\">\nT.\u00a0Ko, V.\u00a0Peddinti, D.\u00a0Povey, and S.\u00a0Khudanpur, ``Audio augmentation for speech recognition,'' in <em id=\"bib.bib24.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. Interspeech</em>, 2015, pp. 3586\u20133589.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[25]</span>\n<span class=\"ltx_bibblock\">\nS.\u00a0Watanabe, T.\u00a0Hori, S.\u00a0Karita, T.\u00a0Hayashi, J.\u00a0Nishitoba, Y.\u00a0Unno, N.\u00a0Enrique Yalta Soplin, J.\u00a0Heymann, M.\u00a0Wiesner, N.\u00a0Chen, A.\u00a0Renduchintala, and T.\u00a0Ochiai, ``ESPnet: End-to-End Speech Processing Toolkit,'' in <em id=\"bib.bib25.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. Interspeech</em>, 2018, pp. 2207\u20132211.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[26]</span>\n<span class=\"ltx_bibblock\">\nR.\u00a0Sonobe, S.\u00a0Takamichi, and H.\u00a0Saruwatari, ``Jsut corpus: free large-scale japanese speech corpus for end-to-end speech synthesis,'' <em id=\"bib.bib26.1.1\" class=\"ltx_emph ltx_font_italic\">ArXiv</em>, vol. abs/1711.00354, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[27]</span>\n<span class=\"ltx_bibblock\">\nR.\u00a0Ardila, M.\u00a0Branson, K.\u00a0Davis, M.\u00a0Henretty, M.\u00a0Kohler, J.\u00a0Meyer, R.\u00a0Morais, L.\u00a0Saunders, F.\u00a0M. Tyers, and G.\u00a0Weber, ``Common voice: A massively-multilingual speech corpus,'' in <em id=\"bib.bib27.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. LREC</em>, 2020, pp. 4211\u20134215.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[28]</span>\n<span class=\"ltx_bibblock\">\nS.\u00a0Ando and H.\u00a0Fujihara, ``Construction of a large-scale japanese asr corpus on tv recordings,'' in <em id=\"bib.bib28.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. ICASSP</em>, 2021, pp. 6948\u20136952.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[29]</span>\n<span class=\"ltx_bibblock\">\nT.\u00a0Kudo, K.\u00a0Yamamoto, and Y.\u00a0Matsumoto, ``Applying conditional random fields to Japanese morphological analysis,'' in <em id=\"bib.bib29.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. EMNLP</em>, 2004, pp. 230\u2013237.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[30]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0P. Kingma and J.\u00a0Ba, ``Adam: A Method for Stochastic Optimization,'' in <em id=\"bib.bib30.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. ICLR</em>, 2015.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[31]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Vaswani, N.\u00a0Shazeer, N.\u00a0Parmar, J.\u00a0Uszkoreit, L.\u00a0Jones, A.\u00a0N. Gomez, L.\u00a0Kaiser, and I.\u00a0Polosukhin, ``Attention is all you need,'' in <em id=\"bib.bib31.1.1\" class=\"ltx_emph ltx_font_italic\">Proc. NeurlPS</em>, 2017, p. 6000\u20136010.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[32]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Heafield, ``KenLM: Faster and smaller language model queries,'' in <em id=\"bib.bib32.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the Sixth Workshop on Statistical Machine Translation</em>, Jul. 2011, pp. 187\u2013197.\n\n</span>\n</li>\n</ul>\n</section>\n</section>\n</section>\n</section>\n</section>\n</section>\n</div>\n</div>\n</figure>\n",
        "footnotes": [],
        "references": []
    }
}