{
    "S4.T1": {
        "caption": "Table 1 :  Ablation study: (a) original model of  [ 2 ] , (b) with normalization in post-processing, (c) with audio format change to waveform, (d) with shallower network (baseline), and (e) with added distribution loss. \u201cSize\u201d, \u201cAudio\u201d, \u201cNorm.\u201d, \u201c \u2112 s superscript \u2112 \ud835\udc60 \\mathcal{L}^{s} \u201d, \u201cAUC\u201d, \u201c#Param\u201d represent network size, audio input type (Mel: Mel-spectogram; Wave.: Waveform), normalization in post-processing, statistics-aware loss, AUC on DFDC, and number of model parameters in millions, respectively.",
        "table": "<figure id=\"S4.T1\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S4.T1.5.1.1\" class=\"ltx_text ltx_font_bold\">Table 1</span>: </span>Ablation study: (a) original model of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a>]</cite>, (b) with normalization in post-processing, (c) with audio format change to waveform, (d) with shallower network (baseline), and (e) with added distribution loss. \u201cSize\u201d, \u201cAudio\u201d, \u201cNorm.\u201d, \u201c<math id=\"S4.T1.2.m1.1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}^{s}\" display=\"inline\"><semantics id=\"S4.T1.2.m1.1b\"><msup id=\"S4.T1.2.m1.1.1\" xref=\"S4.T1.2.m1.1.1.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.T1.2.m1.1.1.2\" xref=\"S4.T1.2.m1.1.1.2.cmml\">\u2112</mi><mi id=\"S4.T1.2.m1.1.1.3\" xref=\"S4.T1.2.m1.1.1.3.cmml\">s</mi></msup><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.2.m1.1c\"><apply id=\"S4.T1.2.m1.1.1.cmml\" xref=\"S4.T1.2.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T1.2.m1.1.1.1.cmml\" xref=\"S4.T1.2.m1.1.1\">superscript</csymbol><ci id=\"S4.T1.2.m1.1.1.2.cmml\" xref=\"S4.T1.2.m1.1.1.2\">\u2112</ci><ci id=\"S4.T1.2.m1.1.1.3.cmml\" xref=\"S4.T1.2.m1.1.1.3\">\ud835\udc60</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.m1.1d\">\\mathcal{L}^{s}</annotation></semantics></math>\u201d, \u201cAUC\u201d, \u201c#Param\u201d represent network size, audio input type (Mel: Mel-spectogram; Wave.: Waveform), normalization in post-processing, statistics-aware loss, AUC on DFDC, and number of model parameters in millions, respectively.</figcaption>\n<div id=\"S4.T1.3\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:174.3pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(82.5pt,-33.2pt) scale(1.61421779874145,1.61421779874145) ;\">\n<table id=\"S4.T1.3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.3.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.1.1.2\" class=\"ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S4.T1.3.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">Size</th>\n<th id=\"S4.T1.3.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">Audio</th>\n<th id=\"S4.T1.3.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Norm.</th>\n<th id=\"S4.T1.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t\"><math id=\"S4.T1.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}^{s}\" display=\"inline\"><semantics id=\"S4.T1.3.1.1.1.m1.1a\"><msup id=\"S4.T1.3.1.1.1.m1.1.1\" xref=\"S4.T1.3.1.1.1.m1.1.1.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.T1.3.1.1.1.m1.1.1.2\" xref=\"S4.T1.3.1.1.1.m1.1.1.2.cmml\">\u2112</mi><mi id=\"S4.T1.3.1.1.1.m1.1.1.3\" xref=\"S4.T1.3.1.1.1.m1.1.1.3.cmml\">s</mi></msup><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.3.1.1.1.m1.1b\"><apply id=\"S4.T1.3.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.3.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T1.3.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T1.3.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"S4.T1.3.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T1.3.1.1.1.m1.1.1.2\">\u2112</ci><ci id=\"S4.T1.3.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T1.3.1.1.1.m1.1.1.3\">\ud835\udc60</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.3.1.1.1.m1.1c\">\\mathcal{L}^{s}</annotation></semantics></math></th>\n<th id=\"S4.T1.3.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">AUC</th>\n<th id=\"S4.T1.3.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">#Param</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">(a)</td>\n<td id=\"S4.T1.3.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Deep</td>\n<td id=\"S4.T1.3.1.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Mel</td>\n<td id=\"S4.T1.3.1.2.1.4\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td id=\"S4.T1.3.1.2.1.5\" class=\"ltx_td ltx_border_rr ltx_border_t\"></td>\n<td id=\"S4.T1.3.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">89.87%</td>\n<td id=\"S4.T1.3.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">122.78</td>\n</tr>\n<tr id=\"S4.T1.3.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">(b)</td>\n<td id=\"S4.T1.3.1.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_r\">Deep</td>\n<td id=\"S4.T1.3.1.3.2.3\" class=\"ltx_td ltx_align_left ltx_border_r\">Mel</td>\n<td id=\"S4.T1.3.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">\u2713</td>\n<td id=\"S4.T1.3.1.3.2.5\" class=\"ltx_td ltx_border_rr\"></td>\n<td id=\"S4.T1.3.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_r\">96.34%</td>\n<td id=\"S4.T1.3.1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_r\">122.78</td>\n</tr>\n<tr id=\"S4.T1.3.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">(c)</td>\n<td id=\"S4.T1.3.1.4.3.2\" class=\"ltx_td ltx_align_left ltx_border_r\">Deep</td>\n<td id=\"S4.T1.3.1.4.3.3\" class=\"ltx_td ltx_align_left ltx_border_r\">Wave.</td>\n<td id=\"S4.T1.3.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">\u2713</td>\n<td id=\"S4.T1.3.1.4.3.5\" class=\"ltx_td ltx_border_rr\"></td>\n<td id=\"S4.T1.3.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_r\">97.13%</td>\n<td id=\"S4.T1.3.1.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_r\">263.67</td>\n</tr>\n<tr id=\"S4.T1.3.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">(d)</td>\n<td id=\"S4.T1.3.1.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_r\">Shallow</td>\n<td id=\"S4.T1.3.1.5.4.3\" class=\"ltx_td ltx_align_left ltx_border_r\">Wave.</td>\n<td id=\"S4.T1.3.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">\u2713</td>\n<td id=\"S4.T1.3.1.5.4.5\" class=\"ltx_td ltx_border_rr\"></td>\n<td id=\"S4.T1.3.1.5.4.6\" class=\"ltx_td ltx_align_center ltx_border_r\">96.55%</td>\n<td id=\"S4.T1.3.1.5.4.7\" class=\"ltx_td ltx_align_center ltx_border_r\">107.10</td>\n</tr>\n<tr id=\"S4.T1.3.1.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">(e)</td>\n<td id=\"S4.T1.3.1.6.5.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">Shallow</td>\n<td id=\"S4.T1.3.1.6.5.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">Wave.</td>\n<td id=\"S4.T1.3.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">\u2713</td>\n<td id=\"S4.T1.3.1.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t\">\u2713</td>\n<td id=\"S4.T1.3.1.6.5.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">96.69%</td>\n<td id=\"S4.T1.3.1.6.5.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">107.10</td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "To enhance the model proposed in [2], we introduce three modifications: 1) post-processing by normalizing the fakeness score based on the score statistics in the training set (Section 3.1.3); 2) changing audio input type from frequency-based to waveform; and (3) reducing the network size. The significance of normalization with the training set in post-processing is evident when comparing Table 1(a) and Table 1(b). Transitioning from Table 1(b) to Table 1(c), we change the audio input type from Mel spectrogram to waveform, resulting in an improved performance. Reducing the network size in Table 1(d) decreases the number of parameters by more than half while maintaining comparable performance, showcasing that a shallower network is sufficient for deepfake detection. The model in Table 1(d) is defined as our baseline from here forth. Incorporating the statistics-aware loss in Table 1(e) further enhances performance without introducing additional parameters. These results underscore the importance of each proposed component.",
            "To assess the generalisation capabilities of the proposed method, we test our model (trained on the dataset DFDC) on the FakeAVCeleb dataset.\nThe models reported in Table 1(a)-(e) achieve AUC scores of 48.57%percent48.5748.57\\%, 56.92%percent56.9256.92\\%, 59.87%percent59.8759.87\\%, 58.82%percent58.8258.82\\%, and 61.39%percent61.3961.39\\%, respectively. The noticable enhancement in performance suggests improved generalization capabilities resulting from the utilization of waveform and the incorporation of statistics-aware loss.\nHowever, the proposed model still lags behind SoA, such as AvoiD-DF [6] with 82.8%percent82.882.8\\% AUC, prompting the need for further investigations and refinements. One potential avenue for improvement could be exploring the benefits of a transformer-based model, as utilized in AvoiD-DF.",
            "To further emphasize the importance of the statistics-aware loss, we visualize in addition to Fig. 1, the feature distributions extracted from models with different settings. Fig. 4(a) and (b) correspond to the models described in Table 1(b) and (c), respectively. Fig. 4(c) represents our baseline trained on a smaller set. As observed, a similar trend occurs in different models, indicating that the proposed statistics-aware loss has the potential to enhance their performance."
        ]
    },
    "S4.T2": {
        "caption": "Table 2 :  Comparison of the proposed model with SoA methods in terms of AUC on the DFDC dataset using only-visual (V), only-audio (A), and audio-visual (AV) modalities.",
        "table": "<figure id=\"S4.T2\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S4.T2.2.1.1\" class=\"ltx_text ltx_font_bold\">Table 2</span>: </span>Comparison of the proposed model with SoA methods in terms of AUC on the DFDC dataset using only-visual (V), only-audio (A), and audio-visual (AV) modalities.</figcaption>\n<div id=\"S4.T2.3\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:112.5pt;vertical-align:-0.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-62.7pt,16.2pt) scale(0.775550969194456,0.775550969194456) ;\">\n<table id=\"S4.T2.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.3.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Method</td>\n<td id=\"S4.T2.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Modality</td>\n<td id=\"S4.T2.3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">AUC</td>\n<td id=\"S4.T2.3.1.1.1.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Method</td>\n<td id=\"S4.T2.3.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Modality</td>\n<td id=\"S4.T2.3.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">AUC</td>\n</tr>\n<tr id=\"S4.T2.3.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Multi-Attention <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite>\n</td>\n<td id=\"S4.T2.3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">V</td>\n<td id=\"S4.T2.3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">84.8%</td>\n<td id=\"S4.T2.3.1.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">MDS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a>]</cite>\n</td>\n<td id=\"S4.T2.3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">AV</td>\n<td id=\"S4.T2.3.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">90.66%</td>\n</tr>\n<tr id=\"S4.T2.3.1.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">SLADD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>\n</td>\n<td id=\"S4.T2.3.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">V</td>\n<td id=\"S4.T2.3.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_rr\">75.2%</td>\n<td id=\"S4.T2.3.1.3.3.4\" class=\"ltx_td ltx_align_left ltx_border_r\">Emotion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">5</a>]</cite>\n</td>\n<td id=\"S4.T2.3.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\">AV</td>\n<td id=\"S4.T2.3.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_r\">84.4%</td>\n</tr>\n<tr id=\"S4.T2.3.1.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">Meso4 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite>\n</td>\n<td id=\"S4.T2.3.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">V</td>\n<td id=\"S4.T2.3.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_rr\">75.3%</td>\n<td id=\"S4.T2.3.1.4.4.4\" class=\"ltx_td ltx_align_left ltx_border_r\">BA-TFD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">18</a>]</cite>\n</td>\n<td id=\"S4.T2.3.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\">AV</td>\n<td id=\"S4.T2.3.1.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_r\">84.6%</td>\n</tr>\n<tr id=\"S4.T2.3.1.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">AASIST <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">14</a>]</cite>\n</td>\n<td id=\"S4.T2.3.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">A</td>\n<td id=\"S4.T2.3.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">68.4%</td>\n<td id=\"S4.T2.3.1.5.5.4\" class=\"ltx_td ltx_align_left ltx_border_r\">AVFakeNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">20</a>]</cite>\n</td>\n<td id=\"S4.T2.3.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\">AV</td>\n<td id=\"S4.T2.3.1.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_r\">86.2%</td>\n</tr>\n<tr id=\"S4.T2.3.1.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">ECAPA-TDNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">15</a>]</cite>\n</td>\n<td id=\"S4.T2.3.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">A</td>\n<td id=\"S4.T2.3.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_rr\">69.8%</td>\n<td id=\"S4.T2.3.1.6.6.4\" class=\"ltx_td ltx_align_left ltx_border_r\">VFD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">17</a>]</cite>\n</td>\n<td id=\"S4.T2.3.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\">AV</td>\n<td id=\"S4.T2.3.1.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_r\">85.13%</td>\n</tr>\n<tr id=\"S4.T2.3.1.7.7\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">RawNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">8</a>]</cite>\n</td>\n<td id=\"S4.T2.3.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">A</td>\n<td id=\"S4.T2.3.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_rr\">56.2%</td>\n<td id=\"S4.T2.3.1.7.7.4\" class=\"ltx_td ltx_align_left ltx_border_r\">AvoiD-DF <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">6</a>]</cite>\n</td>\n<td id=\"S4.T2.3.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\">AV</td>\n<td id=\"S4.T2.3.1.7.7.6\" class=\"ltx_td ltx_align_center ltx_border_r\">94.8%</td>\n</tr>\n<tr id=\"S4.T2.3.1.8.8\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.1.8.8.1\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.T2.3.1.8.8.2\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.T2.3.1.8.8.3\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td id=\"S4.T2.3.1.8.8.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\">SADD (Ours)</td>\n<td id=\"S4.T2.3.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">AV</td>\n<td id=\"S4.T2.3.1.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.3.1.8.8.6.1\" class=\"ltx_text ltx_font_bold\">96.69%</span></td>\n</tr>\n</tbody>\n</table>\n</span></div>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Table 2 compares our approach with SoA methods in terms of AUC on DFDC. When compared to uni-modal methods, either only-visual (V) or only-audio (A), multi-modal methods generally exhibit better performance. In the comparison among audio-visual models, our method demonstrates superiority over other models, including the MDS [2] baseline. We also outperform methods using emotion-based inconsistencies [5]. Furthermore, our performance surpasses identity-based models, such as BA-TFD [18] and VFD [17]. Additionally, we outperform fusion-based models, such as AVoiD-DF [6] and AVFakeNet [20]."
        ]
    },
    "S4.T3": {
        "caption": "Table 3 :  Evaluation on DFDC across a wide range of  \u03b1 \ud835\udefc \\alpha  values. Each model is trained on the smaller training set. In general, our model is robust to a huge range of  \u03b1 \ud835\udefc \\alpha  values, achieving better performance compared to the baseline.",
        "table": "<figure id=\"S4.T3\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span id=\"S4.T3.8.1.1\" class=\"ltx_text ltx_font_bold\">Table 3</span>: </span>Evaluation on DFDC across a wide range of <math id=\"S4.T3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha\" display=\"inline\"><semantics id=\"S4.T3.3.m1.1b\"><mi id=\"S4.T3.3.m1.1.1\" xref=\"S4.T3.3.m1.1.1.cmml\">\u03b1</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.3.m1.1c\"><ci id=\"S4.T3.3.m1.1.1.cmml\" xref=\"S4.T3.3.m1.1.1\">\ud835\udefc</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.3.m1.1d\">\\alpha</annotation></semantics></math> values. Each model is trained on the smaller training set. In general, our model is robust to a huge range of <math id=\"S4.T3.4.m2.1\" class=\"ltx_Math\" alttext=\"\\alpha\" display=\"inline\"><semantics id=\"S4.T3.4.m2.1b\"><mi id=\"S4.T3.4.m2.1.1\" xref=\"S4.T3.4.m2.1.1.cmml\">\u03b1</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.4.m2.1c\"><ci id=\"S4.T3.4.m2.1.1.cmml\" xref=\"S4.T3.4.m2.1.1\">\ud835\udefc</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.4.m2.1d\">\\alpha</annotation></semantics></math> values, achieving better performance compared to the baseline.</figcaption>\n<table id=\"S4.T3.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.6.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><math id=\"S4.T3.5.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha\" display=\"inline\"><semantics id=\"S4.T3.5.1.1.m1.1a\"><mi id=\"S4.T3.5.1.1.m1.1.1\" xref=\"S4.T3.5.1.1.m1.1.1.cmml\">\u03b1</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.5.1.1.m1.1b\"><ci id=\"S4.T3.5.1.1.m1.1.1.cmml\" xref=\"S4.T3.5.1.1.m1.1.1\">\ud835\udefc</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.5.1.1.m1.1c\">\\alpha</annotation></semantics></math></th>\n<th id=\"S4.T3.6.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t\">AUC</th>\n<th id=\"S4.T3.6.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><math id=\"S4.T3.6.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha\" display=\"inline\"><semantics id=\"S4.T3.6.2.2.m1.1a\"><mi id=\"S4.T3.6.2.2.m1.1.1\" xref=\"S4.T3.6.2.2.m1.1.1.cmml\">\u03b1</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.6.2.2.m1.1b\"><ci id=\"S4.T3.6.2.2.m1.1.1.cmml\" xref=\"S4.T3.6.2.2.m1.1.1\">\ud835\udefc</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.6.2.2.m1.1c\">\\alpha</annotation></semantics></math></th>\n<th id=\"S4.T3.6.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">AUC</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.6.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.6.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">0 (baseline)</th>\n<td id=\"S4.T3.6.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">86.52%</td>\n<th id=\"S4.T3.6.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1</th>\n<td id=\"S4.T3.6.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.6.3.1.4.1\" class=\"ltx_text ltx_font_bold\">88.55%</span></td>\n</tr>\n<tr id=\"S4.T3.6.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.6.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">0.01</th>\n<td id=\"S4.T3.6.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">87.33%</td>\n<th id=\"S4.T3.6.4.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">5</th>\n<td id=\"S4.T3.6.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">86.64%</td>\n</tr>\n<tr id=\"S4.T3.6.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.6.5.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">0.05</th>\n<td id=\"S4.T3.6.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">88.14%</td>\n<th id=\"S4.T3.6.5.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">10</th>\n<td id=\"S4.T3.6.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">86.92%</td>\n</tr>\n<tr id=\"S4.T3.6.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T3.6.6.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">0.1</th>\n<td id=\"S4.T3.6.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_rr\">86.57%</td>\n<th id=\"S4.T3.6.6.4.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">50</th>\n<td id=\"S4.T3.6.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">86.59%</td>\n</tr>\n<tr id=\"S4.T3.6.7.5\" class=\"ltx_tr\">\n<th id=\"S4.T3.6.7.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">0.5</th>\n<td id=\"S4.T3.6.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_rr\">87.03%</td>\n<th id=\"S4.T3.6.7.5.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">100</th>\n<td id=\"S4.T3.6.7.5.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">83.51%</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We introduce a weighting hyperparameter \u03b1\ud835\udefc\\alpha in Eq. (3) to adjust the contribution of the statistics-aware loss within the overall loss. We report the obtained results on a smaller training set, when varying the value of \u03b1\ud835\udefc\\alpha . As shown in Table 3, for different values of \u03b1\ud835\udefc\\alpha , the model achieves better performance compared to the baseline. However, setting \u03b1\ud835\udefc\\alpha to relatively large value results in a significant drop in AUC.",
            "We train the models detailed in Table 3 using the smaller training dataset. The model incorporating the statistics-aware loss achieves an improvement of 2%percent22\\% as compared to the baseline, highlighting the relevance of this loss in a limited training data environment."
        ]
    }
}