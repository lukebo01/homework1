{
    "S4.T1": {
        "caption": "TABLE I:  WERs for PUSM and JSTTI-ENC models using different word boundary extractors before self-training. The top-line has been underlined and the best result for each model is shown in bold.",
        "table": "<figure id=\"S4.T1\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE I: </span>WERs for PUSM and JSTTI-ENC models using different word boundary extractors before self-training. The top-line has been underlined and the best result for each model is shown in bold.</figcaption>\n<table id=\"S4.T1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>\n<th id=\"S4.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-JSTTI-ENC (<math id=\"S4.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.m1.1b\"><ci id=\"S4.T1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n<th id=\"S4.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-PUSM(<math id=\"S4.T1.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T1.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T1.2.2.2.m1.1.1\" xref=\"S4.T1.2.2.2.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.2.2.2.m1.1b\"><ci id=\"S4.T1.2.2.2.m1.1.1.cmml\" xref=\"S4.T1.2.2.2.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">VG-HuBERT</th>\n<td id=\"S4.T1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">47.99 (47.99)</td>\n<td id=\"S4.T1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">58.73 (58.77)</td>\n</tr>\n<tr id=\"S4.T1.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">GradSeg</th>\n<td id=\"S4.T1.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">47.53 (44.76)</td>\n<td id=\"S4.T1.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">52.10 (50.84)</td>\n</tr>\n<tr id=\"S4.T1.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ wav2bnd</th>\n<td id=\"S4.T1.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">40.45 (38.08)</td>\n<td id=\"S4.T1.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">44.18 (43.08)</td>\n</tr>\n<tr id=\"S4.T1.2.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ JSTTI E2E-refinement</th>\n<td id=\"S4.T1.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">30.97 (29.39)</td>\n<td id=\"S4.T1.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">34.08 (33.71)</td>\n</tr>\n<tr id=\"S4.T1.2.7.5\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ wav2bnd-large</th>\n<td id=\"S4.T1.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.2.7.5.2.1\" class=\"ltx_text ltx_font_bold\">28.00 (26.51)</span></td>\n<td id=\"S4.T1.2.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.2.7.5.3.1\" class=\"ltx_text ltx_font_bold\">32.11 (31.91)</span></td>\n</tr>\n<tr id=\"S4.T1.2.8.6\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">GradSeg</th>\n<td id=\"S4.T1.2.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">47.53 (44.76)</td>\n<td id=\"S4.T1.2.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">52.10 (50.84)</td>\n</tr>\n<tr id=\"S4.T1.2.9.7\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.9.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ wav2bnd</th>\n<td id=\"S4.T1.2.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\">40.45 (38.08)</td>\n<td id=\"S4.T1.2.9.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\">44.18 (43.08)</td>\n</tr>\n<tr id=\"S4.T1.2.10.8\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.10.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ wav2bnd-large</th>\n<td id=\"S4.T1.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_t\">39.11 (36.43)</td>\n<td id=\"S4.T1.2.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_t\">43.75 (42.59)</td>\n</tr>\n<tr id=\"S4.T1.2.11.9\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.11.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">GradSeg</th>\n<td id=\"S4.T1.2.11.9.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">47.53 (44.76)</td>\n<td id=\"S4.T1.2.11.9.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">52.10 (50.84)</td>\n</tr>\n<tr id=\"S4.T1.2.12.10\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.12.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ JSTTI E2E-refinement</th>\n<td id=\"S4.T1.2.12.10.2\" class=\"ltx_td ltx_align_center ltx_border_t\">44.47 (42.02)</td>\n<td id=\"S4.T1.2.12.10.3\" class=\"ltx_td ltx_align_center ltx_border_t\">50.87 (49.79)</td>\n</tr>\n<tr id=\"S4.T1.2.13.11\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.13.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_tt\">Forced Alignment</th>\n<td id=\"S4.T1.2.13.11.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\"><span id=\"S4.T1.2.13.11.2.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">18.06 (18.41)</span></td>\n<td id=\"S4.T1.2.13.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\"><span id=\"S4.T1.2.13.11.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">20.89 (21.30)</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "By comparing the WERs in Table I and the word boundary metrics in Table II, we see that more accurate word boundaries usually lead to reduced WERs, especially as we gradually improve the word boundaries obtained from GradSeg with wav2bnd, JSTTI E2E-refinement and wav2bnd-large (c.f. Section III-E). Obtaining better word boundaries with XLS-R boundary self-training (wav2bnd) before our proposed JSTTI E2E-refinement routine is important, as jointly finetuning the CNN segmenter and the encoder model pre-trained using GradSeg boundaries (GradSeg + JSTTI E2E-refinement) is less effective than using GradSeg + wav2bnd boundaries (GradSeg + wav2bnd + JSTTI E2E-refinement). We also see that simply doing another run of boundary XLS-R self-training on top of the boundary predictions from GradSeg + wav2bnd (GradSeg + wav2bnd + wav2bnd-large) does not lead to a big improvement for the boundary metrics or the WERs.",
            "We further demonstrate the effectiveness of pseudo-text self-training (c.f. Section III-F). We start from a pre-trained HuBERT-Large (300M) checkpoint and finetune the checkpoint under the CTC loss directly calculated on top of word token pseudo-targets. We repeat the same self-training routine on top of pseudo-transcripts obtained from two different word-level unsupervised ASR models, the JSTTI Transformer encoder and the PUSM model, and select from three different word boundary settings in Table I that are most insightful, namely, the forced alignment setting (top-line), the GradSeg + wav2bnd + JSTTI E2E-refinement + wav2bnd-large setting (best unsupervised word boundary setting; c.f. Table II) and the GradSeg + wav2bnd + JSTTI E2E-refinement setting (second-best unsupervised word boundary setting, with performance close to the best setting). We further provide an absolute top-line result where we finetune with the ground-truth paired transcript. Table III shows the results.",
            "As expected, pseudo-text self-training further brings down the WERs of all selected settings and is more effective for the JSTTI-ENC model than for the PUSM model. With self-training, we obtain a final WER of 20.69% without resorting to using forced alignments. We additionally find that there are strong positive correlations between the pseudo WERs calculated between the word sequence predictions from the self-trained HuBERT-Large model and the pseudo-targets (reported in Table IV), the pseudo-target WERs calculated between the pseudo-targets and the ground-truth transcripts (main results reported in Table I), and the final WERs calculated between the word sequence predictions from the self-trained HuBERT-Large model and the ground-truth transcripts (reported in Table III), for both the JSTTI-ENC model and the PUSM model individually, but not across the two models. This observation shows that the pseudo WER logged during pseudo-text self-training could serve as an unsupervised metric. Unfortunately, obtaining this unsupervised metric is computationally intensive, and we leave a more detailed exploration of universal and efficient unsupervised metrics to future work.",
            "We would like to understand if speech features from a visually grounded word discovery model trained with weakly labeled speech-image pairs offer any benefit for the word-level unsupervised ASR task. Table VI reports the WERs when we switch to frame-level features extracted from VG-HuBERT (from the 10thth{}^{\\text{th}} layer of a public VG-HuBERT3 checkpoint [15]). Comparing the rows in Table VI with the relevant rows in Table I, we see that word-level features based on VG-HuBERT are less effective than those based on HuBERT-Large, regardless of what word boundaries we use to pool the word-level features.\nDespite the performance lag in all settings, applying JSTTI E2E-refinement with VG-HuBERT features, together with the subsequent iteration of wav2bnd-large training (c.f. Section III-E), still reduces the WER significantly. We additionally note that JSTTI E2E-refinement with VG-HuBERT frame-level features did not improve the word token F1: the F1 score dropped to 35.59% after JSTTI E2E-refinement and only rose to 42.86% after wav2bnd-large. This is expected as the VG-HuBERT model\u2019s contextualization tends to push word identity information towards the temporal center of each word [15]. We conclude that JSTTI E2E-refinement and wav2bnd refinement can automatically extract feature-specific word-level information."
        ]
    },
    "S4.T2": {
        "caption": "TABLE II:  Boundary Token Precision (T-P) / Recall (T-R) / F-1 (T-F1) in Percentage (%) (with 20ms tolerance) for Unsupervised Word Boundaries Obtained. The best result is shown in bold.",
        "table": "<figure id=\"S4.T2\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE II: </span>Boundary Token Precision (T-P) / Recall (T-R) / F-1 (T-F1) in Percentage (%) (with 20ms tolerance) for Unsupervised Word Boundaries Obtained. The best result is shown in bold.</figcaption>\n<table id=\"S4.T2.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.3.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Method</th>\n<td id=\"S4.T2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">T-P(<math id=\"S4.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T2.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T2.1.1.1.m1.1.1\" xref=\"S4.T2.1.1.1.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.m1.1b\"><ci id=\"S4.T2.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.m1.1c\">\\uparrow</annotation></semantics></math>)</td>\n<td id=\"S4.T2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">T-R(<math id=\"S4.T2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T2.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T2.2.2.2.m1.1.1\" xref=\"S4.T2.2.2.2.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.2.2.2.m1.1b\"><ci id=\"S4.T2.2.2.2.m1.1.1.cmml\" xref=\"S4.T2.2.2.2.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.2.2.2.m1.1c\">\\uparrow</annotation></semantics></math>)</td>\n<td id=\"S4.T2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">T-F1(<math id=\"S4.T2.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T2.3.3.3.m1.1a\"><mo stretchy=\"false\" id=\"S4.T2.3.3.3.m1.1.1\" xref=\"S4.T2.3.3.3.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.3.3.3.m1.1b\"><ci id=\"S4.T2.3.3.3.m1.1.1.cmml\" xref=\"S4.T2.3.3.3.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.3.3.3.m1.1c\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr id=\"S4.T2.3.4.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">VG-HuBERT</th>\n<td id=\"S4.T2.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">24.94</td>\n<td id=\"S4.T2.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">21.18</td>\n<td id=\"S4.T2.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">22.91</td>\n</tr>\n<tr id=\"S4.T2.3.5.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.5.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">GradSeg</th>\n<td id=\"S4.T2.3.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">36.21</td>\n<td id=\"S4.T2.3.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">36.51</td>\n<td id=\"S4.T2.3.5.2.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">36.36</td>\n</tr>\n<tr id=\"S4.T2.3.6.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.6.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ wav2bnd</th>\n<td id=\"S4.T2.3.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">44.59</td>\n<td id=\"S4.T2.3.6.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">44.11</td>\n<td id=\"S4.T2.3.6.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">44.35</td>\n</tr>\n<tr id=\"S4.T2.3.7.4\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.7.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ JSTTI E2E-refinement</th>\n<td id=\"S4.T2.3.7.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">63.17</td>\n<td id=\"S4.T2.3.7.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">63.56</td>\n<td id=\"S4.T2.3.7.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">63.36</td>\n</tr>\n<tr id=\"S4.T2.3.8.5\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.8.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ wav2bnd-large</th>\n<td id=\"S4.T2.3.8.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.3.8.5.2.1\" class=\"ltx_text ltx_font_bold\">64.47</span></td>\n<td id=\"S4.T2.3.8.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.3.8.5.3.1\" class=\"ltx_text ltx_font_bold\">64.67</span></td>\n<td id=\"S4.T2.3.8.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.3.8.5.4.1\" class=\"ltx_text ltx_font_bold\">64.57</span></td>\n</tr>\n<tr id=\"S4.T2.3.9.6\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.9.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">GradSeg</th>\n<td id=\"S4.T2.3.9.6.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">36.21</td>\n<td id=\"S4.T2.3.9.6.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">36.51</td>\n<td id=\"S4.T2.3.9.6.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">36.36</td>\n</tr>\n<tr id=\"S4.T2.3.10.7\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.10.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ wav2bnd</th>\n<td id=\"S4.T2.3.10.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\">44.59</td>\n<td id=\"S4.T2.3.10.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\">44.11</td>\n<td id=\"S4.T2.3.10.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\">44.35</td>\n</tr>\n<tr id=\"S4.T2.3.11.8\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.11.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ wav2bnd-large</th>\n<td id=\"S4.T2.3.11.8.2\" class=\"ltx_td ltx_align_center ltx_border_t\">44.74</td>\n<td id=\"S4.T2.3.11.8.3\" class=\"ltx_td ltx_align_center ltx_border_t\">44.97</td>\n<td id=\"S4.T2.3.11.8.4\" class=\"ltx_td ltx_align_center ltx_border_t\">44.85</td>\n</tr>\n<tr id=\"S4.T2.3.12.9\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.12.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">GradSeg</th>\n<td id=\"S4.T2.3.12.9.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">36.21</td>\n<td id=\"S4.T2.3.12.9.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">36.51</td>\n<td id=\"S4.T2.3.12.9.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">36.36</td>\n</tr>\n<tr id=\"S4.T2.3.13.10\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.13.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">\u00a0\u00a0\u2003+ JSTTI E2E-refinement</th>\n<td id=\"S4.T2.3.13.10.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">37.94</td>\n<td id=\"S4.T2.3.13.10.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">39.03</td>\n<td id=\"S4.T2.3.13.10.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">38.48</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "By comparing the WERs in Table I and the word boundary metrics in Table II, we see that more accurate word boundaries usually lead to reduced WERs, especially as we gradually improve the word boundaries obtained from GradSeg with wav2bnd, JSTTI E2E-refinement and wav2bnd-large (c.f. Section III-E). Obtaining better word boundaries with XLS-R boundary self-training (wav2bnd) before our proposed JSTTI E2E-refinement routine is important, as jointly finetuning the CNN segmenter and the encoder model pre-trained using GradSeg boundaries (GradSeg + JSTTI E2E-refinement) is less effective than using GradSeg + wav2bnd boundaries (GradSeg + wav2bnd + JSTTI E2E-refinement). We also see that simply doing another run of boundary XLS-R self-training on top of the boundary predictions from GradSeg + wav2bnd (GradSeg + wav2bnd + wav2bnd-large) does not lead to a big improvement for the boundary metrics or the WERs.",
            "We further demonstrate the effectiveness of pseudo-text self-training (c.f. Section III-F). We start from a pre-trained HuBERT-Large (300M) checkpoint and finetune the checkpoint under the CTC loss directly calculated on top of word token pseudo-targets. We repeat the same self-training routine on top of pseudo-transcripts obtained from two different word-level unsupervised ASR models, the JSTTI Transformer encoder and the PUSM model, and select from three different word boundary settings in Table I that are most insightful, namely, the forced alignment setting (top-line), the GradSeg + wav2bnd + JSTTI E2E-refinement + wav2bnd-large setting (best unsupervised word boundary setting; c.f. Table II) and the GradSeg + wav2bnd + JSTTI E2E-refinement setting (second-best unsupervised word boundary setting, with performance close to the best setting). We further provide an absolute top-line result where we finetune with the ground-truth paired transcript. Table III shows the results."
        ]
    },
    "S4.T3": {
        "caption": "TABLE III:  WERs for PUSM and JSTTI-ENC models using different word boundary extractors after self-training. The two top lines have been underlined and the best result for each model is shown in bold. The two numbers for the absolute top-line are the same, as no pseduo-transcripts have been used.",
        "table": "<figure id=\"S4.T3\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE III: </span>WERs for PUSM and JSTTI-ENC models using different word boundary extractors after self-training. The two top lines have been underlined and the best result for each model is shown in bold. The two numbers for the absolute top-line are the same, as no pseduo-transcripts have been used.</figcaption>\n<table id=\"S4.T3.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T3.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.2.2.3.1.1\" class=\"ltx_p\">Method</span>\n</span>\n</th>\n<th id=\"S4.T3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-JSTTI-ENC(<math id=\"S4.T3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T3.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T3.1.1.1.m1.1.1\" xref=\"S4.T3.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.1.1.1.m1.1b\"><ci id=\"S4.T3.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n<th id=\"S4.T3.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-PUSM(<math id=\"S4.T3.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T3.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T3.2.2.2.m1.1.1\" xref=\"S4.T3.2.2.2.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.2.2.2.m1.1b\"><ci id=\"S4.T3.2.2.2.m1.1.1.cmml\" xref=\"S4.T3.2.2.2.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.3.1.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T3.2.3.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.2.3.1.1.1.1\" class=\"ltx_p\">GradSeg \n<br class=\"ltx_break\">+ wav2bnd \n<br class=\"ltx_break\">+ JSTTI E2E-refinement</span>\n</span>\n</th>\n<td id=\"S4.T3.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">21.65</td>\n<td id=\"S4.T3.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">29.40</td>\n</tr>\n<tr id=\"S4.T3.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.4.2.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t\" style=\"width:85.4pt;\">\n<span id=\"S4.T3.2.4.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.2.4.2.1.1.1\" class=\"ltx_p\">+ wav2bnd-large</span>\n</span>\n</th>\n<td id=\"S4.T3.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.2.4.2.2.1\" class=\"ltx_text ltx_font_bold\">20.68</span></td>\n<td id=\"S4.T3.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.2.4.2.3.1\" class=\"ltx_text ltx_font_bold\">28.87</span></td>\n</tr>\n<tr id=\"S4.T3.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.5.3.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T3.2.5.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.2.5.3.1.1.1\" class=\"ltx_p\">Forced Alignment</span>\n</span>\n</th>\n<td id=\"S4.T3.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.2.5.3.2.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">12.99</span></td>\n<td id=\"S4.T3.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.2.5.3.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">18.33</span></td>\n</tr>\n<tr id=\"S4.T3.2.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.6.4.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_bb ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T3.2.6.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.2.6.4.1.1.1\" class=\"ltx_p\">Ground-Truth Transcripts</span>\n</span>\n</th>\n<td id=\"S4.T3.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\"><span id=\"S4.T3.2.6.4.2.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">1.08</span></td>\n<td id=\"S4.T3.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\"><span id=\"S4.T3.2.6.4.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">1.08</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We further demonstrate the effectiveness of pseudo-text self-training (c.f. Section III-F). We start from a pre-trained HuBERT-Large (300M) checkpoint and finetune the checkpoint under the CTC loss directly calculated on top of word token pseudo-targets. We repeat the same self-training routine on top of pseudo-transcripts obtained from two different word-level unsupervised ASR models, the JSTTI Transformer encoder and the PUSM model, and select from three different word boundary settings in Table I that are most insightful, namely, the forced alignment setting (top-line), the GradSeg + wav2bnd + JSTTI E2E-refinement + wav2bnd-large setting (best unsupervised word boundary setting; c.f. Table II) and the GradSeg + wav2bnd + JSTTI E2E-refinement setting (second-best unsupervised word boundary setting, with performance close to the best setting). We further provide an absolute top-line result where we finetune with the ground-truth paired transcript. Table III shows the results.",
            "As expected, pseudo-text self-training further brings down the WERs of all selected settings and is more effective for the JSTTI-ENC model than for the PUSM model. With self-training, we obtain a final WER of 20.69% without resorting to using forced alignments. We additionally find that there are strong positive correlations between the pseudo WERs calculated between the word sequence predictions from the self-trained HuBERT-Large model and the pseudo-targets (reported in Table IV), the pseudo-target WERs calculated between the pseudo-targets and the ground-truth transcripts (main results reported in Table I), and the final WERs calculated between the word sequence predictions from the self-trained HuBERT-Large model and the ground-truth transcripts (reported in Table III), for both the JSTTI-ENC model and the PUSM model individually, but not across the two models. This observation shows that the pseudo WER logged during pseudo-text self-training could serve as an unsupervised metric. Unfortunately, obtaining this unsupervised metric is computationally intensive, and we leave a more detailed exploration of universal and efficient unsupervised metrics to future work."
        ]
    },
    "S4.T4": {
        "caption": "TABLE IV:  Pseudo WERs from the three different word boundary settings in Table  III . Pseudo WERs are calculated by treating the pseudo-transcripts as references and the predictions from the self-trained HuBERT-Large models as hypotheses.",
        "table": "<figure id=\"S4.T4\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE IV: </span>Pseudo WERs from the three different word boundary settings in Table <a href=\"#S4.T3\" title=\"TABLE III \u2023 IV-B Main Results: Word-level Speech Recognition and Segmentation \u2023 IV Experiments \u2023 Towards Unsupervised Speech Recognition Without Pronunciation Models This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>. Pseudo WERs are calculated by treating the pseudo-transcripts as references and the predictions from the self-trained HuBERT-Large models as hypotheses.</figcaption>\n<table id=\"S4.T4.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"width:79.7pt;\">\n<span id=\"S4.T4.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T4.2.2.3.1.1\" class=\"ltx_p\">Method</span>\n</span>\n</th>\n<th id=\"S4.T4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PWER-JSTTI-ENC(<math id=\"S4.T4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T4.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T4.1.1.1.m1.1.1\" xref=\"S4.T4.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.1.1.1.m1.1b\"><ci id=\"S4.T4.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n<th id=\"S4.T4.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PWER-PUSM(<math id=\"S4.T4.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T4.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T4.2.2.2.m1.1.1\" xref=\"S4.T4.2.2.2.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.2.2.2.m1.1b\"><ci id=\"S4.T4.2.2.2.m1.1.1.cmml\" xref=\"S4.T4.2.2.2.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.3.1.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_tt\" style=\"width:79.7pt;\">\n<span id=\"S4.T4.2.3.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T4.2.3.1.1.1.1\" class=\"ltx_p\">GradSeg \n<br class=\"ltx_break\">+ wav2bnd \n<br class=\"ltx_break\">+ JSTTI E2E-refinement</span>\n</span>\n</th>\n<td id=\"S4.T4.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">19.76</td>\n<td id=\"S4.T4.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">14.14</td>\n</tr>\n<tr id=\"S4.T4.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.4.2.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t\" style=\"width:79.7pt;\">\n<span id=\"S4.T4.2.4.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T4.2.4.2.1.1.1\" class=\"ltx_p\">+ wav2bnd-large</span>\n</span>\n</th>\n<td id=\"S4.T4.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">16.45</td>\n<td id=\"S4.T4.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">11.30</td>\n</tr>\n<tr id=\"S4.T4.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.5.3.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_bb ltx_border_tt\" style=\"width:79.7pt;\">\n<span id=\"S4.T4.2.5.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T4.2.5.3.1.1.1\" class=\"ltx_p\">Forced Alignment</span>\n</span>\n</th>\n<td id=\"S4.T4.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">12.17</td>\n<td id=\"S4.T4.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">7.82</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "As expected, pseudo-text self-training further brings down the WERs of all selected settings and is more effective for the JSTTI-ENC model than for the PUSM model. With self-training, we obtain a final WER of 20.69% without resorting to using forced alignments. We additionally find that there are strong positive correlations between the pseudo WERs calculated between the word sequence predictions from the self-trained HuBERT-Large model and the pseudo-targets (reported in Table IV), the pseudo-target WERs calculated between the pseudo-targets and the ground-truth transcripts (main results reported in Table I), and the final WERs calculated between the word sequence predictions from the self-trained HuBERT-Large model and the ground-truth transcripts (reported in Table III), for both the JSTTI-ENC model and the PUSM model individually, but not across the two models. This observation shows that the pseudo WER logged during pseudo-text self-training could serve as an unsupervised metric. Unfortunately, obtaining this unsupervised metric is computationally intensive, and we leave a more detailed exploration of universal and efficient unsupervised metrics to future work."
        ]
    },
    "S4.T5": {
        "caption": "TABLE V:  WERs for PUSM models trained under the modified update strategy against two baselines. The best result for each word boundary setting is shown in bold.",
        "table": "<figure id=\"S4.T5\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE V: </span>WERs for PUSM models trained under the modified update strategy against two baselines. The best result for each word boundary setting is shown in bold.</figcaption>\n<table id=\"S4.T5.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T5.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.3.4\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T5.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.3.4.1.1\" class=\"ltx_p\">Method</span>\n</span>\n</td>\n<td id=\"S4.T5.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.1.1.1.1.1\" class=\"ltx_p\">WER-6k(<math id=\"S4.T5.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T5.1.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T5.1.1.1.1.1.m1.1.1\" xref=\"S4.T5.1.1.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.1.1.1.1.1.m1.1b\"><ci id=\"S4.T5.1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.1.1.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.1.1.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</span>\n</span>\n</td>\n<td id=\"S4.T5.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.2.2.2.1.1\" class=\"ltx_p\">WER-6k-20 (<math id=\"S4.T5.2.2.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T5.2.2.2.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T5.2.2.2.1.1.m1.1.1\" xref=\"S4.T5.2.2.2.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.2.2.2.1.1.m1.1b\"><ci id=\"S4.T5.2.2.2.1.1.m1.1.1.cmml\" xref=\"S4.T5.2.2.2.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.2.2.2.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</span>\n</span>\n</td>\n<td id=\"S4.T5.3.3.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.3.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.3.3.1.1\" class=\"ltx_p\">WER-120k(<math id=\"S4.T5.3.3.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T5.3.3.3.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T5.3.3.3.1.1.m1.1.1\" xref=\"S4.T5.3.3.3.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.3.3.3.1.1.m1.1b\"><ci id=\"S4.T5.3.3.3.1.1.m1.1.1.cmml\" xref=\"S4.T5.3.3.3.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.3.3.3.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.4.1\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.4.1.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T5.3.4.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.4.1.1.1.1\" class=\"ltx_p\">GradSeg + wav2bnd</span>\n</span>\n</td>\n<td id=\"S4.T5.3.4.1.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.3.4.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.4.1.2.1.1\" class=\"ltx_p\">52.33</span>\n</span>\n</td>\n<td id=\"S4.T5.3.4.1.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.3.4.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.4.1.3.1.1\" class=\"ltx_p\">51.27</span>\n</span>\n</td>\n<td id=\"S4.T5.3.4.1.4\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.3.4.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.4.1.4.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.4.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\">44.18</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.5.2\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.5.2.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:85.4pt;\">\n<span id=\"S4.T5.3.5.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.5.2.1.1.1\" class=\"ltx_p\">+ JSTTI E2E-refinement</span>\n</span>\n</td>\n<td id=\"S4.T5.3.5.2.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.3.5.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.5.2.2.1.1\" class=\"ltx_p\">41.29</span>\n</span>\n</td>\n<td id=\"S4.T5.3.5.2.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.3.5.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.5.2.3.1.1\" class=\"ltx_p\">40.62</span>\n</span>\n</td>\n<td id=\"S4.T5.3.5.2.4\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.3.5.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.5.2.4.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.5.2.4.1.1.1\" class=\"ltx_text ltx_font_bold\">34.08</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.6.3\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.6.3.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:85.4pt;\">\n<span id=\"S4.T5.3.6.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.6.3.1.1.1\" class=\"ltx_p\">+ wav2bnd-large</span>\n</span>\n</td>\n<td id=\"S4.T5.3.6.3.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.3.6.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.6.3.2.1.1\" class=\"ltx_p\">39.60</span>\n</span>\n</td>\n<td id=\"S4.T5.3.6.3.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.3.6.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.6.3.3.1.1\" class=\"ltx_p\">38.75</span>\n</span>\n</td>\n<td id=\"S4.T5.3.6.3.4\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.3.6.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.6.3.4.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.6.3.4.1.1.1\" class=\"ltx_text ltx_font_bold\">32.11</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.7.4\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.7.4.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T5.3.7.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.7.4.1.1.1\" class=\"ltx_p\">Forced Alignment</span>\n</span>\n</td>\n<td id=\"S4.T5.3.7.4.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_tt\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.3.7.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.7.4.2.1.1\" class=\"ltx_p\">24.46</span>\n</span>\n</td>\n<td id=\"S4.T5.3.7.4.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_tt\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.3.7.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.7.4.3.1.1\" class=\"ltx_p\">23.91</span>\n</span>\n</td>\n<td id=\"S4.T5.3.7.4.4\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_tt\" style=\"width:28.5pt;\">\n<span id=\"S4.T5.3.7.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.7.4.4.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.7.4.4.1.1.1\" class=\"ltx_text ltx_font_bold\">20.89</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We repeat the three training settings under four unsupervised word boundaries for extracting speech tokens and show the comparison in Table V."
        ]
    },
    "S4.T6": {
        "caption": "TABLE VI:  WERs obtained with JSTTI-ENC and PUSM models with different word boundaries, using VG-HuBERT features (flagged with \u201c-VG\u201d). The top-line has been underlined and the best result for each model is shown in bold.",
        "table": "<figure id=\"S4.T6\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE VI: </span>WERs obtained with JSTTI-ENC and PUSM models with different word boundaries, using VG-HuBERT features (flagged with \u201c-VG\u201d). The top-line has been underlined and the best result for each model is shown in bold.</figcaption>\n<table id=\"S4.T6.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T6.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T6.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" style=\"width:99.6pt;\">\n<span id=\"S4.T6.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.2.3.1.1\" class=\"ltx_p\">Method</span>\n</span>\n</th>\n<th id=\"S4.T6.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.1.1.1.1.1\" class=\"ltx_p\">WER-JSTTI-ENC-VG(<math id=\"S4.T6.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T6.1.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T6.1.1.1.1.1.m1.1.1\" xref=\"S4.T6.1.1.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.1.1.1.1.1.m1.1b\"><ci id=\"S4.T6.1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T6.1.1.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.1.1.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</span>\n</span>\n</th>\n<th id=\"S4.T6.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.2.2.1.1\" class=\"ltx_p\">WER-PUSM-VG(<math id=\"S4.T6.2.2.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T6.2.2.2.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T6.2.2.2.1.1.m1.1.1\" xref=\"S4.T6.2.2.2.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.2.2.2.1.1.m1.1b\"><ci id=\"S4.T6.2.2.2.1.1.m1.1.1.cmml\" xref=\"S4.T6.2.2.2.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.2.2.2.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T6.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T6.2.3.1.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:99.6pt;\">\n<span id=\"S4.T6.2.3.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.3.1.1.1.1\" class=\"ltx_p\">VG-HuBERT</span>\n</span>\n</td>\n<td id=\"S4.T6.2.3.1.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.2.3.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.3.1.2.1.1\" class=\"ltx_p\">50.39 (50.26)</span>\n</span>\n</td>\n<td id=\"S4.T6.2.3.1.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.2.3.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.3.1.3.1.1\" class=\"ltx_p\">56.44 (56.43)</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T6.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T6.2.4.2.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:99.6pt;\">\n<span id=\"S4.T6.2.4.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.4.2.1.1.1\" class=\"ltx_p\">GradSeg</span>\n</span>\n</td>\n<td id=\"S4.T6.2.4.2.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.2.4.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.4.2.2.1.1\" class=\"ltx_p\">50.90 (47.47)</span>\n</span>\n</td>\n<td id=\"S4.T6.2.4.2.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.2.4.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.4.2.3.1.1\" class=\"ltx_p\">53.54 (51.35)</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T6.2.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T6.2.5.3.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:99.6pt;\">\n<span id=\"S4.T6.2.5.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.5.3.1.1.1\" class=\"ltx_p\">+ wav2bnd</span>\n</span>\n</td>\n<td id=\"S4.T6.2.5.3.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.2.5.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.5.3.2.1.1\" class=\"ltx_p\">44.10 (41.31)</span>\n</span>\n</td>\n<td id=\"S4.T6.2.5.3.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.2.5.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.5.3.3.1.1\" class=\"ltx_p\">47.20 (45.79)</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T6.2.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T6.2.6.4.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:99.6pt;\">\n<span id=\"S4.T6.2.6.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.6.4.1.1.1\" class=\"ltx_p\">+ JSTTI E2E-refinement</span>\n</span>\n</td>\n<td id=\"S4.T6.2.6.4.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.2.6.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.6.4.2.1.1\" class=\"ltx_p\">36.29 (35.52)</span>\n</span>\n</td>\n<td id=\"S4.T6.2.6.4.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.2.6.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.6.4.3.1.1\" class=\"ltx_p\">38.43 (38.31)</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T6.2.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T6.2.7.5.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:99.6pt;\">\n<span id=\"S4.T6.2.7.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.7.5.1.1.1\" class=\"ltx_p\">+ wav2bnd-large</span>\n</span>\n</td>\n<td id=\"S4.T6.2.7.5.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.2.7.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.7.5.2.1.1\" class=\"ltx_p\"><span id=\"S4.T6.2.7.5.2.1.1.1\" class=\"ltx_text ltx_font_bold\">33.25 (31.55)</span></span>\n</span>\n</td>\n<td id=\"S4.T6.2.7.5.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.2.7.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.7.5.3.1.1\" class=\"ltx_p\"><span id=\"S4.T6.2.7.5.3.1.1.1\" class=\"ltx_text ltx_font_bold\">37.03 (36.55)</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T6.2.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T6.2.8.6.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_tt\" style=\"width:99.6pt;\">\n<span id=\"S4.T6.2.8.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.8.6.1.1.1\" class=\"ltx_p\">Forced Alignment</span>\n</span>\n</td>\n<td id=\"S4.T6.2.8.6.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_tt\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.2.8.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.8.6.2.1.1\" class=\"ltx_p\"><span id=\"S4.T6.2.8.6.2.1.1.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">22.06 (22.85)</span></span>\n</span>\n</td>\n<td id=\"S4.T6.2.8.6.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_tt\" style=\"width:56.9pt;\">\n<span id=\"S4.T6.2.8.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T6.2.8.6.3.1.1\" class=\"ltx_p\"><span id=\"S4.T6.2.8.6.3.1.1.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">25.26 (25.53)</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We would like to understand if speech features from a visually grounded word discovery model trained with weakly labeled speech-image pairs offer any benefit for the word-level unsupervised ASR task. Table VI reports the WERs when we switch to frame-level features extracted from VG-HuBERT (from the 10thth{}^{\\text{th}} layer of a public VG-HuBERT3 checkpoint [15]). Comparing the rows in Table VI with the relevant rows in Table I, we see that word-level features based on VG-HuBERT are less effective than those based on HuBERT-Large, regardless of what word boundaries we use to pool the word-level features.\nDespite the performance lag in all settings, applying JSTTI E2E-refinement with VG-HuBERT features, together with the subsequent iteration of wav2bnd-large training (c.f. Section III-E), still reduces the WER significantly. We additionally note that JSTTI E2E-refinement with VG-HuBERT frame-level features did not improve the word token F1: the F1 score dropped to 35.59% after JSTTI E2E-refinement and only rose to 42.86% after wav2bnd-large. This is expected as the VG-HuBERT model\u2019s contextualization tends to push word identity information towards the temporal center of each word [15]. We conclude that JSTTI E2E-refinement and wav2bnd refinement can automatically extract feature-specific word-level information."
        ]
    },
    "S4.T7": {
        "caption": "TABLE VII:  WERs obtained with the JSTTI-ENC and the JSTTI-ENC-DEC models under different word boundaries. We underline the top-line results, and the lower WER within each row is shown in bold.",
        "table": "<figure id=\"S4.T7\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE VII: </span>WERs obtained with the JSTTI-ENC and the JSTTI-ENC-DEC models under different word boundaries. We underline the top-line results, and the lower WER within each row is shown in bold.</figcaption>\n<table id=\"S4.T7.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T7.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T7.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>\n<th id=\"S4.T7.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-ENC(<math id=\"S4.T7.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T7.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T7.1.1.1.m1.1.1\" xref=\"S4.T7.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.1.1.1.m1.1b\"><ci id=\"S4.T7.1.1.1.m1.1.1.cmml\" xref=\"S4.T7.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n<th id=\"S4.T7.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-ENC-DEC(<math id=\"S4.T7.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T7.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T7.2.2.2.m1.1.1\" xref=\"S4.T7.2.2.2.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.2.2.2.m1.1b\"><ci id=\"S4.T7.2.2.2.m1.1.1.cmml\" xref=\"S4.T7.2.2.2.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T7.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T7.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">VG-HuBERT</th>\n<td id=\"S4.T7.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T7.2.3.1.2.1\" class=\"ltx_text ltx_font_bold\">47.99 (47.99)</span></td>\n<td id=\"S4.T7.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">56.14 (56.24)</td>\n</tr>\n<tr id=\"S4.T7.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T7.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">GradSeg</th>\n<td id=\"S4.T7.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T7.2.4.2.2.1\" class=\"ltx_text ltx_font_bold\">47.53 (44.76)</span></td>\n<td id=\"S4.T7.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">55.46 (53.42)</td>\n</tr>\n<tr id=\"S4.T7.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T7.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ wav2bnd</th>\n<td id=\"S4.T7.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T7.2.5.3.2.1\" class=\"ltx_text ltx_font_bold\">40.45 (38.08)</span></td>\n<td id=\"S4.T7.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">47.60 (46.03)</td>\n</tr>\n<tr id=\"S4.T7.2.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T7.2.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ JSTTI E2E-refinement</th>\n<td id=\"S4.T7.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T7.2.6.4.2.1\" class=\"ltx_text ltx_font_bold\">30.97 (29.39)</span></td>\n<td id=\"S4.T7.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">39.97 (39.23)</td>\n</tr>\n<tr id=\"S4.T7.2.7.5\" class=\"ltx_tr\">\n<th id=\"S4.T7.2.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\u00a0\u00a0\u2003+ wav2bnd-large</th>\n<td id=\"S4.T7.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T7.2.7.5.2.1\" class=\"ltx_text ltx_font_bold\">28.00 (26.51)</span></td>\n<td id=\"S4.T7.2.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">34.61 (33.97)</td>\n</tr>\n<tr id=\"S4.T7.2.8.6\" class=\"ltx_tr\">\n<th id=\"S4.T7.2.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_tt\">Forced Alignment</th>\n<td id=\"S4.T7.2.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\"><span id=\"S4.T7.2.8.6.2.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">18.06 (18.41)</span></td>\n<td id=\"S4.T7.2.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\"><span id=\"S4.T7.2.8.6.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">27.16 (26.98)</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We report the comparison under different boundary settings in Table VII. Note that the basic JSTTI encoder-decoder model can be similarly augmented with JSTTI E2E-refinement and wav2bnd-large refinement (c.f. Section III-E), and the corresponding results are additionally reported in Table VII. Running JSTTI E2E-refinement and wav2bnd-large refinement with the encoder-decoder model gives us a total of two additional sets of word boundaries, and we further compare the quality of these new boundaries with those obtained with the encoder-only model in Table VIII.",
            "Table VII shows that the JSTTI-ENC-DEC model combined with the encoder state matching method generally gives worse error rates. We believe this is because the encoder-only model directly applies the text output layer to the first encoder layer and could better utilize contextualized representation during evaluation, while the encoder state matching method used for evaluating the encoder-decoder model cannot."
        ]
    },
    "S4.T8": {
        "caption": "TABLE VIII:  WERs obtained with the JSTTI-ENC and the JSTTI-ENC-DEC models after they are re-trained on top of the two sets of refined word boundaries obtained with JSTTI-ENC and the two additional sets of refined boundaries obtained with JSTTI-ENC-DEC",
        "table": "<figure id=\"S4.T8\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE VIII: </span>WERs obtained with the JSTTI-ENC and the JSTTI-ENC-DEC models after they are re-trained on top of the two sets of refined word boundaries obtained with JSTTI-ENC and the two additional sets of refined boundaries obtained with JSTTI-ENC-DEC</figcaption>\n<table id=\"S4.T8.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T8.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T8.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T8.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T8.2.2.3.1.1\" class=\"ltx_p\">Method</span>\n</span>\n</th>\n<th id=\"S4.T8.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-ENC(<math id=\"S4.T8.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T8.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T8.1.1.1.m1.1.1\" xref=\"S4.T8.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T8.1.1.1.m1.1b\"><ci id=\"S4.T8.1.1.1.m1.1.1.cmml\" xref=\"S4.T8.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T8.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n<th id=\"S4.T8.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-ENC-DEC(<math id=\"S4.T8.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T8.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T8.2.2.2.m1.1.1\" xref=\"S4.T8.2.2.2.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T8.2.2.2.m1.1b\"><ci id=\"S4.T8.2.2.2.m1.1.1.cmml\" xref=\"S4.T8.2.2.2.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T8.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T8.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T8.2.3.1.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T8.2.3.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T8.2.3.1.1.1.1\" class=\"ltx_p\">GradSeg \n<br class=\"ltx_break\">+ wav2bnd \n<br class=\"ltx_break\">+ JSTTI E2E-refinement \n<br class=\"ltx_break\">(w/ JSTTI-ENC-DEC)*</span>\n</span>\n</th>\n<td id=\"S4.T8.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">30.50 (28.96)</td>\n<td id=\"S4.T8.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">37.64 (36.82)</td>\n</tr>\n<tr id=\"S4.T8.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T8.2.4.2.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t\" style=\"width:85.4pt;\">\n<span id=\"S4.T8.2.4.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T8.2.4.2.1.1.1\" class=\"ltx_p\">+ wav2bnd-large</span>\n</span>\n</th>\n<td id=\"S4.T8.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">27.97 (26.47)</td>\n<td id=\"S4.T8.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">34.61 (33.97)</td>\n</tr>\n<tr id=\"S4.T8.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T8.2.5.3.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T8.2.5.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T8.2.5.3.1.1.1\" class=\"ltx_p\">GradSeg \n<br class=\"ltx_break\">+ wav2bnd \n<br class=\"ltx_break\">+ JSTTI E2E-refinement \n<br class=\"ltx_break\">(w/ JSTTI-ENC)*</span>\n</span>\n</th>\n<td id=\"S4.T8.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">29.90 (28.26)</td>\n<td id=\"S4.T8.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">37.16 (36.17)</td>\n</tr>\n<tr id=\"S4.T8.2.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T8.2.6.4.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"width:85.4pt;\">\n<span id=\"S4.T8.2.6.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T8.2.6.4.1.1.1\" class=\"ltx_p\">+ wav2bnd-large</span>\n</span>\n</th>\n<td id=\"S4.T8.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">28.00 (26.51)</td>\n<td id=\"S4.T8.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">35.08 (34.15)</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We report the comparison under different boundary settings in Table VII. Note that the basic JSTTI encoder-decoder model can be similarly augmented with JSTTI E2E-refinement and wav2bnd-large refinement (c.f. Section III-E), and the corresponding results are additionally reported in Table VII. Running JSTTI E2E-refinement and wav2bnd-large refinement with the encoder-decoder model gives us a total of two additional sets of word boundaries, and we further compare the quality of these new boundaries with those obtained with the encoder-only model in Table VIII.",
            "By comparing the WERs in the first row and third row of Table VIII within either the left or right column, we see that both the encoder-decoder architecture and the encoder-only architecture are almost equally capable of refining the initial GradSeg + wav2bnd word boundaries via the JSTTI E2E-refinement routine. This continues to hold as we further apply wav2bnd-large refinement, as shown in the second row and the fourth row of Table VIII within either the left or right column."
        ]
    },
    "S4.T9": {
        "caption": "TABLE IX:  WERs for the JSTTI-ENC model for the GradSeg + wav2bnd + JSTTI E2E-refinement method under three different regularization strengths. The best results are shown in bold",
        "table": "<figure id=\"S4.T9\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE IX: </span>WERs for the JSTTI-ENC model for the GradSeg + wav2bnd + JSTTI E2E-refinement method under three different regularization strengths. The best results are shown in bold</figcaption>\n<table id=\"S4.T9.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T9.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T9.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T9.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T9.2.2.3.1.1\" class=\"ltx_p\">Regularization Strength</span>\n</span>\n</th>\n<th id=\"S4.T9.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-JSTTI-ENC(<math id=\"S4.T9.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T9.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T9.1.1.1.m1.1.1\" xref=\"S4.T9.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T9.1.1.1.m1.1b\"><ci id=\"S4.T9.1.1.1.m1.1.1.cmml\" xref=\"S4.T9.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T9.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n<th id=\"S4.T9.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Token F-1(<math id=\"S4.T9.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T9.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T9.2.2.2.m1.1.1\" xref=\"S4.T9.2.2.2.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T9.2.2.2.m1.1b\"><ci id=\"S4.T9.2.2.2.m1.1.1.cmml\" xref=\"S4.T9.2.2.2.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T9.2.2.2.m1.1c\">\\uparrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T9.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T9.2.3.1.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T9.2.3.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T9.2.3.1.1.1.1\" class=\"ltx_p\">Small Regularization</span>\n</span>\n</th>\n<td id=\"S4.T9.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">35.25 (33.25)</td>\n<td id=\"S4.T9.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">58.27</td>\n</tr>\n<tr id=\"S4.T9.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T9.2.4.2.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t\" style=\"width:85.4pt;\">\n<span id=\"S4.T9.2.4.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T9.2.4.2.1.1.1\" class=\"ltx_p\">Medium Regularization</span>\n</span>\n</th>\n<td id=\"S4.T9.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S4.T9.2.4.2.2.1\" class=\"ltx_text ltx_font_bold\">30.97</span> (29.39)</td>\n<td id=\"S4.T9.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T9.2.4.2.3.1\" class=\"ltx_text ltx_font_bold\">63.36</span></td>\n</tr>\n<tr id=\"S4.T9.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T9.2.5.3.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"width:85.4pt;\">\n<span id=\"S4.T9.2.5.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T9.2.5.3.1.1.1\" class=\"ltx_p\">Strong Regularization</span>\n</span>\n</th>\n<td id=\"S4.T9.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T9.2.5.3.2.1\" class=\"ltx_text ltx_font_bold\">30.97 (29.18)</span></td>\n<td id=\"S4.T9.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">63.31</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "We add the word count loss and the word frequency loss (c.f. Eqs. (17), (18) and (19)) for our JSTTI E2E-refinement routine (c.f. Section III-E) to improve the word-level segmental structure. To show that adding this regularization is crucial, we repeat the refinement routine with three distinct sets of regularization strengths. We denote them as small regularization with \u03b31=5,\u03b32=5000formulae-sequencesubscript\ud835\udefe15subscript\ud835\udefe25000\\gamma_{1}=5,\\gamma_{2}=5000, medium regularization with \u03b31=50,\u03b32=500000formulae-sequencesubscript\ud835\udefe150subscript\ud835\udefe2500000\\gamma_{1}=50,\\gamma_{2}=500000 and strong regularization with \u03b31=50000,\u03b32=50000000formulae-sequencesubscript\ud835\udefe150000subscript\ud835\udefe250000000\\gamma_{1}=50000,\\gamma_{2}=50000000. The medium regularization setting here corresponds to what is reported for the setting GradSeg + wav2bnd + JSTTI E2E-refinement in Tables I and II. The three runs are reported in Table IX.",
            "From Table IX, we see that performance is harmed by setting the regularization coefficients \u03b31subscript\ud835\udefe1\\gamma_{1} and \u03b32subscript\ud835\udefe2\\gamma_{2} too small, but that setting them too large is not greatly detrimental."
        ]
    },
    "S4.T10": {
        "caption": "TABLE X:  WERs obtained with the JSTTI-ENC and the PUSM models, trained using discrete speech cluster sequences obtained with different pairs of boundaries and cluster centroids. The result with a lower WER for each pairwise comparison is shown in bold.",
        "table": "<figure id=\"S4.T10\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE X: </span>WERs obtained with the JSTTI-ENC and the PUSM models, trained using discrete speech cluster sequences obtained with different pairs of boundaries and cluster centroids. The result with a lower WER for each pairwise comparison is shown in bold.</figcaption>\n<table id=\"S4.T10.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T10.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T10.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T10.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T10.2.2.3.1.1\" class=\"ltx_p\">Settings</span>\n</span>\n</th>\n<th id=\"S4.T10.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-JSTTI-ENC(<math id=\"S4.T10.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T10.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T10.1.1.1.m1.1.1\" xref=\"S4.T10.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T10.1.1.1.m1.1b\"><ci id=\"S4.T10.1.1.1.m1.1.1.cmml\" xref=\"S4.T10.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T10.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n<th id=\"S4.T10.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-PUSM(<math id=\"S4.T10.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T10.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T10.2.2.2.m1.1.1\" xref=\"S4.T10.2.2.2.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T10.2.2.2.m1.1b\"><ci id=\"S4.T10.2.2.2.m1.1.1.cmml\" xref=\"S4.T10.2.2.2.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T10.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T10.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T10.2.3.1.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T10.2.3.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T10.2.3.1.1.1.1\" class=\"ltx_p\">FA BND &amp; Matched CLUS</span>\n</span>\n</th>\n<td id=\"S4.T10.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T10.2.3.1.2.1\" class=\"ltx_text ltx_font_bold\">17.86 (18.23)</span></td>\n<td id=\"S4.T10.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T10.2.3.1.3.1\" class=\"ltx_text ltx_font_bold\">20.20 (20.60)</span></td>\n</tr>\n<tr id=\"S4.T10.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T10.2.4.2.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t\" style=\"width:85.4pt;\">\n<span id=\"S4.T10.2.4.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T10.2.4.2.1.1.1\" class=\"ltx_p\">FA BND &amp; VG-HUBERT CLUS</span>\n</span>\n</th>\n<td id=\"S4.T10.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">18.06 (18.41)</td>\n<td id=\"S4.T10.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">20.89 (21.30)</td>\n</tr>\n<tr id=\"S4.T10.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T10.2.5.3.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T10.2.5.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T10.2.5.3.1.1.1\" class=\"ltx_p\"><span id=\"S4.T10.2.5.3.1.1.1.1\" class=\"ltx_text ltx_font_italic\">GradSeg + wav2bnd</span> BND &amp; Matched CLUS</span>\n</span>\n</th>\n<td id=\"S4.T10.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">45.21 (43.50)</td>\n<td id=\"S4.T10.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">45.91 (45.46)</td>\n</tr>\n<tr id=\"S4.T10.2.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T10.2.6.4.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t\" style=\"width:85.4pt;\">\n<span id=\"S4.T10.2.6.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T10.2.6.4.1.1.1\" class=\"ltx_p\"><span id=\"S4.T10.2.6.4.1.1.1.1\" class=\"ltx_text ltx_font_italic\">GradSeg + wav2bnd</span> BND &amp; VG-HUBERT CLUS</span>\n</span>\n</th>\n<td id=\"S4.T10.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T10.2.6.4.2.1\" class=\"ltx_text ltx_font_bold\">40.45 (38.08)</span></td>\n<td id=\"S4.T10.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T10.2.6.4.3.1\" class=\"ltx_text ltx_font_bold\">44.18 (43.08)</span></td>\n</tr>\n<tr id=\"S4.T10.2.7.5\" class=\"ltx_tr\">\n<th id=\"S4.T10.2.7.5.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T10.2.7.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T10.2.7.5.1.1.1\" class=\"ltx_p\"><span id=\"S4.T10.2.7.5.1.1.1.1\" class=\"ltx_text ltx_font_italic\">GradSeg + wav2bnd BND</span> \n<br class=\"ltx_break\">&amp; Matched CLUS (w/ JSTTI E2E-refinement)</span>\n</span>\n</th>\n<td id=\"S4.T10.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">38.89 (37.31)</td>\n<td id=\"S4.T10.2.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">39.71 (39.32)</td>\n</tr>\n<tr id=\"S4.T10.2.8.6\" class=\"ltx_tr\">\n<th id=\"S4.T10.2.8.6.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t\" style=\"width:85.4pt;\">\n<span id=\"S4.T10.2.8.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T10.2.8.6.1.1.1\" class=\"ltx_p\"><span id=\"S4.T10.2.8.6.1.1.1.1\" class=\"ltx_text ltx_font_italic\">GradSeg + wav2bnd</span> BND &amp; VG-HUBERT CLUS (w/ JSTTI E2E-refinement)</span>\n</span>\n</th>\n<td id=\"S4.T10.2.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T10.2.8.6.2.1\" class=\"ltx_text ltx_font_bold\">30.97 (29.39)</span></td>\n<td id=\"S4.T10.2.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T10.2.8.6.3.1\" class=\"ltx_text ltx_font_bold\">34.08 (33.71)</span></td>\n</tr>\n<tr id=\"S4.T10.2.9.7\" class=\"ltx_tr\">\n<th id=\"S4.T10.2.9.7.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_tt\" style=\"width:85.4pt;\">\n<span id=\"S4.T10.2.9.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T10.2.9.7.1.1.1\" class=\"ltx_p\"><span id=\"S4.T10.2.9.7.1.1.1.1\" class=\"ltx_text ltx_font_italic\">GradSeg + wav2bnd + JSTTI E2E-refinement + wav2bnd-large</span> BND &amp; Matched CLUS</span>\n</span>\n</th>\n<td id=\"S4.T10.2.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">28.97 (27.63)</td>\n<td id=\"S4.T10.2.9.7.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">32.44 (32.35)</td>\n</tr>\n<tr id=\"S4.T10.2.10.8\" class=\"ltx_tr\">\n<th id=\"S4.T10.2.10.8.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"width:85.4pt;\">\n<span id=\"S4.T10.2.10.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T10.2.10.8.1.1.1\" class=\"ltx_p\"><span id=\"S4.T10.2.10.8.1.1.1.1\" class=\"ltx_text ltx_font_italic\">GradSeg + wav2bnd + JSTTI E2E-refinement + wav2bnd-large</span> BND &amp; VG-HuBERT CLUS</span>\n</span>\n</th>\n<td id=\"S4.T10.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T10.2.10.8.2.1\" class=\"ltx_text ltx_font_bold\">28.00 (26.51)</span></td>\n<td id=\"S4.T10.2.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T10.2.10.8.3.1\" class=\"ltx_text ltx_font_bold\">32.11 (31.91)</span></td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Throughout our experiments, we used different sets of word boundaries for temporal pooling of the word-level feature vectors but kept the cluster centroids fixed across the speech token extraction process (c.f. Section III-B). These cluster centroids were obtained by training a k-means model on top of word-level speech feature vectors mean-pooled using the word boundaries predicted by VG-HuBERT. We denote cluster centroids obtained this way as VG-HuBERT-induced cluster centroids. Doing so saved training time as we could simply finetune a previous JSTTI-ENC model checkpoint once better-quality speech token sequences become available. To show that this choice is indeed optimal, we train the JSTTI-ENC model and the PUSM model in six different settings as shown below, and the results are reported in Table X:"
        ]
    },
    "S4.T11": {
        "caption": "TABLE XI:  WERs for the JSTTI-ENC and PUSM models as we feed models trained with worse-quality speech token sequences (as shown) but tested with more accurately segmented speech tokens ( GradSeg + wav2bnd +JSTTI E2E-refinement + wav2bnd-large  in all cases).",
        "table": "<figure id=\"S4.T11\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE XI: </span>WERs for the JSTTI-ENC and PUSM models as we feed models trained with worse-quality speech token sequences (as shown) but tested with more accurately segmented speech tokens (<span id=\"S4.T11.4.1\" class=\"ltx_text ltx_font_italic\">GradSeg + wav2bnd +JSTTI E2E-refinement + wav2bnd-large</span> in all cases).</figcaption>\n<table id=\"S4.T11.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T11.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T11.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>\n<th id=\"S4.T11.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-JSTTI-ENC(<math id=\"S4.T11.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T11.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T11.1.1.1.m1.1.1\" xref=\"S4.T11.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T11.1.1.1.m1.1b\"><ci id=\"S4.T11.1.1.1.m1.1.1.cmml\" xref=\"S4.T11.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T11.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n<th id=\"S4.T11.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-PUSM(<math id=\"S4.T11.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T11.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T11.2.2.2.m1.1.1\" xref=\"S4.T11.2.2.2.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T11.2.2.2.m1.1b\"><ci id=\"S4.T11.2.2.2.m1.1.1.cmml\" xref=\"S4.T11.2.2.2.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T11.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T11.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T11.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">GradSeg</th>\n<td id=\"S4.T11.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">29.59 (28.29)</td>\n<td id=\"S4.T11.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">35.18 (34.89)</td>\n</tr>\n<tr id=\"S4.T11.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T11.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">+ wav2bnd</th>\n<td id=\"S4.T11.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">29.40 (28.00)</td>\n<td id=\"S4.T11.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">33.82 (33.58)</td>\n</tr>\n<tr id=\"S4.T11.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T11.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">+ JSTTI E2E-refinement*</th>\n<td id=\"S4.T11.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">28.09 (26.85)</td>\n<td id=\"S4.T11.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">32.39 (32.15)</td>\n</tr>\n<tr id=\"S4.T11.2.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T11.2.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">+ wav2bnd-large</th>\n<td id=\"S4.T11.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">28.00 (26.51)</td>\n<td id=\"S4.T11.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">32.11 (31.91)</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "Previously, in Section IV-B, we see that the JSTTI E2E-refinement boundary refinement process depends on the quality of the initial word boundaries: the E2E refinement algorithm can achieve better results when initialized using GradSeg + wav2bnd than when initialized using GradSeg. We wish to determine whether this sensitivity to initial conditions also characterizes the relationship between training and evaluation data, i.e., if the evaluation data are more accurately segmented than the training data, can the model take advantage of the improvement? Table XI shows WERs achieved by models trained using discrete speech sequences obtained from worse boundaries (four different boundary settings listed in the Table) but evaluated using speech token sequences obtained from better boundaries (from GradSeg + wav2bnd + JSTTI E2E-refinement + wav2bnd-large)."
        ]
    },
    "S4.T12": {
        "caption": "TABLE XII:  WERs for the JSTTI-ENC model when we feed either layer one or layer two speech features from the evaluation set to the text output layer (WER-L1-Eval and WER-L2-Eval, respectively). The better results within each row are shown in bold.",
        "table": "<figure id=\"S4.T12\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE XII: </span>WERs for the JSTTI-ENC model when we feed either layer one or layer two speech features from the evaluation set to the text output layer (WER-L1-Eval and WER-L2-Eval, respectively). The better results within each row are shown in bold.</figcaption>\n<table id=\"S4.T12.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T12.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T12.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>\n<th id=\"S4.T12.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-L1-EVAL(<math id=\"S4.T12.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T12.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T12.1.1.1.m1.1.1\" xref=\"S4.T12.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T12.1.1.1.m1.1b\"><ci id=\"S4.T12.1.1.1.m1.1.1.cmml\" xref=\"S4.T12.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T12.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n<th id=\"S4.T12.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-L2-EVAL(<math id=\"S4.T12.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T12.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T12.2.2.2.m1.1.1\" xref=\"S4.T12.2.2.2.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T12.2.2.2.m1.1b\"><ci id=\"S4.T12.2.2.2.m1.1.1.cmml\" xref=\"S4.T12.2.2.2.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T12.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T12.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T12.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">GradSeg</th>\n<td id=\"S4.T12.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T12.2.3.1.2.1\" class=\"ltx_text ltx_font_bold\">47.53 (44.76)</span></td>\n<td id=\"S4.T12.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">56.66 (54.59)</td>\n</tr>\n<tr id=\"S4.T12.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T12.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">+ wav2bnd</th>\n<td id=\"S4.T12.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T12.2.4.2.2.1\" class=\"ltx_text ltx_font_bold\">40.45 (38.08)</span></td>\n<td id=\"S4.T12.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">49.79 (48.02)</td>\n</tr>\n<tr id=\"S4.T12.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T12.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">+ JSTTI E2E-refinement*</th>\n<td id=\"S4.T12.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T12.2.5.3.2.1\" class=\"ltx_text ltx_font_bold\">29.90 (28.26)</span></td>\n<td id=\"S4.T12.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">40.43 (39.02)</td>\n</tr>\n<tr id=\"S4.T12.2.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T12.2.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">+ wav2bnd-large</th>\n<td id=\"S4.T12.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T12.2.6.4.2.1\" class=\"ltx_text ltx_font_bold\">28.00 (26.51)</span></td>\n<td id=\"S4.T12.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">38.74 (37.57)</td>\n</tr>\n<tr id=\"S4.T12.2.7.5\" class=\"ltx_tr\">\n<th id=\"S4.T12.2.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_tt\">Forced Alignment</th>\n<td id=\"S4.T12.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\"><span id=\"S4.T12.2.7.5.2.1\" class=\"ltx_text ltx_font_bold\">18.06 (18.41)</span></td>\n<td id=\"S4.T12.2.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">27.76 (28.00)</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": []
    },
    "S4.T13": {
        "caption": "TABLE XIII:  WERs for the JSTTI-ENC model when we feed either layer one or layer two speech features from the training set to the text output layer (WER-L1-Train and WER-L2-Train, respectively). The better results within each row are shown in bold.",
        "table": "<figure id=\"S4.T13\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE XIII: </span>WERs for the JSTTI-ENC model when we feed either layer one or layer two speech features from the training set to the text output layer (WER-L1-Train and WER-L2-Train, respectively). The better results within each row are shown in bold.</figcaption>\n<table id=\"S4.T13.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T13.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T13.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>\n<th id=\"S4.T13.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-L1-TRAIN(<math id=\"S4.T13.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T13.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T13.1.1.1.m1.1.1\" xref=\"S4.T13.1.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T13.1.1.1.m1.1b\"><ci id=\"S4.T13.1.1.1.m1.1.1.cmml\" xref=\"S4.T13.1.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T13.1.1.1.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n<th id=\"S4.T13.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER-L2-TRAIN(<math id=\"S4.T13.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S4.T13.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T13.2.2.2.m1.1.1\" xref=\"S4.T13.2.2.2.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T13.2.2.2.m1.1b\"><ci id=\"S4.T13.2.2.2.m1.1.1.cmml\" xref=\"S4.T13.2.2.2.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T13.2.2.2.m1.1c\">\\downarrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T13.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T13.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">GradSeg</th>\n<td id=\"S4.T13.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T13.2.3.1.2.1\" class=\"ltx_text ltx_font_bold\">46.44 (43.55)</span></td>\n<td id=\"S4.T13.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">50.97 (48.41)</td>\n</tr>\n<tr id=\"S4.T13.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T13.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">+ wav2bnd</th>\n<td id=\"S4.T13.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T13.2.4.2.2.1\" class=\"ltx_text ltx_font_bold\">39.09 (36.69)</span></td>\n<td id=\"S4.T13.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">43.66 (41.54)</td>\n</tr>\n<tr id=\"S4.T13.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T13.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">+ JSTTI E2E-refinement*</th>\n<td id=\"S4.T13.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T13.2.5.3.2.1\" class=\"ltx_text ltx_font_bold\">28.31 (26.61)</span></td>\n<td id=\"S4.T13.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">32.73 (30.89)</td>\n</tr>\n<tr id=\"S4.T13.2.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T13.2.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">+ wav2bnd-large</th>\n<td id=\"S4.T13.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T13.2.6.4.2.1\" class=\"ltx_text ltx_font_bold\">26.57 (24.92)</span></td>\n<td id=\"S4.T13.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">30.85 (29.11)</td>\n</tr>\n<tr id=\"S4.T13.2.7.5\" class=\"ltx_tr\">\n<th id=\"S4.T13.2.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_tt\">Forced Alignment</th>\n<td id=\"S4.T13.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\"><span id=\"S4.T13.2.7.5.2.1\" class=\"ltx_text ltx_font_bold\">16.58 (16.92)</span></td>\n<td id=\"S4.T13.2.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\">19.89 (20.18)</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": []
    },
    "S4.T14": {
        "caption": "TABLE XIV:  WP, CP, and WNMI calculated with cross-modal quantization on top of layer one (L1) and layer one (L2) features extracted with JSTTI-ENC model on the training set. The better results among the two layers are shown in bold.",
        "table": "<figure id=\"S4.T14\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">TABLE XIV: </span>WP, CP, and WNMI calculated with cross-modal quantization on top of layer one (L1) and layer one (L2) features extracted with JSTTI-ENC model on the training set. The better results among the two layers are shown in bold.</figcaption>\n<table id=\"S4.T14.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T14.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T14.3.3.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Layer</th>\n<th id=\"S4.T14.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WP(<math id=\"S4.T14.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T14.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S4.T14.1.1.1.m1.1.1\" xref=\"S4.T14.1.1.1.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T14.1.1.1.m1.1b\"><ci id=\"S4.T14.1.1.1.m1.1.1.cmml\" xref=\"S4.T14.1.1.1.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T14.1.1.1.m1.1c\">\\uparrow</annotation></semantics></math>)</th>\n<th id=\"S4.T14.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">CP(<math id=\"S4.T14.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T14.2.2.2.m1.1a\"><mo stretchy=\"false\" id=\"S4.T14.2.2.2.m1.1.1\" xref=\"S4.T14.2.2.2.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T14.2.2.2.m1.1b\"><ci id=\"S4.T14.2.2.2.m1.1.1.cmml\" xref=\"S4.T14.2.2.2.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T14.2.2.2.m1.1c\">\\uparrow</annotation></semantics></math>)</th>\n<th id=\"S4.T14.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">CWMI(<math id=\"S4.T14.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S4.T14.3.3.3.m1.1a\"><mo stretchy=\"false\" id=\"S4.T14.3.3.3.m1.1.1\" xref=\"S4.T14.3.3.3.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T14.3.3.3.m1.1b\"><ci id=\"S4.T14.3.3.3.m1.1.1.cmml\" xref=\"S4.T14.3.3.3.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T14.3.3.3.m1.1c\">\\uparrow</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T14.3.4.1\" class=\"ltx_tr\">\n<th id=\"S4.T14.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">L1</th>\n<td id=\"S4.T14.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T14.3.4.1.2.1\" class=\"ltx_text ltx_font_bold\">0.831</span></td>\n<td id=\"S4.T14.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T14.3.4.1.3.1\" class=\"ltx_text ltx_font_bold\">0.550</span></td>\n<td id=\"S4.T14.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T14.3.4.1.4.1\" class=\"ltx_text ltx_font_bold\">0.855</span></td>\n</tr>\n<tr id=\"S4.T14.3.5.2\" class=\"ltx_tr\">\n<th id=\"S4.T14.3.5.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">L2</th>\n<td id=\"S4.T14.3.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.581</td>\n<td id=\"S4.T14.3.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.378</td>\n<td id=\"S4.T14.3.5.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.577</td>\n</tr>\n</tbody>\n</table>\n</figure>\n",
        "footnotes": [],
        "references": [
            "where we denote the marginal distribution over k-means label j\ud835\udc57j as P^L\u200b(j)subscript^\ud835\udc43\ud835\udc3f\ud835\udc57\\hat{P}_{L}(j), the marginal distribution over word label i\ud835\udc56i as P^Y\u200b(j)subscript^\ud835\udc43\ud835\udc4c\ud835\udc57\\hat{P}_{Y}(j),the conditional probability of a word label i\ud835\udc56i given a k-means label j\ud835\udc57j as P^Y\u2223L\u200b(i\u2223j)subscript^\ud835\udc43conditional\ud835\udc4c\ud835\udc3fconditional\ud835\udc56\ud835\udc57\\hat{P}_{Y\\mid L}(i\\mid j), the conditional probability of a k-means label j\ud835\udc57j given a word label i\ud835\udc56i as P^L\u2223Y\u200b(j\u2223i)subscript^\ud835\udc43conditional\ud835\udc3f\ud835\udc4cconditional\ud835\udc57\ud835\udc56\\hat{P}_{L\\mid Y}(j\\mid i), the mutual information between Y\ud835\udc4cY and L\ud835\udc3fL as I\u200b(Y;L)\ud835\udc3c\ud835\udc4c\ud835\udc3fI(Y;L) and the entropy of Y\ud835\udc4cY as H\u200b(Y)\ud835\udc3b\ud835\udc4cH(Y). WP can be interpreted as the word accuracy if we transcribe each k-means class with its most likely word label. CP is the counterpart of WP and measures the average probability that the same word is labeled as the same k-means class. WNMI is an information-theoretic metric that measures the percentage of uncertainty eliminated for a word label Y\ud835\udc4cY after observing its k-means label L\ud835\udc3fL [2]. We only report these metrics in Table XIV for the setting trained with word-level speech token sequences obtained under the boundary setting denoted as GradSeg + wav2bnd + JSTTI E2E-refinement + wav2bnd-large (c.f. Section III-E), as results obtained under other settings are similar.",
            "Table XIV shows better cross-modal WP, CP, and WNMI for layer one rather than layer two, again solidifying the hypothesis that layer one learns a shared representation between speech and text modalities, while the representations from layer 2 are less shared across the two modalities."
        ]
    }
}