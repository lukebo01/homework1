<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 201&ndash;250 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200">order: -announced_date_first; size: 50; page_start: 200; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=250"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200"
              class="pagination-link is-current"
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=250"
              class="pagination-link "
              aria-label="Page 6"
              aria-current="page">6
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="201"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.07259">arXiv:2409.07259</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.07259">pdf</a>, <a href="https://arxiv.org/format/2409.07259">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ManaTTS Persian: a recipe for creating TTS datasets for lower resource languages
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Qharabagh%2C+M+F">Mahta Fetrat Qharabagh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dehghanian%2C+Z">Zahra Dehghanian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rabiee%2C+H+R">Hamid R. Rabiee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.07259v1-abstract-short" style="display: inline;">
        In this study, we introduce ManaTTS, the most extensive publicly accessible single-speaker Persian corpus, and a comprehensive framework for collecting transcribed <span class="search-hit mathjax">speech</span> datasets for the Persian language. ManaTTS, released under the open CC-0 license, comprises approximately 86 hours of audio with a sampling rate of 44.1 kHz. Alongside ManaTTS, we also gene&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07259v1-abstract-full').style.display = 'inline'; document.getElementById('2409.07259v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.07259v1-abstract-full" style="display: none;">
        In this study, we introduce ManaTTS, the most extensive publicly accessible single-speaker Persian corpus, and a comprehensive framework for collecting transcribed <span class="search-hit mathjax">speech</span> datasets for the Persian language. ManaTTS, released under the open CC-0 license, comprises approximately 86 hours of audio with a sampling rate of 44.1 kHz. Alongside ManaTTS, we also generated the VirgoolInformal dataset to evaluate Persian <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models used for forced alignment, extending over 5 hours of audio. The datasets are supported by a fully transparent, MIT-licensed pipeline, a testament to innovation in the field. It includes unique tools for sentence tokenization, bounded audio segmentation, and a novel forced alignment method. This alignment technique is specifically designed for low-resource languages, addressing a crucial need in the field. With this dataset, we trained a Tacotron2-based TTS model, achieving a Mean Opinion Score (MOS) of 3.76, which is remarkably close to the MOS of 3.86 for the utterances generated by the same vocoder and natural spectrogram, and the MOS of 4.01 for the natural waveform, demonstrating the exceptional quality and effectiveness of the corpus.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07259v1-abstract-full').style.display = 'none'; document.getElementById('2409.07259v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">33 pages, 12 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.07255">arXiv:2409.07255</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.07255">pdf</a>, <a href="https://arxiv.org/ps/2409.07255">ps</a>, <a href="https://arxiv.org/format/2409.07255">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EMOdiffhead: Continuously Emotional Control in Talking Head Generation via Diffusion
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jian Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mai%2C+W">Weijian Mai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhijun Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.07255v1-abstract-short" style="display: inline;">
        The task of audio-driven portrait animation involves generating a talking head video using an identity image and an audio track of <span class="search-hit mathjax">speech</span>. While many existing approaches focus on lip synchronization and video quality, few tackle the challenge of generating emotion-driven talking head videos. The ability to control and edit emotions is essential for producing&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07255v1-abstract-full').style.display = 'inline'; document.getElementById('2409.07255v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.07255v1-abstract-full" style="display: none;">
        The task of audio-driven portrait animation involves generating a talking head video using an identity image and an audio track of <span class="search-hit mathjax">speech</span>. While many existing approaches focus on lip synchronization and video quality, few tackle the challenge of generating emotion-driven talking head videos. The ability to control and edit emotions is essential for producing expressive and realistic animations. In response to this challenge, we propose EMOdiffhead, a novel method for emotional talking head video generation that not only enables fine-grained control of emotion categories and intensities but also enables one-shot generation. Given the FLAME 3D model&#39;s linearity in expression modeling, we utilize the DECA method to extract expression vectors, that are combined with audio to guide a diffusion model in generating videos with precise lip synchronization and rich emotional expressiveness. This approach not only enables the learning of rich facial information from emotion-irrelevant data but also facilitates the generation of emotional videos. It effectively overcomes the limitations of emotional data, such as the lack of diversity in facial and background information, and addresses the absence of emotional details in emotion-irrelevant data. Extensive experiments and user studies demonstrate that our approach achieves state-of-the-art performance compared to other emotion portrait animation methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07255v1-abstract-full').style.display = 'none'; document.getElementById('2409.07255v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.07210">arXiv:2409.07210</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.07210">pdf</a>, <a href="https://arxiv.org/format/2409.07210">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing CTC-Based Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Laux%2C+H">Hendrik Laux</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schmeink%2C+A">Anke Schmeink</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.07210v1-abstract-short" style="display: inline;">
        This paper presents LiteVSR2, an enhanced version of our previously introduced efficient approach to Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (VSR). Building upon our knowledge distillation framework from a pre-trained Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07210v1-abstract-full').style.display = 'inline'; document.getElementById('2409.07210v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.07210v1-abstract-full" style="display: none;">
        This paper presents LiteVSR2, an enhanced version of our previously introduced efficient approach to Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (VSR). Building upon our knowledge distillation framework from a pre-trained Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) model, we introduce two key improvements: a stabilized video preprocessing technique and feature normalization in the distillation process. These improvements yield substantial performance gains on the LRS2 and LRS3 benchmarks, positioning LiteVSR2 as the current best CTC-based VSR model without increasing the volume of training data or computational resources utilized. Furthermore, we explore the scalability of our approach by examining performance metrics across varying model complexities and training data volumes. LiteVSR2 maintains the efficiency of its predecessor while significantly enhancing accuracy, thereby demonstrating the potential for resource-efficient advancements in VSR technology.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07210v1-abstract-full').style.display = 'none'; document.getElementById('2409.07210v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.07165">arXiv:2409.07165</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.07165">pdf</a>, <a href="https://arxiv.org/format/2409.07165">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Linear Time Complexity Conformers with SummaryMixing for Streaming <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Parcollet%2C+T">Titouan Parcollet</a>, 
      
      <a href="/search/?searchtype=author&amp;query=van+Dalen%2C+R">Rogier van Dalen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shucong Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batthacharya%2C+S">Sourav Batthacharya</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.07165v1-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07165v1-abstract-full').style.display = 'inline'; document.getElementById('2409.07165v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.07165v1-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) with an encoder equipped with self-attention, whether streaming or non-streaming, takes quadratic time in the length of the <span class="search-hit mathjax">speech</span> utterance. This slows down training and decoding, increase their cost, and limit the deployment of the ASR in constrained devices. SummaryMixing is a promising linear-time complexity alternative to self-attention for non-streaming <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> that, for the first time, preserves or outperforms the accuracy of self-attention models. Unfortunately, the original definition of SummaryMixing is not suited to streaming <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. Hence, this work extends SummaryMixing to a Conformer Transducer that works in both a streaming and an offline mode. It shows that this new linear-time complexity <span class="search-hit mathjax">speech</span> encoder outperforms self-attention in both scenarios while requiring less compute and memory during training and decoding.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07165v1-abstract-full').style.display = 'none'; document.getElementById('2409.07165v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.07114">arXiv:2409.07114</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.07114">pdf</a>, <a href="https://arxiv.org/format/2409.07114">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ICPS59941.2024.10639989">10.1109/ICPS59941.2024.10639989 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Continual and Incremental Learning Approach for TinyML On-device Training Using Dataset Distillation and Model Size Adaption
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=R%C3%BCb%2C+M">Marcus Rüb</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tuchel%2C+P">Philipp Tuchel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sikora%2C+A">Axel Sikora</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mueller-Gritschneder%2C+D">Daniel Mueller-Gritschneder</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.07114v1-abstract-short" style="display: inline;">
        &hellip;TinyML is an emerging field that deploys machine learning models on resource-constrained devices such as microcontrollers, enabling intelligent applications like voice <span class="search-hit mathjax">recognition</span>, anomaly detection, predictive maintenance, and sensor data processing in environments where traditional machine learning models are not feasible. The algorithm solve the challeng&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07114v1-abstract-full').style.display = 'inline'; document.getElementById('2409.07114v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.07114v1-abstract-full" style="display: none;">
        A new algorithm for incremental learning in the context of Tiny Machine learning (TinyML) is presented, which is optimized for low-performance and energy efficient embedded devices. TinyML is an emerging field that deploys machine learning models on resource-constrained devices such as microcontrollers, enabling intelligent applications like voice <span class="search-hit mathjax">recognition</span>, anomaly detection, predictive maintenance, and sensor data processing in environments where traditional machine learning models are not feasible. The algorithm solve the challenge of catastrophic forgetting through the use of knowledge distillation to create a small, distilled dataset. The novelty of the method is that the size of the model can be adjusted dynamically, so that the complexity of the model can be adapted to the requirements of the task. This offers a solution for incremental learning in resource-constrained environments, where both model size and computational efficiency are critical factors. Results show that the proposed algorithm offers a promising approach for TinyML incremental learning on embedded devices. The algorithm was tested on five datasets including: CIFAR10, MNIST, CORE50, HAR, <span class="search-hit mathjax">Speech</span> Commands. The findings indicated that, despite using only 43% of Floating Point Operations (FLOPs) compared to a larger fixed model, the algorithm experienced a negligible accuracy loss of just 1%. In addition, the presented method is memory efficient. While state-of-the-art incremental learning is usually very memory intensive, the method requires only 1% of the original data set.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.07114v1-abstract-full').style.display = 'none'; document.getElementById('2409.07114v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2024 IEEE 7th International Conference on Industrial Cyber-Physical Systems (ICPS)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.06580">arXiv:2409.06580</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.06580">pdf</a>, <a href="https://arxiv.org/format/2409.06580">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring Differences between Human Perception and Model Inference in Audio Event <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+Y">Yizhou Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yanru Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hou%2C+Y">Yuanbo Hou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+X">Xin Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bu%2C+H">Hui Bu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Shengchen Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Botteldooren%2C+D">Dick Botteldooren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Plumbley%2C+M+D">Mark D. Plumbley</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.06580v1-abstract-short" style="display: inline;">
        Audio Event <span class="search-hit mathjax">Recognition</span> (AER) traditionally focuses on detecting and identifying audio events. Most existing AER models tend to detect all potential events without considering their varying significance across different contexts. This makes the AER results detected by existing models often have a large discrepancy with human auditory perception. Although thi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06580v1-abstract-full').style.display = 'inline'; document.getElementById('2409.06580v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.06580v1-abstract-full" style="display: none;">
        Audio Event <span class="search-hit mathjax">Recognition</span> (AER) traditionally focuses on detecting and identifying audio events. Most existing AER models tend to detect all potential events without considering their varying significance across different contexts. This makes the AER results detected by existing models often have a large discrepancy with human auditory perception. Although this is a critical and significant issue, it has not been extensively studied by the Detection and Classification of Sound Scenes and Events (DCASE) community because solving it is time-consuming and labour-intensive. To address this issue, this paper introduces the concept of semantic importance in AER, focusing on exploring the differences between human perception and model inference. This paper constructs a Multi-Annotated Foreground Audio Event <span class="search-hit mathjax">Recognition</span> (MAFAR) dataset, which comprises audio recordings labelled by 10 professional annotators. Through labelling frequency and variance, the MAFAR dataset facilitates the quantification of semantic importance and analysis of human perception. By comparing human annotations with the predictions of ensemble pre-trained models, this paper uncovers a significant gap between human perception and model inference in both semantic identification and existence detection of audio events. Experimental results reveal that human perception tends to ignore subtle or trivial events in the event semantic identification, while model inference is easily affected by events with noises. Meanwhile, in event existence detection, models are usually more sensitive than humans.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06580v1-abstract-full').style.display = 'none'; document.getElementById('2409.06580v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Dataset homepage: https://github.com/Voltmeter00/MAFAR</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.06468">arXiv:2409.06468</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.06468">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Effective Context-Balanced Adaptation Approach for Long-Tailed <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yi-Cheng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pai%2C+L">Li-Ting Pai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+B">Bi-Cheng Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hsin-Wei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+C">Chi-Han Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+B">Berlin Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.06468v1-abstract-short" style="display: inline;">
        End-to-end (E2E) automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06468v1-abstract-full').style.display = 'inline'; document.getElementById('2409.06468v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.06468v1-abstract-full" style="display: none;">
        End-to-end (E2E) automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models have become standard practice for various commercial applications. However, in real-world scenarios, the long-tailed nature of word distribution often leads E2E ASR models to perform well on common words but fall short in recognizing uncommon ones. Recently, the notion of a contextual adapter (CA) was proposed to infuse external knowledge represented by a context word list into E2E ASR models. Although CA can improve <span class="search-hit mathjax">recognition</span> performance on rare words, two crucial data imbalance problems remain. First, when using low-frequency words as context words during training, since these words rarely occur in the utterance, CA becomes prone to overfit on attending to the &lt;no-context&gt; token due to higher-frequency words not being present in the context list. Second, the long-tailed distribution within the context list itself still causes the model to perform poorly on low-frequency context words. In light of this, we explore in-depth the impact of altering the context list to have words with different frequency distributions on model performance, and meanwhile extend CA with a simple yet effective context-balanced learning objective. A series of experiments conducted on the AISHELL-1 benchmark dataset suggests that using all vocabulary words from the training corpus as the context list and pairing them with our balanced objective yields the best performance, demonstrating a significant reduction in character error rate (CER) by up to 1.21% and a more pronounced 9.44% reduction in the error rate of zero-shot words.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06468v1-abstract-full').style.display = 'none'; document.getElementById('2409.06468v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.06429">arXiv:2409.06429</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.06429">pdf</a>, <a href="https://arxiv.org/format/2409.06429">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1186/s40648-022-00231-x">10.1186/s40648-022-00231-x <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Human-mimetic binaural ear design and sound source direction estimation for task realization of musculoskeletal humanoids
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Omura%2C+Y">Yusuke Omura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kawaharazuka%2C+K">Kento Kawaharazuka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nagamatsu%2C+Y">Yuya Nagamatsu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koga%2C+Y">Yuya Koga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nishiura%2C+M">Manabu Nishiura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Toshimitsu%2C+Y">Yasunori Toshimitsu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Asano%2C+Y">Yuki Asano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Okada%2C+K">Kei Okada</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kawasaki%2C+K">Koji Kawasaki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Inaba%2C+M">Masayuki Inaba</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.06429v1-abstract-short" style="display: inline;">
        Human-like environment <span class="search-hit mathjax">recognition</span> by musculoskeletal humanoids is important for task realization in real complex environments and for use as dummies for test subjects. Humans integrate various sensory information to perceive their surroundings, and hearing is particularly useful for recognizing objects out of view or out of touch. In this research, we aim t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06429v1-abstract-full').style.display = 'inline'; document.getElementById('2409.06429v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.06429v1-abstract-full" style="display: none;">
        Human-like environment <span class="search-hit mathjax">recognition</span> by musculoskeletal humanoids is important for task realization in real complex environments and for use as dummies for test subjects. Humans integrate various sensory information to perceive their surroundings, and hearing is particularly useful for recognizing objects out of view or out of touch. In this research, we aim to realize human-like auditory environmental <span class="search-hit mathjax">recognition</span> and task realization for musculoskeletal humanoids by equipping them with a human-like auditory processing system. Humans realize sound-based environmental <span class="search-hit mathjax">recognition</span> by estimating directions of the sound sources and detecting environmental sounds based on changes in the time and frequency domain of incoming sounds and the integration of auditory information in the central nervous system. We propose a human mimetic auditory information processing system, which consists of three components: the human mimetic binaural ear unit, which mimics human ear structure and characteristics, the sound source direction estimation system, and the environmental sound detection system, which mimics processing in the central nervous system. We apply it to Musashi, a human mimetic musculoskeletal humanoid, and have it perform tasks that require sound information outside of view in real noisy environments to confirm the usefulness of the proposed methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06429v1-abstract-full').style.display = 'none'; document.getElementById('2409.06429v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ROBOMECH Journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.06274">arXiv:2409.06274</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.06274">pdf</a>, <a href="https://arxiv.org/format/2409.06274">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Spectral oversubtraction? An approach for <span class="search-hit mathjax">speech</span> enhancement after robot ego <span class="search-hit mathjax">speech</span> filtering in semi-real-time
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yue Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hindriks%2C+K+V">Koen V. Hindriks</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kunneman%2C+F+A">Florian A. Kunneman</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.06274v1-abstract-short" style="display: inline;">
        Spectral subtraction, widely used for its simplicity, has been employed to address the Robot Ego <span class="search-hit mathjax">Speech</span> Filtering (RESF) problem for detecting&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06274v1-abstract-full').style.display = 'inline'; document.getElementById('2409.06274v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.06274v1-abstract-full" style="display: none;">
        Spectral subtraction, widely used for its simplicity, has been employed to address the Robot Ego <span class="search-hit mathjax">Speech</span> Filtering (RESF) problem for detecting <span class="search-hit mathjax">speech</span> contents of human interruption from robot&#39;s single-channel microphone recordings when it is speaking. However, this approach suffers from oversubtraction in the fundamental frequency range (FFR), leading to degraded <span class="search-hit mathjax">speech</span> content <span class="search-hit mathjax">recognition</span>. To address this, we propose a Two-Mask Conformer-based Metric Generative Adversarial Network (CMGAN) to enhance the detected <span class="search-hit mathjax">speech</span> and improve <span class="search-hit mathjax">recognition</span> results. Our model compensates for oversubtracted FFR values with high-frequency information and long-term features and then de-noises the new spectrogram. In addition, we introduce an incremental processing method that allows semi-real-time audio processing with streaming input on a network trained on long fixed-length input. Evaluations of two datasets, including one with unseen noise, demonstrate significant improvements in <span class="search-hit mathjax">recognition</span> accuracy and the effectiveness of the proposed two-mask approach and incremental processing, enhancing the robustness of the proposed RESF pipeline in real-world HRI scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06274v1-abstract-full').style.display = 'none'; document.getElementById('2409.06274v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, 2 figures, submitted to 2025 IEEE ICASSP</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T50
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.06263">arXiv:2409.06263</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.06263">pdf</a>, <a href="https://arxiv.org/format/2409.06263">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Keyword-Aware ASR Error Augmentation for Robust Dialogue State Tracking
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+J">Jihyun Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Im%2C+S">Solee Im</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+W">Wonjun Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+G+G">Gary Geunbae Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.06263v1-abstract-short" style="display: inline;">
        &hellip;systems, identifying important information in conversations. However, its accuracy drops significantly in spoken dialogue environments due to named entity errors from Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems. We introduce a simple yet effective data augmentation method that targets those entities to improve the robus&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06263v1-abstract-full').style.display = 'inline'; document.getElementById('2409.06263v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.06263v1-abstract-full" style="display: none;">
        Dialogue State Tracking (DST) is a key part of task-oriented dialogue systems, identifying important information in conversations. However, its accuracy drops significantly in spoken dialogue environments due to named entity errors from Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) systems. We introduce a simple yet effective data augmentation method that targets those entities to improve the robustness of DST model. Our novel method can control the placement of errors using keyword-highlighted prompts while introducing phonetically similar errors. As a result, our method generated sufficient error patterns on keywords, leading to improved accuracy in noised and low-accuracy ASR environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06263v1-abstract-full').style.display = 'none'; document.getElementById('2409.06263v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.06222">arXiv:2409.06222</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.06222">pdf</a>, <a href="https://arxiv.org/format/2409.06222">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Advancing Topic Segmentation of Broadcasted <span class="search-hit mathjax">Speech</span> with Multilingual Semantic Embeddings
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shukla%2C+S+D">Sakshi Deo Shukla</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Denisov%2C+P">Pavel Denisov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Turan%2C+T">Tugtekin Turan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.06222v1-abstract-short" style="display: inline;">
        Recent advancements in <span class="search-hit mathjax">speech</span>-based topic segmentation have highlighted the potential of pretrained&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06222v1-abstract-full').style.display = 'inline'; document.getElementById('2409.06222v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.06222v1-abstract-full" style="display: none;">
        Recent advancements in <span class="search-hit mathjax">speech</span>-based topic segmentation have highlighted the potential of pretrained <span class="search-hit mathjax">speech</span> encoders to capture semantic representations directly from <span class="search-hit mathjax">speech</span>. Traditionally, topic segmentation has relied on a pipeline approach in which transcripts of the automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems are generated, followed by text-based segmentation algorithms. In this paper, we introduce an end-to-end scheme that bypasses this conventional two-step process by directly employing semantic <span class="search-hit mathjax">speech</span> encoders for segmentation. Focused on the broadcasted news domain, which poses unique challenges due to the diversity of speakers and topics within single recordings, we address the challenge of accessing topic change points efficiently in an end-to-end manner. Furthermore, we propose a new benchmark for spoken news topic segmentation by utilizing a dataset featuring approximately 1000 hours of publicly available recordings across six European languages and including an evaluation set in Hindi to test the model&#39;s cross-domain performance in a cross-lingual, zero-shot scenario. This setup reflects real-world diversity and the need for models adapting to various linguistic settings. Our results demonstrate that while the traditional pipeline approach achieves a state-of-the-art $P_k$ score of 0.2431 for English, our end-to-end model delivers a competitive $P_k$ score of 0.2564. When trained multilingually, these scores further improve to 0.1988 and 0.2370, respectively. To support further research, we release our model along with data preparation scripts, facilitating open research on multilingual spoken news topic segmentation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06222v1-abstract-full').style.display = 'none'; document.getElementById('2409.06222v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.06135">arXiv:2409.06135</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.06135">pdf</a>, <a href="https://arxiv.org/format/2409.06135">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Q">Qi Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mao%2C+B">Binjie Mao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zili Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nie%2C+X">Xing Nie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+P">Pengfei Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Ying Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhen%2C+C">Cheng Zhen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+P">Pengfei Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiang%2C+S">Shiming Xiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.06135v1-abstract-short" style="display: inline;">
        Foley is a term commonly used in filmmaking, referring to the addition of daily sound effects to silent films or videos to enhance the auditory experience. Video-to-Audio (V2A), as a particular type of automatic foley task, presents inherent challenges related to audio-visual synchronization. These challenges encompass maintaining the content consistency between the input video and the generated a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06135v1-abstract-full').style.display = 'inline'; document.getElementById('2409.06135v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.06135v1-abstract-full" style="display: none;">
        Foley is a term commonly used in filmmaking, referring to the addition of daily sound effects to silent films or videos to enhance the auditory experience. Video-to-Audio (V2A), as a particular type of automatic foley task, presents inherent challenges related to audio-visual synchronization. These challenges encompass maintaining the content consistency between the input video and the generated audio, as well as the alignment of temporal and loudness properties within the video. To address these issues, we construct a controllable video-to-audio synthesis model, termed Draw an Audio, which supports multiple input instructions through drawn masks and loudness signals. To ensure content consistency between the synthesized audio and target video, we introduce the Mask-Attention Module (MAM), which employs masked video instruction to enable the model to focus on regions of interest. Additionally, we implement the Time-Loudness Module (TLM), which uses an auxiliary loudness signal to ensure the synthesis of sound that aligns with the video in both loudness and temporal dimensions. Furthermore, we have extended a large-scale V2A dataset, named VGGSound-Caption, by annotating caption prompts. Extensive experiments on challenging benchmarks across two large-scale V2A datasets verify Draw an Audio achieves the state-of-the-art. Project page: https://yannqi.github.io/Draw-an-Audio/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06135v1-abstract-full').style.display = 'none'; document.getElementById('2409.06135v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 11 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.06062">arXiv:2409.06062</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.06062">pdf</a>, <a href="https://arxiv.org/format/2409.06062">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Retrieval Augmented Correction of Named Entity <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Errors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pusateri%2C+E">Ernest Pusateri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Walia%2C+A">Anmol Walia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kashi%2C+A">Anirudh Kashi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bandyopadhyay%2C+B">Bortik Bandyopadhyay</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hyder%2C+N">Nadia Hyder</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahinder%2C+S">Sayantan Mahinder</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Anantha%2C+R">Raviteja Anantha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+D">Daben Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gondala%2C+S">Sashank Gondala</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.06062v1-abstract-short" style="display: inline;">
        In recent years, end-to-end automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06062v1-abstract-full').style.display = 'inline'; document.getElementById('2409.06062v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.06062v1-abstract-full" style="display: none;">
        In recent years, end-to-end automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems have proven themselves remarkably accurate and performant, but these systems still have a significant error rate for entity names which appear infrequently in their training data. In parallel to the rise of end-to-end ASR systems, large language models (LLMs) have proven to be a versatile tool for various natural language processing (NLP) tasks. In NLP tasks where a database of relevant knowledge is available, retrieval augmented generation (RAG) has achieved impressive results when used with LLMs. In this work, we propose a RAG-like technique for correcting <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> entity name errors. Our approach uses a vector database to index a set of relevant entities. At runtime, database queries are generated from possibly errorful textual ASR hypotheses, and the entities retrieved using these queries are fed, along with the ASR hypotheses, to an LLM which has been adapted to correct ASR errors. Overall, our best system achieves 33%-39% relative word error rate reductions on synthetic test sets focused on voice assistant queries of rare music entities without regressing on the STOP test set, a publicly available voice assistant test set covering many domains.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06062v1-abstract-full').style.display = 'none'; document.getElementById('2409.06062v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.06013">arXiv:2409.06013</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.06013">pdf</a>, <a href="https://arxiv.org/format/2409.06013">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improved Visually Prompted Keyword Localisation in Real Low-Resource Settings
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nortje%2C+L">Leanne Nortje</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oneata%2C+D">Dan Oneata</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kamper%2C+H">Herman Kamper</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.06013v1-abstract-short" style="display: inline;">
        Given an image query, visually prompted keyword localisation (VPKL) aims to find occurrences of the depicted word in a <span class="search-hit mathjax">speech</span> collection. This can be useful when transcriptions are not available for a low-resource language (e.g. if it is unwritten). Previous work showed that VPKL can be performed with a visually grounded&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06013v1-abstract-full').style.display = 'inline'; document.getElementById('2409.06013v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.06013v1-abstract-full" style="display: none;">
        Given an image query, visually prompted keyword localisation (VPKL) aims to find occurrences of the depicted word in a <span class="search-hit mathjax">speech</span> collection. This can be useful when transcriptions are not available for a low-resource language (e.g. if it is unwritten). Previous work showed that VPKL can be performed with a visually grounded <span class="search-hit mathjax">speech</span> model trained on paired images and unlabelled <span class="search-hit mathjax">speech</span>. But all experiments were done on English. Moreover, transcriptions were used to get positive and negative pairs for the contrastive loss. This paper introduces a few-shot learning scheme to mine pairs automatically without transcriptions. On English, this results in only a small drop in performance. We also - for the first time - consider VPKL on a real low-resource language, Yoruba. While scores are reasonable, here we see a bigger drop in performance compared to using ground truth pairs because the mining is less accurate in Yoruba.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.06013v1-abstract-full').style.display = 'none'; document.getElementById('2409.06013v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05770">arXiv:2409.05770</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05770">pdf</a>, <a href="https://arxiv.org/format/2409.05770">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Consensus-based Distributed Quantum Kernel Learning for <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+K">Kuan-Cheng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+W">Wenxuan Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+X">Xiaotian Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05770v1-abstract-short" style="display: inline;">
        This paper presents a Consensus-based Distributed Quantum Kernel Learning (CDQKL) framework aimed at improving <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05770v1-abstract-full').style.display = 'inline'; document.getElementById('2409.05770v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05770v1-abstract-full" style="display: none;">
        This paper presents a Consensus-based Distributed Quantum Kernel Learning (CDQKL) framework aimed at improving <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> through distributed quantum computing.CDQKL addresses the challenges of scalability and data privacy in centralized quantum kernel learning. It does this by distributing computational tasks across quantum terminals, which are connected through classical channels. This approach enables the exchange of model parameters without sharing local training data, thereby maintaining data privacy and enhancing computational efficiency. Experimental evaluations on benchmark <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> datasets demonstrate that CDQKL achieves competitive classification accuracy and scalability compared to centralized and local quantum kernel learning models. The distributed nature of CDQKL offers advantages in privacy preservation and computational efficiency, making it suitable for data-sensitive fields such as telecommunications, automotive, and finance. The findings suggest that CDQKL can effectively leverage distributed quantum computing for large-scale machine-learning tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05770v1-abstract-full').style.display = 'none'; document.getElementById('2409.05770v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05750">arXiv:2409.05750</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05750">pdf</a>, <a href="https://arxiv.org/format/2409.05750">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Toolkit for Joint Speaker Diarization and Identification with Application to Speaker-Attributed ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Morrone%2C+G">Giovanni Morrone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zovato%2C+E">Enrico Zovato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brugnara%2C+F">Fabio Brugnara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sartori%2C+E">Enrico Sartori</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Badino%2C+L">Leonardo Badino</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05750v1-abstract-short" style="display: inline;">
        &hellip;in various conditions (e.g., multiple registered speakers&#39; sets, acoustic conditions and languages) and across application domains (e.g. media monitoring, institutional, <span class="search-hit mathjax">speech</span> analytics). In this demonstration we show a practical use-case in which speaker-related information is used jointly with automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05750v1-abstract-full').style.display = 'inline'; document.getElementById('2409.05750v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05750v1-abstract-full" style="display: none;">
        We present a modular toolkit to perform joint speaker diarization and speaker identification. The toolkit can leverage on multiple models and algorithms which are defined in a configuration file. Such flexibility allows our system to work properly in various conditions (e.g., multiple registered speakers&#39; sets, acoustic conditions and languages) and across application domains (e.g. media monitoring, institutional, <span class="search-hit mathjax">speech</span> analytics). In this demonstration we show a practical use-case in which speaker-related information is used jointly with automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> engines to generate speaker-attributed transcriptions. To achieve that, we employ a user-friendly web-based interface to process audio and video inputs with the chosen configuration.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05750v1-abstract-full').style.display = 'none'; document.getElementById('2409.05750v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Show and Tell paper. Presented at Interspeech 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of Interspeech 2024, pp. 3652--3653
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05674">arXiv:2409.05674</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05674">pdf</a>, <a href="https://arxiv.org/format/2409.05674">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Evaluation of real-time transcriptions using end-to-end ASR models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Arriaga%2C+C">Carlos Arriaga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pozo%2C+A">Alejandro Pozo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Conde%2C+J">Javier Conde</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alonso%2C+A">Alvaro Alonso</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05674v2-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) or <span class="search-hit mathjax">Speech</span>-to-text (STT) has greatly evolved in the last few years. Traditional architectures based on pipelines have been replaced by joint end-to-end (E2E) architectures that simplify and streamline the model training process. In addition, new&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05674v2-abstract-full').style.display = 'inline'; document.getElementById('2409.05674v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05674v2-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) or <span class="search-hit mathjax">Speech</span>-to-text (STT) has greatly evolved in the last few years. Traditional architectures based on pipelines have been replaced by joint end-to-end (E2E) architectures that simplify and streamline the model training process. In addition, new AI training methods, such as weak-supervised learning have reduced the need for high-quality audio datasets for model training. However, despite all these advancements, little to no research has been done on real-time transcription. In real-time scenarios, the audio is not pre-recorded, and the input audio must be fragmented to be processed by the ASR systems. To achieve real-time requirements, these fragments must be as short as possible to reduce latency. However, audio cannot be split at any point as dividing an utterance into two separate fragments will generate an incorrect transcription. Also, shorter fragments provide less context for the ASR model. For this reason, it is necessary to design and test different splitting algorithms to optimize the quality and delay of the resulting transcription. In this paper, three audio splitting algorithms are evaluated with different ASR models to determine their impact on both the quality of the transcription and the end-to-end delay. The algorithms are fragmentation at fixed intervals, voice activity detection (VAD), and fragmentation with feedback. The results are compared to the performance of the same model, without audio fragmentation, to determine the effects of this division. The results show that VAD fragmentation provides the best quality with the highest delay, whereas fragmentation at fixed intervals provides the lowest quality and the lowest delay. The newly proposed feedback algorithm exchanges a 2-4% increase in WER for a reduction of 1.5-2s delay, respectively, to the VAD splitting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05674v2-abstract-full').style.display = 'none'; document.getElementById('2409.05674v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 4 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05601">arXiv:2409.05601</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05601">pdf</a>, <a href="https://arxiv.org/format/2409.05601">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Longer is (Not Necessarily) Stronger: Punctuated Long-Sequence Training for Enhanced <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> and Translation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Koluguri%2C+N+R">Nithin Rao Koluguri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bartley%2C+T">Travis Bartley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Hainan Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hrinchuk%2C+O">Oleksii Hrinchuk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Balam%2C+J">Jagadeesh Balam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ginsburg%2C+B">Boris Ginsburg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kucsko%2C+G">Georg Kucsko</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05601v1-abstract-short" style="display: inline;">
        This paper presents a new method for training sequence-to-sequence models for <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05601v1-abstract-full').style.display = 'inline'; document.getElementById('2409.05601v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05601v1-abstract-full" style="display: none;">
        This paper presents a new method for training sequence-to-sequence models for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and translation tasks. Instead of the traditional approach of training models on short segments containing only lowercase or partial punctuation and capitalization (PnC) sentences, we propose training on longer utterances that include complete sentences with proper punctuation and capitalization. We achieve this by using the FastConformer architecture which allows training 1 Billion parameter models with sequences up to 60 seconds long with full attention. However, while training with PnC enhances the overall performance, we observed that accuracy plateaus when training on sequences longer than 40 seconds across various evaluation settings. Our proposed method significantly improves punctuation and capitalization accuracy, showing a 25% relative word error rate (WER) improvement on the Earnings-21 and Earnings-22 benchmarks. Additionally, training on longer audio segments increases the overall model accuracy across <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and translation benchmarks. The model weights and training code are open-sourced though NVIDIA NeMo.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05601v1-abstract-full').style.display = 'none'; document.getElementById('2409.05601v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05430">arXiv:2409.05430</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05430">pdf</a>, <a href="https://arxiv.org/format/2409.05430">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Findings of the 2024 Mandarin Stuttering Event Detection and Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Challenge
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+H">Hongfei Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+R">Rong Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shao%2C+M">Mingchen Shao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+X">Xin Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Lezhi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+L">Lei Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bu%2C+H">Hui Bu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jiaming Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yong Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+J">Jun Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Ming Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Binbin Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+B">Bin Jia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05430v1-abstract-short" style="display: inline;">
        The StutteringSpeech Challenge focuses on advancing <span class="search-hit mathjax">speech</span> technologies for people who stutter, specifically targeting Stuttering Event Detection (SED) and Automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05430v1-abstract-full').style.display = 'inline'; document.getElementById('2409.05430v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05430v1-abstract-full" style="display: none;">
        The StutteringSpeech Challenge focuses on advancing <span class="search-hit mathjax">speech</span> technologies for people who stutter, specifically targeting Stuttering Event Detection (SED) and Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) in Mandarin. The challenge comprises three tracks: (1) SED, which aims to develop systems for detection of stuttering events; (2) ASR, which focuses on creating robust systems for recognizing stuttered <span class="search-hit mathjax">speech</span>; and (3) Research track for innovative approaches utilizing the provided dataset. We utilizes an open-source Mandarin stuttering dataset AS-70, which has been split into new training and test sets for the challenge. This paper presents the dataset, details the challenge tracks, and analyzes the performance of the top systems, highlighting improvements in detection accuracy and reductions in <span class="search-hit mathjax">recognition</span> error rates. Our findings underscore the potential of specialized models and augmentation strategies in developing stuttered <span class="search-hit mathjax">speech</span> technologies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05430v1-abstract-full').style.display = 'none'; document.getElementById('2409.05430v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 2 figures, accepted by SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05307">arXiv:2409.05307</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05307">pdf</a>, <a href="https://arxiv.org/format/2409.05307">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RAL:Redundancy-Aware Lipreading Model Based on Differential Learning with Symmetric Views
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=gu%2C+Z">Zejun gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=jiang%2C+J">Junxia jiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05307v1-abstract-short" style="display: inline;">
        Lip reading involves interpreting a speaker&#39;s <span class="search-hit mathjax">speech</span> by analyzing sequences of lip movements. Currently, most models regard the left and right halves of the lips as a symmetrical whole, lacking a thorough investigation of their differences. However, the left and right halves of the lips are not always symmetrical, and the subtle differences between them&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05307v1-abstract-full').style.display = 'inline'; document.getElementById('2409.05307v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05307v1-abstract-full" style="display: none;">
        Lip reading involves interpreting a speaker&#39;s <span class="search-hit mathjax">speech</span> by analyzing sequences of lip movements. Currently, most models regard the left and right halves of the lips as a symmetrical whole, lacking a thorough investigation of their differences. However, the left and right halves of the lips are not always symmetrical, and the subtle differences between them contain rich semantic information. In this paper, we propose a differential learning strategy with symmetric views (DLSV) to address this issue. Additionally, input images often contain a lot of redundant information unrelated to <span class="search-hit mathjax">recognition</span> results, which can degrade the model&#39;s performance. We present a redundancy-aware operation (RAO) to reduce it. Finally, to leverage the relational information between symmetric views and within each view, we further design an adaptive cross-view interaction module (ACVI). Experiments on LRW and LRW-1000 datasets fully demonstrate the effectiveness of our approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05307v1-abstract-full').style.display = 'none'; document.getElementById('2409.05307v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05148">arXiv:2409.05148</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05148">pdf</a>, <a href="https://arxiv.org/format/2409.05148">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Better Spanish Emotion <span class="search-hit mathjax">Recognition</span> In-the-wild: Bringing Attention to Deep Spectrum Voice Analysis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ortega-Beltr%C3%A1n%2C+E">Elena Ortega-Beltrán</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cabacas-Maso%2C+J">Josep Cabacas-Maso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Benito-Altamirano%2C+I">Ismael Benito-Altamirano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ventura%2C+C">Carles Ventura</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05148v1-abstract-short" style="display: inline;">
        Within the context of creating new Socially Assistive Robots, emotion <span class="search-hit mathjax">recognition</span> has become a key development factor, as it allows the robot to adapt to the user&#39;s emotional state in the wild. In this work, we focused on the analysis of two voice recording Spanish datasets: ELRA-S0329 and EmoMatchSpanishDB. Specifically, we centered our work in the para&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05148v1-abstract-full').style.display = 'inline'; document.getElementById('2409.05148v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05148v1-abstract-full" style="display: none;">
        Within the context of creating new Socially Assistive Robots, emotion <span class="search-hit mathjax">recognition</span> has become a key development factor, as it allows the robot to adapt to the user&#39;s emotional state in the wild. In this work, we focused on the analysis of two voice recording Spanish datasets: ELRA-S0329 and EmoMatchSpanishDB. Specifically, we centered our work in the paralanguage, e.~g. the vocal characteristics that go along with the message and clarifies the meaning. We proposed the use of the DeepSpectrum method, which consists of extracting a visual representation of the audio tracks and feeding them to a pretrained CNN model. For the classification task, DeepSpectrum is often paired with a Support Vector Classifier --DS-SVC--, or a Fully-Connected deep-learning classifier --DS-FC--. We compared the results of the DS-SVC and DS-FC architectures with the state-of-the-art (SOTA) for ELRA-S0329 and EmoMatchSpanishDB. Moreover, we proposed our own classifier based upon Attention Mechanisms, namely DS-AM. We trained all models against both datasets, and we found that our DS-AM model outperforms the SOTA models for the datasets and the SOTA DeepSpectrum architectures. Finally, we trained our DS-AM model in one dataset and tested it in the other, to simulate real-world conditions on how biased is the model to the dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05148v1-abstract-full').style.display = 'none'; document.getElementById('2409.05148v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05089">arXiv:2409.05089</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05089">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Leveraging WaveNet for Dynamic Listening Head Modeling from <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+M">Minh-Duc Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+H">Hyung-Jeong Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+S">Seung-Won Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shin%2C+J">Ji-Eun Shin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+S">Soo-Hyung Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05089v1-abstract-short" style="display: inline;">
        The creation of listener facial responses aims to simulate interactive communication feedback from a listener during a face-to-face conversation. Our goal is to generate believable videos of listeners&#39; heads that respond authentically to a single speaker by a sequence-to-sequence model with an combination of WaveNet and Long short-term memory network. Our approach focuses on capturing the subtle n&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05089v1-abstract-full').style.display = 'inline'; document.getElementById('2409.05089v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05089v1-abstract-full" style="display: none;">
        The creation of listener facial responses aims to simulate interactive communication feedback from a listener during a face-to-face conversation. Our goal is to generate believable videos of listeners&#39; heads that respond authentically to a single speaker by a sequence-to-sequence model with an combination of WaveNet and Long short-term memory network. Our approach focuses on capturing the subtle nuances of listener feedback, ensuring the preservation of individual listener identity while expressing appropriate attitudes and viewpoints. Experiment results show that our method surpasses the baseline models on ViCo benchmark Dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05089v1-abstract-full').style.display = 'none'; document.getElementById('2409.05089v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05032">arXiv:2409.05032</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05032">pdf</a>, <a href="https://arxiv.org/format/2409.05032">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring WavLM Back-ends for <span class="search-hit mathjax">Speech</span> Spoofing and Deepfake Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Stourbe%2C+T">Theophile Stourbe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Miara%2C+V">Victor Miara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lepage%2C+T">Theo Lepage</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dehak%2C+R">Reda Dehak</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05032v1-abstract-short" style="display: inline;">
        This paper describes our submitted systems to the ASVspoof 5 Challenge Track 1: <span class="search-hit mathjax">Speech</span> Deepfake Detection - Open Condition, which consists of a stand-alone&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05032v1-abstract-full').style.display = 'inline'; document.getElementById('2409.05032v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05032v1-abstract-full" style="display: none;">
        This paper describes our submitted systems to the ASVspoof 5 Challenge Track 1: <span class="search-hit mathjax">Speech</span> Deepfake Detection - Open Condition, which consists of a stand-alone <span class="search-hit mathjax">speech</span> deepfake (bonafide vs spoof) detection task. Recently, large-scale self-supervised models become a standard in Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) and other <span class="search-hit mathjax">speech</span> processing tasks. Thus, we leverage a pre-trained WavLM as a front-end model and pool its representations with different back-end techniques. The complete framework is fine-tuned using only the trained dataset of the challenge, similar to the close condition. Besides, we adopt data-augmentation by adding noise and reverberation using MUSAN noise and RIR datasets. We also experiment with codec augmentations to increase the performance of our method. Ultimately, we use the Bosaris toolkit for score calibration and system fusion to get better Cllr scores. Our fused system achieves 0.0937 minDCF, 3.42% EER, 0.1927 Cllr, and 0.1375 actDCF.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05032v1-abstract-full').style.display = 'none'; document.getElementById('2409.05032v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05015">arXiv:2409.05015</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05015">pdf</a>, <a href="https://arxiv.org/format/2409.05015">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Multimodal Emotion <span class="search-hit mathjax">Recognition</span> by Leveraging Acoustic Adaptation and Visual Alignment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Z">Zhixian Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Haifeng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+D">Dongmei Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+L">Lei Xie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05015v2-abstract-short" style="display: inline;">
        Multimodal Emotion <span class="search-hit mathjax">Recognition</span> (MER) aims to automatically identify and understand human emotional states by integrating information from various modalities. However, the scarcity of annotated multimodal data significantly hinders the advancement of this research field. This paper presents our solution for the MER-SEMI sub-challenge of MER 2024. First, to be&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05015v2-abstract-full').style.display = 'inline'; document.getElementById('2409.05015v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05015v2-abstract-full" style="display: none;">
        Multimodal Emotion <span class="search-hit mathjax">Recognition</span> (MER) aims to automatically identify and understand human emotional states by integrating information from various modalities. However, the scarcity of annotated multimodal data significantly hinders the advancement of this research field. This paper presents our solution for the MER-SEMI sub-challenge of MER 2024. First, to better adapt acoustic modality features for the MER task, we experimentally evaluate the contributions of different layers of the pre-trained <span class="search-hit mathjax">speech</span> model HuBERT in emotion <span class="search-hit mathjax">recognition</span>. Based on these observations, we perform Parameter-Efficient Fine-Tuning (PEFT) on the layers identified as most effective for emotion <span class="search-hit mathjax">recognition</span> tasks, thereby achieving optimal adaptation for emotion <span class="search-hit mathjax">recognition</span> with a minimal number of learnable parameters. Second, leveraging the strengths of the acoustic modality, we propose a feature alignment pre-training method. This approach uses large-scale unlabeled data to train a visual encoder, thereby promoting the semantic alignment of visual features within the acoustic feature space. Finally, using the adapted acoustic features, aligned visual features, and lexical features, we employ an attention mechanism for feature fusion. On the MER2024-SEMI test set, the proposed method achieves a weighted F1 score of 88.90%, ranking fourth among all participating teams, validating the effectiveness of our approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05015v2-abstract-full').style.display = 'none'; document.getElementById('2409.05015v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05005">arXiv:2409.05005</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05005">pdf</a>, <a href="https://arxiv.org/format/2409.05005">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Patronizing and Condescending Language in Chinese Videos: A Multimodal Dataset and Detector
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hongbo Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+J">Junyu Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Y">Yan Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+K">Kai Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+L">Liang Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+H">Hongfei Lin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05005v2-abstract-short" style="display: inline;">
        Patronizing and Condescending Language (PCL) is a form of discriminatory toxic <span class="search-hit mathjax">speech</span> targeting vulnerable groups, threatening both online and offline safety. While toxic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05005v2-abstract-full').style.display = 'inline'; document.getElementById('2409.05005v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05005v2-abstract-full" style="display: none;">
        Patronizing and Condescending Language (PCL) is a form of discriminatory toxic <span class="search-hit mathjax">speech</span> targeting vulnerable groups, threatening both online and offline safety. While toxic <span class="search-hit mathjax">speech</span> research has mainly focused on overt toxicity, such as hate <span class="search-hit mathjax">speech</span>, microaggressions in the form of PCL remain underexplored. Additionally, dominant groups&#39; discriminatory facial expressions and attitudes toward vulnerable communities can be more impactful than verbal cues, yet these frame features are often overlooked. In this paper, we introduce the PCLMM dataset, the first Chinese multimodal dataset for PCL, consisting of 715 annotated videos from Bilibili, with high-quality PCL facial frame spans. We also propose the MultiPCL detector, featuring a facial expression detection module for PCL <span class="search-hit mathjax">recognition</span>, demonstrating the effectiveness of modality complementarity in this challenging task. Our work makes an important contribution to advancing microaggression detection within the domain of toxic <span class="search-hit mathjax">speech</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05005v2-abstract-full').style.display = 'none'; document.getElementById('2409.05005v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under review in ICASSP 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.05004">arXiv:2409.05004</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.05004">pdf</a>, <a href="https://arxiv.org/format/2409.05004">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhengyang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shuai Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+M">Mingyang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xuechen Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yamagishi%2C+J">Junichi Yamagishi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+Y">Yanmin Qian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.05004v2-abstract-short" style="display: inline;">
        Voice conversion (VC) aims to modify the speaker&#39;s timbre while retaining <span class="search-hit mathjax">speech</span> content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05004v2-abstract-full').style.display = 'inline'; document.getElementById('2409.05004v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.05004v2-abstract-full" style="display: none;">
        Voice conversion (VC) aims to modify the speaker&#39;s timbre while retaining <span class="search-hit mathjax">speech</span> content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of <span class="search-hit mathjax">speech</span> content information. Recently, in-context learning (ICL) has emerged in text-to-<span class="search-hit mathjax">speech</span> (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source <span class="search-hit mathjax">speech</span>. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion <span class="search-hit mathjax">recognition</span> model into our system. Integration of prosody embeddings notably enhances the system&#39;s capability to preserve source <span class="search-hit mathjax">speech</span> prosody, as validated on the Emotional <span class="search-hit mathjax">Speech</span> Database.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.05004v2-abstract-full').style.display = 'none'; document.getElementById('2409.05004v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.04949">arXiv:2409.04949</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.04949">pdf</a>, <a href="https://arxiv.org/format/2409.04949">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.5121/csit.2024.140604">10.5121/csit.2024.140604 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Attention-Based Efficient Breath Sound Removal in Studio Audio Recordings
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elgiriyewithana%2C+N">Nidula Elgiriyewithana</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kodikara%2C+N+D">N. D. Kodikara</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.04949v1-abstract-short" style="display: inline;">
        In this research, we present an innovative, parameter-efficient model that utilizes the attention U-Net architecture for the automatic detection and eradication of non-<span class="search-hit mathjax">speech</span> vocal sounds, specifically breath sounds, in vocal recordings. This task is of paramount importance in the field of sound engineering, despite being relatively under-explored. The conve&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04949v1-abstract-full').style.display = 'inline'; document.getElementById('2409.04949v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.04949v1-abstract-full" style="display: none;">
        In this research, we present an innovative, parameter-efficient model that utilizes the attention U-Net architecture for the automatic detection and eradication of non-<span class="search-hit mathjax">speech</span> vocal sounds, specifically breath sounds, in vocal recordings. This task is of paramount importance in the field of sound engineering, despite being relatively under-explored. The conventional manual process for detecting and eliminating these sounds requires significant expertise and is extremely time-intensive. Existing automated detection and removal methods often fall short in terms of efficiency and precision. Our proposed model addresses these limitations by offering a streamlined process and superior accuracy, achieved through the application of advanced deep learning techniques. A unique dataset, derived from Device and Produced <span class="search-hit mathjax">Speech</span> (DAPS), was employed for this purpose. The training phase of the model emphasizes a log spectrogram and integrates an early stopping mechanism to prevent overfitting. Our model not only conserves precious time for sound engineers but also enhances the quality and consistency of audio production. This constitutes a significant breakthrough, as evidenced by its comparative efficiency, necessitating only 1.9M parameters and a training duration of 3.2 hours - markedly less than the top-performing models in this domain. The model is capable of generating identical outputs as previous models with drastically improved precision, making it an optimal choice.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04949v1-abstract-full').style.display = 'none'; document.getElementById('2409.04949v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        CS &amp; IT Conference Proceedings, vol. 14, no. 6, 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.04447">arXiv:2409.04447</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.04447">pdf</a>, <a href="https://arxiv.org/format/2409.04447">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Leveraging Contrastive Learning and Self-Training for Multimodal Emotion <span class="search-hit mathjax">Recognition</span> with Limited Labeled Samples
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+Q">Qi Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yutong Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xin%2C+Y">Yi Xin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+X">Xinyu Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+G">Guanglai Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+M">Miao Ma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.04447v1-abstract-short" style="display: inline;">
        The Multimodal Emotion <span class="search-hit mathjax">Recognition</span> challenge MER2024 focuses on recognizing emotions using audio, language, and visual signals. In this paper, we present our submission solutions for the Semi-Supervised Learning Sub-Challenge (MER2024-SEMI), which tackles the issue of limited annotated data in emotion <span class="search-hit mathjax">recognition</span>. Firs&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04447v1-abstract-full').style.display = 'inline'; document.getElementById('2409.04447v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.04447v1-abstract-full" style="display: none;">
        The Multimodal Emotion <span class="search-hit mathjax">Recognition</span> challenge MER2024 focuses on recognizing emotions using audio, language, and visual signals. In this paper, we present our submission solutions for the Semi-Supervised Learning Sub-Challenge (MER2024-SEMI), which tackles the issue of limited annotated data in emotion <span class="search-hit mathjax">recognition</span>. Firstly, to address the class imbalance, we adopt an oversampling strategy. Secondly, we propose a modality representation combinatorial contrastive learning (MR-CCL) framework on the trimodal input data to establish robust initial models. Thirdly, we explore a self-training approach to expand the training set. Finally, we enhance prediction robustness through a multi-classifier weighted soft voting strategy. Our proposed method is validated to be effective on the MER2024-SEMI Challenge, achieving a weighted average F-score of 88.25% and ranking 6th on the leaderboard. Our project is available at https://github.com/WooyoohL/MER2024-SEMI.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04447v1-abstract-full').style.display = 'none'; document.getElementById('2409.04447v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ACM MM Workshop 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.04007">arXiv:2409.04007</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.04007">pdf</a>, <a href="https://arxiv.org/format/2409.04007">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Searching for Effective Preprocessing Method and CNN-based Architecture with Efficient Channel Attention on <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+B">Byunggun Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kwon%2C+Y">Younghun Kwon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.04007v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04007v1-abstract-full').style.display = 'inline'; document.getElementById('2409.04007v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.04007v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) classifies human emotions in <span class="search-hit mathjax">speech</span> with a computer model. Recently, performance in SER has steadily increased as deep learning techniques have adapted. However, unlike many domains that use <span class="search-hit mathjax">speech</span> data, data for training in the SER model is insufficient. This causes overfitting of training of the neural network, resulting in performance degradation. In fact, successful emotion <span class="search-hit mathjax">recognition</span> requires an effective preprocessing method and a model structure that efficiently uses the number of weight parameters. In this study, we propose using eight dataset versions with different frequency-time resolutions to search for an effective emotional <span class="search-hit mathjax">speech</span> preprocessing method. We propose a 6-layer convolutional neural network (CNN) model with efficient channel attention (ECA) to pursue an efficient model structure. In particular, the well-positioned ECA blocks can improve channel feature representation with only a few parameters. With the interactive emotional dyadic motion capture (IEMOCAP) dataset, increasing the frequency resolution in preprocessing emotional <span class="search-hit mathjax">speech</span> can improve emotion <span class="search-hit mathjax">recognition</span> performance. Also, ECA after the deep convolution layer can effectively increase channel feature representation. Consequently, the best result (79.37UA 79.68WA) can be obtained, exceeding the performance of previous SER models. Furthermore, to compensate for the lack of emotional <span class="search-hit mathjax">speech</span> data, we experiment with multiple preprocessing data methods that augment trainable data preprocessed with all different settings from one sample. In the experiment, we can achieve the highest result (80.28UA 80.46WA).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.04007v1-abstract-full').style.display = 'none'; document.getElementById('2409.04007v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.03605">arXiv:2409.03605</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.03605">pdf</a>, <a href="https://arxiv.org/format/2409.03605">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+L">Lingyu Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+X">Xize Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+J">Jintao Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xianjia Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiandong Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+L">Lei Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+F">Fei Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Minglei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Huang Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Z">Zhihu Hu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.03605v1-abstract-short" style="display: inline;">
        &hellip;and image textures by introducing segmentation as intermediate representation. Specifically, given the mask of image employed by a parsing network, we first leverage the <span class="search-hit mathjax">speech</span> to drive the mask and generate talking segmentation. Then we disentangle semantic regions of image into style codes using a mask-guided encoder. Ultimately, we inject the previously g&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03605v1-abstract-full').style.display = 'inline'; document.getElementById('2409.03605v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.03605v1-abstract-full" style="display: none;">
        Audio-driven talking face generation aims to synthesize video with lip movements synchronized to input audio. However, current generative techniques face challenges in preserving intricate regional textures (skin, teeth). To address the aforementioned challenges, we propose a novel framework called SegTalker to decouple lip movements and image textures by introducing segmentation as intermediate representation. Specifically, given the mask of image employed by a parsing network, we first leverage the <span class="search-hit mathjax">speech</span> to drive the mask and generate talking segmentation. Then we disentangle semantic regions of image into style codes using a mask-guided encoder. Ultimately, we inject the previously generated talking segmentation and style codes into a mask-guided StyleGAN to synthesize video frame. In this way, most of textures are fully preserved. Moreover, our approach can inherently achieve background separation and facilitate mask-guided facial local editing. In particular, by editing the mask and swapping the region textures from a given reference image (e.g. hair, lip, eyebrows), our approach enables facial editing seamlessly when generating talking face video. Experiments demonstrate that our proposed approach can effectively preserve texture details and generate temporally consistent video while remaining competitive in lip synchronization. Quantitative and qualitative results on the HDTF and MEAD datasets illustrate the superior performance of our method over existing methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03605v1-abstract-full').style.display = 'none'; document.getElementById('2409.03605v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 7 figures, 3 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.03336">arXiv:2409.03336</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.03336">pdf</a>, <a href="https://arxiv.org/format/2409.03336">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Estimating Indoor Scene Depth Maps from Ultrasonic Echoes
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Honma%2C+J">Junpei Honma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kimura%2C+A">Akisato Kimura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Irie%2C+G">Go Irie</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.03336v2-abstract-short" style="display: inline;">
        Measuring 3D geometric structures of indoor scenes requires dedicated depth sensors, which are not always available. Echo-based depth estimation has recently been studied as a promising alternative solution. All previous studies have assumed the use of echoes in the audible range. However, one major problem is that audible echoes cannot be used in quiet spaces or other situations where producing a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03336v2-abstract-full').style.display = 'inline'; document.getElementById('2409.03336v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.03336v2-abstract-full" style="display: none;">
        Measuring 3D geometric structures of indoor scenes requires dedicated depth sensors, which are not always available. Echo-based depth estimation has recently been studied as a promising alternative solution. All previous studies have assumed the use of echoes in the audible range. However, one major problem is that audible echoes cannot be used in quiet spaces or other situations where producing audible sounds is prohibited. In this paper, we consider echo-based depth estimation using inaudible ultrasonic echoes. While ultrasonic waves provide high measurement accuracy in theory, the actual depth estimation accuracy when ultrasonic echoes are used has remained unclear, due to its disadvantage of being sensitive to noise and susceptible to attenuation. We first investigate the depth estimation accuracy when the frequency of the sound source is restricted to the high-frequency band, and found that the accuracy decreased when the frequency was limited to ultrasonic ranges. Based on this observation, we propose a novel deep learning method to improve the accuracy of ultrasonic echo-based depth estimation by using audible echoes as auxiliary data only during training. Experimental results with a public dataset demonstrate that our method improves the estimation accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03336v2-abstract-full').style.display = 'none'; document.getElementById('2409.03336v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICIP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.03115">arXiv:2409.03115</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.03115">pdf</a>, <a href="https://arxiv.org/format/2409.03115">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Probing self-attention in self-supervised <span class="search-hit mathjax">speech</span> models for cross-linguistic differences
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gopinath%2C+S">Sai Gopinath</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rodriguez%2C+J">Joselyn Rodriguez</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.03115v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> models have gained traction thanks to increase in accuracy from novel transformer architectures. While this impressive increase in performance across automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03115v1-abstract-full').style.display = 'inline'; document.getElementById('2409.03115v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.03115v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> models have gained traction thanks to increase in accuracy from novel transformer architectures. While this impressive increase in performance across automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) benchmarks is noteworthy, there is still much that is unknown about the use of attention mechanisms for <span class="search-hit mathjax">speech</span>-related tasks. For example, while it is assumed that these models are learning language-independent (i.e., universal) <span class="search-hit mathjax">speech</span> representations, there has not yet been an in-depth exploration of what it would mean for the models to be language-independent. In the current paper, we explore this question within the realm of self-attention mechanisms of one small self-supervised <span class="search-hit mathjax">speech</span> transformer model (TERA). We find that even with a small model, the attention heads learned are diverse ranging from almost entirely diagonal to almost entirely global regardless of the training language. We highlight some notable differences in attention patterns between Turkish and English and demonstrate that the models do learn important phonological information during pretraining. We also present a head ablation study which shows that models across languages primarily rely on diagonal heads to classify phonemes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03115v1-abstract-full').style.display = 'none'; document.getElementById('2409.03115v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 18 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T10
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.03059">arXiv:2409.03059</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.03059">pdf</a>, <a href="https://arxiv.org/format/2409.03059">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-2300">10.21437/Interspeech.2024-2300 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantification of stylistic differences in human- and ASR-produced transcripts of African American English
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Heuser%2C+A">Annika Heuser</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kendall%2C+T">Tyler Kendall</a>, 
      
      <a href="/search/?searchtype=author&amp;query=del+Rio%2C+M">Miguel del Rio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McNamara%2C+Q">Quinten McNamara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhandari%2C+N">Nishchal Bhandari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Miller%2C+C">Corey Miller</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jett%C3%A9%2C+M">Migüel Jetté</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.03059v1-abstract-short" style="display: inline;">
        Common measures of accuracy used to assess the performance of automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03059v1-abstract-full').style.display = 'inline'; document.getElementById('2409.03059v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.03059v1-abstract-full" style="display: none;">
        Common measures of accuracy used to assess the performance of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems, as well as human transcribers, conflate multiple sources of error. Stylistic differences, such as verbatim vs non-verbatim, can play a significant role in ASR performance evaluation when differences exist between training and test datasets. The problem is compounded for <span class="search-hit mathjax">speech</span> from underrepresented varieties, where the <span class="search-hit mathjax">speech</span> to orthography mapping is not as standardized. We categorize the kinds of stylistic differences between 6 transcription versions, 4 human- and 2 ASR-produced, of 10 hours of African American English (AAE) <span class="search-hit mathjax">speech</span>. Focusing on verbatim features and AAE morphosyntactic features, we investigate the interactions of these categories with how well transcripts can be compared via word error rate (WER). The results, and overall analysis, help clarify how ASR outputs are a function of the decisions made by the training data&#39;s human transcribers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03059v1-abstract-full').style.display = 'none'; document.getElementById('2409.03059v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in Interspeech 2024 Proceedings, 5 pages excluding references, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02865">arXiv:2409.02865</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02865">pdf</a>, <a href="https://arxiv.org/format/2409.02865">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Visually Grounded <span class="search-hit mathjax">Speech</span> Models for Low-resource Languages and Cognitive Modelling
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nortje%2C+L">Leanne Nortje</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02865v1-abstract-short" style="display: inline;">
        This dissertation examines visually grounded <span class="search-hit mathjax">speech</span> (VGS) models that learn from unlabelled&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02865v1-abstract-full').style.display = 'inline'; document.getElementById('2409.02865v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02865v1-abstract-full" style="display: none;">
        This dissertation examines visually grounded <span class="search-hit mathjax">speech</span> (VGS) models that learn from unlabelled <span class="search-hit mathjax">speech</span> paired with images. It focuses on applications for low-resource languages and understanding human language acquisition. We introduce a task called visually prompted keyword localisation to detect and localise keywords in <span class="search-hit mathjax">speech</span> using images. We demonstrate the effectiveness of VGS models in few-shot learning scenarios for low-resource languages like Yoruba. Additionally, we examine the mutual exclusivity bias in VGS models. Our monolingual VGS model exhibits this bias, but we found that multilingualism does not affect the bias in this VGS model similarly to what is observed in children.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02865v1-abstract-full').style.display = 'none'; document.getElementById('2409.02865v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">PhD Dissertation</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02615">arXiv:2409.02615</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02615">pdf</a>, <a href="https://arxiv.org/format/2409.02615">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        USEF-TSE: Universal Speaker Embedding Free Target Speaker Extraction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+B">Bang Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Ming Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02615v1-abstract-short" style="display: inline;">
        Target speaker extraction aims to isolate the voice of a specific speaker from mixed <span class="search-hit mathjax">speech</span>. Traditionally, this process has relied on extracting a speaker embedding from a reference&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02615v1-abstract-full').style.display = 'inline'; document.getElementById('2409.02615v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02615v1-abstract-full" style="display: none;">
        Target speaker extraction aims to isolate the voice of a specific speaker from mixed <span class="search-hit mathjax">speech</span>. Traditionally, this process has relied on extracting a speaker embedding from a reference <span class="search-hit mathjax">speech</span>, necessitating a speaker <span class="search-hit mathjax">recognition</span> model. However, identifying an appropriate speaker <span class="search-hit mathjax">recognition</span> model can be challenging, and using the target speaker embedding as reference information may not be optimal for target speaker extraction tasks. This paper introduces a Universal Speaker Embedding-Free Target Speaker Extraction (USEF-TSE) framework that operates without relying on speaker embeddings. USEF-TSE utilizes a multi-head cross-attention mechanism as a frame-level target speaker feature extractor. This innovative approach allows mainstream speaker extraction solutions to bypass the dependency on speaker <span class="search-hit mathjax">recognition</span> models and to fully leverage the information available in the enrollment <span class="search-hit mathjax">speech</span>, including speaker characteristics and contextual details. Additionally, USEF-TSE can seamlessly integrate with any time-domain or time-frequency domain <span class="search-hit mathjax">speech</span> separation model to achieve effective speaker extraction. Experimental results show that our proposed method achieves state-of-the-art (SOTA) performance in terms of Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) on the WSJ0-2mix, WHAM!, and WHAMR! datasets, which are standard benchmarks for monaural anechoic, noisy and noisy-reverberant two-speaker <span class="search-hit mathjax">speech</span> separation and speaker extraction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02615v1-abstract-full').style.display = 'none'; document.getElementById('2409.02615v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02565">arXiv:2409.02565</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02565">pdf</a>, <a href="https://arxiv.org/format/2409.02565">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Extraction of Noise-Robust Discrete Units from Self-Supervised <span class="search-hit mathjax">Speech</span> Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Poncelet%2C+J">Jakob Poncelet</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yujun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Van+hamme%2C+H">Hugo Van hamme</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02565v1-abstract-short" style="display: inline;">
        Continuous <span class="search-hit mathjax">speech</span> can be converted into a discrete sequence by deriving discrete units from the hidden features of self-supervised learned (SSL)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02565v1-abstract-full').style.display = 'inline'; document.getElementById('2409.02565v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02565v1-abstract-full" style="display: none;">
        Continuous <span class="search-hit mathjax">speech</span> can be converted into a discrete sequence by deriving discrete units from the hidden features of self-supervised learned (SSL) <span class="search-hit mathjax">speech</span> models. Although SSL models are becoming larger and trained on more data, they are often sensitive to real-life distortions like additive noise or reverberation, which translates to a shift in discrete units. We propose a parameter-efficient approach to generate noise-robust discrete units from pre-trained SSL models by training a small encoder-decoder model, with or without adapters, to simultaneously denoise and discretise the hidden features of the SSL model. The model learns to generate a clean discrete sequence for a noisy utterance, conditioned on the SSL features. The proposed denoiser outperforms several pre-training methods on the tasks of noisy discretisation and noisy <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and can be finetuned to the target environment with a few recordings of unlabeled target data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02565v1-abstract-full').style.display = 'none'; document.getElementById('2409.02565v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at SLT2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02449">arXiv:2409.02449</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02449">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Manohar%2C+K">Kavya Manohar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pillai%2C+L+G">Leena G Pillai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02449v2-abstract-short" style="display: inline;">
        This paper explores the pitfalls in evaluating multilingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models, with a particular focus on Indic language scripts. We investigate the text normalization routine employed by leading ASR models, including OpenAI Whisper, Meta&#39;s MMS, Seamless, and Assembly AI&#39;s Conformer, and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02449v2-abstract-full').style.display = 'inline'; document.getElementById('2409.02449v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02449v2-abstract-full" style="display: none;">
        This paper explores the pitfalls in evaluating multilingual automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models, with a particular focus on Indic language scripts. We investigate the text normalization routine employed by leading ASR models, including OpenAI Whisper, Meta&#39;s MMS, Seamless, and Assembly AI&#39;s Conformer, and their unintended consequences on performance metrics. Our research reveals that current text normalization practices, while aiming to standardize ASR outputs for fair comparison, by removing inconsistencies such as variations in spelling, punctuation, and special characters, are fundamentally flawed when applied to Indic scripts. Through empirical analysis using text similarity scores and in-depth linguistic examination, we demonstrate that these flaws lead to artificially improved performance metrics for Indic languages. We conclude by proposing a shift towards developing text normalization routines that leverage native linguistic expertise, ensuring more robust and accurate evaluations of multilingual ASR models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02449v2-abstract-full').style.display = 'none'; document.getElementById('2409.02449v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to EMNLP 2024 Main</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T50; 91F20; 68T10
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.1; I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02243">arXiv:2409.02243</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02243">pdf</a>, <a href="https://arxiv.org/format/2409.02243">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Novel Audio-Visual Information Fusion System for Mental Disorders Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yichun Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Shuanglin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Naqvi%2C+S+M">Syed Mohsen Naqvi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02243v1-abstract-short" style="display: inline;">
        &hellip;huge. Moreover, most systems are only trained for a specific mental disorder and are not general-purpose. Recently, physiological studies have shown that there are some <span class="search-hit mathjax">speech</span> and facial-related symptoms in a few mental disorders (e.g., depression and ADHD). In this paper, we focus on the emotional expression features of mental disorders and introduce a mult&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02243v1-abstract-full').style.display = 'inline'; document.getElementById('2409.02243v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02243v1-abstract-full" style="display: none;">
        Mental disorders are among the foremost contributors to the global healthcare challenge. Research indicates that timely diagnosis and intervention are vital in treating various mental disorders. However, the early somatization symptoms of certain mental disorders may not be immediately evident, often resulting in their oversight and misdiagnosis. Additionally, the traditional diagnosis methods incur high time and cost. Deep learning methods based on fMRI and EEG have improved the efficiency of the mental disorder detection process. However, the cost of the equipment and trained staff are generally huge. Moreover, most systems are only trained for a specific mental disorder and are not general-purpose. Recently, physiological studies have shown that there are some <span class="search-hit mathjax">speech</span> and facial-related symptoms in a few mental disorders (e.g., depression and ADHD). In this paper, we focus on the emotional expression features of mental disorders and introduce a multimodal mental disorder diagnosis system based on audio-visual information input. Our proposed system is based on spatial-temporal attention networks and innovative uses a less computationally intensive pre-train audio <span class="search-hit mathjax">recognition</span> network to fine-tune the video <span class="search-hit mathjax">recognition</span> module for better results. We also apply the unified system for multiple mental disorders (ADHD and depression) for the first time. The proposed system achieves over 80\% accuracy on the real multimodal ADHD dataset and achieves state-of-the-art results on the depression dataset AVEC 2014.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02243v1-abstract-full').style.display = 'none'; document.getElementById('2409.02243v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">27th International Conference on Information (FUSION)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02239">arXiv:2409.02239</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02239">pdf</a>, <a href="https://arxiv.org/format/2409.02239">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Temporal Order Preserved Optimal Transport-based Cross-modal Knowledge Transfer Learning for ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+X">Xugang Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+P">Peng Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsao%2C+Y">Yu Tsao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kawai%2C+H">Hisashi Kawai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02239v2-abstract-short" style="display: inline;">
        Transferring linguistic knowledge from a pretrained language model (PLM) to an acoustic model has been shown to greatly improve the performance of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). However, due to the heterogeneous feature distributions in cross-modalities, designing an effective model for feature alignment and knowl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02239v2-abstract-full').style.display = 'inline'; document.getElementById('2409.02239v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02239v2-abstract-full" style="display: none;">
        Transferring linguistic knowledge from a pretrained language model (PLM) to an acoustic model has been shown to greatly improve the performance of automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). However, due to the heterogeneous feature distributions in cross-modalities, designing an effective model for feature alignment and knowledge transfer between linguistic and acoustic sequences remains a challenging task. Optimal transport (OT), which efficiently measures probability distribution discrepancies, holds great potential for aligning and transferring knowledge between acoustic and linguistic modalities. Nonetheless, the original OT treats acoustic and linguistic feature sequences as two unordered sets in alignment and neglects temporal order information during OT coupling estimation. Consequently, a time-consuming pretraining stage is required to learn a good alignment between the acoustic and linguistic representations. In this paper, we propose a Temporal Order Preserved OT (TOT)-based Cross-modal Alignment and Knowledge Transfer (CAKT) (TOT-CAKT) for ASR. In the TOT-CAKT, local neighboring frames of acoustic sequences are smoothly mapped to neighboring regions of linguistic sequences, preserving their temporal order relationship in feature alignment and matching. With the TOT-CAKT model framework, we conduct Mandarin ASR experiments with a pretrained Chinese PLM for linguistic knowledge transfer. Our results demonstrate that the proposed TOT-CAKT significantly improves ASR performance compared to several state-of-the-art models employing linguistic knowledge transfer, and addresses the weaknesses of the original OT-based method in sequential feature alignment for ASR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02239v2-abstract-full').style.display = 'none'; document.getElementById('2409.02239v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to IEEE SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02111">arXiv:2409.02111</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02111">pdf</a>, <a href="https://arxiv.org/format/2409.02111">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Y">Yangfan Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Q">Qian Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+G">Guoqi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+H">Huajin Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+G">Gang Pan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02111v1-abstract-short" style="display: inline;">
        Deep learning has revolutionized artificial intelligence (AI), achieving remarkable progress in fields such as computer vision, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and natural language processing. Moreover, the recent success of large language models (LLMs) has fueled a surge in research on large-scale neural networks. However, the es&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02111v1-abstract-full').style.display = 'inline'; document.getElementById('2409.02111v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02111v1-abstract-full" style="display: none;">
        Deep learning has revolutionized artificial intelligence (AI), achieving remarkable progress in fields such as computer vision, <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and natural language processing. Moreover, the recent success of large language models (LLMs) has fueled a surge in research on large-scale neural networks. However, the escalating demand for computing resources and energy consumption has prompted the search for energy-efficient alternatives. Inspired by the human brain, spiking neural networks (SNNs) promise energy-efficient computation with event-driven spikes. To provide future directions toward building energy-efficient large SNN models, we present a survey of existing methods for developing deep spiking neural networks, with a focus on emerging Spiking Transformers. Our main contributions are as follows: (1) an overview of learning methods for deep spiking neural networks, categorized by ANN-to-SNN conversion and direct training with surrogate gradients; (2) an overview of network architectures for deep spiking neural networks, categorized by deep convolutional neural networks (DCNNs) and Transformer architecture; and (3) a comprehensive comparison of state-of-the-art deep SNNs with a focus on emerging Spiking Transformers. We then further discuss and outline future directions toward large-scale SNNs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02111v1-abstract-full').style.display = 'none'; document.getElementById('2409.02111v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 August, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02050">arXiv:2409.02050</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02050">pdf</a>, <a href="https://arxiv.org/format/2409.02050">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Code-Switching <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with LID-Based Collaborative Mixture of Experts Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H">Hukai Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+J">Jiayan Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+K">Kaidi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yishuang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guan%2C+W">Wenhao Guan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+Q">Qingyang Hong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02050v2-abstract-short" style="display: inline;">
        Due to the inherent difficulty in modeling phonetic similarities across different languages, code-switching <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> presents a formidable challenge. This study proposes a Collaborative-MoE, a Mixture of Experts (MoE) model that leverages a collaborative mechanism among expert groups. Initially, a preceding r&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02050v2-abstract-full').style.display = 'inline'; document.getElementById('2409.02050v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02050v2-abstract-full" style="display: none;">
        Due to the inherent difficulty in modeling phonetic similarities across different languages, code-switching <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> presents a formidable challenge. This study proposes a Collaborative-MoE, a Mixture of Experts (MoE) model that leverages a collaborative mechanism among expert groups. Initially, a preceding routing network explicitly learns Language Identification (LID) tasks and selects experts based on acquired LID weights. This process ensures robust routing information to the MoE layer, mitigating interference from diverse language domains on expert network parameter updates. The LID weights are also employed to facilitate inter-group collaboration, enabling the integration of language-specific representations. Furthermore, within each language expert group, a gating network operates unsupervised to foster collaboration on attributes beyond language. Extensive experiments demonstrate the efficacy of our approach, achieving significant performance enhancements compared to alternative methods. Importantly, our method preserves the efficient inference capabilities characteristic of MoE models without necessitating additional pre-training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02050v2-abstract-full').style.display = 'none'; document.getElementById('2409.02050v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by IEEE SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02041">arXiv:2409.02041</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02041">pdf</a>, <a href="https://arxiv.org/format/2409.02041">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The USTC-NERCSLIP Systems for the CHiME-8 NOTSOFAR-1 Challenge
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+S">Shutong Niu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Ruoyu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+J">Jun Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+G">Gaobin Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tu%2C+Y">Yanhui Tu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+S">Siyuan Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+S">Shuangqing Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+H">Huaxin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Haitao Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xueyang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhong%2C+G">Guolong Zhong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+X">Xindi Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jieru Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+M">Mengzhi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+D">Di Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+T">Tian Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+G">Genshun Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+F">Feng Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+J">Jia Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+J">Jianqing Gao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02041v1-abstract-short" style="display: inline;">
        &hellip;rates, background noises, a variable number of speakers, and natural conversation styles. To address these issues, we optimized the system in several aspects: For front-end <span class="search-hit mathjax">speech</span> signal processing, we introduced a data-driven joint training method for diarization and separation (JDS) to enhance audio quality. Additionally, we also integrated traditional gui&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02041v1-abstract-full').style.display = 'inline'; document.getElementById('2409.02041v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02041v1-abstract-full" style="display: none;">
        This technical report outlines our submission system for the CHiME-8 NOTSOFAR-1 Challenge. The primary difficulty of this challenge is the dataset recorded across various conference rooms, which captures real-world complexities such as high overlap rates, background noises, a variable number of speakers, and natural conversation styles. To address these issues, we optimized the system in several aspects: For front-end <span class="search-hit mathjax">speech</span> signal processing, we introduced a data-driven joint training method for diarization and separation (JDS) to enhance audio quality. Additionally, we also integrated traditional guided source separation (GSS) for multi-channel track to provide complementary information for the JDS. For back-end <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, we enhanced Whisper with WavLM, ConvNeXt, and Transformer innovations, applying multi-task training and Noise KLD augmentation, to significantly advance ASR robustness and accuracy. Our system attained a Time-Constrained minimum Permutation Word Error Rate (tcpWER) of 14.265% and 22.989% on the CHiME-8 NOTSOFAR-1 Dev-set-2 multi-channel and single-channel tracks, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02041v1-abstract-full').style.display = 'none'; document.getElementById('2409.02041v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.01813">arXiv:2409.01813</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.01813">pdf</a>, <a href="https://arxiv.org/format/2409.01813">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Reassessing Noise Augmentation Methods in the Context of Adversarial <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pizzi%2C+K">Karla Pizzi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=B%2C+M+P+P">Matías P. Pizarro B</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fischer%2C+A">Asja Fischer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.01813v1-abstract-short" style="display: inline;">
        In this study, we investigate if noise-augmented training can concurrently improve adversarial robustness in automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01813v1-abstract-full').style.display = 'inline'; document.getElementById('2409.01813v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.01813v1-abstract-full" style="display: none;">
        In this study, we investigate if noise-augmented training can concurrently improve adversarial robustness in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems. We conduct a comparative analysis of the adversarial robustness of four different state-of-the-art ASR architectures, where each of the ASR architectures is trained under three different augmentation conditions: one subject to background noise, speed variations, and reverberations, another subject to speed variations only, and a third without any form of data augmentation. The results demonstrate that noise augmentation not only improves model performance on noisy <span class="search-hit mathjax">speech</span> but also the model&#39;s robustness to adversarial attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01813v1-abstract-full').style.display = 'none'; document.getElementById('2409.01813v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.01548">arXiv:2409.01548</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.01548">pdf</a>, <a href="https://arxiv.org/format/2409.01548">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VoxHakka: A Dialectally Diverse Multi-speaker Text-to-<span class="search-hit mathjax">Speech</span> System for Taiwanese Hakka
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Li-Wei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-Shin Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+C">Chen-Chi Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.01548v3-abstract-short" style="display: inline;">
        This paper introduces VoxHakka, a text-to-<span class="search-hit mathjax">speech</span> (TTS) system designed for Taiwanese Hakka, a critically under-resourced language spoken in Taiwan. Leveraging the YourTTS framework, VoxHakka achieves high naturalness and accuracy and low real-time factor in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01548v3-abstract-full').style.display = 'inline'; document.getElementById('2409.01548v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.01548v3-abstract-full" style="display: none;">
        This paper introduces VoxHakka, a text-to-<span class="search-hit mathjax">speech</span> (TTS) system designed for Taiwanese Hakka, a critically under-resourced language spoken in Taiwan. Leveraging the YourTTS framework, VoxHakka achieves high naturalness and accuracy and low real-time factor in <span class="search-hit mathjax">speech</span> synthesis while supporting six distinct Hakka dialects. This is achieved by training the model with dialect-specific data, allowing for the generation of speaker-aware Hakka <span class="search-hit mathjax">speech</span>. To address the scarcity of publicly available Hakka <span class="search-hit mathjax">speech</span> corpora, we employed a cost-effective approach utilizing a web scraping pipeline coupled with automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR)-based data cleaning techniques. This process ensured the acquisition of a high-quality, multi-speaker, multi-dialect dataset suitable for TTS training. Subjective listening tests conducted using comparative mean opinion scores (CMOS) demonstrate that VoxHakka significantly outperforms existing publicly available Hakka TTS systems in terms of pronunciation accuracy, tone correctness, and overall naturalness. This work represents a significant advancement in Hakka language technology and provides a valuable resource for language preservation and revitalization efforts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01548v3-abstract-full').style.display = 'none'; document.getElementById('2409.01548v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to O-COCOSDA 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.01438">arXiv:2409.01438</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.01438">pdf</a>, <a href="https://arxiv.org/format/2409.01438">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Resource-Efficient Adaptation of <span class="search-hit mathjax">Speech</span> Foundation Models for Multi-Speaker ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Weiqing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhawan%2C+K">Kunal Dhawan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+T">Taejin Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Puvvada%2C+K+C">Krishna C. Puvvada</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Medennikov%2C+I">Ivan Medennikov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Majumdar%2C+S">Somshubra Majumdar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H">He Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Balam%2C+J">Jagadeesh Balam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ginsburg%2C+B">Boris Ginsburg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.01438v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> foundation models have achieved state-of-the-art (SoTA) performance across various tasks, such as automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01438v1-abstract-full').style.display = 'inline'; document.getElementById('2409.01438v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.01438v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> foundation models have achieved state-of-the-art (SoTA) performance across various tasks, such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) in hundreds of languages. However, multi-speaker ASR remains a challenging task for these models due to data scarcity and sparsity. In this paper, we present approaches to enable <span class="search-hit mathjax">speech</span> foundation models to process and understand multi-speaker <span class="search-hit mathjax">speech</span> with limited training data. Specifically, we adapt a <span class="search-hit mathjax">speech</span> foundation model for the multi-speaker ASR task using only telephonic data. Remarkably, the adapted model also performs well on meeting data without any fine-tuning, demonstrating the generalization ability of our approach. We conduct several ablation studies to analyze the impact of different parameters and strategies on model performance. Our findings highlight the effectiveness of our methods. Results show that less parameters give better overall cpWER, which, although counter-intuitive, provides insights into adapting <span class="search-hit mathjax">speech</span> foundation models for multi-speaker ASR tasks with minimal annotated data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01438v1-abstract-full').style.display = 'none'; document.getElementById('2409.01438v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by SLT 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.01309">arXiv:2409.01309</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.01309">pdf</a>, <a href="https://arxiv.org/format/2409.01309">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Refined Statistical Bounds for Classification Error Mismatches with Constrained Bayes Error
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Z">Zijian Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Eminyan%2C+V">Vahe Eminyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schl%C3%BCter%2C+R">Ralf Schlüter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ney%2C+H">Hermann Ney</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.01309v2-abstract-short" style="display: inline;">
        &hellip;derived in previous works, employing a different method of derivation. Then, motivated by the observation that the Bayes error is typically low in machine learning tasks like <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and pattern <span class="search-hit mathjax">recognition</span>, we derive a refined Kullback-Leibler-divergence-based bound o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01309v2-abstract-full').style.display = 'inline'; document.getElementById('2409.01309v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.01309v2-abstract-full" style="display: none;">
        In statistical classification/multiple hypothesis testing and machine learning, a model distribution estimated from the training data is usually applied to replace the unknown true distribution in the Bayes decision rule, which introduces a mismatch between the Bayes error and the model-based classification error. In this work, we derive the classification error bound to study the relationship between the Kullback-Leibler divergence and the classification error mismatch. We first reconsider the statistical bounds based on classification error mismatch derived in previous works, employing a different method of derivation. Then, motivated by the observation that the Bayes error is typically low in machine learning tasks like <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and pattern <span class="search-hit mathjax">recognition</span>, we derive a refined Kullback-Leibler-divergence-based bound on the error mismatch with the constraint that the Bayes error is lower than a threshold.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01309v2-abstract-full').style.display = 'none'; document.getElementById('2409.01309v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted at 2024 IEEE Information Theory Workshop</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.01011">arXiv:2409.01011</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.01011">pdf</a>, <a href="https://arxiv.org/format/2409.01011">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yingfa Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+C">Chenlong Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+C">Cong Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+C">Chenyang Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+S">Shi Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+X">Xu Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zhiyuan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+M">Maosong Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.01011v1-abstract-short" style="display: inline;">
        &hellip;single character may be a combination of multiple sub-characters, our tokenizer first adopts character detection to locate character boundaries, and then conducts character <span class="search-hit mathjax">recognition</span> at both the character and sub-character levels. Moreover, to support the academic community, we have also assembled the first large-scale dataset of CBSs with over 100K annota&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01011v1-abstract-full').style.display = 'inline'; document.getElementById('2409.01011v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.01011v1-abstract-full" style="display: none;">
        This study presents a multi-modal multi-granularity tokenizer specifically designed for analyzing ancient Chinese scripts, focusing on the Chu bamboo slip (CBS) script used during the Spring and Autumn and Warring States period (771-256 BCE) in Ancient China. Considering the complex hierarchical structure of ancient Chinese scripts, where a single character may be a combination of multiple sub-characters, our tokenizer first adopts character detection to locate character boundaries, and then conducts character <span class="search-hit mathjax">recognition</span> at both the character and sub-character levels. Moreover, to support the academic community, we have also assembled the first large-scale dataset of CBSs with over 100K annotated character image scans. On the part-of-<span class="search-hit mathjax">speech</span> tagging task built on our dataset, using our tokenizer gives a 5.5% relative improvement in F1-score compared to mainstream sub-word tokenizers. Our work not only aids in further investigations of the specific script but also has the potential to advance research on other forms of ancient Chinese scripts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.01011v1-abstract-full').style.display = 'none'; document.getElementById('2409.01011v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 3 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.00986">arXiv:2409.00986</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.00986">pdf</a>, <a href="https://arxiv.org/format/2409.00986">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Personalized Lip Reading: Adapting to Your Unique Lip Movements with Vision and Language
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yeo%2C+J+H">Jeong Hun Yeo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+C+W">Chae Won Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+H">Hyunjun Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rha%2C+H">Hyeongseop Rha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+S">Seunghee Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+W">Wen-Huang Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ro%2C+Y+M">Yong Man Ro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.00986v1-abstract-short" style="display: inline;">
        Lip reading aims to predict spoken language by analyzing lip movements. Despite advancements in lip reading technologies, performance degrades when models are applied to unseen speakers due to their sensitivity to variations in visual information such as lip appearances. To address this challenge, speaker adaptive lip reading technologies have advanced by focusing on effectively adapting a lip rea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.00986v1-abstract-full').style.display = 'inline'; document.getElementById('2409.00986v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.00986v1-abstract-full" style="display: none;">
        Lip reading aims to predict spoken language by analyzing lip movements. Despite advancements in lip reading technologies, performance degrades when models are applied to unseen speakers due to their sensitivity to variations in visual information such as lip appearances. To address this challenge, speaker adaptive lip reading technologies have advanced by focusing on effectively adapting a lip reading model to target speakers in the visual modality. The effectiveness of adapting language information, such as vocabulary choice, of the target speaker has not been explored in the previous works. Moreover, existing datasets for speaker adaptation have limited vocabulary size and pose variations, limiting the validation of previous speaker-adaptive methods in real-world scenarios. To address these issues, we propose a novel speaker-adaptive lip reading method that adapts a pre-trained model to target speakers at both vision and language levels. Specifically, we integrate prompt tuning and the LoRA approach, applying them to a pre-trained lip reading model to effectively adapt the model to target speakers. In addition, to validate its effectiveness in real-world scenarios, we introduce a new dataset, VoxLRS-SA, derived from VoxCeleb2 and LRS3. It contains a vocabulary of approximately 100K words, offers diverse pose variations, and enables the validation of adaptation methods in wild, sentence-level lip reading for the first time. Through various experiments, we demonstrate that the existing speaker-adaptive method also improves performance in the wild at the sentence level. Moreover, with the proposed adaptation method, we show that the proposed method achieves larger improvements when applied to the target speaker, compared to the previous works.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.00986v1-abstract-full').style.display = 'none'; document.getElementById('2409.00986v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code available: https://github.com/JeongHun0716/Personalized-Lip-Reading</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.00971">arXiv:2409.00971</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.00971">pdf</a>, <a href="https://arxiv.org/format/2409.00971">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Interpretable Convolutional SyncNet
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+S">Sungjoon Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yun%2C+J">Jaesub Yun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+D">Donggeon Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+M">Minsik Park</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.00971v1-abstract-short" style="display: inline;">
        &hellip;interpretation. The probabilistic interpretation allows us to define metrics such as probability at offset and offscreen ratio to evaluate the sync quality of audio-visual (AV) <span class="search-hit mathjax">speech</span> datasets. Furthermore, our model achieves SOTA accuracy of $96.5\%$ on the LRS2 dataset and $93.8\%$ on the LRS3 dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.00971v1-abstract-full').style.display = 'inline'; document.getElementById('2409.00971v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.00971v1-abstract-full" style="display: none;">
        Because videos in the wild can be out of sync for various reasons, a sync-net is used to bring the video back into sync for tasks that require synchronized videos. Previous state-of-the-art (SOTA) sync-nets use InfoNCE loss, rely on the transformer architecture, or both. Unfortunately, the former makes the model&#39;s output difficult to interpret, and the latter is unfriendly with large images, thus limiting the usefulness of sync-nets. In this work, we train a convolutional sync-net using the balanced BCE loss (BBCE), a loss inspired by the binary cross entropy (BCE) and the InfoNCE losses. In contrast to the InfoNCE loss, the BBCE loss does not require complicated sampling schemes. Our model can better handle larger images, and its output can be given a probabilistic interpretation. The probabilistic interpretation allows us to define metrics such as probability at offset and offscreen ratio to evaluate the sync quality of audio-visual (AV) <span class="search-hit mathjax">speech</span> datasets. Furthermore, our model achieves SOTA accuracy of $96.5\%$ on the LRS2 dataset and $93.8\%$ on the LRS3 dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.00971v1-abstract-full').style.display = 'none'; document.getElementById('2409.00971v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8+5 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.00946">arXiv:2409.00946</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.00946">pdf</a>, <a href="https://arxiv.org/format/2409.00946">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Framework for Synthetic Audio Conversations Generation using Large Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kyaw%2C+K+M">Kaung Myat Kyaw</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chan%2C+J+H">Jonathan Hoyin Chan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.00946v1-abstract-short" style="display: inline;">
        &hellip;with multiple persona settings. The framework first creates diverse and coherent text-based dialogues across various topics, which are then converted into audio using text-to-<span class="search-hit mathjax">speech</span> (TTS) systems. Our experiments demonstrate that ConversaSynth effectively generates highquality synthetic audio datasets, which can significantly enhance the training and evaluat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.00946v1-abstract-full').style.display = 'inline'; document.getElementById('2409.00946v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.00946v1-abstract-full" style="display: none;">
        In this paper, we introduce ConversaSynth, a framework designed to generate synthetic conversation audio using large language models (LLMs) with multiple persona settings. The framework first creates diverse and coherent text-based dialogues across various topics, which are then converted into audio using text-to-<span class="search-hit mathjax">speech</span> (TTS) systems. Our experiments demonstrate that ConversaSynth effectively generates highquality synthetic audio datasets, which can significantly enhance the training and evaluation of models for audio tagging, audio classification, and multi-speaker <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. The results indicate that the synthetic datasets generated by ConversaSynth exhibit substantial diversity and realism, making them suitable for developing robust, adaptable audio-based AI systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.00946v1-abstract-full').style.display = 'none'; document.getElementById('2409.00946v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This work has been submitted for consideration at the WI-IAT&#39;24 to be held in December 2024</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=250"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=200"
              class="pagination-link is-current"
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=250"
              class="pagination-link "
              aria-label="Page 6"
              aria-current="page">6
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>