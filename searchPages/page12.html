<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 551&ndash;600 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=550">order: -announced_date_first; size: 50; page_start: 550; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=550">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=500"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=600"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=500"
              class="pagination-link "
              aria-label="Page 11"
              aria-current="page">11
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=550"
              class="pagination-link is-current"
              aria-label="Page 12"
              aria-current="page">12
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=600"
              class="pagination-link "
              aria-label="Page 13"
              aria-current="page">13
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="551"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.15265">arXiv:2406.15265</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.15265">pdf</a>, <a href="https://arxiv.org/format/2406.15265">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Perception of Phonological Assimilation by Neural <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pouw%2C+C">Charlotte Pouw</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kloots%2C+M+d+H">Marianne de Heer Kloots</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alishahi%2C+A">Afra Alishahi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zuidema%2C+W">Willem Zuidema</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.15265v1-abstract-short" style="display: inline;">
        Human listeners effortlessly compensate for phonological changes during <span class="search-hit mathjax">speech</span> perception, often unconsciously inferring the intended sounds. For example, listeners infer the underlying /n/ when hearing an utterance such as &#34;clea[m] pan&#34;, where [m] arises from place assimilation to the following labial [p]. This article explores how the neural&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15265v1-abstract-full').style.display = 'inline'; document.getElementById('2406.15265v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.15265v1-abstract-full" style="display: none;">
        Human listeners effortlessly compensate for phonological changes during <span class="search-hit mathjax">speech</span> perception, often unconsciously inferring the intended sounds. For example, listeners infer the underlying /n/ when hearing an utterance such as &#34;clea[m] pan&#34;, where [m] arises from place assimilation to the following labial [p]. This article explores how the neural <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> model Wav2Vec2 perceives assimilated sounds, and identifies the linguistic knowledge that is implemented by the model to compensate for assimilation during Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR). Using psycholinguistic stimuli, we systematically analyze how various linguistic context cues influence compensation patterns in the model&#39;s output. Complementing these behavioral experiments, our probing experiments indicate that the model shifts its interpretation of assimilated sounds from their acoustic form to their underlying form in its final layers. Finally, our causal intervention experiments suggest that the model relies on minimal phonological context cues to accomplish this shift. These findings represent a step towards better understanding the similarities and differences in phonological processing between neural ASR models and humans.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15265v1-abstract-full').style.display = 'none'; document.getElementById('2406.15265v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication in Computational Linguistics (Special Issue on Language Learning, Representation, and Processing in Humans and Machines)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.15119">arXiv:2406.15119</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.15119">pdf</a>, <a href="https://arxiv.org/format/2406.15119">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> under Resource Constraints with Data Distillation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+Y">Yi Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Z">Zhao Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Z">Zhonghao Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+T+T">Thanh Tam Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+K">Kun Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schultz%2C+T">Tanja Schultz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schuller%2C+B+W">Björn W. Schuller</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.15119v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15119v1-abstract-full').style.display = 'inline'; document.getElementById('2406.15119v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.15119v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) plays a crucial role in human-computer interaction. The emergence of edge devices in the Internet of Things (IoT) presents challenges in constructing intricate deep learning models due to constraints in memory and computational resources. Moreover, emotional <span class="search-hit mathjax">speech</span> data often contains private information, raising concerns about privacy leakage during the deployment of SER models. To address these challenges, we propose a data distillation framework to facilitate efficient development of SER models in IoT applications using a synthesised, smaller, and distilled dataset. Our experiments demonstrate that the distilled dataset can be effectively utilised to train SER models with fixed initialisation, achieving performances comparable to those developed using the original full emotional <span class="search-hit mathjax">speech</span> dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15119v1-abstract-full').style.display = 'none'; document.getElementById('2406.15119v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.15111">arXiv:2406.15111</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.15111">pdf</a>, <a href="https://arxiv.org/format/2406.15111">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Investigating the impact of 2D gesture representation on co-<span class="search-hit mathjax">speech</span> gesture generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Guichoux%2C+T">Teo Guichoux</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soulier%2C+L">Laure Soulier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Obin%2C+N">Nicolas Obin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pelachaud%2C+C">Catherine Pelachaud</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.15111v2-abstract-short" style="display: inline;">
        Co-<span class="search-hit mathjax">speech</span> gestures play a crucial role in the interactions between humans and embodied conversational agents (ECA). Recent deep learning methods enable the generation of realistic, natural co-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15111v2-abstract-full').style.display = 'inline'; document.getElementById('2406.15111v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.15111v2-abstract-full" style="display: none;">
        Co-<span class="search-hit mathjax">speech</span> gestures play a crucial role in the interactions between humans and embodied conversational agents (ECA). Recent deep learning methods enable the generation of realistic, natural co-<span class="search-hit mathjax">speech</span> gestures synchronized with <span class="search-hit mathjax">speech</span>, but such approaches require large amounts of training data. &#34;In-the-wild&#34; datasets, which compile videos from sources such as YouTube through human pose detection models, offer a solution by providing 2D skeleton sequences that are paired with <span class="search-hit mathjax">speech</span>. Concurrently, innovative lifting models have emerged, capable of transforming these 2D pose sequences into their 3D counterparts, leading to large and diverse datasets of 3D gestures. However, the derived 3D pose estimation is essentially a pseudo-ground truth, with the actual ground truth being the 2D motion data. This distinction raises questions about the impact of gesture representation dimensionality on the quality of generated motions, a topic that, to our knowledge, remains largely unexplored. In this work, we evaluate the impact of the dimensionality of the training data, 2D or 3D joint coordinates, on the performance of a multimodal <span class="search-hit mathjax">speech</span>-to-gesture deep generative model. We use a lifting model to convert 2D-generated sequences of body pose to 3D. Then, we compare the sequence of gestures generated directly in 3D to the gestures generated in 2D and lifted to 3D as post-processing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15111v2-abstract-full').style.display = 'none'; document.getElementById('2406.15111v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages. Paper accepted at WACAI 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.14890">arXiv:2406.14890</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.14890">pdf</a>, <a href="https://arxiv.org/format/2406.14890">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        InterBiasing: Boost Unseen Word <span class="search-hit mathjax">Recognition</span> through Biasing Intermediate Predictions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nakagome%2C+Y">Yu Nakagome</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hentschel%2C+M">Michael Hentschel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.14890v1-abstract-short" style="display: inline;">
        Despite recent advances in end-to-end <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14890v1-abstract-full').style.display = 'inline'; document.getElementById('2406.14890v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.14890v1-abstract-full" style="display: none;">
        Despite recent advances in end-to-end <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> methods, their output is biased to the training data&#39;s vocabulary, resulting in inaccurate <span class="search-hit mathjax">recognition</span> of unknown terms or proper nouns. To improve the <span class="search-hit mathjax">recognition</span> accuracy for a given set of such terms, we propose an adaptation parameter-free approach based on Self-conditioned CTC. Our method improves the <span class="search-hit mathjax">recognition</span> accuracy of misrecognized target keywords by substituting their intermediate CTC predictions with corrected labels, which are then passed on to the subsequent layers. First, we create pairs of correct labels and <span class="search-hit mathjax">recognition</span> error instances for a keyword list using Text-to-<span class="search-hit mathjax">Speech</span> and a <span class="search-hit mathjax">recognition</span> model. We use these pairs to replace intermediate prediction errors by the labels. Conditioning the subsequent layers of the encoder on the labels, it is possible to acoustically evaluate the target keywords. Experiments conducted in Japanese demonstrated that our method successfully improved the F1 score for unknown words.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14890v1-abstract-full').style.display = 'none'; document.getElementById('2406.14890v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.14856">arXiv:2406.14856</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.14856">pdf</a>, <a href="https://arxiv.org/format/2406.14856">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Accessible, At-Home Detection of Parkinson&#39;s Disease via Multi-task Video Analysis
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Islam%2C+M+S">Md Saiful Islam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adnan%2C+T">Tariq Adnan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Freyberg%2C+J">Jan Freyberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+S">Sangwu Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abdelkader%2C+A">Abdelrahman Abdelkader</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pawlik%2C+M">Meghan Pawlik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schwartz%2C+C">Cathe Schwartz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jaffe%2C+K">Karen Jaffe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schneider%2C+R+B">Ruth B. Schneider</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dorsey%2C+E+R">E Ray Dorsey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoque%2C+E">Ehsan Hoque</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.14856v1-abstract-short" style="display: inline;">
        &hellip;architecture to detect Parkinson&#39;s disease (PD) by analyzing features extracted from webcam recordings of three tasks: finger tapping, facial expression (smiling), and <span class="search-hit mathjax">speech</span> (uttering a sentence containing all letters of the alphabet). Additionally, the model incorporated Monte Carlo Dropout to improve prediction accuracy by considering uncertainties. T&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14856v1-abstract-full').style.display = 'inline'; document.getElementById('2406.14856v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.14856v1-abstract-full" style="display: none;">
        Limited access to neurological care leads to missed diagnoses of Parkinson&#39;s disease (PD), leaving many individuals unidentified and untreated. We trained a novel neural network-based fusion architecture to detect Parkinson&#39;s disease (PD) by analyzing features extracted from webcam recordings of three tasks: finger tapping, facial expression (smiling), and <span class="search-hit mathjax">speech</span> (uttering a sentence containing all letters of the alphabet). Additionally, the model incorporated Monte Carlo Dropout to improve prediction accuracy by considering uncertainties. The study participants (n = 845, 272 with PD) were randomly split into three sets: 60% for training, 20% for model selection (hyper-parameter tuning), and 20% for final performance evaluation. The dataset consists of 1102 sessions, each session containing videos of all three tasks. Our proposed model achieved significantly better accuracy, area under the ROC curve (AUROC), and sensitivity at non-inferior specificity compared to any single-task model. Withholding uncertain predictions further boosted the performance, achieving 88.0% (95% CI: 87.7% - 88.4%) accuracy, 93.0% (92.8% - 93.2%) AUROC, 79.3% (78.4% - 80.2%) sensitivity, and 92.6% (92.3% - 92.8%) specificity, at the expense of not being able to predict for 2.3% (2.0% - 2.6%) data. Further analysis suggests that the trained model does not exhibit any detectable bias across sex and ethnic subgroups and is most effective for individuals aged between 50 and 80. This accessible, low-cost approach requiring only an internet-enabled device with a webcam and microphone paves the way for convenient PD screening at home, particularly in regions with limited access to clinical specialists.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14856v1-abstract-full').style.display = 'none'; document.getElementById('2406.14856v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.14747">arXiv:2406.14747</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.14747">pdf</a>, <a href="https://arxiv.org/format/2406.14747">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ICASSP48485.2024.10448240">10.1109/ICASSP48485.2024.10448240 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Suresh%2C+V">Varsha Suresh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=A%C3%AFt-Mokhtar%2C+S">Salah Aït-Mokhtar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brun%2C+C">Caroline Brun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Calapodescu%2C+I">Ioan Calapodescu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.14747v1-abstract-short" style="display: inline;">
        Self-supervised learning models have revolutionized the field of <span class="search-hit mathjax">speech</span> processing. However, the process of fine-tuning these models on downstream tasks requires substantial computational resources, particularly when dealing with multiple&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14747v1-abstract-full').style.display = 'inline'; document.getElementById('2406.14747v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.14747v1-abstract-full" style="display: none;">
        Self-supervised learning models have revolutionized the field of <span class="search-hit mathjax">speech</span> processing. However, the process of fine-tuning these models on downstream tasks requires substantial computational resources, particularly when dealing with multiple <span class="search-hit mathjax">speech</span>-processing tasks. In this paper, we explore the potential of adapter-based fine-tuning in developing a unified model capable of effectively handling multiple spoken language processing tasks. The tasks we investigate are Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>, Phoneme <span class="search-hit mathjax">Recognition</span>, Intent Classification, Slot Filling, and Spoken Emotion <span class="search-hit mathjax">Recognition</span>. We validate our approach through a series of experiments on the SUPERB benchmark, and our results indicate that adapter-based fine-tuning enables a single encoder-decoder model to perform multiple <span class="search-hit mathjax">speech</span> processing tasks with an average improvement of 18.4% across the five target tasks while staying efficient in terms of parameter updates.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14747v1-abstract-full').style.display = 'none'; document.getElementById('2406.14747v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICASSP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.14701">arXiv:2406.14701</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.14701">pdf</a>, <a href="https://arxiv.org/format/2406.14701">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Speech</span> Prefix-Tuning with RNNT Loss for Improving LLM Predictions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Baskar%2C+M+K">Murali Karthick Baskar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosenberg%2C+A">Andrew Rosenberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramabhadran%2C+B">Bhuvana Ramabhadran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gaur%2C+N">Neeraj Gaur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+Z">Zhong Meng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.14701v1-abstract-short" style="display: inline;">
        In this paper, we focus on addressing the constraints faced when applying LLMs to ASR. Recent works utilize prefixLM-type models, which directly apply <span class="search-hit mathjax">speech</span> as a prefix to LLMs for ASR. We have found that optimizing&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14701v1-abstract-full').style.display = 'inline'; document.getElementById('2406.14701v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.14701v1-abstract-full" style="display: none;">
        In this paper, we focus on addressing the constraints faced when applying LLMs to ASR. Recent works utilize prefixLM-type models, which directly apply <span class="search-hit mathjax">speech</span> as a prefix to LLMs for ASR. We have found that optimizing <span class="search-hit mathjax">speech</span> prefixes leads to better ASR performance and propose applying RNNT loss to perform <span class="search-hit mathjax">speech</span> prefix-tuning. This is a simple approach and does not increase the model complexity or alter the inference pipeline. We also propose language-based soft prompting to further improve with frozen LLMs. Empirical analysis on realtime testset from 10 Indic languages demonstrate that our proposed <span class="search-hit mathjax">speech</span> prefix-tuning yields improvements with both frozen and fine-tuned LLMs. Our <span class="search-hit mathjax">recognition</span> results on an average of 10 Indics show that the proposed prefix-tuning with RNNT loss results in a 12\% relative improvement in WER over the baseline with a fine-tuned LLM. Our proposed approches with the frozen LLM leads to a 31\% relative improvement over basic soft-prompting prefixLM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14701v1-abstract-full').style.display = 'none'; document.getElementById('2406.14701v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.14559">arXiv:2406.14559</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.14559">pdf</a>, <a href="https://arxiv.org/format/2406.14559">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Disentangled Representation Learning for Environment-agnostic Speaker <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nam%2C+K">KiHyun Nam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Heo%2C+H">Hee-Soo Heo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jung%2C+J">Jee-weon Jung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chung%2C+J+S">Joon Son Chung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.14559v1-abstract-short" style="display: inline;">
        This work presents a framework based on feature disentanglement to learn speaker embeddings that are robust to environmental variations. Our framework utilises an auto-encoder as a disentangler, dividing the input speaker embedding into components related to the speaker and other residual information. We employ a group of objective functions to ensure that the auto-encoder&#39;s code representation -&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14559v1-abstract-full').style.display = 'inline'; document.getElementById('2406.14559v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.14559v1-abstract-full" style="display: none;">
        This work presents a framework based on feature disentanglement to learn speaker embeddings that are robust to environmental variations. Our framework utilises an auto-encoder as a disentangler, dividing the input speaker embedding into components related to the speaker and other residual information. We employ a group of objective functions to ensure that the auto-encoder&#39;s code representation - used as the refined embedding - condenses only the speaker characteristics. We show the versatility of our framework through its compatibility with any existing speaker embedding extractor, requiring no structural modifications or adaptations for integration. We validate the effectiveness of our framework by incorporating it into two popularly used embedding extractors and conducting experiments across various benchmarks. The results show a performance improvement of up to 16%. We release our code for this work to be available https://github.com/kaistmm/voxceleb-disentangler
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14559v1-abstract-full').style.display = 'none'; document.getElementById('2406.14559v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech 2024. The official webpage can be found at https://mm.kaist.ac.kr/projects/voxceleb-disentangler/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.14294">arXiv:2406.14294</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.14294">pdf</a>, <a href="https://arxiv.org/format/2406.14294">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DASB - Discrete Audio and <span class="search-hit mathjax">Speech</span> Benchmark
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mousavi%2C+P">Pooneh Mousavi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Della+Libera%2C+L">Luca Della Libera</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duret%2C+J">Jarod Duret</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ploujnikov%2C+A">Artem Ploujnikov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Subakan%2C+C">Cem Subakan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ravanelli%2C+M">Mirco Ravanelli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.14294v2-abstract-short" style="display: inline;">
        &hellip;the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and <span class="search-hit mathjax">Speech</span> Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14294v2-abstract-full').style.display = 'inline'; document.getElementById('2406.14294v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.14294v2-abstract-full" style="display: none;">
        Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and <span class="search-hit mathjax">Speech</span> Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, speaker identification and verification, emotion <span class="search-hit mathjax">recognition</span>, keyword spotting, and intent classification, as well as generative tasks such as <span class="search-hit mathjax">speech</span> enhancement, separation, and text-to-<span class="search-hit mathjax">speech</span>. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14294v2-abstract-full').style.display = 'none'; document.getElementById('2406.14294v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 5 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.14272">arXiv:2406.14272</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.14272">pdf</a>, <a href="https://arxiv.org/format/2406.14272">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sung-Bin%2C+K">Kim Sung-Bin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chae-Yeon%2C+L">Lee Chae-Yeon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Son%2C+G">Gihun Son</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hyun-Bin%2C+O">Oh Hyun-Bin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ju%2C+J">Janghoon Ju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nam%2C+S">Suekyeong Nam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oh%2C+T">Tae-Hyun Oh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.14272v1-abstract-short" style="display: inline;">
        Recent studies in <span class="search-hit mathjax">speech</span>-driven 3D talking head generation have achieved convincing results in verbal articulations. However, generating accurate lip-syncs degrades when applied to input&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14272v1-abstract-full').style.display = 'inline'; document.getElementById('2406.14272v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.14272v1-abstract-full" style="display: none;">
        Recent studies in <span class="search-hit mathjax">speech</span>-driven 3D talking head generation have achieved convincing results in verbal articulations. However, generating accurate lip-syncs degrades when applied to input <span class="search-hit mathjax">speech</span> in other languages, possibly due to the lack of datasets covering a broad spectrum of facial movements across languages. In this work, we introduce a novel task to generate 3D talking heads from <span class="search-hit mathjax">speeches</span> of diverse languages. We collect a new multilingual 2D video dataset comprising over 420 hours of talking videos in 20 languages. With our proposed dataset, we present a multilingually enhanced model that incorporates language-specific style embeddings, enabling it to capture the unique mouth movements associated with each language. Additionally, we present a metric for assessing lip-sync accuracy in multilingual settings. We demonstrate that training a 3D talking head model with our proposed dataset significantly enhances its multilingual performance. Codes and datasets are available at https://multi-talk.github.io/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14272v1-abstract-full').style.display = 'none'; document.getElementById('2406.14272v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.14266">arXiv:2406.14266</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.14266">pdf</a>, <a href="https://arxiv.org/format/2406.14266">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wr%C3%B3blewska%2C+A">Anna Wróblewska</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Witas%2C+M">Marcel Witas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fra%C5%84czak%2C+K">Kinga Frańczak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Knia%C5%BA%2C+A">Arkadiusz Kniaź</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheong%2C+S+A">Siew Ann Cheong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chee%2C+T+S">Tan Seng Chee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ho%C5%82yst%2C+J">Janusz Hołyst</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paprzycki%2C+M">Marcin Paprzycki</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.14266v1-abstract-short" style="display: inline;">
        &hellip;didactic content. In the developed application text-based models trained on lecture transcriptions, with enhancements to the transcription quality, by adopting an automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> solution are applied. Furthermore, the system offers flexibility for (future) integration of new/additional machine-learning mod&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14266v1-abstract-full').style.display = 'inline'; document.getElementById('2406.14266v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.14266v1-abstract-full" style="display: none;">
        Recently, multiple applications of machine learning have been introduced. They include various possibilities arising when image analysis methods are applied to, broadly understood, video streams. In this context, a novel tool, developed for academic educators to enhance the teaching process by automating, summarizing, and offering prompt feedback on conducting lectures, has been developed. The implemented prototype utilizes machine learning-based techniques to recognise selected didactic and behavioural teachers&#39; features within lecture video recordings.
  Specifically, users (teachers) can upload their lecture videos, which are preprocessed and analysed using machine learning models. Next, users can view summaries of recognized didactic features through interactive charts and tables. Additionally, stored ML-based prediction results support comparisons between lectures based on their didactic content. In the developed application text-based models trained on lecture transcriptions, with enhancements to the transcription quality, by adopting an automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> solution are applied. Furthermore, the system offers flexibility for (future) integration of new/additional machine-learning models and software modules for image and video analysis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.14266v1-abstract-full').style.display = 'none'; document.getElementById('2406.14266v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.13842">arXiv:2406.13842</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.13842">pdf</a>, <a href="https://arxiv.org/format/2406.13842">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint vs Sequential Speaker-Role Detection and Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for Air-traffic Control
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Blatt%2C+A">Alexander Blatt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Krishnan%2C+A">Aravind Krishnan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Klakow%2C+D">Dietrich Klakow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.13842v1-abstract-short" style="display: inline;">
        &hellip;air-traffic control (ATC) data for downstream natural-language processing tasks requires preprocessing steps. Key steps are the transcription of the data via automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and speaker diarization, respectively speaker role detection (SRD) to divide the transcripts into pilot and air-traffic control&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13842v1-abstract-full').style.display = 'inline'; document.getElementById('2406.13842v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.13842v1-abstract-full" style="display: none;">
        Utilizing air-traffic control (ATC) data for downstream natural-language processing tasks requires preprocessing steps. Key steps are the transcription of the data via automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and speaker diarization, respectively speaker role detection (SRD) to divide the transcripts into pilot and air-traffic controller (ATCO) transcripts. While traditional approaches take on these tasks separately, we propose a transformer-based joint ASR-SRD system that solves both tasks jointly while relying on a standard ASR architecture. We compare this joint system against two cascaded approaches for ASR and SRD on multiple ATC datasets. Our study shows in which cases our joint system can outperform the two traditional approaches and in which cases the other architectures are preferable. We additionally evaluate how acoustic and lexical differences influence all architectures and show how to overcome them for our joint architecture.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13842v1-abstract-full').style.display = 'none'; document.getElementById('2406.13842v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.13809">arXiv:2406.13809</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.13809">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Holistic Language-video Representation: the language model-enhanced MSR-Video to Text Dataset
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yuchen Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+Y">Yingxuan Duan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.13809v1-abstract-short" style="display: inline;">
        &hellip;and context-aware for more sophisticated representation learning needs, hence helping all downstream tasks. Our multifaceted video captioning method captures entities, actions, <span class="search-hit mathjax">speech</span> transcripts, aesthetics, and emotional cues, providing detailed and correlating information from the text side to the video side for training. We also develop an agent-like str&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13809v1-abstract-full').style.display = 'inline'; document.getElementById('2406.13809v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.13809v1-abstract-full" style="display: none;">
        A more robust and holistic language-video representation is the key to pushing video understanding forward. Despite the improvement in training strategies, the quality of the language-video dataset is less attention to. The current plain and simple text descriptions and the visual-only focus for the language-video tasks result in a limited capacity in real-world natural language video retrieval tasks where queries are much more complex. This paper introduces a method to automatically enhance video-language datasets, making them more modality and context-aware for more sophisticated representation learning needs, hence helping all downstream tasks. Our multifaceted video captioning method captures entities, actions, <span class="search-hit mathjax">speech</span> transcripts, aesthetics, and emotional cues, providing detailed and correlating information from the text side to the video side for training. We also develop an agent-like strategy using language models to generate high-quality, factual textual descriptions, reducing human intervention and enabling scalability. The method&#39;s effectiveness in improving language-video representation is evaluated through text-video retrieval using the MSR-VTT dataset and several multi-modal retrieval models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13809v1-abstract-full').style.display = 'none'; document.getElementById('2406.13809v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.13579">arXiv:2406.13579</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.13579">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automated Bioacoustic Monitoring for South African Bird Species on Unlabeled Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Doell%2C+M">Michael Doell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kuehn%2C+D">Dominik Kuehn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Suessle%2C+V">Vanessa Suessle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Burnett%2C+M+J">Matthew J. Burnett</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Downs%2C+C+T">Colleen T. Downs</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Weinmann%2C+A">Andreas Weinmann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hergenroether%2C+E">Elke Hergenroether</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.13579v1-abstract-short" style="display: inline;">
        Analyses for biodiversity monitoring based on passive acoustic monitoring (PAM) recordings is time-consuming and challenged by the presence of background noise in recordings. Existing models for sound event detection (SED) worked only on certain avian species and the development of further models required labeled data. The developed framework automatically extracted labeled data from available pla&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13579v1-abstract-full').style.display = 'inline'; document.getElementById('2406.13579v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.13579v1-abstract-full" style="display: none;">
        Analyses for biodiversity monitoring based on passive acoustic monitoring (PAM) recordings is time-consuming and challenged by the presence of background noise in recordings. Existing models for sound event detection (SED) worked only on certain avian species and the development of further models required labeled data. The developed framework automatically extracted labeled data from available platforms for selected avian species. The labeled data were embedded into recordings, including environmental sounds and noise, and were used to train convolutional recurrent neural network (CRNN) models. The models were evaluated on unprocessed real world data recorded in urban KwaZulu-Natal habitats. The Adapted SED-CRNN model reached a F1 score of 0.73, demonstrating its efficiency under noisy, real-world conditions. The proposed approach to automatically extract labeled data for chosen avian species enables an easy adaption of PAM to other species and habitats for future conservation projects.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13579v1-abstract-full').style.display = 'none'; document.getElementById('2406.13579v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">preprint</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        International Conferences in Central Europe on Computer Graphics, Visualization and Computer Vision 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.13502">arXiv:2406.13502</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.13502">pdf</a>, <a href="https://arxiv.org/format/2406.13502">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ManWav: The First Manchu ASR Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Seo%2C+J">Jean Seo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kang%2C+M">Minha Kang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Byun%2C+S">Sungjoo Byun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+S">Sangah Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.13502v1-abstract-short" style="display: inline;">
        This study addresses the widening gap in Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) research between high resource and extremely low resource languages, with a particular focus on Manchu, a critically endangered language. Manchu exemplifies the challenges faced by marginalized linguistic communities in accessing state-of-the-a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13502v1-abstract-full').style.display = 'inline'; document.getElementById('2406.13502v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.13502v1-abstract-full" style="display: none;">
        This study addresses the widening gap in Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) research between high resource and extremely low resource languages, with a particular focus on Manchu, a critically endangered language. Manchu exemplifies the challenges faced by marginalized linguistic communities in accessing state-of-the-art technologies. In a pioneering effort, we introduce the first-ever Manchu ASR model ManWav, leveraging Wav2Vec2-XLSR-53. The results of the first Manchu ASR is promising, especially when trained with our augmented data. Wav2Vec2-XLSR-53 fine-tuned with augmented data demonstrates a 0.02 drop in CER and 0.13 drop in WER compared to the same base model fine-tuned with original data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13502v1-abstract-full').style.display = 'none'; document.getElementById('2406.13502v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL2024/Field Matters</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.13431">arXiv:2406.13431</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.13431">pdf</a>, <a href="https://arxiv.org/format/2406.13431">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Children&#39;s <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> through Discrete Token Enhancement
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sukhadia%2C+V+N">Vrunda N. Sukhadia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chowdhury%2C+S+A">Shammur Absar Chowdhury</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.13431v2-abstract-short" style="display: inline;">
        Children&#39;s <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13431v2-abstract-full').style.display = 'inline'; document.getElementById('2406.13431v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.13431v2-abstract-full" style="display: none;">
        Children&#39;s <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming <span class="search-hit mathjax">speech</span> signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete <span class="search-hit mathjax">speech</span> tokens into children&#39;s <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13431v2-abstract-full').style.display = 'none'; document.getElementById('2406.13431v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.13384">arXiv:2406.13384</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.13384">pdf</a>, <a href="https://arxiv.org/format/2406.13384">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Straight Through Gumbel Softmax Estimator based Bimodal Neural Architecture Search for Audio-Visual Deepfake Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=PN%2C+A+R">Aravinda Reddy PN</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramachandra%2C+R">Raghavendra Ramachandra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rao%2C+K+S">Krothapalli Sreenivasa Rao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mitra%2C+P">Pabitra Mitra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rathod%2C+V">Vinod Rathod</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.13384v1-abstract-short" style="display: inline;">
        Deepfakes are a major security risk for biometric authentication. This technology creates realistic fake videos that can impersonate real people, fooling systems that rely on facial features and voice patterns for identification. Existing multimodal deepfake detectors rely on conventional fusion methods, such as majority rule and ensemble voting, which often struggle to adapt to changing data char&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13384v1-abstract-full').style.display = 'inline'; document.getElementById('2406.13384v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.13384v1-abstract-full" style="display: none;">
        Deepfakes are a major security risk for biometric authentication. This technology creates realistic fake videos that can impersonate real people, fooling systems that rely on facial features and voice patterns for identification. Existing multimodal deepfake detectors rely on conventional fusion methods, such as majority rule and ensemble voting, which often struggle to adapt to changing data characteristics and complex patterns. In this paper, we introduce the Straight-through Gumbel-Softmax (STGS) framework, offering a comprehensive approach to search multimodal fusion model architectures. Using a two-level search approach, the framework optimizes the network architecture, parameters, and performance. Initially, crucial features were efficiently identified from backbone networks, whereas within the cell structure, a weighted fusion operation integrated information from various sources. An architecture that maximizes the classification performance is derived by varying parameters such as temperature and sampling time. The experimental results on the FakeAVCeleb and SWAN-DF datasets demonstrated an impressive AUC value 94.4\% achieved with minimal model parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13384v1-abstract-full').style.display = 'none'; document.getElementById('2406.13384v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.13337">arXiv:2406.13337</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.13337">pdf</a>, <a href="https://arxiv.org/format/2406.13337">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Medical Spoken Named Entity <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Le-Duc%2C+K">Khai Le-Duc</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Thulke%2C+D">David Thulke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tran%2C+H">Hung-Phong Tran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vo-Dang%2C+L">Long Vo-Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+K">Khai-Nguyen Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hy%2C+T">Truong-Son Hy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schl%C3%BCter%2C+R">Ralf Schlüter</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.13337v2-abstract-short" style="display: inline;">
        Spoken Named Entity <span class="search-hit mathjax">Recognition</span> (NER) aims to extracting named entities from <span class="search-hit mathjax">speech</span> and categorizing them into types like person, location, organization, etc. In this work, we present VietMed-NER - the first spoken NER dataset in the medical domain. To our best knowledge, our real-world dataset is the largest spoken NE&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13337v2-abstract-full').style.display = 'inline'; document.getElementById('2406.13337v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.13337v2-abstract-full" style="display: none;">
        Spoken Named Entity <span class="search-hit mathjax">Recognition</span> (NER) aims to extracting named entities from <span class="search-hit mathjax">speech</span> and categorizing them into types like person, location, organization, etc. In this work, we present VietMed-NER - the first spoken NER dataset in the medical domain. To our best knowledge, our real-world dataset is the largest spoken NER dataset in the world in terms of the number of entity types, featuring 18 distinct types. Secondly, we present baseline results using various state-of-the-art pre-trained models: encoder-only and sequence-to-sequence. We found that pre-trained multilingual models XLM-R outperformed all monolingual models on both reference text and ASR output. Also in general, encoders perform better than sequence-to-sequence models for the NER task. By simply translating, the transcript is applicable not just to Vietnamese but to other languages as well. All code, data and models are made publicly available here: https://github.com/leduckhai/MultiMed
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13337v2-abstract-full').style.display = 'none'; document.getElementById('2406.13337v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint, 41 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.13268">arXiv:2406.13268</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.13268">pdf</a>, <a href="https://arxiv.org/format/2406.13268">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CEC: A Noisy Label Detection Method for Speaker <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+Y">Yao Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Y">Yingying Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+Y">Yaqian Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+C">Chenguang Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+F">Fulin Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+J">Junlan Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shilei Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.13268v1-abstract-short" style="display: inline;">
        Noisy labels are inevitable, even in well-annotated datasets. The detection of noisy labels is of significant importance to enhance the robustness of speaker <span class="search-hit mathjax">recognition</span> models. In this paper, we propose a novel noisy label detection approach based on two new statistical metrics: Continuous Inconsistent Counting (CIC) and Total Inconsistent Counting (TIC). T&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13268v1-abstract-full').style.display = 'inline'; document.getElementById('2406.13268v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.13268v1-abstract-full" style="display: none;">
        Noisy labels are inevitable, even in well-annotated datasets. The detection of noisy labels is of significant importance to enhance the robustness of speaker <span class="search-hit mathjax">recognition</span> models. In this paper, we propose a novel noisy label detection approach based on two new statistical metrics: Continuous Inconsistent Counting (CIC) and Total Inconsistent Counting (TIC). These metrics are calculated through Cross-Epoch Counting (CEC) and correspond to the early and late stages of training, respectively. Additionally, we categorize samples based on their prediction results into three categories: inconsistent samples, hard samples, and easy samples. During training, we gradually increase the difficulty of hard samples to update model parameters, preventing noisy labels from being overfitted. Compared to contrastive schemes, our approach not only achieves the best performance in speaker verification but also excels in noisy label detection.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.13268v1-abstract-full').style.display = 'none'; document.getElementById('2406.13268v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.12937">arXiv:2406.12937</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.12937">pdf</a>, <a href="https://arxiv.org/format/2406.12937">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Train Before You Transcribe
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Flynn%2C+R">Robert Flynn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ragni%2C+A">Anton Ragni</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.12937v1-abstract-short" style="display: inline;">
        When there is a mismatch between the training and test domains, current <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems show significant performance degradation. Self-training methods, such as noisy student teacher training, can help address this and enable the adaptation of models under such domain shifts. However, self-training typically&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12937v1-abstract-full').style.display = 'inline'; document.getElementById('2406.12937v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.12937v1-abstract-full" style="display: none;">
        When there is a mismatch between the training and test domains, current <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems show significant performance degradation. Self-training methods, such as noisy student teacher training, can help address this and enable the adaptation of models under such domain shifts. However, self-training typically requires a collection of unlabelled target domain data. For settings where this is not practical, we investigate the benefit of performing noisy student teacher training on recordings in the test set as a test-time adaptation approach. Similarly to the dynamic evaluation approach in language modelling, this enables the transfer of information across utterance boundaries and functions as a method of domain adaptation. A range of in-domain and out-of-domain datasets are used for experiments demonstrating large relative gains of up to 32.2%. Interestingly, our method showed larger gains than the typical self-training setup that utilises separate adaptation data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12937v1-abstract-full').style.display = 'none'; document.getElementById('2406.12937v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.12931">arXiv:2406.12931</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.12931">pdf</a>, <a href="https://arxiv.org/format/2406.12931">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> for Biomedical Data in Bengali Language
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kabir%2C+S">Shariar Kabir</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nahar%2C+N">Nazmun Nahar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saha%2C+S">Shyamasree Saha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rashid%2C+M">Mamunur Rashid</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.12931v1-abstract-short" style="display: inline;">
        This paper presents the development of a prototype Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) system specifically designed for Bengali biomedical data. Recent advancements in Bengali ASR are encouraging, but a lack of domain-specific data limits the creation of practical healthcare ASR models. This project bridges this gap by&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12931v1-abstract-full').style.display = 'inline'; document.getElementById('2406.12931v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.12931v1-abstract-full" style="display: none;">
        This paper presents the development of a prototype Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) system specifically designed for Bengali biomedical data. Recent advancements in Bengali ASR are encouraging, but a lack of domain-specific data limits the creation of practical healthcare ASR models. This project bridges this gap by developing an ASR system tailored for Bengali medical terms like symptoms, severity levels, and diseases, encompassing two major dialects: Bengali and Sylheti. We train and evaluate two popular ASR frameworks on a comprehensive 46-hour Bengali medical corpus. Our core objective is to create deployable health-domain ASR systems for digital health applications, ultimately increasing accessibility for non-technical users in the healthcare sector.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12931v1-abstract-full').style.display = 'none'; document.getElementById('2406.12931v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.12699">arXiv:2406.12699</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.12699">pdf</a>, <a href="https://arxiv.org/format/2406.12699">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bridging the Gap: Integrating Pre-trained <span class="search-hit mathjax">Speech</span> Enhancement and <span class="search-hit mathjax">Recognition</span> Models for Robust <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+K">Kuan-Chen Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">You-Jin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Wei-Lun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yu-Wen Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yi-Ching Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yeh%2C+P">Ping-Cheng Yeh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsao%2C+Y">Yu Tsao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.12699v1-abstract-short" style="display: inline;">
        Noise robustness is critical when applying automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12699v1-abstract-full').style.display = 'inline'; document.getElementById('2406.12699v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.12699v1-abstract-full" style="display: none;">
        Noise robustness is critical when applying automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) in real-world scenarios. One solution involves the used of <span class="search-hit mathjax">speech</span> enhancement (SE) models as the front end of ASR. However, neural network-based (NN-based) SE often introduces artifacts into the enhanced signals and harms ASR performance, particularly when SE and ASR are independently trained. Therefore, this study introduces a simple yet effective SE post-processing technique to address the gap between various pre-trained SE and ASR models. A bridge module, which is a lightweight NN, is proposed to evaluate the signal-level information of the <span class="search-hit mathjax">speech</span> signal. Subsequently, using the signal-level information, the observation addition technique is applied to effectively reduce the shortcomings of SE. The experimental results demonstrate the success of our method in integrating diverse pre-trained SE and ASR models, considerably boosting the ASR robustness. Crucially, no prior knowledge of the ASR or <span class="search-hit mathjax">speech</span> contents is required during the training or inference stages. Moreover, the effectiveness of this approach extends to different datasets without necessitating the fine-tuning of the bridge module, ensuring efficiency and improved generalization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12699v1-abstract-full').style.display = 'none'; document.getElementById('2406.12699v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.12621">arXiv:2406.12621</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.12621">pdf</a>, <a href="https://arxiv.org/format/2406.12621">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Growing Trees on Sounds: Assessing Strategies for End-to-End Dependency Parsing of <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pupier%2C+A">Adrien Pupier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Coavoux%2C+M">Maximin Coavoux</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goulian%2C+J">Jérôme Goulian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lecouteux%2C+B">Benjamin Lecouteux</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.12621v1-abstract-short" style="display: inline;">
        Direct dependency parsing of the <span class="search-hit mathjax">speech</span> signal -- as opposed to parsing&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12621v1-abstract-full').style.display = 'inline'; document.getElementById('2406.12621v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.12621v1-abstract-full" style="display: none;">
        Direct dependency parsing of the <span class="search-hit mathjax">speech</span> signal -- as opposed to parsing <span class="search-hit mathjax">speech</span> transcriptions -- has recently been proposed as a task (Pupier et al. 2022), as a way of incorporating prosodic information in the parsing system and bypassing the limitations of a pipeline approach that would consist of using first an Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) system and then a syntactic parser. In this article, we report on a set of experiments aiming at assessing the performance of two parsing paradigms (graph-based parsing and sequence labeling based parsing) on <span class="search-hit mathjax">speech</span> parsing. We perform this evaluation on a large treebank of spoken French, featuring realistic spontaneous conversations. Our findings show that (i) the graph based approach obtain better results across the board (ii) parsing directly from <span class="search-hit mathjax">speech</span> outperforms a pipeline approach, despite having 30% fewer parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12621v1-abstract-full').style.display = 'none'; document.getElementById('2406.12621v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ACL 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.12611">arXiv:2406.12611</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.12611">pdf</a>, <a href="https://arxiv.org/format/2406.12611">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Rapid Language Adaptation for Multilingual E2E <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Using Encoder Prompting
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kashiwagi%2C+Y">Yosuke Kashiwagi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Futami%2C+H">Hayato Futami</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsunoo%2C+E">Emiru Tsunoo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arora%2C+S">Siddhant Arora</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.12611v1-abstract-short" style="display: inline;">
        End-to-end multilingual <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12611v1-abstract-full').style.display = 'inline'; document.getElementById('2406.12611v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.12611v1-abstract-full" style="display: none;">
        End-to-end multilingual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models handle multiple languages through a single model, often incorporating language identification to automatically detect the language of incoming <span class="search-hit mathjax">speech</span>. Since the common scenario is where the language is already known, these models can perform as language-specific by using language information as prompts, which is particularly beneficial for attention-based encoder-decoder architectures. However, the Connectionist Temporal Classification (CTC) approach, which enhances <span class="search-hit mathjax">recognition</span> via joint decoding and multi-task training, does not normally incorporate language prompts due to its conditionally independent output tokens. To overcome this, we introduce an encoder prompting technique within the self-conditioned CTC framework, enabling language-specific adaptation of the CTC model in a zero-shot manner. Our method has shown to significantly reduce errors by 28% on average and by 41% on low-resource languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12611v1-abstract-full').style.display = 'none'; document.getElementById('2406.12611v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.12544">arXiv:2406.12544</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.12544">pdf</a>, <a href="https://arxiv.org/format/2406.12544">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Integrating Representational Gestures into Automatically Generated Embodied Explanations and its Effects on Understanding and Interaction Quality
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Robrecht%2C+A+S">Amelie Sophie Robrecht</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Voss%2C+H">Hendric Voss</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gottschalk%2C+L">Lisa Gottschalk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kopp%2C+S">Stefan Kopp</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.12544v2-abstract-short" style="display: inline;">
        In human interaction, gestures serve various functions such as marking <span class="search-hit mathjax">speech</span> rhythm, highlighting key elements, and supplementing information. These gestures are also observed in explanatory contexts. However, the impact of gestures on explanations provided by virtual agents remains underexplored. A user study was carried out to investigate how different ty&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12544v2-abstract-full').style.display = 'inline'; document.getElementById('2406.12544v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.12544v2-abstract-full" style="display: none;">
        In human interaction, gestures serve various functions such as marking <span class="search-hit mathjax">speech</span> rhythm, highlighting key elements, and supplementing information. These gestures are also observed in explanatory contexts. However, the impact of gestures on explanations provided by virtual agents remains underexplored. A user study was carried out to investigate how different types of gestures influence perceived interaction quality and listener understanding. This study addresses the effect of gestures in explanation by developing an embodied virtual explainer integrating both beat gestures and iconic gestures to enhance its automatically generated verbal explanations. Our model combines beat gestures generated by a learned <span class="search-hit mathjax">speech</span>-driven synthesis module with manually captured iconic gestures, supporting the agent&#39;s verbal expressions about the board game Quarto! as an explanation scenario. Findings indicate that neither the use of iconic gestures alone nor their combination with beat gestures outperforms the baseline or beat-only conditions in terms of understanding. Nonetheless, compared to prior research, the embodied agent significantly enhances understanding.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12544v2-abstract-full').style.display = 'none'; document.getElementById('2406.12544v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.12387">arXiv:2406.12387</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.12387">pdf</a>, <a href="https://arxiv.org/format/2406.12387">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Performant ASR Models for Medical Entities in Accented <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Afonja%2C+T">Tejumade Afonja</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olatunji%2C+T">Tobi Olatunji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ogun%2C+S">Sewade Ogun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Etori%2C+N+A">Naome A. Etori</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Owodunni%2C+A">Abraham Owodunni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yekini%2C+M">Moshood Yekini</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.12387v1-abstract-short" style="display: inline;">
        Recent strides in automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12387v1-abstract-full').style.display = 'inline'; document.getElementById('2406.12387v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.12387v1-abstract-full" style="display: none;">
        Recent strides in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) have accelerated their application in the medical domain where their performance on accented medical named entities (NE) such as drug names, diagnoses, and lab results, is largely unknown. We rigorously evaluate multiple ASR models on a clinical English dataset of 93 African accents. Our analysis reveals that despite some models achieving low overall word error rates (WER), errors in clinical entities are higher, potentially posing substantial risks to patient safety. To empirically demonstrate this, we extract clinical entities from transcripts, develop a novel algorithm to align ASR predictions with these entities, and compute medical NE Recall, medical WER, and character error rate. Our results show that fine-tuning on accented clinical <span class="search-hit mathjax">speech</span> improves medical WER by a wide margin (25-34 % relative), improving their practical applicability in healthcare environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12387v1-abstract-full').style.display = 'none'; document.getElementById('2406.12387v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.12317">arXiv:2406.12317</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.12317">pdf</a>, <a href="https://arxiv.org/format/2406.12317">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Finding Task-specific Subnetworks in Multi-task Spoken Language Understanding Model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Futami%2C+H">Hayato Futami</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arora%2C+S">Siddhant Arora</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kashiwagi%2C+Y">Yosuke Kashiwagi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsunoo%2C+E">Emiru Tsunoo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.12317v1-abstract-short" style="display: inline;">
        Recently, multi-task spoken language understanding (SLU) models have emerged, designed to address various <span class="search-hit mathjax">speech</span> processing tasks. However, these models often rely on a large number of parameters. Also, they often encounter difficulties in adapting to new data for a specific task without experiencing catastrophic forgetting of previously trained tasks. In th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12317v1-abstract-full').style.display = 'inline'; document.getElementById('2406.12317v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.12317v1-abstract-full" style="display: none;">
        Recently, multi-task spoken language understanding (SLU) models have emerged, designed to address various <span class="search-hit mathjax">speech</span> processing tasks. However, these models often rely on a large number of parameters. Also, they often encounter difficulties in adapting to new data for a specific task without experiencing catastrophic forgetting of previously trained tasks. In this study, we propose finding task-specific subnetworks within a multi-task SLU model via neural network pruning. In addition to model compression, we expect that the forgetting of previously trained tasks can be mitigated by updating only a task-specific subnetwork. We conduct experiments on top of the state-of-the-art multi-task SLU model ``UniverSLU&#39;&#39;, trained for several tasks such as emotion <span class="search-hit mathjax">recognition</span> (ER), intent classification (IC), and automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). We show that pruned models were successful in adapting to additional ASR or IC data with minimal performance degradation on previously trained tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12317v1-abstract-full').style.display = 'none'; document.getElementById('2406.12317v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Interspeech2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.12233">arXiv:2406.12233</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.12233">pdf</a>, <a href="https://arxiv.org/format/2406.12233">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SyncVSR: Data-Efficient Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> with End-to-End Crossmodal Audio Token Synchronization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ahn%2C+Y+J">Young Jin Ahn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+J">Jungwoo Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+S">Sangha Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Choi%2C+J">Jonghyun Choi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+K">Kee-Eung Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.12233v1-abstract-short" style="display: inline;">
        Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (VSR) stands at the intersection of computer vision and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, aiming to interpret spoken content from visual cues. A prominent challenge in VSR is the presence of homophenes-visually similar lip gestu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12233v1-abstract-full').style.display = 'inline'; document.getElementById('2406.12233v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.12233v1-abstract-full" style="display: none;">
        Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (VSR) stands at the intersection of computer vision and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, aiming to interpret spoken content from visual cues. A prominent challenge in VSR is the presence of homophenes-visually similar lip gestures that represent different phonemes. Prior approaches have sought to distinguish fine-grained visemes by aligning visual and auditory semantics, but often fell short of full synchronization. To address this, we present SyncVSR, an end-to-end learning framework that leverages quantized audio for frame-level crossmodal supervision. By integrating a projection layer that synchronizes visual representation with acoustic data, our encoder learns to generate discrete audio tokens from a video sequence in a non-autoregressive manner. SyncVSR shows versatility across tasks, languages, and modalities at the cost of a forward pass. Our empirical evaluations show that it not only achieves state-of-the-art results but also reduces data usage by up to ninefold.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.12233v1-abstract-full').style.display = 'none'; document.getElementById('2406.12233v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.11546">arXiv:2406.11546</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.11546">pdf</a>, <a href="https://arxiv.org/format/2406.11546">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yifan Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Z">Zheshu Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhuo%2C+J">Jianheng Zhuo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+M">Mingyu Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jinpeng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+B">Bo Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+Y">Yexing Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Z">Ziyang Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xunying Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Ziyuan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+K">Ke Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+S">Shuai Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+K">Kai Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Wei-Qiang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+G">Guoguo Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xie Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.11546v1-abstract-short" style="display: inline;">
        The evolution of <span class="search-hit mathjax">speech</span> technology has been spurred by the rapid increase in dataset sizes. Traditional&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11546v1-abstract-full').style.display = 'inline'; document.getElementById('2406.11546v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.11546v1-abstract-full" style="display: none;">
        The evolution of <span class="search-hit mathjax">speech</span> technology has been spurred by the rapid increase in dataset sizes. Traditional <span class="search-hit mathjax">speech</span> models generally depend on a large amount of labeled training data, which is scarce for low-resource languages. This paper presents GigaSpeech 2, a large-scale, multi-domain, multilingual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> corpus. It is designed for low-resource languages and does not rely on paired <span class="search-hit mathjax">speech</span> and text data. GigaSpeech 2 comprises about 30,000 hours of automatically transcribed <span class="search-hit mathjax">speech</span>, including Thai, Indonesian, and Vietnamese, gathered from unlabeled YouTube videos. We also introduce an automated pipeline for data crawling, transcription, and label refinement. Specifically, this pipeline uses Whisper for initial transcription and TorchAudio for forced alignment, combined with multi-dimensional filtering for data quality assurance. A modified Noisy Student Training is developed to further refine flawed pseudo labels iteratively, thus enhancing model performance. Experimental results on our manually transcribed evaluation set and two public test sets from Common Voice and FLEURS confirm our corpus&#39;s high quality and broad applicability. Notably, ASR models trained on GigaSpeech 2 can reduce the word error rate for Thai, Indonesian, and Vietnamese on our challenging and realistic YouTube test set by 25% to 40% compared to the Whisper large-v3 model, with merely 10% model parameters. Furthermore, our ASR models trained on Gigaspeech 2 yield superior performance compared to commercial services. We believe that our newly introduced corpus and pipeline will open a new avenue for low-resource <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and significantly facilitate research in this area.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11546v1-abstract-full').style.display = 'none'; document.getElementById('2406.11546v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.11462">arXiv:2406.11462</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.11462">pdf</a>, <a href="https://arxiv.org/format/2406.11462">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MusicScore: A Dataset for Music Score Modeling and Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yuheng Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+Z">Zheqi Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+Q">Qiuqiang Kong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.11462v1-abstract-short" style="display: inline;">
        &hellip;more semantic information than audio and symbolic representations of music. Previous music score datasets have limited sizes and are mainly designed for optical music <span class="search-hit mathjax">recognition</span> (OMR). There is a lack of research on creating a large-scale benchmark dataset for music modeling and generation. In this work, we propose MusicScore, a large-scale music score data&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11462v1-abstract-full').style.display = 'inline'; document.getElementById('2406.11462v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.11462v1-abstract-full" style="display: none;">
        Music scores are written representations of music and contain rich information about musical components. The visual information on music scores includes notes, rests, staff lines, clefs, dynamics, and articulations. This visual information in music scores contains more semantic information than audio and symbolic representations of music. Previous music score datasets have limited sizes and are mainly designed for optical music <span class="search-hit mathjax">recognition</span> (OMR). There is a lack of research on creating a large-scale benchmark dataset for music modeling and generation. In this work, we propose MusicScore, a large-scale music score dataset collected and processed from the International Music Score Library Project (IMSLP). MusicScore consists of image-text pairs, where the image is a page of a music score and the text is the metadata of the music. The metadata of MusicScore is extracted from the general information section of the IMSLP pages. The metadata includes rich information about the composer, instrument, piece style, and genre of the music pieces. MusicScore is curated into small, medium, and large scales of 400, 14k, and 200k image-text pairs with varying diversity, respectively. We build a score generation system based on a UNet diffusion model to generate visually readable music scores conditioned on text descriptions to benchmark the MusicScore dataset for music score generation. MusicScore is released to the public at https://huggingface.co/datasets/ZheqiDAI/MusicScore.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11462v1-abstract-full').style.display = 'none'; document.getElementById('2406.11462v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Dataset paper, dataset link: https://huggingface.co/datasets/ZheqiDAI/MusicScore</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.11064">arXiv:2406.11064</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.11064">pdf</a>, <a href="https://arxiv.org/format/2406.11064">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Continual Test-time Adaptation for End-to-end <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> on Noisy <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+G">Guan-Ting Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+W">Wei-Ping Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-yi Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.11064v2-abstract-short" style="display: inline;">
        Deep Learning-based end-to-end Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) has made significant strides but still struggles with performance on out-of-domain samples due to domain shifts in real-world scenarios. Test-Time Adaptation (TTA) methods address this issue by adapting models using test samples at inference time. Howeve&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11064v2-abstract-full').style.display = 'inline'; document.getElementById('2406.11064v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.11064v2-abstract-full" style="display: none;">
        Deep Learning-based end-to-end Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (ASR) has made significant strides but still struggles with performance on out-of-domain samples due to domain shifts in real-world scenarios. Test-Time Adaptation (TTA) methods address this issue by adapting models using test samples at inference time. However, current ASR TTA methods have largely focused on non-continual TTA, which limits cross-sample knowledge learning compared to continual TTA. In this work, we first propose a Fast-slow TTA framework for ASR that leverages the advantage of continual and non-continual TTA. Following this framework, we introduce Dynamic SUTA (DSUTA), an entropy-minimization-based continual TTA method for ASR. To enhance DSUTA robustness for time-varying data, we design a dynamic reset strategy to automatically detect domain shifts and reset the model, making it more effective at handling multi-domain data. Our method demonstrates superior performance on various noisy ASR datasets, outperforming both non-continual and continual TTA baselines while maintaining robustness to domain changes without requiring domain boundary information.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11064v2-abstract-full').style.display = 'none'; document.getElementById('2406.11064v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by EMNLP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.11037">arXiv:2406.11037</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.11037">pdf</a>, <a href="https://arxiv.org/format/2406.11037">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NAST: Noise Aware <span class="search-hit mathjax">Speech</span> Tokenization for <span class="search-hit mathjax">Speech</span> Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Messica%2C+S">Shoval Messica</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adi%2C+Y">Yossi Adi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.11037v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> tokenization is the task of representing&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11037v1-abstract-full').style.display = 'inline'; document.getElementById('2406.11037v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.11037v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> tokenization is the task of representing <span class="search-hit mathjax">speech</span> signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, text-to-<span class="search-hit mathjax">speech</span>, etc. More relevant to this study, such representation serves as the basis of <span class="search-hit mathjax">Speech</span> Language Models. In this work, we tackle the task of <span class="search-hit mathjax">speech</span> tokenization under the noisy setup and present NAST: Noise Aware <span class="search-hit mathjax">Speech</span> Tokenization for <span class="search-hit mathjax">Speech</span> Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11037v1-abstract-full').style.display = 'none'; document.getElementById('2406.11037v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.11025">arXiv:2406.11025</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.11025">pdf</a>, <a href="https://arxiv.org/format/2406.11025">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Large Language Models for Dysfluency Detection in Stuttered <span class="search-hit mathjax">Speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wagner%2C+D">Dominik Wagner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bayerl%2C+S+P">Sebastian P. Bayerl</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baumann%2C+I">Ilja Baumann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Riedhammer%2C+K">Korbinian Riedhammer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=N%C3%B6th%2C+E">Elmar Nöth</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bocklet%2C+T">Tobias Bocklet</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.11025v1-abstract-short" style="display: inline;">
        Accurately detecting dysfluencies in spoken language can help to improve the performance of automatic <span class="search-hit mathjax">speech</span> and language processing components and support the development of more inclusive&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11025v1-abstract-full').style.display = 'inline'; document.getElementById('2406.11025v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.11025v1-abstract-full" style="display: none;">
        Accurately detecting dysfluencies in spoken language can help to improve the performance of automatic <span class="search-hit mathjax">speech</span> and language processing components and support the development of more inclusive <span class="search-hit mathjax">speech</span> and language technologies. Inspired by the recent trend towards the deployment of large language models (LLMs) as universal learners and processors of non-lexical inputs, such as audio and video, we approach the task of multi-label dysfluency detection as a language modeling problem. We present hypotheses candidates generated with an automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> system and acoustic representations extracted from an audio encoder model to an LLM, and finetune the system to predict dysfluency labels on three datasets containing English and German stuttered <span class="search-hit mathjax">speech</span>. The experimental results show that our system effectively combines acoustic and lexical information and achieves competitive results on the multi-label stuttering detection task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11025v1-abstract-full').style.display = 'none'; document.getElementById('2406.11025v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.11022">arXiv:2406.11022</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.11022">pdf</a>, <a href="https://arxiv.org/format/2406.11022">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Outlier Reduction with Gated Attention for Improved Post-training Quantization in Large Sequence-to-sequence <span class="search-hit mathjax">Speech</span> Foundation Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wagner%2C+D">Dominik Wagner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baumann%2C+I">Ilja Baumann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Riedhammer%2C+K">Korbinian Riedhammer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bocklet%2C+T">Tobias Bocklet</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.11022v1-abstract-short" style="display: inline;">
        This paper explores the improvement of post-training quantization (PTQ) after knowledge distillation in the Whisper <span class="search-hit mathjax">speech</span> foundation model family. We address the challenge of outliers in weights and activation tensors, known to impede quantization quality in transformer-based language and vision models. Extending this observation to Whisper, we demonstrate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11022v1-abstract-full').style.display = 'inline'; document.getElementById('2406.11022v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.11022v1-abstract-full" style="display: none;">
        This paper explores the improvement of post-training quantization (PTQ) after knowledge distillation in the Whisper <span class="search-hit mathjax">speech</span> foundation model family. We address the challenge of outliers in weights and activation tensors, known to impede quantization quality in transformer-based language and vision models. Extending this observation to Whisper, we demonstrate that these outliers are also present when transformer-based models are trained to perform automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, necessitating mitigation strategies for PTQ. We show that outliers can be reduced by a recently proposed gating mechanism in the attention blocks of the student model, enabling effective 8-bit quantization, and lower word error rates compared to student models without the gating mechanism in place.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11022v1-abstract-full').style.display = 'none'; document.getElementById('2406.11022v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.11016">arXiv:2406.11016</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.11016">pdf</a>, <a href="https://arxiv.org/format/2406.11016">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Optimized Speculative Sampling for GPU Hardware Accelerators
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wagner%2C+D">Dominik Wagner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+S">Seanie Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baumann%2C+I">Ilja Baumann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Seeberger%2C+P">Philipp Seeberger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Riedhammer%2C+K">Korbinian Riedhammer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bocklet%2C+T">Tobias Bocklet</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.11016v2-abstract-short" style="display: inline;">
        &hellip;in significantly greater relative improvements in profiling time, ranging from 37% to 94%, with a minor decline in accuracy. We conduct extensive experiments on both automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and summarization tasks to validate the effectiveness of our optimization methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11016v2-abstract-full').style.display = 'inline'; document.getElementById('2406.11016v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.11016v2-abstract-full" style="display: none;">
        In this work, we optimize speculative sampling for parallel hardware accelerators to improve sampling speed. We notice that substantial portions of the intermediate matrices necessary for speculative sampling can be computed concurrently. This allows us to distribute the workload across multiple GPU threads, enabling simultaneous operations on matrix segments within thread blocks. This results in profiling time improvements ranging from 6% to 13% relative to the baseline implementation, without compromising accuracy. To further accelerate speculative sampling, probability distributions parameterized by softmax are approximated by sigmoid. This approximation approach results in significantly greater relative improvements in profiling time, ranging from 37% to 94%, with a minor decline in accuracy. We conduct extensive experiments on both automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and summarization tasks to validate the effectiveness of our optimization methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11016v2-abstract-full').style.display = 'none'; document.getElementById('2406.11016v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at EMNLP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10993">arXiv:2406.10993</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10993">pdf</a>, <a href="https://arxiv.org/format/2406.10993">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CoSTA: Code-Switched <span class="search-hit mathjax">Speech</span> Translation using Aligned <span class="search-hit mathjax">Speech</span>-Text Interleaving
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shankar%2C+B">Bhavani Shankar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jyothi%2C+P">Preethi Jyothi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhattacharyya%2C+P">Pushpak Bhattacharyya</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10993v1-abstract-short" style="display: inline;">
        Code-switching is a widely prevalent linguistic phenomenon in multilingual societies like India. Building <span class="search-hit mathjax">speech</span>-to-text models for code-switched&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10993v1-abstract-full').style.display = 'inline'; document.getElementById('2406.10993v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10993v1-abstract-full" style="display: none;">
        Code-switching is a widely prevalent linguistic phenomenon in multilingual societies like India. Building <span class="search-hit mathjax">speech</span>-to-text models for code-switched <span class="search-hit mathjax">speech</span> is challenging due to limited availability of datasets. In this work, we focus on the problem of spoken translation (ST) of code-switched <span class="search-hit mathjax">speech</span> in Indian languages to English text. We present a new end-to-end model architecture COSTA that scaffolds on pretrained automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) and machine translation (MT) modules (that are more widely available for many languages). <span class="search-hit mathjax">Speech</span> and ASR text representations are fused using an aligned interleaving scheme and are fed further as input to a pretrained MT module; the whole pipeline is then trained end-to-end for spoken translation using synthetically created ST data. We also release a new evaluation benchmark for code-switched Bengali-English, Hindi-English, Marathi-English and Telugu- English <span class="search-hit mathjax">speech</span> to English text. COSTA significantly outperforms many competitive cascaded and end-to-end multimodal baselines by up to 3.5 BLEU points.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10993v1-abstract-full').style.display = 'none'; document.getElementById('2406.10993v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10956">arXiv:2406.10956</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10956">pdf</a>, <a href="https://arxiv.org/format/2406.10956">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Robust Channel Learning for Large-Scale Radio Speaker Verification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+W">Wenhao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+J">Jianguo Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+W">Wenhuan Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+X">Xugang Lu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10956v1-abstract-short" style="display: inline;">
        Recent research in speaker verification has increasingly focused on achieving robust and reliable <span class="search-hit mathjax">recognition</span> under challenging channel conditions and noisy environments. Identifying speakers in radio communications is particularly difficult due to inherent limitations such as constrained bandwidth and pervasive noise interference. To address this issue, we&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10956v1-abstract-full').style.display = 'inline'; document.getElementById('2406.10956v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10956v1-abstract-full" style="display: none;">
        Recent research in speaker verification has increasingly focused on achieving robust and reliable <span class="search-hit mathjax">recognition</span> under challenging channel conditions and noisy environments. Identifying speakers in radio communications is particularly difficult due to inherent limitations such as constrained bandwidth and pervasive noise interference. To address this issue, we present a Channel Robust Speaker Learning (CRSL) framework that enhances the robustness of the current speaker verification pipeline, considering data source, data augmentation, and the efficiency of model transfer processes. Our framework introduces an augmentation module that mitigates bandwidth variations in radio <span class="search-hit mathjax">speech</span> datasets by manipulating the bandwidth of training inputs. It also addresses unknown noise by introducing noise within the manifold space. Additionally, we propose an efficient fine-tuning method that reduces the need for extensive additional training time and large amounts of data. Moreover, we develop a toolkit for assembling a large-scale radio <span class="search-hit mathjax">speech</span> corpus and establish a benchmark specifically tailored for radio scenario speaker verification studies. Experimental results demonstrate that our proposed methodology effectively enhances performance and mitigates degradation caused by radio transmission in speaker verification tasks. The code will be available on Github.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10956v1-abstract-full').style.display = 'none'; document.getElementById('2406.10956v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 11 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10932">arXiv:2406.10932</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10932">pdf</a>, <a href="https://arxiv.org/format/2406.10932">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Imperceptible Rhythm Backdoor Attacks: Exploring Rhythm Transformation for Embedding Undetectable Vulnerabilities on <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+W">Wenhan Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">Jiangkun Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Y">Yongqiang He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jia Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+W">Weiping Wen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10932v2-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10932v2-abstract-full').style.display = 'inline'; document.getElementById('2406.10932v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10932v2-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">recognition</span> is an essential start ring of human-computer interaction, and recently, deep learning models have achieved excellent success in this task. However, when the model training and private data provider are always separated, some security threats that make deep neural networks (DNNs) abnormal deserve to be researched. In recent years, the typical backdoor attacks have been researched in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems. The existing backdoor methods are based on data poisoning. The attacker adds some incorporated changes to benign <span class="search-hit mathjax">speech</span> spectrograms or changes the <span class="search-hit mathjax">speech</span> components, such as pitch and timbre. As a result, the poisoned data can be detected by human hearing or automatic deep algorithms. To improve the stealthiness of data poisoning, we propose a non-neural and fast algorithm called Random Spectrogram Rhythm Transformation (RSRT) in this paper. The algorithm combines four steps to generate stealthy poisoned utterances. From the perspective of rhythm component transformation, our proposed trigger stretches or squeezes the mel spectrograms and recovers them back to signals. The operation keeps timbre and content unchanged for good stealthiness. Our experiments are conducted on two kinds of <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> tasks, including testing the stealthiness of poisoned samples by speaker verification and automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>. The results show that our method has excellent effectiveness and stealthiness. The rhythm trigger needs a low poisoning rate and gets a very high attack success rate.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10932v2-abstract-full').style.display = 'none'; document.getElementById('2406.10932v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10741">arXiv:2406.10741</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10741">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> Using CNN and Its Use Case in Digital Healthcare
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nigar%2C+N">Nishargo Nigar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10741v1-abstract-short" style="display: inline;">
        The process of identifying human emotion and affective states from <span class="search-hit mathjax">speech</span> is known as&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10741v1-abstract-full').style.display = 'inline'; document.getElementById('2406.10741v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10741v1-abstract-full" style="display: none;">
        The process of identifying human emotion and affective states from <span class="search-hit mathjax">speech</span> is known as <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER). This is based on the observation that tone and pitch in the voice frequently convey underlying emotion. <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">recognition</span> includes the ability to recognize emotions, which is becoming increasingly popular and in high demand. With the help of appropriate factors (such modalities, emotions, intensities, repetitions, etc.) found in the data, my research seeks to use the Convolutional Neural Network (CNN) to distinguish emotions from audio recordings and label them in accordance with the range of different emotions. I have developed a machine learning model to identify emotions from supplied audio files with the aid of machine learning methods. The evaluation is mostly focused on precision, recall, and F1 score, which are common machine learning metrics. To properly set up and train the machine learning framework, the main objective is to investigate the influence and cross-relation of all input and output parameters. To improve the ability to recognize intentions, a key condition for communication, I have evaluated emotions using my specialized machine learning algorithm via voice that would address the emotional state from voice with the help of digital healthcare, bridging the gap between human and artificial intelligence (AI).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10741v1-abstract-full').style.display = 'none'; document.getElementById('2406.10741v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Master&#39;s Thesis at Hamburg University of Technology</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10719">arXiv:2406.10719</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10719">pdf</a>, <a href="https://arxiv.org/format/2406.10719">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Finance">q-fin.CP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Statistical Finance">q-fin.ST</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Trading Devil: Robust backdoor attack via Stochastic investment models and Bayesian approach
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mengara%2C+O">Orson Mengara</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10719v4-abstract-short" style="display: inline;">
        With the growing use of voice-activated systems and <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10719v4-abstract-full').style.display = 'inline'; document.getElementById('2406.10719v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10719v4-abstract-full" style="display: none;">
        With the growing use of voice-activated systems and <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> technologies, the danger of backdoor attacks on audio data has grown significantly. This research looks at a specific type of attack, known as a Stochastic investment-based backdoor attack (MarketBack), in which adversaries strategically manipulate the stylistic properties of audio to fool <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> systems. The security and integrity of machine learning models are seriously threatened by backdoor attacks, in order to maintain the reliability of audio applications and systems, the identification of such attacks becomes crucial in the context of audio data. Experimental results demonstrated that MarketBack is feasible to achieve an average attack success rate close to 100% in seven victim models when poisoning less than 1% of the training data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10719v4-abstract-full').style.display = 'none'; document.getElementById('2406.10719v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">(Last update!, a constructive comment from arxiv led to this latest update ) Stochastic investment models and a Bayesian approach to better modeling of uncertainty : adversarial machine learning or Stochastic market. arXiv admin note: substantial text overlap with arXiv:2402.05967 (see this link to the paper by : Orson Mengara)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10598">arXiv:2406.10598</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10598">pdf</a>, <a href="https://arxiv.org/format/2406.10598">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/odyssey.2024-38">10.21437/odyssey.2024-38 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Double Multi-Head Attention Multimodal System for Odyssey 2024 <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> Challenge
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Costa%2C+F">Federico Costa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=India%2C+M">Miquel India</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hernando%2C+J">Javier Hernando</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10598v1-abstract-short" style="display: inline;">
        As computer-based applications are becoming more integrated into our daily lives, the importance of <span class="search-hit mathjax">Speech</span> Emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10598v1-abstract-full').style.display = 'inline'; document.getElementById('2406.10598v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10598v1-abstract-full" style="display: none;">
        As computer-based applications are becoming more integrated into our daily lives, the importance of <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) has increased significantly. Promoting research with innovative approaches in SER, the Odyssey 2024 <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> Challenge was organized as part of the Odyssey 2024 Speaker and Language <span class="search-hit mathjax">Recognition</span> Workshop. In this paper we describe the Double Multi-Head Attention Multimodal System developed for this challenge. Pre-trained self-supervised models were used to extract informative acoustic and text features. An early fusion strategy was adopted, where a Multi-Head Attention layer transforms these mixed features into complementary contextualized representations. A second attention mechanism is then applied to pool these representations into an utterance-level vector. Our proposed system achieved the third position in the categorical task ranking with a 34.41% Macro-F1 score, where 31 teams participated in total.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10598v1-abstract-full').style.display = 'none'; document.getElementById('2406.10598v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Odyssey 2024: The Speaker and Language <span class="search-hit mathjax">Recognition</span> Workshop</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proc. The Speaker and Language <span class="search-hit mathjax">Recognition</span> Workshop (Odyssey 2024), 266-273
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10591">arXiv:2406.10591</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10591">pdf</a>, <a href="https://arxiv.org/format/2406.10591">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MINT: a Multi-modal Image and Narrative Text Dubbing Dataset for Foley Audio Content Planning and Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+R">Ruibo Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+S">Shuchen Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+H">Hongming Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+T">Tao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiang%2C+C">Chunyu Qiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+Z">Zhengqi Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tao%2C+J">Jianhua Tao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qi%2C+X">Xin Qi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Y">Yi Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaopeng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhiyong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yukun Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xuefei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shuai Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+G">Guanjun Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10591v1-abstract-short" style="display: inline;">
        Foley audio, critical for enhancing the immersive experience in multimedia content, faces significant challenges in the AI-generated content (AIGC) landscape. Despite advancements in AIGC technologies for text and image generation, the foley audio dubbing remains rudimentary due to difficulties in cross-modal scene matching and content correlation. Current text-to-audio technology, which relies on&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10591v1-abstract-full').style.display = 'inline'; document.getElementById('2406.10591v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10591v1-abstract-full" style="display: none;">
        Foley audio, critical for enhancing the immersive experience in multimedia content, faces significant challenges in the AI-generated content (AIGC) landscape. Despite advancements in AIGC technologies for text and image generation, the foley audio dubbing remains rudimentary due to difficulties in cross-modal scene matching and content correlation. Current text-to-audio technology, which relies on detailed and acoustically relevant textual descriptions, falls short in practical video dubbing applications. Existing datasets like AudioSet, AudioCaps, Clotho, Sound-of-Story, and WavCaps do not fully meet the requirements for real-world foley audio dubbing task. To address this, we introduce the Multi-modal Image and Narrative Text Dubbing Dataset (MINT), designed to enhance mainstream dubbing tasks such as literary story audiobooks dubbing, image/silent video dubbing. Besides, to address the limitations of existing TTA technology in understanding and planning complex prompts, a Foley Audio Content Planning, Generation, and Alignment (CPGA) framework is proposed, which includes a content planning module leveraging large language models for complex multi-modal prompts comprehension. Additionally, the training process is optimized using Proximal Policy Optimization based reinforcement learning, significantly improving the alignment and auditory realism of generated foley audio. Experimental results demonstrate that our approach significantly advances the field of foley audio dubbing, providing robust solutions for the challenges of multi-modal dubbing. Even when utilizing the relatively lightweight GPT-2 model, our framework outperforms open-source multimodal large models such as LLaVA, DeepSeek-VL, and Moondream2. The dataset is available at https://github.com/borisfrb/MINT .
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10591v1-abstract-full').style.display = 'none'; document.getElementById('2406.10591v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10447">arXiv:2406.10447</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10447">pdf</a>, <a href="https://arxiv.org/format/2406.10447">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The BabyView dataset: High-resolution egocentric videos of infants&#39; and young children&#39;s everyday experiences
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Long%2C+B">Bria Long</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiang%2C+V">Violet Xiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stojanov%2C+S">Stefan Stojanov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sparks%2C+R+Z">Robert Z. Sparks</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+Z">Zi Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keene%2C+G+E">Grace E. Keene</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+A+W+M">Alvin W. M. Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+S+Y">Steven Y. Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhuang%2C+C">Chengxu Zhuang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marchman%2C+V+A">Virginia A. Marchman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yamins%2C+D+L+K">Daniel L. K. Yamins</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frank%2C+M+C">Michael C. Frank</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10447v1-abstract-short" style="display: inline;">
        &hellip;children spanning 6 months - 5 years of age in both longitudinal, at-home contexts and in a preschool environment. We provide gold-standard annotations for the evaluation of <span class="search-hit mathjax">speech</span> transcription, speaker diarization, and human pose estimation, and evaluate models in each of these domains. We train self-supervised language and vision models and evaluate their&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10447v1-abstract-full').style.display = 'inline'; document.getElementById('2406.10447v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10447v1-abstract-full" style="display: none;">
        Human children far exceed modern machine learning algorithms in their sample efficiency, achieving high performance in key domains with much less data than current models. This &#39;&#39;data gap&#39;&#39; is a key challenge both for building intelligent artificial systems and for understanding human development. Egocentric video capturing children&#39;s experience -- their &#39;&#39;training data&#39;&#39; -- is a key ingredient for comparison of humans and models and for the development of algorithmic innovations to bridge this gap. Yet there are few such datasets available, and extant data are low-resolution, have limited metadata, and importantly, represent only a small set of children&#39;s experiences. Here, we provide the first release of the largest developmental egocentric video dataset to date -- the BabyView dataset -- recorded using a high-resolution camera with a large vertical field-of-view and gyroscope/accelerometer data. This 493 hour dataset includes egocentric videos from children spanning 6 months - 5 years of age in both longitudinal, at-home contexts and in a preschool environment. We provide gold-standard annotations for the evaluation of <span class="search-hit mathjax">speech</span> transcription, speaker diarization, and human pose estimation, and evaluate models in each of these domains. We train self-supervised language and vision models and evaluate their transfer to out-of-distribution tasks including syntactic structure learning, object <span class="search-hit mathjax">recognition</span>, depth estimation, and image segmentation. Although performance in each scales with dataset size, overall performance is relatively lower than when models are trained on curated datasets, especially in the visual domain. Our dataset stands as an open challenge for robust, humanlike AI systems: how can such systems achieve human-levels of success on the same scale and distribution of training data as humans?
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10447v1-abstract-full').style.display = 'none'; document.getElementById('2406.10447v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 2 figures, 4 tables and SI. Submitted to NeurIPS Datasets and Benchmarks</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10313">arXiv:2406.10313</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10313">pdf</a>, <a href="https://arxiv.org/ps/2406.10313">ps</a>, <a href="https://arxiv.org/format/2406.10313">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CNVSRC 2023: The First Chinese Continuous Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Challenge
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chen Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zehua Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiaolou Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lantian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dong Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10313v1-abstract-short" style="display: inline;">
        The first Chinese Continuous Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Challenge aimed to probe the performance of Large Vocabulary Continuous Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (LVC-VSR) on two tasks: (1) Single-speaker VSR for a particular speaker and (2) Multi-s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10313v1-abstract-full').style.display = 'inline'; document.getElementById('2406.10313v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10313v1-abstract-full" style="display: none;">
        The first Chinese Continuous Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Challenge aimed to probe the performance of Large Vocabulary Continuous Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (LVC-VSR) on two tasks: (1) Single-speaker VSR for a particular speaker and (2) Multi-speaker VSR for a set of registered speakers. The challenge yielded highly successful results, with the best submission significantly outperforming the baseline, particularly in the single-speaker task. This paper comprehensively reviews the challenge, encompassing the data profile, task specifications, and baseline system construction. It also summarises the representative techniques employed by the submitted systems, highlighting the most effective approaches. Additional information and resources about this challenge can be accessed through the official website at http://cnceleb.org/competition.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10313v1-abstract-full').style.display = 'none'; document.getElementById('2406.10313v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10284">arXiv:2406.10284</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10284">pdf</a>, <a href="https://arxiv.org/format/2406.10284">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving child <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> with augmented child-like <span class="search-hit mathjax">speech</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yuanyuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yue%2C+Z">Zhengjun Yue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Patel%2C+T">Tanvina Patel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Scharenborg%2C+O">Odette Scharenborg</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10284v1-abstract-short" style="display: inline;">
        State-of-the-art ASRs show suboptimal performance for child <span class="search-hit mathjax">speech</span>. The scarcity of child <span class="search-hit mathjax">speech</span> limits the development of child <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (CSR). Therefore, we studied child-to-child voice conversion (VC) from existing child speake&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10284v1-abstract-full').style.display = 'inline'; document.getElementById('2406.10284v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10284v1-abstract-full" style="display: none;">
        State-of-the-art ASRs show suboptimal performance for child <span class="search-hit mathjax">speech</span>. The scarcity of child <span class="search-hit mathjax">speech</span> limits the development of child <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (CSR). Therefore, we studied child-to-child voice conversion (VC) from existing child speakers in the dataset and additional (new) child speakers via monolingual and cross-lingual (Dutch-to-German) VC, respectively. The results showed that cross-lingual child-to-child VC significantly improved child ASR performance. Experiments on the impact of the quantity of child-to-child cross-lingual VC-generated data on fine-tuning (FT) ASR models gave the best results with two-fold augmentation for our FT-Conformer model and FT-Whisper model which reduced WERs with ~3% absolute compared to the baseline, and with six-fold augmentation for the model trained from scratch, which improved by an absolute 3.6% WER. Moreover, using a small amount of &#34;high-quality&#34; VC-generated data achieved similar results to those of our best-FT models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10284v1-abstract-full').style.display = 'none'; document.getElementById('2406.10284v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 1 figure Accepted to INTERSPEECH 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10275">arXiv:2406.10275</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10275">pdf</a>, <a href="https://arxiv.org/format/2406.10275">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ExHuBERT: Enhancing HuBERT Through Block Extension and Fine-Tuning on 37 Emotion Datasets
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Amiriparian%2C+S">Shahin Amiriparian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Packa%C5%84%2C+F">Filip Packań</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gerczuk%2C+M">Maurice Gerczuk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schuller%2C+B+W">Björn W. Schuller</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10275v1-abstract-short" style="display: inline;">
        Foundation models have shown great promise in <span class="search-hit mathjax">speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10275v1-abstract-full').style.display = 'inline'; document.getElementById('2406.10275v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10275v1-abstract-full" style="display: none;">
        Foundation models have shown great promise in <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) by leveraging their pre-trained representations to capture emotion patterns in <span class="search-hit mathjax">speech</span> signals. To further enhance SER performance across various languages and domains, we propose a novel twofold approach. First, we gather EmoSet++, a comprehensive multi-lingual, multi-cultural <span class="search-hit mathjax">speech</span> emotion corpus with 37 datasets, 150,907 samples, and a total duration of 119.5 hours. Second, we introduce ExHuBERT, an enhanced version of HuBERT achieved by backbone extension and fine-tuning on EmoSet++. We duplicate each encoder layer and its weights, then freeze the first duplicate, integrating an extra zero-initialized linear layer and skip connections to preserve functionality and ensure its adaptability for subsequent fine-tuning. Our evaluation on unseen datasets shows the efficacy of ExHuBERT, setting a new benchmark for various SER tasks. Model and details on EmoSet++: https://huggingface.co/amiriparian/ExHuBERT.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10275v1-abstract-full').style.display = 'none'; document.getElementById('2406.10275v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted at INTERSPEECH 2024</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T10
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10177">arXiv:2406.10177</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10177">pdf</a>, <a href="https://arxiv.org/format/2406.10177">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.21437/Interspeech.2024-2246">10.21437/Interspeech.2024-2246 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Inclusive ASR for Disfluent <span class="search-hit mathjax">Speech</span>: Cascaded Large-Scale Self-Supervised Learning with Targeted Fine-Tuning and Data Augmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mujtaba%2C+D">Dena Mujtaba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahapatra%2C+N+R">Nihar R. Mahapatra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arney%2C+M">Megan Arney</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yaruss%2C+J+S">J. Scott Yaruss</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Herring%2C+C">Caryn Herring</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bin%2C+J">Jia Bin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10177v2-abstract-short" style="display: inline;">
        Automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10177v2-abstract-full').style.display = 'inline'; document.getElementById('2406.10177v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10177v2-abstract-full" style="display: none;">
        Automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) systems often falter while processing stuttering-related disfluencies -- such as involuntary blocks and word repetitions -- yielding inaccurate transcripts. A critical barrier to progress is the scarcity of large, annotated disfluent <span class="search-hit mathjax">speech</span> datasets. Therefore, we present an inclusive ASR design approach, leveraging large-scale self-supervised learning on standard <span class="search-hit mathjax">speech</span> followed by targeted fine-tuning and data augmentation on a smaller, curated dataset of disfluent <span class="search-hit mathjax">speech</span>. Our data augmentation technique enriches training datasets with various disfluencies, enhancing ASR processing of these <span class="search-hit mathjax">speech</span> patterns. Results show that fine-tuning wav2vec 2.0 with even a relatively small, labeled dataset, alongside data augmentation, can significantly reduce word error rates for disfluent <span class="search-hit mathjax">speech</span>. Our approach not only advances ASR inclusivity for people who stutter, but also paves the way for ASRs that can accommodate wider <span class="search-hit mathjax">speech</span> variations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10177v2-abstract-full').style.display = 'none'; document.getElementById('2406.10177v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Included in 2024 Proceedings of INTERSPEECH</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2; K.4
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10152">arXiv:2406.10152</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10152">pdf</a>, <a href="https://arxiv.org/format/2406.10152">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint Speaker Features Learning for Audio-visual Multichannel <span class="search-hit mathjax">Speech</span> Separation and <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+G">Guinan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+J">Jiajun Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Youjun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+M">Mengzhe Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+S">Shujie Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhe Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+Z">Zengrui Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+T">Tianzi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+X">Xurong Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+H">Helen Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xunying Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10152v1-abstract-short" style="display: inline;">
        This paper proposes joint speaker feature learning methods for zero-shot adaptation of audio-visual multichannel <span class="search-hit mathjax">speech</span> separation and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10152v1-abstract-full').style.display = 'inline'; document.getElementById('2406.10152v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10152v1-abstract-full" style="display: none;">
        This paper proposes joint speaker feature learning methods for zero-shot adaptation of audio-visual multichannel <span class="search-hit mathjax">speech</span> separation and <span class="search-hit mathjax">recognition</span> systems. xVector and ECAPA-TDNN speaker encoders are connected using purpose-built fusion blocks and tightly integrated with the complete system training. Experiments conducted on LRS3-TED data simulated multichannel overlapped <span class="search-hit mathjax">speech</span> suggest that joint speaker feature learning consistently improves <span class="search-hit mathjax">speech</span> separation and <span class="search-hit mathjax">recognition</span> performance over the baselines without joint speaker feature estimation. Further analyses reveal performance improvements are strongly correlated with increased inter-speaker discrimination measured using cosine similarity. The best-performing joint speaker feature learning adapted system outperformed the baseline fine-tuned WavLM model by statistically significant WER reductions of 21.6% and 25.3% absolute (67.5% and 83.5% relative) on Dev and Test sets after incorporating WavLM features and video modality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10152v1-abstract-full').style.display = 'none'; document.getElementById('2406.10152v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by Interspeech 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10083">arXiv:2406.10083</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10083">pdf</a>, <a href="https://arxiv.org/format/2406.10083">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Evaluation of <span class="search-hit mathjax">Speech</span> Foundation Models for Spoken Language Understanding
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Arora%2C+S">Siddhant Arora</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pasad%2C+A">Ankita Pasad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chien%2C+C">Chung-Ming Chien</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+J">Jionghao Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+R">Roshan Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jung%2C+J">Jee-weon Jung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhamyal%2C+H">Hira Dhamyal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">William Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shon%2C+S">Suwon Shon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-yi Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Livescu%2C+K">Karen Livescu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10083v1-abstract-short" style="display: inline;">
        &hellip;the need for open resources and benchmarking of complex spoken language understanding (SLU) tasks, including both classification and sequence generation tasks, on natural <span class="search-hit mathjax">speech</span>. The benchmark has demonstrated preliminary success in using pre-trained&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10083v1-abstract-full').style.display = 'inline'; document.getElementById('2406.10083v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10083v1-abstract-full" style="display: none;">
        The Spoken Language Understanding Evaluation (SLUE) suite of benchmark tasks was recently introduced to address the need for open resources and benchmarking of complex spoken language understanding (SLU) tasks, including both classification and sequence generation tasks, on natural <span class="search-hit mathjax">speech</span>. The benchmark has demonstrated preliminary success in using pre-trained <span class="search-hit mathjax">speech</span> foundation models (SFM) for these SLU tasks. However, the community still lacks a fine-grained understanding of the comparative utility of different SFMs. Inspired by this, we ask: which SFMs offer the most benefits for these complex SLU tasks, and what is the most effective approach for incorporating these SFMs? To answer this, we perform an extensive evaluation of multiple supervised and self-supervised SFMs using several evaluation protocols: (i) frozen SFMs with a lightweight prediction head, (ii) frozen SFMs with a complex prediction head, and (iii) fine-tuned SFMs with a lightweight prediction head. Although the supervised SFMs are pre-trained on much more <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> data (with labels), they do not always outperform self-supervised SFMs; the latter tend to perform at least as well as, and sometimes better than, supervised SFMs, especially on the sequence generation tasks in SLUE. While there is no universally optimal way of incorporating SFMs, the complex prediction head gives the best performance for most tasks, although it increases the inference time. We also introduce an open-source toolkit and performance leaderboard, SLUE-PERB, for these tasks and modeling strategies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10083v1-abstract-full').style.display = 'none'; document.getElementById('2406.10083v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ACL Findings 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.10082">arXiv:2406.10082</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.10082">pdf</a>, <a href="https://arxiv.org/format/2406.10082">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Whisper-Flamingo: Integrating Visual Features into Whisper for Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> and Translation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rouditchenko%2C+A">Andrew Rouditchenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+Y">Yuan Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Thomas%2C+S">Samuel Thomas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Karlinsky%2C+L">Leonid Karlinsky</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kuehne%2C+H">Hilde Kuehne</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feris%2C+R">Rogerio Feris</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Glass%2C+J">James Glass</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.10082v1-abstract-short" style="display: inline;">
        Audio-Visual <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10082v1-abstract-full').style.display = 'inline'; document.getElementById('2406.10082v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.10082v1-abstract-full" style="display: none;">
        Audio-Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (AVSR) uses lip-based video to improve performance in noise. Since videos are harder to obtain than audio, the video training data of AVSR models is usually limited to a few thousand hours. In contrast, <span class="search-hit mathjax">speech</span> models such as Whisper are trained with hundreds of thousands of hours of data, and thus learn a better <span class="search-hit mathjax">speech</span>-to-text decoder. The huge training data difference motivates us to adapt Whisper to handle video inputs. Inspired by Flamingo which injects visual features into language models, we propose Whisper-Flamingo which integrates visual features into the Whisper <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and translation model with gated cross attention. Our audio-visual Whisper-Flamingo outperforms audio-only Whisper on English <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and En-X translation for 6 languages in noisy conditions. Moreover, Whisper-Flamingo is a versatile model and conducts all of these tasks using one set of parameters, while prior methods are trained separately on each language.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.10082v1-abstract-full').style.display = 'none'; document.getElementById('2406.10082v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Interspeech 2024. Code https://github.com/roudimit/whisper-flamingo</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=500"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=600"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=500"
              class="pagination-link "
              aria-label="Page 11"
              aria-current="page">11
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=550"
              class="pagination-link is-current"
              aria-label="Page 12"
              aria-current="page">12
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=600"
              class="pagination-link "
              aria-label="Page 13"
              aria-current="page">13
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>