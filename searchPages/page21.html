<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Advanced Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1001&ndash;1050 of 8,623 results
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  
    

    <div class="columns">
      <div class="column is-two-thirds-tablet">
        <p style="margin-bottom: .5em">Query: <a href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=1000">order: -announced_date_first; size: 50; page_start: 1000; classification: Computer Science (cs); include_cross_list: True; terms: AND all=Speech recognition</a></p>
        <div class="buttons">
          <a class="button is-link" href="/search/advanced?terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=1000">Refine query</a><a class="button" href="/search/advanced">New search</a>
        </div>
      </div>
      <div class="column is-one-third-tablet is-hidden-mobile">
        <p class="has-text-right" style="margin-top: 1em">
          
          <a href="/search/?order=-announced_date_first&amp;size=50">Simple Search</a>
          
        </p>
      </div>
    </div>

    
        
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/advanced">
      <div style="display: none;">
        
          
            <input id="advanced" name="advanced" type="hidden" value="">
          
        
          
            <ul id="terms"><li><label for="terms-0">Terms-0</label> <table id="terms-0"><tr><th><label for="terms-0-term">Search term...</label></th><td><input id="terms-0-term" name="terms-0-term" type="text" value="Speech recognition"></td></tr><tr><th><label for="terms-0-operator">Operator</label></th><td><select id="terms-0-operator" name="terms-0-operator"><option selected value="AND">AND</option><option value="OR">OR</option><option value="NOT">NOT</option></select></td></tr><tr><th><label for="terms-0-field">Field</label></th><td><select id="terms-0-field" name="terms-0-field"><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="cross_list_category">Cross-list category</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="author_id">arXiv author ID</option><option selected value="all">All fields</option></select></td></tr></table></li></ul>
          
        
          
            <table id="classification"><tr><th><label for="classification-computer_science">Computer Science (cs)</label></th><td><input checked id="classification-computer_science" name="classification-computer_science" type="checkbox" value="y"></td></tr><tr><th><label for="classification-economics">Economics (econ)</label></th><td><input id="classification-economics" name="classification-economics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-eess">Electrical Engineering and Systems Science (eess)</label></th><td><input id="classification-eess" name="classification-eess" type="checkbox" value="y"></td></tr><tr><th><label for="classification-mathematics">Mathematics (math)</label></th><td><input id="classification-mathematics" name="classification-mathematics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics">Physics</label></th><td><input id="classification-physics" name="classification-physics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-physics_archives">Physics Archives</label></th><td><select id="classification-physics_archives" name="classification-physics_archives"><option selected value="all">all</option><option value="astro-ph">astro-ph</option><option value="cond-mat">cond-mat</option><option value="gr-qc">gr-qc</option><option value="hep-ex">hep-ex</option><option value="hep-lat">hep-lat</option><option value="hep-ph">hep-ph</option><option value="hep-th">hep-th</option><option value="math-ph">math-ph</option><option value="nlin">nlin</option><option value="nucl-ex">nucl-ex</option><option value="nucl-th">nucl-th</option><option value="physics">physics</option><option value="quant-ph">quant-ph</option></select></td></tr><tr><th><label for="classification-q_biology">Quantitative Biology (q-bio)</label></th><td><input id="classification-q_biology" name="classification-q_biology" type="checkbox" value="y"></td></tr><tr><th><label for="classification-q_finance">Quantitative Finance (q-fin)</label></th><td><input id="classification-q_finance" name="classification-q_finance" type="checkbox" value="y"></td></tr><tr><th><label for="classification-statistics">Statistics (stat)</label></th><td><input id="classification-statistics" name="classification-statistics" type="checkbox" value="y"></td></tr><tr><th><label for="classification-include_cross_list">Include cross-list</label></th><td><ul id="classification-include_cross_list"><li><input checked id="classification-include_cross_list-0" name="classification-include_cross_list" type="radio" value="include"> <label for="classification-include_cross_list-0">Include cross-listed papers</label></li><li><input id="classification-include_cross_list-1" name="classification-include_cross_list" type="radio" value="exclude"> <label for="classification-include_cross_list-1">Exclude cross-listed papers</label></li></ul></td></tr></table>
          
        
          
            <table id="date"><tr><th><label for="date-filter_by">Filter by</label></th><td><ul id="date-filter_by"><li><input checked id="date-filter_by-0" name="date-filter_by" type="radio" value="all_dates"> <label for="date-filter_by-0">All dates</label></li><li><input id="date-filter_by-1" name="date-filter_by" type="radio" value="past_12"> <label for="date-filter_by-1">Past 12 months</label></li><li><input id="date-filter_by-2" name="date-filter_by" type="radio" value="specific_year"> <label for="date-filter_by-2">Specific year</label></li><li><input id="date-filter_by-3" name="date-filter_by" type="radio" value="date_range"> <label for="date-filter_by-3">Date range</label></li></ul></td></tr><tr><th><label for="date-year">Year</label></th><td><input id="date-year" name="date-year" type="text" value=""></td></tr><tr><th><label for="date-from_date">From</label></th><td><input id="date-from_date" name="date-from_date" type="text" value=""></td></tr><tr><th><label for="date-to_date">to</label></th><td><input id="date-to_date" name="date-to_date" type="text" value=""></td></tr><tr><th><label for="date-date_type">Apply to</label></th><td><ul id="date-date_type"><li><input checked id="date-date_type-0" name="date-date_type" type="radio" value="submitted_date"> <label for="date-date_type-0">Submission date (most recent)</label></li><li><input id="date-date_type-1" name="date-date_type" type="radio" value="submitted_date_first"> <label for="date-date_type-1">Submission date (original)</label></li><li><input id="date-date_type-2" name="date-date_type" type="radio" value="announced_date_first"> <label for="date-date_type-2">Announcement date</label></li></ul></td></tr></table>
          
        
          
        
          
        
          
            <input id="include_older_versions" name="include_older_versions" type="checkbox" value="y">
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
        


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=950"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=1050"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=950"
              class="pagination-link "
              aria-label="Page 20"
              aria-current="page">20
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=1000"
              class="pagination-link is-current"
              aria-label="Page 21"
              aria-current="page">21
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=1050"
              class="pagination-link "
              aria-label="Page 22"
              aria-current="page">22
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1001"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.03461">arXiv:2403.03461</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.03461">pdf</a>, <a href="https://arxiv.org/format/2403.03461">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Density-Guided Temporal Attention Transformer for Indiscernible Object Counting in Underwater Video
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C">Cheng-Yen Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H">Hsiang-Wei Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+Z">Zhongyu Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wallace%2C+F">Farron Wallace</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hwang%2C+J">Jenq-Neng Hwang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.03461v1-abstract-short" style="display: inline;">
        Dense object counting or crowd counting has come a long way thanks to the recent development in the vision community. However, indiscernible object counting, which aims to count the number of targets that are blended with respect to their surroundings, has been a challenge. Image-based object counting datasets have been the mainstream of the current publicly available datasets. Therefore, we propo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03461v1-abstract-full').style.display = 'inline'; document.getElementById('2403.03461v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.03461v1-abstract-full" style="display: none;">
        Dense object counting or crowd counting has come a long way thanks to the recent development in the vision community. However, indiscernible object counting, which aims to count the number of targets that are blended with respect to their surroundings, has been a challenge. Image-based object counting datasets have been the mainstream of the current publicly available datasets. Therefore, we propose a large-scale dataset called YoutubeFish-35, which contains a total of 35 sequences of high-definition videos with high frame-per-second and more than 150,000 annotated center points across a selected variety of scenes. For benchmarking purposes, we select three mainstream methods for dense object counting and carefully evaluate them on the newly collected dataset. We propose TransVidCount, a new strong baseline that combines density and regression branches along the temporal domain in a unified framework and can effectively tackle indiscernible object counting with state-of-the-art performance on YoutubeFish-35 dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03461v1-abstract-full').style.display = 'none'; document.getElementById('2403.03461v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICASSP 2024 (IEEE International Conference on Acoustics, <span class="search-hit mathjax">Speech</span>, and Signal Processing)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.03435">arXiv:2403.03435</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.03435">pdf</a>, <a href="https://arxiv.org/ps/2403.03435">ps</a>, <a href="https://arxiv.org/format/2403.03435">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VLSP 2023 -- LTER: A Summary of the Challenge on Legal Textual Entailment <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tran%2C+V">Vu Tran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+H">Ha-Thanh Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vo%2C+T">Trung Vo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luu%2C+S+T">Son T. Luu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+H">Hoang-Anh Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Le%2C+N">Ngoc-Cam Le</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Le%2C+T">Thi-Thuy Le</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+M">Minh-Tien Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+T">Truong-Son Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+L">Le-Minh Nguyen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.03435v1-abstract-short" style="display: inline;">
        &hellip;Japanese, and Chinese has been well-established, we introduce the first fundamental research for the Vietnamese language in the legal domain: legal textual entailment <span class="search-hit mathjax">recognition</span> through the Vietnamese Language and <span class="search-hit mathjax">Speech</span> Processing workshop. In analyzing participants&#39; results, we discuss certain linguistic aspect&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03435v1-abstract-full').style.display = 'inline'; document.getElementById('2403.03435v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.03435v1-abstract-full" style="display: none;">
        In this new era of rapid AI development, especially in language processing, the demand for AI in the legal domain is increasingly critical. In the context where research in other languages such as English, Japanese, and Chinese has been well-established, we introduce the first fundamental research for the Vietnamese language in the legal domain: legal textual entailment <span class="search-hit mathjax">recognition</span> through the Vietnamese Language and <span class="search-hit mathjax">Speech</span> Processing workshop. In analyzing participants&#39; results, we discuss certain linguistic aspects critical in the legal domain that pose challenges that need to be addressed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03435v1-abstract-full').style.display = 'none'; document.getElementById('2403.03435v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.03145">arXiv:2403.03145</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.03145">pdf</a>, <a href="https://arxiv.org/format/2403.03145">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yuxin Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+S">Shijie Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+H">Hu Su</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhiqing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yuhao Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zou%2C+W">Wei Zou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+S">Siyang Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Y">Yun Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.03145v1-abstract-short" style="display: inline;">
        Audio-Visual Source Localization (AVSL) aims to locate sounding objects within video frames given the paired audio clips. Existing methods predominantly rely on self-supervised contrastive learning of audio-visual correspondence. Without any bounding-box annotations, they struggle to achieve precise localization, especially for small objects, and suffer from blurry boundaries and false positives.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03145v1-abstract-full').style.display = 'inline'; document.getElementById('2403.03145v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.03145v1-abstract-full" style="display: none;">
        Audio-Visual Source Localization (AVSL) aims to locate sounding objects within video frames given the paired audio clips. Existing methods predominantly rely on self-supervised contrastive learning of audio-visual correspondence. Without any bounding-box annotations, they struggle to achieve precise localization, especially for small objects, and suffer from blurry boundaries and false positives. Moreover, the naive semi-supervised method is poor in fully leveraging the information of abundant unlabeled data. In this paper, we propose a novel semi-supervised learning framework for AVSL, namely Dual Mean-Teacher (DMT), comprising two teacher-student structures to circumvent the confirmation bias issue. Specifically, two teachers, pre-trained on limited labeled data, are employed to filter out noisy samples via the consensus between their predictions, and then generate high-quality pseudo-labels by intersecting their confidence maps. The sufficient utilization of both labeled and unlabeled data and the proposed unbiased framework enable DMT to outperform current state-of-the-art methods by a large margin, with CIoU of 90.4% and 48.8% on Flickr-SoundNet and VGG-Sound Source, obtaining 8.9%, 9.6% and 4.6%, 6.4% improvements over self- and semi-supervised methods respectively, given only 3% positional-annotations. We also extend our framework to some existing AVSL methods and consistently boost their performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03145v1-abstract-full').style.display = 'none'; document.getElementById('2403.03145v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to NeurIPS2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.03095">arXiv:2403.03095</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.03095">pdf</a>, <a href="https://arxiv.org/format/2403.03095">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cross Pseudo-Labeling for Semi-Supervised Audio-Visual Source Localization
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yuxin Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+S">Shijie Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yuhao Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+H">Hu Su</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zou%2C+W">Wei Zou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.03095v1-abstract-short" style="display: inline;">
        Audio-Visual Source Localization (AVSL) is the task of identifying specific sounding objects in the scene given audio cues. In our work, we focus on semi-supervised AVSL with pseudo-labeling. To address the issues with vanilla hard pseudo-labels including bias accumulation, noise sensitivity, and instability, we propose a novel method named Cross Pseudo-Labeling (XPL), wherein two models learn fro&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03095v1-abstract-full').style.display = 'inline'; document.getElementById('2403.03095v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.03095v1-abstract-full" style="display: none;">
        Audio-Visual Source Localization (AVSL) is the task of identifying specific sounding objects in the scene given audio cues. In our work, we focus on semi-supervised AVSL with pseudo-labeling. To address the issues with vanilla hard pseudo-labels including bias accumulation, noise sensitivity, and instability, we propose a novel method named Cross Pseudo-Labeling (XPL), wherein two models learn from each other with the cross-refine mechanism to avoid bias accumulation. We equip XPL with two effective components. Firstly, the soft pseudo-labels with sharpening and pseudo-label exponential moving average mechanisms enable models to achieve gradual self-improvement and ensure stable training. Secondly, the curriculum data selection module adaptively selects pseudo-labels with high quality during training to mitigate potential bias. Experimental results demonstrate that XPL significantly outperforms existing methods, achieving state-of-the-art performance while effectively mitigating confirmation bias and ensuring training stability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03095v1-abstract-full').style.display = 'none'; document.getElementById('2403.03095v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted To ICASSP2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.02938">arXiv:2403.02938</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.02938">pdf</a>, <a href="https://arxiv.org/format/2403.02938">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3582700.3582722">10.1145/3582700.3582722 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AIx Speed: Playback Speed Optimization Using Listening Comprehension of <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kawamura%2C+K">Kazuki Kawamura</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rekimoto%2C+J">Jun Rekimoto</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.02938v1-abstract-short" style="display: inline;">
        &hellip;efficient comprehension of time-series content have been developed. However, there is still room for these systems to further extend human speed-listening ability by generating <span class="search-hit mathjax">speech</span> with playback speed optimized for even finer time units and providing it to humans. In this study, we determine whether humans can hear the optimized&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.02938v1-abstract-full').style.display = 'inline'; document.getElementById('2403.02938v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.02938v1-abstract-full" style="display: none;">
        Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension. To further utilize this capability, systems that automatically adjust the playback speed according to the user&#39;s condition and the type of content to assist in more efficient comprehension of time-series content have been developed. However, there is still room for these systems to further extend human speed-listening ability by generating <span class="search-hit mathjax">speech</span> with playback speed optimized for even finer time units and providing it to humans. In this study, we determine whether humans can hear the optimized <span class="search-hit mathjax">speech</span> and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring <span class="search-hit mathjax">speech</span> intelligibility. The system uses the <span class="search-hit mathjax">speech</span> recognizer score as a proxy for how well a human can hear a certain unit of <span class="search-hit mathjax">speech</span> and maximizes the <span class="search-hit mathjax">speech</span> playback speed to the extent that a human can hear. This method can be used to produce fast but intelligible <span class="search-hit mathjax">speech</span>. In the evaluation experiment, we compared the <span class="search-hit mathjax">speech</span> played back at a constant fast speed and the flexibly speed-up <span class="search-hit mathjax">speech</span> generated by the proposed method in a blind test and confirmed that the proposed method produced <span class="search-hit mathjax">speech</span> that was easier to listen to.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.02938v1-abstract-full').style.display = 'none'; document.getElementById('2403.02938v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        AHs &#39;23: Proceedings of the Augmented Humans International Conference 2023
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.02173">arXiv:2403.02173</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.02173">pdf</a>, <a href="https://arxiv.org/format/2403.02173">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        What has LeBenchmark Learnt about French Syntax?
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dugonji%C4%87%2C+Z">Zdravko Dugonjić</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pupier%2C+A">Adrien Pupier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lecouteux%2C+B">Benjamin Lecouteux</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Coavoux%2C+M">Maximin Coavoux</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.02173v1-abstract-short" style="display: inline;">
        &hellip;LeBenchmark, a pretrained acoustic model trained on 7k hours of spoken French, for syntactic information. Pretrained acoustic models are increasingly used for downstream <span class="search-hit mathjax">speech</span> tasks such as automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.02173v1-abstract-full').style.display = 'inline'; document.getElementById('2403.02173v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.02173v1-abstract-full" style="display: none;">
        The paper reports on a series of experiments aiming at probing LeBenchmark, a pretrained acoustic model trained on 7k hours of spoken French, for syntactic information. Pretrained acoustic models are increasingly used for downstream <span class="search-hit mathjax">speech</span> tasks such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, <span class="search-hit mathjax">speech</span> translation, spoken language understanding or <span class="search-hit mathjax">speech</span> parsing. They are trained on very low level information (the raw <span class="search-hit mathjax">speech</span> signal), and do not have explicit lexical knowledge. Despite that, they obtained reasonable results on tasks that requires higher level linguistic knowledge. As a result, an emerging question is whether these models encode syntactic information. We probe each representation layer of LeBenchmark for syntax, using the Orféo treebank, and observe that it has learnt some syntactic information. Our results show that syntactic information is more easily extractable from the middle layers of the network, after which a very sharp decrease is observed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.02173v1-abstract-full').style.display = 'none'; document.getElementById('2403.02173v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to LREC-COLING 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.02167">arXiv:2403.02167</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.02167">pdf</a>, <a href="https://arxiv.org/format/2403.02167">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EMOVOME Database: Advancing Emotion <span class="search-hit mathjax">Recognition</span> in <span class="search-hit mathjax">Speech</span> Beyond Staged Scenarios
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=G%C3%B3mez-Zaragoz%C3%A1%2C+L">Lucía Gómez-Zaragozá</a>, 
      
      <a href="/search/?searchtype=author&amp;query=del+Amor%2C+R">Rocío del Amor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro-Bleda%2C+M+J">María José Castro-Bleda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Naranjo%2C+V">Valery Naranjo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raya%2C+M+A">Mariano Alcañiz Raya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mar%C3%ADn-Morales%2C+J">Javier Marín-Morales</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.02167v2-abstract-short" style="display: inline;">
        Natural databases for <span class="search-hit mathjax">Speech</span> Emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.02167v2-abstract-full').style.display = 'inline'; document.getElementById('2403.02167v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.02167v2-abstract-full" style="display: none;">
        Natural databases for <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span> (SER) are scarce and often rely on staged scenarios, such as films or television shows, limiting their application in real-world contexts. We developed and publicly released the Emotional Voice Messages (EMOVOME) database, including 999 voice messages from real conversations of 100 Spanish speakers on a messaging app, labeled in continuous and discrete emotions by expert and non-expert annotators. We evaluated speaker-independent SER models using a standard set of acoustic features and transformer-based models. We compared the results with reference databases including acted and elicited <span class="search-hit mathjax">speech</span>, and analyzed the influence of annotators and gender fairness. The pre-trained UniSpeech-SAT-Large model achieved the highest results, 61.64% and 55.57% Unweighted Accuracy (UA) for 3-class valence and arousal prediction respectively on EMOVOME, a 10% improvement over baseline models. For the emotion categories, 42.58% UA was obtained. EMOVOME performed lower than the acted RAVDESS database. The elicited IEMOCAP database also outperformed EMOVOME in predicting emotion categories, while similar results were obtained in valence and arousal. EMOVOME outcomes varied with annotator labels, showing better results and fairness when combining expert and non-expert annotations. This study highlights the gap between staged and real-life scenarios, supporting further advancements in recognizing genuine emotions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.02167v2-abstract-full').style.display = 'none'; document.getElementById('2403.02167v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This article is a merged version of the description of the EMOVOME database in arXiv:2402.17496v1 and the <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> models in arXiv:2403.02167v1. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.5.1; I.5.4
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.02010">arXiv:2403.02010</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.02010">pdf</a>, <a href="https://arxiv.org/format/2403.02010">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SA-SOT: Speaker-Aware Serialized Output Training for Multi-Talker ASR
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+Z">Zhiyun Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+L">Linhao Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+L">Lu Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Z">Zejun Ma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.02010v1-abstract-short" style="display: inline;">
        Multi-talker automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> plays a crucial role in scenarios involving multi-party interactions, such as meetings and conversations. Due to its inherent complexity, this task has been receiving increasing attention. Notably, the serialized output training (SOT) stands out among various approaches because&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.02010v1-abstract-full').style.display = 'inline'; document.getElementById('2403.02010v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.02010v1-abstract-full" style="display: none;">
        Multi-talker automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> plays a crucial role in scenarios involving multi-party interactions, such as meetings and conversations. Due to its inherent complexity, this task has been receiving increasing attention. Notably, the serialized output training (SOT) stands out among various approaches because of its simplistic architecture and exceptional performance. However, the frequent speaker changes in token-level SOT (t-SOT) present challenges for the autoregressive decoder in effectively utilizing context to predict output sequences. To address this issue, we introduce a masked t-SOT label, which serves as the cornerstone of an auxiliary training loss. Additionally, we utilize a speaker similarity matrix to refine the self-attention mechanism of the decoder. This strategic adjustment enhances contextual relationships within the same speaker&#39;s tokens while minimizing interactions between different speakers&#39; tokens. We denote our method as speaker-aware SOT (SA-SOT). Experiments on the Librispeech datasets demonstrate that our SA-SOT obtains a relative cpWER reduction ranging from 12.75% to 22.03% on the multi-talker test sets. Furthermore, with more extensive training, our method achieves an impressive cpWER of 3.41%, establishing a new state-of-the-art result on the LibrispeechMix dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.02010v1-abstract-full').style.display = 'none'; document.getElementById('2403.02010v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.01983">arXiv:2403.01983</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.01983">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Language and <span class="search-hit mathjax">Speech</span> Technology for Central Kurdish Varieties
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ahmadi%2C+S">Sina Ahmadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jaff%2C+D+Q">Daban Q. Jaff</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alam%2C+M+M+I">Md Mahfuz Ibn Alam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Anastasopoulos%2C+A">Antonios Anastasopoulos</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.01983v1-abstract-short" style="display: inline;">
        &hellip;language spoken by over 30 million speakers, is considered a dialect continuum and known for its diversity in language varieties. Previous studies addressing language and <span class="search-hit mathjax">speech</span> technology for Kurdish handle it in a monolithic way as a macro-language, resulting in disparities for dialects and varieties for which there are few resources and tools available. I&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01983v1-abstract-full').style.display = 'inline'; document.getElementById('2403.01983v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.01983v1-abstract-full" style="display: none;">
        Kurdish, an Indo-European language spoken by over 30 million speakers, is considered a dialect continuum and known for its diversity in language varieties. Previous studies addressing language and <span class="search-hit mathjax">speech</span> technology for Kurdish handle it in a monolithic way as a macro-language, resulting in disparities for dialects and varieties for which there are few resources and tools available. In this paper, we take a step towards developing resources for language and <span class="search-hit mathjax">speech</span> technology for varieties of Central Kurdish, creating a corpus by transcribing movies and TV series as an alternative to fieldwork. Additionally, we report the performance of machine translation, automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, and language identification as downstream tasks evaluated on Central Kurdish varieties. Data and models are publicly available under an open license at https://github.com/sinaahmadi/CORDI.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01983v1-abstract-full').style.display = 'none'; document.getElementById('2403.01983v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to LREC-COLING 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.01901">arXiv:2403.01901</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.01901">pdf</a>, <a href="https://arxiv.org/format/2403.01901">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+C">Chao Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+J">Jiazheng Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Weida Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+M">Mingze Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dan%2C+J">Jun Dan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+T">Tianxin Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Siyuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Z">Zhi-Qi Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tai%2C+Y">Ying Tai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+B">Baigui Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.01901v2-abstract-short" style="display: inline;">
        In this paper, we abstract the process of people hearing <span class="search-hit mathjax">speech</span>, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple ide&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01901v2-abstract-full').style.display = 'inline'; document.getElementById('2403.01901v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.01901v2-abstract-full" style="display: none;">
        In this paper, we abstract the process of people hearing <span class="search-hit mathjax">speech</span>, extracting meaningful cues, and creating various dynamically audio-consistent talking faces, termed Listening and Imagining, into the task of high-fidelity diverse talking faces generation from a single audio. Specifically, it involves two critical challenges: one is to effectively decouple identity, content, and emotion from entangled audio, and the other is to maintain intra-video diversity and inter-video consistency. To tackle the issues, we first dig out the intricate relationships among facial factors and simplify the decoupling process, tailoring a Progressive Audio Disentanglement for accurate facial geometry and semantics learning, where each stage incorporates a customized training module responsible for a specific factor. Secondly, to achieve visually diverse and audio-synchronized animation solely from input audio within a single model, we introduce the Controllable Coherent Frame generation, which involves the flexible integration of three trainable adapters with frozen Latent Diffusion Models (LDMs) to focus on maintaining facial geometry and semantics, as well as texture and temporal coherence between frames. In this way, we inherit high-quality diverse generation from LDMs while significantly improving their controllability at a low training cost. Extensive experiments demonstrate the flexibility and effectiveness of our method in handling this paradigm. The codes will be released at https://github.com/modelscope/facechain.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01901v2-abstract-full').style.display = 'none'; document.getElementById('2403.01901v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.01369">arXiv:2403.01369</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.01369">pdf</a>, <a href="https://arxiv.org/format/2403.01369">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel <span class="search-hit mathjax">Speech</span> Enhancement
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shankar%2C+R">Ravi Shankar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+K">Ke Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+B">Buye Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+A">Anurag Kumar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.01369v1-abstract-short" style="display: inline;">
        Self-supervised learned models have been found to be very effective for certain <span class="search-hit mathjax">speech</span> tasks such as automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01369v1-abstract-full').style.display = 'inline'; document.getElementById('2403.01369v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.01369v1-abstract-full" style="display: none;">
        Self-supervised learned models have been found to be very effective for certain <span class="search-hit mathjax">speech</span> tasks such as automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, speaker identification, keyword spotting and others. While the features are undeniably useful in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and associated tasks, their utility in <span class="search-hit mathjax">speech</span> enhancement systems is yet to be firmly established, and perhaps not properly understood. In this paper, we investigate the uses of SSL representations for single-channel <span class="search-hit mathjax">speech</span> enhancement in challenging conditions and find that they add very little value for the enhancement task. Our constraints are designed around on-device real-time <span class="search-hit mathjax">speech</span> enhancement -- model is causal, the compute footprint is small. Additionally, we focus on low SNR conditions where such models struggle to provide good enhancement. In order to systematically examine how SSL representations impact performance of such enhancement models, we propose a variety of techniques to utilize these embeddings which include different forms of knowledge-distillation and pre-training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01369v1-abstract-full').style.display = 'none'; document.getElementById('2403.01369v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages; Shorter form accepted in ICASSP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.01309">arXiv:2403.01309</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.01309">pdf</a>, <a href="https://arxiv.org/format/2403.01309">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VNLP: Turkish NLP Package
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Turker%2C+M">Meliksah Turker</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ari%2C+M+E">Mehmet Erdi Ari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+A">Aydin Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.01309v1-abstract-short" style="display: inline;">
        &hellip;a novel architecture that is both an encoder and an auto-regressive model. NLP tasks solved by VNLP models include but are not limited to Sentiment Analysis, Named Entity <span class="search-hit mathjax">Recognition</span>, Morphological Analysis \&amp; Disambiguation and Part-of-<span class="search-hit mathjax">Speech</span> Tagging. Moreover, it comes with pre-trained word embeddings and corres&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01309v1-abstract-full').style.display = 'inline'; document.getElementById('2403.01309v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.01309v1-abstract-full" style="display: none;">
        In this work, we present VNLP: the first dedicated, complete, open-source, well-documented, lightweight, production-ready, state-of-the-art Natural Language Processing (NLP) package for the Turkish language. It contains a wide variety of tools, ranging from the simplest tasks, such as sentence splitting and text normalization, to the more advanced ones, such as text and token classification models. Its token classification models are based on &#34;Context Model&#34;, a novel architecture that is both an encoder and an auto-regressive model. NLP tasks solved by VNLP models include but are not limited to Sentiment Analysis, Named Entity <span class="search-hit mathjax">Recognition</span>, Morphological Analysis \&amp; Disambiguation and Part-of-<span class="search-hit mathjax">Speech</span> Tagging. Moreover, it comes with pre-trained word embeddings and corresponding SentencePiece Unigram tokenizers. VNLP has an open-source GitHub repository, ReadtheDocs documentation, PyPi package for convenient installation, Python and command-line API and a demo page to test all the functionality. Consequently, our main contribution is a complete, compact, easy-to-install and easy-to-use NLP package for Turkish.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01309v1-abstract-full').style.display = 'none'; document.getElementById('2403.01309v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.01255">arXiv:2403.01255</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.01255">pdf</a>, <a href="https://arxiv.org/format/2403.01255">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.inffus.2024.102422">10.1016/j.inffus.2024.102422 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> using Advanced Deep Learning Approaches: A survey
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kheddar%2C+H">Hamza Kheddar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hemis%2C+M">Mustapha Hemis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Himeur%2C+Y">Yassine Himeur</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.01255v2-abstract-short" style="display: inline;">
        Recent advancements in deep learning (DL) have posed a significant challenge for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). ASR relies on extensive training datasets, including confidential ones, and demands substantial computational and storage resources. Enabling adaptive systems improves ASR performance in dynamic environm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01255v2-abstract-full').style.display = 'inline'; document.getElementById('2403.01255v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.01255v2-abstract-full" style="display: none;">
        Recent advancements in deep learning (DL) have posed a significant challenge for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). ASR relies on extensive training datasets, including confidential ones, and demands substantial computational and storage resources. Enabling adaptive systems improves ASR performance in dynamic environments. DL techniques assume training and testing data originate from the same domain, which is not always true. Advanced DL techniques like deep transfer learning (DTL), federated learning (FL), and reinforcement learning (RL) address these issues. DTL allows high-performance models using small yet related datasets, FL enables training on confidential data without dataset possession, and RL optimizes decision-making in dynamic environments, reducing computation costs. This survey offers a comprehensive review of DTL, FL, and RL-based ASR frameworks, aiming to provide insights into the latest developments and aid researchers and professionals in understanding the current challenges. Additionally, transformers, which are advanced DL techniques heavily used in proposed ASR frameworks, are considered in this survey for their ability to capture extensive dependencies in the input ASR sequence. The paper starts by presenting the background of DTL, FL, RL, and Transformers and then adopts a well-designed taxonomy to outline the state-of-the-art approaches. Subsequently, a critical analysis is conducted to identify the strengths and weaknesses of each framework. Additionally, a comparative study is presented to highlight the existing challenges, paving the way for future research opportunities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01255v2-abstract-full').style.display = 'none'; document.getElementById('2403.01255v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Information Fusion, Elsevier, 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.01229">arXiv:2403.01229</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.01229">pdf</a>, <a href="https://arxiv.org/format/2403.01229">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Quiros%2C+J+V">Jose Vargas Quiros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raman%2C+C">Chirag Raman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+S">Stephanie Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gedik%2C+E">Ekin Gedik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cabrera-Quiros%2C+L">Laura Cabrera-Quiros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hung%2C+H">Hayley Hung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.01229v1-abstract-short" style="display: inline;">
        &hellip;scenarios due to cost, logistics, and privacy concerns. As an alternative, machine learning models trained on video and wearable sensor data make it possible to recognize <span class="search-hit mathjax">speech</span> by detecting its related gestures in an unobtrusive, privacy-preserving way. These models themselves should ideally be trained using labels obtained from the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01229v1-abstract-full').style.display = 'inline'; document.getElementById('2403.01229v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.01229v1-abstract-full" style="display: none;">
        Recognizing speaking in humans is a central task towards understanding social interactions. Ideally, speaking would be detected from individual voice recordings, as done previously for meeting scenarios. However, individual voice recordings are hard to obtain in the wild, especially in crowded mingling scenarios due to cost, logistics, and privacy concerns. As an alternative, machine learning models trained on video and wearable sensor data make it possible to recognize <span class="search-hit mathjax">speech</span> by detecting its related gestures in an unobtrusive, privacy-preserving way. These models themselves should ideally be trained using labels obtained from the <span class="search-hit mathjax">speech</span> signal. However, existing mingling datasets do not contain high quality audio recordings. Instead, speaking status annotations have often been inferred by human annotators from video, without validation of this approach against audio-based ground truth. In this paper we revisit no-audio speaking status estimation by presenting the first publicly available multimodal dataset with high-quality individual <span class="search-hit mathjax">speech</span> recordings of 33 subjects in a professional networking event. We present three baselines for no-audio speaking status segmentation: a) from video, b) from body acceleration (chest-worn accelerometer), c) from body pose tracks. In all cases we predict a 20Hz binary speaking status signal extracted from the audio, a time resolution not available in previous datasets. In addition to providing the signals and ground truth necessary to evaluate a wide range of speaking status detection methods, the availability of audio in REWIND makes it suitable for cross-modality studies not feasible with previous mingling datasets. Finally, our flexible data consent setup creates new challenges for multimodal systems under missing modalities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01229v1-abstract-full').style.display = 'none'; document.getElementById('2403.01229v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.01087">arXiv:2403.01087</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.01087">pdf</a>, <a href="https://arxiv.org/format/2403.01087">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3581783.3611787">10.1145/3581783.3611787 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Accurate Lip-to-<span class="search-hit mathjax">Speech</span> Synthesis in-the-Wild
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hegde%2C+S">Sindhu Hegde</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mukhopadhyay%2C+R">Rudrabha Mukhopadhyay</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jawahar%2C+C+V">C. V. Jawahar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Namboodiri%2C+V">Vinay Namboodiri</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.01087v1-abstract-short" style="display: inline;">
        In this paper, we introduce a novel approach to address the task of synthesizing <span class="search-hit mathjax">speech</span> from silent videos of any in-the-wild speaker solely based on lip movements. The traditional approach of directly generating&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01087v1-abstract-full').style.display = 'inline'; document.getElementById('2403.01087v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.01087v1-abstract-full" style="display: none;">
        In this paper, we introduce a novel approach to address the task of synthesizing <span class="search-hit mathjax">speech</span> from silent videos of any in-the-wild speaker solely based on lip movements. The traditional approach of directly generating <span class="search-hit mathjax">speech</span> from lip videos faces the challenge of not being able to learn a robust language model from <span class="search-hit mathjax">speech</span> alone, resulting in unsatisfactory outcomes. To overcome this issue, we propose incorporating noisy text supervision using a state-of-the-art lip-to-text network that instills language information into our model. The noisy text is generated using a pre-trained lip-to-text model, enabling our approach to work without text annotations during inference. We design a visual text-to-<span class="search-hit mathjax">speech</span> network that utilizes the visual stream to generate accurate <span class="search-hit mathjax">speech</span>, which is in-sync with the silent input video. We perform extensive experiments and ablation studies, demonstrating our approach&#39;s superiority over the current state-of-the-art methods on various benchmark datasets. Further, we demonstrate an essential practical application of our method in assistive technology by generating <span class="search-hit mathjax">speech</span> for an ALS patient who has lost the voice but can make mouth movements. Our demo video, code, and additional details can be found at \url{http://cvit.iiit.ac.in/research/projects/cvit-projects/ms-l2s-itw}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.01087v1-abstract-full').style.display = 'none'; document.getElementById('2403.01087v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages of content, 1 page of references and 4 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        In Proceedings of the 31st ACM International Conference on Multimedia, 2023
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.00370">arXiv:2403.00370</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.00370">pdf</a>, <a href="https://arxiv.org/format/2403.00370">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Post-decoder Biasing for End-to-End <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> of Multi-turn Medical Interview
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Heyang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yanfeng Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.00370v1-abstract-short" style="display: inline;">
        End-to-end (E2E) approach is gradually replacing hybrid models for automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.00370v1-abstract-full').style.display = 'inline'; document.getElementById('2403.00370v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.00370v1-abstract-full" style="display: none;">
        End-to-end (E2E) approach is gradually replacing hybrid models for automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) tasks. However, the optimization of E2E models lacks an intuitive method for handling decoding shifts, especially in scenarios with a large number of domain-specific rare words that hold specific important meanings. Furthermore, the absence of knowledge-intensive <span class="search-hit mathjax">speech</span> datasets in academia has been a significant limiting factor, and the commonly used <span class="search-hit mathjax">speech</span> corpora exhibit significant disparities with realistic conversation. To address these challenges, we present Medical Interview (MED-IT), a multi-turn consultation <span class="search-hit mathjax">speech</span> dataset that contains a substantial number of knowledge-intensive named entities. We also explore methods to enhance the <span class="search-hit mathjax">recognition</span> performance of rare words for E2E models. We propose a novel approach, post-decoder biasing, which constructs a transform probability matrix based on the distribution of training transcriptions. This guides the model to prioritize recognizing words in the biasing list. In our experiments, for subsets of rare words appearing in the training <span class="search-hit mathjax">speech</span> between 10 and 20 times, and between 1 and 5 times, the proposed method achieves a relative improvement of 9.3% and 5.1%, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.00370v1-abstract-full').style.display = 'none'; document.getElementById('2403.00370v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.00274">arXiv:2403.00274</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.00274">pdf</a>, <a href="https://arxiv.org/format/2403.00274">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CustomListener: Text-guided Responsive Interaction for User-friendly Listening Head Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Ying Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhen%2C+C">Cheng Zhen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+T">Tong Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ao%2C+Y">Yingying Ao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+P">Pengfei Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.00274v2-abstract-short" style="display: inline;">
        Listening head generation aims to synthesize a non-verbal responsive listener head by modeling the correlation between the speaker and the listener in dynamic conversion.The applications of listener agent generation in virtual interaction have promoted many works achieving the diverse and fine-grained motion generation. However, they can only manipulate motions through simple emotional labels, but&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.00274v2-abstract-full').style.display = 'inline'; document.getElementById('2403.00274v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.00274v2-abstract-full" style="display: none;">
        Listening head generation aims to synthesize a non-verbal responsive listener head by modeling the correlation between the speaker and the listener in dynamic conversion.The applications of listener agent generation in virtual interaction have promoted many works achieving the diverse and fine-grained motion generation. However, they can only manipulate motions through simple emotional labels, but cannot freely control the listener&#39;s motions. Since listener agents should have human-like attributes (e.g. identity, personality) which can be freely customized by users, this limits their realism. In this paper, we propose a user-friendly framework called CustomListener to realize the free-form text prior guided listener generation. To achieve speaker-listener coordination, we design a Static to Dynamic Portrait module (SDP), which interacts with speaker information to transform static text into dynamic portrait token with completion rhythm and amplitude information. To achieve coherence between segments, we design a Past Guided Generation Module (PGG) to maintain the consistency of customized listener attributes through the motion prior, and utilize a diffusion-based structure conditioned on the portrait token and the motion prior to realize the controllable generation. To train and evaluate our model, we have constructed two text-annotated listening head datasets based on ViCo and RealTalk, which provide text-video paired labels. Extensive experiments have verified the effectiveness of our model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.00274v2-abstract-full').style.display = 'none'; document.getElementById('2403.00274v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by CVPR 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.00212">arXiv:2403.00212</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.00212">pdf</a>, <a href="https://arxiv.org/format/2403.00212">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transcription and translation of videos using fine-tuned XLSR Wav2Vec2 on custom dataset and mBART
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tathe%2C+A">Aniket Tathe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kamble%2C+A">Anand Kamble</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumbharkar%2C+S">Suyash Kumbharkar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhandare%2C+A">Atharva Bhandare</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mitra%2C+A+C">Anirban C. Mitra</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.00212v1-abstract-short" style="display: inline;">
        This research addresses the challenge of training an ASR model for personalized voices with minimal data. Utilizing just 14 minutes of custom audio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to create a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual Self-supervised Representations (XLSR) Wav2Vec2 model is fine-tuned on this dataset. The developed web-bas&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.00212v1-abstract-full').style.display = 'inline'; document.getElementById('2403.00212v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.00212v1-abstract-full" style="display: none;">
        This research addresses the challenge of training an ASR model for personalized voices with minimal data. Utilizing just 14 minutes of custom audio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to create a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual Self-supervised Representations (XLSR) Wav2Vec2 model is fine-tuned on this dataset. The developed web-based GUI efficiently transcribes and translates input Hindi videos. By integrating XLSR Wav2Vec2 and mBART, the system aligns the translated text with the video timeline, delivering an accessible solution for multilingual video content transcription and translation for personalized voice.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.00212v1-abstract-full').style.display = 'none'; document.getElementById('2403.00212v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.19443">arXiv:2402.19443</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.19443">pdf</a>, <a href="https://arxiv.org/format/2402.19443">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Probing the Information Encoded in Neural-based Acoustic Models of Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Raymondaud%2C+Q">Quentin Raymondaud</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rouvier%2C+M">Mickael Rouvier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dufour%2C+R">Richard Dufour</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.19443v1-abstract-short" style="display: inline;">
        Deep learning architectures have made significant progress in terms of performance in many research areas. The automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.19443v1-abstract-full').style.display = 'inline'; document.getElementById('2402.19443v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.19443v1-abstract-full" style="display: none;">
        Deep learning architectures have made significant progress in terms of performance in many research areas. The automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) field has thus benefited from these scientific and technological advances, particularly for acoustic modeling, now integrating deep neural network architectures. However, these performance gains have translated into increased complexity regarding the information learned and conveyed through these black-box architectures. Following many researches in neural networks interpretability, we propose in this article a protocol that aims to determine which and where information is located in an ASR acoustic model (AM). To do so, we propose to evaluate AM performance on a determined set of tasks using intermediate representations (here, at different layer levels). Regarding the performance variation and targeted tasks, we can emit hypothesis about which information is enhanced or perturbed at different architecture steps. Experiments are performed on both speaker verification, acoustic environment classification, gender classification, tempo-distortion detection systems and <span class="search-hit mathjax">speech</span> sentiment/emotion identification. Analysis showed that neural-based AMs hold heterogeneous information that seems surprisingly uncorrelated with phoneme <span class="search-hit mathjax">recognition</span>, such as emotion, sentiment or speaker identity. The low-level hidden layers globally appears useful for the structuring of information while the upper ones would tend to delete useless information for phoneme <span class="search-hit mathjax">recognition</span>.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.19443v1-abstract-full').style.display = 'none'; document.getElementById('2402.19443v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.18923">arXiv:2402.18923</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.18923">pdf</a>, <a href="https://arxiv.org/format/2402.18923">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Inappropriate Pause Detection In Dysarthric <span class="search-hit mathjax">Speech</span> Using Large-Scale <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+J">Jeehyun Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Choi%2C+Y">Yerin Choi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+T">Tae-Jin Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koo%2C+M">Myoung-Wan Koo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.18923v1-abstract-short" style="display: inline;">
        Dysarthria, a common issue among stroke patients, severely impacts <span class="search-hit mathjax">speech</span> intelligibility. Inappropriate pauses are crucial indicators in severity assessment and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.18923v1-abstract-full').style.display = 'inline'; document.getElementById('2402.18923v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.18923v1-abstract-full" style="display: none;">
        Dysarthria, a common issue among stroke patients, severely impacts <span class="search-hit mathjax">speech</span> intelligibility. Inappropriate pauses are crucial indicators in severity assessment and <span class="search-hit mathjax">speech</span>-language therapy. We propose to extend a large-scale <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> model for inappropriate pause detection in dysarthric <span class="search-hit mathjax">speech</span>. To this end, we propose task design, labeling strategy, and a <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> model with an inappropriate pause prediction layer. First, we treat pause detection as <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, using an automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) model to convert <span class="search-hit mathjax">speech</span> into text with pause tags. According to the newly designed task, we label pause locations at the text level and their appropriateness. We collaborate with <span class="search-hit mathjax">speech</span>-language pathologists to establish labeling criteria, ensuring high-quality annotated data. Finally, we extend the ASR model with an inappropriate pause prediction layer for end-to-end inappropriate pause detection. Moreover, we propose a task-tailored metric for evaluating inappropriate pause detection independent of ASR performance. Our experiments show that the proposed method better detects inappropriate pauses in dysarthric <span class="search-hit mathjax">speech</span> than baselines. (Inappropriate Pause Error Rate: 14.47%)
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.18923v1-abstract-full').style.display = 'none'; document.getElementById('2402.18923v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICASSP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.18302">arXiv:2402.18302</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.18302">pdf</a>, <a href="https://arxiv.org/format/2402.18302">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+J">Jiacheng Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jiajun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+K">Kunyu Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+X">Xuan He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhiyong Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stiefelhagen%2C+R">Rainer Stiefelhagen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+K">Kailun Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.18302v2-abstract-short" style="display: inline;">
        This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cos&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.18302v2-abstract-full').style.display = 'inline'; document.getElementById('2402.18302v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.18302v2-abstract-full" style="display: none;">
        This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving. In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking. We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers. The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from both frequency- and spatiotemporal domains. Moreover, we propose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract homogeneous semantic features between expressions and visual objects by learning homogeneous features between different audio and video objects effectively. Aside from the architectural design, we establish the first set of large-scale AR-MOT benchmarks, including Echo-KITTI, Echo-KITTI+, and Echo-BDD. Extensive experiments on the established benchmarks demonstrate the effectiveness of the proposed EchoTrack and its components. The source code and datasets are available at https://github.com/lab206/EchoTrack.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.18302v2-abstract-full').style.display = 'none'; document.getElementById('2402.18302v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to IEEE Transactions on Intelligent Transportation Systems (T-ITS). The source code and datasets are available at https://github.com/lab206/EchoTrack</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.18275">arXiv:2402.18275</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.18275">pdf</a>, <a href="https://arxiv.org/format/2402.18275">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploration of Adapter for Noise Robust Automatic <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+H">Hao Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kawahara%2C+T">Tatsuya Kawahara</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.18275v3-abstract-short" style="display: inline;">
        Adapting an automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.18275v3-abstract-full').style.display = 'inline'; document.getElementById('2402.18275v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.18275v3-abstract-full" style="display: none;">
        Adapting an automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) system to unseen noise environments is crucial. Integrating adapters into neural networks has emerged as a potent technique for transfer learning. This study thoroughly investigates adapter-based ASR adaptation in noisy environments. We conducted experiments using the CHiME--4 dataset. The results show that inserting the adapter in the shallow layer yields superior effectiveness, and there is no significant difference between adapting solely within the shallow layer and adapting across all layers. The simulated data helps the system to improve its performance under real noise conditions. Nonetheless, when the amount of data is the same, the real data is more effective than the simulated data. Multi-condition training is still useful for adapter training. Furthermore, integrating adapters into <span class="search-hit mathjax">speech</span> enhancement-based ASR systems yields substantial improvements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.18275v3-abstract-full').style.display = 'none'; document.getElementById('2402.18275v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.18175">arXiv:2402.18175</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.18175">pdf</a>, <a href="https://arxiv.org/format/2402.18175">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Supervised Spatially Variant PSF Estimation for Aberration-Aware Depth-from-Defocus
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zhuofeng Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Monno%2C+Y">Yusuke Monno</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Okutomi%2C+M">Masatoshi Okutomi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.18175v1-abstract-short" style="display: inline;">
        In this paper, we address the task of aberration-aware depth-from-defocus (DfD), which takes account of spatially variant point spread functions (PSFs) of a real camera. To effectively obtain the spatially variant PSFs of a real camera without requiring any ground-truth PSFs, we propose a novel self-supervised learning method that leverages the pair of real sharp and blurred images, which can be e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.18175v1-abstract-full').style.display = 'inline'; document.getElementById('2402.18175v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.18175v1-abstract-full" style="display: none;">
        In this paper, we address the task of aberration-aware depth-from-defocus (DfD), which takes account of spatially variant point spread functions (PSFs) of a real camera. To effectively obtain the spatially variant PSFs of a real camera without requiring any ground-truth PSFs, we propose a novel self-supervised learning method that leverages the pair of real sharp and blurred images, which can be easily captured by changing the aperture setting of the camera. In our PSF estimation, we assume rotationally symmetric PSFs and introduce the polar coordinate system to more accurately learn the PSF estimation network. We also handle the focus breathing phenomenon that occurs in real DfD situations. Experimental results on synthetic and real data demonstrate the effectiveness of our method regarding both the PSF estimation and the depth estimation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.18175v1-abstract-full').style.display = 'none'; document.getElementById('2402.18175v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        International Conference on Acoustics, <span class="search-hit mathjax">Speech</span>, and Signal Processing (ICASSP), 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.18007">arXiv:2402.18007</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.18007">pdf</a>, <a href="https://arxiv.org/format/2402.18007">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mixer is more than just a model
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+Q">Qingfeng Ji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuxin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+L">Letong Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.18007v2-abstract-short" style="display: inline;">
        &hellip;considerations, it is possible to create more tailored mixers from various perspectives to better suit specific task requirements. This study focuses on the domain of audio <span class="search-hit mathjax">recognition</span>, introducing a novel model named Audio Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates insights from both time and frequency domains. Experimental r&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.18007v2-abstract-full').style.display = 'inline'; document.getElementById('2402.18007v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.18007v2-abstract-full" style="display: none;">
        Recently, MLP structures have regained popularity, with MLP-Mixer standing out as a prominent example. In the field of computer vision, MLP-Mixer is noted for its ability to extract data information from both channel and token perspectives, effectively acting as a fusion of channel and token information. Indeed, Mixer represents a paradigm for information extraction that amalgamates channel and token information. The essence of Mixer lies in its ability to blend information from diverse perspectives, epitomizing the true concept of &#34;mixing&#34; in the realm of neural network architectures. Beyond channel and token considerations, it is possible to create more tailored mixers from various perspectives to better suit specific task requirements. This study focuses on the domain of audio <span class="search-hit mathjax">recognition</span>, introducing a novel model named Audio Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates insights from both time and frequency domains. Experimental results demonstrate that ASM-RH is particularly well-suited for audio data and yields promising outcomes across multiple classification tasks. The models and optimal weights files will be published.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.18007v2-abstract-full').style.display = 'none'; document.getElementById('2402.18007v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.17954">arXiv:2402.17954</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.17954">pdf</a>, <a href="https://arxiv.org/format/2402.17954">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Twists, Humps, and Pebbles: Multilingual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Models Exhibit Gender Performance Gaps
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Attanasio%2C+G">Giuseppe Attanasio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Savoldi%2C+B">Beatrice Savoldi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fucci%2C+D">Dennis Fucci</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hovy%2C+D">Dirk Hovy</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.17954v3-abstract-short" style="display: inline;">
        Current automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models are designed to be used across many languages and tasks without substantial changes. However, this broad language coverage hides performance gaps within languages, for example, across genders. Our study systematically evaluates the performance of two widely used multili&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17954v3-abstract-full').style.display = 'inline'; document.getElementById('2402.17954v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.17954v3-abstract-full" style="display: none;">
        Current automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models are designed to be used across many languages and tasks without substantial changes. However, this broad language coverage hides performance gaps within languages, for example, across genders. Our study systematically evaluates the performance of two widely used multilingual ASR models on three datasets, encompassing 19 languages from eight language families and two speaking conditions. Our findings reveal clear gender disparities, with the advantaged group varying across languages and models. Surprisingly, those gaps are not explained by acoustic or lexical properties. However, probing internal model states reveals a correlation with gendered performance gap. That is, the easier it is to distinguish speaker gender in a language using probes, the more the gap reduces, favoring female speakers. Our results show that gender disparities persist even in state-of-the-art models. Our findings have implications for the improvement of multilingual ASR systems, underscoring the importance of accessibility to training data and nuanced evaluation to predict and mitigate gender gaps. We release all code and artifacts at https://github.com/g8a9/multilingual-asr-gender-gap.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17954v3-abstract-full').style.display = 'none'; document.getElementById('2402.17954v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at EMNLP 2024. Code and artifacts at https://github.com/g8a9/multilingual-asr-gender-gap</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.17775">arXiv:2402.17775</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.17775">pdf</a>, <a href="https://arxiv.org/format/2402.17775">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        WhaleNet: a Novel Deep Learning Architecture for Marine Mammals Vocalizations on Watkins Marine Mammal Sound Database
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Licciardi%2C+A">Alessandro Licciardi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carbone%2C+D">Davide Carbone</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.17775v2-abstract-short" style="display: inline;">
        Marine mammal communication is a complex field, hindered by the diversity of vocalizations and environmental factors. The Watkins Marine Mammal Sound Database (WMMD) constitutes a comprehensive labeled dataset employed in machine learning applications. Nevertheless, the methodologies for data preparation, preprocessing, and classification documented in the literature exhibit considerable variabili&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17775v2-abstract-full').style.display = 'inline'; document.getElementById('2402.17775v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.17775v2-abstract-full" style="display: none;">
        Marine mammal communication is a complex field, hindered by the diversity of vocalizations and environmental factors. The Watkins Marine Mammal Sound Database (WMMD) constitutes a comprehensive labeled dataset employed in machine learning applications. Nevertheless, the methodologies for data preparation, preprocessing, and classification documented in the literature exhibit considerable variability and are typically not applied to the dataset in its entirety. This study initially undertakes a concise review of the state-of-the-art benchmarks pertaining to the dataset, with a particular focus on clarifying data preparation and preprocessing techniques. Subsequently, we explore the utilization of the Wavelet Scattering Transform (WST) and Mel spectrogram as preprocessing mechanisms for feature extraction. In this paper, we introduce \textbf{WhaleNet} (Wavelet Highly Adaptive Learning Ensemble Network), a sophisticated deep ensemble architecture for the classification of marine mammal vocalizations, leveraging both WST and Mel spectrogram for enhanced feature discrimination. By integrating the insights derived from WST and Mel representations, we achieved an improvement in classification accuracy by $8-10\%$ over existing architectures, corresponding to a classification accuracy of $97.61\%$.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17775v2-abstract-full').style.display = 'none'; document.getElementById('2402.17775v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.17723">arXiv:2402.17723</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.17723">pdf</a>, <a href="https://arxiv.org/format/2402.17723">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+Y">Yazhou Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Y">Yingqing He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+Z">Zeyue Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xintao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Q">Qifeng Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.17723v1-abstract-short" style="display: inline;">
        Video and audio content creation serves as the core technique for the movie industry and professional users. Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry. In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-au&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17723v1-abstract-full').style.display = 'inline'; document.getElementById('2402.17723v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.17723v1-abstract-full" style="display: none;">
        Video and audio content creation serves as the core technique for the movie industry and professional users. Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry. In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-audio generation. We observe the powerful generation ability of off-the-shelf video or audio generation models. Thus, instead of training the giant models from scratch, we propose to bridge the existing strong models with a shared latent representation space. Specifically, we propose a multimodality latent aligner with the pre-trained ImageBind model. Our latent aligner shares a similar core as the classifier guidance that guides the diffusion denoising process during inference time. Through carefully designed optimization strategy and loss functions, we show the superior performance of our method on joint video-audio generation, visual-steered audio generation, and audio-steered visual generation tasks. The project website can be found at https://yzxing87.github.io/Seeing-and-Hearing/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17723v1-abstract-full').style.display = 'none'; document.getElementById('2402.17723v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to CVPR 2024. Project website: https://yzxing87.github.io/Seeing-and-Hearing/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.17496">arXiv:2402.17496</a>
        <span>&nbsp;&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Emotional Voice Messages (EMOVOME) database: emotion <span class="search-hit mathjax">recognition</span> in spontaneous voice messages
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zaragoz%C3%A1%2C+L+G">Lucía Gómez Zaragozá</a>, 
      
      <a href="/search/?searchtype=author&amp;query=del+Amor%2C+R">Rocío del Amor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vargas%2C+E+P">Elena Parra Vargas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Naranjo%2C+V">Valery Naranjo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raya%2C+M+A">Mariano Alcañiz Raya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mar%C3%ADn-Morales%2C+J">Javier Marín-Morales</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.17496v2-abstract-short" style="display: inline;">
        Emotional Voice Messages (EMOVOME) is a spontaneous <span class="search-hit mathjax">speech</span> dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. Audios were labeled in valenc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17496v2-abstract-full').style.display = 'inline'; document.getElementById('2402.17496v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.17496v2-abstract-full" style="display: none;">
        Emotional Voice Messages (EMOVOME) is a spontaneous <span class="search-hit mathjax">speech</span> dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension. The experts also provided an extra label corresponding to seven emotion categories. To set a baseline for future investigations using EMOVOME, we implemented emotion <span class="search-hit mathjax">recognition</span> models using both <span class="search-hit mathjax">speech</span> and audio transcriptions. For <span class="search-hit mathjax">speech</span>, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively. For text, we fine-tuned a multilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for valence and arousal respectively. This database will significantly contribute to research on emotion <span class="search-hit mathjax">recognition</span> in the wild, while also providing a unique natural and freely accessible resource for Spanish.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17496v2-abstract-full').style.display = 'none'; document.getElementById('2402.17496v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper has been superseded by arXiv:2403.02167 (merged from the description of the EMOVOME database in arXiv:2402.17496v1 and the <span class="search-hit mathjax">speech</span> emotion <span class="search-hit mathjax">recognition</span> models in arXiv:2403.02167v1)</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.5.1; I.5.4; I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.17482">arXiv:2402.17482</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.17482">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.5220/0012592700003657">10.5220/0012592700003657 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automated Classification of Phonetic Segments in Child <span class="search-hit mathjax">Speech</span> Using Raw Ultrasound Imaging
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ani%2C+S+A">Saja Al Ani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cleland%2C+J">Joanne Cleland</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zoha%2C+A">Ahmed Zoha</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.17482v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> sound disorder (SSD) is defined as a persistent impairment in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17482v1-abstract-full').style.display = 'inline'; document.getElementById('2402.17482v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.17482v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> sound disorder (SSD) is defined as a persistent impairment in <span class="search-hit mathjax">speech</span> sound production leading to reduced <span class="search-hit mathjax">speech</span> intelligibility and hindered verbal communication. Early <span class="search-hit mathjax">recognition</span> and intervention of children with SSD and timely referral to <span class="search-hit mathjax">speech</span> and language therapists (SLTs) for treatment are crucial. Automated detection of <span class="search-hit mathjax">speech</span> impairment is regarded as an efficient method for examining and screening large populations. This study focuses on advancing the automatic diagnosis of SSD in early childhood by proposing a technical solution that integrates ultrasound tongue imaging (UTI) with deep-learning models. The introduced FusionNet model combines UTI data with the extracted texture features to classify UTI. The overarching aim is to elevate the accuracy and efficiency of UTI analysis, particularly for classifying <span class="search-hit mathjax">speech</span> sounds associated with SSD. This study compared the FusionNet approach with standard deep-learning methodologies, highlighting the excellent improvement results of the FusionNet model in UTI classification and the potential of multi-learning in improving UTI classification in <span class="search-hit mathjax">speech</span> therapy clinics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17482v1-abstract-full').style.display = 'none'; document.getElementById('2402.17482v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 17th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 1: BIOIMAGING, 2024, pages 326-331
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.17249">arXiv:2402.17249</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.17249">pdf</a>, <a href="https://arxiv.org/format/2402.17249">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Learning-Based <span class="search-hit mathjax">Speech</span> and Vision Synthesis to Improve Phishing Attack Detection through a Multi-layer Adaptive Framework
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ige%2C+T">Tosin Ige</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kiekintveld%2C+C">Christopher Kiekintveld</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Piplai%2C+A">Aritran Piplai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.17249v1-abstract-short" style="display: inline;">
        &hellip;tactics are being developed to evade detection. In this research, we proposed an adaptable framework that combines Deep learning and Randon Forest to read images, synthesize <span class="search-hit mathjax">speech</span> from deep-fake videos, and natural language processing at various predictions layered to significantly increase the performance of machine learning models for phishing attack dete&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17249v1-abstract-full').style.display = 'inline'; document.getElementById('2402.17249v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.17249v1-abstract-full" style="display: none;">
        The ever-evolving ways attacker continues to im prove their phishing techniques to bypass existing state-of-the-art phishing detection methods pose a mountain of challenges to researchers in both industry and academia research due to the inability of current approaches to detect complex phishing attack. Thus, current anti-phishing methods remain vulnerable to complex phishing because of the increasingly sophistication tactics adopted by attacker coupled with the rate at which new tactics are being developed to evade detection. In this research, we proposed an adaptable framework that combines Deep learning and Randon Forest to read images, synthesize <span class="search-hit mathjax">speech</span> from deep-fake videos, and natural language processing at various predictions layered to significantly increase the performance of machine learning models for phishing attack detection.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17249v1-abstract-full').style.display = 'none'; document.getElementById('2402.17249v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.17189">arXiv:2402.17189</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.17189">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Effective Mixture-Of-Experts Approach For Code-Switching <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> Leveraging Encoder Disentanglement
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+T">Tzu-Ting Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hsin-Wei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yi-Cheng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+C">Chi-Han Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+B">Berlin Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.17189v1-abstract-short" style="display: inline;">
        With the massive developments of end-to-end (E2E) neural networks, recent years have witnessed unprecedented breakthroughs in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). However, the codeswitching phenomenon remains a major obstacle that hinders ASR from perfection, as the lack of labeled data and the variations between langua&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17189v1-abstract-full').style.display = 'inline'; document.getElementById('2402.17189v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.17189v1-abstract-full" style="display: none;">
        With the massive developments of end-to-end (E2E) neural networks, recent years have witnessed unprecedented breakthroughs in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR). However, the codeswitching phenomenon remains a major obstacle that hinders ASR from perfection, as the lack of labeled data and the variations between languages often lead to degradation of ASR performance. In this paper, we focus exclusively on improving the acoustic encoder of E2E ASR to tackle the challenge caused by the codeswitching phenomenon. Our main contributions are threefold: First, we introduce a novel disentanglement loss to enable the lower-layer of the encoder to capture inter-lingual acoustic information while mitigating linguistic confusion at the higher-layer of the encoder. Second, through comprehensive experiments, we verify that our proposed method outperforms the prior-art methods using pretrained dual-encoders, meanwhile having access only to the codeswitching corpus and consuming half of the parameterization. Third, the apparent differentiation of the encoders&#39; output features also corroborates the complementarity between the disentanglement loss and the mixture-of-experts (MoE) architecture.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17189v1-abstract-full').style.display = 'none'; document.getElementById('2402.17189v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICASSP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.17184">arXiv:2402.17184</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.17184">pdf</a>, <a href="https://arxiv.org/format/2402.17184">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Extreme Encoder Output Frame Rate Reduction: Improving Computational Latencies of Large End-to-End Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Prabhavalkar%2C+R">Rohit Prabhavalkar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+Z">Zhong Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Weiran Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stooke%2C+A">Adam Stooke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+X">Xingyu Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Y">Yanzhang He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Narayanan%2C+A">Arun Narayanan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hwang%2C+D">Dongseong Hwang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sainath%2C+T+N">Tara N. Sainath</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moreno%2C+P+J">Pedro J. Moreno</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.17184v1-abstract-short" style="display: inline;">
        The accuracy of end-to-end (E2E) automatic <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17184v1-abstract-full').style.display = 'inline'; document.getElementById('2402.17184v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.17184v1-abstract-full" style="display: none;">
        The accuracy of end-to-end (E2E) automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models continues to improve as they are scaled to larger sizes, with some now reaching billions of parameters. Widespread deployment and adoption of these models, however, requires computationally efficient strategies for decoding. In the present work, we study one such strategy: applying multiple frame reduction layers in the encoder to compress encoder outputs into a small number of output frames. While similar techniques have been investigated in previous work, we achieve dramatically more reduction than has previously been demonstrated through the use of multiple funnel reduction layers. Through ablations, we study the impact of various architectural choices in the encoder to identify the most effective strategies. We demonstrate that we can generate one encoder output frame for every 2.56 sec of input <span class="search-hit mathjax">speech</span>, without significantly affecting word error rate on a large-scale voice search task, while improving encoder and decoder latencies by 48% and 92% respectively, relative to a strong but computationally expensive baseline.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.17184v1-abstract-full').style.display = 'none'; document.getElementById('2402.17184v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to 2024 IEEE International Conference on Acoustics, <span class="search-hit mathjax">Speech</span>, and Signal Processing (ICASSP 2024)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.16124">arXiv:2402.16124</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.16124">pdf</a>, <a href="https://arxiv.org/format/2402.16124">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Y">Yasheng Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+W">Wenqing Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+H">Hang Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+K">Kaisiyuan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koike%2C+H">Hideki Koike</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.16124v1-abstract-short" style="display: inline;">
        While considerable progress has been made in achieving accurate lip synchronization for 3D <span class="search-hit mathjax">speech</span>-driven talking face generation, the task of incorporating expressive facial detail synthesis aligned with the speaker&#39;s speaking status remains challenging. Our goal is to directly leverage the inherent style information conveyed by human&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.16124v1-abstract-full').style.display = 'inline'; document.getElementById('2402.16124v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.16124v1-abstract-full" style="display: none;">
        While considerable progress has been made in achieving accurate lip synchronization for 3D <span class="search-hit mathjax">speech</span>-driven talking face generation, the task of incorporating expressive facial detail synthesis aligned with the speaker&#39;s speaking status remains challenging. Our goal is to directly leverage the inherent style information conveyed by human <span class="search-hit mathjax">speech</span> for generating an expressive talking face that aligns with the speaking status. In this paper, we propose AVI-Talking, an Audio-Visual Instruction system for expressive Talking face generation. This system harnesses the robust contextual reasoning and hallucination capability offered by Large Language Models (LLMs) to instruct the realistic synthesis of 3D talking faces. Instead of directly learning facial movements from human <span class="search-hit mathjax">speech</span>, our two-stage strategy involves the LLMs first comprehending audio information and generating instructions implying expressive facial details seamlessly corresponding to the <span class="search-hit mathjax">speech</span>. Subsequently, a diffusion-based generative network executes these instructions. This two-stage process, coupled with the incorporation of LLMs, enhances model interpretability and provides users with flexibility to comprehend instructions and specify desired operations or modifications. Extensive experiments showcase the effectiveness of our approach in producing vivid talking faces with expressive facial movements and consistent emotional status.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.16124v1-abstract-full').style.display = 'none'; document.getElementById('2402.16124v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.16021">arXiv:2402.16021</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.16021">pdf</a>, <a href="https://arxiv.org/format/2402.16021">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TMT: Tri-Modal Translation between <span class="search-hit mathjax">Speech</span>, Image, and Text by Processing Different Modalities as Different Languages
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+M">Minsu Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jung%2C+J">Jee-weon Jung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rha%2C+H">Hyeongseop Rha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Maiti%2C+S">Soumi Maiti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arora%2C+S">Siddhant Arora</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+X">Xuankai Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ro%2C+Y+M">Yong Man Ro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.16021v1-abstract-short" style="display: inline;">
        &hellip;requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning <span class="search-hit mathjax">speech</span>, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine transla&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.16021v1-abstract-full').style.display = 'inline'; document.getElementById('2402.16021v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.16021v1-abstract-full" style="display: none;">
        The capability to jointly process multi-modal information is becoming an essential task. However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning <span class="search-hit mathjax">speech</span>, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem. To this end, we tokenize <span class="search-hit mathjax">speech</span> and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages. We evaluate the proposed TMT on all six modality translation tasks. TMT outperforms single model counterparts consistently, demonstrating that unifying tasks is beneficial not only for practicality but also for performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.16021v1-abstract-full').style.display = 'none'; document.getElementById('2402.16021v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.15858">arXiv:2402.15858</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.15858">pdf</a>, <a href="https://arxiv.org/format/2402.15858">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FedMM: Federated Multi-Modal Learning with Modality Heterogeneity in Computational Pathology
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+Y">Yuanzhe Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bian%2C+J">Jieming Bian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Jie Xu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.15858v1-abstract-short" style="display: inline;">
        The fusion of complementary multimodal information is crucial in computational pathology for accurate diagnostics. However, existing multimodal learning approaches necessitate access to users&#39; raw data, posing substantial privacy risks. While Federated Learning (FL) serves as a privacy-preserving alternative, it falls short in addressing the challenges posed by heterogeneous (yet possibly overlapp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.15858v1-abstract-full').style.display = 'inline'; document.getElementById('2402.15858v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.15858v1-abstract-full" style="display: none;">
        The fusion of complementary multimodal information is crucial in computational pathology for accurate diagnostics. However, existing multimodal learning approaches necessitate access to users&#39; raw data, posing substantial privacy risks. While Federated Learning (FL) serves as a privacy-preserving alternative, it falls short in addressing the challenges posed by heterogeneous (yet possibly overlapped) modalities data across various hospitals. To bridge this gap, we propose a Federated Multi-Modal (FedMM) learning framework that federatedly trains multiple single-modal feature extractors to enhance subsequent classification performance instead of existing FL that aims to train a unified multimodal fusion model. Any participating hospital, even with small-scale datasets or limited devices, can leverage these federated trained extractors to perform local downstream tasks (e.g., classification) while ensuring data privacy. Through comprehensive evaluations of two publicly available datasets, we demonstrate that FedMM notably outperforms two baselines in accuracy and AUC metrics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.15858v1-abstract-full').style.display = 'none'; document.getElementById('2402.15858v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2024 International Conference on Acoustics, <span class="search-hit mathjax">Speech</span> and Signal Processing (ICASSP 2024)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.15733">arXiv:2402.15733</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.15733">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ArEEG_Chars: Dataset for Envisioned <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> using EEG for Arabic Characters
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Darwish%2C+H">Hazem Darwish</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Malah%2C+A+A">Abdalrahman Al Malah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jallad%2C+K+A">Khloud Al Jallad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghneim%2C+N">Nada Ghneim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.15733v2-abstract-short" style="display: inline;">
        Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic cha&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.15733v2-abstract-full').style.display = 'inline'; document.getElementById('2402.15733v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.15733v2-abstract-full" style="display: none;">
        Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.15733v2-abstract-full').style.display = 'none'; document.getElementById('2402.15733v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.15151">arXiv:2402.15151</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.15151">pdf</a>, <a href="https://arxiv.org/format/2402.15151">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Where Visual <span class="search-hit mathjax">Speech</span> Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual <span class="search-hit mathjax">Speech</span> Processing
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yeo%2C+J+H">Jeong Hun Yeo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+S">Seunghee Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+M">Minsu Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ro%2C+Y+M">Yong Man Ro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.15151v2-abstract-short" style="display: inline;">
        In visual <span class="search-hit mathjax">speech</span> processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.15151v2-abstract-full').style.display = 'inline'; document.getElementById('2402.15151v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.15151v2-abstract-full" style="display: none;">
        In visual <span class="search-hit mathjax">speech</span> processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual <span class="search-hit mathjax">Speech</span> Processing incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to perform multi-tasks of visual <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of an LLM by employing a self-supervised visual <span class="search-hit mathjax">speech</span> model. Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual <span class="search-hit mathjax">speech</span> units. Through the proposed deduplication and Low Rank Adaptation (LoRA), VSP-LLM can be trained in a computationally efficient manner. In the translation dataset, the MuAViC benchmark, we demonstrate that VSP-LLM trained on just 30 hours of labeled data can more effectively translate lip movements compared to the recent model trained with 433 hours of data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.15151v2-abstract-full').style.display = 'none'; document.getElementById('2402.15151v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 May, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">An Erratum was added on the last page of this paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.14888">arXiv:2402.14888</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.14888">pdf</a>, <a href="https://arxiv.org/format/2402.14888">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient data selection employing Semantic Similarity-based Graph Structures for model training
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Petcu%2C+R">Roxana Petcu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Maji%2C+S">Subhadeep Maji</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.14888v1-abstract-short" style="display: inline;">
        &hellip;data through a compute-heavy model or other intensive pre-processing transformations. The application of this approach is demonstrated in the use case of low-resource automated <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.14888v1-abstract-full').style.display = 'inline'; document.getElementById('2402.14888v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.14888v1-abstract-full" style="display: none;">
        Recent developments in natural language processing (NLP) have highlighted the need for substantial amounts of data for models to capture textual information accurately. This raises concerns regarding the computational resources and time required for training such models. This paper introduces Semantics for data SAliency in Model performance Estimation (SeSaME). It is an efficient data sampling mechanism solely based on textual information without passing the data through a compute-heavy model or other intensive pre-processing transformations. The application of this approach is demonstrated in the use case of low-resource automated <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (ASR) models, which excessively rely on text-to-<span class="search-hit mathjax">speech</span> (TTS) calls when using augmented data. SeSaME learns to categorize new incoming data points into <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> difficulty buckets by employing semantic similarity-based graph structures and discrete ASR information from homophilous neighbourhoods through message passing. The results indicate reliable projections of ASR performance, with a 93% accuracy increase when using the proposed method compared to random predictions, bringing non-trivial information on the impact of textual representations in <span class="search-hit mathjax">speech</span> models. Furthermore, a series of experiments show both the benefits and challenges of using the ASR information on incoming data to fine-tune the model. We report a 7% drop in validation loss compared to random sampling, 7% WER drop with non-local aggregation when evaluating against a highly difficult dataset, and 1.8% WER drop with local aggregation and high semantic similarity between datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.14888v1-abstract-full').style.display = 'none'; document.getElementById('2402.14888v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICML 2023 Workshop: Sampling and Optimization in Discrete Space</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.14563">arXiv:2402.14563</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.14563">pdf</a>, <a href="https://arxiv.org/format/2402.14563">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1093/iwc/iwu016">10.1093/iwc/iwu016 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Wizard of Oz Experimentation for Language Technology Applications: Challenges and Tools
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Schl%C3%B6gl%2C+S">Stephan Schlögl</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Doherty%2C+G">Gavin Doherty</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luz%2C+S">Saturnino Luz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.14563v1-abstract-short" style="display: inline;">
        &hellip;extensive engineering effort would otherwise be needed to explore the design possibilities offered by such operations. The WOZ method has been widely used in connection with <span class="search-hit mathjax">speech</span> and language technologies, but advances in sensor technology and pattern&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.14563v1-abstract-full').style.display = 'inline'; document.getElementById('2402.14563v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.14563v1-abstract-full" style="display: none;">
        Wizard of OZ (WOZ) is a well-established method for simulating the functionality and user experience of future systems. Using a human wizard to mimic certain operations of a potential system is particularly useful in situations where extensive engineering effort would otherwise be needed to explore the design possibilities offered by such operations. The WOZ method has been widely used in connection with <span class="search-hit mathjax">speech</span> and language technologies, but advances in sensor technology and pattern <span class="search-hit mathjax">recognition</span> as well as new application areas such as human-robot interaction have made it increasingly relevant to the design of a wider range of interactive systems. In such cases achieving acceptable performance at the user interface level often hinges on resource intensive improvements such as domain tuning, which are better done once the overall design is relatively stable. While WOZ is recognised as a valuable prototyping technique, surprisingly little effort has been put into exploring it from a methodological point of view. Starting from a survey of the literature, this paper presents a systematic investigation and analysis of the design space for WOZ for language technology applications, and proposes a generic architecture for tool support that supports the integration of components for <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> and synthesis as well as for machine translation. This architecture is instantiated in WebWOZ - a new web-based open-source WOZ prototyping platform. The viability of generic support is explored empirically through a series of evaluations. Researchers from a variety of backgrounds were able to create experiments, independent of their previous experience with WOZ. The approach was further validated through a number of real experiments, which also helped to identify a number of possibilities for additional support, and flagged potential issues relating to consistency in Wizard performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.14563v1-abstract-full').style.display = 'none'; document.getElementById('2402.14563v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">28 pages</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Schlogl, S., Doherty, G., &amp; Luz, S. (2015). Wizard of Oz Experimentation for Language Technology Applications: Challenges and Tools. Interacting with Computers 27(6), pp. 592-615
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.14205">arXiv:2402.14205</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.14205">pdf</a>, <a href="https://arxiv.org/format/2402.14205">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Compression Robust Synthetic <span class="search-hit mathjax">Speech</span> Detection Using Patched Spectrogram Transformer
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yadav%2C+A+K+S">Amit Kumar Singh Yadav</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiang%2C+Z">Ziyue Xiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhagtani%2C+K">Kratika Bhagtani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bestagini%2C+P">Paolo Bestagini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tubaro%2C+S">Stefano Tubaro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Delp%2C+E+J">Edward J. Delp</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.14205v1-abstract-short" style="display: inline;">
        Many deep learning synthetic <span class="search-hit mathjax">speech</span> generation tools are readily available. The use of synthetic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.14205v1-abstract-full').style.display = 'inline'; document.getElementById('2402.14205v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.14205v1-abstract-full" style="display: none;">
        Many deep learning synthetic <span class="search-hit mathjax">speech</span> generation tools are readily available. The use of synthetic <span class="search-hit mathjax">speech</span> has caused financial fraud, impersonation of people, and misinformation to spread. For this reason forensic methods that can detect synthetic <span class="search-hit mathjax">speech</span> have been proposed. Existing methods often overfit on one dataset and their performance reduces substantially in practical scenarios such as detecting synthetic <span class="search-hit mathjax">speech</span> shared on social platforms. In this paper we propose, Patched Spectrogram Synthetic <span class="search-hit mathjax">Speech</span> Detection Transformer (PS3DT), a synthetic <span class="search-hit mathjax">speech</span> detector that converts a time domain <span class="search-hit mathjax">speech</span> signal to a mel-spectrogram and processes it in patches using a transformer neural network. We evaluate the detection performance of PS3DT on ASVspoof2019 dataset. Our experiments show that PS3DT performs well on ASVspoof2019 dataset compared to other approaches using spectrogram for synthetic <span class="search-hit mathjax">speech</span> detection. We also investigate generalization performance of PS3DT on In-the-Wild dataset. PS3DT generalizes well than several existing methods on detecting synthetic <span class="search-hit mathjax">speech</span> from an out-of-distribution dataset. We also evaluate robustness of PS3DT to detect telephone quality synthetic <span class="search-hit mathjax">speech</span> and synthetic <span class="search-hit mathjax">speech</span> shared on social platforms (compressed <span class="search-hit mathjax">speech</span>). PS3DT is robust to compression and can detect telephone quality synthetic <span class="search-hit mathjax">speech</span> better than several existing methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.14205v1-abstract-full').style.display = 'none'; document.getElementById('2402.14205v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted as long oral paper at ICMLA 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.14185">arXiv:2402.14185</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.14185">pdf</a>, <a href="https://arxiv.org/format/2402.14185">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HINT: High-quality INPainting Transformer with Mask-Aware Encoding and Enhanced Attention
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+S">Shuang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Atapour-Abarghouei%2C+A">Amir Atapour-Abarghouei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shum%2C+H+P+H">Hubert P. H. Shum</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.14185v1-abstract-short" style="display: inline;">
        &hellip;mechanism interpreting spatial awareness to model the corrupted image at multiple scales. To further enhance the effectiveness of SCAL, motivated by recent advanced in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, we introduce a sandwich structure that places feed-forward networks before and after the SCAL module. We demonstrate the superior pe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.14185v1-abstract-full').style.display = 'inline'; document.getElementById('2402.14185v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.14185v1-abstract-full" style="display: none;">
        Existing image inpainting methods leverage convolution-based downsampling approaches to reduce spatial dimensions. This may result in information loss from corrupted images where the available information is inherently sparse, especially for the scenario of large missing regions. Recent advances in self-attention mechanisms within transformers have led to significant improvements in many computer vision tasks including inpainting. However, limited by the computational costs, existing methods cannot fully exploit the efficacy of long-range modelling capabilities of such models. In this paper, we propose an end-to-end High-quality INpainting Transformer, abbreviated as HINT, which consists of a novel mask-aware pixel-shuffle downsampling module (MPD) to preserve the visible information extracted from the corrupted image while maintaining the integrity of the information available for high-level inferences made within the model. Moreover, we propose a Spatially-activated Channel Attention Layer (SCAL), an efficient self-attention mechanism interpreting spatial awareness to model the corrupted image at multiple scales. To further enhance the effectiveness of SCAL, motivated by recent advanced in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, we introduce a sandwich structure that places feed-forward networks before and after the SCAL module. We demonstrate the superior performance of HINT compared to contemporary state-of-the-art models on four datasets, CelebA, CelebA-HQ, Places2, and Dunhuang.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.14185v1-abstract-full').style.display = 'none'; document.getElementById('2402.14185v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.14154">arXiv:2402.14154</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.14154">pdf</a>, <a href="https://arxiv.org/format/2402.14154">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+Y">Yiqiao Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Choi%2C+M">Minje Choi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Verma%2C+G">Gaurav Verma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jindong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+S">Srijan Kumar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.14154v3-abstract-short" style="display: inline;">
        &hellip;MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate <span class="search-hit mathjax">speech</span> detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.14154v3-abstract-full').style.display = 'inline'; document.getElementById('2402.14154v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.14154v3-abstract-full" style="display: none;">
        Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising solution to these challenges, yet they struggle to accurately interpret human emotions and complex content such as misinformation. This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs&#39; understanding of multimodal social media content. MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate <span class="search-hit mathjax">speech</span> detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need for advancements in models&#39; social understanding capabilities. Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks. However, MLLMs demonstrate performance improvements post fine-tuning, suggesting potential pathways for improvement. Our code and data are available at https://github.com/claws-lab/MMSoc.git.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.14154v3-abstract-full').style.display = 'none'; document.getElementById('2402.14154v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In Proceedings of ACL 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.13957">arXiv:2402.13957</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.13957">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/ICSC59802.2024.00064">10.1109/ICSC59802.2024.00064 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kamuni%2C+N">Navin Kamuni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chintala%2C+S">Sathishkumar Chintala</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kunchakuri%2C+N">Naveen Kunchakuri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Narasimharaju%2C+J+S+A">Jyothi Swaroop Arlagadda Narasimharaju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+V">Venkat Kumar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.13957v2-abstract-short" style="display: inline;">
        Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio <span class="search-hit mathjax">recognition</span>. However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability. This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built on the Dejavu Project&#39;s foundations, the st&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13957v2-abstract-full').style.display = 'inline'; document.getElementById('2402.13957v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.13957v2-abstract-full" style="display: none;">
        Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio <span class="search-hit mathjax">recognition</span>. However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability. This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built on the Dejavu Project&#39;s foundations, the study emphasizes real-world scenario simulations with diverse background noises and distortions. Signal processing, central to Dejavu&#39;s model, includes the Fast Fourier Transform, spectrograms, and peak extraction. The &#34;constellation&#34; concept and fingerprint hashing enable unique song identification. Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency. Storage analysis highlights the critical space-speed trade-off for practical implementation. This research advances audio fingerprinting&#39;s adaptability, addressing challenges in varied environments and applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13957v2-abstract-full').style.display = 'none'; document.getElementById('2402.13957v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        2024 IEEE 18th International Conference on Semantic Computing (ICSC), Laguna Hills, CA, USA, 2024, pp. 341-345
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.13432">arXiv:2402.13432</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.13432">pdf</a>, <a href="https://arxiv.org/format/2402.13432">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Labrak%2C+Y">Yanis Labrak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bazoge%2C+A">Adrien Bazoge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khettari%2C+O+E">Oumaima El Khettari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rouvier%2C+M">Mickael Rouvier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Beaufils%2C+P+C+d">Pacome Constant dit Beaufils</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Grabar%2C+N">Natalia Grabar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Daille%2C+B">Beatrice Daille</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Quiniou%2C+S">Solen Quiniou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Morin%2C+E">Emmanuel Morin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gourraud%2C+P">Pierre-Antoine Gourraud</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dufour%2C+R">Richard Dufour</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.13432v1-abstract-short" style="display: inline;">
        &hellip;present the first-ever publicly available French biomedical language understanding benchmark called DrBenchmark. It encompasses 20 diversified tasks, including named-entity <span class="search-hit mathjax">recognition</span>, part-of-<span class="search-hit mathjax">speech</span> tagging, question-answering, semantic textual similarity, and classification. We evaluate 8 state-of-the-art pre-traine&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13432v1-abstract-full').style.display = 'inline'; document.getElementById('2402.13432v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.13432v1-abstract-full" style="display: none;">
        The biomedical domain has sparked a significant interest in the field of Natural Language Processing (NLP), which has seen substantial advancements with pre-trained language models (PLMs). However, comparing these models has proven challenging due to variations in evaluation protocols across different models. A fair solution is to aggregate diverse downstream tasks into a benchmark, allowing for the assessment of intrinsic PLMs qualities from various perspectives. Although still limited to few languages, this initiative has been undertaken in the biomedical field, notably English and Chinese. This limitation hampers the evaluation of the latest French biomedical models, as they are either assessed on a minimal number of tasks with non-standardized protocols or evaluated using general downstream tasks. To bridge this research gap and account for the unique sensitivities of French, we present the first-ever publicly available French biomedical language understanding benchmark called DrBenchmark. It encompasses 20 diversified tasks, including named-entity <span class="search-hit mathjax">recognition</span>, part-of-<span class="search-hit mathjax">speech</span> tagging, question-answering, semantic textual similarity, and classification. We evaluate 8 state-of-the-art pre-trained masked language models (MLMs) on general and biomedical-specific data, as well as English specific MLMs to assess their cross-lingual capabilities. Our experiments reveal that no single model excels across all tasks, while generalist models are sometimes still competitive.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13432v1-abstract-full').style.display = 'none'; document.getElementById('2402.13432v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at LREC-Coling 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.13349">arXiv:2402.13349</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.13349">pdf</a>, <a href="https://arxiv.org/format/2402.13349">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Aria Everyday Activities Dataset
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lv%2C+Z">Zhaoyang Lv</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Charron%2C+N">Nicholas Charron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moulon%2C+P">Pierre Moulon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gamino%2C+A">Alexander Gamino</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+C">Cheng Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sweeney%2C+C">Chris Sweeney</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Miller%2C+E">Edward Miller</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+H">Huixuan Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meissner%2C+J">Jeff Meissner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+J">Jing Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Somasundaram%2C+K">Kiran Somasundaram</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pesqueira%2C+L">Luis Pesqueira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schwesinger%2C+M">Mark Schwesinger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Parkhi%2C+O">Omkar Parkhi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+Q">Qiao Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=De+Nardi%2C+R">Renzo De Nardi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+S">Shangyi Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saarinen%2C+S">Steve Saarinen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baiyya%2C+V">Vijay Baiyya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zou%2C+Y">Yuyang Zou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Newcombe%2C+R">Richard Newcombe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Engel%2C+J+J">Jakob Julian Engel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+X">Xiaqing Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+C">Carl Ren</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.13349v2-abstract-short" style="display: inline;">
        &hellip;In addition, AEA provides machine perception data including high frequency globally aligned 3D trajectories, scene point cloud, per-frame 3D eye gaze vector and time aligned <span class="search-hit mathjax">speech</span> transcription. In this paper, we demonstrate a few exemplar research applications enabled by this dataset, including neural scene reconstruction and prompted segmentation. AEA is&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13349v2-abstract-full').style.display = 'inline'; document.getElementById('2402.13349v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.13349v2-abstract-full" style="display: none;">
        We present Aria Everyday Activities (AEA) Dataset, an egocentric multimodal open dataset recorded using Project Aria glasses. AEA contains 143 daily activity sequences recorded by multiple wearers in five geographically diverse indoor locations. Each of the recording contains multimodal sensor data recorded through the Project Aria glasses. In addition, AEA provides machine perception data including high frequency globally aligned 3D trajectories, scene point cloud, per-frame 3D eye gaze vector and time aligned <span class="search-hit mathjax">speech</span> transcription. In this paper, we demonstrate a few exemplar research applications enabled by this dataset, including neural scene reconstruction and prompted segmentation. AEA is an open source dataset that can be downloaded from https://www.projectaria.com/datasets/aea/. We are also providing open-source implementations and examples of how to use the dataset in Project Aria Tools https://github.com/facebookresearch/projectaria_tools.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13349v2-abstract-full').style.display = 'none'; document.getElementById('2402.13349v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 February, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Dataset website: https://www.projectaria.com/datasets/aea/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.13208">arXiv:2402.13208</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.13208">pdf</a>, <a href="https://arxiv.org/format/2402.13208">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How do Hyenas deal with Human <span class="search-hit mathjax">Speech</span>? <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> and Translation with ConfHyena
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gaido%2C+M">Marco Gaido</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papi%2C+S">Sara Papi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Negri%2C+M">Matteo Negri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bentivogli%2C+L">Luisa Bentivogli</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.13208v1-abstract-short" style="display: inline;">
        &hellip;and computational complexity. Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for <span class="search-hit mathjax">speech</span> processing, where the long input sequences cause high computational costs. Through experiments in automatic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13208v1-abstract-full').style.display = 'inline'; document.getElementById('2402.13208v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.13208v1-abstract-full" style="display: none;">
        The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity. Consequently, research efforts in the last few years focused on finding more efficient alternatives. Among them, Hyena (Poli et al., 2023) stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity. Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for <span class="search-hit mathjax">speech</span> processing, where the long input sequences cause high computational costs. Through experiments in automatic <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (~1%), which, in most cases, is not statistically significant.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13208v1-abstract-full').style.display = 'none'; document.getElementById('2402.13208v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at LREC-COLING 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.13152">arXiv:2402.13152</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.13152">pdf</a>, <a href="https://arxiv.org/format/2402.13152">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual <span class="search-hit mathjax">Speech</span> Technologies
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Acosta-Triana%2C+J">José-M. Acosta-Triana</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gimeno-G%C3%B3mez%2C+D">David Gimeno-Gómez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mart%C3%ADnez-Hinarejos%2C+C">Carlos-D. Martínez-Hinarejos</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.13152v1-abstract-short" style="display: inline;">
        More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by <span class="search-hit mathjax">speech</span> technologies. Albeit self-supervised&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13152v1-abstract-full').style.display = 'inline'; document.getElementById('2402.13152v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.13152v1-abstract-full" style="display: none;">
        More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by <span class="search-hit mathjax">speech</span> technologies. Albeit self-supervised <span class="search-hit mathjax">speech</span> representations, recent massive <span class="search-hit mathjax">speech</span> corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English. This situation is aggravated when tasks involving both acoustic and visual <span class="search-hit mathjax">speech</span> modalities are addressed. In order to promote research on low-resource languages for audio-visual <span class="search-hit mathjax">speech</span> technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription. In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database not initially conceived for this type of task. The AnnoTheia toolkit, tutorials, and pre-trained models are available on GitHub.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13152v1-abstract-full').style.display = 'none'; document.getElementById('2402.13152v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.13076">arXiv:2402.13076</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.13076">pdf</a>, <a href="https://arxiv.org/format/2402.13076">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Not All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shangguan%2C+Y">Yuan Shangguan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuhao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lai%2C+L">Liangzhen Lai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+E">Ernie Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+C">Changsheng Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+Y">Yangyang Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chandra%2C+V">Vikas Chandra</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.13076v1-abstract-short" style="display: inline;">
        Power consumption plays an important role in on-device streaming <span class="search-hit mathjax">speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13076v1-abstract-full').style.display = 'inline'; document.getElementById('2402.13076v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.13076v1-abstract-full" style="display: none;">
        Power consumption plays an important role in on-device streaming <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span>, as it has a direct impact on the user experience. This study delves into how weight parameters in <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models influence the overall power consumption of these models. We discovered that the impact of weight parameters on power consumption varies, influenced by factors including how often they are invoked and their placement in memory. Armed with this insight, we developed design guidelines aimed at optimizing on-device <span class="search-hit mathjax">speech</span> <span class="search-hit mathjax">recognition</span> models. These guidelines focus on minimizing power use without substantially affecting accuracy. Our method, which employs targeted compression based on the varying sensitivities of weight parameters, demonstrates superior performance compared to state-of-the-art compression methods. It achieves a reduction in energy usage of up to 47% while maintaining similar model accuracy and improving the real-time factor.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13076v1-abstract-full').style.display = 'none'; document.getElementById('2402.13076v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.13018">arXiv:2402.13018</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.13018">pdf</a>, <a href="https://arxiv.org/format/2402.13018">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EMO-SUPERB: An In-depth Look at <span class="search-hit mathjax">Speech</span> Emotion <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+H">Haibin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chou%2C+H">Huang-Cheng Chou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goncalves%2C+L">Lucas Goncalves</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+J">Jiawei Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jang%2C+J+R">Jyh-Shing Roger Jang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+C">Chi-Chun Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-Yi Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.13018v4-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Speech</span> emotion&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13018v4-abstract-full').style.display = 'inline'; document.getElementById('2402.13018v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.13018v4-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Speech</span> emotion <span class="search-hit mathjax">recognition</span> (SER) is a pivotal technology for human-computer interaction systems. However, 80.77% of SER papers yield results that cannot be reproduced. We develop EMO-SUPERB, short for EMOtion <span class="search-hit mathjax">Speech</span> Universal PERformance Benchmark, which aims to enhance open-source initiatives for SER. EMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art <span class="search-hit mathjax">speech</span> self-supervised learning models (SSLMs) for exhaustive evaluation across six open-source SER datasets. EMO-SUPERB streamlines result sharing via an online leaderboard, fostering collaboration within a community-driven benchmark and thereby enhancing the development of SER. On average, 2.58% of annotations are annotated using natural language. SER relies on classification models and is unable to process natural languages, leading to the discarding of these valuable annotations. We prompt ChatGPT to mimic annotators, comprehend natural language annotations, and subsequently re-label the data. By utilizing labels generated by ChatGPT, we consistently achieve an average relative gain of 3.08% across all settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13018v4-abstract-full').style.display = 'none'; document.getElementById('2402.13018v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 March, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">webpage: https://emosuperb.github.io/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.13004">arXiv:2402.13004</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.13004">pdf</a>, <a href="https://arxiv.org/format/2402.13004">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gimeno-G%C3%B3mez%2C+D">David Gimeno-Gómez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mart%C3%ADnez-Hinarejos%2C+C">Carlos-D. Martínez-Hinarejos</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.13004v1-abstract-short" style="display: inline;">
        Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual <span class="search-hit mathjax">Speech</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13004v1-abstract-full').style.display = 'inline'; document.getElementById('2402.13004v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.13004v1-abstract-full" style="display: none;">
        Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual <span class="search-hit mathjax">Speech</span> <span class="search-hit mathjax">Recognition</span> (VSR). Similar to other <span class="search-hit mathjax">speech</span> processing tasks, these end-to-end VSR systems are usually based on encoder-decoder architectures. While encoders are somewhat general, multiple decoding approaches have been explored, such as the conventional hybrid model based on Deep Neural Networks combined with Hidden Markov Models (DNN-HMM) or the Connectionist Temporal Classification (CTC) paradigm. However, there are languages and tasks in which data is scarce, and in this situation, there is not a clear comparison between different types of decoders. Therefore, we focused our study on how the conventional DNN-HMM decoder and its state-of-the-art CTC/Attention counterpart behave depending on the amount of data used for their estimation. We also analyzed to what extent our visual <span class="search-hit mathjax">speech</span> features were able to adapt to scenarios for which they were not explicitly trained, either considering a similar dataset or another collected for a different language. Results showed that the conventional paradigm reached <span class="search-hit mathjax">recognition</span> rates that improve the CTC/Attention model in data-scarcity scenarios along with a reduced training time and fewer parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.13004v1-abstract-full').style.display = 'none'; document.getElementById('2402.13004v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=950"
      class="pagination-previous">Previous
    </a>
    
    
      <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=1050"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=0"
          class="pagination-link "
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                                               
          <li><span class="pagination-ellipsis">&hellip;</span></li>
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=950"
              class="pagination-link "
              aria-label="Page 20"
              aria-current="page">20
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=1000"
              class="pagination-link is-current"
              aria-label="Page 21"
              aria-current="page">21
            </a>
          </li>
          
          
          
          <li>
            <a href="/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Speech+recognition&amp;terms-0-field=all&amp;classification-computer_science=y&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first&amp;start=1050"
              class="pagination-link "
              aria-label="Page 22"
              aria-current="page">22
            </a>
          </li>
          
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

    
  

      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>